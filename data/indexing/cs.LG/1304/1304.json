[{"id": "1304.0035", "submitter": "Ivan Selesnick", "authors": "Po-Yu Chen and Ivan W. Selesnick", "title": "Translation-Invariant Shrinkage/Thresholding of Group Sparse Signals", "comments": "33 pages, 7 figures, 5 tables", "journal-ref": null, "doi": "10.1016/j.sigpro.2013.06", "report-no": null, "categories": "cs.CV cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses signal denoising when large-amplitude coefficients form\nclusters (groups). The L1-norm and other separable sparsity models do not\ncapture the tendency of coefficients to cluster (group sparsity). This work\ndevelops an algorithm, called 'overlapping group shrinkage' (OGS), based on the\nminimization of a convex cost function involving a group-sparsity promoting\npenalty function. The groups are fully overlapping so the denoising method is\ntranslation-invariant and blocking artifacts are avoided. Based on the\nprinciple of majorization-minimization (MM), we derive a simple iterative\nminimization algorithm that reduces the cost function monotonically. A\nprocedure for setting the regularization parameter, based on attenuating the\nnoise to a specified level, is also described. The proposed approach is\nillustrated on speech enhancement, wherein the OGS approach is applied in the\nshort-time Fourier transform (STFT) domain. The denoised speech produced by OGS\ndoes not suffer from musical noise.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 22:00:01 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Chen", "Po-Yu", ""], ["Selesnick", "Ivan W.", ""]]}, {"id": "1304.0160", "submitter": "Nabarun Mondal Mr", "authors": "Nabarun Mondal and Partha P. Ghosh", "title": "Parallel Computation Is ESS", "comments": "Submitted to Theoretical Computer Science - Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are enormous amount of examples of Computation in nature, exemplified\nacross multiple species in biology. One crucial aim for these computations\nacross all life forms their ability to learn and thereby increase the chance of\ntheir survival. In the current paper a formal definition of autonomous learning\nis proposed. From that definition we establish a Turing Machine model for\nlearning, where rule tables can be added or deleted, but can not be modified.\nSequential and parallel implementations of this model are discussed. It is\nfound that for general purpose learning based on this model, the\nimplementations capable of parallel execution would be evolutionarily stable.\nThis is proposed to be of the reasons why in Nature parallelism in computation\nis found in abundance.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2013 06:45:47 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2013 19:54:16 GMT"}, {"version": "v3", "created": "Thu, 11 Apr 2013 16:41:02 GMT"}, {"version": "v4", "created": "Sun, 14 Apr 2013 16:03:07 GMT"}, {"version": "v5", "created": "Thu, 18 Apr 2013 17:37:38 GMT"}, {"version": "v6", "created": "Sun, 24 Nov 2013 16:37:10 GMT"}, {"version": "v7", "created": "Sun, 1 Dec 2013 11:16:28 GMT"}, {"version": "v8", "created": "Wed, 25 Dec 2013 16:18:54 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Mondal", "Nabarun", ""], ["Ghosh", "Partha P.", ""]]}, {"id": "1304.0682", "submitter": "Cem Aksoylar", "authors": "Cem Aksoylar, George Atia, Venkatesh Saligrama", "title": "Sparse Signal Processing with Linear and Nonlinear Observations: A\n  Unified Shannon-Theoretic Approach", "comments": "Final version submitted to Trans. IT", "journal-ref": null, "doi": "10.1109/TIT.2016.2605122", "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive fundamental sample complexity bounds for recovering sparse and\nstructured signals for linear and nonlinear observation models including sparse\nregression, group testing, multivariate regression and problems with missing\nfeatures. In general, sparse signal processing problems can be characterized in\nterms of the following Markovian property. We are given a set of $N$ variables\n$X_1,X_2,\\ldots,X_N$, and there is an unknown subset of variables $S \\subset\n\\{1,\\ldots,N\\}$ that are relevant for predicting outcomes $Y$. More\nspecifically, when $Y$ is conditioned on $\\{X_n\\}_{n\\in S}$ it is conditionally\nindependent of the other variables, $\\{X_n\\}_{n \\not \\in S}$. Our goal is to\nidentify the set $S$ from samples of the variables $X$ and the associated\noutcomes $Y$. We characterize this problem as a version of the noisy channel\ncoding problem. Using asymptotic information theoretic analyses, we establish\nmutual information formulas that provide sufficient and necessary conditions on\nthe number of samples required to successfully recover the salient variables.\nThese mutual information expressions unify conditions for both linear and\nnonlinear observations. We then compute sample complexity bounds for the\naforementioned models, based on the mutual information expressions in order to\ndemonstrate the applicability and flexibility of our results in general sparse\nsignal processing models.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2013 16:35:28 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2013 20:07:20 GMT"}, {"version": "v3", "created": "Fri, 18 Oct 2013 21:57:50 GMT"}, {"version": "v4", "created": "Mon, 11 Nov 2013 21:25:31 GMT"}, {"version": "v5", "created": "Thu, 29 Jan 2015 00:44:11 GMT"}, {"version": "v6", "created": "Sat, 14 Feb 2015 02:03:22 GMT"}, {"version": "v7", "created": "Mon, 18 Jan 2016 21:29:56 GMT"}, {"version": "v8", "created": "Thu, 25 Aug 2016 20:46:55 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Aksoylar", "Cem", ""], ["Atia", "George", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1304.0725", "submitter": "Ashok P", "authors": "P. Ashok, G.M Kadhar Nawaz, E. Elayaraja, V. Vadivel", "title": "Improved Performance of Unsupervised Method by Renovated K-Means", "comments": "7 pages, to strengthen the k means algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a separation of data into groups of similar objects. Every\ngroup called cluster consists of objects that are similar to one another and\ndissimilar to objects of other groups. In this paper, the K-Means algorithm is\nimplemented by three distance functions and to identify the optimal distance\nfunction for clustering methods. The proposed K-Means algorithm is compared\nwith K-Means, Static Weighted K-Means (SWK-Means) and Dynamic Weighted K-Means\n(DWK-Means) algorithm by using Davis Bouldin index, Execution Time and\nIteration count methods. Experimental results show that the proposed K-Means\nalgorithm performed better on Iris and Wine dataset when compared with other\nthree clustering methods.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 05:28:06 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["Ashok", "P.", ""], ["Nawaz", "G. M Kadhar", ""], ["Elayaraja", "E.", ""], ["Vadivel", "V.", ""]]}, {"id": "1304.0730", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman and Pravesh Kothari and Jan Vondrak", "title": "Representation, Approximation and Learning of Submodular Functions Using\n  Low-rank Decision Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of approximate representation and learning of\nsubmodular functions over the uniform distribution on the Boolean hypercube\n$\\{0,1\\}^n$. Our main result is the following structural theorem: any\nsubmodular function is $\\epsilon$-close in $\\ell_2$ to a real-valued decision\ntree (DT) of depth $O(1/\\epsilon^2)$. This immediately implies that any\nsubmodular function is $\\epsilon$-close to a function of at most\n$2^{O(1/\\epsilon^2)}$ variables and has a spectral $\\ell_1$ norm of\n$2^{O(1/\\epsilon^2)}$. It also implies the closest previous result that states\nthat submodular functions can be approximated by polynomials of degree\n$O(1/\\epsilon^2)$ (Cheraghchi et al., 2012). Our result is proved by\nconstructing an approximation of a submodular function by a DT of rank\n$4/\\epsilon^2$ and a proof that any rank-$r$ DT can be $\\epsilon$-approximated\nby a DT of depth $\\frac{5}{2}(r+\\log(1/\\epsilon))$.\n  We show that these structural results can be exploited to give an\nattribute-efficient PAC learning algorithm for submodular functions running in\ntime $\\tilde{O}(n^2) \\cdot 2^{O(1/\\epsilon^{4})}$. The best previous algorithm\nfor the problem requires $n^{O(1/\\epsilon^{2})}$ time and examples (Cheraghchi\net al., 2012) but works also in the agnostic setting. In addition, we give\nimproved learning algorithms for a number of related settings.\n  We also prove that our PAC and agnostic learning algorithms are essentially\noptimal via two lower bounds: (1) an information-theoretic lower bound of\n$2^{\\Omega(1/\\epsilon^{2/3})}$ on the complexity of learning monotone\nsubmodular functions in any reasonable model; (2) computational lower bound of\n$n^{\\Omega(1/\\epsilon^{2/3})}$ based on a reduction to learning of sparse\nparities with noise, widely-believed to be intractable. These are the first\nlower bounds for learning of submodular functions over the uniform\ndistribution.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2013 18:37:35 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["Feldman", "Vitaly", ""], ["Kothari", "Pravesh", ""], ["Vondrak", "Jan", ""]]}, {"id": "1304.0740", "submitter": "Lijun Zhang", "authors": "Lijun Zhang, Tianbao Yang, Rong Jin, Xiaofei He", "title": "O(logT) Projections for Stochastic Optimization of Smooth and Strongly\n  Convex Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional algorithms for stochastic optimization require projecting the\nsolution at each iteration into a given domain to ensure its feasibility. When\nfacing complex domains, such as positive semi-definite cones, the projection\noperation can be expensive, leading to a high computational cost per iteration.\nIn this paper, we present a novel algorithm that aims to reduce the number of\nprojections for stochastic optimization. The proposed algorithm combines the\nstrength of several recent developments in stochastic optimization, including\nmini-batch, extra-gradient, and epoch gradient descent, in order to effectively\nexplore the smoothness and strong convexity. We show, both in expectation and\nwith a high probability, that when the objective function is both smooth and\nstrongly convex, the proposed algorithm achieves the optimal $O(1/T)$ rate of\nconvergence with only $O(\\log T)$ projections. Our empirical study verifies the\ntheoretical result.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2013 19:11:23 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["Zhang", "Lijun", ""], ["Yang", "Tianbao", ""], ["Jin", "Rong", ""], ["He", "Xiaofei", ""]]}, {"id": "1304.0840", "submitter": "Chunhua Shen", "authors": "Peng Wang, Chunhua Shen, Anton van den Hengel", "title": "A Fast Semidefinite Approach to Solving Binary Quadratic Problems", "comments": "Appearing in Proc. IEEE Conf. Computer Vision and Pattern\n  Recognition, 2013", "journal-ref": null, "doi": "10.1109/CVPR.2013.173", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many computer vision problems can be formulated as binary quadratic programs\n(BQPs). Two classic relaxation methods are widely used for solving BQPs,\nnamely, spectral methods and semidefinite programming (SDP), each with their\nown advantages and disadvantages. Spectral relaxation is simple and easy to\nimplement, but its bound is loose. Semidefinite relaxation has a tighter bound,\nbut its computational complexity is high for large scale problems. We present a\nnew SDP formulation for BQPs, with two desirable properties. First, it has a\nsimilar relaxation bound to conventional SDP formulations. Second, compared\nwith conventional SDP methods, the new SDP formulation leads to a significantly\nmore efficient and scalable dual optimization approach, which has the same\ndegree of complexity as spectral methods. Extensive experiments on various\napplications including clustering, image segmentation, co-segmentation and\nregistration demonstrate the usefulness of our SDP formulation for solving\nlarge-scale BQPs.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 04:31:10 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Wang", "Peng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1304.1014", "submitter": "Emanuele Frandi", "authors": "Hector Allende, Emanuele Frandi, Ricardo Nanculef, Claudio Sartori", "title": "A Novel Frank-Wolfe Algorithm. Analysis and Applications to Large-Scale\n  SVM Training", "comments": "REVISED VERSION (October 2013) -- Title and abstract have been\n  revised. Section 5 was added. Some proofs have been summarized (full-length\n  proofs available in the previous version)", "journal-ref": "Information Sciences 285, 66-99, 2014", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been a renewed interest in the machine learning community\nfor variants of a sparse greedy approximation procedure for concave\noptimization known as {the Frank-Wolfe (FW) method}. In particular, this\nprocedure has been successfully applied to train large-scale instances of\nnon-linear Support Vector Machines (SVMs). Specializing FW to SVM training has\nallowed to obtain efficient algorithms but also important theoretical results,\nincluding convergence analysis of training algorithms and new characterizations\nof model sparsity.\n  In this paper, we present and analyze a novel variant of the FW method based\non a new way to perform away steps, a classic strategy used to accelerate the\nconvergence of the basic FW procedure. Our formulation and analysis is focused\non a general concave maximization problem on the simplex. However, the\nspecialization of our algorithm to quadratic forms is strongly related to some\nclassic methods in computational geometry, namely the Gilbert and MDM\nalgorithms.\n  On the theoretical side, we demonstrate that the method matches the\nguarantees in terms of convergence rate and number of iterations obtained by\nusing classic away steps. In particular, the method enjoys a linear rate of\nconvergence, a result that has been recently proved for MDM on quadratic forms.\n  On the practical side, we provide experiments on several classification\ndatasets, and evaluate the results using statistical tests. Experiments show\nthat our method is faster than the FW method with classic away steps, and works\nwell even in the cases in which classic away steps slow down the algorithm.\nFurthermore, these improvements are obtained without sacrificing the predictive\naccuracy of the obtained SVM model.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 17:15:43 GMT"}, {"version": "v2", "created": "Sun, 13 Oct 2013 09:50:26 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Allende", "Hector", ""], ["Frandi", "Emanuele", ""], ["Nanculef", "Ricardo", ""], ["Sartori", "Claudio", ""]]}, {"id": "1304.1018", "submitter": "Ronan Collobert", "authors": "Dimitri Palaz, Ronan Collobert, Mathew Magimai.-Doss", "title": "Estimating Phoneme Class Conditional Probabilities from Raw Speech\n  Signal using Convolutional Neural Networks", "comments": "In Interspeech 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In hybrid hidden Markov model/artificial neural networks (HMM/ANN) automatic\nspeech recognition (ASR) system, the phoneme class conditional probabilities\nare estimated by first extracting acoustic features from the speech signal\nbased on prior knowledge such as, speech perception or/and speech production\nknowledge, and, then modeling the acoustic features with an ANN. Recent\nadvances in machine learning techniques, more specifically in the field of\nimage processing and text processing, have shown that such divide and conquer\nstrategy (i.e., separating feature extraction and modeling steps) may not be\nnecessary. Motivated from these studies, in the framework of convolutional\nneural networks (CNNs), this paper investigates a novel approach, where the\ninput to the ANN is raw speech signal and the output is phoneme class\nconditional probability estimates. On TIMIT phoneme recognition task, we study\ndifferent ANN architectures to show the benefit of CNNs and compare the\nproposed approach against conventional approach where, spectral-based feature\nMFCC is extracted and modeled by a multilayer perceptron. Our studies show that\nthe proposed approach can yield comparable or better phoneme recognition\nperformance when compared to the conventional approach. It indicates that CNNs\ncan learn features relevant for phoneme classification automatically from the\nraw speech signal.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 17:20:41 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2013 11:23:34 GMT"}], "update_date": "2013-06-13", "authors_parsed": [["Palaz", "Dimitri", ""], ["Collobert", "Ronan", ""], ["-Doss", "Mathew Magimai.", ""]]}, {"id": "1304.1192", "submitter": "Lijun Zhang", "authors": "Qi Qian, Rong Jin, Jinfeng Yi, Lijun Zhang, Shenghuo Zhu", "title": "Efficient Distance Metric Learning by Adaptive Sampling and Mini-Batch\n  Stochastic Gradient Descent (SGD)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance metric learning (DML) is an important task that has found\napplications in many domains. The high computational cost of DML arises from\nthe large number of variables to be determined and the constraint that a\ndistance metric has to be a positive semi-definite (PSD) matrix. Although\nstochastic gradient descent (SGD) has been successfully applied to improve the\nefficiency of DML, it can still be computationally expensive because in order\nto ensure that the solution is a PSD matrix, it has to, at every iteration,\nproject the updated distance metric onto the PSD cone, an expensive operation.\nWe address this challenge by developing two strategies within SGD, i.e.\nmini-batch and adaptive sampling, to effectively reduce the number of updates\n(i.e., projections onto the PSD cone) in SGD. We also develop hybrid approaches\nthat combine the strength of adaptive sampling with that of mini-batch online\nlearning techniques to further improve the computational efficiency of SGD for\nDML. We prove the theoretical guarantees for both adaptive sampling and\nmini-batch based approaches for DML. We also conduct an extensive empirical\nstudy to verify the effectiveness of the proposed algorithms for DML.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 21:14:50 GMT"}], "update_date": "2013-04-05", "authors_parsed": [["Qian", "Qi", ""], ["Jin", "Rong", ""], ["Yi", "Jinfeng", ""], ["Zhang", "Lijun", ""], ["Zhu", "Shenghuo", ""]]}, {"id": "1304.1391", "submitter": "Sachin Talathi", "authors": "Manu Nandan, Pramod P. Khargonekar, Sachin S. Talathi", "title": "Fast SVM training using approximate extreme points", "comments": "The manuscript in revised form has been submitted to J. Machine\n  Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of non-linear kernel Support Vector Machines (SVMs) to large\ndatasets is seriously hampered by its excessive training time. We propose a\nmodification, called the approximate extreme points support vector machine\n(AESVM), that is aimed at overcoming this burden. Our approach relies on\nconducting the SVM optimization over a carefully selected subset, called the\nrepresentative set, of the training dataset. We present analytical results that\nindicate the similarity of AESVM and SVM solutions. A linear time algorithm\nbased on convex hulls and extreme points is used to compute the representative\nset in kernel space. Extensive computational experiments on nine datasets\ncompared AESVM to LIBSVM \\citep{LIBSVM}, CVM \\citep{Tsang05}, BVM\n\\citep{Tsang07}, LASVM \\citep{Bordes05}, $\\text{SVM}^{\\text{perf}}$\n\\citep{Joachims09}, and the random features method \\citep{rahimi07}. Our AESVM\nimplementation was found to train much faster than the other methods, while its\nclassification accuracy was similar to that of LIBSVM in all cases. In\nparticular, for a seizure detection dataset, AESVM training was almost $10^3$\ntimes faster than LIBSVM and LASVM and more than forty times faster than CVM\nand BVM. Additionally, AESVM also gave competitively fast classification times.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 15:08:31 GMT"}], "update_date": "2013-04-05", "authors_parsed": [["Nandan", "Manu", ""], ["Khargonekar", "Pramod P.", ""], ["Talathi", "Sachin S.", ""]]}, {"id": "1304.1574", "submitter": "Chao Zhang", "authors": "Chao Zhang, Lei Zhang, Jieping Ye", "title": "Generalization Bounds for Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide a new framework to obtain the generalization bounds\nof the learning process for domain adaptation, and then apply the derived\nbounds to analyze the asymptotical convergence of the learning process. Without\nloss of generality, we consider two kinds of representative domain adaptation:\none is with multiple sources and the other is combining source and target data.\n  In particular, we use the integral probability metric to measure the\ndifference between two domains. For either kind of domain adaptation, we\ndevelop a related Hoeffding-type deviation inequality and a symmetrization\ninequality to achieve the corresponding generalization bound based on the\nuniform entropy number. We also generalized the classical McDiarmid's\ninequality to a more general setting where independent random variables can\ntake values from different domains. By using this inequality, we then obtain\ngeneralization bounds based on the Rademacher complexity. Afterwards, we\nanalyze the asymptotic convergence and the rate of convergence of the learning\nprocess for such kind of domain adaptation. Meanwhile, we discuss the factors\nthat affect the asymptotic behavior of the learning process and the numerical\nexperiments support our theoretical findings as well.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 22:34:55 GMT"}], "update_date": "2013-04-08", "authors_parsed": [["Zhang", "Chao", ""], ["Zhang", "Lei", ""], ["Ye", "Jieping", ""]]}, {"id": "1304.1677", "submitter": "Sowmya Kamath S", "authors": "Sunil Joy Dommati, Ruchi Agrawal, Ram Mohana Reddy G. and S. Sowmya\n  Kamath", "title": "Bug Classification: Feature Extraction and Comparison of Event Model\n  using Na\\\"ive Bayes Approach", "comments": "5 pages, International Conference on Recent Trends in Computer and\n  Information Engineering (ICRTCIE'2012) April 13-15, 2012 Pattaya,\n  http://psrcentre.org/images/extraimages/412138.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In software industries, individuals at different levels from customer to an\nengineer apply diverse mechanisms to detect to which class a particular bug\nshould be allocated. Sometimes while a simple search in Internet might help, in\nmany other cases a lot of effort is spent in analyzing the bug report to\nclassify the bug. So there is a great need of a structured mining algorithm -\nwhere given a crash log, the existing bug database could be mined to find out\nthe class to which the bug should be allocated. This would involve Mining\npatterns and applying different classification algorithms. This paper focuses\non the feature extraction, noise reduction in data and classification of\nnetwork bugs using probabilistic Na\\\"ive Bayes approach. Different event models\nlike Bernoulli and Multinomial are applied on the extracted features. When new,\nunseen bugs are given as input to the algorithms, the performance comparison of\ndifferent algorithms is done on the basis of accuracy and recall parameters.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2013 11:05:18 GMT"}], "update_date": "2013-04-08", "authors_parsed": [["Dommati", "Sunil Joy", ""], ["Agrawal", "Ruchi", ""], ["G.", "Ram Mohana Reddy", ""], ["Kamath", "S. Sowmya", ""]]}, {"id": "1304.1995", "submitter": "Liang Liu", "authors": "Liu Liang", "title": "Image Retrieval using Histogram Factorization and Contextual Similarity\n  Learning", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image retrieval has been a top topic in the field of both computer vision and\nmachine learning for a long time. Content based image retrieval, which tries to\nretrieve images from a database visually similar to a query image, has\nattracted much attention. Two most important issues of image retrieval are the\nrepresentation and ranking of the images. Recently, bag-of-words based method\nhas shown its power as a representation method. Moreover, nonnegative matrix\nfactorization is also a popular way to represent the data samples. In addition,\ncontextual similarity learning has also been studied and proven to be an\neffective method for the ranking problem. However, these technologies have\nnever been used together. In this paper, we developed an effective image\nretrieval system by representing each image using the bag-of-words method as\nhistograms, and then apply the nonnegative matrix factorization to factorize\nthe histograms, and finally learn the ranking score using the contextual\nsimilarity learning method. The proposed novel system is evaluated on a large\nscale image database and the effectiveness is shown.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2013 13:15:17 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2013 17:59:33 GMT"}], "update_date": "2013-04-10", "authors_parsed": [["Liang", "Liu", ""]]}, {"id": "1304.2024", "submitter": "Trong Nghia Hoang", "authors": "Trong Nghia Hoang and Kian Hsiang Low", "title": "A General Framework for Interacting Bayes-Optimally with Self-Interested\n  Agents using Arbitrary Parametric Model and Model Prior", "comments": "23rd International Joint Conference on Artificial Intelligence (IJCAI\n  2013), Extended version with proofs, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Bayesian reinforcement learning (BRL) have shown that\nBayes-optimality is theoretically achievable by modeling the environment's\nlatent dynamics using Flat-Dirichlet-Multinomial (FDM) prior. In\nself-interested multi-agent environments, the transition dynamics are mainly\ncontrolled by the other agent's stochastic behavior for which FDM's\nindependence and modeling assumptions do not hold. As a result, FDM does not\nallow the other agent's behavior to be generalized across different states nor\nspecified using prior domain knowledge. To overcome these practical limitations\nof FDM, we propose a generalization of BRL to integrate the general class of\nparametric models and model priors, thus allowing practitioners' domain\nknowledge to be exploited to produce a fine-grained and compact representation\nof the other agent's behavior. Empirical evaluation shows that our approach\noutperforms existing multi-agent reinforcement learning algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2013 17:00:37 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2013 11:34:51 GMT"}, {"version": "v3", "created": "Sun, 16 Mar 2014 15:10:35 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Hoang", "Trong Nghia", ""], ["Low", "Kian Hsiang", ""]]}, {"id": "1304.2079", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman and Pravesh Kothari", "title": "Learning Coverage Functions and Private Release of Marginals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of approximating and learning coverage functions. A\nfunction $c: 2^{[n]} \\rightarrow \\mathbf{R}^{+}$ is a coverage function, if\nthere exists a universe $U$ with non-negative weights $w(u)$ for each $u \\in U$\nand subsets $A_1, A_2, \\ldots, A_n$ of $U$ such that $c(S) = \\sum_{u \\in\n\\cup_{i \\in S} A_i} w(u)$. Alternatively, coverage functions can be described\nas non-negative linear combinations of monotone disjunctions. They are a\nnatural subclass of submodular functions and arise in a number of applications.\n  We give an algorithm that for any $\\gamma,\\delta>0$, given random and uniform\nexamples of an unknown coverage function $c$, finds a function $h$ that\napproximates $c$ within factor $1+\\gamma$ on all but $\\delta$-fraction of the\npoints in time $poly(n,1/\\gamma,1/\\delta)$. This is the first fully-polynomial\nalgorithm for learning an interesting class of functions in the demanding PMAC\nmodel of Balcan and Harvey (2011). Our algorithms are based on several new\nstructural properties of coverage functions. Using the results in (Feldman and\nKothari, 2014), we also show that coverage functions are learnable agnostically\nwith excess $\\ell_1$-error $\\epsilon$ over all product and symmetric\ndistributions in time $n^{\\log(1/\\epsilon)}$. In contrast, we show that,\nwithout assumptions on the distribution, learning coverage functions is at\nleast as hard as learning polynomial-size disjoint DNF formulas, a class of\nfunctions for which the best known algorithm runs in time\n$2^{\\tilde{O}(n^{1/3})}$ (Klivans and Servedio, 2004).\n  As an application of our learning results, we give simple\ndifferentially-private algorithms for releasing monotone conjunction counting\nqueries with low average error. In particular, for any $k \\leq n$, we obtain\nprivate release of $k$-way marginals with average error $\\bar{\\alpha}$ in time\n$n^{O(\\log(1/\\bar{\\alpha}))}$.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2013 00:06:26 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2013 23:42:11 GMT"}, {"version": "v3", "created": "Wed, 28 May 2014 00:38:46 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Feldman", "Vitaly", ""], ["Kothari", "Pravesh", ""]]}, {"id": "1304.2302", "submitter": "Jonathan Malmaud", "authors": "Dan Lovell, Jonathan Malmaud, Ryan P. Adams, Vikash K. Mansinghka", "title": "ClusterCluster: Parallel Markov Chain Monte Carlo for Dirichlet Process\n  Mixtures", "comments": "12 pages, 10 figures. Submitted to ICML 2013 during third submission\n  cycle", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dirichlet process (DP) is a fundamental mathematical tool for Bayesian\nnonparametric modeling, and is widely used in tasks such as density estimation,\nnatural language processing, and time series modeling. Although MCMC inference\nmethods for the DP often provide a gold standard in terms asymptotic accuracy,\nthey can be computationally expensive and are not obviously parallelizable. We\npropose a reparameterization of the Dirichlet process that induces conditional\nindependencies between the atoms that form the random measure. This conditional\nindependence enables many of the Markov chain transition operators for DP\ninference to be simulated in parallel across multiple cores. Applied to mixture\nmodeling, our approach enables the Dirichlet process to simultaneously learn\nclusters that describe the data and superclusters that define the granularity\nof parallelization. Unlike previous approaches, our technique does not require\nalteration of the model and leaves the true posterior distribution invariant.\nIt also naturally lends itself to a distributed software implementation in\nterms of Map-Reduce, which we test in cluster configurations of over 50\nmachines and 100 cores. We present experiments exploring the parallel\nefficiency and convergence properties of our approach on both synthetic and\nreal-world data, including runs on 1MM data vectors in 256 dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2013 18:34:32 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Lovell", "Dan", ""], ["Malmaud", "Jonathan", ""], ["Adams", "Ryan P.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1304.2331", "submitter": "Niko Br\\\"ummer", "authors": "Niko Brummer and Johan du Preez", "title": "The PAV algorithm optimizes binary proper scoring rules", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been much recent interest in application of the\npool-adjacent-violators (PAV) algorithm for the purpose of calibrating the\nprobabilistic outputs of automatic pattern recognition and machine learning\nalgorithms. Special cost functions, known as proper scoring rules form natural\nobjective functions to judge the goodness of such calibration. We show that for\nbinary pattern classifiers, the non-parametric optimization of calibration,\nsubject to a monotonicity constraint, can be solved by PAV and that this\nsolution is optimal for all regular binary proper scoring rules. This extends\nprevious results which were limited to convex binary proper scoring rules. We\nfurther show that this result holds not only for calibration of probabilities,\nbut also for calibration of log-likelihood-ratios, in which case optimality\nholds independently of the prior probabilities of the pattern classes.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2013 19:49:51 GMT"}], "update_date": "2013-04-11", "authors_parsed": [["Brummer", "Niko", ""], ["Preez", "Johan du", ""]]}, {"id": "1304.2363", "submitter": "Suk Wah Kwok", "authors": "Suk Wah Kwok, Chris Carter", "title": "Multiple decision trees", "comments": "Appears in Proceedings of the Fourth Conference on Uncertainty in\n  Artificial Intelligence (UAI1988)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1988-PG-213-220", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes experiments, on two domains, to investigate the effect\nof averaging over predictions of multiple decision trees, instead of using a\nsingle tree. Other authors have pointed out theoretical and commonsense reasons\nfor preferring the multiple tree approach. Ideally, we would like to consider\npredictions from all trees, weighted by their probability. However, there is a\nvast number of different trees, and it is difficult to estimate the probability\nof each tree. We sidestep the estimation problem by using a modified version of\nthe ID3 algorithm to build good trees, and average over only these trees. Our\nresults are encouraging. For each domain, we managed to produce a small number\nof good trees. We find that it is best to average across sets of trees with\ndifferent structure; this usually gives better performance than any of the\nconstituent trees, including the ID3 tree.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 19:43:53 GMT"}], "update_date": "2013-04-10", "authors_parsed": [["Kwok", "Suk Wah", ""], ["Carter", "Chris", ""]]}, {"id": "1304.2490", "submitter": "Yanhui Xiao", "authors": "Yanhui Xiao, Zhenfeng Zhu, Yao Zhao", "title": "Kernel Reconstruction ICA for Sparse Representation", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent Component Analysis (ICA) is an effective unsupervised tool to\nlearn statistically independent representation. However, ICA is not only\nsensitive to whitening but also difficult to learn an over-complete basis.\nConsequently, ICA with soft Reconstruction cost(RICA) was presented to learn\nsparse representations with over-complete basis even on unwhitened data.\nWhereas RICA is infeasible to represent the data with nonlinear structure due\nto its intrinsic linearity. In addition, RICA is essentially an unsupervised\nmethod and can not utilize the class information. In this paper, we propose a\nkernel ICA model with reconstruction constraint (kRICA) to capture the\nnonlinear features. To bring in the class information, we further extend the\nunsupervised kRICA to a supervised one by introducing a discrimination\nconstraint, namely d-kRICA. This constraint leads to learn a structured basis\nconsisted of basis vectors from different basis subsets corresponding to\ndifferent class labels. Then each subset will sparsely represent well for its\nown class but not for the others. Furthermore, data samples belonging to the\nsame class will have similar representations, and thereby the learned sparse\nrepresentations can take more discriminative power. Experimental results\nvalidate the effectiveness of kRICA and d-kRICA for image classification.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2013 08:45:57 GMT"}], "update_date": "2013-04-10", "authors_parsed": [["Xiao", "Yanhui", ""], ["Zhu", "Zhenfeng", ""], ["Zhao", "Yao", ""]]}, {"id": "1304.2850", "submitter": "Haiping Huang", "authors": "Haiping Huang, K. Y. Michael Wong and Yoshiyuki Kabashima", "title": "Entropy landscape of solutions in the binary perceptron problem", "comments": "21 pages, 6 figures, version accepted by Journal of Physics A:\n  Mathematical and Theoretical", "journal-ref": "J. Phys. A: Math. Theor. 46 (2013) 375002", "doi": "10.1088/1751-8113/46/37/375002", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical picture of the solution space for a binary perceptron is\nstudied. The binary perceptron learns a random classification of input random\npatterns by a set of binary synaptic weights. The learning of this network is\ndifficult especially when the pattern (constraint) density is close to the\ncapacity, which is supposed to be intimately related to the structure of the\nsolution space. The geometrical organization is elucidated by the entropy\nlandscape from a reference configuration and of solution-pairs separated by a\ngiven Hamming distance in the solution space. We evaluate the entropy at the\nannealed level as well as replica symmetric level and the mean field result is\nconfirmed by the numerical simulations on single instances using the proposed\nmessage passing algorithms. From the first landscape (a random configuration as\na reference), we see clearly how the solution space shrinks as more constraints\nare added. From the second landscape of solution-pairs, we deduce the\ncoexistence of clustering and freezing in the solution space.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 06:17:07 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2013 02:15:12 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Huang", "Haiping", ""], ["Wong", "K. Y. Michael", ""], ["Kabashima", "Yoshiyuki", ""]]}, {"id": "1304.2865", "submitter": "Niko Br\\\"ummer", "authors": "Niko Br\\\"ummer and Edward de Villiers", "title": "The BOSARIS Toolkit: Theory, Algorithms and Code for Surviving the New\n  DCF", "comments": "presented at: The NIST SRE'11 Analysis Workshop, Atlanta, December\n  2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The change of two orders of magnitude in the 'new DCF' of NIST's SRE'10,\nrelative to the 'old DCF' evaluation criterion, posed a difficult challenge for\nparticipants and evaluator alike. Initially, participants were at a loss as to\nhow to calibrate their systems, while the evaluator underestimated the required\nnumber of evaluation trials. After the fact, it is now obvious that both\ncalibration and evaluation require very large sets of trials. This poses the\nchallenges of (i) how to decide what number of trials is enough, and (ii) how\nto process such large data sets with reasonable memory and CPU requirements.\nAfter SRE'10, at the BOSARIS Workshop, we built solutions to these problems\ninto the freely available BOSARIS Toolkit. This paper explains the principles\nand algorithms behind this toolkit. The main contributions of the toolkit are:\n1. The Normalized Bayes Error-Rate Plot, which analyses likelihood- ratio\ncalibration over a wide range of DCF operating points. These plots also help in\njudging the adequacy of the sizes of calibration and evaluation databases. 2.\nEfficient algorithms to compute DCF and minDCF for large score files, over the\nrange of operating points required by these plots. 3. A new score file format,\nwhich facilitates working with very large trial lists. 4. A faster logistic\nregression optimizer for fusion and calibration. 5. A principled way to define\nEER (equal error rate), which is of practical interest when the absolute error\ncount is small.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 07:32:31 GMT"}], "update_date": "2013-04-11", "authors_parsed": [["Br\u00fcmmer", "Niko", ""], ["de Villiers", "Edward", ""]]}, {"id": "1304.2994", "submitter": "Francesco Orabona", "authors": "Francesco Orabona, Koby Crammer, Nicol\\`o Cesa-Bianchi", "title": "A Generalized Online Mirror Descent with Applications to Classification\n  and Regression", "comments": null, "journal-ref": "Machine Learning June 2015, Volume 99, Issue 3, pp 411-435", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning algorithms are fast, memory-efficient, easy to implement, and\napplicable to many prediction problems, including classification, regression,\nand ranking. Several online algorithms were proposed in the past few decades,\nsome based on additive updates, like the Perceptron, and some on multiplicative\nupdates, like Winnow. A unifying perspective on the design and the analysis of\nonline algorithms is provided by online mirror descent, a general prediction\nstrategy from which most first-order algorithms can be obtained as special\ncases. We generalize online mirror descent to time-varying regularizers with\ngeneric updates. Unlike standard mirror descent, our more general formulation\nalso captures second order algorithms, algorithms for composite losses and\nalgorithms for adaptive filtering. Moreover, we recover, and sometimes improve,\nknown regret bounds as special cases of our analysis using specific\nregularizers. Finally, we show the power of our approach by deriving a new\nsecond order algorithm with a regret bound invariant with respect to arbitrary\nrescalings of individual features.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 15:26:13 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2013 19:59:10 GMT"}, {"version": "v3", "created": "Mon, 14 Jul 2014 01:05:45 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Orabona", "Francesco", ""], ["Crammer", "Koby", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""]]}, {"id": "1304.3285", "submitter": "Colorado Reed", "authors": "Colorado Reed and Zoubin Ghahramani", "title": "Scaling the Indian Buffet Process via Submodular Maximization", "comments": "13 pages, 8 figures", "journal-ref": "In ICML 2013: JMLR W&CP 28 (3): 1013-1021, 2013", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference for latent feature models is inherently difficult as the inference\nspace grows exponentially with the size of the input data and number of latent\nfeatures. In this work, we use Kurihara & Welling (2008)'s\nmaximization-expectation framework to perform approximate MAP inference for\nlinear-Gaussian latent feature models with an Indian Buffet Process (IBP)\nprior. This formulation yields a submodular function of the features that\ncorresponds to a lower bound on the model evidence. By adding a constant to\nthis function, we obtain a nonnegative submodular function that can be\nmaximized via a greedy algorithm that obtains at least a one-third\napproximation to the optimal solution. Our inference method scales linearly\nwith the size of the input data, and we show the efficacy of our method on the\nlargest datasets currently analyzed using an IBP model.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2013 13:20:51 GMT"}, {"version": "v2", "created": "Wed, 8 May 2013 20:15:08 GMT"}, {"version": "v3", "created": "Tue, 18 Jun 2013 14:24:58 GMT"}, {"version": "v4", "created": "Wed, 24 Jul 2013 19:20:15 GMT"}], "update_date": "2013-07-25", "authors_parsed": [["Reed", "Colorado", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1304.3345", "submitter": "Marzieh Parandehgheibi", "authors": "Marzieh Parandehgheibi", "title": "Probabilistic Classification using Fuzzy Support Vector Machines", "comments": "6 pages, Proceedings of the 6th INFORMS Workshop on Data Mining and\n  Health Informatics (DM-HI 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical applications such as recognizing the type of a tumor as Malignant\nor Benign, a wrong diagnosis can be devastating. Methods like Fuzzy Support\nVector Machines (FSVM) try to reduce the effect of misplaced training points by\nassigning a lower weight to the outliers. However, there are still uncertain\npoints which are similar to both classes and assigning a class by the given\ninformation will cause errors. In this paper, we propose a two-phase\nclassification method which probabilistically assigns the uncertain points to\neach of the classes. The proposed method is applied to the Breast Cancer\nWisconsin (Diagnostic) Dataset which consists of 569 instances in 2 classes of\nMalignant and Benign. This method assigns certain instances to their\nappropriate classes with probability of one, and the uncertain instances to\neach of the classes with associated probabilities. Therefore, based on the\ndegree of uncertainty, doctors can suggest further examinations before making\nthe final diagnosis.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2013 15:44:18 GMT"}], "update_date": "2013-04-12", "authors_parsed": [["Parandehgheibi", "Marzieh", ""]]}, {"id": "1304.3432", "submitter": "Stephen Jose Hanson", "authors": "Stephen Jose Hanson, Malcolm Bauer", "title": "Machine Learning, Clustering, and Polymorphy", "comments": "Appears in Proceedings of the First Conference on Uncertainty in\n  Artificial Intelligence (UAI1985)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1985-PG-117-128", "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a machine induction program (WITT) that attempts to\nmodel human categorization. Properties of categories to which human subjects\nare sensitive includes best or prototypical members, relative contrasts between\nputative categories, and polymorphy (neither necessary or sufficient features).\nThis approach represents an alternative to usual Artificial Intelligence\napproaches to generalization and conceptual clustering which tend to focus on\nnecessary and sufficient feature rules, equivalence classes, and simple search\nand match schemes. WITT is shown to be more consistent with human\ncategorization while potentially including results produced by more traditional\nclustering schemes. Applications of this approach in the domains of expert\nsystems and information retrieval are also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2013 19:56:55 GMT"}], "update_date": "2013-04-15", "authors_parsed": [["Hanson", "Stephen Jose", ""], ["Bauer", "Malcolm", ""]]}, {"id": "1304.3568", "submitter": "Pierre Chainais", "authors": "Pierre Chainais and C\\'edric Richard", "title": "Distributed dictionary learning over a sensor network", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of distributed dictionary learning, where a set of\nnodes is required to collectively learn a common dictionary from noisy\nmeasurements. This approach may be useful in several contexts including sensor\nnetworks. Diffusion cooperation schemes have been proposed to solve the\ndistributed linear regression problem. In this work we focus on a\ndiffusion-based adaptive dictionary learning strategy: each node records\nobservations and cooperates with its neighbors by sharing its local dictionary.\nThe resulting algorithm corresponds to a distributed block coordinate descent\n(alternate optimization). Beyond dictionary learning, this strategy could be\nadapted to many matrix factorization problems and generalized to various\nsettings. This article presents our approach and illustrates its efficiency on\nsome numerical examples.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 08:47:38 GMT"}], "update_date": "2013-04-15", "authors_parsed": [["Chainais", "Pierre", ""], ["Richard", "C\u00e9dric", ""]]}, {"id": "1304.3708", "submitter": "Yevgeny Seldin", "authors": "Yevgeny Seldin and Peter Bartlett and Koby Crammer", "title": "Advice-Efficient Prediction with Expert Advice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advice-efficient prediction with expert advice (in analogy to label-efficient\nprediction) is a variant of prediction with expert advice game, where on each\nround of the game we are allowed to ask for advice of a limited number $M$ out\nof $N$ experts. This setting is especially interesting when asking for advice\nof every expert on every round is expensive. We present an algorithm for\nadvice-efficient prediction with expert advice that achieves\n$O(\\sqrt{\\frac{N}{M}T\\ln N})$ regret on $T$ rounds of the game.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 19:09:56 GMT"}], "update_date": "2013-04-15", "authors_parsed": [["Seldin", "Yevgeny", ""], ["Bartlett", "Peter", ""], ["Crammer", "Koby", ""]]}, {"id": "1304.3745", "submitter": "Khadoudja Ghanem", "authors": "Khadoudja Ghanem", "title": "Towards more accurate clustering method by using dynamic time warping", "comments": "12 pages, 1 figure, 2 tables, journal. arXiv admin note: text overlap\n  with arXiv:1206.3509 by other authors", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP) Vol.3, No.2, March 2013", "doi": "10.5121/ijdkp.2013.3207", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An intrinsic problem of classifiers based on machine learning (ML) methods is\nthat their learning time grows as the size and complexity of the training\ndataset increases. For this reason, it is important to have efficient\ncomputational methods and algorithms that can be applied on large datasets,\nsuch that it is still possible to complete the machine learning tasks in\nreasonable time. In this context, we present in this paper a more accurate\nsimple process to speed up ML methods. An unsupervised clustering algorithm is\ncombined with Expectation, Maximization (EM) algorithm to develop an efficient\nHidden Markov Model (HMM) training. The idea of the proposed process consists\nof two steps. In the first step, training instances with similar inputs are\nclustered and a weight factor which represents the frequency of these instances\nis assigned to each representative cluster. Dynamic Time Warping technique is\nused as a dissimilarity function to cluster similar examples. In the second\nstep, all formulas in the classical HMM training algorithm (EM) associated with\nthe number of training instances are modified to include the weight factor in\nappropriate terms. This process significantly accelerates HMM training while\nmaintaining the same initial, transition and emission probabilities matrixes as\nthose obtained with the classical HMM training algorithm. Accordingly, the\nclassification accuracy is preserved. Depending on the size of the training\nset, speedups of up to 2200 times is possible when the size is about 100.000\ninstances. The proposed approach is not limited to training HMMs, but it can be\nemployed for a large variety of MLs methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 22:23:53 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Ghanem", "Khadoudja", ""]]}, {"id": "1304.3760", "submitter": "Eric Bair", "authors": "Sheila Gaynor and Eric Bair", "title": "Identification of relevant subtypes via preweighted sparse clustering", "comments": "Version 4: 49 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis methods are used to identify homogeneous subgroups in a data\nset. In biomedical applications, one frequently applies cluster analysis in\norder to identify biologically interesting subgroups. In particular, one may\nwish to identify subgroups that are associated with a particular outcome of\ninterest. Conventional clustering methods generally do not identify such\nsubgroups, particularly when there are a large number of high-variance features\nin the data set. Conventional methods may identify clusters associated with\nthese high-variance features when one wishes to obtain secondary clusters that\nare more interesting biologically or more strongly associated with a particular\noutcome of interest. A modification of sparse clustering can be used to\nidentify such secondary clusters or clusters associated with an outcome of\ninterest. This method correctly identifies such clusters of interest in several\nsimulation scenarios. The method is also applied to a large prospective cohort\nstudy of temporomandibular disorders and a leukemia microarray data set.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2013 02:15:20 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2013 14:14:13 GMT"}, {"version": "v3", "created": "Wed, 21 Sep 2016 23:59:53 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Gaynor", "Sheila", ""], ["Bair", "Eric", ""]]}, {"id": "1304.3840", "submitter": "Badreddine Meftahi", "authors": "Badreddine Meftahi, Ourida Ben Boubaker Saidi", "title": "A New Homogeneity Inter-Clusters Measure in SemiSupervised Clustering", "comments": "9 pages, 27 figures, International Journal of Computer Applications\n  see http://www.ijcaonline.org/", "journal-ref": "journal = {International Journal of Computer Applications}, year =\n  {2013}, volume = {66}, number = {24}, pages = {37-45}, month = {March}, note\n  = {Published by Foundation of Computer Science, New York, USA}", "doi": "10.5120/11267-6526", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies in data mining have proposed a new learning called\nsemi-Supervised. Such type of learning combines unlabeled and labeled data\nwhich are hard to obtain. However, in unsupervised methods, the only unlabeled\ndata are used. The problem of significance and the effectiveness of\nsemi-supervised clustering results is becoming of main importance. This paper\npursues the thesis that muchgreater accuracy can be achieved in such clustering\nby improving the similarity computing. Hence, we introduce a new approach of\nsemisupervised clustering using an innovative new homogeneity measure of\ngenerated clusters. Our experimental results demonstrate significantly improved\naccuracy as a result.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2013 20:19:25 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Meftahi", "Badreddine", ""], ["Saidi", "Ourida Ben Boubaker", ""]]}, {"id": "1304.4077", "submitter": "Pritam Ranjan", "authors": "Reshu Agarwal, Pritam Ranjan, Hugh Chipman", "title": "A new Bayesian ensemble of trees classifier for identifying multi-class\n  labels in satellite images", "comments": "31 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of satellite images is a key component of many remote sensing\napplications. One of the most important products of a raw satellite image is\nthe classified map which labels the image pixels into meaningful classes.\nThough several parametric and non-parametric classifiers have been developed\nthus far, accurate labeling of the pixels still remains a challenge. In this\npaper, we propose a new reliable multiclass-classifier for identifying class\nlabels of a satellite image in remote sensing applications. The proposed\nmulticlass-classifier is a generalization of a binary classifier based on the\nflexible ensemble of regression trees model called Bayesian Additive Regression\nTrees (BART). We used three small areas from the LANDSAT 5 TM image, acquired\non August 15, 2009 (path/row: 08/29, L1T product, UTM map projection) over\nKings County, Nova Scotia, Canada to classify the land-use. Several prediction\naccuracy and uncertainty measures have been used to compare the reliability of\nthe proposed classifier with the state-of-the-art classifiers in remote\nsensing.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2013 12:54:52 GMT"}, {"version": "v2", "created": "Fri, 31 May 2013 16:57:33 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Agarwal", "Reshu", ""], ["Ranjan", "Pritam", ""], ["Chipman", "Hugh", ""]]}, {"id": "1304.4344", "submitter": "Conrad Sanderson", "authors": "Mehrtash T. Harandi, Conrad Sanderson, Richard Hartley, Brian C.\n  Lovell", "title": "Sparse Coding and Dictionary Learning for Symmetric Positive Definite\n  Matrices: A Kernel Approach", "comments": null, "journal-ref": "European Conference on Computer Vision, Lecture Notes in Computer\n  Science (LNCS), Vol. 7573, pp. 216-229, 2012", "doi": "10.1007/978-3-642-33709-3_16", "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances suggest that a wide range of computer vision problems can be\naddressed more appropriately by considering non-Euclidean geometry. This paper\ntackles the problem of sparse coding and dictionary learning in the space of\nsymmetric positive definite matrices, which form a Riemannian manifold. With\nthe aid of the recently introduced Stein kernel (related to a symmetric version\nof Bregman matrix divergence), we propose to perform sparse coding by embedding\nRiemannian manifolds into reproducing kernel Hilbert spaces. This leads to a\nconvex and kernel version of the Lasso problem, which can be solved\nefficiently. We furthermore propose an algorithm for learning a Riemannian\ndictionary (used for sparse coding), closely tied to the Stein kernel.\nExperiments on several classification tasks (face recognition, texture\nclassification, person re-identification) show that the proposed sparse coding\napproach achieves notable improvements in discrimination accuracy, in\ncomparison to state-of-the-art methods such as tensor sparse coding, Riemannian\nlocality preserving projection, and symmetry-driven accumulation of local\nfeatures.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 06:47:03 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Harandi", "Mehrtash T.", ""], ["Sanderson", "Conrad", ""], ["Hartley", "Richard", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1304.4610", "submitter": "Yuxin Chen", "authors": "Yuxin Chen, Yuejie Chi", "title": "Spectral Compressed Sensing via Structured Matrix Completion", "comments": "accepted to International Conference on Machine Learning (ICML 2013)", "journal-ref": "Journal of Machine Learning Research, W&CP 28 (3) :414-422, 2013", "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper studies the problem of recovering a spectrally sparse object from a\nsmall number of time domain samples. Specifically, the object of interest with\nambient dimension $n$ is assumed to be a mixture of $r$ complex\nmulti-dimensional sinusoids, while the underlying frequencies can assume any\nvalue in the unit disk. Conventional compressed sensing paradigms suffer from\nthe {\\em basis mismatch} issue when imposing a discrete dictionary on the\nFourier representation. To address this problem, we develop a novel\nnonparametric algorithm, called enhanced matrix completion (EMaC), based on\nstructured matrix completion. The algorithm starts by arranging the data into a\nlow-rank enhanced form with multi-fold Hankel structure, then attempts recovery\nvia nuclear norm minimization. Under mild incoherence conditions, EMaC allows\nperfect recovery as soon as the number of samples exceeds the order of\n$\\mathcal{O}(r\\log^{2} n)$. We also show that, in many instances, accurate\ncompletion of a low-rank multi-fold Hankel matrix is possible when the number\nof observed entries is proportional to the information theoretical limits\n(except for a logarithmic gap). The robustness of EMaC against bounded noise\nand its applicability to super resolution are further demonstrated by numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 20:26:15 GMT"}, {"version": "v2", "created": "Wed, 1 May 2013 00:29:31 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Chen", "Yuxin", ""], ["Chi", "Yuejie", ""]]}, {"id": "1304.4633", "submitter": "Brendan Juba", "authors": "Brendan Juba", "title": "PAC Quasi-automatizability of Resolution over Restricted Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider principled alternatives to unsupervised learning in data mining\nby situating the learning task in the context of the subsequent analysis task.\nSpecifically, we consider a query-answering (hypothesis-testing) task: In the\ncombined task, we decide whether an input query formula is satisfied over a\nbackground distribution by using input examples directly, rather than invoking\na two-stage process in which (i) rules over the distribution are learned by an\nunsupervised learning algorithm and (ii) a reasoning algorithm decides whether\nor not the query formula follows from the learned rules. In a previous work\n(2013), we observed that the learning task could satisfy numerous desirable\ncriteria in this combined context -- effectively matching what could be\nachieved by agnostic learning of CNFs from partial information -- that are not\nknown to be achievable directly. In this work, we show that likewise, there are\nreasoning tasks that are achievable in such a combined context that are not\nknown to be achievable directly (and indeed, have been seriously conjectured to\nbe impossible, cf. (Alekhnovich and Razborov, 2008)). Namely, we test for a\nresolution proof of the query formula of a given size in quasipolynomial time\n(that is, \"quasi-automatizing\" resolution). The learning setting we consider is\na partial-information, restricted-distribution setting that generalizes\nlearning parities over the uniform distribution from partial information,\nanother task that is known not to be achievable directly in various models (cf.\n(Ben-David and Dichterman, 1998) and (Michael, 2010)).\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 22:10:26 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Juba", "Brendan", ""]]}, {"id": "1304.4642", "submitter": "Maris Ozols", "authors": "Andrew M. Childs, Robin Kothari, Maris Ozols, Martin Roetteler", "title": "Easy and hard functions for the Boolean hidden shift problem", "comments": "29 pages, 2 figures", "journal-ref": "Proceedings of TQC 2013, LIPIcs, vol. 22, pp. 50-79, ISBN\n  978-3-939897-55-2 (2013)", "doi": "10.4230/LIPIcs.TQC.2013.50", "report-no": null, "categories": "quant-ph cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the quantum query complexity of the Boolean hidden shift problem.\nGiven oracle access to f(x+s) for a known Boolean function f, the task is to\ndetermine the n-bit string s. The quantum query complexity of this problem\ndepends strongly on f. We demonstrate that the easiest instances of this\nproblem correspond to bent functions, in the sense that an exact one-query\nalgorithm exists if and only if the function is bent. We partially characterize\nthe hardest instances, which include delta functions. Moreover, we show that\nthe problem is easy for random functions, since two queries suffice. Our\nalgorithm for random functions is based on performing the pretty good\nmeasurement on several copies of a certain state; its analysis relies on the\nFourier transform. We also use this approach to improve the quantum rejection\nsampling approach to the Boolean hidden shift problem.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 23:24:38 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["Childs", "Andrew M.", ""], ["Kothari", "Robin", ""], ["Ozols", "Maris", ""], ["Roetteler", "Martin", ""]]}, {"id": "1304.4806", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko", "title": "Unsupervised model-free representation learning", "comments": "The update is the journal version appearing in IEEE IT transactions\n  under the title \"Time-series information and unsupervised learning of\n  representations.\" This version includes important corrections and new\n  results. Some of the results (presented in previous versions) were reported\n  at ISIT'13 and ALT'13", "journal-ref": null, "doi": "10.1109/TIT.2019.2961814", "report-no": null, "categories": "cs.LG q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous control and learning problems face the situation where sequences of\nhigh-dimensional highly dependent data are available but no or little feedback\nis provided to the learner, which makes any inference rather challenging. To\naddress this challenge, we formulate the following problem. Given a series of\nobservations $X_0,\\dots,X_n$ coming from a large (high-dimensional) space\n$\\mathcal X$, find a representation function $f$ mapping $\\mathcal X$ to a\nfinite space $\\mathcal Y$ such that the series $f(X_0),\\dots,f(X_n)$ preserves\nas much information as possible about the original time-series dependence in\n$X_0,\\dots,X_n$. We show that, for stationary time series, the function $f$ can\nbe selected as the one maximizing a certain information criterion that we call\ntime-series information. Some properties of this functions are investigated,\nincluding its uniqueness and consistency of its empirical estimates.\n  Implications for the problem of optimal control are presented.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 13:06:59 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2013 14:00:35 GMT"}, {"version": "v3", "created": "Mon, 22 Jul 2013 09:45:16 GMT"}, {"version": "v4", "created": "Wed, 25 Dec 2019 18:08:49 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Ryabko", "Daniil", ""]]}, {"id": "1304.5063", "submitter": "Hichem Bannour", "authors": "Hichem Bannour and C\\'eline Hudelot", "title": "Combinaison d'information visuelle, conceptuelle, et contextuelle pour\n  la construction automatique de hierarchies semantiques adaptees a\n  l'annotation d'images", "comments": "RFIA 2012 (Reconnaissance des Formes et Intelligence Artificielle)\n  Lyon, France pg. 462-469. 9 pages", "journal-ref": "RFIA 2012 (Reconnaissance des Formes et Intelligence Artificielle)\n  Lyon, France pg. 462-469", "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  This paper proposes a new methodology to automatically build semantic\nhierarchies suitable for image annotation and classification. The building of\nthe hierarchy is based on a new measure of semantic similarity. The proposed\nmeasure incorporates several sources of information: visual, conceptual and\ncontextual as we defined in this paper. The aim is to provide a measure that\nbest represents image semantics. We then propose rules based on this measure,\nfor the building of the final hierarchy, and which explicitly encode\nhierarchical relationships between different concepts. Therefore, the built\nhierarchy is used in a semantic hierarchical classification framework for image\nannotation. Our experiments and results show that the hierarchy built improves\nclassification results.\n  Ce papier propose une nouvelle methode pour la construction automatique de\nhierarchies semantiques adaptees a la classification et a l'annotation\nd'images. La construction de la hierarchie est basee sur une nouvelle mesure de\nsimilarite semantique qui integre plusieurs sources d'informations: visuelle,\nconceptuelle et contextuelle que nous definissons dans ce papier. L'objectif\nest de fournir une mesure qui est plus proche de la semantique des images. Nous\nproposons ensuite des regles, basees sur cette mesure, pour la construction de\nla hierarchie finale qui encode explicitement les relations hierarchiques entre\nles differents concepts. La hierarchie construite est ensuite utilisee dans un\ncadre de classification semantique hierarchique d'images en concepts visuels.\nNos experiences et resultats montrent que la hierarchie construite permet\nd'ameliorer les resultats de la classification.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2013 09:40:12 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2013 11:49:10 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Bannour", "Hichem", ""], ["Hudelot", "C\u00e9line", ""]]}, {"id": "1304.5168", "submitter": "Jialu Liu", "authors": "Jialu Liu", "title": "Image Retrieval based on Bag-of-Words model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article gives a survey for bag-of-words (BoW) or bag-of-features model\nin image retrieval system. In recent years, large-scale image retrieval shows\nsignificant potential in both industry applications and research problems. As\nlocal descriptors like SIFT demonstrate great discriminative power in solving\nvision problems like object recognition, image classification and annotation,\nmore and more state-of-the-art large scale image retrieval systems are trying\nto rely on them. A common way to achieve this is first quantizing local\ndescriptors into visual words, and then applying scalable textual indexing and\nretrieval schemes. We call this model as bag-of-words or bag-of-features model.\nThe goal of this survey is to give an overview of this model and introduce\ndifferent strategies when building the system based on this model.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2013 15:57:34 GMT"}], "update_date": "2013-04-19", "authors_parsed": [["Liu", "Jialu", ""]]}, {"id": "1304.5299", "submitter": "Anoop Korattikara", "authors": "Anoop Korattikara, Yutian Chen, Max Welling", "title": "Austerity in MCMC Land: Cutting the Metropolis-Hastings Budget", "comments": "v4 - version accepted by ICML2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we make Bayesian posterior MCMC sampling more efficient when faced with\nvery large datasets? We argue that computing the likelihood for N datapoints in\nthe Metropolis-Hastings (MH) test to reach a single binary decision is\ncomputationally inefficient. We introduce an approximate MH rule based on a\nsequential hypothesis test that allows us to accept or reject samples with high\nconfidence using only a fraction of the data required for the exact MH rule.\nWhile this method introduces an asymptotic bias, we show that this bias can be\ncontrolled and is more than offset by a decrease in variance due to our ability\nto draw more samples per unit of time.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 02:51:52 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2013 21:13:59 GMT"}, {"version": "v3", "created": "Fri, 19 Jul 2013 18:05:53 GMT"}, {"version": "v4", "created": "Fri, 14 Feb 2014 07:42:15 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Korattikara", "Anoop", ""], ["Chen", "Yutian", ""], ["Welling", "Max", ""]]}, {"id": "1304.5350", "submitter": "Emile Contal", "authors": "Emile Contal and David Buffoni and Alexandre Robicquet and Nicolas\n  Vayatis", "title": "Parallel Gaussian Process Optimization with Upper Confidence Bound and\n  Pure Exploration", "comments": null, "journal-ref": "Proceedings of ECML 2013, pp.225-240", "doi": "10.1007/978-3-642-40988-2_15", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the challenge of maximizing an unknown function f\nfor which evaluations are noisy and are acquired with high cost. An iterative\nprocedure uses the previous measures to actively select the next estimation of\nf which is predicted to be the most useful. We focus on the case where the\nfunction can be evaluated in parallel with batches of fixed size and analyze\nthe benefit compared to the purely sequential procedure in terms of cumulative\nregret. We introduce the Gaussian Process Upper Confidence Bound and Pure\nExploration algorithm (GP-UCB-PE) which combines the UCB strategy and Pure\nExploration in the same batch of evaluations along the parallel iterations. We\nprove theoretical upper bounds on the regret with batches of size K for this\nprocedure which show the improvement of the order of sqrt{K} for fixed\niteration cost over purely sequential versions. Moreover, the multiplicative\nconstants involved have the property of being dimension-free. We also confirm\nempirically the efficiency of GP-UCB-PE on real and synthetic problems compared\nto state-of-the-art competitors.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 09:11:34 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2013 14:35:17 GMT"}, {"version": "v3", "created": "Mon, 2 Sep 2013 07:57:23 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Contal", "Emile", ""], ["Buffoni", "David", ""], ["Robicquet", "Alexandre", ""], ["Vayatis", "Nicolas", ""]]}, {"id": "1304.5457", "submitter": "Joonseok Lee", "authors": "Joonseok Lee, Kisung Lee, Jennifer G. Kim", "title": "Personalized Academic Research Paper Recommendation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A huge number of academic papers are coming out from a lot of conferences and\njournals these days. In these circumstances, most researchers rely on key-based\nsearch or browsing through proceedings of top conferences and journals to find\ntheir related work. To ease this difficulty, we propose a Personalized Academic\nResearch Paper Recommendation System, which recommends related articles, for\neach researcher, that may be interesting to her/him. In this paper, we first\nintroduce our web crawler to retrieve research papers from the web. Then, we\ndefine similarity between two research papers based on the text similarity\nbetween them. Finally, we propose our recommender system developed using\ncollaborative filtering methods. Our evaluation results demonstrate that our\nsystem recommends good quality research papers.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 15:53:53 GMT"}], "update_date": "2013-04-22", "authors_parsed": [["Lee", "Joonseok", ""], ["Lee", "Kisung", ""], ["Kim", "Jennifer G.", ""]]}, {"id": "1304.5504", "submitter": "Tianbao Yang", "authors": "Jianhui Chen, Tianbao Yang, Qihang Lin, Lijun Zhang, Yi Chang", "title": "Optimal Stochastic Strongly Convex Optimization with a Logarithmic\n  Number of Projections", "comments": "Accepted by the 32th Conference on Uncertainty in Artificial\n  Intelligence (UAI 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic strongly convex optimization with a complex inequality\nconstraint. This complex inequality constraint may lead to computationally\nexpensive projections in algorithmic iterations of the stochastic gradient\ndescent~(SGD) methods. To reduce the computation costs pertaining to the\nprojections, we propose an Epoch-Projection Stochastic Gradient\nDescent~(Epro-SGD) method. The proposed Epro-SGD method consists of a sequence\nof epochs; it applies SGD to an augmented objective function at each iteration\nwithin the epoch, and then performs a projection at the end of each epoch.\nGiven a strongly convex optimization and for a total number of $T$ iterations,\nEpro-SGD requires only $\\log(T)$ projections, and meanwhile attains an optimal\nconvergence rate of $O(1/T)$, both in expectation and with a high probability.\nTo exploit the structure of the optimization problem, we propose a proximal\nvariant of Epro-SGD, namely Epro-ORDA, based on the optimal regularized dual\naveraging method. We apply the proposed methods on real-world applications; the\nempirical results demonstrate the effectiveness of our methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 18:51:07 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2013 18:36:52 GMT"}, {"version": "v3", "created": "Fri, 3 May 2013 16:24:20 GMT"}, {"version": "v4", "created": "Tue, 7 May 2013 18:26:55 GMT"}, {"version": "v5", "created": "Fri, 10 May 2013 05:19:10 GMT"}, {"version": "v6", "created": "Tue, 24 May 2016 05:07:06 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Chen", "Jianhui", ""], ["Yang", "Tianbao", ""], ["Lin", "Qihang", ""], ["Zhang", "Lijun", ""], ["Chang", "Yi", ""]]}, {"id": "1304.5575", "submitter": "Qichao Que", "authors": "Qichao Que and Mikhail Belkin", "title": "Inverse Density as an Inverse Problem: The Fredholm Equation Approach", "comments": "Fixing a few typos in last version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of estimating the ratio $\\frac{q}{p}$\nwhere $p$ is a density function and $q$ is another density, or, more generally\nan arbitrary function. Knowing or approximating this ratio is needed in various\nproblems of inference and integration, in particular, when one needs to average\na function with respect to one probability distribution, given a sample from\nanother. It is often referred as {\\it importance sampling} in statistical\ninference and is also closely related to the problem of {\\it covariate shift}\nin transfer learning as well as to various MCMC methods. It may also be useful\nfor separating the underlying geometry of a space, say a manifold, from the\ndensity function defined on it.\n  Our approach is based on reformulating the problem of estimating\n$\\frac{q}{p}$ as an inverse problem in terms of an integral operator\ncorresponding to a kernel, and thus reducing it to an integral equation, known\nas the Fredholm problem of the first kind. This formulation, combined with the\ntechniques of regularization and kernel methods, leads to a principled\nkernel-based framework for constructing algorithms and for analyzing them\ntheoretically.\n  The resulting family of algorithms (FIRE, for Fredholm Inverse Regularized\nEstimator) is flexible, simple and easy to implement.\n  We provide detailed theoretical analysis including concentration bounds and\nconvergence rates for the Gaussian kernel in the case of densities defined on\n$\\R^d$, compact domains in $\\R^d$ and smooth $d$-dimensional sub-manifolds of\nthe Euclidean space.\n  We also show experimental results including applications to classification\nand semi-supervised learning within the covariate shift framework and\ndemonstrate some encouraging experimental comparisons. We also show how the\nparameters of our algorithms can be chosen in a completely unsupervised manner.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2013 00:57:35 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2013 11:46:51 GMT"}], "update_date": "2013-04-26", "authors_parsed": [["Que", "Qichao", ""], ["Belkin", "Mikhail", ""]]}, {"id": "1304.5583", "submitter": "Ameet Talwalkar", "authors": "Ameet Talwalkar, Lester Mackey, Yadong Mu, Shih-Fu Chang, Michael I.\n  Jordan", "title": "Distributed Low-rank Subspace Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision problems ranging from image clustering to motion segmentation to\nsemi-supervised learning can naturally be framed as subspace segmentation\nproblems, in which one aims to recover multiple low-dimensional subspaces from\nnoisy and corrupted input data. Low-Rank Representation (LRR), a convex\nformulation of the subspace segmentation problem, is provably and empirically\naccurate on small problems but does not scale to the massive sizes of modern\nvision datasets. Moreover, past work aimed at scaling up low-rank matrix\nfactorization is not applicable to LRR given its non-decomposable constraints.\nIn this work, we propose a novel divide-and-conquer algorithm for large-scale\nsubspace segmentation that can cope with LRR's non-decomposable constraints and\nmaintains LRR's strong recovery guarantees. This has immediate implications for\nthe scalability of subspace segmentation, which we demonstrate on a benchmark\nface recognition dataset and in simulations. We then introduce novel\napplications of LRR-based subspace segmentation to large-scale semi-supervised\nlearning for multimedia event detection, concept detection, and image tagging.\nIn each case, we obtain state-of-the-art results and order-of-magnitude speed\nups.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2013 03:54:48 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2013 02:55:18 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Talwalkar", "Ameet", ""], ["Mackey", "Lester", ""], ["Mu", "Yadong", ""], ["Chang", "Shih-Fu", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1304.5634", "submitter": "Dacheng Tao", "authors": "Chang Xu, Dacheng Tao, Chao Xu", "title": "A Survey on Multi-view Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In recent years, a great many methods of learning from multi-view data by\nconsidering the diversity of different views have been proposed. These views\nmay be obtained from multiple sources or different feature subsets. In trying\nto organize and highlight similarities and differences between the variety of\nmulti-view learning approaches, we review a number of representative multi-view\nlearning algorithms in different areas and classify them into three groups: 1)\nco-training, 2) multiple kernel learning, and 3) subspace learning. Notably,\nco-training style algorithms train alternately to maximize the mutual agreement\non two distinct views of the data; multiple kernel learning algorithms exploit\nkernels that naturally correspond to different views and combine kernels either\nlinearly or non-linearly to improve learning performance; and subspace learning\nalgorithms aim to obtain a latent subspace shared by multiple views by assuming\nthat the input views are generated from this latent subspace. Though there is\nsignificant variance in the approaches to integrating multiple views to improve\nlearning performance, they mainly exploit either the consensus principle or the\ncomplementary principle to ensure the success of multi-view learning. Since\naccessing multiple views is the fundament of multi-view learning, with the\nexception of study on learning a model from multiple views, it is also valuable\nto study how to construct multiple views and how to evaluate these views.\nOverall, by exploring the consistency and complementary properties of different\nviews, multi-view learning is rendered more effective, more promising, and has\nbetter generalization ability than single-view learning.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2013 14:43:35 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Xu", "Chang", ""], ["Tao", "Dacheng", ""], ["Xu", "Chao", ""]]}, {"id": "1304.5678", "submitter": "Felix Breuer", "authors": "Carly Stambaugh, Hui Yang, Felix Breuer", "title": "Analytic Feature Selection for Support Vector Machines", "comments": "To be presented at 9th International Conference on Machine Learning\n  and Data Mining MLDM 2013. 15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machines (SVMs) rely on the inherent geometry of a data set to\nclassify training data. Because of this, we believe SVMs are an excellent\ncandidate to guide the development of an analytic feature selection algorithm,\nas opposed to the more commonly used heuristic methods. We propose a\nfilter-based feature selection algorithm based on the inherent geometry of a\nfeature set. Through observation, we identified six geometric properties that\ndiffer between optimal and suboptimal feature sets, and have statistically\nsignificant correlations to classifier performance. Our algorithm is based on\nlogistic and linear regression models using these six geometric properties as\npredictor variables. The proposed algorithm achieves excellent results on high\ndimensional text data sets, with features that can be organized into a handful\nof feature types; for example, unigrams, bigrams or semantic structural\nfeatures. We believe this algorithm is a novel and effective approach to\nsolving the feature selection problem for linear SVMs.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2013 23:38:45 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Stambaugh", "Carly", ""], ["Yang", "Hui", ""], ["Breuer", "Felix", ""]]}, {"id": "1304.5758", "submitter": "Sebastien Bubeck", "authors": "S\\'ebastien Bubeck and Che-Yu Liu", "title": "Prior-free and prior-dependent regret bounds for Thompson Sampling", "comments": "A previous version appeared under the title 'A note on the Bayesian\n  regret of Thompson Sampling with an arbitrary prior'", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the stochastic multi-armed bandit problem with a prior\ndistribution on the reward distributions. We are interested in studying\nprior-free and prior-dependent regret bounds, very much in the same spirit as\nthe usual distribution-free and distribution-dependent bounds for the\nnon-Bayesian stochastic bandit. Building on the techniques of Audibert and\nBubeck [2009] and Russo and Roy [2013] we first show that Thompson Sampling\nattains an optimal prior-free bound in the sense that for any prior\ndistribution its Bayesian regret is bounded from above by $14 \\sqrt{n K}$. This\nresult is unimprovable in the sense that there exists a prior distribution such\nthat any algorithm has a Bayesian regret bounded from below by $\\frac{1}{20}\n\\sqrt{n K}$. We also study the case of priors for the setting of Bubeck et al.\n[2013] (where the optimal mean is known as well as a lower bound on the\nsmallest gap) and we show that in this case the regret of Thompson Sampling is\nin fact uniformly bounded over time, thus showing that Thompson Sampling can\ngreatly take advantage of the nice properties of these priors.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2013 15:58:56 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2013 00:48:53 GMT"}], "update_date": "2013-10-04", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Liu", "Che-Yu", ""]]}, {"id": "1304.5793", "submitter": "Hemant Tyagi", "authors": "Hemant Tyagi and Bernd G\\\"artner", "title": "Continuum armed bandit problem of few variables in high dimensions", "comments": "(1) Appeared in proceedings of 11th Workshop on Approximation and\n  Online Algorithms (WAOA 2013). (2) Corrected minor typos in previous version\n  (3) 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the stochastic and adversarial settings of continuum armed\nbandits where the arms are indexed by [0,1]^d. The reward functions r:[0,1]^d\n-> R are assumed to intrinsically depend on at most k coordinate variables\nimplying r(x_1,..,x_d) = g(x_{i_1},..,x_{i_k}) for distinct and unknown\ni_1,..,i_k from {1,..,d} and some locally Holder continuous g:[0,1]^k -> R with\nexponent 0 < alpha <= 1. Firstly, assuming (i_1,..,i_k) to be fixed across\ntime, we propose a simple modification of the CAB1 algorithm where we construct\nthe discrete set of sampling points to obtain a bound of\nO(n^((alpha+k)/(2*alpha+k)) (log n)^((alpha)/(2*alpha+k)) C(k,d)) on the\nregret, with C(k,d) depending at most polynomially in k and sub-logarithmically\nin d. The construction is based on creating partitions of {1,..,d} into k\ndisjoint subsets and is probabilistic, hence our result holds with high\nprobability. Secondly we extend our results to also handle the more general\ncase where (i_1,...,i_k) can change over time and derive regret bounds for the\nsame.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2013 20:03:23 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2013 14:00:54 GMT"}, {"version": "v3", "created": "Mon, 23 Sep 2013 17:30:26 GMT"}, {"version": "v4", "created": "Fri, 22 Aug 2014 14:59:13 GMT"}], "update_date": "2014-08-25", "authors_parsed": [["Tyagi", "Hemant", ""], ["G\u00e4rtner", "Bernd", ""]]}, {"id": "1304.5862", "submitter": "Forrest Briggs", "authors": "Forrest Briggs, Xiaoli Z. Fern, Jed Irvine", "title": "Multi-Label Classifier Chains for Bird Sound", "comments": "6 pages, 1 figure, submission to ICML 2013 workshop on bioacoustics.\n  Note: this is a minor revision- the blind submission format has been replaced\n  with one that shows author names, and a few corrections have been made", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bird sound data collected with unattended microphones for automatic surveys,\nor mobile devices for citizen science, typically contain multiple\nsimultaneously vocalizing birds of different species. However, few works have\nconsidered the multi-label structure in birdsong. We propose to use an ensemble\nof classifier chains combined with a histogram-of-segments representation for\nmulti-label classification of birdsong. The proposed method is compared with\nbinary relevance and three multi-instance multi-label learning (MIML)\nalgorithms from prior work (which focus more on structure in the sound, and\nless on structure in the label sets). Experiments are conducted on two\nreal-world birdsong datasets, and show that the proposed method usually\noutperforms binary relevance (using the same features and base-classifier), and\nis better in some cases and worse in others compared to the MIML algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2013 07:44:05 GMT"}, {"version": "v2", "created": "Wed, 29 May 2013 17:36:07 GMT"}], "update_date": "2013-05-30", "authors_parsed": [["Briggs", "Forrest", ""], ["Fern", "Xiaoli Z.", ""], ["Irvine", "Jed", ""]]}, {"id": "1304.5894", "submitter": "Bruno Cornelis", "authors": "Bruno Cornelis, Yun Yang, Joshua T. Vogelstein, Ann Dooms, Ingrid\n  Daubechies, David Dunson", "title": "Bayesian crack detection in ultra high resolution multimodal images of\n  paintings", "comments": "8 pages, double column", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The preservation of our cultural heritage is of paramount importance. Thanks\nto recent developments in digital acquisition techniques, powerful image\nanalysis algorithms are developed which can be useful non-invasive tools to\nassist in the restoration and preservation of art. In this paper we propose a\nsemi-supervised crack detection method that can be used for high-dimensional\nacquisitions of paintings coming from different modalities. Our dataset\nconsists of a recently acquired collection of images of the Ghent Altarpiece\n(1432), one of Northern Europe's most important art masterpieces. Our goal is\nto build a classifier that is able to discern crack pixels from the background\nconsisting of non-crack pixels, making optimal use of the information that is\nprovided by each modality. To accomplish this we employ a recently developed\nnon-parametric Bayesian classifier, that uses tensor factorizations to\ncharacterize any conditional probability. A prior is placed on the parameters\nof the factorization such that every possible interaction between predictors is\nallowed while still identifying a sparse subset among these predictors. The\nproposed Bayesian classifier, which we will refer to as conditional Bayesian\ntensor factorization or CBTF, is assessed by visually comparing classification\nresults with the Random Forest (RF) algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2013 09:46:47 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2013 09:00:01 GMT"}], "update_date": "2013-04-24", "authors_parsed": [["Cornelis", "Bruno", ""], ["Yang", "Yun", ""], ["Vogelstein", "Joshua T.", ""], ["Dooms", "Ann", ""], ["Daubechies", "Ingrid", ""], ["Dunson", "David", ""]]}, {"id": "1304.5974", "submitter": "Kevin Xu", "authors": "Kevin S. Xu and Alfred O. Hero III", "title": "Dynamic stochastic blockmodels: Statistical models for time-evolving\n  networks", "comments": null, "journal-ref": "Proceedings of the 6th International Conference on Social\n  Computing, Behavioral-Cultural Modeling, and Prediction (2013) 201-210", "doi": "10.1007/978-3-642-37210-0_22", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant efforts have gone into the development of statistical models for\nanalyzing data in the form of networks, such as social networks. Most existing\nwork has focused on modeling static networks, which represent either a single\ntime snapshot or an aggregate view over time. There has been recent interest in\nstatistical modeling of dynamic networks, which are observed at multiple points\nin time and offer a richer representation of many complex phenomena. In this\npaper, we propose a state-space model for dynamic networks that extends the\nwell-known stochastic blockmodel for static networks to the dynamic setting. We\nthen propose a procedure to fit the model using a modification of the extended\nKalman filter augmented with a local search. We apply the procedure to analyze\na dynamic social network of email communication.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2013 15:07:19 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Xu", "Kevin S.", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1304.6383", "submitter": "Constantinos Panagiotakopoulos", "authors": "Constantinos Panagiotakopoulos and Petroula Tsampouka", "title": "The Stochastic Gradient Descent for the Primal L1-SVM Optimization\n  Revisited", "comments": "In v2 the numerical results are obtained using the latest release 1.7\n  of Cygwin and the g++ compiler version 4.5.3. We also consider in the\n  experiments the algorithms SvmSgd and SGD-QN. A slightly shorter version of\n  this paper appeared in ECML/PKDD 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reconsider the stochastic (sub)gradient approach to the unconstrained\nprimal L1-SVM optimization. We observe that if the learning rate is inversely\nproportional to the number of steps, i.e., the number of times any training\npattern is presented to the algorithm, the update rule may be transformed into\nthe one of the classical perceptron with margin in which the margin threshold\nincreases linearly with the number of steps. Moreover, if we cycle repeatedly\nthrough the possibly randomly permuted training set the dual variables defined\nnaturally via the expansion of the weight vector as a linear combination of the\npatterns on which margin errors were made are shown to obey at the end of each\ncomplete cycle automatically the box constraints arising in dual optimization.\nThis renders the dual Lagrangian a running lower bound on the primal objective\ntending to it at the optimum and makes available an upper bound on the relative\naccuracy achieved which provides a meaningful stopping criterion. In addition,\nwe propose a mechanism of presenting the same pattern repeatedly to the\nalgorithm which maintains the above properties. Finally, we give experimental\nevidence that algorithms constructed along these lines exhibit a considerably\nimproved performance.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2013 19:24:02 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2014 10:46:53 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Panagiotakopoulos", "Constantinos", ""], ["Tsampouka", "Petroula", ""]]}, {"id": "1304.6478", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an and Weiran Wang", "title": "The K-modes algorithm for clustering", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many clustering algorithms exist that estimate a cluster centroid, such as\nK-means, K-medoids or mean-shift, but no algorithm seems to exist that clusters\ndata by returning exactly K meaningful modes. We propose a natural definition\nof a K-modes objective function by combining the notions of density and cluster\nassignment. The algorithm becomes K-means and K-medoids in the limit of very\nlarge and very small scales. Computationally, it is slightly slower than\nK-means but much faster than mean-shift or K-medoids. Unlike K-means, it is\nable to find centroids that are valid patterns, truly representative of a\ncluster, even with nonconvex clusters, and appears robust to outliers and\nmisspecification of the scale and number of clusters.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2013 03:59:39 GMT"}], "update_date": "2013-04-25", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""], ["Wang", "Weiran", ""]]}, {"id": "1304.6480", "submitter": "Liwei Wang", "authors": "Yining Wang, Liwei Wang, Yuanzhi Li, Di He, Tie-Yan Liu, Wei Chen", "title": "A Theoretical Analysis of NDCG Type Ranking Measures", "comments": "COLT 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem in ranking is to design a ranking measure for evaluation of\nranking functions. In this paper we study, from a theoretical perspective, the\nwidely used Normalized Discounted Cumulative Gain (NDCG)-type ranking measures.\nAlthough there are extensive empirical studies of NDCG, little is known about\nits theoretical properties. We first show that, whatever the ranking function\nis, the standard NDCG which adopts a logarithmic discount, converges to 1 as\nthe number of items to rank goes to infinity. On the first sight, this result\nis very surprising. It seems to imply that NDCG cannot differentiate good and\nbad ranking functions, contradicting to the empirical success of NDCG in many\napplications. In order to have a deeper understanding of ranking measures in\ngeneral, we propose a notion referred to as consistent distinguishability. This\nnotion captures the intuition that a ranking measure should have such a\nproperty: For every pair of substantially different ranking functions, the\nranking measure can decide which one is better in a consistent manner on almost\nall datasets. We show that NDCG with logarithmic discount has consistent\ndistinguishability although it converges to the same limit for all ranking\nfunctions. We next characterize the set of all feasible discount functions for\nNDCG according to the concept of consistent distinguishability. Specifically we\nshow that whether NDCG has consistent distinguishability depends on how fast\nthe discount decays, and 1/r is a critical point. We then turn to the cut-off\nversion of NDCG, i.e., NDCG@k. We analyze the distinguishability of NDCG@k for\nvarious choices of k and the discount functions. Experimental results on real\nWeb search datasets agree well with the theory.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2013 04:08:23 GMT"}], "update_date": "2013-04-25", "authors_parsed": [["Wang", "Yining", ""], ["Wang", "Liwei", ""], ["Li", "Yuanzhi", ""], ["He", "Di", ""], ["Liu", "Tie-Yan", ""], ["Chen", "Wei", ""]]}, {"id": "1304.6487", "submitter": "Liangli Zhen", "authors": "Liangli Zhen, Zhang Yi, Xi Peng, Dezhong Peng", "title": "Locally linear representation for image clustering", "comments": null, "journal-ref": "Electronics Letters 50 (13), 942-943, 2014", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a key to construct a similarity graph in graph-oriented subspace\nlearning and clustering. In a similarity graph, each vertex denotes a data\npoint and the edge weight represents the similarity between two points. There\nare two popular schemes to construct a similarity graph, i.e., pairwise\ndistance based scheme and linear representation based scheme. Most existing\nworks have only involved one of the above schemes and suffered from some\nlimitations. Specifically, pairwise distance based methods are sensitive to the\nnoises and outliers compared with linear representation based methods. On the\nother hand, there is the possibility that linear representation based\nalgorithms wrongly select inter-subspaces points to represent a point, which\nwill degrade the performance. In this paper, we propose an algorithm, called\nLocally Linear Representation (LLR), which integrates pairwise distance with\nlinear representation together to address the problems. The proposed algorithm\ncan automatically encode each data point over a set of points that not only\ncould denote the objective point with less residual error, but also are close\nto the point in Euclidean space. The experimental results show that our\napproach is promising in subspace learning and subspace clustering.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2013 06:37:07 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 13:28:28 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Zhen", "Liangli", ""], ["Yi", "Zhang", ""], ["Peng", "Xi", ""], ["Peng", "Dezhong", ""]]}, {"id": "1304.6663", "submitter": "Bamdev Mishra", "authors": "B. Mishra, G. Meyer and R. Sepulchre", "title": "Low-rank optimization for distance matrix completion", "comments": "In Proceedings of the 50th IEEE Conference on Decision and Control\n  and European Control Conference, 2011", "journal-ref": null, "doi": "10.1109/CDC.2011.6160810", "report-no": null, "categories": "math.OC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of low-rank distance matrix completion. This\nproblem amounts to recover the missing entries of a distance matrix when the\ndimension of the data embedding space is possibly unknown but small compared to\nthe number of considered data points. The focus is on high-dimensional\nproblems. We recast the considered problem into an optimization problem over\nthe set of low-rank positive semidefinite matrices and propose two efficient\nalgorithms for low-rank distance matrix completion. In addition, we propose a\nstrategy to determine the dimension of the embedding space. The resulting\nalgorithms scale to high-dimensional problems and monotonically converge to a\nglobal solution of the problem. Finally, numerical experiments illustrate the\ngood performance of the proposed algorithms on benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2013 16:52:34 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2013 09:26:19 GMT"}], "update_date": "2013-04-26", "authors_parsed": [["Mishra", "B.", ""], ["Meyer", "G.", ""], ["Sepulchre", "R.", ""]]}, {"id": "1304.6810", "submitter": "Guy Van den Broeck", "authors": "Daan Fierens, Guy Van den Broeck, Joris Renkens, Dimitar Shterionov,\n  Bernd Gutmann, Ingo Thon, Gerda Janssens, Luc De Raedt", "title": "Inference and learning in probabilistic logic programs using weighted\n  Boolean formulas", "comments": "To appear in Theory and Practice of Logic Programming (TPLP)", "journal-ref": "Theory and Practice of Logic Programming 15 (2015) 358-401", "doi": "10.1017/S1471068414000076", "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic logic programs are logic programs in which some of the facts\nare annotated with probabilities. This paper investigates how classical\ninference and learning tasks known from the graphical model community can be\ntackled for probabilistic logic programs. Several such tasks such as computing\nthe marginals given evidence and learning from (partial) interpretations have\nnot really been addressed for probabilistic logic programs before.\n  The first contribution of this paper is a suite of efficient algorithms for\nvarious inference tasks. It is based on a conversion of the program and the\nqueries and evidence to a weighted Boolean formula. This allows us to reduce\nthe inference tasks to well-studied tasks such as weighted model counting,\nwhich can be solved using state-of-the-art methods known from the graphical\nmodel and knowledge compilation literature. The second contribution is an\nalgorithm for parameter estimation in the learning from interpretations\nsetting. The algorithm employs Expectation Maximization, and is built on top of\nthe developed inference algorithms.\n  The proposed approach is experimentally evaluated. The results show that the\ninference algorithms improve upon the state-of-the-art in probabilistic logic\nprogramming and that it is indeed possible to learn the parameters of a\nprobabilistic logic program from interpretations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2013 06:10:55 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Fierens", "Daan", ""], ["Broeck", "Guy Van den", ""], ["Renkens", "Joris", ""], ["Shterionov", "Dimitar", ""], ["Gutmann", "Bernd", ""], ["Thon", "Ingo", ""], ["Janssens", "Gerda", ""], ["De Raedt", "Luc", ""]]}, {"id": "1304.6899", "submitter": "Bal\\'azs Szalkai", "authors": "Bal\\'azs Szalkai", "title": "An implementation of the relational k-means algorithm", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A C# implementation of a generalized k-means variant called relational\nk-means is described here. Relational k-means is a generalization of the\nwell-known k-means clustering method which works for non-Euclidean scenarios as\nwell. The input is an arbitrary distance matrix, as opposed to the traditional\nk-means method, where the clustered objects need to be identified with vectors.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2013 12:59:31 GMT"}], "update_date": "2013-04-26", "authors_parsed": [["Szalkai", "Bal\u00e1zs", ""]]}, {"id": "1304.7045", "submitter": "Ohad Shamir", "authors": "Roi Livni, Shai Shalev-Shwartz, Ohad Shamir", "title": "An Algorithm for Training Polynomial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider deep neural networks, in which the output of each node is a\nquadratic function of its inputs. Similar to other deep architectures, these\nnetworks can compactly represent any function on a finite training set. The\nmain goal of this paper is the derivation of an efficient layer-by-layer\nalgorithm for training such networks, which we denote as the \\emph{Basis\nLearner}. The algorithm is a universal learner in the sense that the training\nerror is guaranteed to decrease at every iteration, and can eventually reach\nzero under mild conditions. We present practical implementations of this\nalgorithm, as well as preliminary experimental results. We also compare our\ndeep architecture to other shallow architectures for learning polynomials, in\nparticular kernel learning.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 00:35:37 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2014 13:14:59 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Livni", "Roi", ""], ["Shalev-Shwartz", "Shai", ""], ["Shamir", "Ohad", ""]]}, {"id": "1304.7158", "submitter": "Antoine Bordes", "authors": "Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston\n  and Oksana Yakhnenko", "title": "Irreflexive and Hierarchical Relations as Translations", "comments": "Submitted at the ICML 2013 workshop \"Structured Learning: Inferring\n  Graphs from Structured and Unstructured Inputs\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of embedding entities and relations of knowledge\nbases in low-dimensional vector spaces. Unlike most existing approaches, which\nare primarily efficient for modeling equivalence relations, our approach is\ndesigned to explicitly model irreflexive relations, such as hierarchies, by\ninterpreting them as translations operating on the low-dimensional embeddings\nof the entities. Preliminary experiments show that, despite its simplicity and\na smaller number of parameters than previous approaches, our approach achieves\nstate-of-the-art performance according to standard evaluation protocols on data\nfrom WordNet and Freebase.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 13:28:47 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Bordes", "Antoine", ""], ["Usunier", "Nicolas", ""], ["Garcia-Duran", "Alberto", ""], ["Weston", "Jason", ""], ["Yakhnenko", "Oksana", ""]]}, {"id": "1304.7230", "submitter": "David Kessler", "authors": "David C. Kessler and Jack Taylor and David B. Dunson", "title": "Learning Densities Conditional on Many Interacting Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a distribution conditional on a set of discrete-valued features is a\ncommonly encountered task. This becomes more challenging with a\nhigh-dimensional feature set when there is the possibility of interaction\nbetween the features. In addition, many frequently applied techniques consider\nonly prediction of the mean, but the complete conditional density is needed to\nanswer more complex questions. We demonstrate a novel nonparametric Bayes\nmethod based upon a tensor factorization of feature-dependent weights for\nGaussian kernels. The method makes use of multistage feature selection for\ndimension reduction. The resulting conditional density morphs flexibly with the\nselected features.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 16:56:30 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2013 21:16:46 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Kessler", "David C.", ""], ["Taylor", "Jack", ""], ["Dunson", "David B.", ""]]}, {"id": "1304.7284", "submitter": "Zenglin Xu", "authors": "Shandian Zhe, Zenglin Xu, and Yuan Qi", "title": "Supervised Heterogeneous Multiview Learning for Joint Association Study\n  and Disease Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given genetic variations and various phenotypical traits, such as Magnetic\nResonance Imaging (MRI) features, we consider two important and related tasks\nin biomedical research: i)to select genetic and phenotypical markers for\ndisease diagnosis and ii) to identify associations between genetic and\nphenotypical data. These two tasks are tightly coupled because underlying\nassociations between genetic variations and phenotypical features contain the\nbiological basis for a disease. While a variety of sparse models have been\napplied for disease diagnosis and canonical correlation analysis and its\nextensions have bee widely used in association studies (e.g., eQTL analysis),\nthese two tasks have been treated separately. To unify these two tasks, we\npresent a new sparse Bayesian approach for joint association study and disease\ndiagnosis. In this approach, common latent features are extracted from\ndifferent data sources based on sparse projection matrices and used to predict\nmultiple disease severity levels based on Gaussian process ordinal regression;\nin return, the disease status is used to guide the discovery of relationships\nbetween the data sources. The sparse projection matrices not only reveal\ninteractions between data sources but also select groups of biomarkers related\nto the disease. To learn the model from data, we develop an efficient\nvariational expectation maximization algorithm. Simulation results demonstrate\nthat our approach achieves higher accuracy in both predicting ordinal labels\nand discovering associations between data sources than alternative methods. We\napply our approach to an imaging genetics dataset for the study of Alzheimer's\nDisease (AD). Our method identifies biologically meaningful relationships\nbetween genetic variations, MRI features, and AD status, and achieves\nsignificantly higher accuracy for predicting ordinal AD stages than the\ncompeting methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 20:47:46 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2013 07:04:04 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Zhe", "Shandian", ""], ["Xu", "Zenglin", ""], ["Qi", "Yuan", ""]]}, {"id": "1304.7465", "submitter": "M. Emre Celebi", "authors": "M. Emre Celebi and Hassan A. Kingravi", "title": "Deterministic Initialization of the K-Means Algorithm Using Hierarchical\n  Clustering", "comments": "23 pages, 3 figures, 10 tables. arXiv admin note: substantial text\n  overlap with arXiv:1209.1960", "journal-ref": "International Journal of Pattern Recognition and Artificial\n  Intelligence 26 (2012) 1250018", "doi": "10.1142/S0218001412500188", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-means is undoubtedly the most widely used partitional clustering algorithm.\nUnfortunately, due to its gradient descent nature, this algorithm is highly\nsensitive to the initial placement of the cluster centers. Numerous\ninitialization methods have been proposed to address this problem. Many of\nthese methods, however, have superlinear complexity in the number of data\npoints, making them impractical for large data sets. On the other hand, linear\nmethods are often random and/or order-sensitive, which renders their results\nunrepeatable. Recently, Su and Dy proposed two highly successful hierarchical\ninitialization methods named Var-Part and PCA-Part that are not only linear,\nbut also deterministic (non-random) and order-invariant. In this paper, we\npropose a discriminant analysis based approach that addresses a common\ndeficiency of these two methods. Experiments on a large and diverse collection\nof data sets from the UCI Machine Learning Repository demonstrate that Var-Part\nand PCA-Part are highly competitive with one of the best random initialization\nmethods to date, i.e., k-means++, and that the proposed approach significantly\nimproves the performance of both hierarchical methods.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2013 13:31:44 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Celebi", "M. Emre", ""], ["Kingravi", "Hassan A.", ""]]}, {"id": "1304.7528", "submitter": "Toke Jansen Hansen", "authors": "Toke J. Hansen and Michael W. Mahoney", "title": "Semi-supervised Eigenvectors for Large-scale Locally-biased Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, one has side information, e.g., labels that are\nprovided in a semi-supervised manner, about a specific target region of a large\ndata set, and one wants to perform machine learning and data analysis tasks\n\"nearby\" that prespecified target region. For example, one might be interested\nin the clustering structure of a data graph near a prespecified \"seed set\" of\nnodes, or one might be interested in finding partitions in an image that are\nnear a prespecified \"ground truth\" set of pixels. Locally-biased problems of\nthis sort are particularly challenging for popular eigenvector-based machine\nlearning and data analysis tools. At root, the reason is that eigenvectors are\ninherently global quantities, thus limiting the applicability of\neigenvector-based methods in situations where one is interested in very local\nproperties of the data.\n  In this paper, we address this issue by providing a methodology to construct\nsemi-supervised eigenvectors of a graph Laplacian, and we illustrate how these\nlocally-biased eigenvectors can be used to perform locally-biased machine\nlearning. These semi-supervised eigenvectors capture\nsuccessively-orthogonalized directions of maximum variance, conditioned on\nbeing well-correlated with an input seed set of nodes that is assumed to be\nprovided in a semi-supervised manner. We show that these semi-supervised\neigenvectors can be computed quickly as the solution to a system of linear\nequations; and we also describe several variants of our basic method that have\nimproved scaling properties. We provide several empirical examples\ndemonstrating how these semi-supervised eigenvectors can be used to perform\nlocally-biased learning; and we discuss the relationship between our results\nand recent machine learning algorithms that use global eigenvectors of the\ngraph Laplacian.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2013 21:52:12 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Hansen", "Toke J.", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1304.7576", "submitter": "Preyas Popat", "authors": "Rina Panigrahy and Preyas Popat", "title": "Fractal structures in Adversarial Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fractals are self-similar recursive structures that have been used in\nmodeling several real world processes. In this work we study how \"fractal-like\"\nprocesses arise in a prediction game where an adversary is generating a\nsequence of bits and an algorithm is trying to predict them. We will see that\nunder a certain formalization of the predictive payoff for the algorithm it is\nmost optimal for the adversary to produce a fractal-like sequence to minimize\nthe algorithm's ability to predict. Indeed it has been suggested before that\nfinancial markets exhibit a fractal-like behavior. We prove that a fractal-like\ndistribution arises naturally out of an optimization from the adversary's\nperspective.\n  In addition, we give optimal trade-offs between predictability and expected\ndeviation (i.e. sum of bits) for our formalization of predictive payoff. This\nresult is motivated by the observation that several time series data exhibit\nhigher deviations than expected for a completely random walk.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2013 07:16:54 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Panigrahy", "Rina", ""], ["Popat", "Preyas", ""]]}, {"id": "1304.7577", "submitter": "Preyas Popat", "authors": "Rina Panigrahy and Preyas Popat", "title": "Optimal amortized regret in every interval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the classical problem of predicting the next bit in a sequence of\nbits. A standard performance measure is {\\em regret} (loss in payoff) with\nrespect to a set of experts. For example if we measure performance with respect\nto two constant experts one that always predicts 0's and another that always\npredicts 1's it is well known that one can get regret $O(\\sqrt T)$ with respect\nto the best expert by using, say, the weighted majority algorithm. But this\nalgorithm does not provide performance guarantee in any interval. There are\nother algorithms that ensure regret $O(\\sqrt {x \\log T})$ in any interval of\nlength $x$. In this paper we show a randomized algorithm that in an amortized\nsense gets a regret of $O(\\sqrt x)$ for any interval when the sequence is\npartitioned into intervals arbitrarily. We empirically estimated the constant\nin the $O()$ for $T$ upto 2000 and found it to be small -- around 2.1. We also\nexperimentally evaluate the efficacy of this algorithm in predicting high\nfrequency stock data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2013 07:17:31 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Panigrahy", "Rina", ""], ["Popat", "Preyas", ""]]}, {"id": "1304.7710", "submitter": "Yun Wei", "authors": "Yun Wei and Chuanyi Ji and Floyd Galvan and Stephen Couvillon and\n  George Orellana and James Momoh", "title": "Learning Geo-Temporal Non-Stationary Failure and Recovery of Power\n  Distribution", "comments": "12 pages, 12 figures, Accepted with minor revisions by TNNLS, Special\n  Issue on Learning in Nonstationary and Evolving Environments. arXiv admin\n  note: text overlap with arXiv:1202.4720", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart energy grid is an emerging area for new applications of machine\nlearning in a non-stationary environment. Such a non-stationary environment\nemerges when large-scale failures occur at power distribution networks due to\nexternal disturbances such as hurricanes and severe storms. Power distribution\nnetworks lie at the edge of the grid, and are especially vulnerable to external\ndisruptions. Quantifiable approaches are lacking and needed to learn\nnon-stationary behaviors of large-scale failure and recovery of power\ndistribution. This work studies such non-stationary behaviors in three aspects.\nFirst, a novel formulation is derived for an entire life cycle of large-scale\nfailure and recovery of power distribution. Second, spatial-temporal models of\nfailure and recovery of power distribution are developed as geo-location based\nmultivariate non-stationary GI(t)/G(t)/Infinity queues. Third, the\nnon-stationary spatial-temporal models identify a small number of parameters to\nbe learned. Learning is applied to two real-life examples of large-scale\ndisruptions. One is from Hurricane Ike, where data from an operational network\nis exact on failures and recoveries. The other is from Hurricane Sandy, where\naggregated data is used for inferring failure and recovery processes at one of\nthe impacted areas. Model parameters are learned using real data. Two findings\nemerge as results of learning: (a) Failure rates behave similarly at the two\ndifferent provider networks for two different hurricanes but differently at the\ngeographical regions. (b) Both rapid- and slow-recovery are present for\nHurricane Ike but only slow recovery is shown for a regional distribution\nnetwork from Hurricane Sandy.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2013 16:48:02 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Wei", "Yun", ""], ["Ji", "Chuanyi", ""], ["Galvan", "Floyd", ""], ["Couvillon", "Stephen", ""], ["Orellana", "George", ""], ["Momoh", "James", ""]]}, {"id": "1304.7851", "submitter": "Guangzhi Qu", "authors": "Rami Abousleiman, Guangzhi Qu, Osamah Rawashdeh", "title": "North Atlantic Right Whale Contact Call Detection", "comments": "6 pages, ICML 2013 Workshop on Machine Learning for Bioacoustics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The North Atlantic right whale (Eubalaena glacialis) is an endangered\nspecies. These whales continuously suffer from deadly vessel impacts alongside\nthe eastern coast of North America. There have been countless efforts to save\nthe remaining 350 - 400 of them. One of the most prominent works is done by\nMarinexplore and Cornell University. A system of hydrophones linked to\nsatellite connected-buoys has been deployed in the whales habitat. These\nhydrophones record and transmit live sounds to a base station. These recording\nmight contain the right whale contact call as well as many other noises. The\nnoise rate increases rapidly in vessel-busy areas such as by the Boston harbor.\nThis paper presents and studies the problem of detecting the North Atlantic\nright whale contact call with the presence of noise and other marine life\nsounds. A novel algorithm was developed to preprocess the sound waves before a\ntree based hierarchical classifier is used to classify the data and provide a\nscore. The developed model was trained with 30,000 data points made available\nthrough the Cornell University Whale Detection Challenge program. Results\nshowed that the developed algorithm had close to 85% success rate in detecting\nthe presence of the North Atlantic right whale.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 03:41:14 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2013 02:01:07 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Abousleiman", "Rami", ""], ["Qu", "Guangzhi", ""], ["Rawashdeh", "Osamah", ""]]}, {"id": "1304.8020", "submitter": "Daniele Calandriello", "authors": "Daniele Calandriello, Gang Niu, Masashi Sugiyama", "title": "Semi-Supervised Information-Maximization Clustering", "comments": "Slightly change metadata. arXiv admin note: text overlap with\n  arXiv:1112.0611", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised clustering aims to introduce prior knowledge in the decision\nprocess of a clustering algorithm. In this paper, we propose a novel\nsemi-supervised clustering algorithm based on the information-maximization\nprinciple. The proposed method is an extension of a previous unsupervised\ninformation-maximization clustering algorithm based on squared-loss mutual\ninformation to effectively incorporate must-links and cannot-links. The\nproposed method is computationally efficient because the clustering solution\ncan be obtained analytically via eigendecomposition. Furthermore, the proposed\nmethod allows systematic optimization of tuning parameters such as the kernel\nwidth, given the degree of belief in the must-links and cannot-links. The\nusefulness of the proposed method is demonstrated through experiments.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 14:59:49 GMT"}, {"version": "v2", "created": "Wed, 1 May 2013 11:53:05 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Calandriello", "Daniele", ""], ["Niu", "Gang", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1304.8087", "submitter": "Aravindan Vijayaraghavan", "authors": "Aditya Bhaskara, Moses Charikar, Aravindan Vijayaraghavan", "title": "Uniqueness of Tensor Decompositions with Applications to Polynomial\n  Identifiability", "comments": "51 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a robust version of the celebrated result of Kruskal on the\nuniqueness of tensor decompositions: we prove that given a tensor whose\ndecomposition satisfies a robust form of Kruskal's rank condition, it is\npossible to approximately recover the decomposition if the tensor is known up\nto a sufficiently small (inverse polynomial) error.\n  Kruskal's theorem has found many applications in proving the identifiability\nof parameters for various latent variable models and mixture models such as\nHidden Markov models, topic models etc. Our robust version immediately implies\nidentifiability using only polynomially many samples in many of these settings.\nThis polynomial identifiability is an essential first step towards efficient\nlearning algorithms for these models.\n  Recently, algorithms based on tensor decompositions have been used to\nestimate the parameters of various hidden variable models efficiently in\nspecial cases as long as they satisfy certain \"non-degeneracy\" properties. Our\nmethods give a way to go beyond this non-degeneracy barrier, and establish\npolynomial identifiability of the parameters under much milder conditions.\nGiven the importance of Kruskal's theorem in the tensor literature, we expect\nthat this robust version will have several applications beyond the settings we\nexplore in this work.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 17:35:37 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Bhaskara", "Aditya", ""], ["Charikar", "Moses", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1304.8132", "submitter": "Zeyuan Allen Zhu", "authors": "Zeyuan Allen Zhu and Silvio Lattanzi and Vahab Mirrokni", "title": "Local Graph Clustering Beyond Cheeger's Inequality", "comments": "An extended abstract of this paper has appeared in the proceedings of\n  the 30th International Conference on Machine Learning (ICML 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications of large-scale graph clustering, we study\nrandom-walk-based LOCAL algorithms whose running times depend only on the size\nof the output cluster, rather than the entire graph. All previously known such\nalgorithms guarantee an output conductance of $\\tilde{O}(\\sqrt{\\phi(A)})$ when\nthe target set $A$ has conductance $\\phi(A)\\in[0,1]$. In this paper, we improve\nit to $$\\tilde{O}\\bigg( \\min\\Big\\{\\sqrt{\\phi(A)},\n\\frac{\\phi(A)}{\\sqrt{\\mathsf{Conn}(A)}} \\Big\\} \\bigg)\\enspace, $$ where the\ninternal connectivity parameter $\\mathsf{Conn}(A) \\in [0,1]$ is defined as the\nreciprocal of the mixing time of the random walk over the induced subgraph on\n$A$.\n  For instance, using $\\mathsf{Conn}(A) = \\Omega(\\lambda(A) / \\log n)$ where\n$\\lambda$ is the second eigenvalue of the Laplacian of the induced subgraph on\n$A$, our conductance guarantee can be as good as\n$\\tilde{O}(\\phi(A)/\\sqrt{\\lambda(A)})$. This builds an interesting connection\nto the recent advance of the so-called improved Cheeger's Inequality [KKL+13],\nwhich says that global spectral algorithms can provide a conductance guarantee\nof $O(\\phi_{\\mathsf{opt}}/\\sqrt{\\lambda_3})$ instead of\n$O(\\sqrt{\\phi_{\\mathsf{opt}}})$.\n  In addition, we provide theoretical guarantee on the clustering accuracy (in\nterms of precision and recall) of the output set. We also prove that our\nanalysis is tight, and perform empirical evaluation to support our theory on\nboth synthetic and real data.\n  It is worth noting that, our analysis outperforms prior work when the cluster\nis well-connected. In fact, the better it is well-connected inside, the more\nsignificant improvement (both in terms of conductance and accuracy) we can\nobtain. Our results shed light on why in practice some random-walk-based\nalgorithms perform better than its previous theory, and help guide future\nresearch about local clustering.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 19:57:36 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2013 18:25:15 GMT"}], "update_date": "2013-11-08", "authors_parsed": [["Zhu", "Zeyuan Allen", ""], ["Lattanzi", "Silvio", ""], ["Mirrokni", "Vahab", ""]]}]