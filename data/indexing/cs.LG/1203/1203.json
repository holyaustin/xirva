[{"id": "1203.0038", "submitter": "Michael Alan Dewar", "authors": "Michael Dewar, Chris Wiggins, Frank Wood", "title": "Inference in Hidden Markov Models with Explicit State Duration\n  Distributions", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2012.2184795", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this letter we borrow from the inference techniques developed for\nunbounded state-cardinality (nonparametric) variants of the HMM and use them to\ndevelop a tuning-parameter free, black-box inference procedure for\nExplicit-state-duration hidden Markov models (EDHMM). EDHMMs are HMMs that have\nlatent states consisting of both discrete state-indicator and discrete\nstate-duration random variables. In contrast to the implicit geometric state\nduration distribution possessed by the standard HMM, EDHMMs allow the direct\nparameterisation and estimation of per-state duration distributions. As most\nduration distributions are defined over the positive integers, truncation or\nother approximations are usually required to perform EDHMM inference.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 22:40:56 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Dewar", "Michael", ""], ["Wiggins", "Chris", ""], ["Wood", "Frank", ""]]}, {"id": "1203.0058", "submitter": "Bo Zhao", "authors": "Bo Zhao, Benjamin I. P. Rubinstein, Jim Gemmell, Jiawei Han", "title": "A Bayesian Approach to Discovering Truth from Conflicting Sources for\n  Data Integration", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 6, pp.\n  550-561 (2012)", "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practical data integration systems, it is common for the data sources\nbeing integrated to provide conflicting information about the same entity.\nConsequently, a major challenge for data integration is to derive the most\ncomplete and accurate integrated records from diverse and sometimes conflicting\nsources. We term this challenge the truth finding problem. We observe that some\nsources are generally more reliable than others, and therefore a good model of\nsource quality is the key to solving the truth finding problem. In this work,\nwe propose a probabilistic graphical model that can automatically infer true\nrecords and source quality without any supervision. In contrast to previous\nmethods, our principled approach leverages a generative process of two types of\nerrors (false positive and false negative) by modeling two different aspects of\nsource quality. In so doing, ours is also the first approach designed to merge\nmulti-valued attribute types. Our method is scalable, due to an efficient\nsampling-based inference algorithm that needs very few iterations in practice\nand enjoys linear time complexity, with an even faster incremental variant.\nExperiments on two real world datasets show that our new method outperforms\nexisting state-of-the-art approaches to the truth finding problem.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 00:17:31 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["Zhao", "Bo", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Gemmell", "Jim", ""], ["Han", "Jiawei", ""]]}, {"id": "1203.0160", "submitter": "Yingyi Bu Yingyi Bu", "authors": "Yingyi Bu, Vinayak Borkar, Michael J. Carey, Joshua Rosen, Neoklis\n  Polyzotis, Tyson Condie, Markus Weimer, Raghu Ramakrishnan", "title": "Scaling Datalog for Machine Learning on Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the case for a declarative foundation for\ndata-intensive machine learning systems. Instead of creating a new system for\neach specific flavor of machine learning task, or hardcoding new optimizations,\nwe argue for the use of recursive queries to program a variety of machine\nlearning systems. By taking this approach, database query optimization\ntechniques can be utilized to identify effective execution plans, and the\nresulting runtime plans can be executed on a single unified data-parallel query\nprocessing engine. As a proof of concept, we consider two programming\nmodels--Pregel and Iterative Map-Reduce-Update---from the machine learning\ndomain, and show how they can be captured in Datalog, tuned for a specific\ntask, and then compiled into an optimized physical plan. Experiments performed\non a large computing cluster with real data demonstrate that this declarative\napproach can provide very good performance while offering both increased\ngenerality and programming ease.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 11:43:43 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2012 10:14:58 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["Bu", "Yingyi", ""], ["Borkar", "Vinayak", ""], ["Carey", "Michael J.", ""], ["Rosen", "Joshua", ""], ["Polyzotis", "Neoklis", ""], ["Condie", "Tyson", ""], ["Weimer", "Markus", ""], ["Ramakrishnan", "Raghu", ""]]}, {"id": "1203.0203", "submitter": "Gabriel Dulac-Arnold", "authors": "Gabriel Dulac-Arnold, Ludovic Denoyer, Philippe Preux, Patrick\n  Gallinari", "title": "Fast Reinforcement Learning with Large Action Sets using\n  Error-Correcting Output Codes for MDP Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Reinforcement Learning in real-world scenarios is strongly limited\nby issues of scale. Most RL learning algorithms are unable to deal with\nproblems composed of hundreds or sometimes even dozens of possible actions, and\ntherefore cannot be applied to many real-world problems. We consider the RL\nproblem in the supervised classification framework where the optimal policy is\nobtained through a multiclass classifier, the set of classes being the set of\nactions of the problem. We introduce error-correcting output codes (ECOCs) in\nthis setting and propose two new methods for reducing complexity when using\nrollouts-based approaches. The first method consists in using an ECOC-based\nclassifier as the multiclass classifier, reducing the learning complexity from\nO(A2) to O(Alog(A)). We then propose a novel method that profits from the\nECOC's coding dictionary to split the initial MDP into O(log(A)) seperate\ntwo-action MDPs. This second method reduces learning complexity even further,\nfrom O(A2) to O(log(A)), thus rendering problems with large action sets\ntractable. We finish by experimentally demonstrating the advantages of our\napproach on a set of benchmark problems, both in speed and performance.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 17:23:15 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Dulac-Arnold", "Gabriel", ""], ["Denoyer", "Ludovic", ""], ["Preux", "Philippe", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1203.0298", "submitter": "Olivia Saierli", "authors": "S. Aruna, S. P. Rajagopalan and L. V. Nandakishore", "title": "Application of Gist SVM in Cancer Detection", "comments": "10 pages", "journal-ref": "Ann. Univ. Tibiscus Comp. Sci. Series IX/2 (2011), 39-48", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the application of GIST SVM in disease prediction\n(detection of cancer). Pattern classification problems can be effectively\nsolved by Support vector machines. Here we propose a classifier which can\ndifferentiate patients having benign and malignant cancer cells. To improve the\naccuracy of classification, we propose to determine the optimal size of the\ntraining set and perform feature selection. To find the optimal size of the\ntraining set, different sizes of training sets are experimented and the one\nwith highest classification rate is selected. The optimal features are selected\nthrough their F-Scores.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 14:40:02 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2012 09:01:19 GMT"}], "update_date": "2012-03-07", "authors_parsed": [["Aruna", "S.", ""], ["Rajagopalan", "S. P.", ""], ["Nandakishore", "L. V.", ""]]}, {"id": "1203.0453", "submitter": "Song Liu Mr", "authors": "Song Liu, Makoto Yamada, Nigel Collier, Masashi Sugiyama", "title": "Change-Point Detection in Time-Series Data by Relative Density-Ratio\n  Estimation", "comments": null, "journal-ref": null, "doi": "10.1016/j.neunet.2013.01.012", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of change-point detection is to discover abrupt property\nchanges lying behind time-series data. In this paper, we present a novel\nstatistical change-point detection algorithm based on non-parametric divergence\nestimation between time-series samples from two retrospective segments. Our\nmethod uses the relative Pearson divergence as a divergence measure, and it is\naccurately and efficiently estimated by a method of direct density-ratio\nestimation. Through experiments on artificial and real-world datasets including\nhuman-activity sensing, speech, and Twitter messages, we demonstrate the\nusefulness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 13:12:03 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2013 06:44:58 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Liu", "Song", ""], ["Yamada", "Makoto", ""], ["Collier", "Nigel", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1203.0550", "submitter": "Afshin Rostamizadeh", "authors": "Corinna Cortes, Mehryar Mohri, Afshin Rostamizadeh", "title": "Algorithms for Learning Kernels Based on Centered Alignment", "comments": null, "journal-ref": "Journal of Machine Learning Research 13 (2012) 795-828", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents new and effective algorithms for learning kernels. In\nparticular, as shown by our empirical results, these algorithms consistently\noutperform the so-called uniform combination solution that has proven to be\ndifficult to improve upon in the past, as well as other algorithms for learning\nkernels based on convex combinations of base kernels in both classification and\nregression. Our algorithms are based on the notion of centered alignment which\nis used as a similarity measure between kernels or kernel matrices. We present\na number of novel algorithmic, theoretical, and empirical results for learning\nkernels based on our notion of centered alignment. In particular, we describe\nefficient algorithms for learning a maximum alignment kernel by showing that\nthe problem can be reduced to a simple QP and discuss a one-stage algorithm for\nlearning both a kernel and a hypothesis based on that kernel using an\nalignment-based regularization. Our theoretical results include a novel\nconcentration bound for centered alignment between kernel matrices, the proof\nof the existence of effective predictors for kernels with high alignment, both\nfor classification and for regression, and the proof of stability-based\ngeneralization bounds for a broad family of algorithms for learning kernels\nbased on centered alignment. We also report the results of experiments with our\ncentered alignment-based algorithms in both classification and regression.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 19:20:42 GMT"}, {"version": "v2", "created": "Tue, 8 Apr 2014 18:30:21 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Cortes", "Corinna", ""], ["Mohri", "Mehryar", ""], ["Rostamizadeh", "Afshin", ""]]}, {"id": "1203.0594", "submitter": "Vitaly Feldman", "authors": "Vitaly Feldman", "title": "Learning DNF Expressions from Fourier Spectrum", "comments": "Appears in Conference on Learning Theory (COLT) 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its introduction by Valiant in 1984, PAC learning of DNF expressions\nremains one of the central problems in learning theory. We consider this\nproblem in the setting where the underlying distribution is uniform, or more\ngenerally, a product distribution. Kalai, Samorodnitsky and Teng (2009) showed\nthat in this setting a DNF expression can be efficiently approximated from its\n\"heavy\" low-degree Fourier coefficients alone. This is in contrast to previous\napproaches where boosting was used and thus Fourier coefficients of the target\nfunction modified by various distributions were needed. This property is\ncrucial for learning of DNF expressions over smoothed product distributions, a\nlearning model introduced by Kalai et al. (2009) and inspired by the seminal\nsmoothed analysis model of Spielman and Teng (2001).\n  We introduce a new approach to learning (or approximating) a polynomial\nthreshold functions which is based on creating a function with range [-1,1]\nthat approximately agrees with the unknown function on low-degree Fourier\ncoefficients. We then describe conditions under which this is sufficient for\nlearning polynomial threshold functions. Our approach yields a new, simple\nalgorithm for approximating any polynomial-size DNF expression from its \"heavy\"\nlow-degree Fourier coefficients alone. Our algorithm greatly simplifies the\nproof of learnability of DNF expressions over smoothed product distributions.\nWe also describe an application of our algorithm to learning monotone DNF\nexpressions over product distributions. Building on the work of Servedio\n(2001), we give an algorithm that runs in time $\\poly((s \\cdot\n\\log{(s/\\eps)})^{\\log{(s/\\eps)}}, n)$, where $s$ is the size of the target DNF\nexpression and $\\eps$ is the accuracy. This improves on $\\poly((s \\cdot\n\\log{(ns/\\eps)})^{\\log{(s/\\eps)} \\cdot \\log{(1/\\eps)}}, n)$ bound of Servedio\n(2001).\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2012 00:43:08 GMT"}, {"version": "v2", "created": "Fri, 4 May 2012 03:47:56 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2013 05:14:46 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Feldman", "Vitaly", ""]]}, {"id": "1203.0631", "submitter": "Dmitry Chistikov", "authors": "Dmitry V. Chistikov", "title": "Checking Tests for Read-Once Functions over Arbitrary Bases", "comments": "Accepted to the 7th International Computer Science Symposium in\n  Russia (CSR 2012), revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Boolean function is called read-once over a basis B if it can be expressed\nby a formula over B where no variable appears more than once. A checking test\nfor a read-once function f over B depending on all its variables is a set of\ninput vectors distinguishing f from all other read-once functions of the same\nvariables. We show that every read-once function f over B has a checking test\ncontaining O(n^l) vectors, where n is the number of relevant variables of f and\nl is the largest arity of functions in B. For some functions, this bound cannot\nbe improved by more than a constant factor. The employed technique involves\nreconstructing f from its l-variable projections and provides a stronger form\nof Kuznetsov's classic theorem on read-once representations.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2012 09:02:40 GMT"}, {"version": "v2", "created": "Fri, 16 Mar 2012 18:36:01 GMT"}, {"version": "v3", "created": "Mon, 28 May 2012 18:16:35 GMT"}], "update_date": "2012-05-29", "authors_parsed": [["Chistikov", "Dmitry V.", ""]]}, {"id": "1203.0683", "submitter": "Daniel Hsu", "authors": "Animashree Anandkumar, Daniel Hsu, Sham M. Kakade", "title": "A Method of Moments for Mixture Models and Hidden Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models are a fundamental tool in applied statistics and machine\nlearning for treating data taken from multiple subpopulations. The current\npractice for estimating the parameters of such models relies on local search\nheuristics (e.g., the EM algorithm) which are prone to failure, and existing\nconsistent methods are unfavorable due to their high computational and sample\ncomplexity which typically scale exponentially with the number of mixture\ncomponents. This work develops an efficient method of moments approach to\nparameter estimation for a broad class of high-dimensional mixture models with\nmany components, including multi-view mixtures of Gaussians (such as mixtures\nof axis-aligned Gaussians) and hidden Markov models. The new method leads to\nrigorous unsupervised learning results for mixture models that were not\nachieved by previous works; and, because of its simplicity, it offers a viable\nalternative to EM for practical deployment.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2012 20:55:54 GMT"}, {"version": "v2", "created": "Fri, 25 May 2012 03:12:28 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2012 21:41:59 GMT"}], "update_date": "2012-09-07", "authors_parsed": [["Anandkumar", "Animashree", ""], ["Hsu", "Daniel", ""], ["Kakade", "Sham M.", ""]]}, {"id": "1203.0697", "submitter": "Animashree Anandkumar", "authors": "A. Anandkumar, D. Hsu, F. Huang and S. M. Kakade", "title": "Learning High-Dimensional Mixtures of Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider unsupervised estimation of mixtures of discrete graphical models,\nwhere the class variable corresponding to the mixture components is hidden and\neach mixture component over the observed variables can have a potentially\ndifferent Markov graph structure and parameters. We propose a novel approach\nfor estimating the mixture components, and our output is a tree-mixture model\nwhich serves as a good approximation to the underlying graphical model mixture.\nOur method is efficient when the union graph, which is the union of the Markov\ngraphs of the mixture components, has sparse vertex separators between any pair\nof observed variables. This includes tree mixtures and mixtures of bounded\ndegree graphs. For such models, we prove that our method correctly recovers the\nunion graph structure and the tree structures corresponding to\nmaximum-likelihood tree approximations of the mixture components. The sample\nand computational complexities of our method scale as $\\poly(p, r)$, for an\n$r$-component mixture of $p$-variate graphical models. We further extend our\nresults to the case when the union graph has sparse local separators between\nany pair of observed variables, such as mixtures of locally tree-like graphs,\nand the mixture components are in the regime of correlation decay.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2012 01:19:25 GMT"}, {"version": "v2", "created": "Sat, 30 Jun 2012 18:54:30 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Anandkumar", "A.", ""], ["Hsu", "D.", ""], ["Huang", "F.", ""], ["Kakade", "S. M.", ""]]}, {"id": "1203.0970", "submitter": "Yuyang Wang", "authors": "Yuyang Wang, Roni Khardon, Pavlos Protopapas", "title": "Infinite Shift-invariant Grouped Multi-task Learning for Gaussian\n  Processes", "comments": "This is an extended version of our ECML 2010 paper entitled\n  \"Shift-invariant Grouped Multi-task Learning for Gaussian Processes\"; ECML\n  PKDD'10 Proceedings of the 2010 European conference on Machine learning and\n  knowledge discovery in databases: Part III", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG astro-ph.IM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning leverages shared information among data sets to improve\nthe learning performance of individual tasks. The paper applies this framework\nfor data where each task is a phase-shifted periodic time series. In\nparticular, we develop a novel Bayesian nonparametric model capturing a mixture\nof Gaussian processes where each task is a sum of a group-specific function and\na component capturing individual variation, in addition to each task being\nphase shifted. We develop an efficient \\textsc{em} algorithm to learn the\nparameters of the model. As a special case we obtain the Gaussian mixture model\nand \\textsc{em} algorithm for phased-shifted periodic time series. Furthermore,\nwe extend the proposed model by using a Dirichlet Process prior and thereby\nleading to an infinite mixture model that is capable of doing automatic model\nselection. A Variational Bayesian approach is developed for inference in this\nmodel. Experiments in regression, classification and class discovery\ndemonstrate the performance of the proposed models using both synthetic data\nand real-world time series data from astrophysics. Our methods are particularly\nuseful when the time series are sparsely and non-synchronously sampled.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 17:07:10 GMT"}, {"version": "v2", "created": "Mon, 20 May 2013 04:07:12 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Wang", "Yuyang", ""], ["Khardon", "Roni", ""], ["Protopapas", "Pavlos", ""]]}, {"id": "1203.1005", "submitter": "Ehsan Elhamifar", "authors": "Ehsan Elhamifar and Rene Vidal", "title": "Sparse Subspace Clustering: Algorithm, Theory, and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.IR cs.IT cs.LG math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world problems, we are dealing with collections of\nhigh-dimensional data, such as images, videos, text and web documents, DNA\nmicroarray data, and more. Often, high-dimensional data lie close to\nlow-dimensional structures corresponding to several classes or categories the\ndata belongs to. In this paper, we propose and study an algorithm, called\nSparse Subspace Clustering (SSC), to cluster data points that lie in a union of\nlow-dimensional subspaces. The key idea is that, among infinitely many possible\nrepresentations of a data point in terms of other points, a sparse\nrepresentation corresponds to selecting a few points from the same subspace.\nThis motivates solving a sparse optimization program whose solution is used in\na spectral clustering framework to infer the clustering of data into subspaces.\nSince solving the sparse optimization program is in general NP-hard, we\nconsider a convex relaxation and show that, under appropriate conditions on the\narrangement of subspaces and the distribution of data, the proposed\nminimization program succeeds in recovering the desired sparse representations.\nThe proposed algorithm can be solved efficiently and can handle data points\nnear the intersections of subspaces. Another key advantage of the proposed\nalgorithm with respect to the state of the art is that it can deal with data\nnuisances, such as noise, sparse outlying entries, and missing entries,\ndirectly by incorporating the model of the data into the sparse optimization\nprogram. We demonstrate the effectiveness of the proposed algorithm through\nexperiments on synthetic data as well as the two real-world problems of motion\nsegmentation and face clustering.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 18:58:32 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2012 17:29:45 GMT"}, {"version": "v3", "created": "Tue, 5 Feb 2013 03:22:00 GMT"}], "update_date": "2013-02-06", "authors_parsed": [["Elhamifar", "Ehsan", ""], ["Vidal", "Rene", ""]]}, {"id": "1203.1007", "submitter": "Stephane Ross", "authors": "Stephane Ross, J. Andrew Bagnell", "title": "Agnostic System Identification for Model-Based Reinforcement Learning", "comments": "8 pages, published in ICML 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem in control is to learn a model of a system from\nobservations that is useful for controller synthesis. To provide good\nperformance guarantees, existing methods must assume that the real system is in\nthe class of models considered during learning. We present an iterative method\nwith strong guarantees even in the agnostic case where the system is not in the\nclass. In particular, we show that any no-regret online learning algorithm can\nbe used to obtain a near-optimal policy, provided some model achieves low\ntraining error and access to a good exploration distribution. Our approach\napplies to both discrete and continuous domains. We demonstrate its efficacy\nand scalability on a challenging helicopter domain from the literature.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 18:58:49 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2012 13:48:40 GMT"}], "update_date": "2012-07-04", "authors_parsed": [["Ross", "Stephane", ""], ["Bagnell", "J. Andrew", ""]]}, {"id": "1203.1483", "submitter": "Eduard Gabriel B\\u{a}z\\u{a}van", "authors": "Eduard Gabriel B\\u{a}z\\u{a}van, Fuxin Li and Cristian Sminchisescu", "title": "Learning Random Kernel Approximations for Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximations based on random Fourier features have recently emerged as an\nefficient and formally consistent methodology to design large-scale kernel\nmachines. By expressing the kernel as a Fourier expansion, features are\ngenerated based on a finite set of random basis projections, sampled from the\nFourier transform of the kernel, with inner products that are Monte Carlo\napproximations of the original kernel. Based on the observation that different\nkernel-induced Fourier sampling distributions correspond to different kernel\nparameters, we show that an optimization process in the Fourier domain can be\nused to identify the different frequency bands that are useful for prediction\non training data. Moreover, the application of group Lasso to random feature\nvectors corresponding to a linear combination of multiple kernels, leads to\nefficient and scalable reformulations of the standard multiple kernel learning\nmodel \\cite{Varma09}. In this paper we develop the linear Fourier approximation\nmethodology for both single and multiple gradient-based kernel learning and\nshow that it produces fast and accurate predictors on a complex dataset such as\nthe Visual Object Challenge 2011 (VOC2011).\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2012 14:33:26 GMT"}], "update_date": "2012-03-08", "authors_parsed": [["B\u0103z\u0103van", "Eduard Gabriel", ""], ["Li", "Fuxin", ""], ["Sminchisescu", "Cristian", ""]]}, {"id": "1203.1596", "submitter": "Hachem Kadri", "authors": "Hachem Kadri (INRIA Lille - Nord Europe), Alain Rakotomamonjy (LITIS),\n  Francis Bach (INRIA Paris - Rocquencourt, LIENS), Philippe Preux (INRIA Lille\n  - Nord Europe)", "title": "Multiple Operator-valued Kernel Learning", "comments": "No. RR-7900 (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positive definite operator-valued kernels generalize the well-known notion of\nreproducing kernels, and are naturally adapted to multi-output learning\nsituations. This paper addresses the problem of learning a finite linear\ncombination of infinite-dimensional operator-valued kernels which are suitable\nfor extending functional data analysis methods to nonlinear contexts. We study\nthis problem in the case of kernel ridge regression for functional responses\nwith an lr-norm constraint on the combination coefficients. The resulting\noptimization problem is more involved than those of multiple scalar-valued\nkernel learning since operator-valued kernels pose more technical and\ntheoretical issues. We propose a multiple operator-valued kernel learning\nalgorithm based on solving a system of linear operator equations by using a\nblock coordinatedescent procedure. We experimentally validate our approach on a\nfunctional regression task in the context of finger movement prediction in\nbrain-computer interfaces.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2012 20:31:17 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2012 18:44:49 GMT"}], "update_date": "2012-06-15", "authors_parsed": [["Kadri", "Hachem", "", "INRIA Lille - Nord Europe"], ["Rakotomamonjy", "Alain", "", "LITIS"], ["Bach", "Francis", "", "INRIA Paris - Rocquencourt, LIENS"], ["Preux", "Philippe", "", "INRIA Lille\n  - Nord Europe"]]}, {"id": "1203.2002", "submitter": "Soni madhulatha Tagaram", "authors": "T Soni Madhulatha", "title": "Graph partitioning advance clustering technique", "comments": "14 pages", "journal-ref": "International Journal of Computer Science and Engineering\n  Survey(IJCSES), February 2012, Volume 3, Number 1", "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Clustering is a common technique for statistical data analysis, Clustering is\nthe process of grouping the data into classes or clusters so that objects\nwithin a cluster have high similarity in comparison to one another, but are\nvery dissimilar to objects in other clusters. Dissimilarities are assessed\nbased on the attribute values describing the objects. Often, distance measures\nare used. Clustering is an unsupervised learning technique, where interesting\npatterns and structures can be found directly from very large data sets with\nlittle or none of the background knowledge. This paper also considers the\npartitioning of m-dimensional lattice graphs using Fiedler's approach, which\nrequires the determination of the eigenvector belonging to the second smallest\nEigenvalue of the Laplacian with K-means partitioning algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2012 07:08:10 GMT"}], "update_date": "2012-03-12", "authors_parsed": [["Madhulatha", "T Soni", ""]]}, {"id": "1203.2177", "submitter": "Masrour Zoghi", "authors": "Nando de Freitas, Alex Smola, Masrour Zoghi", "title": "Regret Bounds for Deterministic Gaussian Process Bandits", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper analyses the problem of Gaussian process (GP) bandits with\ndeterministic observations. The analysis uses a branch and bound algorithm that\nis related to the UCB algorithm of (Srinivas et al., 2010). For GPs with\nGaussian observation noise, with variance strictly greater than zero, (Srinivas\net al., 2010) proved that the regret vanishes at the approximate rate of\n$O(\\frac{1}{\\sqrt{t}})$, where t is the number of observations. To complement\ntheir result, we attack the deterministic case and attain a much faster\nexponential convergence rate. Under some regularity assumptions, we show that\nthe regret decreases asymptotically according to $O(e^{-\\frac{\\tau t}{(\\ln\nt)^{d/4}}})$ with high probability. Here, d is the dimension of the search\nspace and $\\tau$ is a constant that depends on the behaviour of the objective\nfunction near its global maximum.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2012 20:51:37 GMT"}], "update_date": "2012-03-12", "authors_parsed": [["de Freitas", "Nando", ""], ["Smola", "Alex", ""], ["Zoghi", "Masrour", ""]]}, {"id": "1203.2200", "submitter": "Ryan Rossi", "authors": "Ryan Rossi, Brian Gallagher, Jennifer Neville, Keith Henderson", "title": "Role-Dynamics: Fast Mining of Large Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand the structural dynamics of a large-scale social, biological or\ntechnological network, it may be useful to discover behavioral roles\nrepresenting the main connectivity patterns present over time. In this paper,\nwe propose a scalable non-parametric approach to automatically learn the\nstructural dynamics of the network and individual nodes. Roles may represent\nstructural or behavioral patterns such as the center of a star, peripheral\nnodes, or bridge nodes that connect different communities. Our novel approach\nlearns the appropriate structural role dynamics for any arbitrary network and\ntracks the changes over time. In particular, we uncover the specific global\nnetwork dynamics and the local node dynamics of a technological, communication,\nand social network. We identify interesting node and network patterns such as\nstationary and non-stationary roles, spikes/steps in role-memberships (perhaps\nindicating anomalies), increasing/decreasing role trends, among many others.\nOur results indicate that the nodes in each of these networks have distinct\nconnectivity patterns that are non-stationary and evolve considerably over\ntime. Overall, the experiments demonstrate the effectiveness of our approach\nfor fast mining and tracking of the dynamics in large networks. Furthermore,\nthe dynamic structural representation provides a basis for building more\nsophisticated models and tools that are fast for exploring large dynamic\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2012 22:45:34 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Rossi", "Ryan", ""], ["Gallagher", "Brian", ""], ["Neville", "Jennifer", ""], ["Henderson", "Keith", ""]]}, {"id": "1203.2394", "submitter": "Mohamed Ahmed", "authors": "Mohamed Osama Ahmed, Pouyan T. Bibalan, Nando de Freitas and Simon\n  Fauvel", "title": "Decentralized, Adaptive, Look-Ahead Particle Filtering", "comments": "16 pages, 11 figures, Authorship in alphabetical order", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The decentralized particle filter (DPF) was proposed recently to increase the\nlevel of parallelism of particle filtering. Given a decomposition of the state\nspace into two nested sets of variables, the DPF uses a particle filter to\nsample the first set and then conditions on this sample to generate a set of\nsamples for the second set of variables. The DPF can be understood as a variant\nof the popular Rao-Blackwellized particle filter (RBPF), where the second step\nis carried out using Monte Carlo approximations instead of analytical\ninference. As a result, the range of applications of the DPF is broader than\nthe one for the RBPF. In this paper, we improve the DPF in two ways. First, we\nderive a Monte Carlo approximation of the optimal proposal distribution and,\nconsequently, design and implement a more efficient look-ahead DPF. Although\nthe decentralized filters were initially designed to capitalize on parallel\nimplementation, we show that the look-ahead DPF can outperform the standard\nparticle filter even on a single machine. Second, we propose the use of bandit\nalgorithms to automatically configure the state space decomposition of the DPF.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2012 02:09:32 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Ahmed", "Mohamed Osama", ""], ["Bibalan", "Pouyan T.", ""], ["de Freitas", "Nando", ""], ["Fauvel", "Simon", ""]]}, {"id": "1203.2507", "submitter": "Dong Dai", "authors": "Dong Dai, Philippe Rigollet, Tong Zhang", "title": "Deviation optimal learning using greedy Q-aggregation", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1025 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 3, 1878-1905", "doi": "10.1214/12-AOS1025", "report-no": "IMS-AOS-AOS1025", "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a finite family of functions, the goal of model selection aggregation\nis to construct a procedure that mimics the function from this family that is\nthe closest to an unknown regression function. More precisely, we consider a\ngeneral regression model with fixed design and measure the distance between\nfunctions by the mean squared error at the design points. While procedures\nbased on exponential weights are known to solve the problem of model selection\naggregation in expectation, they are, surprisingly, sub-optimal in deviation.\nWe propose a new formulation called Q-aggregation that addresses this\nlimitation; namely, its solution leads to sharp oracle inequalities that are\noptimal in a minimax sense. Moreover, based on the new formulation, we design\ngreedy Q-aggregation procedures that produce sparse aggregation models\nachieving the optimal rate. The convergence and performance of these greedy\nprocedures are illustrated and compared with other standard methods on\nsimulated examples.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2012 14:50:55 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2012 10:11:08 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["Dai", "Dong", ""], ["Rigollet", "Philippe", ""], ["Zhang", "Tong", ""]]}, {"id": "1203.2511", "submitter": "Victor Seal", "authors": "Victor Seal, Arnab Raha, Shovan Maity, Souvik Kr Mitra, Amitava\n  Mukherjee and Mrinal Kanti Naskar", "title": "A Simple Flood Forecasting Scheme Using Wireless Sensor Networks", "comments": "16 pages, 4 figures, published in International Journal Of Ad-Hoc,\n  Sensor And Ubiquitous Computing, February 2012; V. seal et al, 'A Simple\n  Flood Forecasting Scheme Using Wireless Sensor Networks', IJASUC, Feb.2012", "journal-ref": null, "doi": "10.5121/ijasuc.2012.3105", "report-no": null, "categories": "cs.LG cs.CE cs.NI cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a forecasting model designed using WSNs (Wireless Sensor\nNetworks) to predict flood in rivers using simple and fast calculations to\nprovide real-time results and save the lives of people who may be affected by\nthe flood. Our prediction model uses multiple variable robust linear regression\nwhich is easy to understand and simple and cost effective in implementation, is\nspeed efficient, but has low resource utilization and yet provides real time\npredictions with reliable accuracy, thus having features which are desirable in\nany real world algorithm. Our prediction model is independent of the number of\nparameters, i.e. any number of parameters may be added or removed based on the\non-site requirements. When the water level rises, we represent it using a\npolynomial whose nature is used to determine if the water level may exceed the\nflood line in the near future. We compare our work with a contemporary\nalgorithm to demonstrate our improvements over it. Then we present our\nsimulation results for the predicted water level compared to the actual water\nlevel.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2012 18:08:34 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Seal", "Victor", ""], ["Raha", "Arnab", ""], ["Maity", "Shovan", ""], ["Mitra", "Souvik Kr", ""], ["Mukherjee", "Amitava", ""], ["Naskar", "Mrinal Kanti", ""]]}, {"id": "1203.2557", "submitter": "Phil Long", "authors": "David P. Helmbold and Philip M. Long", "title": "On the Necessity of Irrelevant Variables", "comments": "A preliminary version of this paper appeared in the proceedings of\n  ICML'11", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores the effects of relevant and irrelevant boolean variables\non the accuracy of classifiers. The analysis uses the assumption that the\nvariables are conditionally independent given the class, and focuses on a\nnatural family of learning algorithms for such sources when the relevant\nvariables have a small advantage over random guessing. The main result is that\nalgorithms relying predominately on irrelevant variables have error\nprobabilities that quickly go to 0 in situations where algorithms that limit\nthe use of irrelevant variables have errors bounded below by a positive\nconstant. We also show that accurate learning is possible even when there are\nso few examples that one cannot determine with high confidence whether or not\nany individual variable is relevant.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2012 17:17:34 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2012 18:29:37 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2012 23:50:17 GMT"}], "update_date": "2012-06-12", "authors_parsed": [["Helmbold", "David P.", ""], ["Long", "Philip M.", ""]]}, {"id": "1203.2570", "submitter": "Rob Hall", "authors": "Rob Hall, Alessandro Rinaldo, Larry Wasserman", "title": "Differential Privacy for Functions and Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a framework for privately releasing summaries of a\ndatabase. Previous work has focused mainly on methods for which the output is a\nfinite dimensional vector, or an element of some discrete set. We develop\nmethods for releasing functions while preserving differential privacy.\nSpecifically, we show that adding an appropriate Gaussian process to the\nfunction of interest yields differential privacy. When the functions lie in the\nsame RKHS as the Gaussian process, then the correct noise level is established\nby measuring the \"sensitivity\" of the function in the RKHS norm. As examples we\nconsider kernel density estimation, kernel support vector machines, and\nfunctions in reproducing kernel Hilbert spaces.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2012 18:00:49 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Hall", "Rob", ""], ["Rinaldo", "Alessandro", ""], ["Wasserman", "Larry", ""]]}, {"id": "1203.2821", "submitter": "Edoardo Airoldi", "authors": "Hossein Azari Soufiani, Edoardo M Airoldi", "title": "Graphlet decomposition of a weighted network", "comments": "25 pages, 4 figures, 3 tables", "journal-ref": "Journal of Machine Learning Research, Workshop & Conference\n  Proceedings, vol. 22 (AISTATS), 2012", "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the graphlet decomposition of a weighted network, which encodes\na notion of social information based on social structure. We develop a scalable\ninference algorithm, which combines EM with Bron-Kerbosch in a novel fashion,\nfor estimating the parameters of the model underlying graphlets using one\nnetwork sample. We explore some theoretical properties of the graphlet\ndecomposition, including computational complexity, redundancy and expected\naccuracy. We demonstrate graphlets on synthetic and real data. We analyze\nmessaging patterns on Facebook and criminal associations in the 19th century.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2012 14:18:56 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Soufiani", "Hossein Azari", ""], ["Airoldi", "Edoardo M", ""]]}, {"id": "1203.2987", "submitter": "Saurabh  Pal", "authors": "Surjeet Kumar Yadav, Brijesh Bharadwaj and Saurabh Pal", "title": "Mining Education Data to Predict Student's Retention: A comparative\n  Study", "comments": "5 pages. arXiv admin note: substantial text overlap with\n  arXiv:1202.4815", "journal-ref": "(IJCSIS) International Journal of Computer Science and Information\n  Security, Vol. 10, No. 2, 2012, pp113-117", "doi": null, "report-no": null, "categories": "cs.LG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main objective of higher education is to provide quality education to\nstudents. One way to achieve highest level of quality in higher education\nsystem is by discovering knowledge for prediction regarding enrolment of\nstudents in a course. This paper presents a data mining project to generate\npredictive models for student retention management. Given new records of\nincoming students, these predictive models can produce short accurate\nprediction lists identifying students who tend to need the support from the\nstudent retention program most. This paper examines the quality of the\npredictive models generated by the machine learning algorithms. The results\nshow that some of the machines learning algorithms are able to establish\neffective predictive models from the existing student retention data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 02:23:22 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Yadav", "Surjeet Kumar", ""], ["Bharadwaj", "Brijesh", ""], ["Pal", "Saurabh", ""]]}, {"id": "1203.2990", "submitter": "Yoshua Bengio", "authors": "Yoshua Bengio", "title": "Evolving Culture vs Local Minima", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a theory that relates difficulty of learning in deep architectures\nto culture and language. It is articulated around the following hypotheses: (1)\nlearning in an individual human brain is hampered by the presence of effective\nlocal minima; (2) this optimization difficulty is particularly important when\nit comes to learning higher-level abstractions, i.e., concepts that cover a\nvast and highly-nonlinear span of sensory configurations; (3) such high-level\nabstractions are best represented in brains by the composition of many levels\nof representation, i.e., by deep architectures; (4) a human brain can learn\nsuch high-level abstractions if guided by the signals produced by other humans,\nwhich act as hints or indirect supervision for these high-level abstractions;\nand (5), language and the recombination and optimization of mental concepts\nprovide an efficient evolutionary recombination operator, and this gives rise\nto rapid search in the space of communicable ideas that help humans build up\nbetter high-level internal representations of their world. These hypotheses put\ntogether imply that human culture and the evolution of ideas have been crucial\nto counter an optimization difficulty: this optimization difficulty would\notherwise make it very difficult for human brains to capture high-level\nknowledge of the world. The theory is grounded in experimental observations of\nthe difficulties of training deep artificial neural networks. Plausible\nconsequences of this theory for the efficiency of cultural evolutions are\nsketched.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 02:38:35 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2012 20:02:48 GMT"}], "update_date": "2012-11-30", "authors_parsed": [["Bengio", "Yoshua", ""]]}, {"id": "1203.3376", "submitter": "Carlos Gershenson", "authors": "Bruce Edmonds and Carlos Gershenson", "title": "Learning, Social Intelligence and the Turing Test - why an\n  \"out-of-the-box\" Turing Machine will not pass the Turing Test", "comments": "10 pages, invited talk at Turing Centenary Conference CiE 2012,\n  special session on \"The Turing Test and Thinking Machines\"", "journal-ref": "Lecture Notes in Computer Science 7318 (2012) pp. 182-192", "doi": "10.1007/978-3-642-30870-3_18", "report-no": "CPM Report No.: 12-215", "categories": "cs.AI cs.LG nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Turing Test (TT) checks for human intelligence, rather than any putative\ngeneral intelligence. It involves repeated interaction requiring learning in\nthe form of adaption to the human conversation partner. It is a macro-level\npost-hoc test in contrast to the definition of a Turing Machine (TM), which is\na prior micro-level definition. This raises the question of whether learning is\njust another computational process, i.e. can be implemented as a TM. Here we\nargue that learning or adaption is fundamentally different from computation,\nthough it does involve processes that can be seen as computations. To\nillustrate this difference we compare (a) designing a TM and (b) learning a TM,\ndefining them for the purpose of the argument. We show that there is a\nwell-defined sequence of problems which are not effectively designable but are\nlearnable, in the form of the bounded halting problem. Some characteristics of\nhuman intelligence are reviewed including it's: interactive nature, learning\nabilities, imitative tendencies, linguistic ability and context-dependency. A\nstory that explains some of these is the Social Intelligence Hypothesis. If\nthis is broadly correct, this points to the necessity of a considerable period\nof acculturation (social learning in context) if an artificial intelligence is\nto pass the TT. Whilst it is always possible to 'compile' the results of\nlearning into a TM, this would not be a designed TM and would not be able to\ncontinually adapt (pass future TTs). We conclude three things, namely that: a\npurely \"designed\" TM will never pass the TT; that there is no such thing as a\ngeneral intelligence since it necessary involves learning; and that\nlearning/adaption and computation should be clearly distinguished.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 14:47:26 GMT"}], "update_date": "2012-10-10", "authors_parsed": [["Edmonds", "Bruce", ""], ["Gershenson", "Carlos", ""]]}, {"id": "1203.3461", "submitter": "Kaizhu Huang", "authors": "Kaizhu Huang, Rong Jin, Zenglin Xu, Cheng-Lin Liu", "title": "Robust Metric Learning by Smooth Optimization", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-244-251", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing distance metric learning methods assume perfect side\ninformation that is usually given in pairwise or triplet constraints. Instead,\nin many real-world applications, the constraints are derived from side\ninformation, such as users' implicit feedbacks and citations among articles. As\na result, these constraints are usually noisy and contain many mistakes. In\nthis work, we aim to learn a distance metric from noisy constraints by robust\noptimization in a worst-case scenario, to which we refer as robust metric\nlearning. We formulate the learning task initially as a combinatorial\noptimization problem, and show that it can be elegantly transformed to a convex\nprogramming problem. We present an efficient learning algorithm based on smooth\noptimization [7]. It has a worst-case convergence rate of\nO(1/{\\surd}{\\varepsilon}) for smooth optimization problems, where {\\varepsilon}\nis the desired error of the approximate solution. Finally, our empirical study\nwith UCI data sets demonstrate the effectiveness of the proposed method in\ncomparison to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Huang", "Kaizhu", ""], ["Jin", "Rong", ""], ["Xu", "Zenglin", ""], ["Liu", "Cheng-Lin", ""]]}, {"id": "1203.3462", "submitter": "Amrudin Agovic", "authors": "Amrudin Agovic, Arindam Banerjee", "title": "Gaussian Process Topic Models", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-10-19", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Gaussian Process Topic Models (GPTMs), a new family of topic\nmodels which can leverage a kernel among documents while extracting correlated\ntopics. GPTMs can be considered a systematic generalization of the Correlated\nTopic Models (CTMs) using ideas from Gaussian Process (GP) based embedding.\nSince GPTMs work with both a topic covariance matrix and a document kernel\nmatrix, learning GPTMs involves a novel component-solving a suitable Sylvester\nequation capturing both topic and document dependencies. The efficacy of GPTMs\nis demonstrated with experiments evaluating the quality of both topic modeling\nand embedding.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Agovic", "Amrudin", ""], ["Banerjee", "Arindam", ""]]}, {"id": "1203.3463", "submitter": "Amr Ahmed", "authors": "Amr Ahmed, Eric P. Xing", "title": "Timeline: A Dynamic Hierarchical Dirichlet Process Model for Recovering\n  Birth/Death and Evolution of Topics in Text Stream", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-20-29", "categories": "cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models have proven to be a useful tool for discovering latent\nstructures in document collections. However, most document collections often\ncome as temporal streams and thus several aspects of the latent structure such\nas the number of topics, the topics' distribution and popularity are\ntime-evolving. Several models exist that model the evolution of some but not\nall of the above aspects. In this paper we introduce infinite dynamic topic\nmodels, iDTM, that can accommodate the evolution of all the aforementioned\naspects. Our model assumes that documents are organized into epochs, where the\ndocuments within each epoch are exchangeable but the order between the\ndocuments is maintained across epochs. iDTM allows for unbounded number of\ntopics: topics can die or be born at any epoch, and the representation of each\ntopic can evolve according to a Markovian dynamics. We use iDTM to analyze the\nbirth and evolution of topics in the NIPS community and evaluated the efficacy\nof our model on both simulated and real datasets with favorable outcome.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Ahmed", "Amr", ""], ["Xing", "Eric P.", ""]]}, {"id": "1203.3468", "submitter": "Charles Blundell", "authors": "Charles Blundell, Yee Whye Teh, Katherine A. Heller", "title": "Bayesian Rose Trees", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-65-72", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical structure is ubiquitous in data across many domains. There are\nmany hierarchical clustering methods, frequently used by domain experts, which\nstrive to discover this structure. However, most of these methods limit\ndiscoverable hierarchies to those with binary branching structure. This\nlimitation, while computationally convenient, is often undesirable. In this\npaper we explore a Bayesian hierarchical clustering algorithm that can produce\ntrees with arbitrary branching structure at each node, known as rose trees. We\ninterpret these trees as mixtures over partitions of a data set, and use a\ncomputationally efficient, greedy agglomerative algorithm to find the rose\ntrees which have high marginal likelihood given the data. Lastly, we perform\nexperiments which demonstrate that rose trees are better models of data than\nthe typical binary trees returned by other hierarchical clustering algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Blundell", "Charles", ""], ["Teh", "Yee Whye", ""], ["Heller", "Katherine A.", ""]]}, {"id": "1203.3471", "submitter": "Kamalika Chaudhuri", "authors": "Kamalika Chaudhuri, Yoav Freund, Daniel Hsu", "title": "An Online Learning-based Framework for Tracking", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-101-108", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the tracking problem, namely, estimating the hidden state of an\nobject over time, from unreliable and noisy measurements. The standard\nframework for the tracking problem is the generative framework, which is the\nbasis of solutions such as the Bayesian algorithm and its approximation, the\nparticle filters. However, these solutions can be very sensitive to model\nmismatches. In this paper, motivated by online learning, we introduce a new\nframework for tracking. We provide an efficient tracking algorithm for this\nframework. We provide experimental results comparing our algorithm to the\nBayesian algorithm on simulated data. Our experiments show that when there are\nslight model mismatches, our algorithm outperforms the Bayesian algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Chaudhuri", "Kamalika", ""], ["Freund", "Yoav", ""], ["Hsu", "Daniel", ""]]}, {"id": "1203.3472", "submitter": "Yutian Chen", "authors": "Yutian Chen, Max Welling, Alex Smola", "title": "Super-Samples from Kernel Herding", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-109-116", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the herding algorithm to continuous spaces by using the kernel\ntrick. The resulting \"kernel herding\" algorithm is an infinite memory\ndeterministic process that learns to approximate a PDF with a collection of\nsamples. We show that kernel herding decreases the error of expectations of\nfunctions in the Hilbert space at a rate O(1/T) which is much faster than the\nusual O(1/pT) for iid random samples. We illustrate kernel herding by\napproximating Bayesian predictive distributions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Chen", "Yutian", ""], ["Welling", "Max", ""], ["Smola", "Alex", ""]]}, {"id": "1203.3475", "submitter": "Povilas Daniusis", "authors": "Povilas Daniusis, Dominik Janzing, Joris Mooij, Jakob Zscheischler,\n  Bastian Steudel, Kun Zhang, Bernhard Schoelkopf", "title": "Inferring deterministic causal relations", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-143-150", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two variables that are related to each other by an invertible\nfunction. While it has previously been shown that the dependence structure of\nthe noise can provide hints to determine which of the two variables is the\ncause, we presently show that even in the deterministic (noise-free) case,\nthere are asymmetries that can be exploited for causal inference. Our method is\nbased on the idea that if the function and the probability density of the cause\nare chosen independently, then the distribution of the effect will, in a\ncertain sense, depend on the function. We provide a theoretical analysis of\nthis method, showing that it also works in the low noise regime, and link it to\ninformation geometry. We report strong empirical results on various real-world\ndata sets from different domains.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Daniusis", "Povilas", ""], ["Janzing", "Dominik", ""], ["Mooij", "Joris", ""], ["Zscheischler", "Jakob", ""], ["Steudel", "Bastian", ""], ["Zhang", "Kun", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "1203.3476", "submitter": "Gal Elidan", "authors": "Gal Elidan", "title": "Inference-less Density Estimation using Copula Bayesian Networks", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-151-159", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning continuous probabilistic graphical models in the face of\nmissing data. For non-Gaussian models, learning the parameters and structure of\nsuch models depends on our ability to perform efficient inference, and can be\nprohibitive even for relatively modest domains. Recently, we introduced the\nCopula Bayesian Network (CBN) density model - a flexible framework that\ncaptures complex high-dimensional dependency structures while offering direct\ncontrol over the univariate marginals, leading to improved generalization. In\nthis work we show that the CBN model also offers significant computational\nadvantages when training data is partially observed. Concretely, we leverage on\nthe specialized form of the model to derive a computationally amenable learning\nobjective that is a lower bound on the log-likelihood function. Importantly,\nour energy-like bound circumvents the need for costly inference of an auxiliary\ndistribution, thus facilitating practical learning of highdimensional\ndensities. We demonstrate the effectiveness of our approach for learning the\nstructure and parameters of a CBN model for two reallife continuous domains.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Elidan", "Gal", ""]]}, {"id": "1203.3481", "submitter": "Robert Glaubius", "authors": "Robert Glaubius, Terry Tidwell, Christopher Gill, William D. Smart", "title": "Real-Time Scheduling via Reinforcement Learning", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-201-209", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-physical systems, such as mobile robots, must respond adaptively to\ndynamic operating conditions. Effective operation of these systems requires\nthat sensing and actuation tasks are performed in a timely manner.\nAdditionally, execution of mission specific tasks such as imaging a room must\nbe balanced against the need to perform more general tasks such as obstacle\navoidance. This problem has been addressed by maintaining relative utilization\nof shared resources among tasks near a user-specified target level. Producing\noptimal scheduling strategies requires complete prior knowledge of task\nbehavior, which is unlikely to be available in practice. Instead, suitable\nscheduling strategies must be learned online through interaction with the\nsystem. We consider the sample complexity of reinforcement learning in this\ndomain, and demonstrate that while the problem state space is countably\ninfinite, we may leverage the problem's structure to guarantee efficient\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Glaubius", "Robert", ""], ["Tidwell", "Terry", ""], ["Gill", "Christopher", ""], ["Smart", "William D.", ""]]}, {"id": "1203.3483", "submitter": "Mithun Das Gupta", "authors": "Mithun Das Gupta, Thomas S. Huang", "title": "Regularized Maximum Likelihood for Intrinsic Dimension Estimation", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-220-227", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for estimating the intrinsic dimension of a dataset\nby applying the principle of regularized maximum likelihood to the distances\nbetween close neighbors. We propose a regularization scheme which is motivated\nby divergence minimization principles. We derive the estimator by a Poisson\nprocess approximation, argue about its convergence properties and apply it to a\nnumber of simulated and real datasets. We also show it has the best overall\nperformance compared with two other intrinsic dimension estimators.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Gupta", "Mithun Das", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1203.3485", "submitter": "Matthew J. Johnson", "authors": "Matthew J. Johnson, Alan Willsky", "title": "The Hierarchical Dirichlet Process Hidden Semi-Markov Model", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-252-259", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is much interest in the Hierarchical Dirichlet Process Hidden Markov\nModel (HDP-HMM) as a natural Bayesian nonparametric extension of the\ntraditional HMM. However, in many settings the HDP-HMM's strict Markovian\nconstraints are undesirable, particularly if we wish to learn or encode\nnon-geometric state durations. We can extend the HDP-HMM to capture such\nstructure by drawing upon explicit-duration semi-Markovianity, which has been\ndeveloped in the parametric setting to allow construction of highly\ninterpretable models that admit natural prior information on state durations.\nIn this paper we introduce the explicitduration HDP-HSMM and develop posterior\nsampling algorithms for efficient inference in both the direct-assignment and\nweak-limit approximation settings. We demonstrate the utility of the model and\nour inference methods on synthetic data as well as experiments on a speaker\ndiarization problem and an example of learning the patterns in Morse code.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Johnson", "Matthew J.", ""], ["Willsky", "Alan", ""]]}, {"id": "1203.3486", "submitter": "Berk Kapicioglu", "authors": "Berk Kapicioglu, Robert E. Schapire, Martin Wikelski, Tamara Broderick", "title": "Combining Spatial and Telemetric Features for Learning Animal Movement\n  Models", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-260-267", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new graphical model for tracking radio-tagged animals and\nlearning their movement patterns. The model provides a principled way to\ncombine radio telemetry data with an arbitrary set of userdefined, spatial\nfeatures. We describe an efficient stochastic gradient algorithm for fitting\nmodel parameters to data and demonstrate its effectiveness via asymptotic\nanalysis and synthetic experiments. We also apply our model to real datasets,\nand show that it outperforms the most popular radio telemetry software package\nused in ecology. We conclude that integration of different data sources under a\nsingle statistical framework, coupled with appropriate parameter and state\nestimation procedures, produces both accurate location estimates and an\ninterpretable statistical model of animal movement.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Kapicioglu", "Berk", ""], ["Schapire", "Robert E.", ""], ["Wikelski", "Martin", ""], ["Broderick", "Tamara", ""]]}, {"id": "1203.3488", "submitter": "Kevin T. Kelly", "authors": "Kevin T. Kelly, Conor Mayo-Wilson", "title": "Causal Conclusions that Flip Repeatedly and Their Justification", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-277-285", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past two decades, several consistent procedures have been designed\nto infer causal conclusions from observational data. We prove that if the true\ncausal network might be an arbitrary, linear Gaussian network or a discrete\nBayes network, then every unambiguous causal conclusion produced by a\nconsistent method from non-experimental data is subject to reversal as the\nsample size increases any finite number of times. That result, called the\ncausal flipping theorem, extends prior results to the effect that causal\ndiscovery cannot be reliable on a given sample size. We argue that since\nrepeated flipping of causal conclusions is unavoidable in principle for\nconsistent methods, the best possible discovery methods are consistent methods\nthat retract their earlier conclusions no more than necessary. A series of\nsimulations of various methods across a wide range of sample sizes illustrates\nconcretely both the theorem and the principle of comparing methods in terms of\nretractions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Kelly", "Kevin T.", ""], ["Mayo-Wilson", "Conor", ""]]}, {"id": "1203.3489", "submitter": "Arto Klami", "authors": "Arto Klami, Seppo Virtanen, Samuel Kaski", "title": "Bayesian exponential family projections for coupled data sources", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-286-293", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential family extensions of principal component analysis (EPCA) have\nreceived a considerable amount of attention in recent years, demonstrating the\ngrowing need for basic modeling tools that do not assume the squared loss or\nGaussian distribution. We extend the EPCA model toolbox by presenting the first\nexponential family multi-view learning methods of the partial least squares and\ncanonical correlation analysis, based on a unified representation of EPCA as\nmatrix factorization of the natural parameters of exponential family. The\nmodels are based on a new family of priors that are generally usable for all\nsuch factorizations. We also introduce new inference strategies, and\ndemonstrate how the methods outperform earlier ones when the Gaussianity\nassumption does not hold.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Klami", "Arto", ""], ["Virtanen", "Seppo", ""], ["Kaski", "Samuel", ""]]}, {"id": "1203.3491", "submitter": "Ping Li", "authors": "Ping Li", "title": "Robust LogitBoost and Adaptive Base Class (ABC) LogitBoost", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-302-311", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logitboost is an influential boosting algorithm for classification. In this\npaper, we develop robust logitboost to provide an explicit formulation of\ntree-split criterion for building weak learners (regression trees) for\nlogitboost. This formulation leads to a numerically stable implementation of\nlogitboost. We then propose abc-logitboost for multi-class classification, by\ncombining robust logitboost with the prior work of abc-boost. Previously,\nabc-boost was implemented as abc-mart using the mart algorithm. Our extensive\nexperiments on multi-class classification compare four algorithms: mart,\nabcmart, (robust) logitboost, and abc-logitboost, and demonstrate the\nsuperiority of abc-logitboost. Comparisons with other learning methods\nincluding SVM and deep learning are also available through prior publications.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1203.3492", "submitter": "Ping Li", "authors": "Ping Li, Michael W. Mahoney, Yiyuan She", "title": "Approximating Higher-Order Distances Using Random Projections", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-312-321", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a simple method and relevant theoretical analysis for efficiently\nestimating higher-order lp distances. While the analysis mainly focuses on l4,\nour methodology extends naturally to p = 6,8,10..., (i.e., when p is even).\nDistance-based methods are popular in machine learning. In large-scale\napplications, storing, computing, and retrieving the distances can be both\nspace and time prohibitive. Efficient algorithms exist for estimating lp\ndistances if 0 < p <= 2. The task for p > 2 is known to be difficult. Our work\npartially fills this gap.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Li", "Ping", ""], ["Mahoney", "Michael W.", ""], ["She", "Yiyuan", ""]]}, {"id": "1203.3494", "submitter": "Qiang Liu", "authors": "Qiang Liu, Alexander T. Ihler", "title": "Negative Tree Reweighted Belief Propagation", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-332-339", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of lower bounds on the log partition function of a\nMarkov random field which makes use of a reversed Jensen's inequality. In\nparticular, our method approximates the intractable distribution using a linear\ncombination of spanning trees with negative weights. This technique is a\nlower-bound counterpart to the tree-reweighted belief propagation algorithm,\nwhich uses a convex combination of spanning trees with positive weights to\nprovide corresponding upper bounds. We develop algorithms to optimize and\ntighten the lower bounds over the non-convex set of valid parameter values. Our\nalgorithm generalizes mean field approaches (including naive and structured\nmean field approximations), which it includes as a limiting case.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Liu", "Qiang", ""], ["Ihler", "Alexander T.", ""]]}, {"id": "1203.3495", "submitter": "Qi Mao", "authors": "Qi Mao, Ivor W. Tsang", "title": "Parameter-Free Spectral Kernel Learning", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-350-357", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the growing ubiquity of unlabeled data, learning with unlabeled data\nis attracting increasing attention in machine learning. In this paper, we\npropose a novel semi-supervised kernel learning method which can seamlessly\ncombine manifold structure of unlabeled data and Regularized Least-Squares\n(RLS) to learn a new kernel. Interestingly, the new kernel matrix can be\nobtained analytically with the use of spectral decomposition of graph Laplacian\nmatrix. Hence, the proposed algorithm does not require any numerical\noptimization solvers. Moreover, by maximizing kernel target alignment on\nlabeled data, we can also learn model parameters automatically with a\nclosed-form solution. For a given graph Laplacian matrix, our proposed method\ndoes not need to tune any model parameter including the tradeoff parameter in\nRLS and the balance parameter for unlabeled data. Extensive experiments on ten\nbenchmark datasets show that our proposed two-stage parameter-free spectral\nkernel learning algorithm can obtain comparable performance with fine-tuned\nmanifold regularization methods in transductive setting, and outperform\nmultiple kernel learning in supervised setting.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Mao", "Qi", ""], ["Tsang", "Ivor W.", ""]]}, {"id": "1203.3496", "submitter": "Marina Meila", "authors": "Marina Meila, Harr Chen", "title": "Dirichlet Process Mixtures of Generalized Mallows Models", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-358-367", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Dirichlet process mixture model over discrete incomplete\nrankings and study two Gibbs sampling inference techniques for estimating\nposterior clusterings. The first approach uses a slice sampling subcomponent\nfor estimating cluster parameters. The second approach marginalizes out several\ncluster parameters by taking advantage of approximations to the conditional\nposteriors. We empirically demonstrate (1) the effectiveness of this\napproximation for improving convergence, (2) the benefits of the Dirichlet\nprocess model over alternative clustering techniques for ranked data, and (3)\nthe applicability of the approach to exploring large realworld ranking\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Meila", "Marina", ""], ["Chen", "Harr", ""]]}, {"id": "1203.3497", "submitter": "Tetsuro Morimura", "authors": "Tetsuro Morimura, Masashi Sugiyama, Hisashi Kashima, Hirotaka Hachiya,\n  Toshiyuki Tanaka", "title": "Parametric Return Density Estimation for Reinforcement Learning", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-368-375", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most conventional Reinforcement Learning (RL) algorithms aim to optimize\ndecision-making rules in terms of the expected returns. However, especially for\nrisk management purposes, other risk-sensitive criteria such as the\nvalue-at-risk or the expected shortfall are sometimes preferred in real\napplications. Here, we describe a parametric method for estimating density of\nthe returns, which allows us to handle various criteria in a unified manner. We\nfirst extend the Bellman equation for the conditional expected return to cover\na conditional probability density of the returns. Then we derive an extension\nof the TD-learning algorithm for estimating the return densities in an unknown\nenvironment. As test instances, several parametric density estimation\nalgorithms are presented for the Gaussian, Laplace, and skewed Laplace\ndistributions. We show that these algorithms lead to risk-sensitive as well as\nrobust RL paradigms through numerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Morimura", "Tetsuro", ""], ["Sugiyama", "Masashi", ""], ["Kashima", "Hisashi", ""], ["Hachiya", "Hirotaka", ""], ["Tanaka", "Toshiyuki", ""]]}, {"id": "1203.3501", "submitter": "Sebastian Ordyniak", "authors": "Sebastian Ordyniak, Stefan Szeider", "title": "Algorithms and Complexity Results for Exact Bayesian Structure Learning", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-401-408", "categories": "cs.LG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian structure learning is the NP-hard problem of discovering a Bayesian\nnetwork that optimally represents a given set of training data. In this paper\nwe study the computational worst-case complexity of exact Bayesian structure\nlearning under graph theoretic restrictions on the super-structure. The\nsuper-structure (a concept introduced by Perrier, Imoto, and Miyano, JMLR 2008)\nis an undirected graph that contains as subgraphs the skeletons of solution\nnetworks. Our results apply to several variants of score-based Bayesian\nstructure learning where the score of a network decomposes into local scores of\nits nodes. Results: We show that exact Bayesian structure learning can be\ncarried out in non-uniform polynomial time if the super-structure has bounded\ntreewidth and in linear time if in addition the super-structure has bounded\nmaximum degree. We complement this with a number of hardness results. We show\nthat both restrictions (treewidth and degree) are essential and cannot be\ndropped without loosing uniform polynomial time tractability (subject to a\ncomplexity-theoretic assumption). Furthermore, we show that the restrictions\nremain essential if we do not search for a globally optimal network but we aim\nto improve a given network by means of at most k arc additions, arc deletions,\nor arc reversals (k-neighborhood local search).\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Ordyniak", "Sebastian", ""], ["Szeider", "Stefan", ""]]}, {"id": "1203.3506", "submitter": "Miika Pihlaja", "authors": "Miika Pihlaja, Michael Gutmann, Aapo Hyvarinen", "title": "A Family of Computationally Efficient and Simple Estimators for\n  Unnormalized Statistical Models", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-442-449", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new family of estimators for unnormalized statistical models.\nOur family of estimators is parameterized by two nonlinear functions and uses a\nsingle sample from an auxiliary distribution, generalizing Maximum Likelihood\nMonte Carlo estimation of Geyer and Thompson (1992). The family is such that we\ncan estimate the partition function like any other parameter in the model. The\nestimation is done by optimizing an algebraically simple, well defined\nobjective function, which allows for the use of dedicated optimization methods.\nWe establish consistency of the estimator family and give an expression for the\nasymptotic covariance matrix, which enables us to further analyze the influence\nof the nonlinearities and the auxiliary density on estimation performance. Some\nestimators in our family are particularly stable for a wide range of auxiliary\ndensities. Interestingly, a specific choice of the nonlinearity establishes a\nconnection between density estimation and classification by nonlinear logistic\nregression. Finally, the optimal amount of auxiliary samples relative to the\ngiven amount of the data is considered from the perspective of computational\nefficiency.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Pihlaja", "Miika", ""], ["Gutmann", "Michael", ""], ["Hyvarinen", "Aapo", ""]]}, {"id": "1203.3507", "submitter": "Yuan (Alan) Qi", "authors": "Yuan (Alan) Qi, Ahmed H. Abdel-Gawad, Thomas P. Minka", "title": "Sparse-posterior Gaussian Processes for general likelihoods", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-450-457", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) provide a probabilistic nonparametric representation\nof functions in regression, classification, and other problems. Unfortunately,\nexact learning with GPs is intractable for large datasets. A variety of\napproximate GP methods have been proposed that essentially map the large\ndataset into a small set of basis points. Among them, two state-of-the-art\nmethods are sparse pseudo-input Gaussian process (SPGP) (Snelson and\nGhahramani, 2006) and variablesigma GP (VSGP) Walder et al. (2008), which\ngeneralizes SPGP and allows each basis point to have its own length scale.\nHowever, VSGP was only derived for regression. In this paper, we propose a new\nsparse GP framework that uses expectation propagation to directly approximate\ngeneral GP likelihoods using a sparse and smooth basis. It includes both SPGP\nand VSGP for regression as special cases. Plus as an EP algorithm, it inherits\nthe ability to process data online. As a particular choice of approximating\nfamily, we blur each basis point with a Gaussian distribution that has a full\ncovariance matrix representing the data distribution around that basis point;\nas a result, we can summarize local data manifold information with a small set\nof basis points. Our experiments demonstrate that this framework outperforms\nprevious GP classification methods on benchmark datasets in terms of minimizing\ndivergence to the non-sparse GP solution as well as lower misclassification\nrate.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Yuan", "", "", "Alan"], ["Qi", "", ""], ["Abdel-Gawad", "Ahmed H.", ""], ["Minka", "Thomas P.", ""]]}, {"id": "1203.3510", "submitter": "Michael Ramati", "authors": "Michael Ramati, Yuval Shahar", "title": "Irregular-Time Bayesian Networks", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-484-491", "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many fields observations are performed irregularly along time, due to\neither measurement limitations or lack of a constant immanent rate. While\ndiscrete-time Markov models (as Dynamic Bayesian Networks) introduce either\ninefficient computation or an information loss to reasoning about such\nprocesses, continuous-time Markov models assume either a discrete state space\n(as Continuous-Time Bayesian Networks), or a flat continuous state space (as\nstochastic differential equations). To address these problems, we present a new\nmodeling class called Irregular-Time Bayesian Networks (ITBNs), generalizing\nDynamic Bayesian Networks, allowing substantially more compact representations,\nand increasing the expressivity of the temporal dynamics. In addition, a\nglobally optimal solution is guaranteed when learning temporal systems,\nprovided that they are fully observed at the same irregularly spaced\ntime-points, and a semiparametric subclass of ITBNs is introduced to allow\nfurther adaptation to the irregular nature of the available data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Ramati", "Michael", ""], ["Shahar", "Yuval", ""]]}, {"id": "1203.3511", "submitter": "Sebastian Riedel", "authors": "Sebastian Riedel, David A. Smith, Andrew McCallum", "title": "Inference by Minimizing Size, Divergence, or their Sum", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-492-499", "categories": "cs.LG cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We speed up marginal inference by ignoring factors that do not significantly\ncontribute to overall accuracy. In order to pick a suitable subset of factors\nto ignore, we propose three schemes: minimizing the number of model factors\nunder a bound on the KL divergence between pruned and full models; minimizing\nthe KL divergence under a bound on factor count; and minimizing the weighted\nsum of KL divergence and factor count. All three problems are solved using an\napproximation of the KL divergence than can be calculated in terms of marginals\ncomputed on a simple seed graph. Applied to synthetic image denoising and to\nthree different types of NLP parsing models, this technique performs marginal\ninference up to 11 times faster than loopy BP, with graph sizes reduced up to\n98%-at comparable error in marginals and parsing accuracy. We also show that\nminimizing the weighted sum of divergence and size is substantially faster than\nminimizing either of the other objectives based on the approximation to\ndivergence presented here.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Riedel", "Sebastian", ""], ["Smith", "David A.", ""], ["McCallum", "Andrew", ""]]}, {"id": "1203.3516", "submitter": "Aleksandr Simma", "authors": "Aleksandr Simma, Michael I. Jordan", "title": "Modeling Events with Cascades of Poisson Processes", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-546-555", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic model of events in continuous time in which each\nevent triggers a Poisson process of successor events. The ensemble of observed\nevents is thereby modeled as a superposition of Poisson processes. Efficient\ninference is feasible under this model with an EM algorithm. Moreover, the EM\nalgorithm can be implemented as a distributed algorithm, permitting the model\nto be applied to very large datasets. We apply these techniques to the modeling\nof Twitter messages and the revision history of Wikipedia.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Simma", "Aleksandr", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1203.3517", "submitter": "Ajit P. Singh", "authors": "Ajit P. Singh, Geoffrey Gordon", "title": "A Bayesian Matrix Factorization Model for Relational Data", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-556-563", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational learning can be used to augment one data source with other\ncorrelated sources of information, to improve predictive accuracy. We frame a\nlarge class of relational learning problems as matrix factorization problems,\nand propose a hierarchical Bayesian model. Training our Bayesian model using\nrandom-walk Metropolis-Hastings is impractically slow, and so we develop a\nblock Metropolis-Hastings sampler which uses the gradient and Hessian of the\nlikelihood to dynamically tune the proposal. We demonstrate that a predictive\nmodel of brain response to stimuli can be improved by augmenting it with side\ninformation about the stimuli.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Singh", "Ajit P.", ""], ["Gordon", "Geoffrey", ""]]}, {"id": "1203.3518", "submitter": "Jonathan Sorg", "authors": "Jonathan Sorg, Satinder Singh, Richard L. Lewis", "title": "Variance-Based Rewards for Approximate Bayesian Reinforcement Learning", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-564-571", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explore{exploit dilemma is one of the central challenges in Reinforcement\nLearning (RL). Bayesian RL solves the dilemma by providing the agent with\ninformation in the form of a prior distribution over environments; however,\nfull Bayesian planning is intractable. Planning with the mean MDP is a common\nmyopic approximation of Bayesian planning. We derive a novel reward bonus that\nis a function of the posterior distribution over environments, which, when\nadded to the reward in planning with the mean MDP, results in an agent which\nexplores efficiently and effectively. Although our method is similar to\nexisting methods when given an uninformative or unstructured prior, unlike\nexisting methods, our method can exploit structured priors. We prove that our\nmethod results in a polynomial sample complexity and empirically demonstrate\nits advantages in a structured exploration task.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Sorg", "Jonathan", ""], ["Singh", "Satinder", ""], ["Lewis", "Richard L.", ""]]}, {"id": "1203.3519", "submitter": "Gerald Tesauro", "authors": "Gerald Tesauro, V T Rajan, Richard Segal", "title": "Bayesian Inference in Monte-Carlo Tree Search", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-580-588", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte-Carlo Tree Search (MCTS) methods are drawing great interest after\nyielding breakthrough results in computer Go. This paper proposes a Bayesian\napproach to MCTS that is inspired by distributionfree approaches such as UCT\n[13], yet significantly differs in important respects. The Bayesian framework\nallows potentially much more accurate (Bayes-optimal) estimation of node values\nand node uncertainties from a limited number of simulation trials. We further\npropose propagating inference in the tree via fast analytic Gaussian\napproximation methods: this can make the overhead of Bayesian inference\nmanageable in domains such as Go, while preserving high accuracy of\nexpected-value estimates. We find substantial empirical outperformance of UCT\nin an idealized bandit-tree test environment, where we can obtain valuable\ninsights by comparing with known ground truth. Additionally we rigorously prove\non-policy and off-policy convergence of the proposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Tesauro", "Gerald", ""], ["Rajan", "V T", ""], ["Segal", "Richard", ""]]}, {"id": "1203.3520", "submitter": "Jin Tian", "authors": "Jin Tian, Ru He, Lavanya Ram", "title": "Bayesian Model Averaging Using the k-best Bayesian Network Structures", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-589-597", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning Bayesian network structures from data. We\ndevelop an algorithm for finding the k-best Bayesian network structures. We\npropose to compute the posterior probabilities of hypotheses of interest by\nBayesian model averaging over the k-best Bayesian networks. We present\nempirical results on structural discovery over several real and synthetic data\nsets and show that the method outperforms the model selection method and the\nstate of-the-art MCMC methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Tian", "Jin", ""], ["He", "Ru", ""], ["Ram", "Lavanya", ""]]}, {"id": "1203.3521", "submitter": "Maomi Ueno", "authors": "Maomi Ueno", "title": "Learning networks determined by the ratio of prior and data", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-598-605", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent reports have described that the equivalent sample size (ESS) in a\nDirichlet prior plays an important role in learning Bayesian networks. This\npaper provides an asymptotic analysis of the marginal likelihood score for a\nBayesian network. Results show that the ratio of the ESS and sample size\ndetermine the penalty of adding arcs in learning Bayesian networks. The number\nof arcs increases monotonically as the ESS increases; the number of arcs\nmonotonically decreases as the ESS decreases. Furthermore, the marginal\nlikelihood score provides a unified expression of various score metrics by\nchanging prior knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Ueno", "Maomi", ""]]}, {"id": "1203.3522", "submitter": "Michal Valko", "authors": "Michal Valko, Branislav Kveton, Ling Huang, Daniel Ting", "title": "Online Semi-Supervised Learning on Quantized Graphs", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-606-614", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we tackle the problem of online semi-supervised learning\n(SSL). When data arrive in a stream, the dual problems of computation and data\nstorage arise for any SSL method. We propose a fast approximate online SSL\nalgorithm that solves for the harmonic solution on an approximate graph. We\nshow, both empirically and theoretically, that good behavior can be achieved by\ncollapsing nearby points into a set of local \"representative points\" that\nminimize distortion. Moreover, we regularize the harmonic solution to achieve\nbetter stability properties. We apply our algorithm to face recognition and\noptical character recognition applications to show that we can take advantage\nof the manifold structure to outperform the previous methods. Unlike previous\nheuristic approaches, we show that our method yields provable performance\nbounds.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Valko", "Michal", ""], ["Kveton", "Branislav", ""], ["Huang", "Ling", ""], ["Ting", "Daniel", ""]]}, {"id": "1203.3524", "submitter": "Jarno Vanhatalo", "authors": "Jarno Vanhatalo, Aki Vehtari", "title": "Speeding up the binary Gaussian process classification", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-623-631", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GP) are attractive building blocks for many probabilistic\nmodels. Their drawbacks, however, are the rapidly increasing inference time and\nmemory requirement alongside increasing data. The problem can be alleviated\nwith compactly supported (CS) covariance functions, which produce sparse\ncovariance matrices that are fast in computations and cheap to store. CS\nfunctions have previously been used in GP regression but here the focus is in a\nclassification problem. This brings new challenges since the posterior\ninference has to be done approximately. We utilize the expectation propagation\nalgorithm and show how its standard implementation has to be modified to obtain\ncomputational benefits from the sparse covariance matrices. We study four CS\ncovariance functions and show that they may lead to substantial speed up in the\ninference time compared to globally supported functions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Vanhatalo", "Jarno", ""], ["Vehtari", "Aki", ""]]}, {"id": "1203.3526", "submitter": "Tomas Werner", "authors": "Tomas Werner", "title": "Primal View on Belief Propagation", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-651-657", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that fixed points of loopy belief propagation (BP) correspond to\nstationary points of the Bethe variational problem, where we minimize the Bethe\nfree energy subject to normalization and marginalization constraints.\nUnfortunately, this does not entirely explain BP because BP is a dual rather\nthan primal algorithm to solve the Bethe variational problem -- beliefs are\ninfeasible before convergence. Thus, we have no better understanding of BP than\nas an algorithm to seek for a common zero of a system of non-linear functions,\nnot explicitly related to each other. In this theoretical paper, we show that\nthese functions are in fact explicitly related -- they are the partial\nderivatives of a single function of reparameterizations. That means, BP seeks\nfor a stationary point of a single function, without any constraints. This\nfunction has a very natural form: it is a linear combination of local\nlog-partition functions, exactly as the Bethe entropy is the same linear\ncombination of local entropies.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Werner", "Tomas", ""]]}, {"id": "1203.3529", "submitter": "Yan Yan", "authors": "Yan Yan, Romer Rosales, Glenn Fung, Jennifer Dy", "title": "Modeling Multiple Annotator Expertise in the Semi-Supervised Learning\n  Scenario", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-674-682", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning algorithms normally assume that there is at most one annotation or\nlabel per data point. However, in some scenarios, such as medical diagnosis and\non-line collaboration,multiple annotations may be available. In either case,\nobtaining labels for data points can be expensive and time-consuming (in some\ncircumstances ground-truth may not exist). Semi-supervised learning approaches\nhave shown that utilizing the unlabeled data is often beneficial in these\ncases. This paper presents a probabilistic semi-supervised model and algorithm\nthat allows for learning from both unlabeled and labeled data in the presence\nof multiple annotators. We assume that it is known what annotator labeled which\ndata points. The proposed approach produces annotator models that allow us to\nprovide (1) estimates of the true label and (2) annotator variable expertise\nfor both labeled and unlabeled data. We provide numerical comparisons under\nvarious scenarios and with respect to standard semi-supervised learning.\nExperiments showed that the presented approach provides clear advantages over\nmulti-annotator methods that do not use the unlabeled data and over methods\nthat do not use multi-labeler information.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Yan", "Yan", ""], ["Rosales", "Romer", ""], ["Fung", "Glenn", ""], ["Dy", "Jennifer", ""]]}, {"id": "1203.3530", "submitter": "Shuang Hong Yang", "authors": "Shuang Hong Yang, Jiang Bian, Hongyuan Zha", "title": "Hybrid Generative/Discriminative Learning for Automatic Image Annotation", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-683-690", "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic image annotation (AIA) raises tremendous challenges to machine\nlearning as it requires modeling of data that are both ambiguous in input and\noutput, e.g., images containing multiple objects and labeled with multiple\nsemantic tags. Even more challenging is that the number of candidate tags is\nusually huge (as large as the vocabulary size) yet each image is only related\nto a few of them. This paper presents a hybrid generative-discriminative\nclassifier to simultaneously address the extreme data-ambiguity and\noverfitting-vulnerability issues in tasks such as AIA. Particularly: (1) an\nExponential-Multinomial Mixture (EMM) model is established to capture both the\ninput and output ambiguity and in the meanwhile to encourage prediction\nsparsity; and (2) the prediction ability of the EMM model is explicitly\nmaximized through discriminative learning that integrates variational inference\nof graphical models and the pairwise formulation of ordinal regression.\nExperiments show that our approach achieves both superior annotation\nperformance and better tag scalability.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Yang", "Shuang Hong", ""], ["Bian", "Jiang", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1203.3532", "submitter": "Bai Zhang", "authors": "Bai Zhang, Yue Wang", "title": "Learning Structural Changes of Gaussian Graphical Models in Controlled\n  Experiments", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-701-708", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are widely used in scienti fic and engineering research to\nrepresent conditional independence structures between random variables. In many\ncontrolled experiments, environmental changes or external stimuli can often\nalter the conditional dependence between the random variables, and potentially\nproduce significant structural changes in the corresponding graphical models.\nTherefore, it is of great importance to be able to detect such structural\nchanges from data, so as to gain novel insights into where and how the\nstructural changes take place and help the system adapt to the new environment.\nHere we report an effective learning strategy to extract structural changes in\nGaussian graphical model using l1-regularization based convex optimization. We\ndiscuss the properties of the problem formulation and introduce an efficient\nimplementation by the block coordinate descent algorithm. We demonstrate the\nprinciple of the approach on a numerical simulation experiment, and we then\napply the algorithm to the modeling of gene regulatory networks under different\nconditions and obtain promising yet biologically plausible results.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Zhang", "Bai", ""], ["Wang", "Yue", ""]]}, {"id": "1203.3533", "submitter": "Kun Zhang", "authors": "Kun Zhang, Aapo Hyvarinen", "title": "Source Separation and Higher-Order Causal Analysis of MEG and EEG", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-709-716", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Separation of the sources and analysis of their connectivity have been an\nimportant topic in EEG/MEG analysis. To solve this problem in an automatic\nmanner, we propose a two-layer model, in which the sources are conditionally\nuncorrelated from each other, but not independent; the dependence is caused by\nthe causality in their time-varying variances (envelopes). The model is\nidentified in two steps. We first propose a new source separation technique\nwhich takes into account the autocorrelations (which may be time-varying) and\ntime-varying variances of the sources. The causality in the envelopes is then\ndiscovered by exploiting a special kind of multivariate GARCH (generalized\nautoregressive conditional heteroscedasticity) model. The resulting causal\ndiagram gives the effective connectivity between the separated sources; in our\nexperimental results on MEG data, sources with similar functions are grouped\ntogether, with negative influences between groups, and the groups are connected\nvia some interesting sources.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Zhang", "Kun", ""], ["Hyvarinen", "Aapo", ""]]}, {"id": "1203.3534", "submitter": "Kun Zhang", "authors": "Kun Zhang, Bernhard Schoelkopf, Dominik Janzing", "title": "Invariant Gaussian Process Latent Variable Models and Application in\n  Causal Discovery", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-717-724", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In nonlinear latent variable models or dynamic models, if we consider the\nlatent variables as confounders (common causes), the noise dependencies imply\nfurther relations between the observed variables. Such models are then closely\nrelated to causal discovery in the presence of nonlinear confounders, which is\na challenging problem. However, generally in such models the observation noise\nis assumed to be independent across data dimensions, and consequently the noise\ndependencies are ignored. In this paper we focus on the Gaussian process latent\nvariable model (GPLVM), from which we develop an extended model called\ninvariant GPLVM (IGPLVM), which can adapt to arbitrary noise covariances. With\nthe Gaussian process prior put on a particular transformation of the latent\nnonlinear functions, instead of the original ones, the algorithm for IGPLVM\ninvolves almost the same computational loads as that for the original GPLVM.\nBesides its potential application in causal discovery, IGPLVM has the advantage\nthat its estimated latent nonlinear manifold is invariant to any nonsingular\nlinear transformation of the data. Experimental results on both synthetic and\nrealworld data show its encouraging performance in nonlinear manifold learning\nand causal discovery.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Zhang", "Kun", ""], ["Schoelkopf", "Bernhard", ""], ["Janzing", "Dominik", ""]]}, {"id": "1203.3536", "submitter": "Yu Zhang", "authors": "Yu Zhang, Dit-Yan Yeung", "title": "A Convex Formulation for Learning Task Relationships in Multi-Task\n  Learning", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-733-742", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning is a learning paradigm which seeks to improve the\ngeneralization performance of a learning task with the help of some other\nrelated tasks. In this paper, we propose a regularization formulation for\nlearning the relationships between tasks in multi-task learning. This\nformulation can be viewed as a novel generalization of the regularization\nframework for single-task learning. Besides modeling positive task correlation,\nour method, called multi-task relationship learning (MTRL), can also describe\nnegative task correlation and identify outlier tasks based on the same\nunderlying principle. Under this regularization framework, the objective\nfunction of MTRL is convex. For efficiency, we use an alternating method to\nlearn the optimal model parameters for each task as well as the relationships\nbetween tasks. We study MTRL in the symmetric multi-task learning setting and\nthen generalize it to the asymmetric setting as well. We also study the\nrelationships between MTRL and some existing multi-task learning methods.\nExperiments conducted on a toy problem as well as several benchmark data sets\ndemonstrate the effectiveness of MTRL.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Zhang", "Yu", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1203.3537", "submitter": "Qian Zhu", "authors": "Qian Zhu, Branislav Kveton, Lily Mummert, Padmanabhan Pillai", "title": "Automatic Tuning of Interactive Perception Applications", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-743-751", "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive applications incorporating high-data rate sensing and computer\nvision are becoming possible due to novel runtime systems and the use of\nparallel computation resources. To allow interactive use, such applications\nrequire careful tuning of multiple application parameters to meet required\nfidelity and latency bounds. This is a nontrivial task, often requiring expert\nknowledge, which becomes intractable as resources and application load\ncharacteristics change. This paper describes a method for automatic performance\ntuning that learns application characteristics and effects of tunable\nparameters online, and constructs models that are used to maximize fidelity for\na given latency constraint. The paper shows that accurate latency models can be\nlearned online, knowledge of application structure can be used to reduce the\ncomplexity of the learning task, and operating points can be found that achieve\n90% of the optimal fidelity by exploring the parameter space only 3% of the\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Zhu", "Qian", ""], ["Kveton", "Branislav", ""], ["Mummert", "Lily", ""], ["Pillai", "Padmanabhan", ""]]}, {"id": "1203.3783", "submitter": "Gr\\'egoire Montavon", "authors": "Gr\\'egoire Montavon and Klaus-Robert M\\\"uller", "title": "Learning Feature Hierarchies with Centered Deep Boltzmann Machines", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-35289-8_33", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Boltzmann machines are in principle powerful models for extracting the\nhierarchical structure of data. Unfortunately, attempts to train layers jointly\n(without greedy layer-wise pretraining) have been largely unsuccessful. We\npropose a modification of the learning algorithm that initially recenters the\noutput of the activation functions to zero. This modification leads to a better\nconditioned Hessian and thus makes learning easier. We test the algorithm on\nreal data and demonstrate that our suggestion, the centered deep Boltzmann\nmachine, learns a hierarchy of increasingly abstract representations and a\nbetter generative model of data.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2012 19:01:10 GMT"}], "update_date": "2012-12-19", "authors_parsed": [["Montavon", "Gr\u00e9goire", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1203.3832", "submitter": "Saurabh  Pal", "authors": "Surjeet Kumar Yadav and Saurabh Pal", "title": "Data Mining: A Prediction for Performance Improvement of Engineering\n  Students using Classification", "comments": "6 pages, 3 Figures. arXiv admin note: substantial text overlap with\n  arXiv:1202.4815", "journal-ref": "World of Computer Science and Information Technology Journal\n  WCSIT, Vol. 2, No. 2, 51-56, 2012", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Now-a-days the amount of data stored in educational database increasing\nrapidly. These databases contain hidden information for improvement of\nstudents' performance. Educational data mining is used to study the data\navailable in the educational field and bring out the hidden knowledge from it.\nClassification methods like decision trees, Bayesian network etc can be applied\non the educational data for predicting the student's performance in\nexamination. This prediction will help to identify the weak students and help\nthem to score better marks. The C4.5, ID3 and CART decision tree algorithms are\napplied on engineering student's data to predict their performance in the final\nexam. The outcome of the decision tree predicted the number of students who are\nlikely to pass, fail or promoted to next year. The results provide steps to\nimprove the performance of the students who were predicted to fail or promoted.\nAfter the declaration of the results in the final examination the marks\nobtained by the students are fed into the system and the results were analyzed\nfor the next session. The comparative analysis of the results states that the\nprediction has helped the weaker students to improve and brought out betterment\nin the result.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2012 02:06:41 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Yadav", "Surjeet Kumar", ""], ["Pal", "Saurabh", ""]]}, {"id": "1203.3887", "submitter": "Animashree Anandkumar", "authors": "Animashree Anandkumar, Ragupathyraj Valluvan", "title": "Learning loopy graphical models with latent variables: Efficient methods\n  and guarantees", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1070 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 2, 401-435", "doi": "10.1214/12-AOS1070", "report-no": "IMS-AOS-AOS1070", "categories": "stat.ML cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of structure estimation in graphical models with latent variables\nis considered. We characterize conditions for tractable graph estimation and\ndevelop efficient methods with provable guarantees. We consider models where\nthe underlying Markov graph is locally tree-like, and the model is in the\nregime of correlation decay. For the special case of the Ising model, the\nnumber of samples $n$ required for structural consistency of our method scales\nas $n=\\Omega(\\theta_{\\min}^{-\\delta\\eta(\\eta+1)-2}\\log p)$, where p is the\nnumber of variables, $\\theta_{\\min}$ is the minimum edge potential, $\\delta$ is\nthe depth (i.e., distance from a hidden node to the nearest observed nodes),\nand $\\eta$ is a parameter which depends on the bounds on node and edge\npotentials in the Ising model. Necessary conditions for structural consistency\nunder any algorithm are derived and our method nearly matches the lower bound\non sample requirements. Further, the proposed method is practical to implement\nand provides flexibility to control the number of latent variables and the\ncycle lengths in the output graph.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2012 19:09:41 GMT"}, {"version": "v2", "created": "Sat, 30 Jun 2012 19:42:33 GMT"}, {"version": "v3", "created": "Mon, 16 Jul 2012 18:32:38 GMT"}, {"version": "v4", "created": "Mon, 22 Apr 2013 13:43:39 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Anandkumar", "Animashree", ""], ["Valluvan", "Ragupathyraj", ""]]}, {"id": "1203.3935", "submitter": "Hussein Saad", "authors": "Hussein Saad, Amr Mohamed and Tamer ElBatt", "title": "Distributed Cooperative Q-learning for Power Allocation in Cognitive\n  Femtocell Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a distributed reinforcement learning (RL) technique\ncalled distributed power control using Q-learning (DPC-Q) to manage the\ninterference caused by the femtocells on macro-users in the downlink. The DPC-Q\nleverages Q-Learning to identify the sub-optimal pattern of power allocation,\nwhich strives to maximize femtocell capacity, while guaranteeing macrocell\ncapacity level in an underlay cognitive setting. We propose two different\napproaches for the DPC-Q algorithm: namely, independent, and cooperative. In\nthe former, femtocells learn independently from each other while in the latter,\nfemtocells share some information during learning in order to enhance their\nperformance. Simulation results show that the independent approach is capable\nof mitigating the interference generated by the femtocells on macro-users.\nMoreover, the results show that cooperation enhances the performance of the\nfemtocells in terms of speed of convergence, fairness and aggregate femtocell\ncapacity.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2012 10:30:54 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Saad", "Hussein", ""], ["Mohamed", "Amr", ""], ["ElBatt", "Tamer", ""]]}, {"id": "1203.4416", "submitter": "Guillaume Desjardins", "authors": "Guillaume Desjardins and Aaron Courville and Yoshua Bengio", "title": "On Training Deep Boltzmann Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep Boltzmann machine (DBM) has been an important development in the\nquest for powerful \"deep\" probabilistic models. To date, simultaneous or joint\ntraining of all layers of the DBM has been largely unsuccessful with existing\ntraining methods. We introduce a simple regularization scheme that encourages\nthe weight vectors associated with each hidden unit to have similar norms. We\ndemonstrate that this regularization can be easily combined with standard\nstochastic maximum likelihood to yield an effective training strategy for the\nsimultaneous training of all layers of the deep Boltzmann machine.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 12:59:15 GMT"}], "update_date": "2012-03-21", "authors_parsed": [["Desjardins", "Guillaume", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1203.4422", "submitter": "Tomer Michaeli", "authors": "Tomer Michaeli, Yonina C. Eldar, Guillermo Sapiro", "title": "Semi-Supervised Single- and Multi-Domain Regression with Multi-Domain\n  Training", "comments": "24 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problems of multi-domain and single-domain regression based on\ndistinct and unpaired labeled training sets for each of the domains and a large\nunlabeled training set from all domains. We formulate these problems as a\nBayesian estimation with partial knowledge of statistical relations. We propose\na worst-case design strategy and study the resulting estimators. Our analysis\nexplicitly accounts for the cardinality of the labeled sets and includes the\nspecial cases in which one of the labeled sets is very large or, in the other\nextreme, completely missing. We demonstrate our estimators in the context of\nremoving expressions from facial images and in the context of audio-visual word\nrecognition, and provide comparisons to several recently proposed multi-modal\nlearning algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 13:11:32 GMT"}], "update_date": "2012-03-21", "authors_parsed": [["Michaeli", "Tomer", ""], ["Eldar", "Yonina C.", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1203.4523", "submitter": "Francis Bach", "authors": "Francis Bach (INRIA Paris - Rocquencourt, LIENS), Simon Lacoste-Julien\n  (INRIA Paris - Rocquencourt, LIENS), Guillaume Obozinski (INRIA Paris -\n  Rocquencourt, LIENS)", "title": "On the Equivalence between Herding and Conditional Gradient Algorithms", "comments": null, "journal-ref": "ICML 2012 International Conference on Machine Learning, Edimburgh\n  : Royaume-Uni (2012)", "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the herding procedure of Welling (2009) takes exactly the form\nof a standard convex optimization algorithm--namely a conditional gradient\nalgorithm minimizing a quadratic moment discrepancy. This link enables us to\ninvoke convergence results from convex optimization and to consider faster\nalternatives for the task of approximating integrals in a reproducing kernel\nHilbert space. We study the behavior of the different variants through\nnumerical simulations. The experiments indicate that while we can improve over\nherding on the task of approximating integrals, the original herding algorithm\ntends to approach more often the maximum entropy distribution, shedding more\nlight on the learning bias behind herding.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 17:49:56 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2012 08:35:39 GMT"}], "update_date": "2012-09-12", "authors_parsed": [["Bach", "Francis", "", "INRIA Paris - Rocquencourt, LIENS"], ["Lacoste-Julien", "Simon", "", "INRIA Paris - Rocquencourt, LIENS"], ["Obozinski", "Guillaume", "", "INRIA Paris -\n  Rocquencourt, LIENS"]]}, {"id": "1203.4597", "submitter": "Suleyman Kozat Dr.", "authors": "Huseyin Ozkan, Arda Akman, Suleyman S. Kozat", "title": "A Novel Training Algorithm for HMMs with Partial and Noisy Access to the\n  States", "comments": "Submitted to Digital Signal Processing, Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new estimation algorithm for the parameters of an HMM\nas to best account for the observed data. In this model, in addition to the\nobservation sequence, we have \\emph{partial} and \\emph{noisy} access to the\nhidden state sequence as side information. This access can be seen as \"partial\nlabeling\" of the hidden states. Furthermore, we model possible mislabeling in\nthe side information in a joint framework and derive the corresponding EM\nupdates accordingly. In our simulations, we observe that using this side\ninformation, we considerably improve the state recognition performance, up to\n70%, with respect to the \"achievable margin\" defined by the baseline\nalgorithms. Moreover, our algorithm is shown to be robust to the training\nconditions.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 21:31:48 GMT"}], "update_date": "2012-03-22", "authors_parsed": [["Ozkan", "Huseyin", ""], ["Akman", "Arda", ""], ["Kozat", "Suleyman S.", ""]]}, {"id": "1203.4598", "submitter": "Suleyman Kozat Dr.", "authors": "Mehmet A. Donmez, Huseyin A. Inan, Suleyman S. Kozat", "title": "Adaptive Mixture Methods Based on Bregman Divergences", "comments": "Submitted to Digital Signal Processing, Elsevier; IEEE.org", "journal-ref": null, "doi": "10.1016/j.dsp.2012.09.006", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate adaptive mixture methods that linearly combine outputs of $m$\nconstituent filters running in parallel to model a desired signal. We use\n\"Bregman divergences\" and obtain certain multiplicative updates to train the\nlinear combination weights under an affine constraint or without any\nconstraints. We use unnormalized relative entropy and relative entropy to\ndefine two different Bregman divergences that produce an unnormalized\nexponentiated gradient update and a normalized exponentiated gradient update on\nthe mixture weights, respectively. We then carry out the mean and the\nmean-square transient analysis of these adaptive algorithms when they are used\nto combine outputs of $m$ constituent filters. We illustrate the accuracy of\nour results and demonstrate the effectiveness of these updates for sparse\nmixture systems.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 21:32:33 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Donmez", "Mehmet A.", ""], ["Inan", "Huseyin A.", ""], ["Kozat", "Suleyman S.", ""]]}, {"id": "1203.4788", "submitter": "Altay Brusan", "authors": "Altay Brusan", "title": "Very Short Literature Survey From Supervised Learning To Surrogate\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The past century was era of linear systems. Either systems (especially\nindustrial ones) were simple (quasi)linear or linear approximations were\naccurate enough. In addition, just at the ending decades of the century\nprofusion of computing devices were available, before then due to lack of\ncomputational resources it was not easy to evaluate available nonlinear system\nstudies. At the moment both these two conditions changed, systems are highly\ncomplex and also pervasive amount of computation strength is cheap and easy to\nachieve. For recent era, a new branch of supervised learning well known as\nsurrogate modeling (meta-modeling, surface modeling) has been devised which\naimed at answering new needs of modeling realm. This short literature survey is\non to introduce surrogate modeling to whom is familiar with the concepts of\nsupervised learning. Necessity, challenges and visions of the topic are\nconsidered.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2012 17:29:17 GMT"}], "update_date": "2012-03-22", "authors_parsed": [["Brusan", "Altay", ""]]}, {"id": "1203.5124", "submitter": "Liang Zhang", "authors": "Rajiv Khanna, Liang Zhang, Deepak Agarwal, Beechung Chen", "title": "Parallel Matrix Factorization for Binary Response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting user affinity to items is an important problem in applications\nlike content optimization, computational advertising, and many more. While\nbilinear random effect models (matrix factorization) provide state-of-the-art\nperformance when minimizing RMSE through a Gaussian response model on explicit\nratings data, applying it to imbalanced binary response data presents\nadditional challenges that we carefully study in this paper. Data in many\napplications usually consist of users' implicit response that are often binary\n-- clicking an item or not; the goal is to predict click rates, which is often\ncombined with other measures to calculate utilities to rank items at runtime of\nthe recommender systems. Because of the implicit nature, such data are usually\nmuch larger than explicit rating data and often have an imbalanced distribution\nwith a small fraction of click events, making accurate click rate prediction\ndifficult. In this paper, we address two problems. First, we show previous\ntechniques to estimate bilinear random effect models with binary data are less\naccurate compared to our new approach based on adaptive rejection sampling,\nespecially for imbalanced response. Second, we develop a parallel bilinear\nrandom effect model fitting framework using Map-Reduce paradigm that scales to\nmassive datasets. Our parallel algorithm is based on a \"divide and conquer\"\nstrategy coupled with an ensemble approach. Through experiments on the\nbenchmark MovieLens data, a small Yahoo! Front Page data set, and a large\nYahoo! Front Page data set that contains 8M users and 1B binary observations,\nwe show that careful handling of binary response as well as identifiability\nissues are needed to achieve good performance for click rate prediction, and\nthat the proposed adaptive rejection sampler and the partitioning as well as\nensemble techniques significantly improve model performance.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 20:54:53 GMT"}], "update_date": "2012-03-26", "authors_parsed": [["Khanna", "Rajiv", ""], ["Zhang", "Liang", ""], ["Agarwal", "Deepak", ""], ["Chen", "Beechung", ""]]}, {"id": "1203.5181", "submitter": "Frank Nielsen", "authors": "Frank Nielsen", "title": "$k$-MLE: A fast algorithm for learning statistical mixture models", "comments": "31 pages, Extend preliminary paper presented at IEEE ICASSP 2012", "journal-ref": null, "doi": "10.1109/ICASSP.2012.6288022", "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe $k$-MLE, a fast and efficient local search algorithm for learning\nfinite statistical mixtures of exponential families such as Gaussian mixture\nmodels. Mixture models are traditionally learned using the\nexpectation-maximization (EM) soft clustering technique that monotonically\nincreases the incomplete (expected complete) likelihood. Given prescribed\nmixture weights, the hard clustering $k$-MLE algorithm iteratively assigns data\nto the most likely weighted component and update the component models using\nMaximum Likelihood Estimators (MLEs). Using the duality between exponential\nfamilies and Bregman divergences, we prove that the local convergence of the\ncomplete likelihood of $k$-MLE follows directly from the convergence of a dual\nadditively weighted Bregman hard clustering. The inner loop of $k$-MLE can be\nimplemented using any $k$-means heuristic like the celebrated Lloyd's batched\nor Hartigan's greedy swap updates. We then show how to update the mixture\nweights by minimizing a cross-entropy criterion that implies to update weights\nby taking the relative proportion of cluster points, and reiterate the mixture\nparameter update and mixture weight update processes until convergence. Hard EM\nis interpreted as a special case of $k$-MLE when both the component update and\nthe weight update are performed successively in the inner loop. To initialize\n$k$-MLE, we propose $k$-MLE++, a careful initialization of $k$-MLE guaranteeing\nprobabilistically a global bound on the best possible complete likelihood.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2012 06:11:24 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Nielsen", "Frank", ""]]}, {"id": "1203.5422", "submitter": "Jing Lei", "authors": "Jing Lei and Larry Wasserman", "title": "Distribution Free Prediction Bands", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distribution free, nonparametric prediction bands with a special\nfocus on their finite sample behavior. First we investigate and develop\ndifferent notions of finite sample coverage guarantees. Then we give a new\nprediction band estimator by combining the idea of \"conformal prediction\" (Vovk\net al. 2009) with nonparametric conditional density estimation. The proposed\nestimator, called COPS (Conformal Optimized Prediction Set), always has finite\nsample guarantee in a stronger sense than the original conformal prediction\nestimator. Under regularity conditions the estimator converges to an oracle\nband at a minimax optimal rate. A fast approximation algorithm and a data\ndriven method for selecting the bandwidth are developed. The method is\nillustrated first in simulated data. Then, an application shows that the\nproposed method gives desirable prediction intervals in an automatic way, as\ncompared to the classical linear regression modeling.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2012 15:04:02 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["Lei", "Jing", ""], ["Wasserman", "Larry", ""]]}, {"id": "1203.5438", "submitter": "Emile Richard", "authors": "Emile Richard, Andreas Argyriou, Theodoros Evgeniou and Nicolas\n  Vayatis", "title": "A Regularization Approach for Prediction of Edges and Node Features in\n  Dynamic Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the two problems of predicting links in a dynamic graph sequence\nand predicting functions defined at each node of the graph. In many\napplications, the solution of one problem is useful for solving the other.\nIndeed, if these functions reflect node features, then they are related through\nthe graph structure. In this paper, we formulate a hybrid approach that\nsimultaneously learns the structure of the graph and predicts the values of the\nnode-related functions. Our approach is based on the optimization of a joint\nregularization objective. We empirically test the benefits of the proposed\nmethod with both synthetic and real data. The results indicate that joint\nregularization improves prediction performance over the graph evolution and the\nnode features.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2012 18:59:55 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["Richard", "Emile", ""], ["Argyriou", "Andreas", ""], ["Evgeniou", "Theodoros", ""], ["Vayatis", "Nicolas", ""]]}, {"id": "1203.5443", "submitter": "Martin Pelikan", "authors": "Martin Pelikan, Mark W. Hauschild, and Pier Luca Lanzi", "title": "Transfer Learning, Soft Distance-Based Bias, and the Hierarchical BOA", "comments": "Accepted at Parallel Problem Solving from Nature (PPSN XII), 10\n  pages. arXiv admin note: substantial text overlap with arXiv:1201.2241", "journal-ref": null, "doi": null, "report-no": "MEDAL Report No. 2012004", "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An automated technique has recently been proposed to transfer learning in the\nhierarchical Bayesian optimization algorithm (hBOA) based on distance-based\nstatistics. The technique enables practitioners to improve hBOA efficiency by\ncollecting statistics from probabilistic models obtained in previous hBOA runs\nand using the obtained statistics to bias future hBOA runs on similar problems.\nThe purpose of this paper is threefold: (1) test the technique on several\nclasses of NP-complete problems, including MAXSAT, spin glasses and minimum\nvertex cover; (2) demonstrate that the technique is effective even when\nprevious runs were done on problems of different size; (3) provide empirical\nevidence that combining transfer learning with other efficiency enhancement\ntechniques can often yield nearly multiplicative speedups.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2012 20:11:21 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2012 12:47:30 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Pelikan", "Martin", ""], ["Hauschild", "Mark W.", ""], ["Lanzi", "Pier Luca", ""]]}, {"id": "1203.5446", "submitter": "Cyril Voyant", "authors": "Philippe Lauret (PIMENT), Auline Rodler (SPE), Marc Muselli (SPE),\n  Mathieu David (PIMENT), Hadja Diagne (PIMENT), Cyril Voyant (SPE, CHD\n  Castellucio)", "title": "A Bayesian Model Committee Approach to Forecasting Global Solar\n  Radiation", "comments": "WREF 2012 : World Renewable Energy Forum, Denver : United States\n  (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes to use a rather new modelling approach in the realm of\nsolar radiation forecasting. In this work, two forecasting models:\nAutoregressive Moving Average (ARMA) and Neural Network (NN) models are\ncombined to form a model committee. The Bayesian inference is used to affect a\nprobability to each model in the committee. Hence, each model's predictions are\nweighted by their respective probability. The models are fitted to one year of\nhourly Global Horizontal Irradiance (GHI) measurements. Another year (the test\nset) is used for making genuine one hour ahead (h+1) out-of-sample forecast\ncomparisons. The proposed approach is benchmarked against the persistence\nmodel. The very first results show an improvement brought by this approach.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2012 20:58:48 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["Lauret", "Philippe", "", "PIMENT"], ["Rodler", "Auline", "", "SPE"], ["Muselli", "Marc", "", "SPE"], ["David", "Mathieu", "", "PIMENT"], ["Diagne", "Hadja", "", "PIMENT"], ["Voyant", "Cyril", "", "SPE, CHD\n  Castellucio"]]}, {"id": "1203.5716", "submitter": "Giorgio Corani", "authors": "Giorgio Corani and Alessandro Antonucci", "title": "Credal Classification based on AODE and compression coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model averaging (BMA) is an approach to average over alternative\nmodels; yet, it usually gets excessively concentrated around the single most\nprobable model, therefore achieving only sub-optimal classification\nperformance. The compression-based approach (Boulle, 2007) overcomes this\nproblem, averaging over the different models by applying a logarithmic\nsmoothing over the models' posterior probabilities. This approach has shown\nexcellent performances when applied to ensembles of naive Bayes classifiers.\nAODE is another ensemble of models with high performance (Webb, 2005), based on\na collection of non-naive classifiers (called SPODE) whose probabilistic\npredictions are aggregated by simple arithmetic mean. Aggregating the SPODEs\nvia BMA rather than by arithmetic mean deteriorates the performance; instead,\nwe aggregate the SPODEs via the compression coefficients and we show that the\nresulting classifier obtains a slight but consistent improvement over AODE.\nHowever, an important issue in any Bayesian ensemble of models is the\narbitrariness in the choice of the prior over the models. We address this\nproblem by the paradigm of credal classification, namely by substituting the\nunique prior with a set of priors. Credal classifier automatically recognize\nthe prior-dependent instances, namely the instances whose most probable class\nvaries, when different priors are considered; in these cases, credal\nclassifiers remain reliable by returning a set of classes rather than a single\nclass. We thus develop the credal version of both the BMA-based and the\ncompression-based ensemble of SPODEs, substituting the single prior over the\nmodels by a set of priors. Experiments show that both credal classifiers\nprovide higher classification reliability than their determinate counterparts;\nmoreover the compression-based credal classifier compares favorably to previous\ncredal classifiers.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2012 16:25:35 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2012 10:27:30 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Corani", "Giorgio", ""], ["Antonucci", "Alessandro", ""]]}, {"id": "1203.6130", "submitter": "Jordan Rodu", "authors": "Dean P. Foster, Jordan Rodu, Lyle H. Ungar", "title": "Spectral dimensionality reduction for HMMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov Models (HMMs) can be accurately approximated using\nco-occurrence frequencies of pairs and triples of observations by using a fast\nspectral method in contrast to the usual slow methods like EM or Gibbs\nsampling. We provide a new spectral method which significantly reduces the\nnumber of model parameters that need to be estimated, and generates a sample\ncomplexity that does not depend on the size of the observation vocabulary. We\npresent an elementary proof giving bounds on the relative accuracy of\nprobability estimates from our model. (Correlaries show our bounds can be\nweakened to provide either L1 bounds or KL bounds which provide easier direct\ncomparisons to previous work.) Our theorem uses conditions that are checkable\nfrom the data, instead of putting conditions on the unobservable Markov\ntransition matrix.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2012 01:56:32 GMT"}], "update_date": "2012-03-29", "authors_parsed": [["Foster", "Dean P.", ""], ["Rodu", "Jordan", ""], ["Ungar", "Lyle H.", ""]]}, {"id": "1203.6178", "submitter": "Ayaka Sakata", "authors": "Ayaka Sakata and Yoshiyuki Kabashima", "title": "Statistical Mechanics of Dictionary Learning", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": "10.1209/0295-5075/103/28008", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a basis matrix (dictionary) by which objective signals are\nrepresented sparsely is of major relevance in various scientific and\ntechnological fields. We consider a problem to learn a dictionary from a set of\ntraining signals. We employ techniques of statistical mechanics of disordered\nsystems to evaluate the size of the training set necessary to typically succeed\nin the dictionary learning. The results indicate that the necessary size is\nmuch smaller than previously estimated, which theoretically supports and/or\nencourages the use of dictionary learning in practical situations.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2012 07:01:29 GMT"}, {"version": "v2", "created": "Sun, 1 Apr 2012 02:56:07 GMT"}, {"version": "v3", "created": "Fri, 25 Jan 2013 13:01:29 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Sakata", "Ayaka", ""], ["Kabashima", "Yoshiyuki", ""]]}]