[{"id": "0712.0130", "submitter": "Thomas M. Breuel", "authors": "Thomas M. Breuel", "title": "On the Relationship between the Posterior and Optimal Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": null, "abstract": "  For a classification problem described by the joint density $P(\\omega,x)$,\nmodels of $P(\\omega\\eq\\omega'|x,x')$ (the ``Bayesian similarity measure'') have\nbeen shown to be an optimal similarity measure for nearest neighbor\nclassification. This paper analyzes demonstrates several additional properties\nof that conditional distribution. The paper first shows that we can\nreconstruct, up to class labels, the class posterior distribution $P(\\omega|x)$\ngiven $P(\\omega\\eq\\omega'|x,x')$, gives a procedure for recovering the class\nlabels, and gives an asymptotically Bayes-optimal classification procedure. It\nalso shows, given such an optimal similarity measure, how to construct a\nclassifier that outperforms the nearest neighbor classifier and achieves\nBayes-optimal classification rates. The paper then analyzes Bayesian similarity\nin a framework where a classifier faces a number of related classification\ntasks (multitask learning) and illustrates that reconstruction of the class\nposterior distribution is not possible in general. Finally, the paper\nidentifies a distinct class of classification problems using\n$P(\\omega\\eq\\omega'|x,x')$ and shows that using $P(\\omega\\eq\\omega'|x,x')$ to\nsolve those problems is the Bayes optimal solution.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2007 09:38:26 GMT"}], "update_date": "2007-12-04", "authors_parsed": [["Breuel", "Thomas M.", ""]]}, {"id": "0712.0451", "submitter": "Alejandro Chinea Manrique De Lara", "authors": "Alejandro Chinea Manrique De Lara", "title": "A Reactive Tabu Search Algorithm for Stimuli Generation in\n  Psycholinguistics", "comments": "Artificial Intelligence in Science and Technology AISAT 2004\n  Conference. 8 pages, 5 figures, 3 tables", "journal-ref": "Artificial Intelligence in Science and Technology AISAT 2004\n  Conference", "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.DM cs.LG", "license": null, "abstract": "  The generation of meaningless \"words\" matching certain statistical and/or\nlinguistic criteria is frequently needed for experimental purposes in\nPsycholinguistics. Such stimuli receive the name of pseudowords or nonwords in\nthe Cognitive Neuroscience literatue. The process for building nonwords\nsometimes has to be based on linguistic units such as syllables or morphemes,\nresulting in a numerical explosion of combinations when the size of the\nnonwords is increased. In this paper, a reactive tabu search scheme is proposed\nto generate nonwords of variables size. The approach builds pseudowords by\nusing a modified Metaheuristic algorithm based on a local search procedure\nenhanced by a feedback-based scheme. Experimental results show that the new\nalgorithm is a practical and effective tool for nonword generation.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2007 08:52:46 GMT"}], "update_date": "2007-12-05", "authors_parsed": [["De Lara", "Alejandro Chinea Manrique", ""]]}, {"id": "0712.0653", "submitter": "Sumio Watanabe", "authors": "Sumio Watanabe", "title": "Equations of States in Singular Statistical Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning machines which have hierarchical structures or hidden variables are\nsingular statistical models because they are nonidentifiable and their Fisher\ninformation matrices are singular. In singular statistical models, neither the\nBayes a posteriori distribution converges to the normal distribution nor the\nmaximum likelihood estimator satisfies asymptotic normality. This is the main\nreason why it has been difficult to predict their generalization performances\nfrom trained states. In this paper, we study four errors, (1) Bayes\ngeneralization error, (2) Bayes training error, (3) Gibbs generalization error,\nand (4) Gibbs training error, and prove that there are mathematical relations\namong these errors. The formulas proved in this paper are equations of states\nin statistical estimation because they hold for any true distribution, any\nparametric model, and any a priori distribution. Also we show that Bayes and\nGibbs generalization errors are estimated by Bayes and Gibbs training errors,\nand propose widely applicable information criteria which can be applied to both\nregular and singular statistical models.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2007 05:39:07 GMT"}, {"version": "v2", "created": "Mon, 11 May 2009 05:49:09 GMT"}], "update_date": "2009-05-11", "authors_parsed": [["Watanabe", "Sumio", ""]]}, {"id": "0712.0840", "submitter": "Leonid (Aryeh) Kontorovich", "authors": "Leonid (Aryeh) Kontorovich", "title": "A Universal Kernel for Learning Regular Languages", "comments": "7 pages", "journal-ref": "The 5th International Workshop on Mining and Learning with Graphs,\n  2007", "doi": null, "report-no": null, "categories": "cs.LG cs.DM", "license": null, "abstract": "  We give a universal kernel that renders all the regular languages linearly\nseparable. We are not able to compute this kernel efficiently and conjecture\nthat it is intractable, but we do have an efficient $\\eps$-approximation.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2007 22:25:03 GMT"}], "update_date": "2007-12-07", "authors_parsed": [["Leonid", "", "", "Aryeh"], ["Kontorovich", "", ""]]}, {"id": "0712.0938", "submitter": "Kumar Eswaran Dr.", "authors": "Dasika Ratna Deepthi, G.R.Aditya Krishna and K. Eswaran", "title": "Automatic Pattern Classification by Unsupervised Learning Using\n  Dimensionality Reduction of Data with Mirroring Neural Networks", "comments": "Presented in IEEE International Conference on Advances in Computer\n  Vision and Information Technology (ACVIT-07), Nov. 28-30 2007", "journal-ref": "IEEE International Conference on Advances in Computer Vision and\n  Information Tech. (IEEE, ACVIT-07), pp. 354 - 360 (2007)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": null, "abstract": "  This paper proposes an unsupervised learning technique by using Multi-layer\nMirroring Neural Network and Forgy's clustering algorithm. Multi-layer\nMirroring Neural Network is a neural network that can be trained with\ngeneralized data inputs (different categories of image patterns) to perform\nnon-linear dimensionality reduction and the resultant low-dimensional code is\nused for unsupervised pattern classification using Forgy's algorithm. By\nadapting the non-linear activation function (modified sigmoidal function) and\ninitializing the weights and bias terms to small random values, mirroring of\nthe input pattern is initiated. In training, the weights and bias terms are\nchanged in such a way that the input presented is reproduced at the output by\nback propagating the error. The mirroring neural network is capable of reducing\nthe input vector to a great degree (approximately 1/30th the original size) and\nalso able to reconstruct the input pattern at the output layer from this\nreduced code units. The feature set (output of central hidden layer) extracted\nfrom this network is fed to Forgy's algorithm, which classify input data\npatterns into distinguishable classes. In the implementation of Forgy's\nalgorithm, initial seed points are selected in such a way that they are distant\nenough to be perfectly grouped into different categories. Thus a new method of\nunsupervised learning is formulated and demonstrated in this paper. This method\ngave impressive results when applied to classification of different image\npatterns.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2007 13:52:04 GMT"}], "update_date": "2008-12-15", "authors_parsed": [["Deepthi", "Dasika Ratna", ""], ["Krishna", "G. R. Aditya", ""], ["Eswaran", "K.", ""]]}, {"id": "0712.1402", "submitter": "Allan Sly", "authors": "Guy Bresler, Elchanan Mossel, Allan Sly", "title": "Reconstruction of Markov Random Fields from Samples: Some Easy\n  Observations and Algorithms", "comments": "14 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov random fields are used to model high dimensional distributions in a\nnumber of applied areas. Much recent interest has been devoted to the\nreconstruction of the dependency structure from independent samples from the\nMarkov random fields. We analyze a simple algorithm for reconstructing the\nunderlying graph defining a Markov random field on $n$ nodes and maximum degree\n$d$ given observations. We show that under mild non-degeneracy conditions it\nreconstructs the generating graph with high probability using $\\Theta(d\n\\epsilon^{-2}\\delta^{-4} \\log n)$ samples where $\\epsilon,\\delta$ depend on the\nlocal interactions. For most local interaction $\\eps,\\delta$ are of order\n$\\exp(-O(d))$.\n  Our results are optimal as a function of $n$ up to a multiplicative constant\ndepending on $d$ and the strength of the local interactions. Our results seem\nto be the first results for general models that guarantee that {\\em the}\ngenerating model is reconstructed. Furthermore, we provide explicit $O(n^{d+2}\n\\epsilon^{-2}\\delta^{-4} \\log n)$ running time bound. In cases where the\nmeasure on the graph has correlation decay, the running time is $O(n^2 \\log n)$\nfor all fixed $d$. We also discuss the effect of observing noisy samples and\nshow that as long as the noise level is low, our algorithm is effective. On the\nother hand, we construct an example where large noise implies\nnon-identifiability even for generic noise and interactions. Finally, we\nbriefly show that in some simple cases, models with hidden nodes can also be\nrecovered.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2007 06:50:36 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2010 19:30:26 GMT"}], "update_date": "2010-03-09", "authors_parsed": [["Bresler", "Guy", ""], ["Mossel", "Elchanan", ""], ["Sly", "Allan", ""]]}, {"id": "0712.2497", "submitter": "Fangwen Fu", "authors": "Fangwen Fu and Mihaela van der Schaar", "title": "A New Theoretic Foundation for Cross-Layer Optimization", "comments": "39 pages, 10 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": null, "abstract": "  Cross-layer optimization solutions have been proposed in recent years to\nimprove the performance of network users operating in a time-varying,\nerror-prone wireless environment. However, these solutions often rely on ad-hoc\noptimization approaches, which ignore the different environmental dynamics\nexperienced at various layers by a user and violate the layered network\narchitecture of the protocol stack by requiring layers to provide access to\ntheir internal protocol parameters to other layers. This paper presents a new\ntheoretic foundation for cross-layer optimization, which allows each layer to\nmake autonomous decisions individually, while maximizing the utility of the\nwireless user by optimally determining what information needs to be exchanged\namong layers. Hence, this cross-layer framework does not change the current\nlayered architecture. Specifically, because the wireless user interacts with\nthe environment at various layers of the protocol stack, the cross-layer\noptimization problem is formulated as a layered Markov decision process (MDP)\nin which each layer adapts its own protocol parameters and exchanges\ninformation (messages) with other layers in order to cooperatively maximize the\nperformance of the wireless user. The message exchange mechanism for\ndetermining the optimal cross-layer transmission strategies has been designed\nfor both off-line optimization and on-line dynamic adaptation. We also show\nthat many existing cross-layer optimization algorithms can be formulated as\nsimplified, sub-optimal, versions of our layered MDP framework.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2007 06:50:43 GMT"}], "update_date": "2007-12-18", "authors_parsed": [["Fu", "Fangwen", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "0712.2869", "submitter": "Daniel \\v{S}tefankovi\\v{c}", "authors": "Satyaki Mahalanabis, Daniel Stefankovic", "title": "Density estimation in linear time", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": null, "abstract": "  We consider the problem of choosing a density estimate from a set of\ndistributions F, minimizing the L1-distance to an unknown distribution\n(Devroye, Lugosi 2001). Devroye and Lugosi analyze two algorithms for the\nproblem: Scheffe tournament winner and minimum distance estimate. The Scheffe\ntournament estimate requires fewer computations than the minimum distance\nestimate, but has strictly weaker guarantees than the latter.\n  We focus on the computational aspect of density estimation. We present two\nalgorithms, both with the same guarantee as the minimum distance estimate. The\nfirst one, a modification of the minimum distance estimate, uses the same\nnumber (quadratic in |F|) of computations as the Scheffe tournament. The second\none, called ``efficient minimum loss-weight estimate,'' uses only a linear\nnumber of computations, assuming that F is preprocessed.\n  We also give examples showing that the guarantees of the algorithms cannot be\nimproved and explore randomized algorithms for density estimation.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2007 03:30:05 GMT"}], "update_date": "2007-12-19", "authors_parsed": [["Mahalanabis", "Satyaki", ""], ["Stefankovic", "Daniel", ""]]}, {"id": "0712.3402", "submitter": "Francis Bach", "authors": "Francis Bach (WILLOW Project - Inria/Ens)", "title": "Graph kernels between point clouds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": null, "abstract": "  Point clouds are sets of points in two or three dimensions. Most kernel\nmethods for learning on sets of points have not yet dealt with the specific\ngeometrical invariances and practical constraints associated with point clouds\nin computer vision and graphics. In this paper, we present extensions of graph\nkernels for point clouds, which allow to use kernel methods for such ob jects\nas shapes, line drawings, or any three-dimensional point clouds. In order to\ndesign rich and numerically efficient kernels with as few free parameters as\npossible, we use kernels between covariance matrices and their factorizations\non graphical models. We derive polynomial time dynamic programming recursions\nand present applications to recognition of handwritten digits and Chinese\ncharacters from few training examples.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2007 13:06:50 GMT"}], "update_date": "2007-12-21", "authors_parsed": [["Bach", "Francis", "", "WILLOW Project - Inria/Ens"]]}, {"id": "0712.3654", "submitter": "Alejandro Chinea Manrique De Lara", "authors": "Alejandro Chinea Manrique De Lara, Juan Manuel Moreno, Arostegui Jordi\n  Madrenas, Joan Cabestany", "title": "Improving the Performance of PieceWise Linear Separation Incremental\n  Algorithms for Practical Hardware Implementations", "comments": "10 pages, 1 figure, 3 tables", "journal-ref": "Biological and Artificial Computation: From Neuroscience to\n  Technology, J.Mira, R.Moreno-Diaz, J.Cabestany (eds.), pp. 607-616,\n  Springer-Verlag, 1997", "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": null, "abstract": "  In this paper we shall review the common problems associated with Piecewise\nLinear Separation incremental algorithms. This kind of neural models yield poor\nperformances when dealing with some classification problems, due to the\nevolving schemes used to construct the resulting networks. So as to avoid this\nundesirable behavior we shall propose a modification criterion. It is based\nupon the definition of a function which will provide information about the\nquality of the network growth process during the learning phase. This function\nis evaluated periodically as the network structure evolves, and will permit, as\nwe shall show through exhaustive benchmarks, to considerably improve the\nperformance(measured in terms of network complexity and generalization\ncapabilities) offered by the networks generated by these incremental models.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2007 10:05:52 GMT"}], "update_date": "2007-12-24", "authors_parsed": [["De Lara", "Alejandro Chinea Manrique", ""], ["Moreno", "Juan Manuel", ""], ["Madrenas", "Arostegui Jordi", ""], ["Cabestany", "Joan", ""]]}, {"id": "0712.3807", "submitter": "Jianguo Liu", "authors": "Jian-Guo Liu, Bing-Hong Wang, Qiang Guo", "title": "Improved Collaborative Filtering Algorithm via Information\n  Transformation", "comments": "5 pages, 4 figures", "journal-ref": "Int. J. Mod. Phys. C 20(2), 285-293 (2009)", "doi": "10.1142/S0129183109013613", "report-no": null, "categories": "cs.LG cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a spreading activation approach for collaborative\nfiltering (SA-CF). By using the opinion spreading process, the similarity\nbetween any users can be obtained. The algorithm has remarkably higher accuracy\nthan the standard collaborative filtering (CF) using Pearson correlation.\nFurthermore, we introduce a free parameter $\\beta$ to regulate the\ncontributions of objects to user-user correlations. The numerical results\nindicate that decreasing the influence of popular objects can further improve\nthe algorithmic accuracy and personality. We argue that a better algorithm\nshould simultaneously require less computation and generate higher accuracy.\nAccordingly, we further propose an algorithm involving only the top-$N$ similar\nneighbors for each target user, which has both less computational complexity\nand higher algorithmic accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2007 14:25:18 GMT"}, {"version": "v2", "created": "Fri, 12 Dec 2008 16:31:13 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2009 15:30:56 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Liu", "Jian-Guo", ""], ["Wang", "Bing-Hong", ""], ["Guo", "Qiang", ""]]}, {"id": "0712.4273", "submitter": "Olivier Cappe", "authors": "Olivier Capp\\'e (LTCI), Eric Moulines (LTCI)", "title": "Online EM Algorithm for Latent Data Models", "comments": "Version that includes the corrigendum published in volume 73, part 5\n  (2011), of the Journal of the Royal Statistical Society, Series B + the\n  correction of a typo in Eqs. (32-33)", "journal-ref": "Journal of the Royal Statistical Society: Series B, Royal\n  Statistical Society, 2009, 71 (3), pp.593-613", "doi": "10.1111/j.1467-9868.2009.00698.x", "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution, we propose a generic online (also sometimes called\nadaptive or recursive) version of the Expectation-Maximisation (EM) algorithm\napplicable to latent variable models of independent observations. Compared to\nthe algorithm of Titterington (1984), this approach is more directly connected\nto the usual EM algorithm and does not rely on integration with respect to the\ncomplete data distribution. The resulting algorithm is usually simpler and is\nshown to achieve convergence to the stationary points of the Kullback-Leibler\ndivergence between the marginal distribution of the observation and the model\ndistribution at the optimal rate, i.e., that of the maximum likelihood\nestimator. In addition, the proposed approach is also suitable for conditional\n(or regression) models, as illustrated in the case of the mixture of linear\nregressions model.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2007 19:44:34 GMT"}, {"version": "v2", "created": "Thu, 4 Sep 2008 14:36:55 GMT"}, {"version": "v3", "created": "Fri, 2 Dec 2011 14:59:41 GMT"}, {"version": "v4", "created": "Wed, 1 Mar 2017 13:40:32 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Capp\u00e9", "Olivier", "", "LTCI"], ["Moulines", "Eric", "", "LTCI"]]}]