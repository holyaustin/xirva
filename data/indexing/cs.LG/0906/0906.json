[{"id": "0906.0052", "submitter": "Brian Tomasik", "authors": "Brian Tomasik", "title": "A Minimum Description Length Approach to Multitask Feature Selection", "comments": "29 pages, 3 figures, undergraduate thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Many regression problems involve not one but several response variables\n(y's). Often the responses are suspected to share a common underlying\nstructure, in which case it may be advantageous to share information across\nthem; this is known as multitask learning. As a special case, we can use\nmultiple responses to better identify shared predictive features -- a project\nwe might call multitask feature selection.\n  This thesis is organized as follows. Section 1 introduces feature selection\nfor regression, focusing on ell_0 regularization methods and their\ninterpretation within a Minimum Description Length (MDL) framework. Section 2\nproposes a novel extension of MDL feature selection to the multitask setting.\nThe approach, called the \"Multiple Inclusion Criterion\" (MIC), is designed to\nborrow information across regression tasks by more easily selecting features\nthat are associated with multiple responses. We show in experiments on\nsynthetic and real biological data sets that MIC can reduce prediction error in\nsettings where features are at least partially shared across responses. Section\n3 surveys hypothesis testing by regression with a single response, focusing on\nthe parallel between the standard Bonferroni correction and an MDL approach.\nMirroring the ideas in Section 2, Section 4 proposes a novel MIC approach to\nhypothesis testing with multiple responses and shows that on synthetic data\nwith significant sharing of features across responses, MIC sometimes\noutperforms standard FDR-controlling methods in terms of finding true positives\nfor a given level of false positives. Section 5 concludes.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2009 03:41:37 GMT"}], "update_date": "2009-06-02", "authors_parsed": [["Tomasik", "Brian", ""]]}, {"id": "0906.0211", "submitter": "Sumio Watanabe", "authors": "Sumio Watanabe", "title": "Equations of States in Statistical Learning for a Nonparametrizable and\n  Regular Case", "comments": null, "journal-ref": null, "doi": "10.1587/transfun.E93.A.617", "report-no": null, "categories": "cs.LG", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Many learning machines that have hierarchical structure or hidden variables\nare now being used in information science, artificial intelligence, and\nbioinformatics. However, several learning machines used in such fields are not\nregular but singular statistical models, hence their generalization performance\nis still left unknown. To overcome these problems, in the previous papers, we\nproved new equations in statistical learning, by which we can estimate the\nBayes generalization loss from the Bayes training loss and the functional\nvariance, on the condition that the true distribution is a singularity\ncontained in a learning machine. In this paper, we prove that the same\nequations hold even if a true distribution is not contained in a parametric\nmodel. Also we prove that, the proposed equations in a regular case are\nasymptotically equivalent to the Takeuchi information criterion. Therefore, the\nproposed equations are always applicable without any condition on the unknown\ntrue distribution.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2009 04:47:15 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2009 03:25:56 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Watanabe", "Sumio", ""]]}, {"id": "0906.0470", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Juan-Manuel Torres-Moreno and Mirta B. Gordon", "title": "An optimal linear separator for the Sonar Signals Classification task", "comments": "8 pages, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of classifying sonar signals from rocks and mines first studied\nby Gorman and Sejnowski has become a benchmark against which many learning\nalgorithms have been tested. We show that both the training set and the test\nset of this benchmark are linearly separable, although with different\nhyperplanes. Moreover, the complete set of learning and test patterns together,\nis also linearly separable. We give the weights that separate these sets, which\nmay be used to compare results found by other algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2009 11:52:36 GMT"}], "update_date": "2009-06-03", "authors_parsed": [["Torres-Moreno", "Juan-Manuel", ""], ["Gordon", "Mirta B.", ""]]}, {"id": "0906.0861", "submitter": "Florentina Pintea", "authors": "A. A. Shumeyko, S. L. Sotnik", "title": "Using Genetic Algorithms for Texts Classification Problems", "comments": "16 pages, exposed on 5th International Conference \"Actualities and\n  Perspectives on Hardware and Software\" - APHS2009, Timisoara, Romania", "journal-ref": "Ann. Univ. Tibiscus Comp. Sci. Series VII(2009),325-340", "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The avalanche quantity of the information developed by mankind has led to\nconcept of automation of knowledge extraction - Data Mining ([1]). This\ndirection is connected with a wide spectrum of problems - from recognition of\nthe fuzzy set to creation of search machines. Important component of Data\nMining is processing of the text information. Such problems lean on concept of\nclassification and clustering ([2]). Classification consists in definition of\nan accessory of some element (text) to one of in advance created classes.\nClustering means splitting a set of elements (texts) on clusters which quantity\nare defined by localization of elements of the given set in vicinities of these\nsome natural centers of these clusters. Realization of a problem of\nclassification initially should lean on the given postulates, basic of which -\nthe aprioristic information on primary set of texts and a measure of affinity\nof elements and classes.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2009 09:41:47 GMT"}], "update_date": "2009-06-05", "authors_parsed": [["Shumeyko", "A. A.", ""], ["Sotnik", "S. L.", ""]]}, {"id": "0906.0872", "submitter": "Boris Yangel", "authors": "Boris Yangel", "title": "Fast Weak Learner Based on Genetic Algorithm", "comments": "4 pages, acmsiggraph latex style packed with the latex source in the\n  single archive", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approach to the acceleration of parametric weak classifier boosting is\nproposed. Weak classifier is called parametric if it has fixed number of\nparameters and, so, can be represented as a point into multidimensional space.\nGenetic algorithm is used instead of exhaustive search to learn parameters of\nsuch classifier. Proposed approach also takes cases when effective algorithm\nfor learning some of the classifier parameters exists into account. Experiments\nconfirm that such an approach can dramatically decrease classifier training\ntime while keeping both training and test errors small.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2009 10:25:08 GMT"}], "update_date": "2009-06-05", "authors_parsed": [["Yangel", "Boris", ""]]}, {"id": "0906.1713", "submitter": "Marcus Hutter", "authors": "Marcus Hutter", "title": "Feature Reinforcement Learning: Part I: Unstructured MDPs", "comments": "24 LaTeX pages, 5 diagrams", "journal-ref": "Journal of Artificial General Intelligence, 1 (2009) pages 3-24", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General-purpose, intelligent, learning agents cycle through sequences of\nobservations, actions, and rewards that are complex, uncertain, unknown, and\nnon-Markovian. On the other hand, reinforcement learning is well-developed for\nsmall finite state Markov decision processes (MDPs). Up to now, extracting the\nright state representations out of bare observations, that is, reducing the\ngeneral agent setup to the MDP framework, is an art that involves significant\neffort by designers. The primary goal of this work is to automate the reduction\nprocess and thereby significantly expand the scope of many existing\nreinforcement learning algorithms and the agents that employ them. Before we\ncan think of mechanizing this search for suitable MDPs, we need a formal\nobjective criterion. The main contribution of this article is to develop such a\ncriterion. I also integrate the various parts into one learning algorithm.\nExtensions to more realistic dynamic Bayesian networks are developed in Part\nII. The role of POMDPs is also considered there.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2009 12:50:29 GMT"}], "update_date": "2009-12-30", "authors_parsed": [["Hutter", "Marcus", ""]]}, {"id": "0906.1814", "submitter": "Renqiang Min", "authors": "Martin Renqiang Min, David A. Stanley, Zineng Yuan, Anthony Bonner,\n  and Zhaolei Zhang", "title": "Large-Margin kNN Classification Using a Deep Encoder Network", "comments": "13 pages (preliminary version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  KNN is one of the most popular classification methods, but it often fails to\nwork well with inappropriate choice of distance metric or due to the presence\nof numerous class-irrelevant features. Linear feature transformation methods\nhave been widely applied to extract class-relevant information to improve kNN\nclassification, which is very limited in many applications. Kernels have been\nused to learn powerful non-linear feature transformations, but these methods\nfail to scale to large datasets. In this paper, we present a scalable\nnon-linear feature mapping method based on a deep neural network pretrained\nwith restricted boltzmann machines for improving kNN classification in a\nlarge-margin framework, which we call DNet-kNN. DNet-kNN can be used for both\nclassification and for supervised dimensionality reduction. The experimental\nresults on two benchmark handwritten digit datasets show that DNet-kNN has much\nbetter performance than large-margin kNN using a linear mapping and kNN based\non a deep autoencoder pretrained with retricted boltzmann machines.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2009 20:06:45 GMT"}], "update_date": "2009-06-11", "authors_parsed": [["Min", "Martin Renqiang", ""], ["Stanley", "David A.", ""], ["Yuan", "Zineng", ""], ["Bonner", "Anthony", ""], ["Zhang", "Zhaolei", ""]]}, {"id": "0906.2027", "submitter": "Sewoong Oh", "authors": "Raghunandan H. Keshavan, Andrea Montanari and Sewoong Oh", "title": "Matrix Completion from Noisy Entries", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a matrix M of low-rank, we consider the problem of reconstructing it\nfrom noisy observations of a small, random subset of its entries. The problem\narises in a variety of applications, from collaborative filtering (the `Netflix\nproblem') to structure-from-motion and positioning. We study a low complexity\nalgorithm introduced by Keshavan et al.(2009), based on a combination of\nspectral techniques and manifold optimization, that we call here OptSpace. We\nprove performance guarantees that are order-optimal in a number of\ncircumstances.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2009 00:22:58 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2012 17:37:45 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Keshavan", "Raghunandan H.", ""], ["Montanari", "Andrea", ""], ["Oh", "Sewoong", ""]]}, {"id": "0906.2635", "submitter": "Tom\\'a\\v{s} Vina\\v{r}", "authors": "Tom\\'a\\v{s} Vina\\v{r}, Bro\\v{n}a Brejov\\'a, Giltae Song, Adam Siepel", "title": "Bayesian History Reconstruction of Complex Human Gene Clusters on a\n  Phylogeny", "comments": null, "journal-ref": "Comparative Genomics, International Workshop (RECOMB-CG), 5817\n  volume of Lecture Notes in Bioinformatics, pp. 150-163, Budapest, September\n  2009. Springer", "doi": "10.1007/978-3-642-04744-2_13", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clusters of genes that have evolved by repeated segmental duplication present\ndifficult challenges throughout genomic analysis, from sequence assembly to\nfunctional analysis. Improved understanding of these clusters is of utmost\nimportance, since they have been shown to be the source of evolutionary\ninnovation, and have been linked to multiple diseases, including HIV and a\nvariety of cancers. Previously, Zhang et al. (2008) developed an algorithm for\nreconstructing parsimonious evolutionary histories of such gene clusters, using\nonly human genomic sequence data. In this paper, we propose a probabilistic\nmodel for the evolution of gene clusters on a phylogeny, and an MCMC algorithm\nfor reconstruction of duplication histories from genomic sequences in multiple\nspecies. Several projects are underway to obtain high quality BAC-based\nassemblies of duplicated clusters in multiple species, and we anticipate that\nour method will be useful in analyzing these valuable new data sets.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2009 08:43:51 GMT"}], "update_date": "2010-01-25", "authors_parsed": [["Vina\u0159", "Tom\u00e1\u0161", ""], ["Brejov\u00e1", "Bro\u0148a", ""], ["Song", "Giltae", ""], ["Siepel", "Adam", ""]]}, {"id": "0906.2895", "submitter": "Velimir Ilic", "authors": "Velimir M. Ilic, Miomir S. Stankovic, Branimir T. Todorovic", "title": "Entropy Message Passing", "comments": "5 pages, 1 figure, to appear in IEEE Transactions on Information\n  Theory", "journal-ref": null, "doi": "10.1109/TIT.2010.2090235", "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a new message passing algorithm for cycle-free factor\ngraphs. The proposed \"entropy message passing\" (EMP) algorithm may be viewed as\nsum-product message passing over the entropy semiring, which has previously\nappeared in automata theory. The primary use of EMP is to compute the entropy\nof a model. However, EMP can also be used to compute expressions that appear in\nexpectation maximization and in gradient descent algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2009 10:58:50 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2009 23:03:13 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2009 12:48:20 GMT"}, {"version": "v4", "created": "Thu, 13 May 2010 16:40:01 GMT"}, {"version": "v5", "created": "Sat, 15 May 2010 22:23:39 GMT"}, {"version": "v6", "created": "Mon, 8 Nov 2010 01:37:37 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Ilic", "Velimir M.", ""], ["Stankovic", "Miomir S.", ""], ["Todorovic", "Branimir T.", ""]]}, {"id": "0906.3923", "submitter": "Daiki Koizumi", "authors": "Daiki Koizumi, Toshiyasu Matsushima, and Shigeichi Hirasawa", "title": "Bayesian Forecasting of WWW Traffic on the Time Varying Poisson Model", "comments": "8 pages, 6 figures. This paper was published in Proceeding of The\n  2009 International Conference on Parallel and Distributed Processing\n  Techniques and Applications (PDPTA'09) in July, 2009. In version of v4,\n  research grants are included in acknowledgment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic forecasting from past observed traffic data with small calculation\ncomplexity is one of important problems for planning of servers and networks.\nFocusing on World Wide Web (WWW) traffic as fundamental investigation, this\npaper would deal with Bayesian forecasting of network traffic on the time\nvarying Poisson model from a viewpoint from statistical decision theory. Under\nthis model, we would show that the estimated forecasting value is obtained by\nsimple arithmetic calculation and expresses real WWW traffic well from both\ntheoretical and empirical points of view.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2009 07:48:12 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2009 08:41:17 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2009 07:02:06 GMT"}, {"version": "v4", "created": "Thu, 3 Dec 2009 02:15:11 GMT"}], "update_date": "2009-12-03", "authors_parsed": [["Koizumi", "Daiki", ""], ["Matsushima", "Toshiyasu", ""], ["Hirasawa", "Shigeichi", ""]]}, {"id": "0906.4032", "submitter": "Karsten Borgwardt M.", "authors": "Karsten M. Borgwardt, Zoubin Ghahramani", "title": "Bayesian two-sample tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present two classes of Bayesian approaches to the\ntwo-sample problem. Our first class of methods extends the Bayesian t-test to\ninclude all parametric models in the exponential family and their conjugate\npriors. Our second class of methods uses Dirichlet process mixtures (DPM) of\nsuch conjugate-exponential distributions as flexible nonparametric priors over\nthe unknown distributions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2009 15:25:23 GMT"}], "update_date": "2009-06-23", "authors_parsed": [["Borgwardt", "Karsten M.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "0906.4172", "submitter": "R Doomun", "authors": "Anjana Pandey, K.R.Pardasani", "title": "Rough Set Model for Discovering Hybrid Association Rules", "comments": "5 pages, International Journal of Computer Science and Information\n  Security", "journal-ref": "IJCSIS June 2009 Issue, Vol. 2", "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the mining of hybrid association rules with rough set approach\nis investigated as the algorithm RSHAR.The RSHAR algorithm is constituted of\ntwo steps mainly. At first, to join the participant tables into a general table\nto generate the rules which is expressing the relationship between two or more\ndomains that belong to several different tables in a database. Then we apply\nthe mapping code on selected dimension, which can be added directly into the\ninformation system as one certain attribute. To find the association rules,\nfrequent itemsets are generated in second step where candidate itemsets are\ngenerated through equivalence classes and also transforming the mapping code in\nto real dimensions. The searching method for candidate itemset is similar to\napriori algorithm. The analysis of the performance of algorithm has been\ncarried out.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2009 06:24:57 GMT"}], "update_date": "2009-06-24", "authors_parsed": [["Pandey", "Anjana", ""], ["Pardasani", "K. R.", ""]]}, {"id": "0906.4539", "submitter": "Michael Mahoney", "authors": "Michael W. Mahoney and Hariharan Narayanan", "title": "Learning with Spectral Kernels and Heavy-Tailed Data", "comments": "21 pages. Substantially revised and extended relative to the first\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two ubiquitous aspects of large-scale data analysis are that the data often\nhave heavy-tailed properties and that diffusion-based or spectral-based methods\nare often used to identify and extract structure of interest. Perhaps\nsurprisingly, popular distribution-independent methods such as those based on\nthe VC dimension fail to provide nontrivial results for even simple learning\nproblems such as binary classification in these two settings. In this paper, we\ndevelop distribution-dependent learning methods that can be used to provide\ndimension-independent sample complexity bounds for the binary classification\nproblem in these two popular settings. In particular, we provide bounds on the\nsample complexity of maximum margin classifiers when the magnitude of the\nentries in the feature vector decays according to a power law and also when\nlearning is performed with the so-called Diffusion Maps kernel. Both of these\nresults rely on bounding the annealed entropy of gap-tolerant classifiers in a\nHilbert space. We provide such a bound, and we demonstrate that our proof\ntechnique generalizes to the case when the margin is measured with respect to\nmore general Banach space norms. The latter result is of potential interest in\ncases where modeling the relationship between data elements as a dot product in\na Hilbert space is too restrictive.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2009 18:38:31 GMT"}, {"version": "v2", "created": "Mon, 10 May 2010 17:19:30 GMT"}], "update_date": "2010-05-11", "authors_parsed": [["Mahoney", "Michael W.", ""], ["Narayanan", "Hariharan", ""]]}, {"id": "0906.4582", "submitter": "Patrick J. Wolfe", "authors": "Mohamed-Ali Belabbas and Patrick J. Wolfe", "title": "On landmark selection and sampling in high-dimensional data analysis", "comments": "18 pages, 6 figures, submitted for publication", "journal-ref": "Philosophical Transactions of the Royal Society, Series A, vol.\n  367, pp. 4295-4312, 2009", "doi": "10.1098/rsta.2009.0161", "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the spectral analysis of appropriately defined kernel\nmatrices has emerged as a principled way to extract the low-dimensional\nstructure often prevalent in high-dimensional data. Here we provide an\nintroduction to spectral methods for linear and nonlinear dimension reduction,\nemphasizing ways to overcome the computational limitations currently faced by\npractitioners with massive datasets. In particular, a data subsampling or\nlandmark selection process is often employed to construct a kernel based on\npartial information, followed by an approximate spectral analysis termed the\nNystrom extension. We provide a quantitative framework to analyse this\nprocedure, and use it to demonstrate algorithmic performance bounds on a range\nof practical approaches designed to optimize the landmark selection process. We\ncompare the practical implications of these bounds by way of real-world\nexamples drawn from the field of computer vision, whereby low-dimensional\nmanifold structure is shown to emerge from high-dimensional video data streams.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2009 23:40:22 GMT"}], "update_date": "2010-04-20", "authors_parsed": [["Belabbas", "Mohamed-Ali", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "0906.4663", "submitter": "R Doomun", "authors": "Hafeez Ullah Amin, Abdur Rashid Khan", "title": "Acquiring Knowledge for Evaluation of Teachers Performance in Higher\n  Education using a Questionnaire", "comments": "7 pages, International Journal of Computer Science and Information\n  Security (IJCSIS)", "journal-ref": "IJCSIS, June 2009 Issue, Vol. 2, No.1", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the step by step knowledge acquisition process by\nchoosing a structured method through using a questionnaire as a knowledge\nacquisition tool. Here we want to depict the problem domain as, how to evaluate\nteachers performance in higher education through the use of expert system\ntechnology. The problem is how to acquire the specific knowledge for a selected\nproblem efficiently and effectively from human experts and encode it in the\nsuitable computer format. Acquiring knowledge from human experts in the process\nof expert systems development is one of the most common problems cited till\nyet. This questionnaire was sent to 87 domain experts within all public and\nprivate universities in Pakistani. Among them 25 domain experts sent their\nvaluable opinions. Most of the domain experts were highly qualified, well\nexperienced and highly responsible persons. The whole questionnaire was divided\ninto 15 main groups of factors, which were further divided into 99 individual\nquestions. These facts were analyzed further to give a final shape to the\nquestionnaire. This knowledge acquisition technique may be used as a learning\ntool for further research work.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2009 11:09:39 GMT"}], "update_date": "2009-06-26", "authors_parsed": [["Amin", "Hafeez Ullah", ""], ["Khan", "Abdur Rashid", ""]]}, {"id": "0906.4779", "submitter": "Jascha Sohl-Dickstein", "authors": "Jascha Sohl-Dickstein, Peter Battaglino and Michael R. DeWeese", "title": "Minimum Probability Flow Learning", "comments": "Updated to match ICML conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting probabilistic models to data is often difficult, due to the general\nintractability of the partition function and its derivatives. Here we propose a\nnew parameter estimation technique that does not require computing an\nintractable normalization factor or sampling from the equilibrium distribution\nof the model. This is achieved by establishing dynamics that would transform\nthe observed data distribution into the model distribution, and then setting as\nthe objective the minimization of the KL divergence between the data\ndistribution and the distribution produced by running the dynamics for an\ninfinitesimal time. Score matching, minimum velocity learning, and certain\nforms of contrastive divergence are shown to be special cases of this learning\ntechnique. We demonstrate parameter estimation in Ising models, deep belief\nnetworks and an independent component analysis model of natural scenes. In the\nIsing model case, current state of the art techniques are outperformed by at\nleast an order of magnitude in learning time, with lower error in recovered\ncoupling parameters.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2009 19:15:44 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2009 02:20:21 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2010 07:03:16 GMT"}, {"version": "v4", "created": "Sun, 25 Sep 2011 01:33:51 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Sohl-Dickstein", "Jascha", ""], ["Battaglino", "Peter", ""], ["DeWeese", "Michael R.", ""]]}, {"id": "0906.5110", "submitter": "Susmit Jha", "authors": "Susmit Jha", "title": "Statistical Analysis of Privacy and Anonymity Guarantees in Randomized\n  Security Protocol Implementations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security protocols often use randomization to achieve probabilistic\nnon-determinism. This non-determinism, in turn, is used in obfuscating the\ndependence of observable values on secret data. Since the correctness of\nsecurity protocols is very important, formal analysis of security protocols has\nbeen widely studied in literature. Randomized security protocols have also been\nanalyzed using formal techniques such as process-calculi and probabilistic\nmodel checking. In this paper, we consider the problem of validating\nimplementations of randomized protocols. Unlike previous approaches which treat\nthe protocol as a white-box, our approach tries to verify an implementation\nprovided as a black box. Our goal is to infer the secrecy guarantees provided\nby a security protocol through statistical techniques. We learn the\nprobabilistic dependency of the observable outputs on secret inputs using\nBayesian network. This is then used to approximate the leakage of secret. In\norder to evaluate the accuracy of our statistical approach, we compare our\ntechnique with the probabilistic model checking technique on two examples:\ncrowds protocol and dining crypotgrapher's protocol.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2009 23:24:14 GMT"}], "update_date": "2009-06-30", "authors_parsed": [["Jha", "Susmit", ""]]}, {"id": "0906.5151", "submitter": "Hal Daum\\'e III", "authors": "Hal Daum\\'e III", "title": "Unsupervised Search-based Structured Prediction", "comments": null, "journal-ref": "Proceedings of the International Conference on Machine Learning,\n  2009", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an adaptation and application of a search-based structured\nprediction algorithm \"Searn\" to unsupervised learning problems. We show that it\nis possible to reduce unsupervised learning to supervised learning and\ndemonstrate a high-quality unsupervised shift-reduce parsing model. We\nadditionally show a close connection between unsupervised Searn and expectation\nmaximization. Finally, we demonstrate the efficacy of a semi-supervised\nextension. The key idea that enables this is an application of the predict-self\nidea for unsupervised learning.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2009 17:47:22 GMT"}], "update_date": "2009-06-30", "authors_parsed": [["Daum\u00e9", "Hal", "III"]]}, {"id": "0906.5325", "submitter": "Nicholas Mastronarde", "authors": "Nicholas Mastronarde and Mihaela van der Schaar", "title": "Online Reinforcement Learning for Dynamic Multimedia Systems", "comments": "35 pages, 11 figures, 10 tables", "journal-ref": "IEEE Trans. on Image Processing, vol. 19, no. 2, pp. 290-305, Feb.\n  2010", "doi": "10.1109/TIP.2009.2035228", "report-no": null, "categories": "cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our previous work, we proposed a systematic cross-layer framework for\ndynamic multimedia systems, which allows each layer to make autonomous and\nforesighted decisions that maximize the system's long-term performance, while\nmeeting the application's real-time delay constraints. The proposed solution\nsolved the cross-layer optimization offline, under the assumption that the\nmultimedia system's probabilistic dynamics were known a priori. In practice,\nhowever, these dynamics are unknown a priori and therefore must be learned\nonline. In this paper, we address this problem by allowing the multimedia\nsystem layers to learn, through repeated interactions with each other, to\nautonomously optimize the system's long-term performance at run-time. We\npropose two reinforcement learning algorithms for optimizing the system under\ndifferent design constraints: the first algorithm solves the cross-layer\noptimization in a centralized manner, and the second solves it in a\ndecentralized manner. We analyze both algorithms in terms of their required\ncomputation, memory, and inter-layer communication overheads. After noting that\nthe proposed reinforcement learning algorithms learn too slowly, we introduce a\ncomplementary accelerated learning algorithm that exploits partial knowledge\nabout the system's dynamics in order to dramatically improve the system's\nperformance. In our experiments, we demonstrate that decentralized learning can\nperform as well as centralized learning, while enabling the layers to act\nautonomously. Additionally, we show that existing application-independent\nreinforcement learning algorithms, and existing myopic learning algorithms\ndeployed in multimedia systems, perform significantly worse than our proposed\napplication-aware and foresighted learning methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2009 17:48:40 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Mastronarde", "Nicholas", ""], ["van der Schaar", "Mihaela", ""]]}]