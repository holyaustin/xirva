[{"id": "1204.0033", "submitter": "Ryan Rossi", "authors": "Ryan A. Rossi, Luke K. McDowell, David W. Aha and Jennifer Neville", "title": "Transforming Graph Representations for Statistical Relational Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational data representations have become an increasingly important topic\ndue to the recent proliferation of network datasets (e.g., social, biological,\ninformation networks) and a corresponding increase in the application of\nstatistical relational learning (SRL) algorithms to these domains. In this\narticle, we examine a range of representation issues for graph-based relational\ndata. Since the choice of relational data representation for the nodes, links,\nand features can dramatically affect the capabilities of SRL algorithms, we\nsurvey approaches and opportunities for relational representation\ntransformation designed to improve the performance of these algorithms. This\nleads us to introduce an intuitive taxonomy for data representation\ntransformations in relational domains that incorporates link transformation and\nnode transformation as symmetric representation tasks. In particular, the\ntransformation tasks for both nodes and links include (i) predicting their\nexistence, (ii) predicting their label or type, (iii) estimating their weight\nor importance, and (iv) systematically constructing their relevant features. We\nmotivate our taxonomy through detailed examples and use it to survey and\ncompare competing approaches for each of these tasks. We also discuss general\nconditions for transforming links, nodes, and features. Finally, we highlight\nchallenges that remain to be addressed.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2012 21:38:52 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Rossi", "Ryan A.", ""], ["McDowell", "Luke K.", ""], ["Aha", "David W.", ""], ["Neville", "Jennifer", ""]]}, {"id": "1204.0047", "submitter": "Ali Jalali", "authors": "Ali Jalali, Javad Azimi, Xiaoli Fern and Ruofei Zhang", "title": "A Lipschitz Exploration-Exploitation Scheme for Bayesian Optimization", "comments": "ECML 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of optimizing unknown costly-to-evaluate functions has been\nstudied for a long time in the context of Bayesian Optimization. Algorithms in\nthis field aim to find the optimizer of the function by asking only a few\nfunction evaluations at locations carefully selected based on a posterior\nmodel. In this paper, we assume the unknown function is Lipschitz continuous.\nLeveraging the Lipschitz property, we propose an algorithm with a distinct\nexploration phase followed by an exploitation phase. The exploration phase aims\nto select samples that shrink the search space as much as possible. The\nexploitation phase then focuses on the reduced search space and selects samples\nclosest to the optimizer. Considering the Expected Improvement (EI) as a\nbaseline, we empirically show that the proposed algorithm significantly\noutperforms EI.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2012 23:39:29 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2013 18:03:20 GMT"}], "update_date": "2013-07-17", "authors_parsed": [["Jalali", "Ali", ""], ["Azimi", "Javad", ""], ["Fern", "Xiaoli", ""], ["Zhang", "Ruofei", ""]]}, {"id": "1204.0136", "submitter": "Satyen Kale", "authors": "Elad Hazan, Satyen Kale, Shai Shalev-Shwartz", "title": "Near-Optimal Algorithms for Online Matrix Prediction", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several online prediction problems of recent interest the comparison class\nis composed of matrices with bounded entries. For example, in the online\nmax-cut problem, the comparison class is matrices which represent cuts of a\ngiven graph and in online gambling the comparison class is matrices which\nrepresent permutations over n teams. Another important example is online\ncollaborative filtering in which a widely used comparison class is the set of\nmatrices with a small trace norm. In this paper we isolate a property of\nmatrices, which we call (beta,tau)-decomposability, and derive an efficient\nonline learning algorithm, that enjoys a regret bound of O*(sqrt(beta tau T))\nfor all problems in which the comparison class is composed of\n(beta,tau)-decomposable matrices. By analyzing the decomposability of cut\nmatrices, triangular matrices, and low trace-norm matrices, we derive near\noptimal regret bounds for online max-cut, online gambling, and online\ncollaborative filtering. In particular, this resolves (in the affirmative) an\nopen problem posed by Abernethy (2010); Kleinberg et al (2010). Finally, we\nderive lower bounds for the three problems and show that our upper bounds are\noptimal up to logarithmic factors. In particular, our lower bound for the\nonline collaborative filtering problem resolves another open problem posed by\nShamir and Srebro (2011).\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2012 21:15:28 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Hazan", "Elad", ""], ["Kale", "Satyen", ""], ["Shalev-Shwartz", "Shai", ""]]}, {"id": "1204.0170", "submitter": "Jia Zeng", "authors": "Jia Zeng, Zhi-Qiang Liu and Xiao-Qin Cao", "title": "A New Approach to Speeding Up Topic Modeling", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet allocation (LDA) is a widely-used probabilistic topic\nmodeling paradigm, and recently finds many applications in computer vision and\ncomputational biology. In this paper, we propose a fast and accurate batch\nalgorithm, active belief propagation (ABP), for training LDA. Usually batch LDA\nalgorithms require repeated scanning of the entire corpus and searching the\ncomplete topic space. To process massive corpora having a large number of\ntopics, the training iteration of batch LDA algorithms is often inefficient and\ntime-consuming. To accelerate the training speed, ABP actively scans the subset\nof corpus and searches the subset of topic space for topic modeling, therefore\nsaves enormous training time in each iteration. To ensure accuracy, ABP selects\nonly those documents and topics that contribute to the largest residuals within\nthe residual belief propagation (RBP) framework. On four real-world corpora,\nABP performs around $10$ to $100$ times faster than state-of-the-art batch LDA\nalgorithms with a comparable topic modeling accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 07:07:27 GMT"}, {"version": "v2", "created": "Tue, 8 Apr 2014 02:17:47 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Zeng", "Jia", ""], ["Liu", "Zhi-Qiang", ""], ["Cao", "Xiao-Qin", ""]]}, {"id": "1204.0171", "submitter": "Mete Ozay", "authors": "Mete Ozay, Fatos T. Yarman Vural", "title": "A New Fuzzy Stacked Generalization Technique and Analysis of its\n  Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a new Stacked Generalization technique called Fuzzy Stacked\nGeneralization (FSG) is proposed to minimize the difference between N -sample\nand large-sample classification error of the Nearest Neighbor classifier. The\nproposed FSG employs a new hierarchical distance learning strategy to minimize\nthe error difference. For this purpose, we first construct an ensemble of\nbase-layer fuzzy k- Nearest Neighbor (k-NN) classifiers, each of which receives\na different feature set extracted from the same sample set. The fuzzy\nmembership values computed at the decision space of each fuzzy k-NN classifier\nare concatenated to form the feature vectors of a fusion space. Finally, the\nfeature vectors are fed to a meta-layer classifier to learn the degree of\naccuracy of the decisions of the base-layer classifiers for meta-layer\nclassification. Rather than the power of the individual base layer-classifiers,\ndiversity and cooperation of the classifiers become an important issue to\nimprove the overall performance of the proposed FSG. A weak base-layer\nclassifier may boost the overall performance more than a strong classifier, if\nit is capable of recognizing the samples, which are not recognized by the rest\nof the classifiers, in its own feature space. The experiments explore the type\nof the collaboration among the individual classifiers required for an improved\nperformance of the suggested architecture. Experiments on multiple feature\nreal-world datasets show that the proposed FSG performs better than the state\nof the art ensemble learning algorithms such as Adaboost, Random Subspace and\nRotation Forest. On the other hand, compatible performances are observed in the\nexperiments on single feature multi-attribute datasets.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2012 07:16:47 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2012 19:32:21 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2012 06:39:31 GMT"}, {"version": "v4", "created": "Thu, 1 Nov 2012 14:53:55 GMT"}, {"version": "v5", "created": "Mon, 12 Aug 2013 21:13:37 GMT"}], "update_date": "2013-08-14", "authors_parsed": [["Ozay", "Mete", ""], ["Vural", "Fatos T. Yarman", ""]]}, {"id": "1204.0566", "submitter": "Andrew Cotter", "authors": "Andrew Cotter, Shai Shalev-Shwartz, Nathan Srebro", "title": "The Kernelized Stochastic Batch Perceptron", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for training kernel Support Vector Machines,\nestablish learning runtime guarantees for our method that are better then those\nof any other known kernelized SVM optimization approach, and show that our\nmethod works well in practice compared to existing alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2012 00:33:53 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2012 12:14:24 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Cotter", "Andrew", ""], ["Shalev-Shwartz", "Shai", ""], ["Srebro", "Nathan", ""]]}, {"id": "1204.0684", "submitter": "Matthias Scholz", "authors": "Matthias Scholz", "title": "Validation of nonlinear PCA", "comments": "12 pages, 5 figures", "journal-ref": "Neural Processing Letters, 2012", "doi": "10.1007/s11063-012-9220-6", "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear principal component analysis (PCA) can be extended to a nonlinear PCA\nby using artificial neural networks. But the benefit of curved components\nrequires a careful control of the model complexity. Moreover, standard\ntechniques for model selection, including cross-validation and more generally\nthe use of an independent test set, fail when applied to nonlinear PCA because\nof its inherent unsupervised characteristics. This paper presents a new\napproach for validating the complexity of nonlinear PCA models by using the\nerror in missing data estimation as a criterion for model selection. It is\nmotivated by the idea that only the model of optimal complexity is able to\npredict missing values with the highest accuracy. While standard test set\nvalidation usually favours over-fitted nonlinear PCA models, the proposed model\nvalidation approach correctly selects the optimal model complexity.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2012 13:22:07 GMT"}], "update_date": "2012-04-04", "authors_parsed": [["Scholz", "Matthias", ""]]}, {"id": "1204.0870", "submitter": "Alexander Rakhlin", "authors": "Alexander Rakhlin, Ohad Shamir, Karthik Sridharan", "title": "Relax and Localize: From Value to Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a principled way of deriving online learning algorithms from a\nminimax analysis. Various upper bounds on the minimax value, previously thought\nto be non-constructive, are shown to yield algorithms. This allows us to\nseamlessly recover known methods and to derive new ones. Our framework also\ncaptures such \"unorthodox\" methods as Follow the Perturbed Leader and the R^2\nforecaster. We emphasize that understanding the inherent complexity of the\nlearning problem leads to the development of algorithms.\n  We define local sequential Rademacher complexities and associated algorithms\nthat allow us to obtain faster rates in online learning, similarly to\nstatistical learning theory. Based on these localized complexities we build a\ngeneral adaptive method that can take advantage of the suboptimality of the\nobserved sequence.\n  We present a number of new algorithms, including a family of randomized\nmethods that use the idea of a \"random playout\". Several new versions of the\nFollow-the-Perturbed-Leader algorithms are presented, as well as methods based\non the Littlestone's dimension, efficient methods for matrix completion with\ntrace norm, and algorithms for the problems of transductive learning and\nprediction with static experts.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2012 05:49:56 GMT"}], "update_date": "2012-04-05", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Shamir", "Ohad", ""], ["Sridharan", "Karthik", ""]]}, {"id": "1204.0885", "submitter": "Andri Mirzal", "authors": "Andri Mirzal, Shinichiro Yoshii, Masashi Furukawa", "title": "PID Parameters Optimization by Using Genetic Algorithm", "comments": "12 pages, 4 figures", "journal-ref": "ISTECS Journal, Vol. 8, pp. 34-43, 2006", "doi": null, "report-no": null, "categories": "cs.SY cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Time delays are components that make time-lag in systems response. They arise\nin physical, chemical, biological and economic systems, as well as in the\nprocess of measurement and computation. In this work, we implement Genetic\nAlgorithm (GA) in determining PID controller parameters to compensate the delay\nin First Order Lag plus Time Delay (FOLPD) and compare the results with\nIterative Method and Ziegler-Nichols rule results.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2012 08:17:32 GMT"}], "update_date": "2012-04-05", "authors_parsed": [["Mirzal", "Andri", ""], ["Yoshii", "Shinichiro", ""], ["Furukawa", "Masashi", ""]]}, {"id": "1204.1259", "submitter": "Bal\\'azs Hidasi", "authors": "Bal\\'azs Hidasi, Domonkos Tikk", "title": "Fast ALS-based tensor factorization for context-aware recommendation\n  from implicit feedback", "comments": "Accepted for ECML/PKDD 2012, presented on 25th September 2012,\n  Bristol, UK", "journal-ref": "Proceedings of the 2012 European conference on Machine Learning\n  and Knowledge Discovery in Databases - Volume Part II", "doi": "10.1007/978-3-642-33486-3_5", "report-no": null, "categories": "cs.LG cs.IR cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Albeit, the implicit feedback based recommendation problem - when only the\nuser history is available but there are no ratings - is the most typical\nsetting in real-world applications, it is much less researched than the\nexplicit feedback case. State-of-the-art algorithms that are efficient on the\nexplicit case cannot be straightforwardly transformed to the implicit case if\nscalability should be maintained. There are few if any implicit feedback\nbenchmark datasets, therefore new ideas are usually experimented on explicit\nbenchmarks. In this paper, we propose a generic context-aware implicit feedback\nrecommender algorithm, coined iTALS. iTALS apply a fast, ALS-based tensor\nfactorization learning method that scales linearly with the number of non-zero\nelements in the tensor. The method also allows us to incorporate diverse\ncontext information into the model while maintaining its computational\nefficiency. In particular, we present two such context-aware implementation\nvariants of iTALS. The first incorporates seasonality and enables to\ndistinguish user behavior in different time intervals. The other views the user\nhistory as sequential information and has the ability to recognize usage\npattern typical to certain group of items, e.g. to automatically tell apart\nproduct types or categories that are typically purchased repetitively\n(collectibles, grocery goods) or once (household appliances). Experiments\nperformed on three implicit datasets (two proprietary ones and an implicit\nvariant of the Netflix dataset) show that by integrating context-aware\ninformation with our factorization framework into the state-of-the-art implicit\nrecommender algorithm the recommendation quality improves significantly.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 15:34:30 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2013 15:33:31 GMT"}], "update_date": "2013-04-05", "authors_parsed": [["Hidasi", "Bal\u00e1zs", ""], ["Tikk", "Domonkos", ""]]}, {"id": "1204.1276", "submitter": "Sivan Sabato", "authors": "Sivan Sabato, Nathan Srebro and Naftali Tishby", "title": "Distribution-Dependent Sample Complexity of Large Margin Learning", "comments": "arXiv admin note: text overlap with arXiv:1011.5053", "journal-ref": "S. Sabato, N. Srebro and N. Tishby, \"Distribution-Dependent Sample\n  Complexity of Large Margin Learning\", Journal of Machine Learning Research,\n  14(Jul):2119-2149, 2013", "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain a tight distribution-specific characterization of the sample\ncomplexity of large-margin classification with L2 regularization: We introduce\nthe margin-adapted dimension, which is a simple function of the second order\nstatistics of the data distribution, and show distribution-specific upper and\nlower bounds on the sample complexity, both governed by the margin-adapted\ndimension of the data distribution. The upper bounds are universal, and the\nlower bounds hold for the rich family of sub-Gaussian distributions with\nindependent features. We conclude that this new quantity tightly characterizes\nthe true sample complexity of large-margin classification. To prove the lower\nbound, we develop several new tools of independent interest. These include new\nconnections between shattering and hardness of learning, new properties of\nshattering with linear classifiers, and a new lower bound on the smallest\neigenvalue of a random Gram matrix generated by sub-Gaussian variables. Our\nresults can be used to quantitatively compare large margin learning to other\nlearning rules, and to improve the effectiveness of methods that use sample\ncomplexity bounds, such as active learning.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 16:59:29 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2012 14:00:26 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2013 15:58:47 GMT"}, {"version": "v4", "created": "Wed, 18 Sep 2013 13:17:05 GMT"}], "update_date": "2013-09-19", "authors_parsed": [["Sabato", "Sivan", ""], ["Srebro", "Nathan", ""], ["Tishby", "Naftali", ""]]}, {"id": "1204.1437", "submitter": "Suvrit Sra", "authors": "Suvrit Sra", "title": "Fast projections onto mixed-norm balls with applications", "comments": "Preprint of paper under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint sparsity offers powerful structural cues for feature selection,\nespecially for variables that are expected to demonstrate a \"grouped\" behavior.\nSuch behavior is commonly modeled via group-lasso, multitask lasso, and related\nmethods where feature selection is effected via mixed-norms. Several mixed-norm\nbased sparse models have received substantial attention, and for some cases\nefficient algorithms are also available. Surprisingly, several constrained\nsparse models seem to be lacking scalable algorithms. We address this\ndeficiency by presenting batch and online (stochastic-gradient) optimization\nmethods, both of which rely on efficient projections onto mixed-norm balls. We\nillustrate our methods by applying them to the multitask lasso. We conclude by\nmentioning some open problems.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2012 08:55:38 GMT"}], "update_date": "2012-04-09", "authors_parsed": [["Sra", "Suvrit", ""]]}, {"id": "1204.1467", "submitter": "Ali Soltan Mohammadi", "authors": "Ali Soltan Mohammadi and L. Asadzadeh and D. D. Rezaee", "title": "Learning Fuzzy {\\beta}-Certain and {\\beta}-Possible rules from\n  incomplete quantitative data by rough sets", "comments": "hi thanks for attention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rough-set theory proposed by Pawlak, has been widely used in dealing with\ndata classification problems. The original rough-set model is, however, quite\nsensitive to noisy data. Tzung thus proposed deals with the problem of\nproducing a set of fuzzy certain and fuzzy possible rules from quantitative\ndata with a predefined tolerance degree of uncertainty and misclassification.\nThis model allowed, which combines the variable precision rough-set model and\nthe fuzzy set theory, is thus proposed to solve this problem. This paper thus\ndeals with the problem of producing a set of fuzzy certain and fuzzy possible\nrules from incomplete quantitative data with a predefined tolerance degree of\nuncertainty and misclassification. A new method, incomplete quantitative data\nfor rough-set model and the fuzzy set theory, is thus proposed to solve this\nproblem. It first transforms each quantitative value into a fuzzy set of\nlinguistic terms using membership functions and then finding incomplete\nquantitative data with lower and the fuzzy upper approximations. It second\ncalculates the fuzzy {\\beta}-lower and the fuzzy {\\beta}-upper approximations.\nThe certain and possible rules are then generated based on these fuzzy\napproximations. These rules can then be used to classify unknown objects.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2012 12:42:24 GMT"}], "update_date": "2012-04-09", "authors_parsed": [["Mohammadi", "Ali Soltan", ""], ["Asadzadeh", "L.", ""], ["Rezaee", "D. D.", ""]]}, {"id": "1204.1564", "submitter": "Jose Fontanari", "authors": "Paulo F. C. Tilles and Jose F. Fontanari", "title": "Minimal model of associative learning for cross-situational lexicon\n  acquisition", "comments": null, "journal-ref": "J. Math. Psych. 56, 396-403 (2012)", "doi": "10.1016/j.jmp.2012.11.002", "report-no": null, "categories": "q-bio.NC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An explanation for the acquisition of word-object mappings is the associative\nlearning in a cross-situational scenario. Here we present analytical results of\nthe performance of a simple associative learning algorithm for acquiring a\none-to-one mapping between $N$ objects and $N$ words based solely on the\nco-occurrence between objects and words. In particular, a learning trial in our\nlearning scenario consists of the presentation of $C + 1 < N$ objects together\nwith a target word, which refers to one of the objects in the context. We find\nthat the learning times are distributed exponentially and the learning rates\nare given by $\\ln{[\\frac{N(N-1)}{C + (N-1)^{2}}]}$ in the case the $N$ target\nwords are sampled randomly and by $\\frac{1}{N} \\ln [\\frac{N-1}{C}] $ in the\ncase they follow a deterministic presentation sequence. This learning\nperformance is much superior to those exhibited by humans and more realistic\nlearning algorithms in cross-situational experiments. We show that introduction\nof discrimination limitations using Weber's law and forgetting reduce the\nperformance of the associative algorithm to the human level.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2012 20:57:07 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2012 12:04:12 GMT"}, {"version": "v3", "created": "Wed, 22 Aug 2012 12:15:10 GMT"}, {"version": "v4", "created": "Mon, 17 Dec 2012 16:58:04 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Tilles", "Paulo F. C.", ""], ["Fontanari", "Jose F.", ""]]}, {"id": "1204.1624", "submitter": "Wassim Jouini", "authors": "Wassim Jouini and Christophe Moy", "title": "UCB Algorithm for Exponential Distributions", "comments": "10 pages. Introduces Multiplicative Upper Confidence Bound (MUCB)\n  algorithms for Multi-Armed Bandit problems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We introduce in this paper a new algorithm for Multi-Armed Bandit (MAB)\nproblems. A machine learning paradigm popular within Cognitive Network related\ntopics (e.g., Spectrum Sensing and Allocation). We focus on the case where the\nrewards are exponentially distributed, which is common when dealing with\nRayleigh fading channels. This strategy, named Multiplicative Upper Confidence\nBound (MUCB), associates a utility index to every available arm, and then\nselects the arm with the highest index. For every arm, the associated index is\nequal to the product of a multiplicative factor by the sample mean of the\nrewards collected by this arm. We show that the MUCB policy has a low\ncomplexity and is order optimal.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 12:17:03 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Jouini", "Wassim", ""], ["Moy", "Christophe", ""]]}, {"id": "1204.1681", "submitter": "Mohamed Ali Mahjoub", "authors": "Fradj Ben Lamine, Karim Kalti, Mohamed Ali Mahjoub", "title": "The threshold EM algorithm for parameter learning in bayesian network\n  with incomplete data", "comments": "6 pages", "journal-ref": "(IJACSA) International Journal of Advanced Computer Science and\n  Applications, Vol. 2, No. 7, pp 86-90, July 2011", "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks (BN) are used in a big range of applications but they have\none issue concerning parameter learning. In real application, training data are\nalways incomplete or some nodes are hidden. To deal with this problem many\nlearning parameter algorithms are suggested foreground EM, Gibbs sampling and\nRBE algorithms. In order to limit the search space and escape from local maxima\nproduced by executing EM algorithm, this paper presents a learning parameter\nalgorithm that is a fusion of EM and RBE algorithms. This algorithm\nincorporates the range of a parameter into the EM algorithm. This range is\ncalculated by the first step of RBE algorithm allowing a regularization of each\nparameter in bayesian network after the maximization step of the EM algorithm.\nThe threshold EM algorithm is applied in brain tumor diagnosis and show some\nadvantages and disadvantages over the EM algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 21:09:48 GMT"}], "update_date": "2012-04-12", "authors_parsed": [["Lamine", "Fradj Ben", ""], ["Kalti", "Karim", ""], ["Mahjoub", "Mohamed Ali", ""]]}, {"id": "1204.1685", "submitter": "Martin Azizyan", "authors": "Martin Azizyan, Aarti Singh, Larry Wasserman", "title": "Density-sensitive semisupervised inference", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1092 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 2, 751-771", "doi": "10.1214/13-AOS1092", "report-no": "IMS-AOS-AOS1092", "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semisupervised methods are techniques for using labeled data\n$(X_1,Y_1),\\ldots,(X_n,Y_n)$ together with unlabeled data $X_{n+1},\\ldots,X_N$\nto make predictions. These methods invoke some assumptions that link the\nmarginal distribution $P_X$ of X to the regression function f(x). For example,\nit is common to assume that f is very smooth over high density regions of\n$P_X$. Many of the methods are ad-hoc and have been shown to work in specific\nexamples but are lacking a theoretical foundation. We provide a minimax\nframework for analyzing semisupervised methods. In particular, we study methods\nbased on metrics that are sensitive to the distribution $P_X$. Our model\nincludes a parameter $\\alpha$ that controls the strength of the semisupervised\nassumption. We then use the data to adapt to $\\alpha$.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 21:49:22 GMT"}, {"version": "v2", "created": "Fri, 24 May 2013 13:14:50 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Azizyan", "Martin", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1204.1688", "submitter": "John C. Duchi", "authors": "John C. Duchi, Lester Mackey, Michael I. Jordan", "title": "The asymptotics of ranking algorithms", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1142 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 5, 2292-2323", "doi": "10.1214/13-AOS1142", "report-no": "IMS-AOS-AOS1142", "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the predictive problem of supervised ranking, where the task is\nto rank sets of candidate items returned in response to queries. Although there\nexist statistical procedures that come with guarantees of consistency in this\nsetting, these procedures require that individuals provide a complete ranking\nof all items, which is rarely feasible in practice. Instead, individuals\nroutinely provide partial preference information, such as pairwise comparisons\nof items, and more practical approaches to ranking have aimed at modeling this\npartial preference data directly. As we show, however, such an approach raises\nserious theoretical challenges. Indeed, we demonstrate that many commonly used\nsurrogate losses for pairwise comparison data do not yield consistency;\nsurprisingly, we show inconsistency even in low-noise settings. With these\nnegative results as motivation, we present a new approach to supervised ranking\nbased on aggregation of partial preferences, and we develop $U$-statistic-based\nempirical risk minimization procedures. We present an asymptotic analysis of\nthese new procedures, showing that they yield consistency results that parallel\nthose available for classification. We complement our theoretical results with\nan experiment studying the new procedures in a large-scale web-ranking task.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 22:33:22 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2013 18:04:41 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2013 09:25:03 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Duchi", "John C.", ""], ["Mackey", "Lester", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1204.1800", "submitter": "Debarghya Ghoshdastidar", "authors": "Debarghya Ghoshdastidar and Ambedkar Dukkipati", "title": "On Power-law Kernels, corresponding Reproducing Kernel Hilbert Space and\n  Applications", "comments": "7 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The role of kernels is central to machine learning. Motivated by the\nimportance of power-law distributions in statistical modeling, in this paper,\nwe propose the notion of power-law kernels to investigate power-laws in\nlearning problem. We propose two power-law kernels by generalizing Gaussian and\nLaplacian kernels. This generalization is based on distributions, arising out\nof maximization of a generalized information measure known as nonextensive\nentropy that is very well studied in statistical mechanics. We prove that the\nproposed kernels are positive definite, and provide some insights regarding the\ncorresponding Reproducing Kernel Hilbert Space (RKHS). We also study practical\nsignificance of both kernels in classification and regression, and present some\nsimulation results.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 05:53:27 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2013 07:12:43 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Ghoshdastidar", "Debarghya", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "1204.1909", "submitter": "Long Tran-Thanh", "authors": "Long Tran-Thanh, Archie Chapman, Alex Rogers, Nicholas R. Jennings", "title": "Knapsack based Optimal Policies for Budget-Limited Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In budget-limited multi-armed bandit (MAB) problems, the learner's actions\nare costly and constrained by a fixed budget. Consequently, an optimal\nexploitation policy may not be to pull the optimal arm repeatedly, as is the\ncase in other variants of MAB, but rather to pull the sequence of different\narms that maximises the agent's total reward within the budget. This difference\nfrom existing MABs means that new approaches to maximising the total reward are\nrequired. Given this, we develop two pulling policies, namely: (i) KUBE; and\n(ii) fractional KUBE. Whereas the former provides better performance up to 40%\nin our experimental settings, the latter is computationally less expensive. We\nalso prove logarithmic upper bounds for the regret of both policies, and show\nthat these bounds are asymptotically optimal (i.e. they only differ from the\nbest possible regret by a constant factor).\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 15:56:56 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Tran-Thanh", "Long", ""], ["Chapman", "Archie", ""], ["Rogers", "Alex", ""], ["Jennings", "Nicholas R.", ""]]}, {"id": "1204.1956", "submitter": "Rong Ge", "authors": "Sanjeev Arora, Rong Ge, Ankur Moitra", "title": "Learning Topic Models - Going beyond SVD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic Modeling is an approach used for automatic comprehension and\nclassification of data in a variety of settings, and perhaps the canonical\napplication is in uncovering thematic structure in a corpus of documents. A\nnumber of foundational works both in machine learning and in theory have\nsuggested a probabilistic model for documents, whereby documents arise as a\nconvex combination of (i.e. distribution on) a small number of topic vectors,\neach topic vector being a distribution on words (i.e. a vector of\nword-frequencies). Similar models have since been used in a variety of\napplication areas; the Latent Dirichlet Allocation or LDA model of Blei et al.\nis especially popular.\n  Theoretical studies of topic modeling focus on learning the model's\nparameters assuming the data is actually generated from it. Existing approaches\nfor the most part rely on Singular Value Decomposition(SVD), and consequently\nhave one of two limitations: these works need to either assume that each\ndocument contains only one topic, or else can only recover the span of the\ntopic vectors instead of the topic vectors themselves.\n  This paper formally justifies Nonnegative Matrix Factorization(NMF) as a main\ntool in this context, which is an analog of SVD where all vectors are\nnonnegative. Using this tool we give the first polynomial-time algorithm for\nlearning topic models without the above two limitations. The algorithm uses a\nfairly mild assumption about the underlying topic matrix called separability,\nwhich is usually found to hold in real-life data. A compelling feature of our\nalgorithm is that it generalizes to models that incorporate topic-topic\ncorrelations, such as the Correlated Topic Model and the Pachinko Allocation\nModel.\n  We hope that this paper will motivate further theoretical results that use\nNMF as a replacement for SVD - just as NMF has come to replace SVD in many\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 19:33:47 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2012 01:08:52 GMT"}], "update_date": "2012-04-13", "authors_parsed": [["Arora", "Sanjeev", ""], ["Ge", "Rong", ""], ["Moitra", "Ankur", ""]]}, {"id": "1204.2058", "submitter": "Shalini Puri", "authors": "Shalini Puri and Sona Kaushik", "title": "A technical study and analysis on fuzzy similarity based models for text\n  classification", "comments": "15 pages, 3 tables, 1 figure", "journal-ref": "International Journal of Data Mining & Knowledge Management\n  Process (IJDKP) March, 2012, Vol. 2, Number 2,pp. 1-15", "doi": "10.5121/ijdkp.2012.2201", "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this new and current era of technology, advancements and techniques,\nefficient and effective text document classification is becoming a challenging\nand highly required area to capably categorize text documents into mutually\nexclusive categories. Fuzzy similarity provides a way to find the similarity of\nfeatures among various documents. In this paper, a technical review on various\nfuzzy similarity based models is given. These models are discussed and compared\nto frame out their use and necessity. A tour of different methodologies is\nprovided which is based upon fuzzy similarity related concerns. It shows that\nhow text and web documents are categorized efficiently into different\ncategories. Various experimental results of these models are also discussed.\nThe technical comparisons among each model's parameters are shown in the form\nof a 3-D chart. Such study and technical review provide a strong base of\nresearch work done on fuzzy similarity based text document categorization.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 06:59:48 GMT"}], "update_date": "2012-04-11", "authors_parsed": [["Puri", "Shalini", ""], ["Kaushik", "Sona", ""]]}, {"id": "1204.2061", "submitter": "Shalini Puri", "authors": "Shalini Puri", "title": "A Fuzzy Similarity Based Concept Mining Model for Text Classification", "comments": "7 Pages, 3 Figures, 2 Tables, International Journal of Advanced\n  Computer Science and Applications(IJACSA)", "journal-ref": "Volume 2, Number 11, pp. 115 - 121, November, 2011", "doi": null, "report-no": null, "categories": "cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text Classification is a challenging and a red hot field in the current\nscenario and has great importance in text categorization applications. A lot of\nresearch work has been done in this field but there is a need to categorize a\ncollection of text documents into mutually exclusive categories by extracting\nthe concepts or features using supervised learning paradigm and different\nclassification algorithms. In this paper, a new Fuzzy Similarity Based Concept\nMining Model (FSCMM) is proposed to classify a set of text documents into pre -\ndefined Category Groups (CG) by providing them training and preparing on the\nsentence, document and integrated corpora levels along with feature reduction,\nambiguity removal on each level to achieve high system performance. Fuzzy\nFeature Category Similarity Analyzer (FFCSA) is used to analyze each extracted\nfeature of Integrated Corpora Feature Vector (ICFV) with the corresponding\ncategories or classes. This model uses Support Vector Machine Classifier (SVMC)\nto classify correctly the training data patterns into two groups; i. e., + 1\nand - 1, thereby producing accurate and correct results. The proposed model\nworks efficiently and effectively with great performance and high - accuracy\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 07:05:20 GMT"}], "update_date": "2012-04-11", "authors_parsed": [["Puri", "Shalini", ""]]}, {"id": "1204.2069", "submitter": "Keisuke Yamazaki", "authors": "Keisuke Yamazaki", "title": "Asymptotic Accuracy of Distribution-Based Estimation for Latent\n  Variables", "comments": "25pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical statistical models are widely employed in information science\nand data engineering. The models consist of two types of variables: observable\nvariables that represent the given data and latent variables for the\nunobservable labels. An asymptotic analysis of the models plays an important\nrole in evaluating the learning process; the result of the analysis is applied\nnot only to theoretical but also to practical situations, such as optimal model\nselection and active learning. There are many studies of generalization errors,\nwhich measure the prediction accuracy of the observable variables. However, the\naccuracy of estimating the latent variables has not yet been elucidated. For a\nquantitative evaluation of this, the present paper formulates\ndistribution-based functions for the errors in the estimation of the latent\nvariables. The asymptotic behavior is analyzed for both the maximum likelihood\nand the Bayes methods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 07:50:07 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2012 02:10:58 GMT"}, {"version": "v3", "created": "Thu, 6 Dec 2012 07:27:52 GMT"}, {"version": "v4", "created": "Thu, 20 Feb 2014 00:46:44 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Yamazaki", "Keisuke", ""]]}, {"id": "1204.2311", "submitter": "Bin Shen", "authors": "Bin Shen, Luo Si, Rongrong Ji, Baodi Liu", "title": "Robust Nonnegative Matrix Factorization via $L_1$ Norm Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonnegative Matrix Factorization (NMF) is a widely used technique in many\napplications such as face recognition, motion segmentation, etc. It\napproximates the nonnegative data in an original high dimensional space with a\nlinear representation in a low dimensional space by using the product of two\nnonnegative matrices. In many applications data are often partially corrupted\nwith large additive noise. When the positions of noise are known, some existing\nvariants of NMF can be applied by treating these corrupted entries as missing\nvalues. However, the positions are often unknown in many real world\napplications, which prevents the usage of traditional NMF or other existing\nvariants of NMF. This paper proposes a Robust Nonnegative Matrix Factorization\n(RobustNMF) algorithm that explicitly models the partial corruption as large\nadditive noise without requiring the information of positions of noise. In\npractice, large additive noise can be used to model outliers. In particular,\nthe proposed method jointly approximates the clean data matrix with the product\nof two nonnegative matrices and estimates the positions and values of\noutliers/noise. An efficient iterative optimization algorithm with a solid\ntheoretical justification has been proposed to learn the desired matrix\nfactorization. Experimental results demonstrate the advantages of the proposed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 01:03:03 GMT"}], "update_date": "2012-04-12", "authors_parsed": [["Shen", "Bin", ""], ["Si", "Luo", ""], ["Ji", "Rongrong", ""], ["Liu", "Baodi", ""]]}, {"id": "1204.2477", "submitter": "Matthew Johnson", "authors": "Matthew James Johnson", "title": "A Simple Explanation of A Spectral Algorithm for Learning Hidden Markov\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple linear algebraic explanation of the algorithm in \"A Spectral\nAlgorithm for Learning Hidden Markov Models\" (COLT 2009). Most of the content\nis in Figure 2; the text just makes everything precise in four nearly-trivial\nclaims.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 15:35:43 GMT"}], "update_date": "2012-04-12", "authors_parsed": [["Johnson", "Matthew James", ""]]}, {"id": "1204.2523", "submitter": "Khalid El-Arini", "authors": "Khalid El-Arini, Emily B. Fox, Carlos Guestrin", "title": "Concept Modeling with Superwords", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In information retrieval, a fundamental goal is to transform a document into\nconcepts that are representative of its content. The term \"representative\" is\nin itself challenging to define, and various tasks require different\ngranularities of concepts. In this paper, we aim to model concepts that are\nsparse over the vocabulary, and that flexibly adapt their content based on\nother relevant semantic information such as textual structure or associated\nimage features. We explore a Bayesian nonparametric model based on nested beta\nprocesses that allows for inferring an unknown number of strictly sparse\nconcepts. The resulting model provides an inherently different representation\nof concepts than a standard LDA (or HDP) based topic model, and allows for\ndirect incorporation of semantic features. We demonstrate the utility of this\nrepresentation on multilingual blog data and the Congressional Record.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 18:53:58 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["El-Arini", "Khalid", ""], ["Fox", "Emily B.", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1204.2581", "submitter": "Sheng Gao", "authors": "Sheng Gao and Ludovic Denoyer and Patrick Gallinari", "title": "Modeling Relational Data via Latent Factor Blockmodel", "comments": "10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of modeling relational data, which\nappear in many applications such as social network analysis, recommender\nsystems and bioinformatics. Previous studies either consider latent feature\nbased models but disregarding local structure in the network, or focus\nexclusively on capturing local structure of objects based on latent blockmodels\nwithout coupling with latent characteristics of objects. To combine the\nbenefits of the previous work, we propose a novel model that can simultaneously\nincorporate the effect of latent features and covariates if any, as well as the\neffect of latent structure that may exist in the data. To achieve this, we\nmodel the relation graph as a function of both latent feature factors and\nlatent cluster memberships of objects to collectively discover globally\npredictive intrinsic properties of objects and capture latent block structure\nin the network to improve prediction performance. We also develop an\noptimization transfer algorithm based on the generalized EM-style strategy to\nlearn the latent factors. We prove the efficacy of our proposed model through\nthe link prediction task and cluster analysis task, and extensive experiments\non the synthetic data and several real world datasets suggest that our proposed\nLFBM model outperforms the other state of the art approaches in the evaluated\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 22:14:05 GMT"}], "update_date": "2012-04-13", "authors_parsed": [["Gao", "Sheng", ""], ["Denoyer", "Ludovic", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1204.2588", "submitter": "Sheng Gao", "authors": "Sheng Gao and Ludovic Denoyer and Patrick Gallinari", "title": "Probabilistic Latent Tensor Factorization Model for Link Pattern\n  Prediction in Multi-relational Networks", "comments": "19pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at the problem of link pattern prediction in collections of\nobjects connected by multiple relation types, where each type may play a\ndistinct role. While common link analysis models are limited to single-type\nlink prediction, we attempt here to capture the correlations among different\nrelation types and reveal the impact of various relation types on performance\nquality. For that, we define the overall relations between object pairs as a\n\\textit{link pattern} which consists in interaction pattern and connection\nstructure in the network, and then use tensor formalization to jointly model\nand predict the link patterns, which we refer to as \\textit{Link Pattern\nPrediction} (LPP) problem. To address the issue, we propose a Probabilistic\nLatent Tensor Factorization (PLTF) model by introducing another latent factor\nfor multiple relation types and furnish the Hierarchical Bayesian treatment of\nthe proposed probabilistic model to avoid overfitting for solving the LPP\nproblem. To learn the proposed model we develop an efficient Markov Chain Monte\nCarlo sampling method. Extensive experiments are conducted on several real\nworld datasets and demonstrate significant improvements over several existing\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 22:58:46 GMT"}], "update_date": "2012-04-13", "authors_parsed": [["Gao", "Sheng", ""], ["Denoyer", "Ludovic", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1204.2609", "submitter": "Xiong Li", "authors": "Xiong Li and Tai Sing Lee and Yuncai Liu", "title": "Stochastic Feature Mapping for PAC-Bayes Classification", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic generative modeling of data distributions can potentially\nexploit hidden information which is useful for discriminative classification.\nThis observation has motivated the development of approaches that couple\ngenerative and discriminative models for classification. In this paper, we\npropose a new approach to couple generative and discriminative models in an\nunified framework based on PAC-Bayes risk theory. We first derive the\nmodel-parameter-independent stochastic feature mapping from a practical MAP\nclassifier operating on generative models. Then we construct a linear\nstochastic classifier equipped with the feature mapping, and derive the\nexplicit PAC-Bayes risk bounds for such classifier for both supervised and\nsemi-supervised learning. Minimizing the risk bound, using an EM-like iterative\nprocedure, results in a new posterior over hidden variables (E-step) and the\nupdate rules of model parameters (M-step). The derivation of the posterior is\nalways feasible due to the way of equipping feature mapping and the explicit\nform of bounding risk. The derived posterior allows the tuning of generative\nmodels and subsequently the feature mappings for better classification. The\nderived update rules of the model parameters are same to those of the uncoupled\nmodels as the feature mapping is model-parameter-independent. Our experiments\nshow that the coupling between data modeling generative model and the\ndiscriminative classifier via a stochastic feature mapping in this framework\nleads to a general classification tool with state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2012 03:49:15 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2012 02:44:25 GMT"}], "update_date": "2012-04-17", "authors_parsed": [["Li", "Xiong", ""], ["Lee", "Tai Sing", ""], ["Liu", "Yuncai", ""]]}, {"id": "1204.3251", "submitter": "Vladimir Vovk", "authors": "Valentina Fedorova, Alex Gammerman, Ilia Nouretdinov, and Vladimir\n  Vovk", "title": "Plug-in martingales for testing exchangeability on-line", "comments": "8 pages, 7 figures; ICML 2012 Conference Proceedings", "journal-ref": null, "doi": null, "report-no": "On-line Compression Modelling Project (New Series), Working Paper 04", "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard assumption in machine learning is the exchangeability of data,\nwhich is equivalent to assuming that the examples are generated from the same\nprobability distribution independently. This paper is devoted to testing the\nassumption of exchangeability on-line: the examples arrive one by one, and\nafter receiving each example we would like to have a valid measure of the\ndegree to which the assumption of exchangeability has been falsified. Such\nmeasures are provided by exchangeability martingales. We extend known\ntechniques for constructing exchangeability martingales and show that our new\nmethod is competitive with the martingales introduced before. Finally we\ninvestigate the performance of our testing method on two benchmark datasets,\nUSPS and Statlog Satellite data; for the former, the known techniques give\nsatisfactory results, but for the latter our new more flexible method becomes\nnecessary.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2012 10:21:57 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2012 09:36:27 GMT"}], "update_date": "2012-06-29", "authors_parsed": [["Fedorova", "Valentina", ""], ["Gammerman", "Alex", ""], ["Nouretdinov", "Ilia", ""], ["Vovk", "Vladimir", ""]]}, {"id": "1204.3514", "submitter": "Avrim Blum", "authors": "Maria-Florina Balcan, Avrim Blum, Shai Fine, and Yishay Mansour", "title": "Distributed Learning, Communication Complexity and Privacy", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of PAC-learning from distributed data and analyze\nfundamental communication complexity questions involved. We provide general\nupper and lower bounds on the amount of communication needed to learn well,\nshowing that in addition to VC-dimension and covering number, quantities such\nas the teaching-dimension and mistake-bound of a class play an important role.\nWe also present tight results for a number of common concept classes including\nconjunctions, parity functions, and decision lists. For linear separators, we\nshow that for non-concentrated distributions, we can use a version of the\nPerceptron algorithm to learn with much less communication than the number of\nupdates given by the usual margin bound. We also show how boosting can be\nperformed in a generic manner in the distributed setting to achieve\ncommunication with only logarithmic dependence on 1/epsilon for any concept\nclass, and demonstrate how recent work on agnostic learning from\nclass-conditional queries can be used to achieve low communication in agnostic\nsettings as well. We additionally present an analysis of privacy, considering\nboth differential privacy and a notion of distributional privacy that is\nespecially appealing in this context.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 15:10:32 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2012 21:42:21 GMT"}, {"version": "v3", "created": "Fri, 25 May 2012 15:53:51 GMT"}], "update_date": "2012-05-28", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Blum", "Avrim", ""], ["Fine", "Shai", ""], ["Mansour", "Yishay", ""]]}, {"id": "1204.3523", "submitter": "Avishek Saha", "authors": "Hal Daume III, Jeff M. Phillips, Avishek Saha, Suresh\n  Venkatasubramanian", "title": "Efficient Protocols for Distributed Classification and Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In distributed learning, the goal is to perform a learning task over data\ndistributed across multiple nodes with minimal (expensive) communication. Prior\nwork (Daume III et al., 2012) proposes a general model that bounds the\ncommunication required for learning classifiers while allowing for $\\eps$\ntraining error on linearly separable data adversarially distributed across\nnodes.\n  In this work, we develop key improvements and extensions to this basic model.\nOur first result is a two-party multiplicative-weight-update based protocol\nthat uses $O(d^2 \\log{1/\\eps})$ words of communication to classify distributed\ndata in arbitrary dimension $d$, $\\eps$-optimally. This readily extends to\nclassification over $k$ nodes with $O(kd^2 \\log{1/\\eps})$ words of\ncommunication. Our proposed protocol is simple to implement and is considerably\nmore efficient than baselines compared, as demonstrated by our empirical\nresults.\n  In addition, we illustrate general algorithm design paradigms for doing\nefficient learning over distributed data. We show how to solve\nfixed-dimensional and high dimensional linear programming efficiently in a\ndistributed setting where constraints may be distributed across nodes. Since\nmany learning problems can be viewed as convex optimization problems where\nconstraints are generated by individual points, this models many typical\ndistributed learning scenarios. Our techniques make use of a novel connection\nfrom multipass streaming, as well as adapting the multiplicative-weight-update\nframework more generally to a distributed setting. As a consequence, our\nmethods extend to the wide range of problems solvable using these techniques.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 15:25:50 GMT"}], "update_date": "2012-04-17", "authors_parsed": [["Daume", "Hal", "III"], ["Phillips", "Jeff M.", ""], ["Saha", "Avishek", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "1204.3611", "submitter": "Seyda Ertekin", "authors": "Seyda Ertekin, Haym Hirsh, Cynthia Rudin", "title": "Learning to Predict the Wisdom of Crowds", "comments": "Presented at Collective Intelligence conference, 2012\n  (arXiv:1204.2991)", "journal-ref": null, "doi": null, "report-no": "CollectiveIntelligence/2012/62", "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of \"approximating the crowd\" is that of estimating the crowd's\nmajority opinion by querying only a subset of it. Algorithms that approximate\nthe crowd can intelligently stretch a limited budget for a crowdsourcing task.\nWe present an algorithm, \"CrowdSense,\" that works in an online fashion to\ndynamically sample subsets of labelers based on an exploration/exploitation\ncriterion. The algorithm produces a weighted combination of a subset of the\nlabelers' votes that approximates the crowd's opinion.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 19:39:13 GMT"}], "update_date": "2012-04-17", "authors_parsed": [["Ertekin", "Seyda", ""], ["Hirsh", "Haym", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1204.3968", "submitter": "Pierre Sermanet", "authors": "Pierre Sermanet, Soumith Chintala, Yann LeCun", "title": "Convolutional Neural Networks Applied to House Numbers Digit\n  Classification", "comments": "4 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We classify digits of real-world house numbers using convolutional neural\nnetworks (ConvNets). ConvNets are hierarchical feature learning neural networks\nwhose structure is biologically inspired. Unlike many popular vision approaches\nthat are hand-designed, ConvNets can automatically learn a unique set of\nfeatures optimized for a given task. We augmented the traditional ConvNet\narchitecture by learning multi-stage features and by using Lp pooling and\nestablish a new state-of-the-art of 94.85% accuracy on the SVHN dataset (45.2%\nerror improvement). Furthermore, we analyze the benefits of different pooling\nmethods and multi-stage features in ConvNets. The source code and a tutorial\nare available at eblearn.sf.net.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 03:48:38 GMT"}], "update_date": "2012-04-19", "authors_parsed": [["Sermanet", "Pierre", ""], ["Chintala", "Soumith", ""], ["LeCun", "Yann", ""]]}, {"id": "1204.3972", "submitter": "Bo Dai", "authors": "Yuan Qi and Bo Dai and Yao Zhu", "title": "EigenGP: Sparse Gaussian process models with data-dependent\n  eigenfunctions", "comments": "10 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) provide a nonparametric representation of functions.\nHowever, classical GP inference suffers from high computational cost and it is\ndifficult to design nonstationary GP priors in practice. In this paper, we\npropose a sparse Gaussian process model, EigenGP, based on the Karhunen-Loeve\n(KL) expansion of a GP prior. We use the Nystrom approximation to obtain data\ndependent eigenfunctions and select these eigenfunctions by evidence\nmaximization. This selection reduces the number of eigenfunctions in our model\nand provides a nonstationary covariance function. To handle nonlinear\nlikelihoods, we develop an efficient expectation propagation (EP) inference\nalgorithm, and couple it with expectation maximization for eigenfunction\nselection. Because the eigenfunctions of a Gaussian kernel are associated with\nclusters of samples - including both the labeled and unlabeled - selecting\nrelevant eigenfunctions enables EigenGP to conduct semi-supervised learning.\nOur experimental results demonstrate improved predictive performance of EigenGP\nover alternative state-of-the-art sparse GP and semisupervised learning methods\nfor regression, classification, and semisupervised classification.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 04:43:24 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2012 21:23:54 GMT"}, {"version": "v3", "created": "Wed, 13 Mar 2013 21:55:59 GMT"}], "update_date": "2013-03-15", "authors_parsed": [["Qi", "Yuan", ""], ["Dai", "Bo", ""], ["Zhu", "Yao", ""]]}, {"id": "1204.4145", "submitter": "Karthik Sridharan Karthik Sridharan", "authors": "Karthik Sridharan", "title": "Learning From An Optimization Viewpoint", "comments": "Thesis supervisor : Nati Srebro Thesis Committee : David McAllester,\n  Arkadi Nemirovski, Alexander Razborov, Nati Srebro", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this dissertation we study statistical and online learning problems from\nan optimization viewpoint.The dissertation is divided into two parts :\n  I. We first consider the question of learnability for statistical learning\nproblems in the general learning setting. The question of learnability is well\nstudied and fully characterized for binary classification and for real valued\nsupervised learning problems using the theory of uniform convergence. However\nwe show that for the general learning setting uniform convergence theory fails\nto characterize learnability. To fill this void we use stability of learning\nalgorithms to fully characterize statistical learnability in the general\nsetting. Next we consider the problem of online learning. Unlike the\nstatistical learning framework there is a dearth of generic tools that can be\nused to establish learnability and rates for online learning problems in\ngeneral. We provide online analogs to classical tools from statistical learning\ntheory like Rademacher complexity, covering numbers, etc. We further use these\ntools to fully characterize learnability for online supervised learning\nproblems.\n  II. In the second part, for general classes of convex learning problems, we\nprovide appropriate mirror descent (MD) updates for online and statistical\nlearning of these problems. Further, we show that the the MD is near optimal\nfor online convex learning and for most cases, is also near optimal for\nstatistical convex learning. We next consider the problem of convex\noptimization and show that oracle complexity can be lower bounded by the so\ncalled fat-shattering dimension of the associated linear class. Thus we\nestablish a strong connection between offline convex optimization problems and\nstatistical learning problems. We also show that for a large class of high\ndimensional optimization problems, MD is in fact near optimal even for convex\noptimization.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 17:17:56 GMT"}], "update_date": "2012-04-19", "authors_parsed": [["Sridharan", "Karthik", ""]]}, {"id": "1204.4166", "submitter": "Yandong Guo", "authors": "Yuan Qi and Yandong Guo", "title": "Message passing with relaxed moment matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian learning is often hampered by large computational expense. As a\npowerful generalization of popular belief propagation, expectation propagation\n(EP) efficiently approximates the exact Bayesian computation. Nevertheless, EP\ncan be sensitive to outliers and suffer from divergence for difficult cases. To\naddress this issue, we propose a new approximate inference approach, relaxed\nexpectation propagation (REP). It relaxes the moment matching requirement of\nexpectation propagation by adding a relaxation factor into the KL minimization.\nWe penalize this relaxation with a $l_1$ penalty. As a result, when two\ndistributions in the relaxed KL divergence are similar, the relaxation factor\nwill be penalized to zero and, therefore, we obtain the original moment\nmatching; In the presence of outliers, these two distributions are\nsignificantly different and the relaxation factor will be used to reduce the\ncontribution of the outlier. Based on this penalized KL minimization, REP is\nrobust to outliers and can greatly improve the posterior approximation quality\nover EP. To examine the effectiveness of REP, we apply it to Gaussian process\nclassification, a task known to be suitable to EP. Our classification results\non synthetic and UCI benchmark datasets demonstrate significant improvement of\nREP over EP and Power EP--in terms of algorithmic stability, estimation\naccuracy and predictive performance.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 19:21:59 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2012 16:02:21 GMT"}], "update_date": "2012-08-30", "authors_parsed": [["Qi", "Yuan", ""], ["Guo", "Yandong", ""]]}, {"id": "1204.4200", "submitter": "Richard Preen", "authors": "Richard J. Preen and Larry Bull", "title": "Discrete Dynamical Genetic Programming in XCS", "comments": "arXiv admin note: substantial text overlap with arXiv:1201.5604", "journal-ref": "In Proceedings of the 11th annual conference on genetic and\n  evolutionary computation, GECCO '09, pp. 1299-1306. ACM, 2009", "doi": "10.1145/1569901.1570075", "report-no": null, "categories": "cs.AI cs.LG cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of representation schemes have been presented for use within\nLearning Classifier Systems, ranging from binary encodings to neural networks.\nThis paper presents results from an investigation into using a discrete\ndynamical system representation within the XCS Learning Classifier System. In\nparticular, asynchronous random Boolean networks are used to represent the\ntraditional condition-action production system rules. It is shown possible to\nuse self-adaptive, open-ended evolution to design an ensemble of such discrete\ndynamical systems within XCS to solve a number of well-known test problems.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 20:30:23 GMT"}, {"version": "v2", "created": "Sat, 18 Oct 2014 12:20:46 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Preen", "Richard J.", ""], ["Bull", "Larry", ""]]}, {"id": "1204.4202", "submitter": "Richard Preen", "authors": "Richard J. Preen and Larry Bull", "title": "Fuzzy Dynamical Genetic Programming in XCSF", "comments": "2 page GECCO 2011 poster paper", "journal-ref": "In Proceedings of the 13th annual conference companion on genetic\n  and evolutionary computation, GECCO '11, pp. 167-168. ACM, 2011", "doi": "10.1145/2001858.2001952", "report-no": null, "categories": "cs.AI cs.LG cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of representation schemes have been presented for use within\nLearning Classifier Systems, ranging from binary encodings to Neural Networks,\nand more recently Dynamical Genetic Programming (DGP). This paper presents\nresults from an investigation into using a fuzzy DGP representation within the\nXCSF Learning Classifier System. In particular, asynchronous Fuzzy Logic\nNetworks are used to represent the traditional condition-action production\nsystem rules. It is shown possible to use self-adaptive, open-ended evolution\nto design an ensemble of such fuzzy dynamical systems within XCSF to solve\nseveral well-known continuous-valued test problems.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 20:40:18 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Preen", "Richard J.", ""], ["Bull", "Larry", ""]]}, {"id": "1204.4294", "submitter": "Brijnesh Jain", "authors": "Brijnesh J. Jain and Klaus Obermayer", "title": "Learning in Riemannian Orbifolds", "comments": "arXiv admin note: substantial text overlap with arXiv:1001.0921", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning in Riemannian orbifolds is motivated by existing machine learning\nalgorithms that directly operate on finite combinatorial structures such as\npoint patterns, trees, and graphs. These methods, however, lack statistical\njustification. This contribution derives consistency results for learning\nproblems in structured domains and thereby generalizes learning in vector\nspaces and manifolds.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2012 09:29:10 GMT"}], "update_date": "2012-04-20", "authors_parsed": [["Jain", "Brijnesh J.", ""], ["Obermayer", "Klaus", ""]]}, {"id": "1204.4329", "submitter": "Patrick Taillandier", "authors": "Patrick Taillandier (UMMISCO), Alexis Drogoul (UMMISCO, MSI)", "title": "Supervised feature evaluation by consistency analysis: application to\n  measure sets used to characterise geographic objects", "comments": null, "journal-ref": "International Conference on Knowledge and Systems Engineering,\n  Hanoi : Viet Nam (2010)", "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, supervised learning is commonly used in many domains. Indeed, many\nworks propose to learn new knowledge from examples that translate the expected\nbehaviour of the considered system. A key issue of supervised learning concerns\nthe description language used to represent the examples. In this paper, we\npropose a method to evaluate the feature set used to describe them. Our method\nis based on the computation of the consistency of the example base. We carried\nout a case study in the domain of geomatic in order to evaluate the sets of\nmeasures used to characterise geographic objects. The case study shows that our\nmethod allows to give relevant evaluations of measure sets.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2012 12:03:20 GMT"}], "update_date": "2012-04-20", "authors_parsed": [["Taillandier", "Patrick", "", "UMMISCO"], ["Drogoul", "Alexis", "", "UMMISCO, MSI"]]}, {"id": "1204.4332", "submitter": "Patrick Taillandier", "authors": "Patrick Taillandier (UMMISCO), Julien Gaffuri (COGIT)", "title": "Designing generalisation evaluation function through human-machine\n  dialogue", "comments": null, "journal-ref": "GIScience, Zurich : Switzerland (2010)", "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated generalisation has known important improvements these last few\nyears. However, an issue that still deserves more study concerns the automatic\nevaluation of generalised data. Indeed, many automated generalisation systems\nrequire the utilisation of an evaluation function to automatically assess\ngeneralisation outcomes. In this paper, we propose a new approach dedicated to\nthe design of such a function. This approach allows an imperfectly defined\nevaluation function to be revised through a man-machine dialogue. The user\ngives its preferences to the system by comparing generalisation outcomes.\nMachine Learning techniques are then used to improve the evaluation function.\nAn experiment carried out on buildings shows that our approach significantly\nimproves generalisation evaluation functions defined by users.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2012 12:10:10 GMT"}], "update_date": "2012-04-20", "authors_parsed": [["Taillandier", "Patrick", "", "UMMISCO"], ["Gaffuri", "Julien", "", "COGIT"]]}, {"id": "1204.4521", "submitter": "Ayan Acharya", "authors": "Ayan Acharya, Eduardo R. Hruschka, Joydeep Ghosh", "title": "A Privacy-Aware Bayesian Approach for Combining Classifier and Cluster\n  Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a privacy-aware Bayesian approach that combines\nensembles of classifiers and clusterers to perform semi-supervised and\ntransductive learning. We consider scenarios where instances and their\nclassification/clustering results are distributed across different data sites\nand have sharing restrictions. As a special case, the privacy aware computation\nof the model when instances of the target data are distributed across different\ndata sites, is also discussed. Experimental results show that the proposed\napproach can provide good classification accuracies while adhering to the\ndata/model sharing constraints.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 03:01:56 GMT"}], "update_date": "2012-04-23", "authors_parsed": [["Acharya", "Ayan", ""], ["Hruschka", "Eduardo R.", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1204.4539", "submitter": "Julien Mairal", "authors": "Julien Mairal and Bin Yu", "title": "Supervised Feature Selection in Graphs with Path Coding Penalties and\n  Network Flows", "comments": "37 pages; to appear in the Journal of Machine Learning Research\n  (JMLR)", "journal-ref": "Journal of Machine Learning Research 14(Aug) (2013) 2449-2485", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider supervised learning problems where the features are embedded in a\ngraph, such as gene expressions in a gene network. In this context, it is of\nmuch interest to automatically select a subgraph with few connected components;\nby exploiting prior knowledge, one can indeed improve the prediction\nperformance or obtain results that are easier to interpret. Regularization or\npenalty functions for selecting features in graphs have recently been proposed,\nbut they raise new algorithmic challenges. For example, they typically require\nsolving a combinatorially hard selection problem among all connected subgraphs.\nIn this paper, we propose computationally feasible strategies to select a\nsparse and well-connected subset of features sitting on a directed acyclic\ngraph (DAG). We introduce structured sparsity penalties over paths on a DAG\ncalled \"path coding\" penalties. Unlike existing regularization functions that\nmodel long-range interactions between features in a graph, path coding\npenalties are tractable. The penalties and their proximal operators involve\npath selection problems, which we efficiently solve by leveraging network flow\noptimization. We experimentally show on synthetic, image, and genomic data that\nour approach is scalable and leads to more connected subgraphs than other\nregularization functions for graphs.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 06:24:37 GMT"}, {"version": "v2", "created": "Sat, 30 Mar 2013 10:16:21 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2013 13:12:00 GMT"}], "update_date": "2013-09-20", "authors_parsed": [["Mairal", "Julien", ""], ["Yu", "Bin", ""]]}, {"id": "1204.4710", "submitter": "Sebastien Bubeck", "authors": "Jean-Yves Audibert, S\\'ebastien Bubeck and G\\'abor Lugosi", "title": "Regret in Online Combinatorial Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address online linear optimization problems when the possible actions of\nthe decision maker are represented by binary vectors. The regret of the\ndecision maker is the difference between her realized loss and the best loss\nshe would have achieved by picking, in hindsight, the best possible action. Our\ngoal is to understand the magnitude of the best possible (minimax) regret. We\nstudy the problem under three different assumptions for the feedback the\ndecision maker receives: full information, and the partial information models\nof the so-called \"semi-bandit\" and \"bandit\" problems. Combining the Mirror\nDescent algorithm and the INF (Implicitely Normalized Forecaster) strategy, we\nare able to prove optimal bounds for the semi-bandit case. We also recover the\noptimal bounds for the full information setting. In the bandit case we discuss\nexisting results in light of a new lower bound, and suggest a conjecture on the\noptimal regret in that case. Finally we also prove that the standard\nexponentially weighted average forecaster is provably suboptimal in the setting\nof online combinatorial optimization.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 19:26:05 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2013 22:04:06 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Audibert", "Jean-Yves", ""], ["Bubeck", "S\u00e9bastien", ""], ["Lugosi", "G\u00e1bor", ""]]}, {"id": "1204.4717", "submitter": "Anil Aswani", "authors": "Anil Aswani, Neal Master, Jay Taneja, Andrew Krioukov, David Culler,\n  Claire Tomlin", "title": "Energy-Efficient Building HVAC Control Using Hybrid System LBMPC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving the energy-efficiency of heating, ventilation, and air-conditioning\n(HVAC) systems has the potential to realize large economic and societal\nbenefits. This paper concerns the system identification of a hybrid system\nmodel of a building-wide HVAC system and its subsequent control using a hybrid\nsystem formulation of learning-based model predictive control (LBMPC). Here,\nthe learning refers to model updates to the hybrid system model that\nincorporate the heating effects due to occupancy, solar effects, outside air\ntemperature (OAT), and equipment, in addition to integrator dynamics inherently\npresent in low-level control. Though we make significant modeling\nsimplifications, our corresponding controller that uses this model is able to\nexperimentally achieve a large reduction in energy usage without any\ndegradations in occupant comfort. It is in this way that we justify the\nmodeling simplifications that we have made. We conclude by presenting results\nfrom experiments on our building HVAC testbed, which show an average of 1.5MWh\nof energy savings per day (p = 0.002) with a 95% confidence interval of 1.0MWh\nto 2.1MWh of energy savings.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 19:55:30 GMT"}], "update_date": "2012-04-23", "authors_parsed": [["Aswani", "Anil", ""], ["Master", "Neal", ""], ["Taneja", "Jay", ""], ["Krioukov", "Andrew", ""], ["Culler", "David", ""], ["Tomlin", "Claire", ""]]}, {"id": "1204.4990", "submitter": "Patrick Taillandier", "authors": "Patrick Taillandier (UMMISCO), Julien Gaffuri (COGIT)", "title": "Objective Function Designing Led by User Preferences Acquisition", "comments": "International Conference on Information Technology and Applications,\n  Hanoi : Viet Nam (2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real world problems can be defined as optimisation problems in which the\naim is to maximise an objective function. The quality of obtained solution is\ndirectly linked to the pertinence of the used objective function. However,\ndesigning such function, which has to translate the user needs, is usually\nfastidious. In this paper, a method to help user objective functions designing\nis proposed. Our approach, which is highly interactive, is based on man machine\ndialogue and more particularly on the comparison of problem instance solutions\nby the user. We propose an experiment in the domain of cartographic\ngeneralisation that shows promising results.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2012 08:02:19 GMT"}], "update_date": "2012-04-24", "authors_parsed": [["Taillandier", "Patrick", "", "UMMISCO"], ["Gaffuri", "Julien", "", "COGIT"]]}, {"id": "1204.4991", "submitter": "Patrick Taillandier", "authors": "Patrick Taillandier (COGIT, UMMISCO), C\\'ecile Duch\\^ene (COGIT),\n  Alexis Drogoul (UMMISCO, MSI)", "title": "Knowledge revision in systems based on an informed tree search strategy\n  : application to cartographic generalisation", "comments": "Knowledge Revision; Problem Solving; Informed Tree Search Strategy;\n  Cartographic Generalisation., Paris : France (2008)", "journal-ref": null, "doi": "10.1145/1456223.1456281", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real world problems can be expressed as optimisation problems. Solving\nthis kind of problems means to find, among all possible solutions, the one that\nmaximises an evaluation function. One approach to solve this kind of problem is\nto use an informed search strategy. The principle of this kind of strategy is\nto use problem-specific knowledge beyond the definition of the problem itself\nto find solutions more efficiently than with an uninformed strategy. This kind\nof strategy demands to define problem-specific knowledge (heuristics). The\nefficiency and the effectiveness of systems based on it directly depend on the\nused knowledge quality. Unfortunately, acquiring and maintaining such knowledge\ncan be fastidious. The objective of the work presented in this paper is to\npropose an automatic knowledge revision approach for systems based on an\ninformed tree search strategy. Our approach consists in analysing the system\nexecution logs and revising knowledge based on these logs by modelling the\nrevision problem as a knowledge space exploration problem. We present an\nexperiment we carried out in an application domain where informed search\nstrategies are often used: cartographic generalisation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2012 08:03:06 GMT"}], "update_date": "2012-04-24", "authors_parsed": [["Taillandier", "Patrick", "", "COGIT, UMMISCO"], ["Duch\u00eane", "C\u00e9cile", "", "COGIT"], ["Drogoul", "Alexis", "", "UMMISCO, MSI"]]}, {"id": "1204.5043", "submitter": "Andreas Argyriou", "authors": "Andreas Argyriou and Rina Foygel and Nathan Srebro", "title": "Sparse Prediction with the $k$-Support Norm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a novel norm that corresponds to the tightest convex relaxation of\nsparsity combined with an $\\ell_2$ penalty. We show that this new {\\em\n$k$-support norm} provides a tighter relaxation than the elastic net and is\nthus a good replacement for the Lasso or the elastic net in sparse prediction\nproblems. Through the study of the $k$-support norm, we also bound the\nlooseness of the elastic net, thus shedding new light on it and providing\njustification for its use.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2012 12:35:56 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2012 08:59:52 GMT"}], "update_date": "2012-06-13", "authors_parsed": [["Argyriou", "Andreas", ""], ["Foygel", "Rina", ""], ["Srebro", "Nathan", ""]]}, {"id": "1204.5309", "submitter": "Simon Hawe", "authors": "Simon Hawe, Martin Kleinsteuber, and Klaus Diepold", "title": "Analysis Operator Learning and Its Application to Image Reconstruction", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": "10.1109/TIP.2013.2246175", "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploiting a priori known structural information lies at the core of many\nimage reconstruction methods that can be stated as inverse problems. The\nsynthesis model, which assumes that images can be decomposed into a linear\ncombination of very few atoms of some dictionary, is now a well established\ntool for the design of image reconstruction algorithms. An interesting\nalternative is the analysis model, where the signal is multiplied by an\nanalysis operator and the outcome is assumed to be the sparse. This approach\nhas only recently gained increasing interest. The quality of reconstruction\nmethods based on an analysis model severely depends on the right choice of the\nsuitable operator.\n  In this work, we present an algorithm for learning an analysis operator from\ntraining images. Our method is based on an $\\ell_p$-norm minimization on the\nset of full rank matrices with normalized columns. We carefully introduce the\nemployed conjugate gradient method on manifolds, and explain the underlying\ngeometry of the constraints. Moreover, we compare our approach to\nstate-of-the-art methods for image denoising, inpainting, and single image\nsuper-resolution. Our numerical results show competitive performance of our\ngeneral approach in all presented applications compared to the specialized\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2012 08:56:42 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2012 19:34:35 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2013 11:51:49 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Hawe", "Simon", ""], ["Kleinsteuber", "Martin", ""], ["Diepold", "Klaus", ""]]}, {"id": "1204.5721", "submitter": "Sebastien Bubeck", "authors": "S\\'ebastien Bubeck and Nicol\\`o Cesa-Bianchi", "title": "Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit\n  Problems", "comments": "To appear in Foundations and Trends in Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-armed bandit problems are the most basic examples of sequential\ndecision problems with an exploration-exploitation trade-off. This is the\nbalance between staying with the option that gave highest payoffs in the past\nand exploring new options that might give higher payoffs in the future.\nAlthough the study of bandit problems dates back to the Thirties,\nexploration-exploitation trade-offs arise in several modern applications, such\nas ad placement, website optimization, and packet routing. Mathematically, a\nmulti-armed bandit is defined by the payoff process associated with each\noption. In this survey, we focus on two extreme cases in which the analysis of\nregret is particularly simple and elegant: i.i.d. payoffs and adversarial\npayoffs. Besides the basic setting of finitely many actions, we also analyze\nsome of the most important variants and extensions, such as the contextual\nbandit model.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2012 18:04:32 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2012 18:50:58 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Bubeck", "S\u00e9bastien", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""]]}, {"id": "1204.5802", "submitter": "Dusko Pavlovic", "authors": "Dusko Pavlovic", "title": "Quantitative Concept Analysis", "comments": "16 pages, 3 figures, ICFCA 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal Concept Analysis (FCA) begins from a context, given as a binary\nrelation between some objects and some attributes, and derives a lattice of\nconcepts, where each concept is given as a set of objects and a set of\nattributes, such that the first set consists of all objects that satisfy all\nattributes in the second, and vice versa. Many applications, though, provide\ncontexts with quantitative information, telling not just whether an object\nsatisfies an attribute, but also quantifying this satisfaction. Contexts in\nthis form arise as rating matrices in recommender systems, as occurrence\nmatrices in text analysis, as pixel intensity matrices in digital image\nprocessing, etc. Such applications have attracted a lot of attention, and\nseveral numeric extensions of FCA have been proposed. We propose the framework\nof proximity sets (proxets), which subsume partially ordered sets (posets) as\nwell as metric spaces. One feature of this approach is that it extracts from\nquantified contexts quantified concepts, and thus allows full use of the\navailable information. Another feature is that the categorical approach allows\nanalyzing any universal properties that the classical FCA and the new versions\nmay have, and thus provides structural guidance for aligning and combining the\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 01:35:10 GMT"}], "update_date": "2012-04-27", "authors_parsed": [["Pavlovic", "Dusko", ""]]}, {"id": "1204.5810", "submitter": "Marco Molinaro", "authors": "Marco Molinaro and R. Ravi", "title": "Geometry of Online Packing Linear Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider packing LP's with $m$ rows where all constraint coefficients are\nnormalized to be in the unit interval. The n columns arrive in random order and\nthe goal is to set the corresponding decision variables irrevocably when they\narrive so as to obtain a feasible solution maximizing the expected reward.\nPrevious (1 - \\epsilon)-competitive algorithms require the right-hand side of\nthe LP to be Omega((m/\\epsilon^2) log (n/\\epsilon)), a bound that worsens with\nthe number of columns and rows. However, the dependence on the number of\ncolumns is not required in the single-row case and known lower bounds for the\ngeneral case are also independent of n.\n  Our goal is to understand whether the dependence on n is required in the\nmulti-row case, making it fundamentally harder than the single-row version. We\nrefute this by exhibiting an algorithm which is (1 - \\epsilon)-competitive as\nlong as the right-hand sides are Omega((m^2/\\epsilon^2) log (m/\\epsilon)). Our\ntechniques refine previous PAC-learning based approaches which interpret the\nonline decisions as linear classifications of the columns based on sampled dual\nprices. The key ingredient of our improvement comes from a non-standard\ncovering argument together with the realization that only when the columns of\nthe LP belong to few 1-d subspaces we can obtain small such covers; bounding\nthe size of the cover constructed also relies on the geometry of linear\nclassifiers. General packing LP's are handled by perturbing the input columns,\nwhich can be seen as making the learning problem more robust.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 02:06:44 GMT"}], "update_date": "2012-04-27", "authors_parsed": [["Molinaro", "Marco", ""], ["Ravi", "R.", ""]]}, {"id": "1204.6078", "submitter": "Yucheng Low", "authors": "Yucheng Low, Joseph Gonzalez, Aapo Kyrola, Danny Bickson, Carlos\n  Guestrin, Joseph M. Hellerstein", "title": "Distributed GraphLab: A Framework for Machine Learning in the Cloud", "comments": "VLDB2012", "journal-ref": "Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 8, pp.\n  716-727 (2012)", "doi": null, "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While high-level data parallel frameworks, like MapReduce, simplify the\ndesign and implementation of large-scale data processing systems, they do not\nnaturally or efficiently support many important data mining and machine\nlearning algorithms and can lead to inefficient learning systems. To help fill\nthis critical void, we introduced the GraphLab abstraction which naturally\nexpresses asynchronous, dynamic, graph-parallel computation while ensuring data\nconsistency and achieving a high degree of parallel performance in the\nshared-memory setting. In this paper, we extend the GraphLab framework to the\nsubstantially more challenging distributed setting while preserving strong data\nconsistency guarantees. We develop graph based extensions to pipelined locking\nand data versioning to reduce network congestion and mitigate the effect of\nnetwork latency. We also introduce fault tolerance to the GraphLab abstraction\nusing the classic Chandy-Lamport snapshot algorithm and demonstrate how it can\nbe easily implemented by exploiting the GraphLab abstraction itself. Finally,\nwe evaluate our distributed implementation of the GraphLab abstraction on a\nlarge Amazon EC2 deployment and show 1-2 orders of magnitude performance gains\nover Hadoop-based implementations.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 23:25:20 GMT"}], "update_date": "2012-04-30", "authors_parsed": [["Low", "Yucheng", ""], ["Gonzalez", "Joseph", ""], ["Kyrola", "Aapo", ""], ["Bickson", "Danny", ""], ["Guestrin", "Carlos", ""], ["Hellerstein", "Joseph M.", ""]]}, {"id": "1204.6250", "submitter": "Abdul Ghani Abro", "authors": "Abdul Ghani Abro, Junita Mohamad Saleh", "title": "Feature Selection for Generator Excitation Neurocontroller Development\n  Using Filter Technique", "comments": "10-Pages, 10-Figures, 8-Tables, International Journal of Computer\n  Science Issues, Vol. 8, Issue 5, No 3, September 2011", "journal-ref": "International Journal of Computer Science Issues,PP. 108-117, Vol.\n  8, Issue 5, No 3, September 2011", "doi": null, "report-no": null, "categories": "cs.SY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Essentially, motive behind using control system is to generate suitable\ncontrol signal for yielding desired response of a physical process. Control of\nsynchronous generator has always remained very critical in power system\noperation and control. For certain well known reasons power generators are\nnormally operated well below their steady state stability limit. This raises\ndemand for efficient and fast controllers. Artificial intelligence has been\nreported to give revolutionary outcomes in the field of control engineering.\nArtificial Neural Network (ANN), a branch of artificial intelligence has been\nused for nonlinear and adaptive control, utilizing its inherent observability.\nThe overall performance of neurocontroller is dependent upon input features\ntoo. Selecting optimum features to train a neurocontroller optimally is very\ncritical. Both quality and size of data are of equal importance for better\nperformance. In this work filter technique is employed to select independent\nfactors for ANN training.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2011 04:52:31 GMT"}], "update_date": "2012-04-30", "authors_parsed": [["Abro", "Abdul Ghani", ""], ["Saleh", "Junita Mohamad", ""]]}, {"id": "1204.6325", "submitter": "Konstantinos Chorianopoulos", "authors": "Konstantinos Chorianopoulos, Vassiliki Tsaknaki", "title": "CELL: Connecting Everyday Life in an archipeLago", "comments": "This paper has been withdrawn by the author due to some errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the design of a seamless broadcast communication system that\nbrings together the distributed community of remote secondary education\nschools. In contrast to higher education, primary and secondary education\nestablishments should remain distributed, in order to maintain a balance of\nurban and rural life in the developing and the developed world. We plan to\ndeploy an ambient and social interactive TV platform (physical installation,\nauthoring tools, interactive content) that supports social communication in a\npositive way. In particular, we present the physical design and the conceptual\nmodel of the system.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2012 20:10:16 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2012 08:45:46 GMT"}], "update_date": "2012-06-05", "authors_parsed": [["Chorianopoulos", "Konstantinos", ""], ["Tsaknaki", "Vassiliki", ""]]}, {"id": "1204.6509", "submitter": "Fabrice Rossi", "authors": "Brieuc Conan-Guez (LITA), Fabrice Rossi (SAMM)", "title": "Dissimilarity Clustering by Hierarchical Multi-Level Refinement", "comments": "20-th European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN 2012), Bruges : Belgium (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce in this paper a new way of optimizing the natural extension of\nthe quantization error using in k-means clustering to dissimilarity data. The\nproposed method is based on hierarchical clustering analysis combined with\nmulti-level heuristic refinement. The method is computationally efficient and\nachieves better quantization errors than the\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2012 19:31:15 GMT"}], "update_date": "2012-05-02", "authors_parsed": [["Conan-Guez", "Brieuc", "", "LITA"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1204.6583", "submitter": "Takafumi Kanamori Dr.", "authors": "Takafumi Kanamori, Akiko Takeda, Taiji Suzuki", "title": "A Conjugate Property between Loss Functions and Uncertainty Sets in\n  Classification Problems", "comments": "41 pages, 4 figures. The shorter version is accepted by COLT2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In binary classification problems, mainly two approaches have been proposed;\none is loss function approach and the other is uncertainty set approach. The\nloss function approach is applied to major learning algorithms such as support\nvector machine (SVM) and boosting methods. The loss function represents the\npenalty of the decision function on the training samples. In the learning\nalgorithm, the empirical mean of the loss function is minimized to obtain the\nclassifier. Against a backdrop of the development of mathematical programming,\nnowadays learning algorithms based on loss functions are widely applied to\nreal-world data analysis. In addition, statistical properties of such learning\nalgorithms are well-understood based on a lots of theoretical works. On the\nother hand, the learning method using the so-called uncertainty set is used in\nhard-margin SVM, mini-max probability machine (MPM) and maximum margin MPM. In\nthe learning algorithm, firstly, the uncertainty set is defined for each binary\nlabel based on the training samples. Then, the best separating hyperplane\nbetween the two uncertainty sets is employed as the decision function. This is\nregarded as an extension of the maximum-margin approach. The uncertainty set\napproach has been studied as an application of robust optimization in the field\nof mathematical programming. The statistical properties of learning algorithms\nwith uncertainty sets have not been intensively studied. In this paper, we\nconsider the relation between the above two approaches. We point out that the\nuncertainty set is described by using the level set of the conjugate of the\nloss function. Based on such relation, we study statistical properties of\nlearning algorithms using uncertainty sets.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 09:53:08 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Kanamori", "Takafumi", ""], ["Takeda", "Akiko", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1204.6610", "submitter": "Jia Zeng", "authors": "Jia Zeng, Xiao-Qin Cao and Zhi-Qiang Liu", "title": "Residual Belief Propagation for Topic Modeling", "comments": "6 pages, 8 figures", "journal-ref": "Advanced Data Mining and Applications Lecture Notes in Computer\n  Science Volume 7713, 739-752, 2012", "doi": null, "report-no": null, "categories": "cs.LG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast convergence speed is a desired property for training latent Dirichlet\nallocation (LDA), especially in online and parallel topic modeling for massive\ndata sets. This paper presents a novel residual belief propagation (RBP)\nalgorithm to accelerate the convergence speed for training LDA. The proposed\nRBP uses an informed scheduling scheme for asynchronous message passing, which\npasses fast-convergent messages with a higher priority to influence those\nslow-convergent messages at each learning iteration. Extensive empirical\nstudies confirm that RBP significantly reduces the training time until\nconvergence while achieves a much lower predictive perplexity than other\nstate-of-the-art training algorithms for LDA, including variational Bayes (VB),\ncollapsed Gibbs sampling (GS), loopy belief propagation (BP), and residual VB\n(RVB).\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 12:18:40 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Zeng", "Jia", ""], ["Cao", "Xiao-Qin", ""], ["Liu", "Zhi-Qiang", ""]]}, {"id": "1204.6703", "submitter": "Daniel Hsu", "authors": "Animashree Anandkumar, Dean P. Foster, Daniel Hsu, Sham M. Kakade,\n  Yi-Kai Liu", "title": "A Spectral Algorithm for Latent Dirichlet Allocation", "comments": "Changed title to match conference version, which appears in Advances\n  in Neural Information Processing Systems 25, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of topic modeling can be seen as a generalization of the\nclustering problem, in that it posits that observations are generated due to\nmultiple latent factors (e.g., the words in each document are generated as a\nmixture of several active topics, as opposed to just one). This increased\nrepresentational power comes at the cost of a more challenging unsupervised\nlearning problem of estimating the topic probability vectors (the distributions\nover words for each topic), when only the words are observed and the\ncorresponding topics are hidden.\n  We provide a simple and efficient learning procedure that is guaranteed to\nrecover the parameters for a wide class of mixture models, including the\npopular latent Dirichlet allocation (LDA) model. For LDA, the procedure\ncorrectly recovers both the topic probability vectors and the prior over the\ntopics, using only trigram statistics (i.e., third order moments, which may be\nestimated with documents containing just three words). The method, termed\nExcess Correlation Analysis (ECA), is based on a spectral decomposition of low\norder moments (third and fourth order) via two singular value decompositions\n(SVDs). Moreover, the algorithm is scalable since the SVD operations are\ncarried out on $k\\times k$ matrices, where $k$ is the number of latent factors\n(e.g. the number of topics), rather than in the $d$-dimensional observed space\n(typically $d \\gg k$).\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 17:06:06 GMT"}, {"version": "v2", "created": "Tue, 22 May 2012 02:08:38 GMT"}, {"version": "v3", "created": "Tue, 3 Jul 2012 20:01:11 GMT"}, {"version": "v4", "created": "Thu, 17 Jan 2013 21:01:29 GMT"}], "update_date": "2013-01-21", "authors_parsed": [["Anandkumar", "Animashree", ""], ["Foster", "Dean P.", ""], ["Hsu", "Daniel", ""], ["Kakade", "Sham M.", ""], ["Liu", "Yi-Kai", ""]]}]