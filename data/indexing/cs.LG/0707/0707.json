[{"id": "0707.0498", "submitter": "Roy Murphy Dr", "authors": "Roy E. Murphy", "title": "The Role of Time in the Creation of Knowledge", "comments": "Adaptive Processes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT", "license": null, "abstract": "  This paper I assume that in humans the creation of knowledge depends on a\ndiscrete time, or stage, sequential decision-making process subjected to a\nstochastic, information transmitting environment. For each time-stage, this\nenvironment randomly transmits Shannon type information-packets to the\ndecision-maker, who examines each of them for relevancy and then determines his\noptimal choices. Using this set of relevant information-packets, the\ndecision-maker adapts, over time, to the stochastic nature of his environment,\nand optimizes the subjective expected rate-of-growth of knowledge. The\ndecision-maker's optimal actions, lead to a decision function that involves,\nover time, his view of the subjective entropy of the environmental process and\nother important parameters at each time-stage of the process. Using this model\nof human behavior, one could create psychometric experiments using computer\nsimulation and real decision-makers, to play programmed games to measure the\nresulting human performance.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2007 20:43:43 GMT"}], "update_date": "2007-07-13", "authors_parsed": [["Murphy", "Roy E.", ""]]}, {"id": "0707.0701", "submitter": "Alexandre d'Aspremont", "authors": "Ronny Luss, Alexandre d'Aspremont", "title": "Clustering and Feature Selection using Sparse Principal Component\n  Analysis", "comments": "More experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the application of sparse principal component\nanalysis (PCA) to clustering and feature selection problems. Sparse PCA seeks\nsparse factors, or linear combinations of the data variables, explaining a\nmaximum amount of variance in the data while having only a limited number of\nnonzero coefficients. PCA is often used as a simple clustering technique and\nsparse factors allow us here to interpret the clusters in terms of a reduced\nset of variables. We begin with a brief introduction and motivation on sparse\nPCA and detail our implementation of the algorithm in d'Aspremont et al.\n(2005). We then apply these results to some classic clustering and feature\nselection problems arising in biology.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2007 21:53:11 GMT"}, {"version": "v2", "created": "Wed, 8 Oct 2008 18:41:53 GMT"}], "update_date": "2008-10-08", "authors_parsed": [["Luss", "Ronny", ""], ["d'Aspremont", "Alexandre", ""]]}, {"id": "0707.0704", "submitter": "Alexandre d'Aspremont", "authors": "Onureena Banerjee, Laurent El Ghaoui, Alexandre d'Aspremont", "title": "Model Selection Through Sparse Maximum Likelihood Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": null, "abstract": "  We consider the problem of estimating the parameters of a Gaussian or binary\ndistribution in such a way that the resulting undirected graphical model is\nsparse. Our approach is to solve a maximum likelihood problem with an added\nl_1-norm penalty term. The problem as formulated is convex but the memory\nrequirements and complexity of existing interior point methods are prohibitive\nfor problems with more than tens of nodes. We present two new algorithms for\nsolving problems with at least a thousand nodes in the Gaussian case. Our first\nalgorithm uses block coordinate descent, and can be interpreted as recursive\nl_1-norm penalized regression. Our second algorithm, based on Nesterov's first\norder method, yields a complexity estimate with a better dependence on problem\nsize than existing interior point methods. Using a log determinant relaxation\nof the log partition function (Wainwright & Jordan (2006)), we show that these\nsame algorithms can be used to solve an approximate sparse maximum likelihood\nproblem for the binary case. We test our algorithms on synthetic data, as well\nas on gene expression and senate voting records data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2007 22:13:42 GMT"}], "update_date": "2007-07-06", "authors_parsed": [["Banerjee", "Onureena", ""], ["Ghaoui", "Laurent El", ""], ["d'Aspremont", "Alexandre", ""]]}, {"id": "0707.0705", "submitter": "Alexandre d'Aspremont", "authors": "Alexandre d'Aspremont, Francis Bach, Laurent El Ghaoui", "title": "Optimal Solutions for Sparse Principal Component Analysis", "comments": "Revised journal version. More efficient optimality conditions and new\n  examples in subset selection and sparse recovery. Original version is in ICML\n  proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": null, "abstract": "  Given a sample covariance matrix, we examine the problem of maximizing the\nvariance explained by a linear combination of the input variables while\nconstraining the number of nonzero coefficients in this combination. This is\nknown as sparse principal component analysis and has a wide array of\napplications in machine learning and engineering. We formulate a new\nsemidefinite relaxation to this problem and derive a greedy algorithm that\ncomputes a full set of good solutions for all target numbers of non zero\ncoefficients, with total complexity O(n^3), where n is the number of variables.\nWe then use the same relaxation to derive sufficient conditions for global\noptimality of a solution, which can be tested in O(n^3) per pattern. We discuss\napplications in subset selection and sparse recovery and show on artificial\nexamples and biological data that our algorithm does provide globally optimal\nsolutions in many cases.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2007 22:28:28 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2007 22:30:22 GMT"}, {"version": "v3", "created": "Fri, 24 Aug 2007 00:49:31 GMT"}, {"version": "v4", "created": "Fri, 9 Nov 2007 17:27:11 GMT"}], "update_date": "2011-11-10", "authors_parsed": [["d'Aspremont", "Alexandre", ""], ["Bach", "Francis", ""], ["Ghaoui", "Laurent El", ""]]}, {"id": "0707.0805", "submitter": "Xinjia Chen", "authors": "Xinjia Chen", "title": "A New Generalization of Chebyshev Inequality for Random Vectors", "comments": "7 pages, 1 figure; added some references", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we derive a new generalization of Chebyshev inequality for\nrandom vectors. We demonstrate that the new generalization is much less\nconservative than the classical generalization.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2007 15:28:05 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2011 15:08:17 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Chen", "Xinjia", ""]]}, {"id": "0707.1452", "submitter": "Xavier Polanco", "authors": "Xavier Polanco (INIST)", "title": "Clusters, Graphs, and Networks for Analysing Internet-Web-Supported\n  Communication within a Virtual Community", "comments": null, "journal-ref": "Advances in Knowledge Organization (2002) 364-371", "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": null, "abstract": "  The proposal is to use clusters, graphs and networks as models in order to\nanalyse the Web structure. Clusters, graphs and networks provide knowledge\nrepresentation and organization. Clusters were generated by co-site analysis.\nThe sample is a set of academic Web sites from the countries belonging to the\nEuropean Union. These clusters are here revisited from the point of view of\ngraph theory and social network analysis. This is a quantitative and structural\nanalysis. In fact, the Internet is a computer network that connects people and\norganizations. Thus we may consider it to be a social network. The set of Web\nacademic sites represents an empirical social network, and is viewed as a\nvirtual community. The network structural properties are here analysed applying\ntogether cluster analysis, graph theory and social network analysis.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2007 13:47:32 GMT"}], "update_date": "2007-07-11", "authors_parsed": [["Polanco", "Xavier", "", "INIST"]]}, {"id": "0707.3087", "submitter": "Ciamac Moallemi", "authors": "Vivek F. Farias, Ciamac C. Moallemi, Tsachy Weissman, Benjamin Van Roy", "title": "Universal Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an agent interacting with an unmodeled environment. At each time,\nthe agent makes an observation, takes an action, and incurs a cost. Its actions\ncan influence future observations and costs. The goal is to minimize the\nlong-term average cost. We propose a novel algorithm, known as the active LZ\nalgorithm, for optimal control based on ideas from the Lempel-Ziv scheme for\nuniversal data compression and prediction. We establish that, under the active\nLZ algorithm, if there exists an integer $K$ such that the future is\nconditionally independent of the past given a window of $K$ consecutive actions\nand observations, then the average cost converges to the optimum. Experimental\nresults involving the game of Rock-Paper-Scissors illustrate merits of the\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2007 14:51:39 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2009 19:41:57 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2009 00:58:34 GMT"}], "update_date": "2009-07-22", "authors_parsed": [["Farias", "Vivek F.", ""], ["Moallemi", "Ciamac C.", ""], ["Weissman", "Tsachy", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "0707.3390", "submitter": "Francis Bach", "authors": "Francis Bach (WILLOW Project - Inria/Ens)", "title": "Consistency of the group Lasso and multiple kernel learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": null, "abstract": "  We consider the least-square regression problem with regularization by a\nblock 1-norm, i.e., a sum of Euclidean norms over spaces of dimensions larger\nthan one. This problem, referred to as the group Lasso, extends the usual\nregularization by the 1-norm where all spaces have dimension one, where it is\ncommonly referred to as the Lasso. In this paper, we study the asymptotic model\nconsistency of the group Lasso. We derive necessary and sufficient conditions\nfor the consistency of group Lasso under practical assumptions, such as model\nmisspecification. When the linear predictors and Euclidean norms are replaced\nby functions and reproducing kernel Hilbert norms, the problem is usually\nreferred to as multiple kernel learning and is commonly used for learning from\nheterogeneous data sources and for non linear variable selection. Using tools\nfrom functional analysis, and in particular covariance operators, we extend the\nconsistency results to this infinite dimensional case and also propose an\nadaptive scheme to obtain a consistent model estimate, even when the necessary\ncondition required for the non adaptive scheme is not satisfied.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2007 14:35:20 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2008 10:10:31 GMT"}], "update_date": "2008-01-28", "authors_parsed": [["Bach", "Francis", "", "WILLOW Project - Inria/Ens"]]}, {"id": "0707.3479", "submitter": "Alp Atici", "authors": "Alp Atici, Rocco A. Servedio", "title": "Quantum Algorithms for Learning and Testing Juntas", "comments": "15 pages, 1 figure. Uses synttree package. To appear in Quantum\n  Information Processing", "journal-ref": "Quantum Information Processing, Vol. 6, No. 5, 323 - 348 (2007)", "doi": "10.1007/s11128-007-0061-6", "report-no": null, "categories": "quant-ph cs.LG", "license": null, "abstract": "  In this article we develop quantum algorithms for learning and testing\njuntas, i.e. Boolean functions which depend only on an unknown set of k out of\nn input variables. Our aim is to develop efficient algorithms:\n  - whose sample complexity has no dependence on n, the dimension of the domain\nthe Boolean functions are defined over;\n  - with no access to any classical or quantum membership (\"black-box\")\nqueries. Instead, our algorithms use only classical examples generated\nuniformly at random and fixed quantum superpositions of such classical\nexamples;\n  - which require only a few quantum examples but possibly many classical\nrandom examples (which are considered quite \"cheap\" relative to quantum\nexamples).\n  Our quantum algorithms are based on a subroutine FS which enables sampling\naccording to the Fourier spectrum of f; the FS subroutine was used in earlier\nwork of Bshouty and Jackson on quantum learning. Our results are as follows:\n  - We give an algorithm for testing k-juntas to accuracy $\\epsilon$ that uses\n$O(k/\\epsilon)$ quantum examples. This improves on the number of examples used\nby the best known classical algorithm.\n  - We establish the following lower bound: any FS-based k-junta testing\nalgorithm requires $\\Omega(\\sqrt{k})$ queries.\n  - We give an algorithm for learning $k$-juntas to accuracy $\\epsilon$ that\nuses $O(\\epsilon^{-1} k\\log k)$ quantum examples and $O(2^k \\log(1/\\epsilon))$\nrandom examples. We show that this learning algorithms is close to optimal by\ngiving a related lower bound.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2007 13:17:55 GMT"}], "update_date": "2007-10-16", "authors_parsed": [["Atici", "Alp", ""], ["Servedio", "Rocco A.", ""]]}]