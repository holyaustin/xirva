[{"id": "0908.0050", "submitter": "Julien Mairal", "authors": "Julien Mairal (INRIA Rocquencourt), Francis Bach (INRIA Rocquencourt),\n  Jean Ponce (INRIA Rocquencourt, LIENS), Guillermo Sapiro", "title": "Online Learning for Matrix Factorization and Sparse Coding", "comments": "revised version", "journal-ref": "Journal of Machine Learning Research 11 (2010) 19--60", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse coding--that is, modelling data vectors as sparse linear combinations\nof basis elements--is widely used in machine learning, neuroscience, signal\nprocessing, and statistics. This paper focuses on the large-scale matrix\nfactorization problem that consists of learning the basis set, adapting it to\nspecific data. Variations of this problem include dictionary learning in signal\nprocessing, non-negative matrix factorization and sparse principal component\nanalysis. In this paper, we propose to address these tasks with a new online\noptimization algorithm, based on stochastic approximations, which scales up\ngracefully to large datasets with millions of training samples, and extends\nnaturally to various matrix factorization formulations, making it suitable for\na wide range of learning problems. A proof of convergence is presented, along\nwith experiments with natural images and genomic data demonstrating that it\nleads to state-of-the-art performance in terms of speed and optimization for\nboth small and large datasets.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2009 06:09:18 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2010 07:33:02 GMT"}], "update_date": "2010-02-11", "authors_parsed": [["Mairal", "Julien", "", "INRIA Rocquencourt"], ["Bach", "Francis", "", "INRIA Rocquencourt"], ["Ponce", "Jean", "", "INRIA Rocquencourt, LIENS"], ["Sapiro", "Guillermo", ""]]}, {"id": "0908.0319", "submitter": "Sarah Filippi", "authors": "Sarah Filippi (LTCI), Olivier Capp\\'e (LTCI), Aur\\'elien Garivier\n  (LTCI)", "title": "Regret Bounds for Opportunistic Channel Access", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of opportunistic channel access in a primary system\ncomposed of independent Gilbert-Elliot channels where the secondary (or\nopportunistic) user does not dispose of a priori information regarding the\nstatistical characteristics of the system. It is shown that this problem may be\ncast into the framework of model-based learning in a specific class of\nPartially Observed Markov Decision Processes (POMDPs) for which we introduce an\nalgorithm aimed at striking an optimal tradeoff between the exploration (or\nestimation) and exploitation requirements. We provide finite horizon regret\nbounds for this algorithm as well as a numerical evaluation of its performance\nin the single channel model as well as in the case of stochastically identical\nchannels.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2009 19:25:58 GMT"}], "update_date": "2009-08-04", "authors_parsed": [["Filippi", "Sarah", "", "LTCI"], ["Capp\u00e9", "Olivier", "", "LTCI"], ["Garivier", "Aur\u00e9lien", "", "LTCI"]]}, {"id": "0908.0570", "submitter": "Piyush Rai", "authors": "Piyush Rai and Hal Daum\\'e III", "title": "The Infinite Hierarchical Factor Regression Model", "comments": null, "journal-ref": "NIPS 2008", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonparametric Bayesian factor regression model that accounts for\nuncertainty in the number of factors, and the relationship between factors. To\naccomplish this, we propose a sparse variant of the Indian Buffet Process and\ncouple this with a hierarchical model over factors, based on Kingman's\ncoalescent. We apply this model to two problems (factor analysis and factor\nregression) in gene-expression data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2009 01:10:09 GMT"}], "update_date": "2009-08-06", "authors_parsed": [["Rai", "Piyush", ""], ["Daum\u00e9", "Hal", "III"]]}, {"id": "0908.0572", "submitter": "Piyush Rai", "authors": "Piyush Rai, Hal Daum\\'e III, Suresh Venkatasubramanian", "title": "Streamed Learning: One-Pass SVMs", "comments": null, "journal-ref": "IJCAI 2009", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a streaming model for large-scale classification (in the context\nof $\\ell_2$-SVM) by leveraging connections between learning and computational\ngeometry. The streaming model imposes the constraint that only a single pass\nover the data is allowed. The $\\ell_2$-SVM is known to have an equivalent\nformulation in terms of the minimum enclosing ball (MEB) problem, and an\nefficient algorithm based on the idea of \\emph{core sets} exists (Core Vector\nMachine, CVM). CVM learns a $(1+\\varepsilon)$-approximate MEB for a set of\npoints and yields an approximate solution to corresponding SVM instance.\nHowever CVM works in batch mode requiring multiple passes over the data. This\npaper presents a single-pass SVM which is based on the minimum enclosing ball\nof streaming data. We show that the MEB updates for the streaming case can be\neasily adapted to learn the SVM weight vector in a way similar to using online\nstochastic gradient updates. Our algorithm performs polylogarithmic computation\nat each example, and requires very small and constant storage. Experimental\nresults show that, even in such restrictive settings, we can learn efficiently\nin just one pass and get accuracies comparable to other state-of-the-art SVM\nsolvers (batch and online). We also give an analysis of the algorithm, and\ndiscuss some open issues and possible extensions.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2009 00:40:23 GMT"}], "update_date": "2009-08-06", "authors_parsed": [["Rai", "Piyush", ""], ["Daum\u00e9", "Hal", "III"], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "0908.0772", "submitter": "Daniel Golovin", "authors": "Daniel Golovin, Andreas Krause, and Matthew Streeter", "title": "Online Learning of Assignments that Maximize Submodular Functions", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": "0908.0772", "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Which ads should we display in sponsored search in order to maximize our\nrevenue? How should we dynamically rank information sources to maximize value\nof information? These applications exhibit strong diminishing returns:\nSelection of redundant ads and information sources decreases their marginal\nutility. We show that these and other problems can be formalized as repeatedly\nselecting an assignment of items to positions to maximize a sequence of\nmonotone submodular functions that arrive one by one. We present an efficient\nalgorithm for this general problem and analyze it in the no-regret model. Our\nalgorithm possesses strong theoretical guarantees, such as a performance ratio\nthat converges to the optimal constant of 1-1/e. We empirically evaluate our\nalgorithm on two real-world online optimization problems on the web: ad\nallocation with submodular utilities, and dynamically ranking blogs to detect\ninformation cascades.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2009 23:56:22 GMT"}], "update_date": "2009-08-07", "authors_parsed": [["Golovin", "Daniel", ""], ["Krause", "Andreas", ""], ["Streeter", "Matthew", ""]]}, {"id": "0908.0939", "submitter": "William White III", "authors": "Eddie White", "title": "Clustering for Improved Learning in Maze Traversal Problem", "comments": "29 pages, 15 figures, Undergraduate Honors Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maze traversal problem (finding the shortest distance to the goal from\nany position in a maze) has been an interesting challenge in computational\nintelligence. Recent work has shown that the cellular simultaneous recurrent\nneural network (CSRN) can solve this problem for simple mazes. This thesis\nfocuses on exploiting relevant information about the maze to improve learning\nand decrease the training time for the CSRN to solve mazes. Appropriate\nvariables are identified to create useful clusters using relevant information.\nThe CSRN was next modified to allow for an additional external input. With this\nadditional input, several methods were tested and results show that clustering\nthe mazes improves the overall learning of the traversal problem for the CSRN.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2009 19:48:20 GMT"}], "update_date": "2009-08-07", "authors_parsed": [["White", "Eddie", ""]]}, {"id": "0908.0984", "submitter": "R Doomun", "authors": "C. Balasubramanian, K. Duraiswamy", "title": "An Application of Bayesian classification to Interval Encoded Temporal\n  mining with prioritized items", "comments": "7 pages IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS July 2009, ISSN 1947 5500, Impact Factor 0.423", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 3, No. 1, July 2009, USA", "doi": null, "report-no": "ISSN 1947 5500", "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real life, media information has time attributes either implicitly or\nexplicitly known as temporal data. This paper investigates the usefulness of\napplying Bayesian classification to an interval encoded temporal database with\nprioritized items. The proposed method performs temporal mining by encoding the\ndatabase with weighted items which prioritizes the items according to their\nimportance from the user perspective. Naive Bayesian classification helps in\nmaking the resulting temporal rules more effective. The proposed priority based\ntemporal mining (PBTM) method added with classification aids in solving\nproblems in a well informed and systematic manner. The experimental results are\nobtained from the complaints database of the telecommunications system, which\nshows the feasibility of this method of classification based temporal mining.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2009 05:36:51 GMT"}], "update_date": "2009-08-10", "authors_parsed": [["Balasubramanian", "C.", ""], ["Duraiswamy", "K.", ""]]}, {"id": "0908.1769", "submitter": "Bert Huang", "authors": "Bert Huang and Tony Jebara", "title": "Approximating the Permanent with Belief Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This work describes a method of approximating matrix permanents efficiently\nusing belief propagation. We formulate a probability distribution whose\npartition function is exactly the permanent, then use Bethe free energy to\napproximate this partition function. After deriving some speedups to standard\nbelief propagation, the resulting algorithm requires $(n^2)$ time per\niteration. Finally, we demonstrate the advantages of using this approximation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2009 18:27:54 GMT"}], "update_date": "2009-08-13", "authors_parsed": [["Huang", "Bert", ""], ["Jebara", "Tony", ""]]}, {"id": "0908.3265", "submitter": "Nitin Salodkar", "authors": "Nitin Salodkar and Abhay Karandikar", "title": "Rate Constrained Random Access over a Fading Channel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.LG cs.NI", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  In this paper, we consider uplink transmissions involving multiple users\ncommunicating with a base station over a fading channel. We assume that the\nbase station does not coordinate the transmissions of the users and hence the\nusers employ random access communication. The situation is modeled as a\nnon-cooperative repeated game with incomplete information. Each user attempts\nto minimize its long term power consumption subject to a minimum rate\nrequirement. We propose a two timescale stochastic gradient algorithm (TTSGA)\nfor tuning the users' transmission probabilities. The algorithm includes a\n'waterfilling threshold update mechanism' that ensures that the rate\nconstraints are satisfied. We prove that under the algorithm, the users'\ntransmission probabilities converge to a Nash equilibrium. Moreover, we also\nprove that the rate constraints are satisfied; this is also demonstrated using\nsimulation studies.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2009 16:45:32 GMT"}], "update_date": "2009-08-25", "authors_parsed": [["Salodkar", "Nitin", ""], ["Karandikar", "Abhay", ""]]}, {"id": "0908.3706", "submitter": "Somak Raychaudhury", "authors": "Juan C. Cuevas-Tello, Peter Tino, Somak Raychaudhury, Xin Yao, Markus\n  Harva", "title": "Uncovering delayed patterns in noisy and irregularly sampled time\n  series: an astronomy application", "comments": "36 pages, 10 figures, 16 tables, accepted for publication in Pattern\n  Recognition. This is a shortened version of the article: interested readers\n  are urged to refer to the published version", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.CO astro-ph.IM cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the time delay between two signals\nrepresenting delayed, irregularly sampled and noisy versions of the same\nunderlying pattern. We propose and demonstrate an evolutionary algorithm for\nthe (hyper)parameter estimation of a kernel-based technique in the context of\nan astronomical problem, namely estimating the time delay between two\ngravitationally lensed signals from a distant quasar. Mixed types (integer and\nreal) are used to represent variables within the evolutionary algorithm. We\ntest the algorithm on several artificial data sets, and also on real\nastronomical observations of quasar Q0957+561. By carrying out a statistical\nanalysis of the results we present a detailed comparison of our method with the\nmost popular methods for time delay estimation in astrophysics. Our method\nyields more accurate and more stable time delay estimates: for Q0957+561, we\nobtain 419.6 days for the time delay between images A and B. Our methodology\ncan be readily applied to current state-of-the-art optical monitoring data in\nastronomy, but can also be applied in other disciplines involving similar time\nseries data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2009 23:21:39 GMT"}], "update_date": "2009-08-27", "authors_parsed": [["Cuevas-Tello", "Juan C.", ""], ["Tino", "Peter", ""], ["Raychaudhury", "Somak", ""], ["Yao", "Xin", ""], ["Harva", "Markus", ""]]}, {"id": "0908.4144", "submitter": "Ping Li", "authors": "Ping Li", "title": "ABC-LogitBoost for Multi-class Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop abc-logitboost, based on the prior work on abc-boost and robust\nlogitboost. Our extensive experiments on a variety of datasets demonstrate the\nconsiderable improvement of abc-logitboost over logitboost and abc-mart.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2009 07:09:19 GMT"}], "update_date": "2009-08-31", "authors_parsed": [["Li", "Ping", ""]]}]