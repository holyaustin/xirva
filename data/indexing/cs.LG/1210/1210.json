[{"id": "1210.0066", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu", "title": "Iterative Reweighted Minimization Methods for $l_p$ Regularized\n  Unconstrained Nonlinear Programming", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study general $l_p$ regularized unconstrained minimization\nproblems. In particular, we derive lower bounds for nonzero entries of first-\nand second-order stationary points, and hence also of local minimizers of the\n$l_p$ minimization problems. We extend some existing iterative reweighted $l_1$\n(IRL1) and $l_2$ (IRL2) minimization methods to solve these problems and\nproposed new variants for them in which each subproblem has a closed form\nsolution. Also, we provide a unified convergence analysis for these methods. In\naddition, we propose a novel Lipschitz continuous $\\epsilon$-approximation to\n$\\|x\\|^p_p$. Using this result, we develop new IRL1 methods for the $l_p$\nminimization problems and showed that any accumulation point of the sequence\ngenerated by these methods is a first-order stationary point, provided that the\napproximation parameter $\\epsilon$ is below a computable threshold value. This\nis a remarkable result since all existing iterative reweighted minimization\nmethods require that $\\epsilon$ be dynamically updated and approach zero. Our\ncomputational results demonstrate that the new IRL1 method is generally more\nstable than the existing IRL1 methods [21,18] in terms of objective function\nvalue and CPU time.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2012 01:42:57 GMT"}], "update_date": "2012-10-02", "authors_parsed": [["Lu", "Zhaosong", ""]]}, {"id": "1210.0077", "submitter": "Marcus Hutter", "authors": "Peter Sunehag and Marcus Hutter", "title": "Optimistic Agents are Asymptotically Optimal", "comments": "13 LaTeX pages", "journal-ref": "Proc. 25th Australasian Joint Conference on Artificial\n  Intelligence (AusAI 2012) 15-26", "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use optimism to introduce generic asymptotically optimal reinforcement\nlearning agents. They achieve, with an arbitrary finite or compact class of\nenvironments, asymptotically optimal behavior. Furthermore, in the finite\ndeterministic case we provide finite error bounds.\n", "versions": [{"version": "v1", "created": "Sat, 29 Sep 2012 04:58:22 GMT"}], "update_date": "2013-05-17", "authors_parsed": [["Sunehag", "Peter", ""], ["Hutter", "Marcus", ""]]}, {"id": "1210.0473", "submitter": "Giovanni Cavallanti", "authors": "Giovanni Cavallanti, Nicol\\`o Cesa-Bianchi", "title": "Memory Constraint Online Multitask Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate online kernel algorithms which simultaneously process multiple\nclassification tasks while a fixed constraint is imposed on the size of their\nactive sets. We focus in particular on the design of algorithms that can\nefficiently deal with problems where the number of tasks is extremely high and\nthe task data are large scale. Two new projection-based algorithms are\nintroduced to efficiently tackle those issues while presenting different trade\noffs on how the available memory is managed with respect to the prior\ninformation about the learning tasks. Theoretically sound budget algorithms are\ndevised by coupling the Randomized Budget Perceptron and the Forgetron\nalgorithms with the multitask kernel. We show how the two seemingly contrasting\nproperties of learning from multiple tasks and keeping a constant memory\nfootprint can be balanced, and how the sharing of the available space among\ndifferent tasks is automatically taken care of. We propose and discuss new\ninsights on the multitask kernel. Experiments show that online kernel multitask\nalgorithms running on a budget can efficiently tackle real world learning\nproblems involving multiple tasks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 17:08:25 GMT"}], "update_date": "2012-10-02", "authors_parsed": [["Cavallanti", "Giovanni", ""], ["Cesa-Bianchi", "Nicol\u00f2", ""]]}, {"id": "1210.0508", "submitter": "Vladimir Kolmogorov", "authors": "Rustem Takhanov and Vladimir Kolmogorov", "title": "Inference algorithms for pattern-based CRFs on sequence data", "comments": "Algorithmica accepted version", "journal-ref": "Algorithmica, September 2016, Volume 76, Issue 1, pp 17-46", "doi": "10.1007/s00453-015-0017-7", "report-no": null, "categories": "cs.LG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Conditional Random Fields (CRFs) with pattern-based potentials\ndefined on a chain. In this model the energy of a string (labeling) $x_1...x_n$\nis the sum of terms over intervals $[i,j]$ where each term is non-zero only if\nthe substring $x_i...x_j$ equals a prespecified pattern $\\alpha$. Such CRFs can\nbe naturally applied to many sequence tagging problems.\n  We present efficient algorithms for the three standard inference tasks in a\nCRF, namely computing (i) the partition function, (ii) marginals, and (iii)\ncomputing the MAP. Their complexities are respectively $O(n L)$, $O(n L\n\\ell_{max})$ and $O(n L \\min\\{|D|,\\log (\\ell_{max}+1)\\})$ where $L$ is the\ncombined length of input patterns, $\\ell_{max}$ is the maximum length of a\npattern, and $D$ is the input alphabet. This improves on the previous\nalgorithms of (Ye et al., 2009) whose complexities are respectively $O(n L\n|D|)$, $O(n |\\Gamma| L^2 \\ell_{max}^2)$ and $O(n L |D|)$, where $|\\Gamma|$ is\nthe number of input patterns.\n  In addition, we give an efficient algorithm for sampling. Finally, we\nconsider the case of non-positive weights. (Komodakis & Paragios, 2009) gave an\n$O(n L)$ algorithm for computing the MAP. We present a modification that has\nthe same worst-case complexity but can beat it in the best case.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 19:13:59 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2012 16:16:58 GMT"}, {"version": "v3", "created": "Thu, 8 Nov 2012 00:11:30 GMT"}, {"version": "v4", "created": "Sat, 29 Dec 2012 22:13:01 GMT"}, {"version": "v5", "created": "Fri, 20 Jan 2017 08:00:44 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Takhanov", "Rustem", ""], ["Kolmogorov", "Vladimir", ""]]}, {"id": "1210.0563", "submitter": "Tao Hu", "authors": "Tao Hu and Dmitri B. Chklovskii", "title": "Sparse LMS via Online Linearized Bregman Iteration", "comments": "11 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a version of least-mean-square (LMS) algorithm for sparse system\nidentification. Our algorithm called online linearized Bregman iteration (OLBI)\nis derived from minimizing the cumulative prediction error squared along with\nan l1-l2 norm regularizer. By systematically treating the non-differentiable\nregularizer we arrive at a simple two-step iteration. We demonstrate that OLBI\nis bias free and compare its operation with existing sparse LMS algorithms by\nrederiving them in the online convex optimization framework. We perform\nconvergence analysis of OLBI for white input signals and derive theoretical\nexpressions for both the steady state and instantaneous mean square deviations\n(MSD). We demonstrate numerically that OLBI improves the performance of LMS\ntype algorithms for signals generated from sparse tap weights.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2012 20:28:09 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Hu", "Tao", ""], ["Chklovskii", "Dmitri B.", ""]]}, {"id": "1210.0645", "submitter": "Yingzhen Yang", "authors": "Yingzhen Yang, Thomas S. Huang", "title": "Nonparametric Unsupervised Classification", "comments": "Submitted to ALT 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised classification methods learn a discriminative classifier from\nunlabeled data, which has been proven to be an effective way of simultaneously\nclustering the data and training a classifier from the data. Various\nunsupervised classification methods obtain appealing results by the classifiers\nlearned in an unsupervised manner. However, existing methods do not consider\nthe misclassification error of the unsupervised classifiers except unsupervised\nSVM, so the performance of the unsupervised classifiers is not fully evaluated.\nIn this work, we study the misclassification error of two popular classifiers,\ni.e. the nearest neighbor classifier (NN) and the plug-in classifier, in the\nsetting of unsupervised classification.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 04:22:50 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2012 00:21:57 GMT"}, {"version": "v3", "created": "Wed, 14 Nov 2012 03:53:04 GMT"}, {"version": "v4", "created": "Mon, 19 Nov 2012 00:40:43 GMT"}, {"version": "v5", "created": "Mon, 20 May 2013 22:11:10 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["Yang", "Yingzhen", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1210.0685", "submitter": "Rodolphe Jenatton", "authors": "Rodolphe Jenatton (CMAP), R\\'emi Gribonval (INRIA - IRISA), Francis\n  Bach (LIENS, INRIA Paris - Rocquencourt)", "title": "Local stability and robustness of sparse dictionary learning in the\n  presence of noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular approach within the signal processing and machine learning\ncommunities consists in modelling signals as sparse linear combinations of\natoms selected from a learned dictionary. While this paradigm has led to\nnumerous empirical successes in various fields ranging from image to audio\nprocessing, there have only been a few theoretical arguments supporting these\nevidences. In particular, sparse coding, or sparse dictionary learning, relies\non a non-convex procedure whose local minima have not been fully analyzed yet.\nIn this paper, we consider a probabilistic model of sparse signals, and show\nthat, with high probability, sparse coding admits a local minimum around the\nreference dictionary generating the signals. Our study takes into account the\ncase of over-complete dictionaries and noisy signals, thus extending previous\nwork limited to noiseless settings and/or under-complete dictionaries. The\nanalysis we conduct is non-asymptotic and makes it possible to understand how\nthe key quantities of the problem, such as the coherence or the level of noise,\ncan scale with respect to the dimension of the signals, the number of atoms,\nthe sparsity and the number of observations.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 07:48:08 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Jenatton", "Rodolphe", "", "CMAP"], ["Gribonval", "R\u00e9mi", "", "INRIA - IRISA"], ["Bach", "Francis", "", "LIENS, INRIA Paris - Rocquencourt"]]}, {"id": "1210.0690", "submitter": "Santiago Videla", "authors": "Santiago Videla (INRIA - IRISA), Carito Guziolowski (IRCCyN), Federica\n  Eduati (DEI, EBI), Sven Thiele (INRIA - IRISA), Niels Grabe, Julio\n  Saez-Rodriguez (EBI), Anne Siegel (INRIA - IRISA)", "title": "Revisiting the Training of Logic Models of Protein Signaling Networks\n  with a Formal Approach based on Answer Set Programming", "comments": null, "journal-ref": "CMSB - 10th Computational Methods in Systems Biology 2012 7605\n  (2012) 342-361", "doi": "10.1007/978-3-642-33636-2_20", "report-no": null, "categories": "q-bio.QM cs.AI cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental question in systems biology is the construction and training to\ndata of mathematical models. Logic formalisms have become very popular to model\nsignaling networks because their simplicity allows us to model large systems\nencompassing hundreds of proteins. An approach to train (Boolean) logic models\nto high-throughput phospho-proteomics data was recently introduced and solved\nusing optimization heuristics based on stochastic methods. Here we demonstrate\nhow this problem can be solved using Answer Set Programming (ASP), a\ndeclarative problem solving paradigm, in which a problem is encoded as a\nlogical program such that its answer sets represent solutions to the problem.\nASP has significant improvements over heuristic methods in terms of efficiency\nand scalability, it guarantees global optimality of solutions as well as\nprovides a complete set of solutions. We illustrate the application of ASP with\nin silico cases based on realistic networks and data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 07:52:52 GMT"}, {"version": "v2", "created": "Sat, 22 Dec 2012 07:39:43 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Videla", "Santiago", "", "INRIA - IRISA"], ["Guziolowski", "Carito", "", "IRCCyN"], ["Eduati", "Federica", "", "DEI, EBI"], ["Thiele", "Sven", "", "INRIA - IRISA"], ["Grabe", "Niels", "", "EBI"], ["Saez-Rodriguez", "Julio", "", "EBI"], ["Siegel", "Anne", "", "INRIA - IRISA"]]}, {"id": "1210.0699", "submitter": "Xavier Bresson", "authors": "Xavier Bresson and Ruiliang Zhang", "title": "TV-SVM: Total Variation Support Vector Machine for Semi-Supervised Data\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce semi-supervised data classification algorithms based on total\nvariation (TV), Reproducing Kernel Hilbert Space (RKHS), support vector machine\n(SVM), Cheeger cut, labeled and unlabeled data points. We design binary and\nmulti-class semi-supervised classification algorithms. We compare the TV-based\nclassification algorithms with the related Laplacian-based algorithms, and show\nthat TV classification perform significantly better when the number of labeled\ndata is small.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 08:40:46 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Bresson", "Xavier", ""], ["Zhang", "Ruiliang", ""]]}, {"id": "1210.0734", "submitter": "Artemy Kolchinsky", "authors": "Artemy Kolchinsky, An\\'alia Louren\\c{c}o, Lang Li, Luis M. Rocha", "title": "Evaluation of linear classifiers on articles containing pharmacokinetic\n  evidence of drug-drug interactions", "comments": "Pacific Symposium on Biocomputing, 2013", "journal-ref": "Pac Symp Biocomput. 2013:409-20", "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background. Drug-drug interaction (DDI) is a major cause of morbidity and\nmortality. [...] Biomedical literature mining can aid DDI research by\nextracting relevant DDI signals from either the published literature or large\nclinical databases. However, though drug interaction is an ideal area for\ntranslational research, the inclusion of literature mining methodologies in DDI\nworkflows is still very preliminary. One area that can benefit from literature\nmining is the automatic identification of a large number of potential DDIs,\nwhose pharmacological mechanisms and clinical significance can then be studied\nvia in vitro pharmacology and in populo pharmaco-epidemiology. Experiments. We\nimplemented a set of classifiers for identifying published articles relevant to\nexperimental pharmacokinetic DDI evidence. These documents are important for\nidentifying causal mechanisms behind putative drug-drug interactions, an\nimportant step in the extraction of large numbers of potential DDIs. We\nevaluate performance of several linear classifiers on PubMed abstracts, under\ndifferent feature transformation and dimensionality reduction methods. In\naddition, we investigate the performance benefits of including various\npublicly-available named entity recognition features, as well as a set of\ninternally-developed pharmacokinetic dictionaries. Results. We found that\nseveral classifiers performed well in distinguishing relevant and irrelevant\nabstracts. We found that the combination of unigram and bigram textual features\ngave better performance than unigram features alone, and also that\nnormalization transforms that adjusted for feature frequency and document\nlength improved classification. For some classifiers, such as linear\ndiscriminant analysis (LDA), proper dimensionality reduction had a large impact\non performance. Finally, the inclusion of NER features and dictionaries was\nfound not to help classification.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 11:34:57 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Kolchinsky", "Artemy", ""], ["Louren\u00e7o", "An\u00e1lia", ""], ["Li", "Lang", ""], ["Rocha", "Luis M.", ""]]}, {"id": "1210.0758", "submitter": "Daniele Cerra", "authors": "Daniele Cerra and Mihai Datcu", "title": "A fast compression-based similarity measure with applications to\n  content-based image retrieval", "comments": "Pre-print", "journal-ref": "Journal of Visual Communication and Image Representation, vol. 23,\n  no. 2, pp. 293-302, 2012", "doi": "10.1016/j.jvcir.2011.10.009", "report-no": null, "categories": "stat.ML cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compression-based similarity measures are effectively employed in\napplications on diverse data types with a basically parameter-free approach.\nNevertheless, there are problems in applying these techniques to\nmedium-to-large datasets which have been seldom addressed. This paper proposes\na similarity measure based on compression with dictionaries, the Fast\nCompression Distance (FCD), which reduces the complexity of these methods,\nwithout degradations in performance. On its basis a content-based color image\nretrieval system is defined, which can be compared to state-of-the-art methods\nbased on invariant color features. Through the FCD a better understanding of\ncompression-based techniques is achieved, by performing experiments on datasets\nwhich are larger than the ones analyzed so far in literature.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 13:04:49 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Cerra", "Daniele", ""], ["Datcu", "Mihai", ""]]}, {"id": "1210.0762", "submitter": "Fabrice Rossi", "authors": "Mohamed Khalil El Mahrsi (LTCI), Fabrice Rossi (SAMM)", "title": "Graph-Based Approaches to Clustering Network-Constrained Trajectory Data", "comments": null, "journal-ref": "Workshop on New Frontiers in Mining Complex Patterns (NFMCP 2012),\n  held at ECML-PKDD 2012, Bristol : United Kingdom (2012)", "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though clustering trajectory data attracted considerable attention in\nthe last few years, most of prior work assumed that moving objects can move\nfreely in an euclidean space and did not consider the eventual presence of an\nunderlying road network and its influence on evaluating the similarity between\ntrajectories. In this paper, we present two approaches to clustering\nnetwork-constrained trajectory data. The first approach discovers clusters of\ntrajectories that traveled along the same parts of the road network. The second\napproach is segment-oriented and aims to group together road segments based on\ntrajectories that they have in common. Both approaches use a graph model to\ndepict the interactions between observations w.r.t. their similarity and\ncluster this similarity graph using a community detection algorithm. We also\npresent experimental results obtained on synthetic data to showcase our\npropositions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 13:17:33 GMT"}], "update_date": "2012-10-04", "authors_parsed": [["Mahrsi", "Mohamed Khalil El", "", "LTCI"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1210.0824", "submitter": "Zoltan Szabo", "authors": "Zoltan Szabo, Andras Lorincz", "title": "Distributed High Dimensional Information Theoretical Image Registration\n  via Random Projections", "comments": null, "journal-ref": "Zoltan Szabo, Andras Lorincz: Distributed High Dimensional\n  Information Theoretical Image Registration via Random Projections. Digital\n  Signal Processing, 22(6):894-902, 2012", "doi": "10.1016/j.dsp.2012.04.018", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information theoretical measures, such as entropy, mutual information, and\nvarious divergences, exhibit robust characteristics in image registration\napplications. However, the estimation of these quantities is computationally\nintensive in high dimensions. On the other hand, consistent estimation from\npairwise distances of the sample points is possible, which suits random\nprojection (RP) based low dimensional embeddings. We adapt the RP technique to\nthis task by means of a simple ensemble method. To the best of our knowledge,\nthis is the first distributed, RP based information theoretical image\nregistration approach. The efficiency of the method is demonstrated through\nnumerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 16:08:53 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Szabo", "Zoltan", ""], ["Lorincz", "Andras", ""]]}, {"id": "1210.0864", "submitter": "Ilias Diakonikolas", "authors": "Siu-on Chan, Ilias Diakonikolas, Rocco A. Servedio, Xiaorui Sun", "title": "Learning mixtures of structured distributions over discrete domains", "comments": "preliminary full version of soda'13 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathfrak{C}$ be a class of probability distributions over the discrete\ndomain $[n] = \\{1,...,n\\}.$ We show that if $\\mathfrak{C}$ satisfies a rather\ngeneral condition -- essentially, that each distribution in $\\mathfrak{C}$ can\nbe well-approximated by a variable-width histogram with few bins -- then there\nis a highly efficient (both in terms of running time and sample complexity)\nalgorithm that can learn any mixture of $k$ unknown distributions from\n$\\mathfrak{C}.$\n  We analyze several natural types of distributions over $[n]$, including\nlog-concave, monotone hazard rate and unimodal distributions, and show that\nthey have the required structural property of being well-approximated by a\nhistogram with few bins. Applying our general algorithm, we obtain\nnear-optimally efficient algorithms for all these mixture learning problems.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2012 18:07:13 GMT"}], "update_date": "2012-10-03", "authors_parsed": [["Chan", "Siu-on", ""], ["Diakonikolas", "Ilias", ""], ["Servedio", "Rocco A.", ""], ["Sun", "Xiaorui", ""]]}, {"id": "1210.0954", "submitter": "Guo-Jun Qi", "authors": "Guo-Jun Qi, Charu Aggarwal, Pierre Moulin, Thomas Huang", "title": "Learning from Collective Intelligence in Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective intelligence, which aggregates the shared information from large\ncrowds, is often negatively impacted by unreliable information sources with the\nlow quality data. This becomes a barrier to the effective use of collective\nintelligence in a variety of applications. In order to address this issue, we\npropose a probabilistic model to jointly assess the reliability of sources and\nfind the true data. We observe that different sources are often not independent\nof each other. Instead, sources are prone to be mutually influenced, which\nmakes them dependent when sharing information with each other. High dependency\nbetween sources makes collective intelligence vulnerable to the overuse of\nredundant (and possibly incorrect) information from the dependent sources.\nThus, we reveal the latent group structure among dependent sources, and\naggregate the information at the group level rather than from individual\nsources directly. This can prevent the collective intelligence from being\ninappropriately dominated by dependent sources. We will also explicitly reveal\nthe reliability of groups, and minimize the negative impacts of unreliable\ngroups. Experimental results on real-world data sets show the effectiveness of\nthe proposed approach with respect to existing algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2012 01:34:00 GMT"}], "update_date": "2012-10-04", "authors_parsed": [["Qi", "Guo-Jun", ""], ["Aggarwal", "Charu", ""], ["Moulin", "Pierre", ""], ["Huang", "Thomas", ""]]}, {"id": "1210.1104", "submitter": "Arturo Ribes", "authors": "Arturo Ribes, Jes\\'us Cerquides, Yiannis Demiris and Ram\\'on L\\'opez\n  de M\\'antaras", "title": "Sensory Anticipation of Optical Flow in Mobile Robotics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to anticipate dangerous events, like a collision, an agent needs to\nmake long-term predictions. However, those are challenging due to uncertainties\nin internal and external variables and environment dynamics. A sensorimotor\nmodel is acquired online by the mobile robot using a state-of-the-art method\nthat learns the optical flow distribution in images, both in space and time.\nThe learnt model is used to anticipate the optical flow up to a given time\nhorizon and to predict an imminent collision by using reinforcement learning.\nWe demonstrate that multi-modal predictions reduce to simpler distributions\nonce actions are taken into account.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2012 13:36:32 GMT"}], "update_date": "2012-10-04", "authors_parsed": [["Ribes", "Arturo", ""], ["Cerquides", "Jes\u00fas", ""], ["Demiris", "Yiannis", ""], ["de M\u00e1ntaras", "Ram\u00f3n L\u00f3pez", ""]]}, {"id": "1210.1121", "submitter": "Krishnakumar Balasubramanian", "authors": "Krishnakumar Balasubramanian, Kai Yu, Guy Lebanon", "title": "Smooth Sparse Coding via Marginal Regression for Learning Sparse\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a novel framework for learning sparse representations,\nbased on two statistical techniques: kernel smoothing and marginal regression.\nThe proposed approach provides a flexible framework for incorporating feature\nsimilarity or temporal information present in data sets, via non-parametric\nkernel smoothing. We provide generalization bounds for dictionary learning\nusing smooth sparse coding and show how the sample complexity depends on the L1\nnorm of kernel function used. Furthermore, we propose using marginal regression\nfor obtaining sparse codes, which significantly improves the speed and allows\none to scale to large dictionary sizes easily. We demonstrate the advantages of\nthe proposed approach, both in terms of accuracy and speed by extensive\nexperimentation on several real data sets. In addition, we demonstrate how the\nproposed approach could be used for improving semi-supervised sparse coding.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2012 14:26:59 GMT"}], "update_date": "2012-10-04", "authors_parsed": [["Balasubramanian", "Krishnakumar", ""], ["Yu", "Kai", ""], ["Lebanon", "Guy", ""]]}, {"id": "1210.1190", "submitter": "Abhishek Kumar", "authors": "Abhishek Kumar, Vikas Sindhwani, Prabhanjan Kambadur", "title": "Fast Conical Hull Algorithms for Near-separable Non-negative Matrix\n  Factorization", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The separability assumption (Donoho & Stodden, 2003; Arora et al., 2012)\nturns non-negative matrix factorization (NMF) into a tractable problem.\nRecently, a new class of provably-correct NMF algorithms have emerged under\nthis assumption. In this paper, we reformulate the separable NMF problem as\nthat of finding the extreme rays of the conical hull of a finite set of\nvectors. From this geometric perspective, we derive new separable NMF\nalgorithms that are highly scalable and empirically noise robust, and have\nseveral other favorable properties in relation to existing methods. A parallel\nimplementation of our algorithm demonstrates high scalability on shared- and\ndistributed-memory machines.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2012 18:37:47 GMT"}], "update_date": "2012-10-04", "authors_parsed": [["Kumar", "Abhishek", ""], ["Sindhwani", "Vikas", ""], ["Kambadur", "Prabhanjan", ""]]}, {"id": "1210.1258", "submitter": "Mariya Ishteva", "authors": "Mariya Ishteva, Haesun Park, Le Song", "title": "Unfolding Latent Tree Structures using 4th Order Tensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering the latent structure from many observed variables is an important\nyet challenging learning task. Existing approaches for discovering latent\nstructures often require the unknown number of hidden states as an input. In\nthis paper, we propose a quartet based approach which is \\emph{agnostic} to\nthis number. The key contribution is a novel rank characterization of the\ntensor associated with the marginal distribution of a quartet. This\ncharacterization allows us to design a \\emph{nuclear norm} based test for\nresolving quartet relations. We then use the quartet test as a subroutine in a\ndivide-and-conquer algorithm for recovering the latent tree structure. Under\nmild conditions, the algorithm is consistent and its error probability decays\nexponentially with increasing sample size. We demonstrate that the proposed\napproach compares favorably to alternatives. In a real world stock dataset, it\nalso discovers meaningful groupings of variables, and produces a model that\nfits the data better.\n", "versions": [{"version": "v1", "created": "Wed, 3 Oct 2012 23:30:24 GMT"}], "update_date": "2012-10-05", "authors_parsed": [["Ishteva", "Mariya", ""], ["Park", "Haesun", ""], ["Song", "Le", ""]]}, {"id": "1210.1317", "submitter": "Phong Nguyen", "authors": "Phong Nguyen, Jun Wang, Melanie Hilario and Alexandros Kalousis", "title": "Learning Heterogeneous Similarity Measures for Hybrid-Recommendations in\n  Meta-Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of meta-mining has appeared recently and extends the traditional\nmeta-learning in two ways. First it does not learn meta-models that provide\nsupport only for the learning algorithm selection task but ones that support\nthe whole data-mining process. In addition it abandons the so called black-box\napproach to algorithm description followed in meta-learning. Now in addition to\nthe datasets, algorithms also have descriptors, workflows as well. For the\nlatter two these descriptions are semantic, describing properties of the\nalgorithms. With the availability of descriptors both for datasets and data\nmining workflows the traditional modelling techniques followed in\nmeta-learning, typically based on classification and regression algorithms, are\nno longer appropriate. Instead we are faced with a problem the nature of which\nis much more similar to the problems that appear in recommendation systems. The\nmost important meta-mining requirements are that suggestions should use only\ndatasets and workflows descriptors and the cold-start problem, e.g. providing\nworkflow suggestions for new datasets.\n  In this paper we take a different view on the meta-mining modelling problem\nand treat it as a recommender problem. In order to account for the meta-mining\nspecificities we derive a novel metric-based-learning recommender approach. Our\nmethod learns two homogeneous metrics, one in the dataset and one in the\nworkflow space, and a heterogeneous one in the dataset-workflow space. All\nlearned metrics reflect similarities established from the dataset-workflow\npreference matrix. We demonstrate our method on meta-mining over biological\n(microarray datasets) problems. The application of our method is not limited to\nthe meta-mining problem, its formulations is general enough so that it can be\napplied on problems with similar requirements.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2012 07:17:37 GMT"}], "update_date": "2012-10-05", "authors_parsed": [["Nguyen", "Phong", ""], ["Wang", "Jun", ""], ["Hilario", "Melanie", ""], ["Kalousis", "Alexandros", ""]]}, {"id": "1210.1461", "submitter": "Shusen Wang", "authors": "Shusen Wang, Zhihua Zhang, Jian Li", "title": "A Scalable CUR Matrix Decomposition Algorithm: Lower Time Complexity and\n  Tighter Bound", "comments": "accepted by NIPS 2012", "journal-ref": "Shusen Wang and Zhihua Zhang. A Scalable CUR Matrix Decomposition\n  Algorithm: Lower Time Complexity and Tighter Bound. In Advances in Neural\n  Information Processing Systems 25, 2012", "doi": null, "report-no": null, "categories": "cs.LG cs.DM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CUR matrix decomposition is an important extension of Nystr\\\"{o}m\napproximation to a general matrix. It approximates any data matrix in terms of\na small number of its columns and rows. In this paper we propose a novel\nrandomized CUR algorithm with an expected relative-error bound. The proposed\nalgorithm has the advantages over the existing relative-error CUR algorithms\nthat it possesses tighter theoretical bound and lower time complexity, and that\nit can avoid maintaining the whole data matrix in main memory. Finally,\nexperiments on several real-world datasets demonstrate significant improvement\nover the existing relative-error algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2012 14:23:34 GMT"}], "update_date": "2012-10-05", "authors_parsed": [["Wang", "Shusen", ""], ["Zhang", "Zhihua", ""], ["Li", "Jian", ""]]}, {"id": "1210.1766", "submitter": "Jun Zhu", "authors": "Jun Zhu, Ning Chen, and Eric P. Xing", "title": "Bayesian Inference with Posterior Regularization and applications to\n  Infinite Latent SVMs", "comments": "49 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Bayesian models, especially nonparametric Bayesian methods, rely on\nspecially conceived priors to incorporate domain knowledge for discovering\nimproved latent representations. While priors can affect posterior\ndistributions through Bayes' rule, imposing posterior regularization is\narguably more direct and in some cases more natural and general. In this paper,\nwe present regularized Bayesian inference (RegBayes), a novel computational\nframework that performs posterior inference with a regularization term on the\ndesired post-data posterior distribution under an information theoretical\nformulation. RegBayes is more flexible than the procedure that elicits expert\nknowledge via priors, and it covers both directed Bayesian networks and\nundirected Markov networks whose Bayesian formulation results in hybrid chain\ngraph models. When the regularization is induced from a linear operator on the\nposterior distributions, such as the expectation operator, we present a general\nconvex-analysis theorem to characterize the solution of RegBayes. Furthermore,\nwe present two concrete examples of RegBayes, infinite latent support vector\nmachines (iLSVM) and multi-task infinite latent support vector machines\n(MT-iLSVM), which explore the large-margin idea in combination with a\nnonparametric Bayesian model for discovering predictive latent features for\nclassification and multi-task learning, respectively. We present efficient\ninference methods and report empirical studies on several benchmark datasets,\nwhich appear to demonstrate the merits inherited from both large-margin\nlearning and Bayesian nonparametrics. Such results were not available until\nnow, and contribute to push forward the interface between these two important\nsubfields, which have been largely treated as isolated in the community.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2012 14:10:20 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2013 09:33:44 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2014 06:31:12 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Zhu", "Jun", ""], ["Chen", "Ning", ""], ["Xing", "Eric P.", ""]]}, {"id": "1210.1928", "submitter": "Shrihari Vasudevan", "authors": "Shrihari Vasudevan and Arman Melkumyan and Steven Scheding", "title": "Information fusion in multi-task Gaussian processes", "comments": "53 pages, 33 figures; improved presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper evaluates heterogeneous information fusion using multi-task\nGaussian processes in the context of geological resource modeling.\nSpecifically, it empirically demonstrates that information integration across\nheterogeneous information sources leads to superior estimates of all the\nquantities being modeled, compared to modeling them individually. Multi-task\nGaussian processes provide a powerful approach for simultaneous modeling of\nmultiple quantities of interest while taking correlations between these\nquantities into consideration. Experiments are performed on large scale real\nsensor data.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2012 08:11:01 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2012 04:25:31 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2013 03:42:50 GMT"}], "update_date": "2013-09-06", "authors_parsed": [["Vasudevan", "Shrihari", ""], ["Melkumyan", "Arman", ""], ["Scheding", "Steven", ""]]}, {"id": "1210.1960", "submitter": "Wittawat Jitkrittum", "authors": "Wittawat Jitkrittum, Hirotaka Hachiya, Masashi Sugiyama", "title": "Feature Selection via L1-Penalized Squared-Loss Mutual Information", "comments": "25 pages", "journal-ref": null, "doi": "10.1587/transinf.E96.D.1513", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is a technique to screen out less important features. Many\nexisting supervised feature selection algorithms use redundancy and relevancy\nas the main criteria to select features. However, feature interaction,\npotentially a key characteristic in real-world problems, has not received much\nattention. As an attempt to take feature interaction into account, we propose\nL1-LSMI, an L1-regularization based algorithm that maximizes a squared-loss\nvariant of mutual information between selected features and outputs. Numerical\nresults show that L1-LSMI performs well in handling redundancy, detecting\nnon-linear dependency, and considering feature interaction.\n", "versions": [{"version": "v1", "created": "Sat, 6 Oct 2012 14:16:33 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Jitkrittum", "Wittawat", ""], ["Hachiya", "Hirotaka", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1210.2051", "submitter": "Achilles Beros", "authors": "Achilles Beros", "title": "Anomalous Vacillatory Learning", "comments": "6 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1986, Osherson, Stob and Weinstein asked whether two variants of anomalous\nvacillatory learning, TxtFex^*_* and TxtFext^*_*, could be distinguished. In\nboth, a machine is permitted to vacillate between a finite number of hypotheses\nand to make a finite number of errors. TxtFext^*_*-learning requires that\nhypotheses output infinitely often must describe the same finite variant of the\ncorrect set, while TxtFex^*_*-learning permits the learner to vacillate between\nfinitely many different finite variants of the correct set. In this paper we\nshow that TxtFex^*_* \\neq TxtFext^*_*, thereby answering the question posed by\nOsherson, \\textit{et al}. We prove this in a strong way by exhibiting a family\nin TxtFex^*_2 \\setminus {TxtFext}^*_*.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2012 13:21:17 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2013 23:01:08 GMT"}], "update_date": "2013-02-12", "authors_parsed": [["Beros", "Achilles", ""]]}, {"id": "1210.2085", "submitter": "John Duchi", "authors": "John C. Duchi and Michael I. Jordan and Martin J. Wainwright", "title": "Privacy Aware Learning", "comments": "60 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study statistical risk minimization problems under a privacy model in\nwhich the data is kept confidential even from the learner. In this local\nprivacy framework, we establish sharp upper and lower bounds on the convergence\nrates of statistical estimation procedures. As a consequence, we exhibit a\nprecise tradeoff between the amount of privacy the data preserves and the\nutility, as measured by convergence rate, of any statistical estimator or\nlearning procedure.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2012 18:27:03 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2013 17:53:36 GMT"}], "update_date": "2013-10-11", "authors_parsed": [["Duchi", "John C.", ""], ["Jordan", "Michael I.", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1210.2162", "submitter": "Peter Welinder", "authors": "Peter Welinder and Max Welling and Pietro Perona", "title": "Semisupervised Classifier Evaluation and Recalibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How many labeled examples are needed to estimate a classifier's performance\non a new dataset? We study the case where data is plentiful, but labels are\nexpensive. We show that by making a few reasonable assumptions on the structure\nof the data, it is possible to estimate performance curves, with confidence\nbounds, using a small number of ground truth labels. Our approach, which we\ncall Semisupervised Performance Evaluation (SPE), is based on a generative\nmodel for the classifier's confidence scores. In addition to estimating the\nperformance of classifiers on new datasets, SPE can be used to recalibrate a\nclassifier by re-estimating the class-conditional confidence distributions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2012 07:15:57 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Welinder", "Peter", ""], ["Welling", "Max", ""], ["Perona", "Pietro", ""]]}, {"id": "1210.2164", "submitter": "Yuheng Hu", "authors": "Yuheng Hu, Ajita John, Fei Wang, Doree Duncan Seligmann, Subbarao\n  Kambhampati", "title": "ET-LDA: Joint Topic Modeling For Aligning, Analyzing and Sensemaking of\n  Public Events and Their Twitter Feeds", "comments": "errors in reference, delete for now", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media channels such as Twitter have emerged as popular platforms for\ncrowds to respond to public events such as speeches, sports and debates. While\nthis promises tremendous opportunities to understand and make sense of the\nreception of an event from the social media, the promises come entwined with\nsignificant technical challenges. In particular, given an event and an\nassociated large scale collection of tweets, we need approaches to effectively\nalign tweets and the parts of the event they refer to. This in turn raises\nquestions about how to segment the event into smaller yet meaningful parts, and\nhow to figure out whether a tweet is a general one about the entire event or\nspecific one aimed at a particular segment of the event. In this work, we\npresent ET-LDA, an effective method for aligning an event and its tweets\nthrough joint statistical modeling of topical influences from the events and\ntheir associated tweets. The model enables the automatic segmentation of the\nevents and the characterization of tweets into two categories: (1) episodic\ntweets that respond specifically to the content in the segments of the events,\nand (2) steady tweets that respond generally about the events. We present an\nefficient inference method for this model, and a comprehensive evaluation of\nits effectiveness over existing methods. In particular, through a user study,\nwe demonstrate that users find the topics, the segments, the alignment, and the\nepisodic tweets discovered by ET-LDA to be of higher quality and more\ninteresting as compared to the state-of-the-art, with improvements in the range\nof 18-41%.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2012 07:24:38 GMT"}, {"version": "v2", "created": "Sun, 21 Oct 2012 08:57:20 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2012 05:48:55 GMT"}], "update_date": "2012-12-24", "authors_parsed": [["Hu", "Yuheng", ""], ["John", "Ajita", ""], ["Wang", "Fei", ""], ["Seligmann", "Doree Duncan", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1210.2179", "submitter": "Jia Zeng", "authors": "Jia Zeng, Zhi-Qiang Liu and Xiao-Qin Cao", "title": "Fast Online EM for Big Topic Modeling", "comments": "14 pages, 12 figures in IEEE Transactions on Knowledge and Data\n  Engineering, 2016", "journal-ref": null, "doi": "10.1109/TKDE.2015.2492565", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expectation-maximization (EM) algorithm can compute the\nmaximum-likelihood (ML) or maximum a posterior (MAP) point estimate of the\nmixture models or latent variable models such as latent Dirichlet allocation\n(LDA), which has been one of the most popular probabilistic topic modeling\nmethods in the past decade. However, batch EM has high time and space\ncomplexities to learn big LDA models from big data streams. In this paper, we\npresent a fast online EM (FOEM) algorithm that infers the topic distribution\nfrom the previously unseen documents incrementally with constant memory\nrequirements. Within the stochastic approximation framework, we show that FOEM\ncan converge to the local stationary point of the LDA's likelihood function. By\ndynamic scheduling for the fast speed and parameter streaming for the low\nmemory usage, FOEM is more efficient for some lifelong topic modeling tasks\nthan the state-of-the-art online LDA algorithms to handle both big data and big\nmodels (aka, big topic modeling) on just a PC.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2012 08:17:18 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2013 04:17:05 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2015 13:49:04 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Zeng", "Jia", ""], ["Liu", "Zhi-Qiang", ""], ["Cao", "Xiao-Qin", ""]]}, {"id": "1210.2289", "submitter": "Annie I-An Chen", "authors": "Annie I. Chen and Asuman Ozdaglar", "title": "A Fast Distributed Proximal-Gradient Method", "comments": "10 pages (including 2-page appendix); 1 figure; submitted to Allerton\n  2012 on July 10, 2012; accepted by Allerton 2012, October 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a distributed proximal-gradient method for optimizing the average\nof convex functions, each of which is the private local objective of an agent\nin a network with time-varying topology. The local objectives have distinct\ndifferentiable components, but they share a common nondifferentiable component,\nwhich has a favorable structure suitable for effective computation of the\nproximal operator. In our method, each agent iteratively updates its estimate\nof the global minimum by optimizing its local objective function, and\nexchanging estimates with others via communication in the network. Using\nNesterov-type acceleration techniques and multiple communication steps per\niteration, we show that this method converges at the rate 1/k (where k is the\nnumber of communication rounds between the agents), which is faster than the\nconvergence rate of the existing distributed methods for solving this problem.\nThe superior convergence rate of our method is also verified by numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2012 14:14:13 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Chen", "Annie I.", ""], ["Ozdaglar", "Asuman", ""]]}, {"id": "1210.2346", "submitter": "Tamir Hazan", "authors": "Tamir Hazan, Alexander Schwing, David McAllester and Raquel Urtasun", "title": "Blending Learning and Inference in Structured Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we derive an efficient algorithm to learn the parameters of\nstructured predictors in general graphical models. This algorithm blends the\nlearning and inference tasks, which results in a significant speedup over\ntraditional approaches, such as conditional random fields and structured\nsupport vector machines. For this purpose we utilize the structures of the\npredictors to describe a low dimensional structured prediction task which\nencourages local consistencies within the different structures while learning\nthe parameters of the model. Convexity of the learning task provides the means\nto enforce the consistencies between the different parts. The\ninference-learning blending algorithm that we propose is guaranteed to converge\nto the optimum of the low dimensional primal and dual programs. Unlike many of\nthe existing approaches, the inference-learning blending allows us to learn\nefficiently high-order graphical models, over regions of any size, and very\nlarge number of parameters. We demonstrate the effectiveness of our approach,\nwhile presenting state-of-the-art results in stereo estimation, semantic\nsegmentation, shape reconstruction, and indoor scene understanding.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2012 17:19:43 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2013 16:07:47 GMT"}], "update_date": "2013-09-02", "authors_parsed": [["Hazan", "Tamir", ""], ["Schwing", "Alexander", ""], ["McAllester", "David", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1210.2381", "submitter": "Shiva Kasiviswanathan", "authors": "Shiva Prasad Kasiviswanathan, Mark Rudelson, Adam Smith", "title": "The Power of Linear Reconstruction Attacks", "comments": "30 pages, to appear in ACM-SIAM Symposium on Discrete Algorithms\n  (SODA 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.LG math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the power of linear reconstruction attacks in statistical data\nprivacy, showing that they can be applied to a much wider range of settings\nthan previously understood. Linear attacks have been studied before (Dinur and\nNissim PODS'03, Dwork, McSherry and Talwar STOC'07, Kasiviswanathan, Rudelson,\nSmith and Ullman STOC'10, De TCC'12, Muthukrishnan and Nikolov STOC'12) but\nhave so far been applied only in settings with releases that are obviously\nlinear.\n  Consider a database curator who manages a database of sensitive information\nbut wants to release statistics about how a sensitive attribute (say, disease)\nin the database relates to some nonsensitive attributes (e.g., postal code,\nage, gender, etc). We show one can mount linear reconstruction attacks based on\nany release that gives: a) the fraction of records that satisfy a given\nnon-degenerate boolean function. Such releases include contingency tables\n(previously studied by Kasiviswanathan et al., STOC'10) as well as more complex\noutputs like the error rate of classifiers such as decision trees; b) any one\nof a large class of M-estimators (that is, the output of empirical risk\nminimization algorithms), including the standard estimators for linear and\nlogistic regression.\n  We make two contributions: first, we show how these types of releases can be\ntransformed into a linear format, making them amenable to existing\npolynomial-time reconstruction algorithms. This is already perhaps surprising,\nsince many of the above releases (like M-estimators) are obtained by solving\nhighly nonlinear formulations. Second, we show how to analyze the resulting\nattacks under various distributional assumptions on the data. Specifically, we\nconsider a setting in which the same statistic (either a) or b) above) is\nreleased about how the sensitive attribute relates to all subsets of size k\n(out of a total of d) nonsensitive boolean attributes.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2012 19:01:53 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Kasiviswanathan", "Shiva Prasad", ""], ["Rudelson", "Mark", ""], ["Smith", "Adam", ""]]}, {"id": "1210.2613", "submitter": "Vittorio Perduca", "authors": "Vittorio Perduca, Gregory Nuel", "title": "Measuring the Influence of Observations in HMMs through the\n  Kullback-Leibler Distance", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2012.2235830", "report-no": null, "categories": "cs.IT cs.LG math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We measure the influence of individual observations on the sequence of the\nhidden states of the Hidden Markov Model (HMM) by means of the Kullback-Leibler\ndistance (KLD). Namely, we consider the KLD between the conditional\ndistribution of the hidden states' chain given the complete sequence of\nobservations and the conditional distribution of the hidden chain given all the\nobservations but the one under consideration. We introduce a linear complexity\nalgorithm for computing the influence of all the observations. As an\nillustration, we investigate the application of our algorithm to the problem of\ndetecting outliers in HMM data series.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 14:30:51 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2012 16:48:18 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Perduca", "Vittorio", ""], ["Nuel", "Gregory", ""]]}, {"id": "1210.2640", "submitter": "Eric Eaton", "authors": "Eric Eaton, Marie desJardins, Sara Jacob", "title": "Multi-view constrained clustering with an incomplete mapping between\n  views", "comments": null, "journal-ref": "Knowledge and Information Systems 38(1): 231-257, 2014", "doi": "10.1007/s10115-012-0577-7", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view learning algorithms typically assume a complete bipartite mapping\nbetween the different views in order to exchange information during the\nlearning process. However, many applications provide only a partial mapping\nbetween the views, creating a challenge for current methods. To address this\nproblem, we propose a multi-view algorithm based on constrained clustering that\ncan operate with an incomplete mapping. Given a set of pairwise constraints in\neach view, our approach propagates these constraints using a local similarity\nmeasure to those instances that can be mapped to the other views, allowing the\npropagated constraints to be transferred across views via the partial mapping.\nIt uses co-EM to iteratively estimate the propagation within each view based on\nthe current clustering model, transfer the constraints across views, and then\nupdate the clustering model. By alternating the learning process between views,\nthis approach produces a unified clustering model that is consistent with all\nviews. We show that this approach significantly improves clustering performance\nover several other methods for transferring constraints and allows multi-view\nclustering to be reliably applied when given a limited mapping between the\nviews. Our evaluation reveals that the propagated constraints have high\nprecision with respect to the true clusters in the data, explaining their\nbenefit to clustering performance in both single- and multi-view learning\nscenarios.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 15:25:01 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Eaton", "Eric", ""], ["desJardins", "Marie", ""], ["Jacob", "Sara", ""]]}, {"id": "1210.2771", "submitter": "Zhixiang Eddie Xu", "authors": "Zhixiang Xu, Matt J. Kusner, Kilian Q. Weinberger, Minmin Chen", "title": "Cost-Sensitive Tree of Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, machine learning algorithms have successfully entered large-scale\nreal-world industrial applications (e.g. search engines and email spam\nfilters). Here, the CPU cost during test time must be budgeted and accounted\nfor. In this paper, we address the challenge of balancing the test-time cost\nand the classifier accuracy in a principled fashion. The test-time cost of a\nclassifier is often dominated by the computation required for feature\nextraction-which can vary drastically across eatures. We decrease this\nextraction time by constructing a tree of classifiers, through which test\ninputs traverse along individual paths. Each path extracts different features\nand is optimized for a specific sub-partition of the input space. By only\ncomputing features for inputs that benefit from them the most, our cost\nsensitive tree of classifiers can match the high accuracies of the current\nstate-of-the-art at a small fraction of the computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2012 22:17:42 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2012 21:45:56 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2013 17:56:54 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Xu", "Zhixiang", ""], ["Kusner", "Matt J.", ""], ["Weinberger", "Kilian Q.", ""], ["Chen", "Minmin", ""]]}, {"id": "1210.2984", "submitter": "Francesca A. Lisi", "authors": "Francesca A. Lisi", "title": "Learning Onto-Relational Rules with Inductive Logic Programming", "comments": "18 pages. arXiv admin note: text overlap with arXiv:1003.2586", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rules complement and extend ontologies on the Semantic Web. We refer to these\nrules as onto-relational since they combine DL-based ontology languages and\nKnowledge Representation formalisms supporting the relational data model within\nthe tradition of Logic Programming and Deductive Databases. Rule authoring is a\nvery demanding Knowledge Engineering task which can be automated though\npartially by applying Machine Learning algorithms. In this chapter we show how\nInductive Logic Programming (ILP), born at the intersection of Machine Learning\nand Logic Programming and considered as a major approach to Relational\nLearning, can be adapted to Onto-Relational Learning. For the sake of\nillustration, we provide details of a specific Onto-Relational Learning\nsolution to the problem of learning rule-based definitions of DL concepts and\nroles with ILP.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2012 16:56:41 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2012 18:25:34 GMT"}], "update_date": "2012-10-30", "authors_parsed": [["Lisi", "Francesca A.", ""]]}, {"id": "1210.3139", "submitter": "Pardeep Kumar", "authors": "Pardeep Kumar, Nitin, Vivek Kumar Sehgal and Durg Singh Chauhan", "title": "A Benchmark to Select Data Mining Based Classification Algorithms For\n  Business Intelligence And Decision Support Systems", "comments": "18 Pages, 11 Figures, 6 Tables, Journal", "journal-ref": "International Journal of Data Mining and Knowledge Discovery\n  Process, September 2012, ISSN: 2230-9608", "doi": "10.5121/ijdkp.2012.2503", "report-no": null, "categories": "cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DSS serve the management, operations, and planning levels of an organization\nand help to make decisions, which may be rapidly changing and not easily\nspecified in advance. Data mining has a vital role to extract important\ninformation to help in decision making of a decision support system.\nIntegration of data mining and decision support systems (DSS) can lead to the\nimproved performance and can enable the tackling of new types of problems.\nArtificial Intelligence methods are improving the quality of decision support,\nand have become embedded in many applications ranges from ant locking\nautomobile brakes to these days interactive search engines. It provides various\nmachine learning techniques to support data mining. The classification is one\nof the main and valuable tasks of data mining. Several types of classification\nalgorithms have been suggested, tested and compared to determine the future\ntrends based on unseen data. There has been no single algorithm found to be\nsuperior over all others for all data sets. The objective of this paper is to\ncompare various classification algorithms that have been frequently used in\ndata mining for decision support systems. Three decision trees based\nalgorithms, one artificial neural network, one statistical, one support vector\nmachines with and without ada boost and one clustering algorithm are tested and\ncompared on four data sets from different domains in terms of predictive\naccuracy, error rate, classification index, comprehensibility and training\ntime. Experimental results demonstrate that Genetic Algorithm (GA) and support\nvector machines based algorithms are better in terms of predictive accuracy.\nSVM without adaboost shall be the first choice in context of speed and\npredictive accuracy. Adaboost improves the accuracy of SVM but on the cost of\nlarge training time.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 06:43:56 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Kumar", "Pardeep", ""], ["Nitin", "", ""], ["Sehgal", "Vivek Kumar", ""], ["Chauhan", "Durg Singh", ""]]}, {"id": "1210.3288", "submitter": "Willie Neiswanger", "authors": "Willie Neiswanger, Frank Wood", "title": "Unsupervised Detection and Tracking of Arbitrary Objects with Dependent\n  Dirichlet Process Mixtures", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a technique for the unsupervised detection and tracking\nof arbitrary objects in videos. It is intended to reduce the need for detection\nand localization methods tailored to specific object types and serve as a\ngeneral framework applicable to videos with varied objects, backgrounds, and\nimage qualities. The technique uses a dependent Dirichlet process mixture\n(DDPM) known as the Generalized Polya Urn (GPUDDPM) to model image pixel data\nthat can be easily and efficiently extracted from the regions in a video that\nrepresent objects. This paper describes a specific implementation of the model\nusing spatial and color pixel data extracted via frame differencing and gives\ntwo algorithms for performing inference in the model to accomplish detection\nand tracking. This technique is demonstrated on multiple synthetic and\nbenchmark video datasets that illustrate its ability to, without modification,\ndetect and track objects with diverse physical characteristics moving over\nnon-uniform backgrounds and through occlusion.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 16:30:15 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Neiswanger", "Willie", ""], ["Wood", "Frank", ""]]}, {"id": "1210.3384", "submitter": "Shankar Vembu", "authors": "Wei Jiao, Shankar Vembu, Amit G. Deshwar, Lincoln Stein, Quaid Morris", "title": "Inferring clonal evolution of tumors from single nucleotide somatic\n  mutations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.PE q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-throughput sequencing allows the detection and quantification of\nfrequencies of somatic single nucleotide variants (SNV) in heterogeneous tumor\ncell populations. In some cases, the evolutionary history and population\nfrequency of the subclonal lineages of tumor cells present in the sample can be\nreconstructed from these SNV frequency measurements. However, automated methods\nto do this reconstruction are not available and the conditions under which\nreconstruction is possible have not been described.\n  We describe the conditions under which the evolutionary history can be\nuniquely reconstructed from SNV frequencies from single or multiple samples\nfrom the tumor population and we introduce a new statistical model, PhyloSub,\nthat infers the phylogeny and genotype of the major subclonal lineages\nrepresented in the population of cancer cells. It uses a Bayesian nonparametric\nprior over trees that groups SNVs into major subclonal lineages and\nautomatically estimates the number of lineages and their ancestry. We sample\nfrom the joint posterior distribution over trees to identify evolutionary\nhistories and cell population frequencies that have the highest probability of\ngenerating the observed SNV frequency data. When multiple phylogenies are\nconsistent with a given set of SNV frequencies, PhyloSub represents the\nuncertainty in the tumor phylogeny using a partial order plot. Experiments on a\nsimulated dataset and two real datasets comprising tumor samples from acute\nmyeloid leukemia and chronic lymphocytic leukemia patients demonstrate that\nPhyloSub can infer both linear (or chain) and branching lineages and its\ninferences are in good agreement with ground truth, where it is available.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2012 22:20:33 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2012 18:41:13 GMT"}, {"version": "v3", "created": "Sun, 16 Jun 2013 18:35:00 GMT"}, {"version": "v4", "created": "Sat, 2 Nov 2013 21:38:34 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Jiao", "Wei", ""], ["Vembu", "Shankar", ""], ["Deshwar", "Amit G.", ""], ["Stein", "Lincoln", ""], ["Morris", "Quaid", ""]]}, {"id": "1210.3456", "submitter": "Mingjun Zhong", "authors": "Mingjun Zhong, Rong Liu, Bo Liu", "title": "Bayesian Analysis for miRNA and mRNA Interactions Using Expression Data", "comments": "21 pages, 11 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG q-bio.GN q-bio.MN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MicroRNAs (miRNAs) are small RNA molecules composed of 19-22 nt, which play\nimportant regulatory roles in post-transcriptional gene regulation by\ninhibiting the translation of the mRNA into proteins or otherwise cleaving the\ntarget mRNA. Inferring miRNA targets provides useful information for\nunderstanding the roles of miRNA in biological processes that are potentially\ninvolved in complex diseases. Statistical methodologies for point estimation,\nsuch as the Least Absolute Shrinkage and Selection Operator (LASSO) algorithm,\nhave been proposed to identify the interactions of miRNA and mRNA based on\nsequence and expression data. In this paper, we propose using the Bayesian\nLASSO (BLASSO) and the non-negative Bayesian LASSO (nBLASSO) to analyse the\ninteractions between miRNA and mRNA using expression data. The proposed\nBayesian methods explore the posterior distributions for those parameters\nrequired to model the miRNA-mRNA interactions. These approaches can be used to\nobserve the inferred effects of the miRNAs on the targets by plotting the\nposterior distributions of those parameters. For comparison purposes, the Least\nSquares Regression (LSR), Ridge Regression (RR), LASSO, non-negative LASSO\n(nLASSO), and the proposed Bayesian approaches were applied to four public\ndatasets. We concluded that nLASSO and nBLASSO perform best in terms of\nsensitivity and specificity. Compared to the point estimate algorithms, which\nonly provide single estimates for those parameters, the Bayesian methods are\nmore meaningful and provide credible intervals, which take into account the\nuncertainty of the inferred interactions of the miRNA and mRNA. Furthermore,\nBayesian methods naturally provide statistical significance to select\nconvincing inferred interactions, while point estimate algorithms require a\nmanually chosen threshold, which is less meaningful, to choose the possible\ninteractions.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2012 09:03:14 GMT"}, {"version": "v2", "created": "Mon, 30 Jun 2014 10:16:51 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Zhong", "Mingjun", ""], ["Liu", "Rong", ""], ["Liu", "Bo", ""]]}, {"id": "1210.3926", "submitter": "Julian McAuley", "authors": "Julian McAuley, Jure Leskovec, Dan Jurafsky", "title": "Learning Attitudes and Attributes from Multi-Aspect Reviews", "comments": "11 pages, 6 figures, extended version of our ICDM 2012 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of online reviews consist of plain-text feedback together with a\nsingle numeric score. However, there are multiple dimensions to products and\nopinions, and understanding the `aspects' that contribute to users' ratings may\nhelp us to better understand their individual preferences. For example, a\nuser's impression of an audiobook presumably depends on aspects such as the\nstory and the narrator, and knowing their opinions on these aspects may help us\nto recommend better products. In this paper, we build models for rating systems\nin which such dimensions are explicit, in the sense that users leave separate\nratings for each aspect of a product. By introducing new corpora consisting of\nfive million reviews, rated with between three and six aspects, we evaluate our\nmodels on three prediction tasks: First, we use our model to uncover which\nparts of a review discuss which of the rated aspects. Second, we use our model\nto summarize reviews, which for us means finding the sentences that best\nexplain a user's rating. Finally, since aspect ratings are optional in many of\nthe datasets we consider, we use our model to recover those ratings that are\nmissing from a user's evaluation. Our model matches state-of-the-art approaches\non existing small-scale datasets, while scaling to the real-world datasets we\nintroduce. Moreover, our model is able to `disentangle' content and sentiment\nwords: we automatically learn content words that are indicative of a particular\naspect as well as the aspect-specific sentiment words that are indicative of a\nparticular rating.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 07:36:57 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2012 16:14:35 GMT"}], "update_date": "2012-11-01", "authors_parsed": [["McAuley", "Julian", ""], ["Leskovec", "Jure", ""], ["Jurafsky", "Dan", ""]]}, {"id": "1210.4006", "submitter": "Maayan Harel Maayan Harel", "authors": "Maayan Harel and Shie Mannor", "title": "The Perturbed Variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new discrepancy score between two distributions that gives an\nindication on their similarity. While much research has been done to determine\nif two samples come from exactly the same distribution, much less research\nconsidered the problem of determining if two finite samples come from similar\ndistributions. The new score gives an intuitive interpretation of similarity;\nit optimally perturbs the distributions so that they best fit each other. The\nscore is defined between distributions, and can be efficiently estimated from\nsamples. We provide convergence bounds of the estimated score, and develop\nhypothesis testing procedures that test if two data sets come from similar\ndistributions. The statistical power of this procedures is presented in\nsimulations. We also compare the score's capacity to detect similarity with\nthat of other known measures on real data.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 12:43:03 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Harel", "Maayan", ""], ["Mannor", "Shie", ""]]}, {"id": "1210.4081", "submitter": "Bogdan Savchynskyy", "authors": "Bogdan Savchynskyy and Stefan Schmidt", "title": "Getting Feasible Variable Estimates From Infeasible Ones: MRF Local\n  Polytope Study", "comments": "20 page, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CV cs.DS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method for construction of approximate feasible primal\nsolutions from dual ones for large-scale optimization problems possessing\ncertain separability properties. Whereas infeasible primal estimates can\ntypically be produced from (sub-)gradients of the dual function, it is often\nnot easy to project them to the primal feasible set, since the projection\nitself has a complexity comparable to the complexity of the initial problem. We\npropose an alternative efficient method to obtain feasibility and show that its\nproperties influencing the convergence to the optimum are similar to the\nproperties of the Euclidean projection. We apply our method to the local\npolytope relaxation of inference problems for Markov Random Fields and\ndemonstrate its superiority over existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 15:55:34 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Savchynskyy", "Bogdan", ""], ["Schmidt", "Stefan", ""]]}, {"id": "1210.4184", "submitter": "Sotirios Chatzis", "authors": "Sotirios P. Chatzis and Dimitrios Korkinof and Yiannis Demiris", "title": "The Kernel Pitman-Yor Process", "comments": "This is a Technical Report summarizing our ongoing work on the Kernel\n  Pitman-Yor Process. Experiments will be added by D. Korkinof prior to journal\n  or conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose the kernel Pitman-Yor process (KPYP) for\nnonparametric clustering of data with general spatial or temporal\ninterdependencies. The KPYP is constructed by first introducing an infinite\nsequence of random locations. Then, based on the stick-breaking construction of\nthe Pitman-Yor process, we define a predictor-dependent random probability\nmeasure by considering that the discount hyperparameters of the\nBeta-distributed random weights (stick variables) of the process are not\nuniform among the weights, but controlled by a kernel function expressing the\nproximity between the location assigned to each weight and the given\npredictors.\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2012 20:14:23 GMT"}], "update_date": "2012-10-17", "authors_parsed": [["Chatzis", "Sotirios P.", ""], ["Korkinof", "Dimitrios", ""], ["Demiris", "Yiannis", ""]]}, {"id": "1210.4276", "submitter": "Bertrand Lebichot", "authors": "Bertrand Lebichot, Ilkka Kivim\\\"aki, Kevin Fran\\c{c}oisse and Marco\n  Saerens", "title": "Semi-Supervised Classification Through the Bag-of-Paths Group\n  Betweenness", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel, well-founded, betweenness measure, called the\nBag-of-Paths (BoP) betweenness, as well as its extension, the BoP group\nbetweenness, to tackle semisupervised classification problems on weighted\ndirected graphs. The objective of semi-supervised classification is to assign a\nlabel to unlabeled nodes using the whole topology of the graph and the labeled\nnodes at our disposal. The BoP betweenness relies on a bag-of-paths framework\nassigning a Boltzmann distribution on the set of all possible paths through the\nnetwork such that long (high-cost) paths have a low probability of being picked\nfrom the bag, while short (low-cost) paths have a high probability of being\npicked. Within that context, the BoP betweenness of node j is defined as the\nsum of the a posteriori probabilities that node j lies in-between two arbitrary\nnodes i, k, when picking a path starting in i and ending in k. Intuitively, a\nnode typically receives a high betweenness if it has a large probability of\nappearing on paths connecting two arbitrary nodes of the network. This quantity\ncan be computed in closed form by inverting a n x n matrix where n is the\nnumber of nodes. For the group betweenness, the paths are constrained to start\nand end in nodes within the same class, therefore defining a group betweenness\nfor each class. Unlabeled nodes are then classified according to the class\nshowing the highest group betweenness. Experiments on various real-world data\nsets show that BoP group betweenness outperforms all the tested state\nof-the-art methods. The benefit of the BoP betweenness is particularly\nnoticeable when only a few labeled nodes are available.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 07:31:59 GMT"}], "update_date": "2012-10-17", "authors_parsed": [["Lebichot", "Bertrand", ""], ["Kivim\u00e4ki", "Ilkka", ""], ["Fran\u00e7oisse", "Kevin", ""], ["Saerens", "Marco", ""]]}, {"id": "1210.4347", "submitter": "Krikamol Muandet", "authors": "Krikamol Muandet", "title": "Hilbert Space Embedding for Dirichlet Process Mixtures", "comments": "NIPS 2012 Workshop in confluence between kernel methods and graphical\n  models", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a Hilbert space embedding for Dirichlet Process mixture\nmodels via a stick-breaking construction of Sethuraman. Although Bayesian\nnonparametrics offers a powerful approach to construct a prior that avoids the\nneed to specify the model size/complexity explicitly, an exact inference is\noften intractable. On the other hand, frequentist approaches such as kernel\nmachines, which suffer from the model selection/comparison problems, often\nbenefit from efficient learning algorithms. This paper discusses the\npossibility to combine the best of both worlds by using the Dirichlet Process\nmixture model as a case study.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 10:26:29 GMT"}], "update_date": "2012-10-17", "authors_parsed": [["Muandet", "Krikamol", ""]]}, {"id": "1210.4460", "submitter": "Yaman Aksu Ph.D.", "authors": "Yaman Aksu", "title": "Fast SVM-based Feature Elimination Utilizing Data Radius, Hard-Margin,\n  Soft-Margin", "comments": "Incomplete but good, again. To Apr 28 version, made few misc text and\n  notation improvements including typo corrections, probably mostly in\n  Appendix, but probably best to read in whole again. New results for one of\n  the datasets (Leukemia gene dataset)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Margin maximization in the hard-margin sense, proposed as feature elimination\ncriterion by the MFE-LO method, is combined here with data radius utilization\nto further aim to lower generalization error, as several published bounds and\nbound-related formulations pertaining to lowering misclassification risk (or\nerror) pertain to radius e.g. product of squared radius and weight vector\nsquared norm. Additionally, we propose additional novel feature elimination\ncriteria that, while instead being in the soft-margin sense, too can utilize\ndata radius, utilizing previously published bound-related formulations for\napproaching radius for the soft-margin sense, whereby e.g. a focus was on the\nprinciple stated therein as \"finding a bound whose minima are in a region with\nsmall leave-one-out values may be more important than its tightness\". These\nadditional criteria we propose combine radius utilization with a novel and\ncomputationally low-cost soft-margin light classifier retraining approach we\ndevise named QP1; QP1 is the soft-margin alternative to the hard-margin LO. We\ncorrect an error in the MFE-LO description, find MFE-LO achieves the highest\ngeneralization accuracy among the previously published margin-based feature\nelimination (MFE) methods, discuss some limitations of MFE-LO, and find our\nnovel methods herein outperform MFE-LO, attain lower test set classification\nerror rate. On several datasets that each both have a large number of features\nand fall into the `large features few samples' dataset category, and on\ndatasets with lower (low-to-intermediate) number of features, our novel methods\ngive promising results. Especially, among our methods the tunable ones, that do\nnot employ (the non-tunable) LO approach, can be tuned more aggressively in the\nfuture than herein, to aim to demonstrate for them even higher performance than\nherein.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 15:54:36 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2013 16:28:17 GMT"}, {"version": "v3", "created": "Mon, 28 Apr 2014 21:15:46 GMT"}, {"version": "v4", "created": "Sun, 11 May 2014 11:47:07 GMT"}], "update_date": "2014-05-13", "authors_parsed": [["Aksu", "Yaman", ""]]}, {"id": "1210.4481", "submitter": "Yingzhen Yang", "authors": "Yingzhen Yang, Xinqi Chu, Tian-Tsong Ng, Alex Yong-Sang Chia,\n  Shuicheng Yan, Thomas S. Huang", "title": "Epitome for Automatic Image Colorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image colorization adds color to grayscale images. It not only increases the\nvisual appeal of grayscale images, but also enriches the information contained\nin scientific images that lack color information. Most existing methods of\ncolorization require laborious user interaction for scribbles or image\nsegmentation. To eliminate the need for human labor, we develop an automatic\nimage colorization method using epitome. Built upon a generative graphical\nmodel, epitome is a condensed image appearance and shape model which also\nproves to be an effective summary of color information for the colorization\ntask. We train the epitome from the reference images and perform inference in\nthe epitome to colorize grayscale images, rendering better colorization results\nthan previous method in our experiments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2012 06:35:04 GMT"}], "update_date": "2012-10-17", "authors_parsed": [["Yang", "Yingzhen", ""], ["Chu", "Xinqi", ""], ["Ng", "Tian-Tsong", ""], ["Chia", "Alex Yong-Sang", ""], ["Yan", "Shuicheng", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1210.4601", "submitter": "Chunhua Shen", "authors": "Chunhua Shen, Sakrapee Paisitkriangkrai, Anton van den Hengel", "title": "A Direct Approach to Multi-class Boosting and Extensions", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting methods combine a set of moderately accurate weaklearners to form a\nhighly accurate predictor. Despite the practical importance of multi-class\nboosting, it has received far less attention than its binary counterpart. In\nthis work, we propose a fully-corrective multi-class boosting formulation which\ndirectly solves the multi-class problem without dividing it into multiple\nbinary classification problems. In contrast, most previous multi-class boosting\nalgorithms decompose a multi-boost problem into multiple binary boosting\nproblems. By explicitly deriving the Lagrange dual of the primal optimization\nproblem, we are able to construct a column generation-based fully-corrective\napproach to boosting which directly optimizes multi-class classification\nperformance. The new approach not only updates all weak learners' coefficients\nat every iteration, but does so in a manner flexible enough to accommodate\nvarious loss functions and regularizations. For example, it enables us to\nintroduce structural sparsity through mixed-norm regularization to promote\ngroup sparsity and feature sharing. Boosting with shared features is\nparticularly beneficial in complex prediction problems where features can be\nexpensive to compute. Our experiments on various data sets demonstrate that our\ndirect multi-class boosting generalizes as well as, or better than, a range of\ncompeting multi-class boosting methods. The end result is a highly effective\nand compact ensemble classifier which can be trained in a distributed fashion.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2012 00:22:31 GMT"}], "update_date": "2012-10-18", "authors_parsed": [["Shen", "Chunhua", ""], ["Paisitkriangkrai", "Sakrapee", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1210.4657", "submitter": "Hamidou Tembine", "authors": "Hamidou Tembine, Raul Tempone and Pedro Vilanova", "title": "Mean-Field Learning: a Survey", "comments": "36 pages. 5 figures. survey style", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.GT cs.MA math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study iterative procedures for stationary equilibria in\ngames with large number of players. Most of learning algorithms for games with\ncontinuous action spaces are limited to strict contraction best reply maps in\nwhich the Banach-Picard iteration converges with geometrical convergence rate.\nWhen the best reply map is not a contraction, Ishikawa-based learning is\nproposed. The algorithm is shown to behave well for Lipschitz continuous and\npseudo-contractive maps. However, the convergence rate is still unsatisfactory.\nSeveral acceleration techniques are presented. We explain how cognitive users\ncan improve the convergence rate based only on few number of measurements. The\nmethodology provides nice properties in mean field games where the payoff\nfunction depends only on own-action and the mean of the mean-field (first\nmoment mean-field games). A learning framework that exploits the structure of\nsuch games, called, mean-field learning, is proposed. The proposed mean-field\nlearning framework is suitable not only for games but also for non-convex\nglobal optimization problems. Then, we introduce mean-field learning without\nfeedback and examine the convergence to equilibria in beauty contest games,\nwhich have interesting applications in financial markets. Finally, we provide a\nfully distributed mean-field learning and its speedup versions for satisfactory\nsolution in wireless networks. We illustrate the convergence rate improvement\nwith numerical examples.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2012 07:51:56 GMT"}], "update_date": "2012-10-18", "authors_parsed": [["Tembine", "Hamidou", ""], ["Tempone", "Raul", ""], ["Vilanova", "Pedro", ""]]}, {"id": "1210.4695", "submitter": "David Balduzzi", "authors": "David Balduzzi", "title": "Regulating the information in spikes: a useful bias", "comments": "NIPS 2012 workshop on Information in Perception and Action", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bias/variance tradeoff is fundamental to learning: increasing a model's\ncomplexity can improve its fit on training data, but potentially worsens\nperformance on future samples. Remarkably, however, the human brain\neffortlessly handles a wide-range of complex pattern recognition tasks. On the\nbasis of these conflicting observations, it has been argued that useful biases\nin the form of \"generic mechanisms for representation\" must be hardwired into\ncortex (Geman et al).\n  This note describes a useful bias that encourages cooperative learning which\nis both biologically plausible and rigorously justified.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2012 11:12:02 GMT"}], "update_date": "2012-10-18", "authors_parsed": [["Balduzzi", "David", ""]]}, {"id": "1210.4792", "submitter": "Vikas Sindhwani", "authors": "Vikas Sindhwani and Minh Ha Quang and Aurelie C. Lozano", "title": "Scalable Matrix-valued Kernel Learning for High-dimensional Nonlinear\n  Multivariate Regression and Granger Causality", "comments": "22 pages. Presentation changes; Corrections made to Theorem 2\n  (section 6.2) in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general matrix-valued multiple kernel learning framework for\nhigh-dimensional nonlinear multivariate regression problems. This framework\nallows a broad class of mixed norm regularizers, including those that induce\nsparsity, to be imposed on a dictionary of vector-valued Reproducing Kernel\nHilbert Spaces. We develop a highly scalable and eigendecomposition-free\nalgorithm that orchestrates two inexact solvers for simultaneously learning\nboth the input and output components of separable matrix-valued kernels. As a\nkey application enabled by our framework, we show how high-dimensional causal\ninference tasks can be naturally cast as sparse function estimation problems,\nleading to novel nonlinear extensions of a class of Graphical Granger Causality\ntechniques. Our algorithmic developments and extensive empirical studies are\ncomplemented by theoretical analyses in terms of Rademacher generalization\nbounds.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2012 16:57:48 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2013 02:19:07 GMT"}], "update_date": "2013-03-11", "authors_parsed": [["Sindhwani", "Vikas", ""], ["Quang", "Minh Ha", ""], ["Lozano", "Aurelie C.", ""]]}, {"id": "1210.4839", "submitter": "Stephane Caron", "authors": "Stephane Caron, Branislav Kveton, Marc Lelarge, Smriti Bhagat", "title": "Leveraging Side Observations in Stochastic Bandits", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-142-151", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers stochastic bandits with side observations, a model that\naccounts for both the exploration/exploitation dilemma and relationships\nbetween arms. In this setting, after pulling an arm i, the decision maker also\nobserves the rewards for some other actions related to i. We will see that this\nmodel is suited to content recommendation in social networks, where users'\nreactions may be endorsed or not by their friends. We provide efficient\nalgorithms based on upper confidence bounds (UCBs) to leverage this additional\ninformation and derive new bounds improving on standard regret guarantees. We\nalso evaluate these policies in the context of movie recommendation in social\nnetworks: experiments on real datasets show substantial learning rate speedups\nranging from 2.2x to 14x on dense networks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:32:09 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Caron", "Stephane", ""], ["Kveton", "Branislav", ""], ["Lelarge", "Marc", ""], ["Bhagat", "Smriti", ""]]}, {"id": "1210.4841", "submitter": "Dhruv Batra", "authors": "Dhruv Batra", "title": "An Efficient Message-Passing Algorithm for the M-Best MAP Problem", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-121-130", "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much effort has been directed at algorithms for obtaining the highest\nprobability configuration in a probabilistic random field model known as the\nmaximum a posteriori (MAP) inference problem. In many situations, one could\nbenefit from having not just a single solution, but the top M most probable\nsolutions known as the M-Best MAP problem. In this paper, we propose an\nefficient message-passing based algorithm for solving the M-Best MAP problem.\nSpecifically, our algorithm solves the recently proposed Linear Programming\n(LP) formulation of M-Best MAP [7], while being orders of magnitude faster than\na generic LP-solver. Our approach relies on studying a particular partial\nLagrangian relaxation of the M-Best MAP LP which exposes a natural\ncombinatorial structure of the problem that we exploit.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:32:34 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Batra", "Dhruv", ""]]}, {"id": "1210.4843", "submitter": "Raman Arora", "authors": "Raman Arora, Ofer Dekel, Ambuj Tewari", "title": "Deterministic MDPs with Adversarial Rewards and Bandit Feedback", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-93-101", "categories": "cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Markov decision process with deterministic state transition\ndynamics, adversarially generated rewards that change arbitrarily from round to\nround, and a bandit feedback model in which the decision maker only observes\nthe rewards it receives. In this setting, we present a novel and efficient\nonline decision making algorithm named MarcoPolo. Under mild assumptions on the\nstructure of the transition dynamics, we prove that MarcoPolo enjoys a regret\nof O(T^(3/4)sqrt(log(T))) against the best deterministic policy in hindsight.\nSpecifically, our analysis does not rely on the stringent unichain assumption,\nwhich dominates much of the previous work on this topic.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:34:04 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Arora", "Raman", ""], ["Dekel", "Ofer", ""], ["Tewari", "Ambuj", ""]]}, {"id": "1210.4846", "submitter": "Saeed Amizadeh", "authors": "Saeed Amizadeh, Bo Thiesson, Milos Hauskrecht", "title": "Variational Dual-Tree Framework for Large-Scale Transition Matrix\n  Approximation", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-64-73", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, non-parametric methods utilizing random walks on graphs have\nbeen used to solve a wide range of machine learning problems, but in their\nsimplest form they do not scale well due to the quadratic complexity. In this\npaper, a new dual-tree based variational approach for approximating the\ntransition matrix and efficiently performing the random walk is proposed. The\napproach exploits a connection between kernel density estimation, mixture\nmodeling, and random walk on graphs in an optimization of the transition matrix\nfor the data graph that ties together edge transitions probabilities that are\nsimilar. Compared to the de facto standard approximation method based on\nk-nearestneighbors, we demonstrate order of magnitudes speedup without\nsacrificing accuracy for Label Propagation tasks on benchmark data sets in\nsemi-supervised learning.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:34:45 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Amizadeh", "Saeed", ""], ["Thiesson", "Bo", ""], ["Hauskrecht", "Milos", ""]]}, {"id": "1210.4850", "submitter": "Raja Hafiz Affandi", "authors": "Raja Hafiz Affandi, Alex Kulesza, Emily B. Fox", "title": "Markov Determinantal Point Processes", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-26-35", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A determinantal point process (DPP) is a random process useful for modeling\nthe combinatorial problem of subset selection. In particular, DPPs encourage a\nrandom subset Y to contain a diverse set of items selected from a base set Y.\nFor example, we might use a DPP to display a set of news headlines that are\nrelevant to a user's interests while covering a variety of topics. Suppose,\nhowever, that we are asked to sequentially select multiple diverse sets of\nitems, for example, displaying new headlines day-by-day. We might want these\nsets to be diverse not just individually but also through time, offering\nheadlines today that are unlike the ones shown yesterday. In this paper, we\nconstruct a Markov DPP (M-DPP) that models a sequence of random sets {Yt}. The\nproposed M-DPP defines a stationary process that maintains DPP margins.\nCrucially, the induced union process Zt = Yt u Yt-1 is also marginally\nDPP-distributed. Jointly, these properties imply that the sequence of random\nsets are encouraged to be diverse both at a given time step as well as across\ntime steps. We describe an exact, efficient sampling procedure, and a method\nfor incrementally learning a quality measure over items in the base set Y based\non external preferences. We apply the M-DPP to the task of sequentially\ndisplaying diverse and relevant news articles to a user with topic preferences.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:35:39 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Affandi", "Raja Hafiz", ""], ["Kulesza", "Alex", ""], ["Fox", "Emily B.", ""]]}, {"id": "1210.4851", "submitter": "Sreangsu Acharyya", "authors": "Sreangsu Acharyya, Oluwasanmi Koyejo, Joydeep Ghosh", "title": "Learning to Rank With Bregman Divergences and Monotone Retargeting", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-15-25", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel approach for learning to rank (LETOR) based on\nthe notion of monotone retargeting. It involves minimizing a divergence between\nall monotonic increasing transformations of the training scores and a\nparameterized prediction function. The minimization is both over the\ntransformations as well as over the parameters. It is applied to Bregman\ndivergences, a large class of \"distance like\" functions that were recently\nshown to be the unique class that is statistically consistent with the\nnormalized discounted gain (NDCG) criterion [19]. The algorithm uses\nalternating projection style updates, in which one set of simultaneous\nprojections can be computed independent of the Bregman divergence and the other\nreduces to parameter estimation of a generalized linear model. This results in\neasily implemented, efficiently parallelizable algorithm for the LETOR task\nthat enjoys global optimum guarantees under mild conditions. We present\nempirical results on benchmark datasets showing that this approach can\noutperform the state of the art NDCG consistent techniques.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:35:52 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Acharyya", "Sreangsu", ""], ["Koyejo", "Oluwasanmi", ""], ["Ghosh", "Joydeep", ""]]}, {"id": "1210.4855", "submitter": "Sunil Kumar Gupta", "authors": "Sunil Kumar Gupta, Dinh Q. Phung, Svetha Venkatesh", "title": "A Slice Sampler for Restricted Hierarchical Beta Process with\n  Applications to Shared Subspace Learning", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-316-325", "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical beta process has found interesting applications in recent years.\nIn this paper we present a modified hierarchical beta process prior with\napplications to hierarchical modeling of multiple data sources. The novel use\nof the prior over a hierarchical factor model allows factors to be shared\nacross different sources. We derive a slice sampler for this model, enabling\ntractable inference even when the likelihood and the prior over parameters are\nnon-conjugate. This allows the application of the model in much wider contexts\nwithout restrictions. We present two different data generative models a linear\nGaussianGaussian model for real valued data and a linear Poisson-gamma model\nfor count data. Encouraging transfer learning results are shown for two real\nworld applications text modeling and content based image retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:37:29 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Gupta", "Sunil Kumar", ""], ["Phung", "Dinh Q.", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1210.4856", "submitter": "Roger Grosse", "authors": "Roger Grosse, Ruslan R Salakhutdinov, William T. Freeman, Joshua B.\n  Tenenbaum", "title": "Exploiting compositionality to explore a large space of model structures", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-306-315", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent proliferation of richly structured probabilistic models raises the\nquestion of how to automatically determine an appropriate model for a dataset.\nWe investigate this question for a space of matrix decomposition models which\ncan express a variety of widely used models from unsupervised learning. To\nenable model selection, we organize these models into a context-free grammar\nwhich generates a wide variety of structures through the compositional\napplication of a few simple rules. We use our grammar to generically and\nefficiently infer latent components and estimate predictive likelihood for\nnearly 2500 structures using a small toolbox of reusable algorithms. Using a\ngreedy search over our grammar, we automatically choose the decomposition\nstructure from raw data by evaluating only a small fraction of all models. The\nproposed method typically finds the correct structure for synthetic data and\nbacks off gracefully to simpler models under heavy noise. It learns sensible\nstructures for datasets as diverse as image patches, motion capture, 20\nQuestions, and U.S. Senate votes, all using exactly the same code.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:37:41 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Grosse", "Roger", ""], ["Salakhutdinov", "Ruslan R", ""], ["Freeman", "William T.", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1210.4859", "submitter": "Dinesh Garg", "authors": "Dinesh Garg, Sourangshu Bhattacharya, S. Sundararajan, Shirish Shevade", "title": "Mechanism Design for Cost Optimal PAC Learning in the Presence of\n  Strategic Noisy Annotators", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-275-285", "categories": "cs.LG cs.GT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of Probably Approximate Correct (PAC) learning of a\nbinary classifier from noisy labeled examples acquired from multiple annotators\n(each characterized by a respective classification noise rate). First, we\nconsider the complete information scenario, where the learner knows the noise\nrates of all the annotators. For this scenario, we derive sample complexity\nbound for the Minimum Disagreement Algorithm (MDA) on the number of labeled\nexamples to be obtained from each annotator. Next, we consider the incomplete\ninformation scenario, where each annotator is strategic and holds the\nrespective noise rate as a private information. For this scenario, we design a\ncost optimal procurement auction mechanism along the lines of Myerson's optimal\nauction design framework in a non-trivial manner. This mechanism satisfies\nincentive compatibility property, thereby facilitating the learner to elicit\ntrue noise rates of all the annotators.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:38:13 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Garg", "Dinesh", ""], ["Bhattacharya", "Sourangshu", ""], ["Sundararajan", "S.", ""], ["Shevade", "Shirish", ""]]}, {"id": "1210.4860", "submitter": "Antonino Freno", "authors": "Antonino Freno, Mikaela Keller, Gemma C. Garriga, Marc Tommasi", "title": "Spectral Estimation of Conditional Random Graph Models for Large-Scale\n  Network Data", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-265-274", "categories": "cs.SI cs.LG physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models for graphs have been typically committed to strong prior\nassumptions concerning the form of the modeled distributions. Moreover, the\nvast majority of currently available models are either only suitable for\ncharacterizing some particular network properties (such as degree distribution\nor clustering coefficient), or they are aimed at estimating joint probability\ndistributions, which is often intractable in large-scale networks. In this\npaper, we first propose a novel network statistic, based on the Laplacian\nspectrum of graphs, which allows to dispense with any parametric assumption\nconcerning the modeled network properties. Second, we use the defined statistic\nto develop the Fiedler random graph model, switching the focus from the\nestimation of joint probability distributions to a more tractable conditional\nestimation setting. After analyzing the dependence structure characterizing\nFiedler random graphs, we evaluate them experimentally in edge prediction over\nseveral real-world networks, showing that they allow to reach a much higher\nprediction accuracy than various alternative statistical models.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:38:22 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Freno", "Antonino", ""], ["Keller", "Mikaela", ""], ["Garriga", "Gemma C.", ""], ["Tommasi", "Marc", ""]]}, {"id": "1210.4862", "submitter": "Miroslav Dudik", "authors": "Miroslav Dudik, Dumitru Erhan, John Langford, Lihong Li", "title": "Sample-efficient Nonstationary Policy Evaluation for Contextual Bandits", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-247-254", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and prove properties of a new offline policy evaluator for an\nexploration learning setting which is superior to previous evaluators. In\nparticular, it simultaneously and correctly incorporates techniques from\nimportance weighting, doubly robust evaluation, and nonstationary policy\nevaluation approaches. In addition, our approach allows generating longer\nhistories by careful control of a bias-variance tradeoff, and further decreases\nvariance by incorporating information about randomness of the target policy.\nEmpirical evidence from synthetic and realworld exploration learning problems\nshows the new evaluator successfully unifies previous approaches and uses\ninformation an order of magnitude more efficiently.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:38:45 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Dudik", "Miroslav", ""], ["Erhan", "Dumitru", ""], ["Langford", "John", ""], ["Li", "Lihong", ""]]}, {"id": "1210.4867", "submitter": "Jaesik Choi", "authors": "Jaesik Choi, Eyal Amir", "title": "Lifted Relational Variational Inference", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-196-206", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid continuous-discrete models naturally represent many real-world\napplications in robotics, finance, and environmental engineering. Inference\nwith large-scale models is challenging because relational structures\ndeteriorate rapidly during inference with observations. The main contribution\nof this paper is an efficient relational variational inference algorithm that\nfactors largescale probability models into simpler variational models, composed\nof mixtures of iid (Bernoulli) random variables. The algorithm takes\nprobability relational models of largescale hybrid systems and converts them to\na close-to-optimal variational models. Then, it efficiently calculates marginal\nprobabilities on the variational models by using a latent (or lifted) variable\nelimination or a lifted stochastic sampling. This inference is unique because\nit maintains the relational structure upon individual observations and during\ninference steps.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:39:37 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Choi", "Jaesik", ""], ["Amir", "Eyal", ""]]}, {"id": "1210.4869", "submitter": "Guang Ling", "authors": "Guang Ling, Haiqin Yang, Michael R. Lyu, Irwin King", "title": "Response Aware Model-Based Collaborative Filtering", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-501-510", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work on recommender systems mainly focus on fitting the ratings\nprovided by users. However, the response patterns, i.e., some items are rated\nwhile others not, are generally ignored. We argue that failing to observe such\nresponse patterns can lead to biased parameter estimation and sub-optimal model\nperformance. Although several pieces of work have tried to model users'\nresponse patterns, they miss the effectiveness and interpretability of the\nsuccessful matrix factorization collaborative filtering approaches. To bridge\nthe gap, in this paper, we unify explicit response models and PMF to establish\nthe Response Aware Probabilistic Matrix Factorization (RAPMF) framework. We\nshow that RAPMF subsumes PMF as a special case. Empirically we demonstrate the\nmerits of RAPMF from various aspects.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:40:52 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Ling", "Guang", ""], ["Yang", "Haiqin", ""], ["Lyu", "Michael R.", ""], ["King", "Irwin", ""]]}, {"id": "1210.4870", "submitter": "Christopher H. Lin", "authors": "Christopher H. Lin, Mausam, Daniel Weld", "title": "Crowdsourcing Control: Moving Beyond Multiple Choice", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-491-500", "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To ensure quality results from crowdsourced tasks, requesters often aggregate\nworker responses and use one of a plethora of strategies to infer the correct\nanswer from the set of noisy responses. However, all current models assume\nprior knowledge of all possible outcomes of the task. While not an unreasonable\nassumption for tasks that can be posited as multiple-choice questions (e.g.\nn-ary classification), we observe that many tasks do not naturally fit this\nparadigm, but instead demand a free-response formulation where the outcome\nspace is of infinite size (e.g. audio transcription). We model such tasks with\na novel probabilistic graphical model, and design and implement LazySusan, a\ndecision-theoretic controller that dynamically requests responses as necessary\nin order to infer answers to these tasks. We also design an EM algorithm to\njointly learn the parameters of our model while inferring the correct answers\nto multiple tasks at a time. Live experiments on Amazon Mechanical Turk\ndemonstrate the superiority of LazySusan at solving SAT Math questions,\neliminating 83.2% of the error and achieving greater net utility compared to\nthe state-ofthe-art strategy, majority-voting. We also show in live experiments\nthat our EM algorithm outperforms majority-voting on a visualization task that\nwe design.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:41:19 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Lin", "Christopher H.", ""], ["Mausam", "", ""], ["Weld", "Daniel", ""]]}, {"id": "1210.4871", "submitter": "Hui Lin", "authors": "Hui Lin, Jeff A. Bilmes", "title": "Learning Mixtures of Submodular Shells with Application to Document\n  Summarization", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-479-490", "categories": "cs.LG cs.CL cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to learn a mixture of submodular \"shells\" in a\nlarge-margin setting. A submodular shell is an abstract submodular function\nthat can be instantiated with a ground set and a set of parameters to produce a\nsubmodular function. A mixture of such shells can then also be so instantiated\nto produce a more complex submodular function. What our algorithm learns are\nthe mixture weights over such shells. We provide a risk bound guarantee when\nlearning in a large-margin structured-prediction setting using a projected\nsubgradient method when only approximate submodular optimization is possible\n(such as with submodular function maximization). We apply this method to the\nproblem of multi-document summarization and produce the best results reported\nso far on the widely used NIST DUC-05 through DUC-07 document summarization\ncorpora.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:41:30 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Lin", "Hui", ""], ["Bilmes", "Jeff A.", ""]]}, {"id": "1210.4872", "submitter": "Lingbo Li", "authors": "Lingbo Li, XianXing Zhang, Mingyuan Zhou, Lawrence Carin", "title": "Nested Dictionary Learning for Hierarchical Organization of Imagery and\n  Text", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-469-478", "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A tree-based dictionary learning model is developed for joint analysis of\nimagery and associated text. The dictionary learning may be applied directly to\nthe imagery from patches, or to general feature vectors extracted from patches\nor superpixels (using any existing method for image feature extraction). Each\nimage is associated with a path through the tree (from root to a leaf), and\neach of the multiple patches in a given image is associated with one node in\nthat path. Nodes near the tree root are shared between multiple paths,\nrepresenting image characteristics that are common among different types of\nimages. Moving toward the leaves, nodes become specialized, representing\ndetails in image classes. If available, words (text) are also jointly modeled,\nwith a path-dependent probability over words. The tree structure is inferred\nvia a nested Dirichlet process, and a retrospective stick-breaking sampler is\nused to infer the tree depth and width.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:41:42 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Li", "Lingbo", ""], ["Zhang", "XianXing", ""], ["Zhou", "Mingyuan", ""], ["Carin", "Lawrence", ""]]}, {"id": "1210.4876", "submitter": "Kshitij Judah", "authors": "Kshitij Judah, Alan Fern, Thomas G. Dietterich", "title": "Active Imitation Learning via Reduction to I.I.D. Active Learning", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-428-437", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In standard passive imitation learning, the goal is to learn a target policy\nby passively observing full execution trajectories of it. Unfortunately,\ngenerating such trajectories can require substantial expert effort and be\nimpractical in some cases. In this paper, we consider active imitation learning\nwith the goal of reducing this effort by querying the expert about the desired\naction at individual states, which are selected based on answers to past\nqueries and the learner's interactions with an environment simulator. We\nintroduce a new approach based on reducing active imitation learning to i.i.d.\nactive learning, which can leverage progress in the i.i.d. setting. Our first\ncontribution, is to analyze reductions for both non-stationary and stationary\npolicies, showing that the label complexity (number of queries) of active\nimitation learning can be substantially less than passive learning. Our second\ncontribution, is to introduce a practical algorithm inspired by the reductions,\nwhich is shown to be highly effective in four test domains compared to a number\nof alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:43:04 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Judah", "Kshitij", ""], ["Fern", "Alan", ""], ["Dietterich", "Thomas G.", ""]]}, {"id": "1210.4880", "submitter": "Jesse Hostetler", "authors": "Jesse Hostetler, Ethan W. Dereszynski, Thomas G. Dietterich, Alan Fern", "title": "Inferring Strategies from Limited Reconnaissance in Real-time Strategy\n  Games", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-367-376", "categories": "cs.AI cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In typical real-time strategy (RTS) games, enemy units are visible only when\nthey are within sight range of a friendly unit. Knowledge of an opponent's\ndisposition is limited to what can be observed through scouting. Information is\ncostly, since units dedicated to scouting are unavailable for other purposes,\nand the enemy will resist scouting attempts. It is important to infer as much\nas possible about the opponent's current and future strategy from the available\nobservations. We present a dynamic Bayes net model of strategies in the RTS\ngame Starcraft that combines a generative model of how strategies relate to\nobservable quantities with a principled framework for incorporating evidence\ngained via scouting. We demonstrate the model's ability to infer unobserved\naspects of the game from realistic observations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:43:47 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Hostetler", "Jesse", ""], ["Dereszynski", "Ethan W.", ""], ["Dietterich", "Thomas G.", ""], ["Fern", "Alan", ""]]}, {"id": "1210.4881", "submitter": "Tamir Hazan", "authors": "Tamir Hazan, Jian Peng, Amnon Shashua", "title": "Tightening Fractional Covering Upper Bounds on the Partition Function\n  for High-Order Region Graphs", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-356-366", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new approach for tightening upper bounds on the\npartition function. Our upper bounds are based on fractional covering bounds on\nthe entropy function, and result in a concave program to compute these bounds\nand a convex program to tighten them. To solve these programs effectively for\ngeneral region graphs we utilize the entropy barrier method, thus decomposing\nthe original programs by their dual programs and solve them with dual block\noptimization scheme. The entropy barrier method provides an elegant framework\nto generalize the message-passing scheme to high-order region graph, as well as\nto solve the block dual steps in closed-form. This is a key for computational\nrelevancy for large problems with thousands of regions.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:43:59 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Hazan", "Tamir", ""], ["Peng", "Jian", ""], ["Shashua", "Amnon", ""]]}, {"id": "1210.4883", "submitter": "Leonard K. M. Poon", "authors": "Leonard K. M. Poon, April H. Liu, Tengfei Liu, Nevin Lianwen Zhang", "title": "A Model-Based Approach to Rounding in Spectral Clustering", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-685-694", "categories": "cs.LG cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spectral clustering, one defines a similarity matrix for a collection of\ndata points, transforms the matrix to get the Laplacian matrix, finds the\neigenvectors of the Laplacian matrix, and obtains a partition of the data using\nthe leading eigenvectors. The last step is sometimes referred to as rounding,\nwhere one needs to decide how many leading eigenvectors to use, to determine\nthe number of clusters, and to partition the data points. In this paper, we\npropose a novel method for rounding. The method differs from previous methods\nin three ways. First, we relax the assumption that the number of clusters\nequals the number of eigenvectors used. Second, when deciding the number of\nleading eigenvectors to use, we not only rely on information contained in the\nleading eigenvectors themselves, but also use subsequent eigenvectors. Third,\nour method is model-based and solves all the three subproblems of rounding\nusing a class of graphical models called latent tree models. We evaluate our\nmethod on both synthetic and real-world data. The results show that our method\nworks correctly in the ideal case where between-clusters similarity is 0, and\ndegrades gracefully as one moves away from the ideal case.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:45:11 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Poon", "Leonard K. M.", ""], ["Liu", "April H.", ""], ["Liu", "Tengfei", ""], ["Zhang", "Nevin Lianwen", ""]]}, {"id": "1210.4884", "submitter": "Ankur P. Parikh", "authors": "Ankur P. Parikh, Le Song, Mariya Ishteva, Gabi Teodoru, Eric P. Xing", "title": "A Spectral Algorithm for Latent Junction Trees", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-675-684", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable models are an elegant framework for capturing rich\nprobabilistic dependencies in many applications. However, current approaches\ntypically parametrize these models using conditional probability tables, and\nlearning relies predominantly on local search heuristics such as Expectation\nMaximization. Using tensor algebra, we propose an alternative parameterization\nof latent variable models (where the model structures are junction trees) that\nstill allows for computation of marginals among observed variables. While this\nnovel representation leads to a moderate increase in the number of parameters\nfor junction trees of low treewidth, it lets us design a local-minimum-free\nalgorithm for learning this parameterization. The main computation of the\nalgorithm involves only tensor operations and SVDs which can be orders of\nmagnitude faster than EM algorithms for large datasets. To our knowledge, this\nis the first provably consistent parameter learning technique for a large class\nof low-treewidth latent graphical models beyond trees. We demonstrate the\nadvantages of our method on synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:45:30 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Parikh", "Ankur P.", ""], ["Song", "Le", ""], ["Ishteva", "Mariya", ""], ["Teodoru", "Gabi", ""], ["Xing", "Eric P.", ""]]}, {"id": "1210.4887", "submitter": "Yu Nishiyama", "authors": "Yu Nishiyama, Abdeslam Boularias, Arthur Gretton, Kenji Fukumizu", "title": "Hilbert Space Embeddings of POMDPs", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-644-653", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A nonparametric approach for policy learning for POMDPs is proposed. The\napproach represents distributions over the states, observations, and actions as\nembeddings in feature spaces, which are reproducing kernel Hilbert spaces.\nDistributions over states given the observations are obtained by applying the\nkernel Bayes' rule to these distribution embeddings. Policies and value\nfunctions are defined on the feature space over states, which leads to a\nfeature space expression for the Bellman equation. Value iteration may then be\nused to estimate the optimal value function and associated policy. Experimental\nresults confirm that the correct policy is learned using the feature space\nrepresentation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:46:07 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Nishiyama", "Yu", ""], ["Boularias", "Abdeslam", ""], ["Gretton", "Arthur", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1210.4888", "submitter": "Teppo Niinimaki", "authors": "Teppo Niinimaki, Pekka Parviainen", "title": "Local Structure Discovery in Bayesian Networks", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-634-643", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a Bayesian network structure from data is an NP-hard problem and\nthus exact algorithms are feasible only for small data sets. Therefore, network\nstructures for larger networks are usually learned with various heuristics.\nAnother approach to scaling up the structure learning is local learning. In\nlocal learning, the modeler has one or more target variables that are of\nspecial interest; he wants to learn the structure near the target variables and\nis not interested in the rest of the variables. In this paper, we present a\nscore-based local learning algorithm called SLL. We conjecture that our\nalgorithm is theoretically sound in the sense that it is optimal in the limit\nof large sample size. Empirical results suggest that SLL is competitive when\ncompared to the constraint-based HITON algorithm. We also study the prospects\nof constructing the network structure for the whole node set based on local\nresults by presenting two algorithms and comparing them to several heuristics.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:46:17 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Niinimaki", "Teppo", ""], ["Parviainen", "Pekka", ""]]}, {"id": "1210.4889", "submitter": "Kira Mourao", "authors": "Kira Mourao, Luke S. Zettlemoyer, Ronald P. A. Petrick, Mark Steedman", "title": "Learning STRIPS Operators from Noisy and Incomplete Observations", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-614-623", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agents learning to act autonomously in real-world domains must acquire a\nmodel of the dynamics of the domain in which they operate. Learning domain\ndynamics can be challenging, especially where an agent only has partial access\nto the world state, and/or noisy external sensors. Even in standard STRIPS\ndomains, existing approaches cannot learn from noisy, incomplete observations\ntypical of real-world domains. We propose a method which learns STRIPS action\nmodels in such domains, by decomposing the problem into first learning a\ntransition function between states in the form of a set of classifiers, and\nthen deriving explicit STRIPS rules from the classifiers' parameters. We\nevaluate our approach on simulated standard planning domains from the\nInternational Planning Competition, and show that it learns useful domain\ndescriptions from noisy, incomplete observations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:46:26 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Mourao", "Kira", ""], ["Zettlemoyer", "Luke S.", ""], ["Petrick", "Ronald P. A.", ""], ["Steedman", "Mark", ""]]}, {"id": "1210.4892", "submitter": "Marwan A. Mattar", "authors": "Marwan A. Mattar, Allen R. Hanson, Erik G. Learned-Miller", "title": "Unsupervised Joint Alignment and Clustering using Bayesian\n  Nonparametrics", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-584-593", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint alignment of a collection of functions is the process of independently\ntransforming the functions so that they appear more similar to each other.\nTypically, such unsupervised alignment algorithms fail when presented with\ncomplex data sets arising from multiple modalities or make restrictive\nassumptions about the form of the functions or transformations, limiting their\ngenerality. We present a transformed Bayesian infinite mixture model that can\nsimultaneously align and cluster a data set. Our model and associated learning\nscheme offer two key advantages: the optimal number of clusters is determined\nin a data-driven fashion through the use of a Dirichlet process prior, and it\ncan accommodate any transformation function parameterized by a continuous\nparameter vector. As a result, it is applicable to a wide range of data types,\nand transformation functions. We present positive results on synthetic\ntwo-dimensional data, on a set of one-dimensional curves, and on various image\ndata sets, showing large improvements over previous work. We discuss several\nvariations of the model and conclude with directions for future work.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:47:18 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Mattar", "Marwan A.", ""], ["Hanson", "Allen R.", ""], ["Learned-Miller", "Erik G.", ""]]}, {"id": "1210.4893", "submitter": "Sridhar Mahadevan", "authors": "Sridhar Mahadevan, Bo Liu", "title": "Sparse Q-learning with Mirror Descent", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-564-573", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores a new framework for reinforcement learning based on\nonline convex optimization, in particular mirror descent and related\nalgorithms. Mirror descent can be viewed as an enhanced gradient method,\nparticularly suited to minimization of convex functions in highdimensional\nspaces. Unlike traditional gradient methods, mirror descent undertakes gradient\nupdates of weights in both the dual space and primal space, which are linked\ntogether using a Legendre transform. Mirror descent can be viewed as a proximal\nalgorithm where the distance generating function used is a Bregman divergence.\nA new class of proximal-gradient based temporal-difference (TD) methods are\npresented based on different Bregman divergences, which are more powerful than\nregular TD learning. Examples of Bregman divergences that are studied include\np-norm functions, and Mahalanobis distance based on the covariance of sample\ngradients. A new family of sparse mirror-descent reinforcement learning methods\nare proposed, which are able to find sparse fixed points of an l1-regularized\nBellman equation at significantly less computational cost than previous methods\nbased on second-order matrix methods. An experimental study of mirror-descent\nreinforcement learning is presented using discrete and continuous Markov\ndecision processes.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:47:32 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Mahadevan", "Sridhar", ""], ["Liu", "Bo", ""]]}, {"id": "1210.4896", "submitter": "Daniel Lowd", "authors": "Daniel Lowd", "title": "Closed-Form Learning of Markov Networks from Dependency Networks", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-533-542", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov networks (MNs) are a powerful way to compactly represent a joint\nprobability distribution, but most MN structure learning methods are very slow,\ndue to the high cost of evaluating candidates structures. Dependency networks\n(DNs) represent a probability distribution as a set of conditional probability\ndistributions. DNs are very fast to learn, but the conditional distributions\nmay be inconsistent with each other and few inference algorithms support DNs.\nIn this paper, we present a closed-form method for converting a DN into an MN,\nallowing us to enjoy both the efficiency of DN learning and the convenience of\nthe MN representation. When the DN is consistent, this conversion is exact. For\ninconsistent DNs, we present averaging methods that significantly improve the\napproximation. In experiments on 12 standard datasets, our methods are orders\nof magnitude faster than and often more accurate than combining conditional\ndistributions using weight learning.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:48:08 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Lowd", "Daniel", ""]]}, {"id": "1210.4898", "submitter": "Gavin Taylor", "authors": "Gavin Taylor, Ron Parr", "title": "Value Function Approximation in Noisy Environments Using Locally\n  Smoothed Regularized Approximate Linear Programs", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-835-842", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Petrik et al. demonstrated that L1Regularized Approximate Linear\nProgramming (RALP) could produce value functions and policies which compared\nfavorably to established linear value function approximation techniques like\nLSPI. RALP's success primarily stems from the ability to solve the feature\nselection and value function approximation steps simultaneously. RALP's\nperformance guarantees become looser if sampled next states are used. For very\nnoisy domains, RALP requires an accurate model rather than samples, which can\nbe unrealistic in some practical scenarios. In this paper, we demonstrate this\nweakness, and then introduce Locally Smoothed L1-Regularized Approximate Linear\nProgramming (LS-RALP). We demonstrate that LS-RALP mitigates inaccuracies\nstemming from noise even without an accurate model. We show that, given some\nsmoothness assumptions, as the number of samples increases, error from noise\napproaches zero, and provide experimental examples of LS-RALP's success on\ncommon reinforcement learning benchmark problems.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:50:15 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Taylor", "Gavin", ""], ["Parr", "Ron", ""]]}, {"id": "1210.4899", "submitter": "Daniel Tarlow", "authors": "Daniel Tarlow, Kevin Swersky, Richard S. Zemel, Ryan Prescott Adams,\n  Brendan J. Frey", "title": "Fast Exact Inference for Recursive Cardinality Models", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-825-834", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardinality potentials are a generally useful class of high order potential\nthat affect probabilities based on how many of D binary variables are active.\nMaximum a posteriori (MAP) inference for cardinality potential models is\nwell-understood, with efficient computations taking O(DlogD) time. Yet\nefficient marginalization and sampling have not been addressed as thoroughly in\nthe machine learning community. We show that there exists a simple algorithm\nfor computing marginal probabilities and drawing exact joint samples that runs\nin O(Dlog2 D) time, and we show how to frame the algorithm as efficient belief\npropagation in a low order tree-structured model that includes additional\nauxiliary variables. We then develop a new, more general class of models,\ntermed Recursive Cardinality models, which take advantage of this efficiency.\nFinally, we show how to do efficient exact inference in models composed of a\ntree structure and a cardinality potential. We explore the expressive power of\nRecursive Cardinality models and empirically demonstrate their utility.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:50:25 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Tarlow", "Daniel", ""], ["Swersky", "Kevin", ""], ["Zemel", "Richard S.", ""], ["Adams", "Ryan Prescott", ""], ["Frey", "Brendan J.", ""]]}, {"id": "1210.4902", "submitter": "David Sontag", "authors": "David Sontag, Do Kook Choe, Yitao Li", "title": "Efficiently Searching for Frustrated Cycles in MAP Inference", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-795-804", "categories": "cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dual decomposition provides a tractable framework for designing algorithms\nfor finding the most probable (MAP) configuration in graphical models. However,\nfor many real-world inference problems, the typical decomposition has a large\nintegrality gap, due to frustrated cycles. One way to tighten the relaxation is\nto introduce additional constraints that explicitly enforce cycle consistency.\nEarlier work showed that cluster-pursuit algorithms, which iteratively\nintroduce cycle and other higherorder consistency constraints, allows one to\nexactly solve many hard inference problems. However, these algorithms\nexplicitly enumerate a candidate set of clusters, limiting them to triplets or\nother short cycles. We solve the search problem for cycle constraints, giving a\nnearly linear time algorithm for finding the most frustrated cycle of arbitrary\nlength. We show how to use this search algorithm together with the dual\ndecomposition framework and clusterpursuit. The new algorithm exactly solves\nMAP inference problems arising from relational classification and stereo\nvision.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:51:21 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Sontag", "David", ""], ["Choe", "Do Kook", ""], ["Li", "Yitao", ""]]}, {"id": "1210.4905", "submitter": "Ricardo Silva", "authors": "Ricardo Silva", "title": "Latent Composite Likelihood Learning for the Structured Canonical\n  Correlation Model", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-765-774", "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable models are used to estimate variables of interest quantities\nwhich are observable only up to some measurement error. In many studies, such\nvariables are known but not precisely quantifiable (such as \"job satisfaction\"\nin social sciences and marketing, \"analytical ability\" in educational testing,\nor \"inflation\" in economics). This leads to the development of measurement\ninstruments to record noisy indirect evidence for such unobserved variables\nsuch as surveys, tests and price indexes. In such problems, there are\npostulated latent variables and a given measurement model. At the same time,\nother unantecipated latent variables can add further unmeasured confounding to\nthe observed variables. The problem is how to deal with unantecipated latents\nvariables. In this paper, we provide a method loosely inspired by canonical\ncorrelation that makes use of background information concerning the \"known\"\nlatent variables. Given a partially specified structure, it provides a\nstructure learning approach to detect \"unknown unknowns,\" the confounding\neffect of potentially infinitely many other latent variables. This is done\nwithout explicitly modeling such extra latent factors. Because of the special\nstructure of the problem, we are able to exploit a new variation of composite\nlikelihood fitting to efficiently learn this structure. Validation is provided\nwith experiments in synthetic data and the analysis of a large survey done with\na sample of over 100,000 staff members of the National Health Service of the\nUnited Kingdom.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:51:50 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Silva", "Ricardo", ""]]}, {"id": "1210.4909", "submitter": "Jens Roeder", "authors": "Jens Roeder, Boaz Nadler, Kevin Kunzmann, Fred A. Hamprecht", "title": "Active Learning with Distributional Estimates", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-715-725", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active Learning (AL) is increasingly important in a broad range of\napplications. Two main AL principles to obtain accurate classification with few\nlabeled data are refinement of the current decision boundary and exploration of\npoorly sampled regions. In this paper we derive a novel AL scheme that balances\nthese two principles in a natural way. In contrast to many AL strategies, which\nare based on an estimated class conditional probability ^p(y|x), a key\ncomponent of our approach is to view this quantity as a random variable, hence\nexplicitly considering the uncertainty in its estimated value. Our main\ncontribution is a novel mathematical framework for uncertainty-based AL, and a\ncorresponding AL scheme, where the uncertainty in ^p(y|x) is modeled by a\nsecond-order distribution. On the practical side, we show how to approximate\nsuch second-order distributions for kernel density classification. Finally, we\nfind that over a large number of UCI, USPS and Caltech4 datasets, our AL scheme\nachieves significantly better learning curves than popular AL methods such as\nuncertainty sampling and error reduction sampling, when all use the same kernel\ndensity classifier.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:53:17 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Roeder", "Jens", ""], ["Nadler", "Boaz", ""], ["Kunzmann", "Kevin", ""], ["Hamprecht", "Fred A.", ""]]}, {"id": "1210.4910", "submitter": "Khaled S. Refaat", "authors": "Khaled S. Refaat, Arthur Choi, Adnan Darwiche", "title": "New Advances and Theoretical Insights into EDML", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-705-714", "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  EDML is a recently proposed algorithm for learning MAP parameters in Bayesian\nnetworks. In this paper, we present a number of new advances and insights on\nthe EDML algorithm. First, we provide the multivalued extension of EDML,\noriginally proposed for Bayesian networks over binary variables. Next, we\nidentify a simplified characterization of EDML that further implies a simple\nfixed-point algorithm for the convex optimization problem that underlies it.\nThis characterization further reveals a connection between EDML and EM: a fixed\npoint of EDML is a fixed point of EM, and vice versa. We thus identify also a\nnew characterization of EM fixed points, but in the semantics of EDML. Finally,\nwe propose a hybrid EDML/EM algorithm that takes advantage of the improved\nempirical convergence behavior of EDML, while maintaining the monotonic\nimprovement property of EM.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:53:29 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Refaat", "Khaled S.", ""], ["Choi", "Arthur", ""], ["Darwiche", "Adnan", ""]]}, {"id": "1210.4913", "submitter": "Changhe Yuan", "authors": "Changhe Yuan, Brandon Malone", "title": "An Improved Admissible Heuristic for Learning Optimal Bayesian Networks", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-924-933", "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently two search algorithms, A* and breadth-first branch and bound\n(BFBnB), were developed based on a simple admissible heuristic for learning\nBayesian network structures that optimize a scoring function. The heuristic\nrepresents a relaxation of the learning problem such that each variable chooses\noptimal parents independently. As a result, the heuristic may contain many\ndirected cycles and result in a loose bound. This paper introduces an improved\nadmissible heuristic that tries to avoid directed cycles within small groups of\nvariables. A sparse representation is also introduced to store only the unique\noptimal parent choices. Empirical results show that the new techniques\nsignificantly improved the efficiency and scalability of A* and BFBnB on most\nof datasets tested in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:55:57 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Yuan", "Changhe", ""], ["Malone", "Brandon", ""]]}, {"id": "1210.4914", "submitter": "Jason Weston", "authors": "Jason Weston, John Blitzer", "title": "Latent Structured Ranking", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-903-913", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many latent (factorized) models have been proposed for recommendation tasks\nlike collaborative filtering and for ranking tasks like document or image\nretrieval and annotation. Common to all those methods is that during inference\nthe items are scored independently by their similarity to the query in the\nlatent embedding space. The structure of the ranked list (i.e. considering the\nset of items returned as a whole) is not taken into account. This can be a\nproblem because the set of top predictions can be either too diverse (contain\nresults that contradict each other) or are not diverse enough. In this paper we\nintroduce a method for learning latent structured rankings that improves over\nexisting methods by providing the right blend of predictions at the top of the\nranked list. Particular emphasis is put on making this method scalable.\nEmpirical results on large scale image annotation and music recommendation\ntasks show improvements over existing approaches.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:56:08 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Weston", "Jason", ""], ["Blitzer", "John", ""]]}, {"id": "1210.4917", "submitter": "Jun Wang", "authors": "Jun Wang, Yinglong Xia", "title": "Fast Graph Construction Using Auction Algorithm", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-873-882", "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practical machine learning systems, graph based data representation has\nbeen widely used in various learning paradigms, ranging from unsupervised\nclustering to supervised classification. Besides those applications with\nnatural graph or network structure data, such as social network analysis and\nrelational learning, many other applications often involve a critical step in\nconverting data vectors to an adjacency graph. In particular, a sparse subgraph\nextracted from the original graph is often required due to both theoretic and\npractical needs. Previous study clearly shows that the performance of different\nlearning algorithms, e.g., clustering and classification, benefits from such\nsparse subgraphs with balanced node connectivity. However, the existing graph\nconstruction methods are either computationally expensive or with\nunsatisfactory performance. In this paper, we utilize a scalable method called\nauction algorithm and its parallel extension to recover a sparse yet nearly\nbalanced subgraph with significantly reduced computational cost. Empirical\nstudy and comparison with the state-ofart approaches clearly demonstrate the\nsuperiority of the proposed method in both efficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:56:43 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Wang", "Jun", ""], ["Xia", "Yinglong", ""]]}, {"id": "1210.4918", "submitter": "Thomas J. Walsh", "authors": "Thomas J. Walsh, Sergiu Goschin", "title": "Dynamic Teaching in Sequential Decision Making Environments", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-863-872", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe theoretical bounds and a practical algorithm for teaching a model\nby demonstration in a sequential decision making environment. Unlike previous\nefforts that have optimized learners that watch a teacher demonstrate a static\npolicy, we focus on the teacher as a decision maker who can dynamically choose\ndifferent policies to teach different parts of the environment. We develop\nseveral teaching frameworks based on previously defined supervised protocols,\nsuch as Teaching Dimension, extending them to handle noise and sequences of\ninputs encountered in an MDP.We provide theoretical bounds on the learnability\nof several important model classes in this setting and suggest a practical\nalgorithm for dynamic teaching.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:56:54 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Walsh", "Thomas J.", ""], ["Goschin", "Sergiu", ""]]}, {"id": "1210.4919", "submitter": "Mirwaes Wahabzada", "authors": "Mirwaes Wahabzada, Kristian Kersting, Christian Bauckhage, Christoph\n  Roemer, Agim Ballvora, Francisco Pinto, Uwe Rascher, Jens Leon, Lutz Ploemer", "title": "Latent Dirichlet Allocation Uncovers Spectral Characteristics of Drought\n  Stressed Plants", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-852-862", "categories": "cs.LG cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the adaptation process of plants to drought stress is essential\nin improving management practices, breeding strategies as well as engineering\nviable crops for a sustainable agriculture in the coming decades.\nHyper-spectral imaging provides a particularly promising approach to gain such\nunderstanding since it allows to discover non-destructively spectral\ncharacteristics of plants governed primarily by scattering and absorption\ncharacteristics of the leaf internal structure and biochemical constituents.\nSeveral drought stress indices have been derived using hyper-spectral imaging.\nHowever, they are typically based on few hyper-spectral images only, rely on\ninterpretations of experts, and consider few wavelengths only. In this study,\nwe present the first data-driven approach to discovering spectral drought\nstress indices, treating it as an unsupervised labeling problem at massive\nscale. To make use of short range dependencies of spectral wavelengths, we\ndevelop an online variational Bayes algorithm for latent Dirichlet allocation\nwith convolved Dirichlet regularizer. This approach scales to massive datasets\nand, hence, provides a more objective complement to plant physiological\npractices. The spectral topics found conform to plant physiological knowledge\nand can be computed in a fraction of the time compared to existing LDA\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:57:06 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Wahabzada", "Mirwaes", ""], ["Kersting", "Kristian", ""], ["Bauckhage", "Christian", ""], ["Roemer", "Christoph", ""], ["Ballvora", "Agim", ""], ["Pinto", "Francisco", ""], ["Rascher", "Uwe", ""], ["Leon", "Jens", ""], ["Ploemer", "Lutz", ""]]}, {"id": "1210.4920", "submitter": "Seppo Virtanen", "authors": "Seppo Virtanen, Yangqing Jia, Arto Klami, Trevor Darrell", "title": "Factorized Multi-Modal Topic Model", "comments": "Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty\n  in Artificial Intelligence (UAI2012)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2012-PG-843-851", "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modal data collections, such as corpora of paired images and text\nsnippets, require analysis methods beyond single-view component and topic\nmodels. For continuous observations the current dominant approach is based on\nextensions of canonical correlation analysis, factorizing the variation into\ncomponents shared by the different modalities and those private to each of\nthem. For count data, multiple variants of topic models attempting to tie the\nmodalities together have been presented. All of these, however, lack the\nability to learn components private to one modality, and consequently will try\nto force dependencies even between minimally correlating modalities. In this\nwork we combine the two approaches by presenting a novel HDP-based topic model\nthat automatically learns both shared and private topics. The model is shown to\nbe especially useful for querying the contents of one domain given samples of\nthe other.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2012 17:57:22 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Virtanen", "Seppo", ""], ["Jia", "Yangqing", ""], ["Klami", "Arto", ""], ["Darrell", "Trevor", ""]]}, {"id": "1210.5034", "submitter": "Pierre Machart", "authors": "Pierre Machart (LIF, LSIS), Sandrine Anthoine (LATP), Luca Baldassarre\n  (EPFL)", "title": "Optimal Computational Trade-Off of Inexact Proximal Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the trade-off between convergence rate and\ncomputational cost when minimizing a composite functional with\nproximal-gradient methods, which are popular optimisation tools in machine\nlearning. We consider the case when the proximity operator is computed via an\niterative procedure, which provides an approximation of the exact proximity\noperator. In that case, we obtain algorithms with two nested loops. We show\nthat the strategy that minimizes the computational cost to reach a solution\nwith a desired accuracy in finite time is to set the number of inner iterations\nto a constant, which differs from the strategy indicated by a convergence rate\nanalysis. In the process, we also present a new procedure called SIP (that is\nSpeedy Inexact Proximal-gradient algorithm) that is both computationally\nefficient and easy to implement. Our numerical experiments confirm the\ntheoretical findings and suggest that SIP can be a very competitive alternative\nto the standard procedure.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2012 06:27:10 GMT"}, {"version": "v2", "created": "Sun, 21 Oct 2012 06:17:08 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Machart", "Pierre", "", "LIF, LSIS"], ["Anthoine", "Sandrine", "", "LATP"], ["Baldassarre", "Luca", "", "EPFL"]]}, {"id": "1210.5128", "submitter": "Yu Wang", "authors": "Yu Wang, Weikang Qian, Shuchang Zhang and Bo Yuan", "title": "A Novel Learning Algorithm for Bayesian Network and Its Efficient\n  Implementation on GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational inference of causal relationships underlying complex networks,\nsuch as gene-regulatory pathways, is NP-complete due to its combinatorial\nnature when permuting all possible interactions. Markov chain Monte Carlo\n(MCMC) has been introduced to sample only part of the combinations while still\nguaranteeing convergence and traversability, which therefore becomes widely\nused. However, MCMC is not able to perform efficiently enough for networks that\nhave more than 15~20 nodes because of the computational complexity. In this\npaper, we use general purpose processor (GPP) and general purpose graphics\nprocessing unit (GPGPU) to implement and accelerate a novel Bayesian network\nlearning algorithm. With a hash-table-based memory-saving strategy and a novel\ntask assigning strategy, we achieve a 10-fold acceleration per iteration than\nusing a serial GPP. Specially, we use a greedy method to search for the best\ngraph from a given order. We incorporate a prior component in the current\nscoring function, which further facilitates the searching. Overall, we are able\nto apply this system to networks with more than 60 nodes, allowing inferences\nand modeling of bigger and more complex networks than current methods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2012 14:02:12 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Wang", "Yu", ""], ["Qian", "Weikang", ""], ["Zhang", "Shuchang", ""], ["Yuan", "Bo", ""]]}, {"id": "1210.5135", "submitter": "Yang Lu", "authors": "Yang Lu, Mengying Wang, Menglu Li, Qili Zhu, Bo Yuan", "title": "LSBN: A Large-Scale Bayesian Structure Learning Framework for Model\n  Averaging", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The motivation for this paper is to apply Bayesian structure learning using\nModel Averaging in large-scale networks. Currently, Bayesian model averaging\nalgorithm is applicable to networks with only tens of variables, restrained by\nits super-exponential complexity. We present a novel framework, called\nLSBN(Large-Scale Bayesian Network), making it possible to handle networks with\ninfinite size by following the principle of divide-and-conquer. The method of\nLSBN comprises three steps. In general, LSBN first performs the partition by\nusing a second-order partition strategy, which achieves more robust results.\nLSBN conducts sampling and structure learning within each overlapping community\nafter the community is isolated from other variables by Markov Blanket. Finally\nLSBN employs an efficient algorithm, to merge structures of overlapping\ncommunities into a whole. In comparison with other four state-of-art\nlarge-scale network structure learning algorithms such as ARACNE, PC, Greedy\nSearch and MMHC, LSBN shows comparable results in five common benchmark\ndatasets, evaluated by precision, recall and f-score. What's more, LSBN makes\nit possible to learn large-scale Bayesian structure by Model Averaging which\nused to be intractable. In summary, LSBN provides an scalable and parallel\nframework for the reconstruction of network structures. Besides, the complete\ninformation of overlapping communities serves as the byproduct, which could be\nused to mine meaningful clusters in biological networks, such as\nprotein-protein-interaction network or gene regulatory network, as well as in\nsocial network.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2012 14:15:40 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Lu", "Yang", ""], ["Wang", "Mengying", ""], ["Li", "Menglu", ""], ["Zhu", "Qili", ""], ["Yuan", "Bo", ""]]}, {"id": "1210.5196", "submitter": "Rina Foygel", "authors": "Rina Foygel, Nathan Srebro, Ruslan Salakhutdinov", "title": "Matrix reconstruction with the local max norm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new family of matrix norms, the \"local max\" norms,\ngeneralizing existing methods such as the max norm, the trace norm (nuclear\nnorm), and the weighted or smoothed weighted trace norms, which have been\nextensively used in the literature as regularizers for matrix reconstruction\nproblems. We show that this new family can be used to interpolate between the\n(weighted or unweighted) trace norm and the more conservative max norm. We test\nthis interpolation on simulated data and on the large-scale Netflix and\nMovieLens ratings data, and find improved accuracy relative to the existing\nmatrix norms. We also provide theoretical results showing learning guarantees\nfor some of the new norms.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2012 17:30:43 GMT"}], "update_date": "2012-10-19", "authors_parsed": [["Foygel", "Rina", ""], ["Srebro", "Nathan", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1210.5323", "submitter": "Xu Zhiqiang", "authors": "Zhiqiang Xu", "title": "The performance of orthogonal multi-matching pursuit under RIP", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The orthogonal multi-matching pursuit (OMMP) is a natural extension of\northogonal matching pursuit (OMP). We denote the OMMP with the parameter $M$ as\nOMMP(M) where $M\\geq 1$ is an integer. The main difference between OMP and\nOMMP(M) is that OMMP(M) selects $M$ atoms per iteration, while OMP only adds\none atom to the optimal atom set. In this paper, we study the performance of\northogonal multi-matching pursuit (OMMP) under RIP. In particular, we show\nthat, when the measurement matrix A satisfies $(9s, 1/10)$-RIP, there exists an\nabsolutely constant $M_0\\leq 8$ so that OMMP(M_0) can recover $s$-sparse signal\nwithin $s$ iterations. We furthermore prove that, for slowly-decaying\n$s$-sparse signal, OMMP(M) can recover s-sparse signal within $O(\\frac{s}{M})$\niterations for a large class of $M$. In particular, for $M=s^a$ with $a\\in\n[0,1/2]$, OMMP(M) can recover slowly-decaying $s$-sparse signal within\n$O(s^{1-a})$ iterations. The result implies that OMMP can reduce the\ncomputational complexity heavily.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 06:03:05 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2012 03:34:56 GMT"}, {"version": "v3", "created": "Wed, 17 Jul 2013 05:50:32 GMT"}], "update_date": "2013-07-18", "authors_parsed": [["Xu", "Zhiqiang", ""]]}, {"id": "1210.5338", "submitter": "Cyril Furtlehner", "authors": "Cyril Furtlehner, Yufei Han, Jean-Marc Lasgouttes and Victorin Martin", "title": "Pairwise MRF Calibration by Perturbation of the Bethe Reference Point", "comments": "54 pages, 8 figure. section 5 and refs added in V2", "journal-ref": null, "doi": null, "report-no": "Inria RR-8059", "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate different ways of generating approximate solutions to the\npairwise Markov random field (MRF) selection problem. We focus mainly on the\ninverse Ising problem, but discuss also the somewhat related inverse Gaussian\nproblem because both types of MRF are suitable for inference tasks with the\nbelief propagation algorithm (BP) under certain conditions. Our approach\nconsists in to take a Bethe mean-field solution obtained with a maximum\nspanning tree (MST) of pairwise mutual information, referred to as the\n\\emph{Bethe reference point}, for further perturbation procedures. We consider\nthree different ways following this idea: in the first one, we select and\ncalibrate iteratively the optimal links to be added starting from the Bethe\nreference point; the second one is based on the observation that the natural\ngradient can be computed analytically at the Bethe point; in the third one,\nassuming no local field and using low temperature expansion we develop a dual\nloop joint model based on a well chosen fundamental cycle basis. We indeed\nidentify a subclass of planar models, which we refer to as \\emph{Bethe-dual\ngraph models}, having possibly many loops, but characterized by a singly\nconnected dual factor graph, for which the partition function and the linear\nresponse can be computed exactly in respectively O(N) and $O(N^2)$ operations,\nthanks to a dual weight propagation (DWP) message passing procedure that we set\nup. When restricted to this subclass of models, the inverse Ising problem being\nconvex, becomes tractable at any temperature. Experimental tests on various\ndatasets with refined $L_0$ or $L_1$ regularization procedures indicate that\nthese approaches may be competitive and useful alternatives to existing ones.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 08:08:55 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2013 17:32:44 GMT"}], "update_date": "2013-02-04", "authors_parsed": [["Furtlehner", "Cyril", ""], ["Han", "Yufei", ""], ["Lasgouttes", "Jean-Marc", ""], ["Martin", "Victorin", ""]]}, {"id": "1210.5394", "submitter": "Arash Amini", "authors": "Arash Amini, Ulugbek S. Kamilov, Emrah Bostan and Michael Unser", "title": "Bayesian Estimation for Continuous-Time Sparse Stochastic Processes", "comments": "To appear in IEEE TSP", "journal-ref": null, "doi": "10.1109/TSP.2012.2226446", "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider continuous-time sparse stochastic processes from which we have\nonly a finite number of noisy/noiseless samples. Our goal is to estimate the\nnoiseless samples (denoising) and the signal in-between (interpolation\nproblem).\n  By relying on tools from the theory of splines, we derive the joint a priori\ndistribution of the samples and show how this probability density function can\nbe factorized. The factorization enables us to tractably implement the maximum\na posteriori and minimum mean-square error (MMSE) criteria as two statistical\napproaches for estimating the unknowns. We compare the derived statistical\nmethods with well-known techniques for the recovery of sparse signals, such as\nthe $\\ell_1$ norm and Log ($\\ell_1$-$\\ell_0$ relaxation) regularization\nmethods. The simulation results show that, under certain conditions, the\nperformance of the regularization techniques can be very close to that of the\nMMSE estimator.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 12:15:28 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Amini", "Arash", ""], ["Kamilov", "Ulugbek S.", ""], ["Bostan", "Emrah", ""], ["Unser", "Michael", ""]]}, {"id": "1210.5474", "submitter": "Guillaume Desjardins", "authors": "Guillaume Desjardins and Aaron Courville and Yoshua Bengio", "title": "Disentangling Factors of Variation via Generative Entangling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we propose a novel model family with the objective of learning to\ndisentangle the factors of variation in data. Our approach is based on the\nspike-and-slab restricted Boltzmann machine which we generalize to include\nhigher-order interactions among multiple latent variables. Seen from a\ngenerative perspective, the multiplicative interactions emulates the entangling\nof factors of variation. Inference in the model can be seen as disentangling\nthese generative factors. Unlike previous attempts at disentangling latent\nfactors, the proposed model is trained using no supervised information\nregarding the latent factors. We apply our model to the task of facial\nexpression classification.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 17:16:48 GMT"}], "update_date": "2012-10-22", "authors_parsed": [["Desjardins", "Guillaume", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1210.5544", "submitter": "Cem Tekin", "authors": "Cem Tekin, Mingyan Liu", "title": "Online Learning in Decentralized Multiuser Resource Sharing Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the general scenario of resource sharing in a\ndecentralized system when the resource rewards/qualities are time-varying and\nunknown to the users, and using the same resource by multiple users leads to\nreduced quality due to resource sharing. Firstly, we consider a\nuser-independent reward model with no communication between the users, where a\nuser gets feedback about the congestion level in the resource it uses.\nSecondly, we consider user-specific rewards and allow costly communication\nbetween the users. The users have a cooperative goal of achieving the highest\nsystem utility. There are multiple obstacles in achieving this goal such as the\ndecentralized nature of the system, unknown resource qualities, communication,\ncomputation and switching costs. We propose distributed learning algorithms\nwith logarithmic regret with respect to the optimal allocation. Our logarithmic\nregret result holds under both i.i.d. and Markovian reward models, as well as\nunder communication, computation and switching costs.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 21:31:50 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Tekin", "Cem", ""], ["Liu", "Mingyan", ""]]}, {"id": "1210.5631", "submitter": "Mu Zhu", "authors": "Jennifer Nguyen, Mu Zhu", "title": "Content-boosted Matrix Factorization Techniques for Recommender Systems", "comments": null, "journal-ref": "Statistical Analysis and Data Mining, Vol. 6, pp. 286 - 301,\n  August 2013", "doi": "10.1002/sam.11184", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many businesses are using recommender systems for marketing outreach.\nRecommendation algorithms can be either based on content or driven by\ncollaborative filtering. We study different ways to incorporate content\ninformation directly into the matrix factorization approach of collaborative\nfiltering. These content-boosted matrix factorization algorithms not only\nimprove recommendation accuracy, but also provide useful insights about the\ncontents, as well as make recommendations more easily interpretable.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2012 14:39:39 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2013 22:52:39 GMT"}], "update_date": "2013-08-09", "authors_parsed": [["Nguyen", "Jennifer", ""], ["Zhu", "Mu", ""]]}, {"id": "1210.5644", "submitter": "Philipp Kr\\\"ahenb\\\"uhl", "authors": "Philipp Kr\\\"ahenb\\\"uhl and Vladlen Koltun", "title": "Efficient Inference in Fully Connected CRFs with Gaussian Edge\n  Potentials", "comments": "NIPS 2011", "journal-ref": "Advances in Neural Information Processing Systems 24 (2011)\n  109-117", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most state-of-the-art techniques for multi-class image segmentation and\nlabeling use conditional random fields defined over pixels or image regions.\nWhile region-level models often feature dense pairwise connectivity,\npixel-level models are considerably larger and have only permitted sparse graph\nstructures. In this paper, we consider fully connected CRF models defined on\nthe complete set of pixels in an image. The resulting graphs have billions of\nedges, making traditional inference algorithms impractical. Our main\ncontribution is a highly efficient approximate inference algorithm for fully\nconnected CRF models in which the pairwise edge potentials are defined by a\nlinear combination of Gaussian kernels. Our experiments demonstrate that dense\nconnectivity at the pixel level substantially improves segmentation and\nlabeling accuracy.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2012 17:41:23 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Kr\u00e4henb\u00fchl", "Philipp", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1210.5830", "submitter": "Sylvain Arlot", "authors": "Sylvain Arlot (SIERRA, DI-ENS), Matthieu Lerasle (JAD)", "title": "Choice of V for V-Fold Cross-Validation in Least-Squares Density\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies V-fold cross-validation for model selection in\nleast-squares density estimation. The goal is to provide theoretical grounds\nfor choosing V in order to minimize the least-squares loss of the selected\nestimator. We first prove a non-asymptotic oracle inequality for V-fold\ncross-validation and its bias-corrected version (V-fold penalization). In\nparticular, this result implies that V-fold penalization is asymptotically\noptimal in the nonparametric case. Then, we compute the variance of V-fold\ncross-validation and related criteria, as well as the variance of key\nquantities for model selection performance. We show that these variances depend\non V like 1+4/(V-1), at least in some particular cases, suggesting that the\nperformance increases much from V=2 to V=5 or 10, and then is almost constant.\nOverall, this can explain the common advice to take V=5---at least in our\nsetting and when the computational power is limited---, as supported by some\nsimulation experiments. An oracle inequality and exact formulas for the\nvariance are also proved for Monte-Carlo cross-validation, also known as\nrepeated cross-validation, where the parameter V is replaced by the number B of\nrandom splits of the data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 08:22:57 GMT"}, {"version": "v2", "created": "Tue, 22 Jul 2014 07:06:19 GMT"}, {"version": "v3", "created": "Sun, 11 Oct 2015 11:10:53 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Arlot", "Sylvain", "", "SIERRA, DI-ENS"], ["Lerasle", "Matthieu", "", "JAD"]]}, {"id": "1210.5840", "submitter": "Purushottam Kar", "authors": "Purushottam Kar and Prateek Jain", "title": "Supervised Learning with Similarity Functions", "comments": "To appear in the proceedings of NIPS 2012, 30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of general supervised learning when data can only be\naccessed through an (indefinite) similarity function between data points.\nExisting work on learning with indefinite kernels has concentrated solely on\nbinary/multi-class classification problems. We propose a model that is generic\nenough to handle any supervised learning task and also subsumes the model\npreviously proposed for classification. We give a \"goodness\" criterion for\nsimilarity functions w.r.t. a given supervised learning task and then adapt a\nwell-known landmarking technique to provide efficient algorithms for supervised\nlearning using \"good\" similarity functions. We demonstrate the effectiveness of\nour model on three important super-vised learning problems: a) real-valued\nregression, b) ordinal regression and c) ranking where we show that our method\nguarantees bounded generalization error. Furthermore, for the case of\nreal-valued regression, we give a natural goodness definition that, when used\nin conjunction with a recent result in sparse vector recovery, guarantees a\nsparse predictor with bounded generalization error. Finally, we report results\nof our learning algorithms on regression and ordinal regression tasks using\nnon-PSD similarity functions and demonstrate the effectiveness of our\nalgorithms, especially that of the sparse landmark selection algorithm that\nachieves significantly higher accuracies than the baseline methods while\noffering reduced computational costs.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 08:55:13 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Kar", "Purushottam", ""], ["Jain", "Prateek", ""]]}, {"id": "1210.5873", "submitter": "Ayodeji Akinduko Mr", "authors": "A. A. Akinduko and E. M. Mirkes", "title": "Initialization of Self-Organizing Maps: Principal Components Versus\n  Random Initialization. A Case Study", "comments": "18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The performance of the Self-Organizing Map (SOM) algorithm is dependent on\nthe initial weights of the map. The different initialization methods can\nbroadly be classified into random and data analysis based initialization\napproach. In this paper, the performance of random initialization (RI) approach\nis compared to that of principal component initialization (PCI) in which the\ninitial map weights are chosen from the space of the principal component.\nPerformance is evaluated by the fraction of variance unexplained (FVU).\nDatasets were classified into quasi-linear and non-linear and it was observed\nthat RI performed better for non-linear datasets; however the performance of\nPCI approach remains inconclusive for quasi-linear datasets.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 11:17:31 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Akinduko", "A. A.", ""], ["Mirkes", "E. M.", ""]]}, {"id": "1210.6001", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko, J\\'er\\'emie Mary", "title": "Reducing statistical time-series problems to binary classification", "comments": "In proceedings of NIPS 2012, pp. 2069-2077", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how binary classification methods developed to work on i.i.d. data\ncan be used for solving statistical problems that are seemingly unrelated to\nclassification and concern highly-dependent time series. Specifically, the\nproblems of time-series clustering, homogeneity testing and the three-sample\nproblem are addressed. The algorithms that we construct for solving these\nproblems are based on a new metric between time-series distributions, which can\nbe evaluated using binary classification methods. Universal consistency of the\nproposed algorithms is proven under most general assumptions. The theoretical\nresults are illustrated with experiments on synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 19:02:21 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2013 10:25:38 GMT"}, {"version": "v3", "created": "Fri, 7 Jun 2013 09:45:45 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Ryabko", "Daniil", ""], ["Mary", "J\u00e9r\u00e9mie", ""]]}, {"id": "1210.6287", "submitter": "Parikshit Ram", "authors": "Ryan R. Curtin, Parikshit Ram, Alexander G. Gray", "title": "Fast Exact Max-Kernel Search", "comments": "Under submission in SIAM Data Mining conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide applicability of kernels makes the problem of max-kernel search\nubiquitous and more general than the usual similarity search in metric spaces.\nWe focus on solving this problem efficiently. We begin by characterizing the\ninherent hardness of the max-kernel search problem with a novel notion of\ndirectional concentration. Following that, we present a method to use an $O(n\n\\log n)$ algorithm to index any set of objects (points in $\\Real^\\dims$ or\nabstract objects) directly in the Hilbert space without any explicit feature\nrepresentations of the objects in this space. We present the first provably\n$O(\\log n)$ algorithm for exact max-kernel search using this index. Empirical\nresults for a variety of data sets as well as abstract objects demonstrate up\nto 4 orders of magnitude speedup in some cases. Extensions for approximate\nmax-kernel search are also presented.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2012 16:51:31 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2012 19:14:20 GMT"}], "update_date": "2012-10-29", "authors_parsed": [["Curtin", "Ryan R.", ""], ["Ram", "Parikshit", ""], ["Gray", "Alexander G.", ""]]}, {"id": "1210.6292", "submitter": "Alvaro Martinez", "authors": "\\'Alvaro Mart\\'inez-P\\'erez", "title": "A density-sensitive hierarchical clustering method", "comments": "25 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a hierarchical clustering method: $\\alpha$-unchaining single\nlinkage or $SL(\\alpha)$. The input of this algorithm is a finite metric space\nand a certain parameter $\\alpha$. This method is sensitive to the density of\nthe distribution and offers some solution to the so called chaining effect. We\nalso define a modified version, $SL^*(\\alpha)$, to treat the chaining through\npoints or small blocks. We study the theoretical properties of these methods\nand offer some theoretical background for the treatment of chaining effects.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2012 17:12:01 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2014 11:29:49 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Mart\u00ednez-P\u00e9rez", "\u00c1lvaro", ""]]}, {"id": "1210.6293", "submitter": "Ryan Curtin", "authors": "Ryan R. Curtin, James R. Cline, N.P. Slagle, William B. March,\n  Parikshit Ram, Nishant A. Mehta, Alexander G. Gray", "title": "MLPACK: A Scalable C++ Machine Learning Library", "comments": "Submitted to JMLR MLOSS (http://jmlr.csail.mit.edu/mloss/)", "journal-ref": "Journal of Machine Learning Research 14 (2013) 801-805", "doi": null, "report-no": null, "categories": "cs.MS cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MLPACK is a state-of-the-art, scalable, multi-platform C++ machine learning\nlibrary released in late 2011 offering both a simple, consistent API accessible\nto novice users and high performance and flexibility to expert users by\nleveraging modern features of C++. MLPACK provides cutting-edge algorithms\nwhose benchmarks exhibit far better performance than other leading machine\nlearning libraries. MLPACK version 1.0.3, licensed under the LGPL, is available\nat http://www.mlpack.org.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2012 17:15:03 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Curtin", "Ryan R.", ""], ["Cline", "James R.", ""], ["Slagle", "N. P.", ""], ["March", "William B.", ""], ["Ram", "Parikshit", ""], ["Mehta", "Nishant A.", ""], ["Gray", "Alexander G.", ""]]}, {"id": "1210.6321", "submitter": "Ryohei Hisano", "authors": "Ryohei Hisano, Didier Sornette, Takayuki Mizuno, Takaaki Ohnishi,\n  Tsutomu Watanabe", "title": "High quality topic extraction from business news explains abnormal\n  financial market volatility", "comments": "The previous version of this article included an error. This is a\n  revised version", "journal-ref": null, "doi": "10.1371/journal.pone.0064846", "report-no": null, "categories": "stat.ML cs.LG cs.SI physics.soc-ph q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the mutual relationships between information flows and social\nactivity in society today is one of the cornerstones of the social sciences. In\nfinancial economics, the key issue in this regard is understanding and\nquantifying how news of all possible types (geopolitical, environmental,\nsocial, financial, economic, etc.) affect trading and the pricing of firms in\norganized stock markets. In this article, we seek to address this issue by\nperforming an analysis of more than 24 million news records provided by\nThompson Reuters and of their relationship with trading activity for 206 major\nstocks in the S&P US stock index. We show that the whole landscape of news that\naffect stock price movements can be automatically summarized via simple\nregularized regressions between trading activity and news information pieces\ndecomposed, with the help of simple topic modeling techniques, into their\n\"thematic\" features. Using these methods, we are able to estimate and quantify\nthe impacts of news on trading. We introduce network-based visualization\ntechniques to represent the whole landscape of news information associated with\na basket of stocks. The examination of the words that are representative of the\ntopic distributions confirms that our method is able to extract the significant\npieces of information influencing the stock market. Our results show that one\nof the most puzzling stylized fact in financial economies, namely that at\ncertain times trading volumes appear to be \"abnormally large,\" can be partially\nexplained by the flow of news. In this sense, our results prove that there is\nno \"excess trading,\" when restricting to times when news are genuinely novel\nand provide relevant financial information.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2012 18:31:46 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2012 17:16:47 GMT"}, {"version": "v3", "created": "Tue, 19 Mar 2013 10:29:11 GMT"}, {"version": "v4", "created": "Sat, 23 Mar 2013 14:34:36 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Hisano", "Ryohei", ""], ["Sornette", "Didier", ""], ["Mizuno", "Takayuki", ""], ["Ohnishi", "Takaaki", ""], ["Watanabe", "Tsutomu", ""]]}, {"id": "1210.6497", "submitter": "Zhipeng Luo", "authors": "Daifeng Li, Ying Ding, Xin Shuai, Golden Guo-zheng Sun, Jie Tang,\n  Zhipeng Luo, Jingwei Zhang and Guo Zhang", "title": "Topic-Level Opinion Influence Model(TOIM): An Investigation Using\n  Tencent Micro-Blogging", "comments": "PLOS ONE Manuscript Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining user opinion from Micro-Blogging has been extensively studied on the\nmost popular social networking sites such as Twitter and Facebook in the U.S.,\nbut few studies have been done on Micro-Blogging websites in other countries\n(e.g. China). In this paper, we analyze the social opinion influence on\nTencent, one of the largest Micro-Blogging websites in China, endeavoring to\nunveil the behavior patterns of Chinese Micro-Blogging users. This paper\nproposes a Topic-Level Opinion Influence Model (TOIM) that simultaneously\nincorporates topic factor and social direct influence in a unified\nprobabilistic framework. Based on TOIM, two topic level opinion influence\npropagation and aggregation algorithms are developed to consider the indirect\ninfluence: CP (Conservative Propagation) and NCP (None Conservative\nPropagation). Users' historical social interaction records are leveraged by\nTOIM to construct their progressive opinions and neighbors' opinion influence\nthrough a statistical learning process, which can be further utilized to\npredict users' future opinions on some specific topics. To evaluate and test\nthis proposed model, an experiment was designed and a sub-dataset from Tencent\nMicro-Blogging was used. The experimental results show that TOIM outperforms\nbaseline methods on predicting users' opinion. The applications of CP and NCP\nhave no significant differences and could significantly improve recall and\nF1-measure of TOIM.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2012 11:51:21 GMT"}], "update_date": "2012-10-25", "authors_parsed": [["Li", "Daifeng", ""], ["Ding", "Ying", ""], ["Shuai", "Xin", ""], ["Sun", "Golden Guo-zheng", ""], ["Tang", "Jie", ""], ["Luo", "Zhipeng", ""], ["Zhang", "Jingwei", ""], ["Zhang", "Guo", ""]]}, {"id": "1210.6511", "submitter": "Fabrice Rossi", "authors": "Marie Cottrell (SAMM), Madalina Olteanu (SAMM), Fabrice Rossi (SAMM),\n  Joseph Rynkiewicz (SAMM), Nathalie Villa-Vialaneix (SAMM)", "title": "Neural Networks for Complex Data", "comments": null, "journal-ref": "K\\\"unstliche Intelligenz 26, 4 (2012) 373-380", "doi": "10.1007/s13218-012-0207-2", "report-no": null, "categories": "cs.NE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks are simple and efficient machine learning tools.\nDefined originally in the traditional setting of simple vector data, neural\nnetwork models have evolved to address more and more difficulties of complex\nreal world problems, ranging from time evolving data to sophisticated data\nstructures such as graphs and functions. This paper summarizes advances on\nthose themes from the last decade, with a focus on results obtained by members\nof the SAMM team of Universit\\'e Paris 1\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2012 12:37:53 GMT"}], "update_date": "2012-10-26", "authors_parsed": [["Cottrell", "Marie", "", "SAMM"], ["Olteanu", "Madalina", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"], ["Rynkiewicz", "Joseph", "", "SAMM"], ["Villa-Vialaneix", "Nathalie", "", "SAMM"]]}, {"id": "1210.6707", "submitter": "Emanuele Coviello", "authors": "Emanuele Coviello and Antoni B. Chan and Gert R.G. Lanckriet", "title": "Clustering hidden Markov models with variational HEM", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hidden Markov model (HMM) is a widely-used generative model that copes\nwith sequential data, assuming that each observation is conditioned on the\nstate of a hidden Markov chain. In this paper, we derive a novel algorithm to\ncluster HMMs based on the hierarchical EM (HEM) algorithm. The proposed\nalgorithm i) clusters a given collection of HMMs into groups of HMMs that are\nsimilar, in terms of the distributions they represent, and ii) characterizes\neach group by a \"cluster center\", i.e., a novel HMM that is representative for\nthe group, in a manner that is consistent with the underlying generative model\nof the HMM. To cope with intractable inference in the E-step, the HEM algorithm\nis formulated as a variational optimization problem, and efficiently solved for\nthe HMM case by leveraging an appropriate variational approximation. The\nbenefits of the proposed algorithm, which we call variational HEM (VHEM), are\ndemonstrated on several tasks involving time-series data, such as hierarchical\nclustering of motion capture sequences, and automatic annotation and retrieval\nof music and of online hand-writing data, showing improvements over current\nmethods. In particular, our variational HEM algorithm effectively leverages\nlarge amounts of data when learning annotation models by using an efficient\nhierarchical estimation procedure, which reduces learning times and memory\nrequirements, while improving model robustness through better regularization.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2012 23:57:35 GMT"}], "update_date": "2012-10-26", "authors_parsed": [["Coviello", "Emanuele", ""], ["Chan", "Antoni B.", ""], ["Lanckriet", "Gert R. G.", ""]]}, {"id": "1210.6738", "submitter": "John Paisley", "authors": "John Paisley, Chong Wang, David M. Blei and Michael I. Jordan", "title": "Nested Hierarchical Dirichlet Processes", "comments": "To appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, Special Issue on Bayesian Nonparametrics", "journal-ref": null, "doi": "10.1109/TPAMI.2014.2318728", "report-no": null, "categories": "stat.ML cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a nested hierarchical Dirichlet process (nHDP) for hierarchical\ntopic modeling. The nHDP is a generalization of the nested Chinese restaurant\nprocess (nCRP) that allows each word to follow its own path to a topic node\naccording to a document-specific distribution on a shared tree. This alleviates\nthe rigid, single-path formulation of the nCRP, allowing a document to more\neasily express thematic borrowings as a random effect. We derive a stochastic\nvariational inference algorithm for the model, in addition to a greedy subtree\nselection method for each document, which allows for efficient inference using\nmassive collections of text documents. We demonstrate our algorithm on 1.8\nmillion documents from The New York Times and 3.3 million documents from\nWikipedia.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2012 04:25:00 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2012 16:03:19 GMT"}, {"version": "v3", "created": "Wed, 9 Oct 2013 19:46:20 GMT"}, {"version": "v4", "created": "Fri, 2 May 2014 16:36:57 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Paisley", "John", ""], ["Wang", "Chong", ""], ["Blei", "David M.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1210.6766", "submitter": "Mohammad Golbabaee Mohammad Golbabaee", "authors": "Afsaneh Asaei, Mohammad Golbabaee, Herv\\'e Bourlard, Volkan Cevher", "title": "Structured Sparsity Models for Multiparty Speech Recovery from\n  Reverberant Recordings", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the multi-party speech recovery problem through modeling the\nacoustic of the reverberant chambers. Our approach exploits structured sparsity\nmodels to perform room modeling and speech recovery. We propose a scheme for\ncharacterizing the room acoustic from the unknown competing speech sources\nrelying on localization of the early images of the speakers by sparse\napproximation of the spatial spectra of the virtual sources in a free-space\nmodel. The images are then clustered exploiting the low-rank structure of the\nspectro-temporal components belonging to each source. This enables us to\nidentify the early support of the room impulse response function and its unique\nmap to the room geometry. To further tackle the ambiguity of the reflection\nratios, we propose a novel formulation of the reverberation model and estimate\nthe absorption coefficients through a convex optimization exploiting joint\nsparsity model formulated upon spatio-spectral sparsity of concurrent speech\nrepresentation. The acoustic parameters are then incorporated for separating\nindividual speech signals through either structured sparse recovery or inverse\nfiltering the acoustic channels. The experiments conducted on real data\nrecordings demonstrate the effectiveness of the proposed approach for\nmulti-party speech recovery and recognition.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2012 09:22:59 GMT"}], "update_date": "2012-10-26", "authors_parsed": [["Asaei", "Afsaneh", ""], ["Golbabaee", "Mohammad", ""], ["Bourlard", "Herv\u00e9", ""], ["Cevher", "Volkan", ""]]}, {"id": "1210.6891", "submitter": "Clifton Phua", "authors": "Clifton Phua, Hong Cao, Jo\\~ao B\\'artolo Gomes, Minh Nhut Nguyen", "title": "Predicting Near-Future Churners and Win-Backs in the Telecommunications\n  Industry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this work, we presented the strategies and techniques that we have\ndeveloped for predicting the near-future churners and win-backs for a telecom\ncompany. On a large-scale and real-world database containing customer profiles\nand some transaction data from a telecom company, we first analyzed the data\nschema, developed feature computation strategies and then extracted a large set\nof relevant features that can be associated with the customer churning and\nreturning behaviors. Our features include both the original driver factors as\nwell as some derived features. We evaluated our features on the imbalance\ncorrected dataset, i.e. under-sampled dataset and compare a large number of\nexisting machine learning tools, especially decision tree-based classifiers,\nfor predicting the churners and win-backs. In general, we find RandomForest and\nSimpleCart learning algorithms generally perform well and tend to provide us\nwith highly competitive prediction performance. Among the top-15 driver factors\nthat signal the churn behavior, we find that the service utilization, e.g. last\ntwo months' download and upload volume, last three months' average upload and\ndownload, and the payment related factors are the most indicative features for\npredicting if churn will happen soon. Such features can collectively tell\ndiscrepancies between the service plans, payments and the dynamically changing\nutilization needs of the customers. Our proposed features and their\ncomputational strategy exhibit reasonable precision performance to predict\nchurn behavior in near future.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2012 05:56:45 GMT"}], "update_date": "2012-10-26", "authors_parsed": [["Phua", "Clifton", ""], ["Cao", "Hong", ""], ["Gomes", "Jo\u00e3o B\u00e1rtolo", ""], ["Nguyen", "Minh Nhut", ""]]}, {"id": "1210.6912", "submitter": "Gaurav Pandey", "authors": "Gaurav Pandey and Sahil Manocha and Gowtham Atluri and Vipin Kumar", "title": "Enhancing the functional content of protein interaction networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.CE cs.LG q-bio.GN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein interaction networks are a promising type of data for studying\ncomplex biological systems. However, despite the rich information embedded in\nthese networks, they face important data quality challenges of noise and\nincompleteness that adversely affect the results obtained from their analysis.\nHere, we explore the use of the concept of common neighborhood similarity\n(CNS), which is a form of local structure in networks, to address these issues.\nAlthough several CNS measures have been proposed in the literature, an\nunderstanding of their relative efficacies for the analysis of interaction\nnetworks has been lacking. We follow the framework of graph transformation to\nconvert the given interaction network into a transformed network corresponding\nto a variety of CNS measures evaluated. The effectiveness of each measure is\nthen estimated by comparing the quality of protein function predictions\nobtained from its corresponding transformed network with those from the\noriginal network. Using a large set of S. cerevisiae interactions, and a set of\n136 GO terms, we find that several of the transformed networks produce more\naccurate predictions than those obtained from the original network. In\nparticular, the $HC.cont$ measure proposed here performs particularly well for\nthis task. Further investigation reveals that the two major factors\ncontributing to this improvement are the abilities of CNS measures, especially\n$HC.cont$, to prune out noisy edges and introduce new links between\nfunctionally related proteins.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2012 17:13:57 GMT"}], "update_date": "2012-10-29", "authors_parsed": [["Pandey", "Gaurav", ""], ["Manocha", "Sahil", ""], ["Atluri", "Gowtham", ""], ["Kumar", "Vipin", ""]]}, {"id": "1210.7047", "submitter": "Zhipeng Luo", "authors": "Daifeng Li, Zhipeng Luo, Golden Guo-zheng Sun, Jie Tang, Jingwei Zhang", "title": "User-level Weibo Recommendation incorporating Social Influence based on\n  Semi-Supervised Algorithm", "comments": "to be sumitted in JASIST", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tencent Weibo, as one of the most popular micro-blogging services in China,\nhas attracted millions of users, producing 30-60 millions of weibo (similar as\ntweet in Twitter) daily. With the overload problem of user generate content,\nTencent users find it is more and more hard to browse and find valuable\ninformation at the first time. In this paper, we propose a Factor Graph based\nweibo recommendation algorithm TSI-WR (Topic-Level Social Influence based Weibo\nRecommendation), which could help Tencent users to find most suitable\ninformation. The main innovation is that we consider both direct and indirect\nsocial influence from topic level based on social balance theory. The main\nadvantages of adopting this strategy are that it could first build a more\naccurate description of latent relationship between two users with weak\nconnections, which could help to solve the data sparsity problem; second\nprovide a more accurate recommendation for a certain user from a wider range.\nOther meaningful contextual information is also combined into our model, which\ninclude: Users profile, Users influence, Content of weibos, Topic information\nof weibos and etc. We also design a semi-supervised algorithm to further reduce\nthe influence of data sparisty. The experiments show that all the selected\nvariables are important and the proposed model outperforms several baseline\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2012 03:04:34 GMT"}], "update_date": "2012-10-29", "authors_parsed": [["Li", "Daifeng", ""], ["Luo", "Zhipeng", ""], ["Sun", "Golden Guo-zheng", ""], ["Tang", "Jie", ""], ["Zhang", "Jingwei", ""]]}, {"id": "1210.7054", "submitter": "Youwei Zhang", "authors": "Youwei Zhang, Laurent El Ghaoui", "title": "Large-Scale Sparse Principal Component Analysis with Application to Text\n  Data", "comments": "Appeared in the proceedings of NIPS 2011; The Neural Information\n  Processing Systems Conference (NIPS), Granada, Spain, December 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse PCA provides a linear combination of small number of features that\nmaximizes variance across data. Although Sparse PCA has apparent advantages\ncompared to PCA, such as better interpretability, it is generally thought to be\ncomputationally much more expensive. In this paper, we demonstrate the\nsurprising fact that sparse PCA can be easier than PCA in practice, and that it\ncan be reliably applied to very large data sets. This comes from a rigorous\nfeature elimination pre-processing result, coupled with the favorable fact that\nfeatures in real-life data typically have exponentially decreasing variances,\nwhich allows for many features to be eliminated. We introduce a fast block\ncoordinate ascent algorithm with much better computational complexity than the\nexisting first-order ones. We provide experimental results obtained on text\ncorpora involving millions of documents and hundreds of thousands of features.\nThese results illustrate how Sparse PCA can help organize a large corpus of\ntext data in a user-interpretable way, providing an attractive alternative\napproach to topic models.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2012 05:35:26 GMT"}], "update_date": "2012-10-29", "authors_parsed": [["Zhang", "Youwei", ""], ["Ghaoui", "Laurent El", ""]]}, {"id": "1210.7056", "submitter": "Zhongqi Lu", "authors": "Zhongqi Lu and Erheng Zhong and Lili Zhao and Wei Xiang and Weike Pan\n  and Qiang Yang", "title": "Selective Transfer Learning for Cross Domain Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering (CF) aims to predict users' ratings on items\naccording to historical user-item preference data. In many real-world\napplications, preference data are usually sparse, which would make models\noverfit and fail to give accurate predictions. Recently, several research works\nshow that by transferring knowledge from some manually selected source domains,\nthe data sparseness problem could be mitigated. However for most cases, parts\nof source domain data are not consistent with the observations in the target\ndomain, which may misguide the target domain model building. In this paper, we\npropose a novel criterion based on empirical prediction error and its variance\nto better capture the consistency across domains in CF settings. Consequently,\nwe embed this criterion into a boosting framework to perform selective\nknowledge transfer. Comparing to several state-of-the-art methods, we show that\nour proposed selective transfer learning framework can significantly improve\nthe accuracy of rating prediction tasks on several real-world recommendation\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2012 05:36:57 GMT"}], "update_date": "2012-10-29", "authors_parsed": [["Lu", "Zhongqi", ""], ["Zhong", "Erheng", ""], ["Zhao", "Lili", ""], ["Xiang", "Wei", ""], ["Pan", "Weike", ""], ["Yang", "Qiang", ""]]}, {"id": "1210.7070", "submitter": "Shai Bagon", "authors": "Shai Bagon and Meirav Galun", "title": "A Multiscale Framework for Challenging Discrete Optimization", "comments": "5 pages, 1 figure, To appear in NIPS Workshop on Optimization for\n  Machine Learning (December 2012). Camera-ready version. Fixed typos,\n  acknowledgements added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Current state-of-the-art discrete optimization methods struggle behind when\nit comes to challenging contrast-enhancing discrete energies (i.e., favoring\ndifferent labels for neighboring variables). This work suggests a multiscale\napproach for these challenging problems. Deriving an algebraic representation\nallows us to coarsen any pair-wise energy using any interpolation in a\nprincipled algebraic manner. Furthermore, we propose an energy-aware\ninterpolation operator that efficiently exposes the multiscale landscape of the\nenergy yielding an effective coarse-to-fine optimization scheme. Results on\nchallenging contrast-enhancing energies show significant improvement over\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2012 09:08:55 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2012 13:26:02 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2012 10:11:10 GMT"}], "update_date": "2012-11-05", "authors_parsed": [["Bagon", "Shai", ""], ["Galun", "Meirav", ""]]}, {"id": "1210.7362", "submitter": "Shai Bagon", "authors": "Shai Bagon", "title": "Discrete Energy Minimization, beyond Submodularity: Applications and\n  Approximations", "comments": "Doctoral dissertation, Weizmann Institute of Science. Under the\n  supervision of Prof. Michal Irani and Dr Meirav Galun Corrected typos.\n  Citation added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG math.OC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this thesis I explore challenging discrete energy minimization problems\nthat arise mainly in the context of computer vision tasks. This work motivates\nthe use of such \"hard-to-optimize\" non-submodular functionals, and proposes\nmethods and algorithms to cope with the NP-hardness of their optimization.\nConsequently, this thesis revolves around two axes: applications and\napproximations. The applications axis motivates the use of such\n\"hard-to-optimize\" energies by introducing new tasks. As the energies become\nless constrained and structured one gains more expressive power for the\nobjective function achieving more accurate models. Results show how\nchallenging, hard-to-optimize, energies are more adequate for certain computer\nvision applications. To overcome the resulting challenging optimization tasks\nthe second axis of this thesis proposes approximation algorithms to cope with\nthe NP-hardness of the optimization. Experiments show that these new methods\nyield good results for representative challenging problems.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2012 19:12:49 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2012 21:09:53 GMT"}], "update_date": "2012-11-09", "authors_parsed": [["Bagon", "Shai", ""]]}, {"id": "1210.7461", "submitter": "Cesar Roberto de Souza", "authors": "C\\'esar Roberto de Souza, Ednaldo Brigante Pizzolato, Mauro dos Santos\n  Anjo", "title": "Recognizing Static Signs from the Brazilian Sign Language: Comparing\n  Large-Margin Decision Directed Acyclic Graphs, Voting Support Vector Machines\n  and Artificial Neural Networks", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore and detail our experiments in a\nhigh-dimensionality, multi-class image classification problem often found in\nthe automatic recognition of Sign Languages. Here, our efforts are directed\ntowards comparing the characteristics, advantages and drawbacks of creating and\ntraining Support Vector Machines disposed in a Directed Acyclic Graph and\nArtificial Neural Networks to classify signs from the Brazilian Sign Language\n(LIBRAS). We explore how the different heuristics, hyperparameters and\nmulti-class decision schemes affect the performance, efficiency and ease of use\nfor each classifier. We provide hyperparameter surface maps capturing accuracy\nand efficiency, comparisons between DDAGs and 1-vs-1 SVMs, and effects of\nheuristics when training ANNs with Resilient Backpropagation. We report\nstatistically significant results using Cohen's Kappa statistic for contingency\ntables.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2012 13:55:07 GMT"}], "update_date": "2012-10-30", "authors_parsed": [["de Souza", "C\u00e9sar Roberto", ""], ["Pizzolato", "Ednaldo Brigante", ""], ["Anjo", "Mauro dos Santos", ""]]}, {"id": "1210.7559", "submitter": "Daniel Hsu", "authors": "Anima Anandkumar and Rong Ge and Daniel Hsu and Sham M. Kakade and\n  Matus Telgarsky", "title": "Tensor decompositions for learning latent variable models", "comments": null, "journal-ref": "Journal of Machine Learning Research, 15(Aug):2773-2832, 2014", "doi": null, "report-no": null, "categories": "cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers a computationally and statistically efficient parameter\nestimation method for a wide class of latent variable models---including\nGaussian mixture models, hidden Markov models, and latent Dirichlet\nallocation---which exploits a certain tensor structure in their low-order\nobservable moments (typically, of second- and third-order). Specifically,\nparameter estimation is reduced to the problem of extracting a certain\n(orthogonal) decomposition of a symmetric tensor derived from the moments; this\ndecomposition can be viewed as a natural generalization of the singular value\ndecomposition for matrices. Although tensor decompositions are generally\nintractable to compute, the decomposition of these specially structured tensors\ncan be efficiently obtained by a variety of approaches, including power\niterations and maximization approaches (similar to the case of matrices). A\ndetailed analysis of a robust tensor power method is provided, establishing an\nanalogue of Wedin's perturbation theorem for the singular vectors of matrices.\nThis implies a robust and computationally tractable estimation approach for\nseveral popular latent variable models.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2012 04:38:41 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2012 00:59:17 GMT"}, {"version": "v3", "created": "Sat, 1 Mar 2014 19:06:31 GMT"}, {"version": "v4", "created": "Thu, 13 Nov 2014 22:43:15 GMT"}], "update_date": "2014-11-17", "authors_parsed": [["Anandkumar", "Anima", ""], ["Ge", "Rong", ""], ["Hsu", "Daniel", ""], ["Kakade", "Sham M.", ""], ["Telgarsky", "Matus", ""]]}, {"id": "1210.7657", "submitter": "Antonio Giuliano Zippo Dr.", "authors": "Antonio Giuliano Zippo", "title": "Text Classification with Compression Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work concerns a comparison of SVM kernel methods in text categorization\ntasks. In particular I define a kernel function that estimates the similarity\nbetween two objects computing by their compressed lengths. In fact, compression\nalgorithms can detect arbitrarily long dependencies within the text strings.\nData text vectorization looses information in feature extractions and is highly\nsensitive by textual language. Furthermore, these methods are language\nindependent and require no text preprocessing. Moreover, the accuracy computed\non the datasets (Web-KB, 20ng and Reuters-21578), in some case, is greater than\nGaussian, linear and polynomial kernels. The method limits are represented by\ncomputational time complexity of the Gram matrix and by very poor performance\non non-textual datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2012 13:30:27 GMT"}], "update_date": "2012-10-30", "authors_parsed": [["Zippo", "Antonio Giuliano", ""]]}, {"id": "1210.8291", "submitter": "Huanhuan Chen", "authors": "Huanhuan Chen, Peter Tino, Xin Yao, and Ali Rodan", "title": "Learning in the Model Space for Fault Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of large scaled sensor networks facilitates the collection of\nlarge amounts of real-time data to monitor and control complex engineering\nsystems. However, in many cases the collected data may be incomplete or\ninconsistent, while the underlying environment may be time-varying or\nun-formulated. In this paper, we have developed an innovative cognitive fault\ndiagnosis framework that tackles the above challenges. This framework\ninvestigates fault diagnosis in the model space instead of in the signal space.\nLearning in the model space is implemented by fitting a series of models using\na series of signal segments selected with a rolling window. By investigating\nthe learning techniques in the fitted model space, faulty models can be\ndiscriminated from healthy models using one-class learning algorithm. The\nframework enables us to construct fault library when unknown faults occur,\nwhich can be regarded as cognitive fault isolation. This paper also\ntheoretically investigates how to measure the pairwise distance between two\nmodels in the model space and incorporates the model distance into the learning\nalgorithm in the model space. The results on three benchmark applications and\none simulated model for the Barcelona water distribution network have confirmed\nthe effectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 10:42:32 GMT"}], "update_date": "2012-11-01", "authors_parsed": [["Chen", "Huanhuan", ""], ["Tino", "Peter", ""], ["Yao", "Xin", ""], ["Rodan", "Ali", ""]]}, {"id": "1210.8353", "submitter": "Alex Susemihl", "authors": "Chris H\\\"ausler, Alex Susemihl", "title": "Temporal Autoencoding Restricted Boltzmann Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much work has been done refining and characterizing the receptive fields\nlearned by deep learning algorithms. A lot of this work has focused on the\ndevelopment of Gabor-like filters learned when enforcing sparsity constraints\non a natural image dataset. Little work however has investigated how these\nfilters might expand to the temporal domain, namely through training on natural\nmovies. Here we investigate exactly this problem in established temporal deep\nlearning algorithms as well as a new learning paradigm suggested here, the\nTemporal Autoencoding Restricted Boltzmann Machine (TARBM).\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 14:55:50 GMT"}], "update_date": "2012-11-01", "authors_parsed": [["H\u00e4usler", "Chris", ""], ["Susemihl", "Alex", ""]]}, {"id": "1210.8385", "submitter": "Rupesh Kumar Srivastava", "authors": "Rupesh Kumar Srivastava, Bas R. Steunebrink and J\\\"urgen Schmidhuber", "title": "First Experiments with PowerPlay", "comments": "13 pages, 6 figures. Extends preliminary work presented at\n  ICDL-EpiRob 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Like a scientist or a playing child, PowerPlay not only learns new skills to\nsolve given problems, but also invents new interesting problems by itself. By\ndesign, it continually comes up with the fastest to find, initially novel, but\neventually solvable tasks. It also continually simplifies or compresses or\nspeeds up solutions to previous tasks. Here we describe first experiments with\nPowerPlay. A self-delimiting recurrent neural network SLIM RNN is used as a\ngeneral computational problem solving architecture. Its connection weights can\nencode arbitrary, self-delimiting, halting or non-halting programs affecting\nboth environment (through effectors) and internal states encoding abstractions\nof event sequences. Our PowerPlay-driven SLIM RNN learns to become an\nincreasingly general solver of self-invented problems, continually adding new\nproblem solving procedures to its growing skill repertoire. Extending a recent\nconference paper, we identify interesting, emerging, developmental stages of\nour open-ended system. We also show how it automatically self-modularizes,\nfrequently re-using code for previously invented skills, always trying to\ninvent novel tasks that can be quickly validated because they do not require\ntoo many weight changes affecting too many previous tasks.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 16:41:37 GMT"}], "update_date": "2012-11-01", "authors_parsed": [["Srivastava", "Rupesh Kumar", ""], ["Steunebrink", "Bas R.", ""], ["Schmidhuber", "J\u00fcrgen", ""]]}]