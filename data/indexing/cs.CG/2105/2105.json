[{"id": "2105.00017", "submitter": "Mamoru Doi", "authors": "Mamoru Doi", "title": "Negative 3D gadgets in origami extrusions with a supporting triangle on\n  the back side", "comments": "36 pages, 21 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our previous two papers, we studied (positive) 3D gadgets in origami\nextrusions which create a top face parallel to the ambient paper and two side\nfaces sharing a ridge with two simple outgoing pleats. Then a natural problem\ncomes up whether it is possible to construct a `negative' 3D gadget from any\npositive one having the same net without changing the outgoing pleats, that is,\nto sink the top and two side faces of any positive 3D gadget to the reverse\nside without changing the outgoing pleats. Of course, simply sinking the faces\ncauses a tear of the paper, and thus we have to modify the crease pattern.\nThere are two known constructions of negative 3D gadgets before ours, but they\ndo not solve this problem because their outgoing pleats are different from\npositive ones. In the present paper we give an affirmative solution to the\nabove problem. For this purpose, we present three constructions of negative 3D\ngadgets with a supporting triangle on the back side, which are based on our\nprevious ones of positive 3D gadgets. The first two are an extension of those\npresented in our previous paper, and the third is new. We prove that our first\nand third constructions solve the problem. Our solutions enable us to deal with\npositive and negative 3D gadgets on the same basis, so that we can construct\nfrom an origami extrusion constructed with 3D gadgets its negative using the\nsame pleats if there are no interferences among the 3D gadgets. We also treat\nrepetition/division of negative 3D gadgets under certain conditions, which\nreduces their interferences with others.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 18:05:15 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Doi", "Mamoru", ""]]}, {"id": "2105.00518", "submitter": "Tao Hou", "authors": "Tamal K. Dey, Tao Hou", "title": "Computing Optimal Persistent Cycles for Levelset Zigzag on Manifold-like\n  Complexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In standard persistent homology, a persistent cycle born and dying with a\npersistence interval (bar) associates the bar with a concrete topological\nrepresentative, which provides means to effectively navigate back from the\nbarcode to the topological space. Among the possibly many, optimal persistent\ncycles bring forth further information due to having guaranteed quality.\nHowever, topological features usually go through variations in the lifecycle of\na bar which a single persistent cycle may not capture. Hence, for persistent\nhomology induced from PL functions, we propose levelset persistent cycles\nconsisting of a sequence of cycles that depict the evolution of homological\nfeatures from birth to death. Our definition is based on levelset zigzag\npersistence which involves four types of persistence intervals as opposed to\nthe two types in standard persistence. For each of the four types, we present a\npolynomial-time algorithm computing an optimal sequence of levelset persistent\n$p$-cycles for the so-called weak $(p+1)$-pseudomanifolds. Given that optimal\ncycle problems for homology are NP-hard in general, our results are useful in\npractice because weak pseudomanifolds do appear in applications. Our algorithms\ndraw upon an idea of relating optimal cycles to min-cuts in a graph that we\nexploited earlier for standard persistent cycles. Note that levelset zigzag\nposes non-trivial challenges for the approach because a sequence of optimal\ncycles instead of a single one needs to be computed in this case.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 17:55:31 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Dey", "Tamal K.", ""], ["Hou", "Tao", ""]]}, {"id": "2105.00656", "submitter": "Shankar Sastry", "authors": "Shankar P Sastry", "title": "A 3D Advancing-Front Delaunay Mesh Refinement Algorithm", "comments": "28 pages, 13 figures; submitted to Computational Geometry: Theory and\n  Application", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  I present a 3D advancing-front mesh refinement algorithm that generates a\nconstrained Delaunay mesh for any piecewise linear complex (PLC) and extend\nthis algorithm to produce truly Delaunay meshes for any PLC. First, as in my\nrecently published 2D algorithm, I split the input line segments such that the\nlength of the subsegments is asymptotically proportional to the local feature\nsize (LFS). For each facet, I refine the mesh such that the edge lengths and\nthe radius of the circumcircle of every triangular element are asymptotically\nproportional to the LFS. Finally, I refine the volume mesh to produce a\nconstrained Delaunay mesh whose tetrahedral elements are well graded and have a\nradius-edge ratio less than some $\\omega^* > 2/\\sqrt{3}$ (except ``near'' small\ninput angles). I extend this algorithm to generate truly Delaunay meshes by\nensuring that every triangular element on a facet satisfies Gabriel's\ncondition, i.e., its diametral sphere is empty. On an ``apex'' vertex where\nmultiple facets intersect, Gabriel's condition is satisfied by a modified\nsplit-on-a-sphere (SOS) technique. On a line where multiple facets intersect,\nGabriel's condition is satisfied by mirroring meshes near the line of\nintersection. The SOS technique ensures that the triangles on a facet near the\napex vertex have angles that are proportional to the angular feature size\n(AFS), a term I define in the paper. All tetrahedra (except ``near'' small\ninput angles) are well graded and have a radius-edge ratio less than $\\omega^*\n> \\sqrt{2}$ for a truly Delaunay mesh. The upper bounds for the radius-edge\nratio are an improvement by a factor of $\\sqrt{2}$ over current\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 07:13:46 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Sastry", "Shankar P", ""]]}, {"id": "2105.01390", "submitter": "Siddharth Barman", "authors": "Siddharth Barman, Ramakrishnan Krishnamurthy, Saladi Rahul", "title": "Optimal Algorithms for Range Searching over Multi-Armed Bandits", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a multi-armed bandit (MAB) version of the range-searching\nproblem. In its basic form, range searching considers as input a set of points\n(on the real line) and a collection of (real) intervals. Here, with each\nspecified point, we have an associated weight, and the problem objective is to\nfind a maximum-weight point within every given interval.\n  The current work addresses range searching with stochastic weights: each\npoint corresponds to an arm (that admits sample access) and the point's weight\nis the (unknown) mean of the underlying distribution. In this MAB setup, we\ndevelop sample-efficient algorithms that find, with high probability,\nnear-optimal arms within the given intervals, i.e., we obtain PAC (probably\napproximately correct) guarantees. We also provide an algorithm for a\ngeneralization wherein the weight of each point is a multi-dimensional vector.\nThe sample complexities of our algorithms depend, in particular, on the size of\nthe optimal hitting set of the given intervals.\n  Finally, we establish lower bounds proving that the obtained sample\ncomplexities are essentially tight. Our results highlight the significance of\ngeometric constructs -- specifically, hitting sets -- in our MAB setting.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 09:52:16 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Barman", "Siddharth", ""], ["Krishnamurthy", "Ramakrishnan", ""], ["Rahul", "Saladi", ""]]}, {"id": "2105.01961", "submitter": "Youjia Zhou", "authors": "Youjia Zhou, Nathaniel Saul, Ilkin Safarli, Bala Krishnamoorthy, Bei\n  Wang", "title": "Stitch Fix for Mapper and Information Gains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mapper construction is a powerful tool from topological data analysis\nthat is designed for the analysis and visualization of multivariate data. In\nthis paper, we investigate a method for stitching a pair of univariate mappers\ntogether into a bivariate mapper and study topological notions of information\ngains during such a process. We further provide implementations that visualize\nsuch information gains.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 10:15:33 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Zhou", "Youjia", ""], ["Saul", "Nathaniel", ""], ["Safarli", "Ilkin", ""], ["Krishnamoorthy", "Bala", ""], ["Wang", "Bei", ""]]}, {"id": "2105.02483", "submitter": "Jongmin Choi", "authors": "Jongmin Choi, Dahye Jeong, Hee-Kap Ahn", "title": "Covering Convex Polygons by Two Congruent Disks", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the planar two-center problem for a convex polygon: given a\nconvex polygon in the plane, find two congruent disks of minimum radius whose\nunion contains the polygon. We present an $O(n\\log n)$-time algorithm for the\ntwo-center problem for a convex polygon, where $n$ is the number of vertices of\nthe polygon. This improves upon the previous best algorithm for the problem.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 07:31:27 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 03:07:36 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 01:31:44 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Choi", "Jongmin", ""], ["Jeong", "Dahye", ""], ["Ahn", "Hee-Kap", ""]]}, {"id": "2105.02827", "submitter": "Eklavya Sharma", "authors": "Arindam Khan, Eklavya Sharma", "title": "Tight Approximation Algorithms for Geometric Bin Packing with Skewed\n  Items", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Two-dimensional Bin Packing (2BP) problem, we are given a set of\nrectangles of height and width at most one and our goal is to find an\naxis-aligned nonoverlapping packing of these rectangles into the minimum number\nof unit square bins. The problem admits no APTAS and the current best\napproximation ratio is $1.406$ by Bansal and Khan [SODA'14]. A well-studied\nvariant of the problem is Guillotine Two-dimensional Bin Packing (G2BP), where\nall rectangles must be packed in such a way that every rectangle in the packing\ncan be obtained by recursively applying a sequence of end-to-end axis-parallel\ncuts, also called guillotine cuts. Bansal, Lodi, and Sviridenko [FOCS'05]\nobtained an APTAS for this problem. Let $\\lambda$ be the smallest constant such\nthat for every set $I$ of items, the number of bins in the optimal solution to\nG2BP for $I$ is upper bounded by $\\lambda\\operatorname{opt}(I) + c$, where\n$\\operatorname{opt}(I)$ is the number of bins in the optimal solution to 2BP\nfor $I$ and $c$ is a constant. It is known that $4/3 \\le \\lambda \\le 1.692$.\nBansal and Khan [SODA'14] conjectured that $\\lambda = 4/3$. The conjecture, if\ntrue, will imply a $(4/3+\\varepsilon)$-approximation algorithm for 2BP.\nAccording to convention, for a given constant $\\delta>0$, a rectangle is large\nif both its height and width are at least $\\delta$, and otherwise it is called\nskewed. We make progress towards the conjecture by showing $\\lambda = 4/3$ for\nskewed instance, i.e., when all input rectangles are skewed. Even for this\ncase, the previous best upper bound on $\\lambda$ was roughly 1.692. We also\ngive an APTAS for 2BP for skewed instance, though general 2BP does not admit an\nAPTAS.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:16:11 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Khan", "Arindam", ""], ["Sharma", "Eklavya", ""]]}, {"id": "2105.02837", "submitter": "Kevin Buchin", "authors": "Kevin Buchin, Mart Hagedoorn, Irina Kostitsyna, Max van Mulken", "title": "Dots & Boxes is PSPACE-complete", "comments": "18 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exactly 20 years ago at MFCS, Demaine posed the open problem whether the game\nof Dots & Boxes is PSPACE-complete. Dots & Boxes has been studied extensively,\nwith for instance a chapter in Berlekamp et al. \"Winning Ways for Your\nMathematical Plays\", a whole book on the game \"The Dots and Boxes Game:\nSophisticated Child's Play\" by Berlekamp, and numerous articles in the \"Games\nof No Chance\" series. While known to be NP-hard, the question of its complexity\nremained open. We resolve this question, proving that the game is\nPSPACE-complete by a reduction from a game played on propositional formulas.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:26:23 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Buchin", "Kevin", ""], ["Hagedoorn", "Mart", ""], ["Kostitsyna", "Irina", ""], ["van Mulken", "Max", ""]]}, {"id": "2105.02914", "submitter": "Andrew Alseth", "authors": "Andrew Alseth and Daniel Hader and Matthew J. Patitz", "title": "Self-Replication via Tile Self-Assembly (extended abstract)", "comments": "Updated to include changes reflecting reviewer comments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present a model containing modifications to the\nSignal-passing Tile Assembly Model (STAM), a tile-based self-assembly model\nwhose tiles are capable of activating and deactivating glues based on the\nbinding of other glues. These modifications consist of an extension to 3D, the\nability of tiles to form \"flexible\" bonds that allow bound tiles to rotate\nrelative to each other, and allowing tiles of multiple shapes within the same\nsystem. We call this new model the STAM*, and we present a series of\nconstructions within it that are capable of self-replicating behavior. Namely,\nthe input seed assemblies to our STAM* systems can encode either \"genomes\"\nspecifying the instructions for building a target shape, or can be copies of\nthe target shape with instructions built in. A universal tile set exists for\nany target shape (at scale factor 2), and from a genome assembly creates\ninfinite copies of the genome as well as the target shape. An input target\nstructure, on the other hand, can be \"deconstructed\" by the universal tile set\nto form a genome encoding it, which will then replicate and also initiate the\ngrowth of copies of assemblies of the target shape. Since the lengths of the\ngenomes for these constructions are proportional to the number of points in the\ntarget shape, we also present a replicator which utilizes hierarchical\nself-assembly to greatly reduce the size of the genomes required. The main\ngoals of this work are to examine minimal requirements of self-assembling\nsystems capable of self-replicating behavior, with the aim of better\nunderstanding self-replication in nature as well as understanding the\ncomplexity of mimicking it.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 18:51:06 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 19:20:16 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Alseth", "Andrew", ""], ["Hader", "Daniel", ""], ["Patitz", "Matthew J.", ""]]}, {"id": "2105.02956", "submitter": "Pierre-Alain Fayolle", "authors": "Markus Friedrich and Pierre-Alain Fayolle", "title": "Reconstruction of Convex Polytope Compositions from 3D Point-clouds", "comments": "In Proceedings of the 16th International Joint Conference on Computer\n  Vision, Imaging and Computer Graphics Theory and Applications - Volume 1:\n  GRAPP, ISBN 978-989-758-488-6 ISSN 2184-4321, pages 75-84", "journal-ref": null, "doi": "10.5220/0010297100750084", "report-no": null, "categories": "cs.CV cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reconstructing a composition (union) of convex polytopes that perfectly fits\nthe corresponding input point-cloud is a hard optimization problem with\ninteresting applications in reverse engineering and rigid body dynamics\nsimulations. We propose a pipeline that first extracts a set of planes, then\npartitions the input point-cloud into weakly convex clusters and finally\ngenerates a set of convex polytopes as the intersection of fitted planes for\neach partition. Finding the best-fitting convex polytopes is formulated as a\ncombinatorial optimization problem over the set of fitted planes and is solved\nusing an Evolutionary Algorithm. For convex clustering, we employ two different\nmethods and detail their strengths and weaknesses in a thorough evaluation\nbased on multiple input data-sets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 00:14:55 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Friedrich", "Markus", ""], ["Fayolle", "Pierre-Alain", ""]]}, {"id": "2105.03386", "submitter": "Rak-Kyeong Seong", "authors": "Rak-Kyeong Seong, Chanho Min, Sang-Hoon Han, Jaeho Yang, Seungwoo Nam,\n  Kyusam Oh", "title": "Topology and Routing Problems: The Circular Frame", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we solve the problem of finding non-intersecting paths between\npoints on a plane with a new approach by borrowing ideas from geometric\ntopology, in particular, from the study of polygonal schema in mathematics. We\nuse a topological transformation on the 2-dimensional planar routing\nenvironment that simplifies the routing problem into a problem of connecting\npoints on a circle with straight line segments that do not intersect in the\ninterior of the circle. These points are either the points that need to be\nconnected by non-intersecting paths or special `reference' points that\nparametrize the topology of the original environment prior to the\ntransformation. When all the necessary points on the circle are fully\nconnected, the transformation is reversed such that the line segments combine\nto become the non-intersecting paths that connect the start and end points in\nthe original environment. We interpret the transformed environment in which the\nrouting problem is solved as a new data structure where any routing problem can\nbe solved efficiently. We perform experiments and show that the routing time\nand success rate of the new routing algorithm outperforms the ones for the\nA*-algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 16:46:05 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Seong", "Rak-Kyeong", ""], ["Min", "Chanho", ""], ["Han", "Sang-Hoon", ""], ["Yang", "Jaeho", ""], ["Nam", "Seungwoo", ""], ["Oh", "Kyusam", ""]]}, {"id": "2105.04712", "submitter": "Aleksandar Nikolov", "authors": "Deepanshu Kush, Aleksandar Nikolov, Haohua Tang", "title": "Near Neighbor Search via Efficient Average Distortion Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent series of papers by Andoni, Naor, Nikolov, Razenshteyn, and\nWaingarten (STOC 2018, FOCS 2018) has given approximate near neighbour search\n(NNS) data structures for a wide class of distance metrics, including all\nnorms. In particular, these data structures achieve approximation on the order\nof $p$ for $\\ell_p^d$ norms with space complexity nearly linear in the dataset\nsize $n$ and polynomial in the dimension $d$, and query time sub-linear in $n$\nand polynomial in $d$. The main shortcoming is the exponential in $d$\npre-processing time required for their construction.\n  In this paper, we describe a more direct framework for constructing NNS data\nstructures for general norms. More specifically, we show via an algorithmic\nreduction that an efficient NNS data structure for a given metric is implied by\nan efficient average distortion embedding of it into $\\ell_1$ or into Euclidean\nspace. In particular, the resulting data structures require only polynomial\npre-processing time, as long as the embedding can be computed in polynomial\ntime. As a concrete instantiation of this framework, we give an NNS data\nstructure for $\\ell_p$ with efficient pre-processing that matches the\napproximation factor, space and query complexity of the aforementioned data\nstructure of Andoni et al. On the way, we resolve a question of Naor (Analysis\nand Geometry in Metric Spaces, 2014) and provide an explicit, efficiently\ncomputable embedding of $\\ell_p$, for $p \\ge 2$, into $\\ell_2$ with (quadratic)\naverage distortion on the order of $p$. We expect our approach to pave the way\nfor constructing efficient NNS data structures for all norms.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 23:49:35 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Kush", "Deepanshu", ""], ["Nikolov", "Aleksandar", ""], ["Tang", "Haohua", ""]]}, {"id": "2105.05151", "submitter": "Aruni Choudhary", "authors": "Aruni Choudhary, Michael Kerber, Sharath Raghvendra", "title": "Improved Approximate Rips Filtrations with Shifted Integer Lattices and\n  Cubical Complexes", "comments": "To appear in Journal of Applied and Computational Topology. arXiv\n  admin note: substantial text overlap with arXiv:1706.07399", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.AT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rips complexes are important structures for analyzing topological features of\nmetric spaces. Unfortunately, generating these complexes is expensive because\nof a combinatorial explosion in the complex size. For $n$ points in\n$\\mathbb{R}^d$, we present a scheme to construct a $2$-approximation of the\nfiltration of the Rips complex in the $L_\\infty$-norm, which extends to a\n$2d^{0.25}$-approximation in the Euclidean case. The $k$-skeleton of the\nresulting approximation has a total size of $n2^{O(d\\log k +d)}$. The scheme is\nbased on the integer lattice and simplicial complexes based on the barycentric\nsubdivision of the $d$-cube.\n  We extend our result to use cubical complexes in place of simplicial\ncomplexes by introducing cubical maps between complexes. We get the same\napproximation guarantee as the simplicial case, while reducing the total size\nof the approximation to only $n2^{O(d)}$ (cubical) cells.\n  There are two novel techniques that we use in this paper. The first is the\nuse of acyclic carriers for proving our approximation result. In our\napplication, these are maps which relate the Rips complex and the approximation\nin a relatively simple manner and greatly reduce the complexity of showing the\napproximation guarantee. The second technique is what we refer to as scale\nbalancing, which is a simple trick to improve the approximation ratio under\ncertain conditions.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 16:07:35 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Choudhary", "Aruni", ""], ["Kerber", "Michael", ""], ["Raghvendra", "Sharath", ""]]}, {"id": "2105.05183", "submitter": "Juan Xu", "authors": "Ruben Becker, Michael Sagraloff, Vikram Sharma, Juan Xu, Chee Yap", "title": "Complexity Analysis of Root Clustering for a Complex Polynomial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $F(z)$ be an arbitrary complex polynomial. We introduce the local root\nclustering problem, to compute a set of natural $\\varepsilon$-clusters of roots\nof $F(z)$ in some box region $B_0$ in the complex plane. This may be viewed as\nan extension of the classical root isolation problem. Our contribution is\ntwo-fold: we provide an efficient certified subdivision algorithm for this\nproblem, and we provide a bit-complexity analysis based on the local geometry\nof the root clusters.\n  Our computational model assumes that arbitrarily good approximations of the\ncoefficients of $F$ are provided by means of an oracle at the cost of reading\nthe coefficients. Our algorithmic techniques come from a companion paper\n(Becker et al., 2018) and are based on the Pellet test, Graeffe and Newton\niterations, and are independent of Sch\\\"onhage's splitting circle method. Our\nalgorithm is relatively simple and promises to be efficient in practice.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 16:36:19 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Becker", "Ruben", ""], ["Sagraloff", "Michael", ""], ["Sharma", "Vikram", ""], ["Xu", "Juan", ""], ["Yap", "Chee", ""]]}, {"id": "2105.05275", "submitter": "Federico L\\'opez", "authors": "Federico L\\'opez, Beatrice Pozzetti, Steve Trettel, Anna Wienhard", "title": "Hermitian Symmetric Spaces for Graph Embeddings", "comments": "13 pages, 1 figure. Accepted at NeurIPS 2020 workshop on Differential\n  Geometry meets Deep Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning faithful graph representations as sets of vertex embeddings has\nbecome a fundamental intermediary step in a wide range of machine learning\napplications. The quality of the embeddings is usually determined by how well\nthe geometry of the target space matches the structure of the data. In this\nwork we learn continuous representations of graphs in spaces of symmetric\nmatrices over C. These spaces offer a rich geometry that simultaneously admits\nhyperbolic and Euclidean subspaces, and are amenable to analysis and explicit\ncomputations. We implement an efficient method to learn embeddings and compute\ndistances, and develop the tools to operate with such spaces. The proposed\nmodels are able to automatically adapt to very dissimilar arrangements without\nany apriori estimates of graph features. On various datasets with very diverse\nstructural properties and reconstruction measures our model ties the results of\ncompetitive baselines for geometrically pure graphs and outperforms them for\ngraphs with mixed geometric features, showcasing the versatility of our\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 18:14:52 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["L\u00f3pez", "Federico", ""], ["Pozzetti", "Beatrice", ""], ["Trettel", "Steve", ""], ["Wienhard", "Anna", ""]]}, {"id": "2105.05784", "submitter": "Christian Rieck", "authors": "Jakob Keller, Christian Rieck, Christian Scheffer, Arne Schmidt", "title": "Particle-Based Assembly Using Precise Global Control", "comments": "20 pages, 12 figures, full version of an extended abstract accepted\n  for publication in the proceedings of the 17th Algorithms and Data Structures\n  Symposium (WADS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In micro- and nano-scale systems, particles can be moved by using an external\nforce like gravity or a magnetic field. In the presence of adhesive particles\nthat can attach to each other, the challenge is to decide whether a shape is\nconstructible. Previous work provides a class of shapes for which\nconstructibility can be decided efficiently, when particles move maximally into\nthe same direction on actuation.\n  In this paper, we consider a stronger model. On actuation, each particle\nmoves one unit step into the given direction. We prove that deciding\nconstructibility is NP-hard for three-dimensional shapes, and that a maximum\nconstructible shape can be approximated. The same approximation algorithm\napplies for 2D. We further present linear-time algorithms to decide whether a\ntree-shape in 2D or 3D is constructible. If scaling is allowed, we show that\nthe $c$-scaled copy of every non-degenerate polyomino is constructible, for\nevery $c \\geq 2$.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:00:12 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Keller", "Jakob", ""], ["Rieck", "Christian", ""], ["Scheffer", "Christian", ""], ["Schmidt", "Arne", ""]]}, {"id": "2105.06669", "submitter": "Warut Suksompong", "authors": "Edith Elkind, Erel Segal-Halevi, Warut Suksompong", "title": "Keep Your Distance: Land Division With Separation", "comments": "Appears in the 30th International Joint Conference on Artificial\n  Intelligence (IJCAI), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is part of an ongoing endeavor to bring the theory of fair\ndivision closer to practice by handling requirements from real-life\napplications. We focus on two requirements originating from the division of\nland estates: (1) each agent should receive a plot of a usable geometric shape,\nand (2) plots of different agents must be physically separated. With these\nrequirements, the classic fairness notion of proportionality is impractical,\nsince it may be impossible to attain any multiplicative approximation of it. In\ncontrast, the ordinal maximin share approximation, introduced by Budish in\n2011, provides meaningful fairness guarantees. We prove upper and lower bounds\non achievable maximin share guarantees when the usable shapes are squares, fat\nrectangles, or arbitrary axes-aligned rectangles, and explore the algorithmic\nand query complexity of finding fair partitions in this setting.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 07:03:21 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Elkind", "Edith", ""], ["Segal-Halevi", "Erel", ""], ["Suksompong", "Warut", ""]]}, {"id": "2105.06992", "submitter": "Fabrizio Montecchiani", "authors": "Therese Biedl and Giuseppe Liotta and Jayson Lynch and Fabrizio\n  Montecchiani", "title": "Generalized LR-drawings of trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The LR-drawing-method is a method of drawing an ordered rooted binary tree\nbased on drawing one root-to-leaf path on a vertical line and attaching\nrecursively obtained drawings of the subtrees on the left and right. In this\npaper, we study how to generalize this drawing-method to trees of higher arity.\nWe first prove that (with some careful modifications) the proof of existence of\na special root-to-leaf path transfers to trees of higher arity. Then we use\nsuch paths to obtain generalized LR-drawings of trees of arbitrary arity.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 17:56:59 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Biedl", "Therese", ""], ["Liotta", "Giuseppe", ""], ["Lynch", "Jayson", ""], ["Montecchiani", "Fabrizio", ""]]}, {"id": "2105.07025", "submitter": "Lu Li", "authors": "Lu Li, Connor Thompson, Gregory Henselman-Petrusek, Chad Giusti, Lori\n  Ziegelmeier", "title": "Minimal Cycle Representatives in Persistent Homology using Linear\n  Programming: an Empirical Study with User's Guide", "comments": null, "journal-ref": null, "doi": "10.3389/frai.2021.681117", "report-no": null, "categories": "math.AT cs.CG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cycle representatives of persistent homology classes can be used to provide\ndescriptions of topological features in data. However, the non-uniqueness of\nthese representatives creates ambiguity and can lead to many different\ninterpretations of the same set of classes. One approach to solving this\nproblem is to optimize the choice of representative against some measure that\nis meaningful in the context of the data. In this work, we provide a study of\nthe effectiveness and computational cost of several $\\ell_1$-minimization\noptimization procedures for constructing homological cycle bases for persistent\nhomology with rational coefficients in dimension one, including\nuniform-weighted and length-weighted edge-loss algorithms as well as\nuniform-weighted and area-weighted triangle-loss algorithms. We conduct these\noptimizations via standard linear programming methods, applying general-purpose\nsolvers to optimize over column bases of simplicial boundary matrices.\n  Our key findings are: (i) optimization is effective in reducing the size of\ncycle representatives, (ii) the computational cost of optimizing a basis of\ncycle representatives exceeds the cost of computing such a basis in most data\nsets we consider, (iii) the choice of linear solvers matters a lot to the\ncomputation time of optimizing cycles, (iv) the computation time of solving an\ninteger program is not significantly longer than the computation time of\nsolving a linear program for most of the cycle representatives, using the\nGurobi linear solver, (v) strikingly, whether requiring integer solutions or\nnot, we almost always obtain a solution with the same cost and almost all\nsolutions found have entries in {-1, 0, 1} and therefore, are also solutions to\na restricted $\\ell_0$ optimization problem, and (vi) we obtain qualitatively\ndifferent results for generators in Erd\\H{o}s-R\\'enyi random clique complexes.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 18:38:48 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 23:39:25 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Li", "Lu", ""], ["Thompson", "Connor", ""], ["Henselman-Petrusek", "Gregory", ""], ["Giusti", "Chad", ""], ["Ziegelmeier", "Lori", ""]]}, {"id": "2105.07892", "submitter": "Rak-Kyeong Seong", "authors": "Rak-Kyeong Seong, Jaeho Yang, Sang-Hoon Han", "title": "Topology for Substrate Routing in Semiconductor Package Design", "comments": "24 pages, 22 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a new signal routing method for solving routing\nproblems that occur in the design process of semiconductor package substrates.\nOur work uses a topological transformation of the layers of the package\nsubstrate in order to simplify the routing problem into a problem of connecting\npoints on a circle with non-intersecting straight line segments. The circle,\nwhich we call the Circular Frame, is a polygonal schema, which is originally\nused in topology to study the topological structure of 2-manifolds. We show\nthrough experiments that our new routing method based on the Circular Frame\ncompetes with certain grid-based routing algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 16:40:17 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Seong", "Rak-Kyeong", ""], ["Yang", "Jaeho", ""], ["Han", "Sang-Hoon", ""]]}, {"id": "2105.07997", "submitter": "Irene Parada", "authors": "Irina Kostitsyna, Irene Parada, Willem Sonke, Bettina Speckmann, Jules\n  Wulms", "title": "Compacting Squares", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Edge-connected configurations of squares are a common model for modular\nrobots in two dimensions. A well-established way to reconfigure such modular\nrobots are so-called sliding moves. Dumitrescu and Pach [Graphs and\nCombinatorics, 2006] proved that it is always possible to reconfigure one\nedge-connected configuration of $n$ squares into any other using at most\n$O(n^2)$ sliding moves, while keeping the configuration connected at all times.\n  For certain configurations $\\Omega(n^2)$ sliding moves are necessary.\nHowever, significantly fewer moves may be sufficient. In this paper we present\na novel input-sensitive in-place algorithm which requires only $O(\\bar{P} n)$\nsliding moves to transform one configuration into the other, where $\\bar{P}$ is\nthe maximum perimeter of the respective bounding boxes. Our Gather&Compact\nalgorithm is built on the basic principle that well-connected components of\nmodular robots can be transformed efficiently. Hence we iteratively increase\nthe connectivity within a configuration, to finally arrive at a single solid\n$xy$-monotone component.\n  We implemented Gather&Compact and compared it experimentally to the in-place\nmodification by Moreno and Sacrist\\'an [EuroCG 2020] of the Dumitrescu and Pach\nalgorithm (MSDP). Our experiments show that Gather&Compact consistently\noutperforms MSDP by a significant margin, on all types of square\nconfigurations.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 16:28:25 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Kostitsyna", "Irina", ""], ["Parada", "Irene", ""], ["Sonke", "Willem", ""], ["Speckmann", "Bettina", ""], ["Wulms", "Jules", ""]]}, {"id": "2105.08016", "submitter": "Ge Zhang", "authors": "Ge Zhang, Or Litany, Srinath Sridhar, Leonidas Guibas", "title": "StrobeNet: Category-Level Multiview Reconstruction of Articulated\n  Objects", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present StrobeNet, a method for category-level 3D reconstruction of\narticulating objects from one or more unposed RGB images. Reconstructing\ngeneral articulating object categories % has important applications, but is\nchallenging since objects can have wide variation in shape, articulation,\nappearance and topology. We address this by building on the idea of\ncategory-level articulation canonicalization -- mapping observations to a\ncanonical articulation which enables correspondence-free multiview aggregation.\nOur end-to-end trainable neural network estimates feature-enriched canonical 3D\npoint clouds, articulation joints, and part segmentation from one or more\nunposed images of an object. These intermediate estimates are used to generate\na final implicit 3D reconstruction.Our approach reconstructs objects even when\nthey are observed in different articulations in images with large baselines,\nand animation of reconstructed shapes. Quantitative and qualitative evaluations\non different object categories show that our method is able to achieve high\nreconstruction accuracy, especially as more views are added.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 17:05:42 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhang", "Ge", ""], ["Litany", "Or", ""], ["Sridhar", "Srinath", ""], ["Guibas", "Leonidas", ""]]}, {"id": "2105.08113", "submitter": "Omer Bobrowski", "authors": "Yohai Reani, Omer Bobrowski", "title": "A Coupled Alpha Complex", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The alpha complex is a subset of the Delaunay triangulation and is often used\nin computational geometry and topology. One of the main drawbacks of using the\nalpha complex is that it is non-monotone, in the sense that if ${\\cal\nX}\\subset{\\cal X}'$ it is not necessarily (and generically not) the case that\nthe corresponding alpha complexes satisfy ${\\cal A}_r({\\cal X})\\subset{\\cal\nA}_r({\\cal X}')$. The lack of monotonicity may introduce significant\ncomputational costs when using the alpha complex, and in some cases even render\nit unusable. In this work we present a new construction based on the alpha\ncomplex, that is homotopy equivalent to the alpha complex while maintaining\nmonotonicity. We provide the formal definitions and algorithms required to\nconstruct this complex, and to compute its homology. In addition, we analyze\nthe size of this complex in order to argue that it is not significantly more\ncostly to use than the standard alpha complex.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 18:50:03 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Reani", "Yohai", ""], ["Bobrowski", "Omer", ""]]}, {"id": "2105.08115", "submitter": "Eduardo Fernandez", "authors": "Eduardo Fern\\'andez, Can Ayas, Matthijs Langelaar, Pierre Duysinx", "title": "Topology Optimization for Large-Scale Additive Manufacturing: Generating\n  designs tailored to the deposition nozzle size", "comments": null, "journal-ref": "Virtual and Physical Prototyping (2021): 1-25", "doi": "10.1080/17452759.2021.1914893", "report-no": null, "categories": "cs.CG math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Additive Manufacturing (AM) processes intended for large scale components\ndeposit large volumes of material to shorten process duration. This reduces the\nresolution of the AM process, which is typically defined by the size of the\ndeposition nozzle. If the resolution limitation is not considered when\ndesigning for Large-Scale Additive Manufacturing (LSAM), difficulties can arise\nin the manufacturing process, which may require the adaptation of the\ndeposition parameters. This work incorporates the nozzle size constraint into\nTopology Optimization (TO) in order to generate optimized designs suitable to\nthe process resolution. This article proposes and compares two methods, which\nare based on existing TO techniques that enable control of minimum and maximum\nmember size, and of minimum cavity size. The first method requires the minimum\nand maximum member size to be equal to the deposition nozzle size, thus design\nfeatures of uniform width are obtained in the optimized design. The second\nmethod defines the size of the solid members sufficiently small for the\nresulting structure to resemble a structural skeleton, which can be interpreted\nas the deposition path. Through filtering and projection techniques, the thin\nstructures are thickened according to the chosen nozzle size. Thus, a topology\ntailored to the size of the deposition nozzle is obtained along with a\ndeposition proposal. The methods are demonstrated and assessed using 2D and 3D\nbenchmark problems.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 18:55:15 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Fern\u00e1ndez", "Eduardo", ""], ["Ayas", "Can", ""], ["Langelaar", "Matthijs", ""], ["Duysinx", "Pierre", ""]]}, {"id": "2105.08124", "submitter": "Giordano Da Lozzo", "authors": "Steven Chaplick and Giordano Da Lozzo and Emilio Di Giacomo and\n  Giuseppe Liotta and Fabrizio Montecchiani", "title": "Planar Drawings with Few Slopes of Halin Graphs and Nested Pseudotrees", "comments": "Extended version of \"Planar Drawings with Few Slopes of Halin Graphs\n  and Nested Pseudotrees\" to appear in the Proceedings of the 17th Algorithms\n  and Data Structures Symposium (WADS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The $\\textit{planar slope number}$ $psn(G)$ of a planar graph $G$ is the\nminimum number of edge slopes in a planar straight-line drawing of $G$. It is\nknown that $psn(G) \\in O(c^\\Delta)$ for every planar graph $G$ of degree\n$\\Delta$. This upper bound has been improved to $O(\\Delta^5)$ if $G$ has\ntreewidth three, and to $O(\\Delta)$ if $G$ has treewidth two. In this paper we\nprove $psn(G) \\in \\Theta(\\Delta)$ when $G$ is a Halin graph, and thus has\ntreewidth three. Furthermore, we present the first polynomial upper bound on\nthe planar slope number for a family of graphs having treewidth four. Namely we\nshow that $O(\\Delta^2)$ slopes suffice for nested pseudotrees.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 19:28:44 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Chaplick", "Steven", ""], ["Da Lozzo", "Giordano", ""], ["Di Giacomo", "Emilio", ""], ["Liotta", "Giuseppe", ""], ["Montecchiani", "Fabrizio", ""]]}, {"id": "2105.08305", "submitter": "Andr\\'e van Renssen", "authors": "Zachary Abel, Hugo Akitaya, Man-Kwun Chiu, Erik D. Demaine, Martin L.\n  Demaine, Adam Hesterberg, Matias Korman, Jayson Lynch, Andr\\'e van Renssen,\n  Marcel Roeloffzen", "title": "Snipperclips: Cutting Tools into Desired Polygons using Themselves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study Snipperclips, a computer puzzle game whose objective is to create a\ntarget shape with two tools. The tools start as constant-complexity shapes, and\neach tool can snip (i.e., subtract its current shape from) the other tool. We\nstudy the computational problem of, given a target shape represented by a\npolygonal domain of $n$ vertices, is it possible to create it as one of the\ntools' shape via a sequence of snip operations? If so, how many snip operations\nare required? We consider several variants of the problem (such as allowing the\ntools to be disconnected and/or using an undo operation) and bound the number\nof operations needed for each of the variants.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 06:45:33 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Abel", "Zachary", ""], ["Akitaya", "Hugo", ""], ["Chiu", "Man-Kwun", ""], ["Demaine", "Erik D.", ""], ["Demaine", "Martin L.", ""], ["Hesterberg", "Adam", ""], ["Korman", "Matias", ""], ["Lynch", "Jayson", ""], ["van Renssen", "Andr\u00e9", ""], ["Roeloffzen", "Marcel", ""]]}, {"id": "2105.08406", "submitter": "Manfred Scheucher", "authors": "Manfred Scheucher", "title": "A SAT attack on higher dimensional Erd\\H{o}s--Szekeres numbers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A famous result by Erd\\H{o}s and Szekeres (1935) asserts that, for every $k,d\n\\in \\mathbb{N}$, there is a smallest integer $n = g^{(d)}(k)$, such that every\nset of at least $n$ points in $\\mathbb{R}^d$ in general position contains a\n$k$-gon, i.e., a subset of $k$ points which is in convex position. We present a\nSAT model for higher dimensional point sets which is based on chirotopes, and\nuse modern SAT solvers to investigate Erd\\H{o}s--Szekeres numbers in dimensions\n$d=3,4,5$. We show $g^{(3)}(7) \\le 13$, $g^{(4)}(8) \\le 13$, and $g^{(5)}(9)\n\\le 13$, which are the first improvements for decades. For the setting of\n$k$-holes (i.e., $k$-gons with no other points in the convex hull), where\n$h^{(d)}(k)$ denotes the minimum number $n$ such that every set of at least $n$\npoints in $\\mathbb{R}^d$ in general position contains a $k$-hole, we show\n$h^{(3)}(7) \\le 14$, $h^{(4)}(8) \\le 13$, and $h^{(5)}(9) \\le 13$. Moreover,\nall obtained bounds are sharp in the setting of chirotopes and we conjecture\nthem to be sharp also in the original setting of point sets.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 09:58:40 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Scheucher", "Manfred", ""]]}, {"id": "2105.09047", "submitter": "Pantea Haghighatkhah", "authors": "Pantea Haghighatkhah, Wouter Meulemans, Bettina Speckman, J\\'er\\^ome\n  Urhausen, Kevin Verbeek", "title": "Obstructing Classification via Projection", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning and data mining techniques are effective tools to classify\nlarge amounts of data. But they tend to preserve any inherent bias in the data,\nfor example, with regards to gender or race. Removing such bias from data or\nthe learned representations is quite challenging. In this paper we study a\ngeometric problem which models a possible approach for bias removal. Our input\nis a set of points P in Euclidean space R^d and each point is labeled with k\nbinary-valued properties. A priori we assume that it is \"easy\" to classify the\ndata according to each property. Our goal is to obstruct the classification\naccording to one property by a suitable projection to a lower-dimensional\nEuclidean space R^m (m < d), while classification according to all other\nproperties remains easy.\n  What it means for classification to be easy depends on the classification\nmodel used. We first consider classification by linear separability as employed\nby support vector machines. We use Kirchberger's Theorem to show that, under\ncertain conditions, a simple projection to R^(d-1) suffices to eliminate the\nlinear separability of one of the properties whilst maintaining the linear\nseparability of the other properties. We also study the problem of maximizing\nthe linear \"inseparability\" of the chosen property. Second, we consider more\ncomplex forms of separability and prove a connection between the number of\nprojections required to obstruct classification and the Helly-type properties\nof such separabilities.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 10:28:15 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Haghighatkhah", "Pantea", ""], ["Meulemans", "Wouter", ""], ["Speckman", "Bettina", ""], ["Urhausen", "J\u00e9r\u00f4me", ""], ["Verbeek", "Kevin", ""]]}, {"id": "2105.09217", "submitter": "Gautam K. Das", "authors": "Pawan K. Mishra and Gautam K. Das", "title": "Approximation Algorithms For The Euclidean Dispersion Problems", "comments": "17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we consider the Euclidean dispersion problems. Let\n$P=\\{p_{1}, p_{2}, \\ldots, p_{n}\\}$ be a set of $n$ points in $\\mathbb{R}^2$.\nFor each point $p \\in P$ and $S \\subseteq P$, we define $cost_{\\gamma}(p,S)$ as\nthe sum of Euclidean distance from $p$ to the nearest $\\gamma $ point in $S\n\\setminus \\{p\\}$. We define $cost_{\\gamma}(S)=\\min_{p \\in\nS}\\{cost_{\\gamma}(p,S)\\}$ for $S \\subseteq P$. In the $\\gamma$-dispersion\nproblem, a set $P$ of $n$ points in $\\mathbb{R}^2$ and a positive integer $k\n\\in [\\gamma+1,n]$ are given. The objective is to find a subset $S\\subseteq P$\nof size $k$ such that $cost_{\\gamma}(S)$ is maximized. We consider both\n$2$-dispersion and $1$-dispersion problem in $\\mathbb{R}^2$. Along with these,\nwe also consider $2$-dispersion problem when points are placed on a line. In\nthis paper, we propose a simple polynomial time $(2\\sqrt 3 + \\epsilon )$-factor\napproximation algorithm for the $2$-dispersion problem, for any $\\epsilon > 0$,\nwhich is an improvement over the best known approximation factor $4\\sqrt3$\n[Amano, K. and Nakano, S. I., An approximation algorithm for the $2$-dispersion\nproblem, IEICE Transactions on Information and Systems, Vol. 103(3), pp.\n506-508, 2020]. Next, we develop a common framework for designing an\napproximation algorithm for the Euclidean dispersion problem. With this common\nframework, we improve the approximation factor to $2\\sqrt 3$ for the\n$2$-dispersion problem in $\\mathbb{R}^2$. Using the same framework, we propose\na polynomial time algorithm, which returns an optimal solution for the\n$2$-dispersion problem when points are placed on a line. Moreover, to show the\neffectiveness of the framework, we also propose a $2$-factor approximation\nalgorithm for the $1$-dispersion problem in $\\mathbb{R}^2$.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 15:56:30 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Mishra", "Pawan K.", ""], ["Das", "Gautam K.", ""]]}, {"id": "2105.09313", "submitter": "Gautam K. Das", "authors": "Pawan K. Mishra and Gautam K. Das", "title": "Approximation Algorithms For The Dispersion Problems in a Metric Space", "comments": "9. arXiv admin note: text overlap with arXiv:2105.09217", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we consider the $c$-dispersion problem in a metric space\n$(X,d)$. Let $P=\\{p_{1}, p_{2}, \\ldots, p_{n}\\}$ be a set of $n$ points in a\nmetric space $(X,d)$. For each point $p \\in P$ and $S \\subseteq P$, we define\n$cost_{c}(p,S)$ as the sum of distances from $p$ to the nearest $c $ points in\n$S \\setminus \\{p\\}$, where $c\\geq 1$ is a fixed integer. We define\n$cost_{c}(S)=\\min_{p \\in S}\\{cost_{c}(p,S)\\}$ for $S \\subseteq P$. In the\n$c$-dispersion problem, a set $P$ of $n$ points in a metric space $(X,d)$ and a\npositive integer $k \\in [c+1,n]$ are given. The objective is to find a subset\n$S\\subseteq P$ of size $k$ such that $cost_{c}(S)$ is maximized. We propose a\nsimple polynomial time greedy algorithm that produces a $2c$-factor\napproximation result for the $c$-dispersion problem in a metric space. The best\nknown result for the $c$-dispersion problem in the Euclidean metric space\n$(X,d)$ is $2c^2$, where $P \\subseteq \\mathbb{R}^2$ and the distance function\nis Euclidean distance [ Amano, K. and Nakano, S. I., Away from Rivals, CCCG,\npp.68-71, 2018 ]. We also prove that the $c$-dispersion problem in a metric\nspace is $W[1]$-hard.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 16:01:21 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 13:42:45 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Mishra", "Pawan K.", ""], ["Das", "Gautam K.", ""]]}, {"id": "2105.09438", "submitter": "Craig Kaplan", "authors": "Craig S. Kaplan", "title": "Heesch Numbers of Unmarked Polyforms", "comments": "18 Pages, 9 Figures, 6 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A shape's Heesch number is the number of layers of copies of the shape that\ncan be placed around it without gaps or overlaps. Experimentation and\nexhaustive searching have turned up examples of shapes with finite Heesch\nnumbers up to six, but nothing higher. The computational problem of classifying\nsimple families of shapes by Heesch number can provide more experimental data\nto fuel our understanding of this topic. I present a technique for computing\nHeesch numbers of non-tiling polyforms using a SAT solver, and the results of\nexhaustive computation of Heesch numbers up to 19-ominoes, 17-hexes, and\n24-iamonds.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 00:00:42 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Kaplan", "Craig S.", ""]]}, {"id": "2105.09667", "submitter": "Sebastien Tixeuil", "authors": "Adam Heriban (NPA), S\\'ebastien Tixeuil (NPA, LINCS)", "title": "Unreliable Sensors for Reliable Efficient Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of existing Distributed Computing literature about mobile\nrobotic swarms considers computability issues: characterizing the set of system\nhypotheses that enables problem solvability. By contrast, the focus of this\nwork is to investigate complexity issues: obtaining quantitative results about\na given problem that admits solutions. Our quantitative measurements rely on a\nnewly developed simulation framework to benchmark pen and paper designs. First,\nwe consider the maximum traveled distance when gathering robots at a given\nlocation, not known beforehand (both in the two robots and in the n robots\nsettings) in the classical OBLOT model, for the FSYNC, SSYNC, and ASYNC\nschedulers. This particular metric appears relevant as it correlates closely to\nwhat would be real world fuel consumption. Then, we introduce the possibility\nof errors in the vision of robots, and assess the behavior of known rendezvous\n(aka two robots gathering) and leader election protocols when sensors are\nunreliable. We also introduce two new algorithms, one for fuel efficient\nconvergence, and one for leader election, that operate reliably despite\nunreliable sensors.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 10:55:22 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Heriban", "Adam", "", "NPA"], ["Tixeuil", "S\u00e9bastien", "", "NPA, LINCS"]]}, {"id": "2105.09772", "submitter": "Marco Attene PhD", "authors": "Marco Attene", "title": "Indirect predicates for geometric constructions", "comments": "In Computer-Aided Design, 2020", "journal-ref": null, "doi": "10.1016/j.cad.2020.102856", "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Geometric predicates are a basic ingredient to implement a vast range of\nalgorithms in computational geometry. Modern implementations employ floating\npoint filtering techniques to combine efficiency and robustness, and\nstate-of-the-art predicates are guaranteed to be always exact while being only\nslightly slower than corresponding (inexact) floating point implementations.\nUnfortunately, if the input to these predicates is an intermediate construction\nof an algorithm, its floating point representation may be affected by an\napproximation error, and correctness is no longer guaranteed. This paper\nintroduces the concept of indirect geometric predicate: instead of taking the\nintermediate construction as an explicit input, an indirect predicate considers\nthe primitive geometric elements which are combined to produce such a\nconstruction. This makes it possible to keep track of the floating point\napproximation, and thus to exploit efficient filters and expansion arithmetic\nto exactly resolve the predicate with minimal overhead with respect to a naive\nfloating point implementation. As a representative example, we show how to\nextend standard predicates to the case of points of intersection of linear\nelements (i.e. lines and planes) and show that, on classical problems, this\napproach outperforms state-of-the-art solutions based on lazy exact\nintermediate representations.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 14:20:48 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Attene", "Marco", ""]]}, {"id": "2105.09922", "submitter": "Aleksandr Popov", "authors": "Kevin Buchin, Maarten L\\\"offler, Tim Ophelders, Aleksandr Popov,\n  J\\'er\\^ome Urhausen, Kevin Verbeek", "title": "Computing the Fr\\'echet Distance Between Uncertain Curves in One\n  Dimension", "comments": "27 pages, 12 figures. This is the full version of the paper to appear\n  at WADS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of computing the Fr\\'echet distance between two\ncurves for which the exact locations of the vertices are unknown. Each vertex\nmay be placed in a given uncertainty region for that vertex, and the objective\nis to place vertices so as to minimise the Fr\\'echet distance. This problem was\nrecently shown to be NP-hard in 2D, and it is unclear how to compute an optimal\nvertex placement at all.\n  We present the first general algorithmic framework for this problem. We prove\nthat it results in a polynomial-time algorithm for curves in 1D with intervals\nas uncertainty regions. In contrast, we show that the problem is NP-hard in 1D\nin the case that vertices are placed to maximise the Fr\\'echet distance.\n  We also study the weak Fr\\'echet distance between uncertain curves. While\nfinding the optimal placement of vertices seems more difficult than the regular\nFr\\'echet distance -- and indeed we can easily prove that the problem is\nNP-hard in 2D -- the optimal placement of vertices in 1D can be computed in\npolynomial time. Finally, we investigate the discrete weak Fr\\'echet distance,\nfor which, somewhat surprisingly, the problem is NP-hard already in 1D.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 17:36:34 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Buchin", "Kevin", ""], ["L\u00f6ffler", "Maarten", ""], ["Ophelders", "Tim", ""], ["Popov", "Aleksandr", ""], ["Urhausen", "J\u00e9r\u00f4me", ""], ["Verbeek", "Kevin", ""]]}, {"id": "2105.10774", "submitter": "Erik Demaine", "authors": "Zachary Abel, Erik D. Demaine, Martin L. Demaine, Jason S. Ku, Jayson\n  Lynch, Jin-ichi Itoh, Chie Nara", "title": "Continuous Flattening of All Polyhedral Manifolds using Countably\n  Infinite Creases", "comments": "14 pages, 7 figures", "journal-ref": "Computational Geometry: Theory and Applications, volume 98,\n  October 2021, Article 101773", "doi": "10.1016/j.comgeo.2021.101773", "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that any finite polyhedral manifold in 3D can be continuously\nflattened into 2D while preserving intrinsic distances and avoiding crossings,\nanswering a 19-year-old open problem, if we extend standard folding models to\nallow for countably infinite creases. The most general cases previously known\nto be continuously flattenable were convex polyhedra and semi-orthogonal\npolyhedra. For non-orientable manifolds, even the existence of an instantaneous\nflattening (flat folded state) is a new result. Our solution extends a method\nfor flattening semi-orthogonal polyhedra: slice the polyhedron along parallel\nplanes and flatten the polyhedral strips between consecutive planes. We adapt\nthis approach to arbitrary nonconvex polyhedra by generalizing strip flattening\nto nonorthogonal corners and slicing along a countably infinite number of\nparallel planes, with slices densely approaching every vertex of the manifold.\nWe also show that the area of the polyhedron that needs to support moving\ncreases (which are necessary for closed polyhedra by the Bellows Theorem) can\nbe made arbitrarily small.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 17:24:27 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Abel", "Zachary", ""], ["Demaine", "Erik D.", ""], ["Demaine", "Martin L.", ""], ["Ku", "Jason S.", ""], ["Lynch", "Jayson", ""], ["Itoh", "Jin-ichi", ""], ["Nara", "Chie", ""]]}, {"id": "2105.10776", "submitter": "Sariel Har-Peled", "authors": "Sariel Har-Peled, Timothy Zhou", "title": "How Packed Is It, Really?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The congestion of a curve is a measure of how much it zigzags around locally.\nMore precisely, a curve $\\pi$ is $c$-packed if the length of the curve lying\ninside any ball is at most $c$ times the radius of the ball, and its congestion\nis the maximum $c$ for which $\\pi$ is $c$-packed. This paper presents a\nrandomized $(288+\\varepsilon)$-approximation algorithm for computing the\ncongestion of a curve (or any set of segments in constant dimension). It runs\nin $O( n \\log^2 n)$ time and succeeds with high probability. Although the\napproximation factor is large, the running time improves over the previous\nfastest constant approximation algorithm \\cite{gsw-appc-20}, which runs in\n(roughly) $O(n^{4/3})$ time. We carefully combine new ideas with known\ntechniques to obtain our new, near-linear time algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 17:31:56 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Har-Peled", "Sariel", ""], ["Zhou", "Timothy", ""]]}, {"id": "2105.11371", "submitter": "Krist\\'of Husz\\'ar", "authors": "Krist\\'of Husz\\'ar", "title": "On the pathwidth of hyperbolic 3-manifolds", "comments": "21 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GT cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  According to Mostow's celebrated rigidity theorem, the geometry of closed\nhyperbolic 3-manifolds is already determined by their topology. In particular,\nthe volume of such manifolds is a topological invariant and, as such, has been\ninvestigated for half a century.\n  Motivated by the algorithmic study of 3-manifolds, Maria and Purcell have\nrecently shown that every closed hyperbolic 3-manifold M with volume vol(M)\nadmits a triangulation with dual graph of treewidth at most C vol(M), for some\nuniversal constant C.\n  Here we improve on this result by showing that the volume provides a linear\nupper bound even on the pathwidth of the dual graph of some triangulation,\nwhich can potentially be much larger than the treewidth. Our proof relies on a\nsynthesis of tools from 3-manifold theory: generalized Heegaard splittings,\namalgamations, and the thick-thin decomposition of hyperbolic 3-manifolds. We\nprovide an illustrated exposition of this toolbox and also discuss the\nalgorithmic consequences of the result.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 16:03:34 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Husz\u00e1r", "Krist\u00f3f", ""]]}, {"id": "2105.11996", "submitter": "Navid Talebanfard", "authors": "Pavel Hrube\\v{s}, Navid Talebanfard", "title": "On the extension complexity of polytopes separating subsets of the\n  Boolean cube", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that\n  1. for every $A\\subseteq \\{0, 1\\}^n$, there exists a polytope $P\\subseteq\n\\mathbb{R}^n$ with $P \\cap \\{0, 1\\}^n = A$ and extension complexity\n$O(2^{n/2})$,\n  2. there exists an $A\\subseteq \\{0, 1\\}^n$ such that the extension complexity\nof any $P$ with $P\\cap \\{0, 1\\}^n = A$ must be at least\n$2^{\\frac{n}{3}(1-o(1))}$.\n  We also remark that the extension complexity of any 0/1-polytope in\n$\\mathbb{R}^n$ is at most $O(2^n/n)$ and pose the problem whether the upper\nbound can be improved to $O(2^{cn})$, for $c<1$.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 14:59:02 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Hrube\u0161", "Pavel", ""], ["Talebanfard", "Navid", ""]]}, {"id": "2105.12208", "submitter": "Yu Qin", "authors": "Yu Qin, Brittany Terese Fasy, Carola Wenk, and Brian Summa", "title": "A Domain-Oblivious Approach for Learning Concise Representations of\n  Filtered Topological Spaces", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistence diagrams have been widely used to quantify the underlying\nfeatures of filtered topological spaces in data visualization. In many\napplications, computing distances between diagrams is essential; however,\ncomputing these distances has been challenging due to the computational cost.\nIn this paper, we propose a persistence diagram hashing framework that learns a\nbinary code representation of persistence diagrams, which allows for fast\ncomputation of distances. This framework is built upon a generative adversarial\nnetwork (GAN) with a diagram distance loss function to steer the learning\nprocess. Instead of attempting to transform diagrams into vectorized\nrepresentations, we hash diagrams into binary codes, which have natural\nadvantages in large-scale tasks. The training of this model is domain-oblivious\nin that it can be computed purely from synthetic, randomly created diagrams. As\na consequence, our proposed method is directly applicable to various datasets\nwithout the need of retraining the model. These binary codes, when compared\nusing fast Hamming distance, better maintain topological similarity properties\nbetween datasets than other vectorized representations. To evaluate this\nmethod, we apply our framework to the problem of diagram clustering and we\ncompare the quality and performance of our approach to the state-of-the-art. In\naddition, we show the scalability of our approach on a dataset with 10k\npersistence diagrams, which is not possible with current techniques. Moreover,\nour experimental results demonstrate that our method is significantly faster\nwith less memory usage, while retaining comparable or better quality\ncomparisons.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 20:44:28 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Qin", "Yu", ""], ["Fasy", "Brittany Terese", ""], ["Wenk", "Carola", ""], ["Summa", "Brian", ""]]}, {"id": "2105.12247", "submitter": "Sayan Nag", "authors": "Sayan Nag", "title": "Graph Self Supervised Learning: the BT, the HSIC, and the VICReg", "comments": "Paper Accepted in the Weakly Supervised Representation Learning\n  Workshop, IJCAI 2021 (IJCAI2021-WSRL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CG cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised learning and pre-training strategies have developed over the\nlast few years especially for Convolutional Neural Networks (CNNs). Recently\napplication of such methods can also be noticed for Graph Neural Networks\n(GNNs) . In this paper, we have used a graph based self-supervised learning\nstrategy with different loss functions (Barlow Twins[Zbontar et al., 2021],\nHSIC[Tsai et al., 2021], VICReg[Bardes et al., 2021]) which have shown\npromising results when applied with CNNs previously. We have also proposed a\nhybrid loss function combining the advantages of VICReg and HSIC and called it\nas VICRegHSIC. The performance of these aforementioned methods have been\ncompared when applied to different datasets such as MUTAG, PROTEINS and\nIMDB-Binary. Moreover, the impact of different batch sizes, projector\ndimensions and data augmentation strategies have also been explored\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 22:34:19 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 04:51:26 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 03:18:43 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Nag", "Sayan", ""]]}, {"id": "2105.12415", "submitter": "Tobias Weinzierl", "authors": "Peter J. Noble, Tobias Weinzierl", "title": "A multiresolution Discrete Element Method for triangulated objects with\n  implicit timestepping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulations of many rigid bodies colliding with each other sometimes yield\nparticularly interesting results if the colliding objects differ significantly\nin size and are non-spherical. The most expensive part within such a simulation\ncode is the collision detection. We propose a family of novel multiscale\ncollision detection algorithms that can be applied to triangulated objects\nwithin explicit and implicit time stepping methods. They are well-suited to\nhandle objects that cannot be represented by analytical shapes or assemblies of\nanalytical objects. Inspired by multigrid methods and adaptive mesh refinement,\nwe determine collision points iteratively over a resolution hierarchy, and\ncombine a functional minimisation plus penalty parameters with the actual\ncomparision-based geometric distance calculation. Coarse surrogate geometry\nrepresentations identify \"no collision\" scenarios early on and otherwise yield\nan educated guess which triangle subsets of the next finer level potentially\nyield collisions. They prune the search tree, and furthermore feed conservative\ncontact force estimates into the iterative solve behind an implicit time\nstepping. Implicit time stepping and non-analytical shapes often yield\nprohibitive high compute cost for rigid body simulations. Our approach reduces\nthese cost algorithmically by one to two orders of magnitude. It also exhibits\nhigh vectorisation efficiency due to its iterative nature.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 09:11:33 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Noble", "Peter J.", ""], ["Weinzierl", "Tobias", ""]]}, {"id": "2105.12452", "submitter": "Nathan Van Beusekom", "authors": "Nathan van Beusekom, Irene Parada, Bettina Speckmann", "title": "Crossing Numbers of Beyond-Planar Graphs Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph drawing beyond planarity focuses on drawings of high visual quality for\nnon-planar graphs which are characterized by certain forbidden edge\nconfigurations. A natural criterion for the quality of a drawing is the number\nof edge crossings. The question then arises whether beyond-planar drawings have\na significantly larger crossing number than unrestricted drawings. Chimani et\nal. [GD'19] gave bounds for the ratio between the crossing number of three\nclasses of beyond-planar graphs and the unrestricted crossing number. In this\npaper we extend their results to the main currently known classes of\nbeyond-planar graphs characterized by forbidden edge configurations and answer\nseveral of their open questions.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 10:24:21 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["van Beusekom", "Nathan", ""], ["Parada", "Irene", ""], ["Speckmann", "Bettina", ""]]}, {"id": "2105.13040", "submitter": "Maximilian Pfister", "authors": "Patrizio Angelini, Michael A. Bekos, Fabrizio Montecchiani, Maximilian\n  Pfister", "title": "On Morphing 1-Planar Drawings", "comments": "To appear in Proceedings of the 47th International Workshop on\n  Graph-Theoretic Concepts in Computer Science (WG 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computing a morph between two drawings of a graph is a classical problem in\ncomputational geometry and graph drawing. While this problem has been widely\nstudied in the context of planar graphs, very little is known about the\nexistence of topology-preserving morphs for pairs of non-planar graph drawings.\nWe make a step towards this problem by showing that a topology-preserving morph\nalways exists for drawings of a meaningful family of $1$-planar graphs. While\nour proof is constructive, the vertices may follow trajectories of unbounded\ncomplexity.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 10:16:43 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Angelini", "Patrizio", ""], ["Bekos", "Michael A.", ""], ["Montecchiani", "Fabrizio", ""], ["Pfister", "Maximilian", ""]]}, {"id": "2105.13042", "submitter": "Giuseppe Prencipe", "authors": "David Kirkpatrick and Irina Kostitsyna and Alfredo Navarra and\n  Giuseppe Prencipe and Nicola Santoro", "title": "Separating Bounded and Unbounded Asynchrony for Autonomous Robots: Point\n  Convergence with Limited Visibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Among fundamental problems in the context of distributed computing by\nautonomous mobile entities, one of the most representative and well studied is\n{\\sc Point Convergence}: given an arbitrary initial configuration of identical\nentities, disposed in the Euclidean plane, move in such a way that, for all\n$\\eps>0$, a configuration in which the separation between all entities is at\nmost $\\eps$ is eventually reached and maintained.\n  The problem has been previously studied in a variety of settings, including\nfull visibility, exact measurements (like distances and angles), and\nsynchronous activation of entities. Our study concerns the minimal assumptions\nunder which entities, moving asynchronously with limited and unknown visibility\nrange and subject to limited imprecision in measurements, can be guaranteed to\nconverge in this way.\n  We present an algorithm that solves {\\sc Point Convergence}, for entities in\nthe plane, in such a setting, provided the degree of asynchrony is bounded:\nwhile any one entity is active, any other entity can be activated at most $k$\ntimes, for some arbitrarily large but fixed $k$. This provides a strong\npositive answer to a decade old open question posed by Katreniak.\n  We also prove that in a comparable setting that permits unbounded asynchrony,\n{\\sc Point Convergence} in the plane is impossible, contingent on the natural\nassumption that algorithms maintain the (visible) connectivity among entities\npresent in the initial configuration. This variant, that we call {\\sc Cohesive\nConvergence}, serves to distinguish the power of bounded and unbounded\nasynchrony in the control of autonomous mobile entities, settling at the same\ntime a long-standing question whether in the Euclidean plane synchronously\nscheduled entities are more powerful than asynchronously scheduled entities.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 10:20:59 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Kirkpatrick", "David", ""], ["Kostitsyna", "Irina", ""], ["Navarra", "Alfredo", ""], ["Prencipe", "Giuseppe", ""], ["Santoro", "Nicola", ""]]}, {"id": "2105.13168", "submitter": "Rhaleb Zayer", "authors": "Alexander Weinrauch, Hans-Peter Seidel, Daniel Mlakar, Markus\n  Steinberger, Rhaleb Zayer", "title": "A Variational Loop Shrinking Analogy for Handle and Tunnel Detection and\n  Reeb Graph Construction on Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG math.AT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The humble loop shrinking property played a central role in the inception of\nmodern topology but it has been eclipsed by more abstract algebraic formalism.\nThis is particularly true in the context of detecting relevant non-contractible\nloops on surfaces where elaborate homological and/or graph theoretical\nconstructs are favored in algorithmic solutions. In this work, we devise a\nvariational analogy to the loop shrinking property and show that it yields a\nsimple, intuitive, yet powerful solution allowing a streamlined treatment of\nthe problem of handle and tunnel loop detection. Our formalization tracks the\nevolution of a diffusion front randomly initiated on a single location on the\nsurface. Capitalizing on a diffuse interface representation combined with a set\nof rules for concurrent front interactions, we develop a dynamic data structure\nfor tracking the evolution on the surface encoded as a sparse matrix which\nserves for performing both diffusion numerics and loop detection and acts as\nthe workhorse of our fully parallel implementation. The substantiated results\nsuggest our approach outperforms state of the art and robustly copes with\nhighly detailed geometric models. As a byproduct, our approach can be used to\nconstruct Reeb graphs by diffusion thus avoiding commonly encountered issues\nwhen using Morse functions.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 14:22:34 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Weinrauch", "Alexander", ""], ["Seidel", "Hans-Peter", ""], ["Mlakar", "Daniel", ""], ["Steinberger", "Markus", ""], ["Zayer", "Rhaleb", ""]]}, {"id": "2105.13277", "submitter": "Yotam Erel", "authors": "Amir Barda, Yotam Erel, Amit H. Bermano", "title": "MeshCNN Fundamentals: Geometric Learning through a Reconstructable\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mesh-based learning is one of the popular approaches nowadays to learn\nshapes. The most established backbone in this field is MeshCNN. In this paper,\nwe propose infusing MeshCNN with geometric reasoning to achieve higher quality\nlearning. Through careful analysis of the way geometry is represented\nthrough-out the network, we submit that this representation should be rigid\nmotion invariant, and should allow reconstructing the original geometry.\nAccordingly, we introduce the first and second fundamental forms as an\nedge-centric, rotation and translation invariant, reconstructable\nrepresentation. In addition, we update the originally proposed pooling scheme\nto be more geometrically driven. We validate our analysis through\nexperimentation, and present consistent improvement upon the MeshCNN baseline,\nas well as other more elaborate state-of-the-art architectures. Furthermore, we\ndemonstrate this fundamental forms-based representation opens the door to\naccessible generative machine learning over meshes.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 16:22:44 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Barda", "Amir", ""], ["Erel", "Yotam", ""], ["Bermano", "Amit H.", ""]]}, {"id": "2105.13921", "submitter": "Oleg Smirnov", "authors": "Oleg Smirnov", "title": "TensorFlow RiemOpt: a library for optimization on Riemannian manifolds", "comments": "The library code is available at\n  https://github.com/master/tensorflow-riemopt", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The adoption of neural networks and deep learning in non-Euclidean domains\nhas been hindered until recently by the lack of scalable and efficient learning\nframeworks. Existing toolboxes in this space were mainly motivated by research\nand education use cases, whereas practical aspects, such as deploying and\nmaintaining machine learning models, were often overlooked.\n  We attempt to bridge this gap by proposing TensorFlow RiemOpt, a Python\nlibrary for optimization on Riemannian manifolds in TensorFlow. The library is\ndesigned with the aim for a seamless integration with the TensorFlow ecosystem,\ntargeting not only research, but also streamlining production machine learning\npipelines.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 10:42:09 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 18:50:43 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Smirnov", "Oleg", ""]]}, {"id": "2105.14305", "submitter": "Ryuhei Uehara", "authors": "Tonan Kamata, Akira Kadoguchi, Takashi Horiyama, and Ryuhei Uehara", "title": "Efficient Folding Algorithms for Regular Polyhedra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the folding problem that asks if a polygon P can be folded to\na polyhedron Q for given P and Q. Recently, an efficient algorithm for this\nproblem has been developed when Q is a box. We extend this idea to regular\npolyhedra, also known as Platonic solids. The basic idea of our algorithms is\ncommon, which is called stamping. However, the computational complexities of\nthem are different depending on their geometric properties. We developed four\nalgorithms for the problem as follows. (1) An algorithm for a regular\ntetrahedron, which can be extended to a tetramonohedron. (2) An algorithm for a\nregular hexahedron (or a cube), which is much efficient than the previously\nknown one. (3) An algorithm for a general deltahedron, which contains the cases\nthat Q is a regular octahedron or a regular icosahedron. (4) An algorithm for a\nregular dodecahedron. Combining these algorithms, we can conclude that the\nfolding problem can be solved pseudo-polynomial time when Q is a regular\npolyhedron and other related solid.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 14:03:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kamata", "Tonan", ""], ["Kadoguchi", "Akira", ""], ["Horiyama", "Takashi", ""], ["Uehara", "Ryuhei", ""]]}]