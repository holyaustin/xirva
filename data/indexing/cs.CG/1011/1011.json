[{"id": "1011.0344", "submitter": "Michael Sagraloff", "authors": "Michael Sagraloff", "title": "On the Complexity of Real Root Isolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.NA cs.SC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach to isolate the real roots of a square-free\npolynomial $F=\\sum_{i=0}^n A_i x^i$ with real coefficients. It is assumed that\neach coefficient of $F$ can be approximated to any specified error bound. The\npresented method is exact, complete and deterministic. Due to its similarities\nto the Descartes method, we also consider it practical and easy to implement.\nCompared to previous approaches, our new method achieves a significantly better\nbit complexity. It is further shown that the hardness of isolating the real\nroots of $F$ is exclusively determined by the geometry of the roots and not by\nthe complexity or the size of the coefficients. For the special case where $F$\nhas integer coefficients of maximal bitsize $\\tau$, our bound on the bit\ncomplexity writes as $\\tilde{O}(n^3\\tau^2)$ which improves the best bounds\nknown for existing practical algorithms by a factor of $n=deg F$. The crucial\nidea underlying the new approach is to run an approximate version of the\nDescartes method, where, in each subdivision step, we only consider\napproximations of the intermediate results to a certain precision. We give an\nupper bound on the maximal precision that is needed for isolating the roots of\n$F$. For integer polynomials, this bound is by a factor $n$ lower than that of\nthe precision needed when using exact arithmetic explaining the improved bound\non the bit complexity.\n", "versions": [{"version": "v1", "created": "Mon, 1 Nov 2010 15:29:23 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2011 13:26:06 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Sagraloff", "Michael", ""]]}, {"id": "1011.1263", "submitter": "Alexandr Andoni", "authors": "Alexandr Andoni, Robert Krauthgamer, Krzysztof Onak", "title": "Streaming Algorithms from Precision Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A technique introduced by Indyk and Woodruff [STOC 2005] has inspired several\nrecent advances in data-stream algorithms. We show that a number of these\nresults follow easily from the application of a single probabilistic method\ncalled Precision Sampling. Using this method, we obtain simple data-stream\nalgorithms that maintain a randomized sketch of an input vector\n$x=(x_1,...x_n)$, which is useful for the following applications. 1) Estimating\nthe $F_k$-moment of $x$, for $k>2$. 2) Estimating the $\\ell_p$-norm of $x$, for\n$p\\in[1,2]$, with small update time. 3) Estimating cascaded norms\n$\\ell_p(\\ell_q)$ for all $p,q>0$. 4) $\\ell_1$ sampling, where the goal is to\nproduce an element $i$ with probability (approximately) $|x_i|/\\|x\\|_1$. It\nextends to similarly defined $\\ell_p$-sampling, for $p\\in [1,2]$.\n  For all these applications the algorithm is essentially the same: scale the\nvector x entry-wise by a well-chosen random vector, and run a heavy-hitter\nestimation algorithm on the resulting vector. Our sketch is a linear function\nof x, thereby allowing general updates to the vector x.\n  Precision Sampling itself addresses the problem of estimating a sum\n$\\sum_{i=1}^n a_i$ from weak estimates of each real $a_i\\in[0,1]$. More\nprecisely, the estimator first chooses a desired precision $u_i\\in(0,1]$ for\neach $i\\in[n]$, and then it receives an estimate of every $a_i$ within additive\n$u_i$. Its goal is to provide a good approximation to $\\sum a_i$ while keeping\na tab on the \"approximation cost\" $\\sum_i (1/u_i)$. Here we refine previous\nwork [Andoni, Krauthgamer, and Onak, FOCS 2010] which shows that as long as\n$\\sum a_i=\\Omega(1)$, a good multiplicative approximation can be achieved using\ntotal precision of only $O(n\\log n)$.\n", "versions": [{"version": "v1", "created": "Thu, 4 Nov 2010 20:13:20 GMT"}, {"version": "v2", "created": "Sat, 23 Apr 2011 02:26:14 GMT"}], "update_date": "2011-04-26", "authors_parsed": [["Andoni", "Alexandr", ""], ["Krauthgamer", "Robert", ""], ["Onak", "Krzysztof", ""]]}, {"id": "1011.1602", "submitter": "Matthias K\\\"oppe", "authors": "Velleda Baldoni, Nicole Berline, Jes\\'us A. De Loera, Matthias\n  K\\\"oppe, Mich\\`ele Vergne", "title": "Computation of the highest coefficients of weighted Ehrhart\n  quasi-polynomials of rational polyhedra", "comments": "34 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article concerns the computational problem of counting the lattice\npoints inside convex polytopes, when each point must be counted with a weight\nassociated to it. We describe an efficient algorithm for computing the highest\ndegree coefficients of the weighted Ehrhart quasi-polynomial for a rational\nsimple polytope in varying dimension, when the weights of the lattice points\nare given by a polynomial function h. Our technique is based on a refinement of\nan algorithm of A. Barvinok [Computing the Ehrhart quasi-polynomial of a\nrational simplex, Math. Comp. 75 (2006), pp. 1449--1466] in the unweighted case\n(i.e., h = 1). In contrast to Barvinok's method, our method is local, obtains\nan approximation on the level of generating functions, handles the general\nweighted case, and provides the coefficients in closed form as step polynomials\nof the dilation. To demonstrate the practicality of our approach we report on\ncomputational experiments which show even our simple implementation can compete\nwith state of the art software.\n", "versions": [{"version": "v1", "created": "Sun, 7 Nov 2010 01:27:49 GMT"}], "update_date": "2010-11-09", "authors_parsed": [["Baldoni", "Velleda", ""], ["Berline", "Nicole", ""], ["De Loera", "Jes\u00fas A.", ""], ["K\u00f6ppe", "Matthias", ""], ["Vergne", "Mich\u00e8le", ""]]}, {"id": "1011.1787", "submitter": "Bernd R. Schlei", "authors": "B. R. Schlei", "title": "Volume-Enclosing Surface Extraction", "comments": "24 pages, 33 figures, 4 tables, final version", "journal-ref": "Computers and Graphics, Volume 36, Issue 2, 2012, Pages 111 - 130", "doi": "10.1016/j.cag.2011.12.008", "report-no": null, "categories": "cs.CG cs.GR nucl-th", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new method, which allows for the construction of\ntriangular isosurfaces from three-dimensional data sets, such as 3D image data\nand/or numerical simulation data that are based on regularly shaped, cubic\nlattices. This novel volume-enclosing surface extraction technique, which has\nbeen named VESTA, can produce up to six different results due to the nature of\nthe discretized 3D space under consideration. VESTA is neither template-based\nnor it is necessarily required to operate on 2x2x2 voxel cell neighborhoods\nonly. The surface tiles are determined with a very fast and robust construction\ntechnique while potential ambiguities are detected and resolved. Here, we\nprovide an in-depth comparison between VESTA and various versions of the\nwell-known and very popular Marching Cubes algorithm for the very first time.\nIn an application section, we demonstrate the extraction of VESTA isosurfaces\nfor various data sets ranging from computer tomographic scan data to simulation\ndata of relativistic hydrodynamic fireball expansions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Nov 2010 08:53:59 GMT"}, {"version": "v2", "created": "Mon, 15 Nov 2010 16:52:38 GMT"}, {"version": "v3", "created": "Fri, 23 Sep 2011 08:46:13 GMT"}, {"version": "v4", "created": "Fri, 8 Jun 2012 14:14:44 GMT"}], "update_date": "2012-06-11", "authors_parsed": [["Schlei", "B. R.", ""]]}, {"id": "1011.1917", "submitter": "Zoran Sunic", "authors": "Zoran Sunic", "title": "Normal art galleries: wall in - all in", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of a normal gallery, a gallery in which any\nconfiguration of guards that visually covers the walls covers the entire\ngallery. We show that any star gallery is normal and any gallery with at most\ntwo reflex corners is normal. A polynomial time algorithm is provided deciding\nif, for a given polygon and a finite set of positions, there exists a\nconfiguration of guards in some of these positions that visually covers the\nwalls but not the entire gallery.\n", "versions": [{"version": "v1", "created": "Mon, 8 Nov 2010 21:11:38 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2012 04:08:09 GMT"}], "update_date": "2012-02-29", "authors_parsed": [["Sunic", "Zoran", ""]]}, {"id": "1011.2258", "submitter": "Benjamin Schweinhart", "authors": "Robert MacPherson and Benjamin Schweinhart", "title": "Measuring Shape with Topology", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cond-mat.mtrl-sci cs.CG math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a measure of shape which is appropriate for the study of a\ncomplicated geometric structure, defined using the topology of neighborhoods of\nthe structure. One aspect of this measure gives a new notion of fractal\ndimension. We demonstrate the utility and computability of this measure by\napplying it to branched polymers, Brownian trees, and self-avoiding random\nwalks.\n", "versions": [{"version": "v1", "created": "Wed, 10 Nov 2010 02:11:15 GMT"}, {"version": "v2", "created": "Thu, 16 Dec 2010 23:49:44 GMT"}], "update_date": "2010-12-20", "authors_parsed": [["MacPherson", "Robert", ""], ["Schweinhart", "Benjamin", ""]]}, {"id": "1011.2269", "submitter": "Xingguo Shao", "authors": "Xingguo Shao, Qingguo Wang, Peter C Y Chen, Zhencai Zhu, Bin Zi", "title": "Forward Kinematics Analysis and Tension Distribution of a Cable-Driven\n  Sinking Winches Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the forward kinematics and tension distribution of\nsinking winches mechanism, which is a type of four-cable-driven partly\nconstrained parallel robot. Conventional studies on forward kinematics of\ncable-driven parallel robot assumed that all cables are taut. Actually, given\nthe lengths of four cables, some cables may be slack when the platform is in\nstatic equilibrium. Therefore, in this paper, the tension state (tautness or\nslackness) of cables is considered in the forward kinematics model. We propose\nTraversal-Solving-Algorithm, which can indicate the tension state of cables,\nand further determine the pose of the platform, if the lengths of four cables\nare given. The effectiveness of the algorithm is verified by four examples. The\nresults of this paper can be used to control sinking winches mechanism to\nachieve the level and stable motion of the platform, and to make the tension\ndistribution of cables as uniform as possible.\n", "versions": [{"version": "v1", "created": "Wed, 10 Nov 2010 03:35:00 GMT"}], "update_date": "2010-11-11", "authors_parsed": [["Shao", "Xingguo", ""], ["Wang", "Qingguo", ""], ["Chen", "Peter C Y", ""], ["Zhu", "Zhencai", ""], ["Zi", "Bin", ""]]}, {"id": "1011.2488", "submitter": "Luca Tesei", "authors": "Ezio Bartocci, Diletta Romana Cacciagrano, Maria Rita Di Berardini,\n  Emanuela Merelli and Luca Tesei", "title": "Shape Calculus: Timed Operational Semantics and Well-formedness", "comments": "42 pages, 4 figures, extended version of Bartocci, E.; Cacciagrano,\n  D. R.; Di Berardini, M. R.; Merelli, E. & Tesei, L. Timed Operational\n  Semantics and Well-formedness of Shape Calculus. Scientific Annals of\n  Computer Science, 20, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CE cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Shape Calculus is a bio-inspired calculus for describing 3D shapes moving\nin a space. A shape forms a 3D process when combined with a behaviour.\nBehaviours are specified with a timed CCS-like process algebra using a notion\nof channel that models naturally binding sites on the surface of shapes.\nProcesses can represent molecules or other mobile objects and can be part of\nnetworks of processes that move simultaneously and interact in a given\ngeometrical space. The calculus embeds collision detection and response,\nbinding of compatible 3D processes and splitting of previously established\nbonds. In this work the full formal timed operational semantics of the calculus\nis provided, together with examples that illustrate the use of the calculus in\na well-known biological scenario. Moreover, a result of well-formedness about\nthe evolution of a given network of well-formed 3D processes is proved.\n", "versions": [{"version": "v1", "created": "Tue, 9 Nov 2010 11:33:59 GMT"}], "update_date": "2010-11-11", "authors_parsed": [["Bartocci", "Ezio", ""], ["Cacciagrano", "Diletta Romana", ""], ["Di Berardini", "Maria Rita", ""], ["Merelli", "Emanuela", ""], ["Tesei", "Luca", ""]]}, {"id": "1011.4169", "submitter": "Benjamin Burton", "authors": "Benjamin A. Burton", "title": "The Pachner graph and the simplification of 3-sphere triangulations", "comments": "17 pages, 4 figures, 4 tables; v2: incorporate connectedness into\n  Theorem 4, other minor revisions; to appear in SoCG 2011", "journal-ref": "SCG '11: Proceedings of the Twenty-Seventh Annual Symposium on\n  Computational Geometry, ACM, 2011, pp. 153-162", "doi": "10.1145/1998196.1998220", "report-no": null, "categories": "math.GT cs.CG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to have fast and effective methods for simplifying 3-manifold\ntriangulations without losing any topological information. In theory this is\ndifficult: we might need to make a triangulation super-exponentially more\ncomplex before we can make it smaller than its original size. Here we present\nexperimental work suggesting that for 3-sphere triangulations the reality is\nfar different: we never need to add more than two tetrahedra, and we never need\nmore than a handful of local modifications. If true in general, these extremely\nsurprising results would have significant implications for decision algorithms\nand the study of triangulations in 3-manifold topology.\n  The algorithms behind these experiments are interesting in their own right.\nKey techniques include the isomorph-free generation of all 3-manifold\ntriangulations of a given size, polynomial-time computable signatures that\nidentify triangulations uniquely up to isomorphism, and parallel algorithms for\nstudying finite level sets in the infinite Pachner graph.\n", "versions": [{"version": "v1", "created": "Thu, 18 Nov 2010 11:00:46 GMT"}, {"version": "v2", "created": "Wed, 23 Feb 2011 06:19:32 GMT"}], "update_date": "2011-06-16", "authors_parsed": [["Burton", "Benjamin A.", ""]]}, {"id": "1011.4955", "submitter": "Steve Oudot", "authors": "David Arthur and Steve Y. Oudot", "title": "Reverse Nearest Neighbors Search in High Dimensions using\n  Locality-Sensitive Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": "INRIA RR-7084", "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of finding reverse nearest neighbors efficiently.\nAlthough provably good solutions exist for this problem in low or fixed\ndimensions, to this date the methods proposed in high dimensions are mostly\nheuristic. We introduce a method that is both provably correct and efficient in\nall dimensions, based on a reduction of the problem to one instance of\n$\\e$-nearest neighbor search plus a controlled number of instances of {\\em\nexhaustive $r$-\\pleb}, a variant of {\\em Point Location among Equal Balls}\nwhere all the $r$-balls centered at the data points that contain the query\npoint are sought for, not just one. The former problem has been extensively\nstudied and elegantly solved in high dimensions using Locality-Sensitive\nHashing (LSH) techniques. By contrast, the latter problem has a complexity that\nis still not fully understood. We revisit the analysis of the LSH scheme for\nexhaustive $r$-\\pleb using a somewhat refined notion of locality-sensitive\nfamily of hash function, which brings out a meaningful output-sensitive term in\nthe complexity of the problem. Our analysis, combined with a non-isometric\nlifting of the data, enables us to answer exhaustive $r$-\\pleb queries (and\ndown the road reverse nearest neighbors queries) efficiently. Along the way, we\nobtain a simple algorithm for answering exact nearest neighbor queries, whose\ncomplexity is parametrized by some {\\em condition number} measuring the\ninherent difficulty of a given instance of the problem.\n", "versions": [{"version": "v1", "created": "Mon, 22 Nov 2010 21:29:23 GMT"}], "update_date": "2010-11-24", "authors_parsed": [["Arthur", "David", ""], ["Oudot", "Steve Y.", ""]]}, {"id": "1011.5320", "submitter": "Sheng-Gwo Chen", "authors": "Wen-Haw Chen and Sheng-Gwo Chen", "title": "Computation of the shortest path between two curves on a parametric\n  surface by geodesic-like method", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the geodesic-like algorithm for the computation of\nthe shortest path between two objects on NURBS surfaces and periodic surfaces.\nThis method can improve the distance problem not only on surfaces but in\n$\\mathbb{R}^3$. Moreover, the geodesic-like algorithm also provides an\nefficient approach to simulate the minimal geodesic between two holes on a\nNURBS surfaces.\n", "versions": [{"version": "v1", "created": "Wed, 24 Nov 2010 09:18:28 GMT"}], "update_date": "2010-11-25", "authors_parsed": [["Chen", "Wen-Haw", ""], ["Chen", "Sheng-Gwo", ""]]}, {"id": "1011.5553", "submitter": "Steven Gortler", "authors": "Steven J. Gortler, Craig Gotsman, Ligang Liu, and Dylan P. Thurston", "title": "On affine rigidity", "comments": "Updated abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We define the notion of affine rigidity of a hypergraph and prove a variety\nof fundamental results for this notion. First, we show that affine rigidity can\nbe determined by the rank of a specific matrix which implies that affine\nrigidity is a generic property of the hypergraph.Then we prove that if a graph\nis is $(d+1)$-vertex-connected, then it must be \"generically neighborhood\naffinely rigid\" in $d$-dimensional space. This implies that if a graph is\n$(d+1)$-vertex-connected then any generic framework of its squared graph must\nbe universally rigid.\n  Our results, and affine rigidity more generally, have natural applications in\npoint registration and localization, as well as connections to manifold\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 25 Nov 2010 05:11:34 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2013 17:45:46 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2013 13:32:55 GMT"}], "update_date": "2013-08-14", "authors_parsed": [["Gortler", "Steven J.", ""], ["Gotsman", "Craig", ""], ["Liu", "Ligang", ""], ["Thurston", "Dylan P.", ""]]}, {"id": "1011.5666", "submitter": "Daniel Dadush", "authors": "Daniel Dadush and Chris Peikert and Santosh Vempala", "title": "Enumerative Lattice Algorithms in Any Norm via M-Ellipsoid Coverings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a novel algorithm for enumerating lattice points in any convex body,\nand give applications to several classic lattice problems, including the\nShortest and Closest Vector Problems (SVP and CVP, respectively) and Integer\nProgramming (IP). Our enumeration technique relies on a classical concept from\nasymptotic convex geometry known as the M-ellipsoid, and uses as a crucial\nsubroutine the recent algorithm of Micciancio and Voulgaris (STOC 2010) for\nlattice problems in the l_2 norm. As a main technical contribution, which may\nbe of independent interest, we build on the techniques of Klartag (Geometric\nand Functional Analysis, 2006) to give an expected 2^O(n)-time algorithm for\ncomputing an M-ellipsoid for any n-dimensional convex body.\n  As applications, we give deterministic 2^{O(n)}-time and -space algorithms\nfor solving exact SVP, and exact CVP when the target point is sufficiently\nclose to the lattice, on n-dimensional lattices in any (semi-)norm given an\nM-ellipsoid of the unit ball. In many norms of interest, including all l_p\nnorms, an M-ellipsoid is computable in deterministic poly(n) time, in which\ncase these algorithms are fully deterministic. Here our approach may be seen as\na derandomization of the \"AKS sieve\" for exact SVP and CVP (Ajtai, Kumar, and\nSivakumar; STOC 2001 and CCC 2002).\n  As a further application of our SVP algorithm, we derive an expected\nO(f*(n))^n-time algorithm for Integer Programming, where f*(n) denotes the\noptimal bound in the so-called \"flatness theorem,\" which satisfies f*(n) =\nO(n^{4/3} \\polylog(n)) and is conjectured to be f*(n)=\\Theta(n). Our runtime\nimproves upon the previous best of O(n^{2})^{n} by Hildebrand and Koppe (2010).\n", "versions": [{"version": "v1", "created": "Thu, 25 Nov 2010 18:24:17 GMT"}, {"version": "v2", "created": "Mon, 29 Nov 2010 23:14:37 GMT"}, {"version": "v3", "created": "Sun, 5 Dec 2010 23:49:29 GMT"}, {"version": "v4", "created": "Sun, 12 Jun 2011 20:40:54 GMT"}], "update_date": "2011-06-14", "authors_parsed": [["Dadush", "Daniel", ""], ["Peikert", "Chris", ""], ["Vempala", "Santosh", ""]]}, {"id": "1011.5920", "submitter": "Yaniv Altshuler", "authors": "Yaniv Altshuler and Alfred Bruckstein", "title": "On Short Cuts - or - Fencing in Rectangular Strips", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider an isoperimetric inequality for the \"free\nperimeter\" of a planar shape inside a rectangular domain, the free perimeter\nbeing the length of the shape boundary that does not touch the border of the\ndomain.\n", "versions": [{"version": "v1", "created": "Fri, 26 Nov 2010 21:41:48 GMT"}], "update_date": "2010-11-30", "authors_parsed": [["Altshuler", "Yaniv", ""], ["Bruckstein", "Alfred", ""]]}, {"id": "1011.6002", "submitter": "Matthias K\\\"oppe", "authors": "Velleda Baldoni, Nicole Berline, Matthias K\\\"oppe, Mich\\`ele Vergne", "title": "Intermediate Sums on Polyhedra: Computation and Real Ehrhart Theory", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": "10.1112/S0025579312000101", "report-no": null, "categories": "math.CO cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study intermediate sums, interpolating between integrals and discrete\nsums, which were introduced by A. Barvinok [Computing the Ehrhart\nquasi-polynomial of a rational simplex, Math. Comp. 75 (2006), 1449--1466]. For\na given semi-rational polytope P and a rational subspace L, we integrate a\ngiven polynomial function h over all lattice slices of the polytope P parallel\nto the subspace L and sum up the integrals. We first develop an algorithmic\ntheory of parametric intermediate generating functions. Then we study the\nEhrhart theory of these intermediate sums, that is, the dependence of the\nresult as a function of a dilation of the polytope. We provide an algorithm to\ncompute the resulting Ehrhart quasi-polynomials in the form of explicit step\npolynomials. These formulas are naturally valid for real (not just integer)\ndilations and thus provide a direct approach to real Ehrhart theory.\n", "versions": [{"version": "v1", "created": "Sat, 27 Nov 2010 21:47:29 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Baldoni", "Velleda", ""], ["Berline", "Nicole", ""], ["K\u00f6ppe", "Matthias", ""], ["Vergne", "Mich\u00e8le", ""]]}, {"id": "1011.6049", "submitter": "Thomas Houit", "authors": "Thomas Houit and Frank Nielsen", "title": "Video Stippling", "comments": "12 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider rendering color videos using a non-photo-realistic\nart form technique commonly called stippling. Stippling is the art of rendering\nimages using point sets, possibly with various attributes like sizes,\nelementary shapes, and colors. Producing nice stippling is attractive not only\nfor the sake of image depiction but also because it yields a compact vectorial\nformat for storing the semantic information of media. Moreover, stippling is by\nconstruction easily tunable to various device resolutions without suffering\nfrom bitmap sampling artifacts when resizing. The underlying core technique for\nstippling images is to compute a centroidal Voronoi tessellation on a\nwell-designed underlying density. This density relates to the image content,\nand is used to compute a weighted Voronoi diagram. By considering videos as\nimage sequences and initializing properly the stippling of one image by the\nresult of its predecessor, one avoids undesirable point flickering artifacts\nand can produce stippled videos that nevertheless still exhibit noticeable\nartifacts. To overcome this, our method improves over the naive scheme by\nconsidering dynamic point creation and deletion according to the current scene\nsemantic complexity, and show how to effectively vectorize video while\nadjusting for both color and contrast characteristics. Furthermore, we explain\nhow to produce high quality stippled ``videos'' (eg., fully dynamic\nspatio-temporal point sets) for media containing various fading effects, like\nquick motions of objects or progressive shot changes. We report on practical\nperformances of our implementation, and present several stippled video results\nrendered on-the-fly using our viewer that allows both spatio-temporal dynamic\nrescaling (eg., upscale vectorially frame rate).\n", "versions": [{"version": "v1", "created": "Sun, 28 Nov 2010 15:04:34 GMT"}], "update_date": "2010-11-30", "authors_parsed": [["Houit", "Thomas", ""], ["Nielsen", "Frank", ""]]}, {"id": "1011.6057", "submitter": "Cynthia Vinzant", "authors": "Daniel Plaumann, Bernd Sturmfels, Cynthia Vinzant", "title": "Computing Linear Matrix Representations of Helton-Vinnikov Curves", "comments": "19 pages, 3 figures, minor revisions; Mathematical Methods in\n  Systems, Optimization and Control, Birkhauser, Basel", "journal-ref": "Operator Theory: Advances and Applications, Vol 222, 2012, pp.\n  259-277", "doi": "10.1007/978-3-0348-0411-0_19", "report-no": null, "categories": "math.AG cs.CG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Helton and Vinnikov showed that every rigidly convex curve in the real plane\nbounds a spectrahedron. This leads to the computational problem of explicitly\nproducing a symmetric (positive definite) linear determinantal representation\nfor a given curve. We study three approaches to this problem: an algebraic\napproach via solving polynomial equations, a geometric approach via contact\ncurves, and an analytic approach via theta functions. These are explained,\ncompared, and tested experimentally for low degree instances.\n", "versions": [{"version": "v1", "created": "Sun, 28 Nov 2010 16:06:52 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2013 13:16:56 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Plaumann", "Daniel", ""], ["Sturmfels", "Bernd", ""], ["Vinzant", "Cynthia", ""]]}, {"id": "1011.6076", "submitter": "Marc Arnaudon", "authors": "Marc Arnaudon (LMA), Frank Nielsen (LIX)", "title": "Medians and means in Finsler geometry", "comments": null, "journal-ref": "LMS J. Comput. Math. 15 (2012) 23-37", "doi": "10.1112/S1461157010000513", "report-no": null, "categories": "math.DG cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate existence and uniqueness of p-means and the median of a\nprobability measure on a Finsler manifold, in relation with the convexity of\nthe support of the measure. We prove that the p-mean is the limit point of a\ncontinuous time gradient flow. Under some additional condition which is always\nsatisfied for larger than or equal to 2, a discretization of this path\nconverges to the p-mean. This provides an algorithm for determining those\nFinsler center points.\n", "versions": [{"version": "v1", "created": "Sun, 28 Nov 2010 19:55:18 GMT"}, {"version": "v2", "created": "Sat, 25 Jun 2011 06:47:41 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Arnaudon", "Marc", "", "LMA"], ["Nielsen", "Frank", "", "LIX"]]}, {"id": "1011.6481", "submitter": "R Inkulu", "authors": "Rajasekhar Inkulu, Sanjiv Kapoor, S. N. Maheshwari", "title": "A near optimal algorithm for finding Euclidean shortest path in\n  polygonal domain", "comments": "50 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm to find an {\\it Euclidean Shortest Path} from a\nsource vertex $s$ to a sink vertex $t$ in the presence of obstacles in $\\Re^2$.\nOur algorithm takes $O(T+m(\\lg{m})(\\lg{n}))$ time and $O(n)$ space. Here,\n$O(T)$ is the time to triangulate the polygonal region, $m$ is the number of\nobstacles, and $n$ is the number of vertices. This bound is close to the known\nlower bound of $O(n+m\\lg{m})$ time and $O(n)$ space. Our approach involve\nprogressing shortest path wavefront as in continuous Dijkstra-type method, and\nconfining its expansion to regions of interest.\n", "versions": [{"version": "v1", "created": "Tue, 30 Nov 2010 08:42:45 GMT"}], "update_date": "2010-12-01", "authors_parsed": [["Inkulu", "Rajasekhar", ""], ["Kapoor", "Sanjiv", ""], ["Maheshwari", "S. N.", ""]]}, {"id": "1011.6498", "submitter": "R Inkulu", "authors": "Rajasekhar Inkulu, Sanjiv Kapoor", "title": "Approximate Shortest Path through a Weighted Planar Subdivision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approximation algorithm for finding a shortest path\nbetween two points $s$ and $t$ in a weighted planar subdivision $\\PS$. Each\nface $f$ of $\\PS$ is associated with a weight $w_f$, and the cost of travel\nalong a line segment on $f$ is $w_f$ multiplied by the Euclidean norm of that\nline segment. The cost of a path which traverses across several faces of the\nsubdivision is the sum of the costs of travel along each face. Our algorithm\nprogreeses the discretized shortest path wavefront from source $s$, and takes\npolynomial time in finding an $\\epsilon$-approximate shortest path.\n", "versions": [{"version": "v1", "created": "Tue, 30 Nov 2010 09:38:47 GMT"}], "update_date": "2010-12-01", "authors_parsed": [["Inkulu", "Rajasekhar", ""], ["Kapoor", "Sanjiv", ""]]}]