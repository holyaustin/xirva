[{"id": "1903.00353", "submitter": "Alo Roosing", "authors": "A. Roosing, O. T. Strickson, N. Nikiforakis", "title": "Fast Distance Fields for Fluid Dynamics Mesh Generation on Graphics\n  Hardware", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a CUDA accelerated implementation of the Characteristic/Scan\nConversion algorithm to generate narrow band signed distance fields in\nlogically Cartesian grids. We outline an approach of task and data management\non GPUs based on an input of a closed triangulated surface with the aim of\nreducing pre-processing and mesh-generation times. The work demonstrates a fast\nsigned distance field generation of triangulated surfaces with tens of\nthousands to several million features in high resolution domains. We present\nimprovements to the robustness of the original algorithm and an overview of\nhandling geometric data.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 15:13:45 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Roosing", "A.", ""], ["Strickson", "O. T.", ""], ["Nikiforakis", "N.", ""]]}, {"id": "1903.00686", "submitter": "Tom Hanika", "authors": "Dominik D\\\"urrschnabel and Tom Hanika and Gerd Stumme", "title": "DimDraw -- A novel tool for drawing concept lattices", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept lattice drawings are an important tool to visualize complex relations\nin data in a simple manner to human readers. Many attempts were made to\ntransfer classical graph drawing approaches to order diagrams. Although those\nmethods are satisfying for some lattices they unfortunately perform poorly in\ngeneral. In this work we present a novel tool to draw concept lattices that is\npurely motivated by the order structure.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 11:44:39 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["D\u00fcrrschnabel", "Dominik", ""], ["Hanika", "Tom", ""], ["Stumme", "Gerd", ""]]}, {"id": "1903.00899", "submitter": "Long Zeng", "authors": "Zeng Long, Dong zhi-kai, Xu yi-fan", "title": "Robust corner and tangent point detection for strokes with deep learning\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust corner and tangent point detection (CTPD) tool is critical for\nsketch-based engineering modeling. This paper proposes a robust CTPD approach\nfor hand-drawn strokes with deep learning approach. Its robustness for users,\nstroke shapes and biased datasets is improved due to multiscaled point contexts\nand a vote scheme. Firstly, all stroke points are classified into segments by\ntwo deep learning networks, based on scaled point contexts which mimic human's\nperception. Then, a vote scheme is adopted to analyze the merge conditions and\noperations for adjacent segments. If most points agree with a stroke's type,\nthis type is accepted. Finally, new corners and tangent points are inserted at\ntransition points. The algorithm's performance is experimented with 1500\nstrokes of 20 shapes. Results show that our algorithm can achieve 95.3% for\nall-or-nothing accuracy and 88.6% accuracy for biased datasets, compared to\n84.6% and 71% of the state-of-the-art CTPD technique, which is heuristic and\nempirical-based.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 13:17:09 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Long", "Zeng", ""], ["zhi-kai", "Dong", ""], ["yi-fan", "Xu", ""]]}, {"id": "1903.01067", "submitter": "Antoni Rosinol", "authors": "Antoni Rosinol, Torsten Sattler, Marc Pollefeys, Luca Carlone", "title": "Incremental Visual-Inertial 3D Mesh Generation with Structural\n  Regularities", "comments": "7 pages, 5 figures, ICRA accepted", "journal-ref": "IEEE Int. Conf. Robot. Autom. (ICRA), 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual-Inertial Odometry (VIO) algorithms typically rely on a point cloud\nrepresentation of the scene that does not model the topology of the\nenvironment. A 3D mesh instead offers a richer, yet lightweight, model.\nNevertheless, building a 3D mesh out of the sparse and noisy 3D landmarks\ntriangulated by a VIO algorithm often results in a mesh that does not fit the\nreal scene. In order to regularize the mesh, previous approaches decouple state\nestimation from the 3D mesh regularization step, and either limit the 3D mesh\nto the current frame or let the mesh grow indefinitely. We propose instead to\ntightly couple mesh regularization and state estimation by detecting and\nenforcing structural regularities in a novel factor-graph formulation. We also\npropose to incrementally build the mesh by restricting its extent to the\ntime-horizon of the VIO optimization; the resulting 3D mesh covers a larger\nportion of the scene than a per-frame approach while its memory usage and\ncomputational complexity remain bounded. We show that our approach successfully\nregularizes the mesh, while improving localization accuracy, when structural\nregularities are present, and remains operational in scenes without\nregularities.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 04:24:50 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 16:36:41 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Rosinol", "Antoni", ""], ["Sattler", "Torsten", ""], ["Pollefeys", "Marc", ""], ["Carlone", "Luca", ""]]}, {"id": "1903.01291", "submitter": "Fahad Panolan", "authors": "Fedor V. Fomin, Daniel Lokshtanov, Fahad Panolan, Saket Saurabh,\n  Meirav Zehavi", "title": "Decomposition of Map Graphs with Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bidimensionality is the most common technique to design subexponential-time\nparameterized algorithms on special classes of graphs, particularly planar\ngraphs. The core engine behind it is a combinatorial lemma of Robertson,\nSeymour and Thomas that states that every planar graph either has a\n$\\sqrt{k}\\times \\sqrt{k}$-grid as a minor, or its treewidth is $O(\\sqrt{k})$.\nHowever, bidimensionality theory cannot be extended directly to several\nwell-known classes of geometric graphs. Nevertheless, a relaxation of this\nlemma has been proven useful for unit disk graphs. Inspired by this, we prove a\nnew decomposition lemma for map graphs. Informally, our lemma states the\nfollowing. For any map graph $G$, there exists a collection $(U_1,\\ldots,U_t)$\nof cliques of $G$ with the following property: $G$ either contains a\n$\\sqrt{k}\\times \\sqrt{k}$-grid as a minor, or it admits a tree decomposition\nwhere every bag is the union of $O(\\sqrt{k})$ of the cliques in the above\ncollection. The new lemma appears to be a handy tool in the design of\nsubexponential parameterized algorithms on map graphs. We demonstrate its\nusability by designing algorithms on map graphs with running time\n$2^{O({\\sqrt{k}\\log{k}})} \\cdot n^{O(1)}$ for the Connected Planar $\\cal\nF$-Deletion problem (that encompasses problems such as Feedback Vertex Set and\nVertex Cover). Obtaining subexponential algorithms for Longest Cycle/Path and\nCycle Packing is more challenging. We have to construct tree decompositions\nwith more powerful properties and to prove sublinear bounds on the number of\nways an optimum solution could \"cross\" bags in these decompositions.\n  For Longest Cycle/Path, these are the first subexponential-time parameterized\nalgorithms on map graphs. For Feedback Vertex Set and Cycle Packing, we improve\nupon known $2^{O({k^{0.75}\\log{k}})} \\cdot n^{O(1)}$-time algorithms on map\ngraphs.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 14:55:58 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Fomin", "Fedor V.", ""], ["Lokshtanov", "Daniel", ""], ["Panolan", "Fahad", ""], ["Saurabh", "Saket", ""], ["Zehavi", "Meirav", ""]]}, {"id": "1903.01417", "submitter": "Haitao Wang", "authors": "Haitao Wang", "title": "A Divide-and-Conquer Algorithm for Two-Point $L_1$ Shortest Path Queries\n  in Polygonal Domains", "comments": "A preliminary version to appear in SoCG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathcal{P}$ be a polygonal domain of $h$ holes and $n$ vertices. We\nstudy the problem of constructing a data structure that can compute a shortest\npath between $s$ and $t$ in $\\mathcal{P}$ under the $L_1$ metric for any two\nquery points $s$ and $t$. To do so, a standard approach is to first find a set\nof $n_s$ \"gateways\" for $s$ and a set of $n_t$ \"gateways\" for $t$ such that\nthere exist a shortest $s$-$t$ path containing a gateway of $s$ and a gateway\nof $t$, and then compute a shortest $s$-$t$ path using these gateways. Previous\nalgorithms all take quadratic $O(n_s\\cdot n_t)$ time to solve this problem. In\nthis paper, we propose a divide-and-conquer technique that solves the problem\nin $O(n_s + n_t \\log n_s)$ time. As a consequence, we construct a data\nstructure of $O(n+(h^2\\log^3 h/\\log\\log h))$ size in $O(n+(h^2\\log^4 h/\\log\\log\nh))$ time such that each query can be answered in $O(\\log n)$ time.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 18:17:51 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Wang", "Haitao", ""]]}, {"id": "1903.01601", "submitter": "Behnam Malmir", "authors": "Behnam Malmir, Shing I Chang, Malgorzata Rys and Dylan Darter", "title": "Quantifying Gait Changes Using Microsoft Kinect and Sample Entropy", "comments": "This article is an updated version of a paper entitled 'Quantifying\n  Gait Changes Using Microsoft Kinect and Sample Entropy' presented at the 2018\n  Industrial and Systems Engineering Research Conference (ISERC) in Orlando,\n  Florida (May 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CG cs.HC stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This study describes a method to quantify potential gait changes in human\nsubjects. Microsoft Kinect devices were used to provide and track coordinates\nof fifteen different joints of a subject over time. Three male subjects walk a\n10-foot path multiple times with and without motion-restricting devices. Their\nwalking patterns were recorded via two Kinect devices through frontal and\nsagittal planes. A modified sample entropy (SE) value was computed to quantify\nthe variability of the time series for each joint. The SE values with and\nwithout motion-restricting devices were used to compare the changes in each\njoint. The preliminary results of the experiments show that the proposed\nquantification method can detect differences in walking patterns with and\nwithout motion-restricting devices. The proposed method has the potential to be\napplied to track personal progress in physical therapy sessions.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 00:16:23 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Malmir", "Behnam", ""], ["Chang", "Shing I", ""], ["Rys", "Malgorzata", ""], ["Darter", "Dylan", ""]]}, {"id": "1903.01805", "submitter": "Esther Galby", "authors": "Nicolas Champseix, Esther Galby, Andrea Munaro and Bernard Ries", "title": "CPG graphs: Some structural and hardness results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we continue the systematic study of Contact graphs of Paths on\na Grid (CPG graphs) initiated in [Deniz et al., 2018]. A CPG graph is a graph\nfor which there exists a collection of pairwise interiorly disjoint paths on a\ngrid in one-to-one correspondence with its vertex set such that two vertices\nare adjacent if and only if the corresponding paths touch at a grid-point. If\nevery such path has at most $k$ bends for some $k \\geq 0$, the graph is said to\nbe $B_k$-CPG.\n  We first show that, for any $k \\geq 0$, the class of $B_k$-CPG graphs is\nstrictly contained in the class of $B_{k+1}$-CPG graphs even within the class\nof planar graphs, thus implying that there exists no $k \\geq 0$ such that every\nplanar CPG graph is $B_k$-CPG. The main result of the paper is that recognizing\nCPG graphs and $B_k$-CPG graphs with $k \\geq 1$ is $\\mathsf{NP}$-complete.\nMoreover, we show that the same remains true even within the class of planar\ngraphs in the case $k \\geq 3$. We then consider several graph problems\nrestricted to CPG graphs and show, in particular, that Independent Set and\nClique Cover remain $\\mathsf{NP}$-hard for $B_0$-CPG graphs. Finally, we\nconsider the related classes $B_k$-EPG of edge-intersection graphs of paths\nwith at most $k$ bends on a grid. Although it is possible to optimally color a\n$B_0$-EPG graph in polynomial time, as this class coincides with that of\ninterval graphs, we show that, in contrast, 3-Colorability is\n$\\mathsf{NP}$-complete for $B_1$-EPG graphs.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 13:09:34 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 11:08:59 GMT"}, {"version": "v3", "created": "Mon, 27 Jan 2020 18:40:56 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Champseix", "Nicolas", ""], ["Galby", "Esther", ""], ["Munaro", "Andrea", ""], ["Ries", "Bernard", ""]]}, {"id": "1903.02185", "submitter": "Suthee Ruangwises", "authors": "Suthee Ruangwises, Toshiya Itoh", "title": "Stable Noncrossing Matchings", "comments": "This paper has appeared at IWOCA 2019", "journal-ref": null, "doi": "10.1007/978-3-030-25005-8_33", "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of $n$ men represented by $n$ points lying on a line, and $n$\nwomen represented by $n$ points lying on another parallel line, with each\nperson having a list that ranks some people of opposite gender as his/her\nacceptable partners in strict order of preference. In this problem, we want to\nmatch people of opposite genders to satisfy people's preferences as well as\nmaking the edges not crossing one another geometrically. A noncrossing blocking\npair w.r.t. a matching $M$ is a pair $(m,w)$ of a man and a woman such that\nthey are not matched with each other but prefer each other to their own\npartners in $M$, and the segment $(m,w)$ does not cross any edge in $M$. A\nweakly stable noncrossing matching (WSNM) is a noncrossing matching that does\nnot admit any noncrossing blocking pair. In this paper, we prove the existence\nof a WSNM in any instance by developing an $O(n^2)$ algorithm to find one in a\ngiven instance.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 05:50:40 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 09:31:00 GMT"}, {"version": "v3", "created": "Wed, 23 Oct 2019 07:31:08 GMT"}, {"version": "v4", "created": "Fri, 25 Oct 2019 08:54:56 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Ruangwises", "Suthee", ""], ["Itoh", "Toshiya", ""]]}, {"id": "1903.02353", "submitter": "Leonie Ryvkin", "authors": "Hugo A Akitaya, Maike Buchin, Leonie Ryvkin, J\\'er\\^ome Urhausen", "title": "The $k$-Fr\\'echet distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new distance measure for comparing polygonal chains: the\n$k$-Fr\\'echet distance. As the name implies, it is closely related to the\nwell-studied Fr\\'echet distance but detects similarities between curves that\nresemble each other only piecewise. The parameter $k$ denotes the number of\nsubcurves into which we divide the input curves. The $k$-Fr\\'echet distance\nprovides a nice transition between (weak) Fr\\'echet distance and Hausdorff\ndistance. However, we show that deciding this distance measure turns out to be\nNP-complete, which is interesting since both (weak) Fr\\'echet and Hausdorff\ndistance are computable in polynomial time. Nevertheless, we give several\npossibilities to deal with the hardness of the $k$-Fr\\'echet distance: besides\nan exponential-time algorithm for the general case, we give a polynomial-time\nalgorithm for $k=2$, i.e., we ask that we subdivide our input curves into two\nsubcurves each. We also present an approximation algorithm that outputs a\nnumber of subcurves of at most twice the optimal size. Finally, we give an FPT\nalgorithm using parameters $k$ (the number of allowed subcurves) and $z$ (the\nnumber of segments of one curve that intersects the $\\varepsilon$-neighborhood\nof a point on the other curve).\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 13:14:02 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Akitaya", "Hugo A", ""], ["Buchin", "Maike", ""], ["Ryvkin", "Leonie", ""], ["Urhausen", "J\u00e9r\u00f4me", ""]]}, {"id": "1903.02388", "submitter": "Jiadong Han", "authors": "Han Jiadong", "title": "An Adaptive Grid Algorithm for Computing the Homology Group of\n  Semialgebraic Set", "comments": "arXiv admin note: text overlap with arXiv:1706.07473 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG math.GT math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Looking for an efficient algorithm for the computation of the homology groups\nof an algebraic set or even a semi-algebraic set is an important problem in the\neffective real algebraic geometry. Recently, Peter Burgisser, Felipe Cucker and\nPierre Lairez wrote a paper [1], which made a step forward by giving an\nalgorithm of weak exponential time. However, the algorithm is not not\npractical. In my thesis, I will introduce my improvement of this algorithm\nusing an adaptive grid algorithm on the unit sphere.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 19:23:11 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Jiadong", "Han", ""]]}, {"id": "1903.02429", "submitter": "Loic Le Folgoc", "authors": "Loic Le Folgoc, Daniel C. Castro, Jeremy Tan, Bishesh Khanal,\n  Konstantinos Kamnitsas, Ian Walker, Amir Alansary, Ben Glocker", "title": "Controlling Meshes via Curvature: Spin Transformations for\n  Pose-Invariant Shape Processing", "comments": "Accepted for publication at the 26th international conference on\n  Information Processing in Medical Imaging (IPMI 2019)", "journal-ref": "IPMI 2019. LNCS, vol 11492, pp 221-234. Springer, Cham", "doi": "10.1007/978-3-030-20351-1_17", "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate discrete spin transformations, a geometric framework to\nmanipulate surface meshes by controlling mean curvature. Applications include\nsurface fairing -- flowing a mesh onto say, a reference sphere -- and mesh\nextrusion -- e.g., rebuilding a complex shape from a reference sphere and\ncurvature specification. Because they operate in curvature space, these\noperations can be conducted very stably across large deformations with no need\nfor remeshing. Spin transformations add to the algorithmic toolbox for\npose-invariant shape analysis. Mathematically speaking, mean curvature is a\nshape invariant and in general fully characterizes closed shapes (together with\nthe metric). Computationally speaking, spin transformations make that\nrelationship explicit. Our work expands on a discrete formulation of spin\ntransformations. Like their smooth counterpart, discrete spin transformations\nare naturally close to conformal (angle-preserving). This quasi-conformality\ncan nevertheless be relaxed to satisfy the desired trade-off between area\ndistortion and angle preservation. We derive such constraints and propose a\nformulation in which they can be efficiently incorporated. The approach is\nshowcased on subcortical structures.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 14:52:48 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Folgoc", "Loic Le", ""], ["Castro", "Daniel C.", ""], ["Tan", "Jeremy", ""], ["Khanal", "Bishesh", ""], ["Kamnitsas", "Konstantinos", ""], ["Walker", "Ian", ""], ["Alansary", "Amir", ""], ["Glocker", "Ben", ""]]}, {"id": "1903.02444", "submitter": "Stephane Breuils", "authors": "St\\'ephane Breuils (LIGM), Vincent Nozick (LIGM), Laurent Fuchs\n  (XLIM-ASALI), Akihiro Sugimoto (NII)", "title": "Efficient representation and manipulation of quadratic surfaces using\n  Geometric Algebras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quadratic surfaces gain more and more attention among the Geometric Algebra\ncommunity and some frameworks were proposed in order to represent, transform,\nand intersect these quadratic surfaces. As far as the authors know, none of\nthese frameworks support all the operations required to completely handle these\nsurfaces. Some frameworks do not allow the construction of quadratic surfaces\nfrom control points when others do not allow to transform these quadratic\nsurfaces. However , if we consider all the frameworks together, then all the\nrequired operations over quadratic are covered. This paper presents a\nunification of these frameworks that enables to represent any quadratic\nsurfaces either using control points or from the coefficients of its implicit\nform. The proposed approach also allows to transform any quadratic surfaces and\nto compute their intersection and to easily extract some geometric properties .\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 07:19:49 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Breuils", "St\u00e9phane", "", "LIGM"], ["Nozick", "Vincent", "", "LIGM"], ["Fuchs", "Laurent", "", "XLIM-ASALI"], ["Sugimoto", "Akihiro", "", "NII"]]}, {"id": "1903.02645", "submitter": "Aur\\'elien Ooms", "authors": "Sergio Cabello, Jean Cardinal, John Iacono, Stefan Langerman, Pat\n  Morin, Aur\\'elien Ooms", "title": "Encoding 3SUM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following problem: given three sets of real numbers, output a\nword-RAM data structure from which we can efficiently recover the sign of the\nsum of any triple of numbers, one in each set. This is similar to a previous\nwork by some of the authors to encode the order type of a finite set of points.\nWhile this previous work showed that it was possible to achieve slightly\nsubquadratic space and logarithmic query time, we show here that for the\nsimpler 3SUM problem, one can achieve an encoding that takes\n$\\tilde{O}(N^{\\frac 32})$ space for inputs sets of size $N$ and allows constant\ntime queries in the word-RAM.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 22:56:24 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Cabello", "Sergio", ""], ["Cardinal", "Jean", ""], ["Iacono", "John", ""], ["Langerman", "Stefan", ""], ["Morin", "Pat", ""], ["Ooms", "Aur\u00e9lien", ""]]}, {"id": "1903.02758", "submitter": "Arnold Filtser", "authors": "Arnold Filtser", "title": "A face cover perspective to $\\ell_1$ embeddings of planar graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was conjectured by Gupta et al. [Combinatorica04] that every planar graph\ncan be embedded into $\\ell_1$ with constant distortion. However, given an\n$n$-vertex weighted planar graph, the best upper bound on the distortion is\nonly $O(\\sqrt{\\log n})$, by Rao [SoCG99]. In this paper we study the case where\nthere is a set $K$ of terminals, and the goal is to embed only the terminals\ninto $\\ell_1$ with low distortion. In a seminal paper, Okamura and Seymour\n[J.Comb.Theory81] showed that if all the terminals lie on a single face, they\ncan be embedded isometrically into $\\ell_1$. The more general case, where the\nset of terminals can be covered by $\\gamma$ faces, was studied by Lee and\nSidiropoulos [STOC09] and Chekuri et al. [J.Comb.Theory13]. The state of the\nart is an upper bound of $O(\\log \\gamma)$ by Krauthgamer, Lee and Rika\n[SODA19]. Our contribution is a further improvement on the upper bound to\n$O(\\sqrt{\\log\\gamma})$. Since every planar graph has at most $O(n)$ faces, any\nfurther improvement on this result, will be a major breakthrough, directly\nimproving upon Rao's long standing upper bound. Moreover, it is well known that\nthe flow-cut gap equals to the distortion of the best embedding into $\\ell_1$.\nTherefore, our result provides a polynomial time $O(\\sqrt{\\log\n\\gamma})$-approximation to the sparsest cut problem on planar graphs, for the\ncase where all the demand pairs can be covered by $\\gamma$ faces.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 07:36:00 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2019 08:02:08 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Filtser", "Arnold", ""]]}, {"id": "1903.02958", "submitter": "Tim R. Davidson", "authors": "Luca Falorsi, Pim de Haan, Tim R. Davidson, Patrick Forr\\'e", "title": "Reparameterizing Distributions on Lie Groups", "comments": "AISTATS (2019), code available at https://github.com/pimdh/relie", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CG cs.LG math.PR math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reparameterizable densities are an important way to learn probability\ndistributions in a deep learning setting. For many distributions it is possible\nto create low-variance gradient estimators by utilizing a `reparameterization\ntrick'. Due to the absence of a general reparameterization trick, much research\nhas recently been devoted to extend the number of reparameterizable\ndistributional families. Unfortunately, this research has primarily focused on\ndistributions defined in Euclidean space, ruling out the usage of one of the\nmost influential class of spaces with non-trivial topologies: Lie groups. In\nthis work we define a general framework to create reparameterizable densities\non arbitrary Lie groups, and provide a detailed practitioners guide to further\nthe ease of usage. We demonstrate how to create complex and multimodal\ndistributions on the well known oriented group of 3D rotations,\n$\\operatorname{SO}(3)$, using normalizing flows. Our experiments on applying\nsuch distributions in a Bayesian setting for pose estimation on objects with\ndiscrete and continuous symmetries, showcase their necessity in achieving\nrealistic uncertainty estimates.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 14:49:30 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Falorsi", "Luca", ""], ["de Haan", "Pim", ""], ["Davidson", "Tim R.", ""], ["Forr\u00e9", "Patrick", ""]]}, {"id": "1903.03014", "submitter": "Xavier Goaoc", "authors": "Xavier Goaoc, Andreas Holmsen and Cyril Nicaud", "title": "An Experimental Study of Forbidden Patterns in Geometric Permutations by\n  Combinatorial Lifting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of deciding if a given triple of permutations can be\nrealized as geometric permutations of disjoint convex sets in $\\mathbb{R}^3$.\nWe show that this question, which is equivalent to deciding the emptiness of\ncertain semi-algebraic sets bounded by cubic polynomials, can be \"lifted\" to a\npurely combinatorial problem. We propose an effective algorithm for that\nproblem, and use it to gain new insights into the structure of geometric\npermutations.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 16:02:20 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Goaoc", "Xavier", ""], ["Holmsen", "Andreas", ""], ["Nicaud", "Cyril", ""]]}, {"id": "1903.03211", "submitter": "Ioannis Psarros", "authors": "Anne Driemel, Andr\\'e Nusser, Jeff M. Phillips, Ioannis Psarros", "title": "The VC Dimension of Metric Balls under Fr\\'echet and Hausdorff Distances", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Vapnik-Chervonenkis dimension provides a notion of complexity for systems\nof sets. If the VC dimension is small, then knowing this can drastically\nsimplify fundamental computational tasks such as classification, range\ncounting, and density estimation through the use of sampling bounds. We analyze\nset systems where the ground set $X$ is a set of polygonal curves in\n$\\mathbb{R}^d$ and the sets $\\mathcal{R}$ are metric balls defined by curve\nsimilarity metrics, such as the Fr\\'echet distance and the Hausdorff distance,\nas well as their discrete counterparts. We derive upper and lower bounds on the\nVC dimension that imply useful sampling bounds in the setting that the number\nof curves is large, but the complexity of the individual curves is small. Our\nupper bounds are either near-quadratic or near-linear in the complexity of the\ncurves that define the ranges and they are logarithmic in the complexity of the\ncurves that define the ground set.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 22:53:39 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 10:38:50 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Driemel", "Anne", ""], ["Nusser", "Andr\u00e9", ""], ["Phillips", "Jeff M.", ""], ["Psarros", "Ioannis", ""]]}, {"id": "1903.03693", "submitter": "Sariel Har-Peled", "authors": "Sariel Har-Peled, Mitchell Jones, Saladi Rahul", "title": "Active Learning a Convex Body in Low Dimensions", "comments": "Talk based on results in the paper is available here:\n  https://youtu.be/5Epyh2lHrFQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a set $P \\subseteq \\Re^d$ of $n$ points, and a convex body $C$\nprovided via a separation oracle. The task at hand is to decide for each point\nof $P$ if it is in $C$ using the fewest number of oracle queries. We show that\none can solve this problem in two and three dimensions using $O( h(P) \\log n)$\nqueries, where $h(P)$ is the largest subset of points of $P$ in convex\nposition. Furthermore, we show that in two dimensions one can solve this\nproblem using $O( v(P,C) \\log^2 n )$ oracle queries, where $v(P, C)$ is a lower\nbound on the minimum number of queries that any algorithm for this specific\ninstance requires.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 23:00:36 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 19:39:35 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 16:33:37 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 00:31:47 GMT"}, {"version": "v5", "created": "Wed, 31 Mar 2021 18:27:05 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Har-Peled", "Sariel", ""], ["Jones", "Mitchell", ""], ["Rahul", "Saladi", ""]]}, {"id": "1903.04015", "submitter": "Wenbo Zhao", "authors": "Wenbo Zhao, Xianming Liu, Yongsen Zhao, Xiaopeng Fan, Debin Zhao", "title": "NormalNet: Learning-based Normal Filtering for Mesh Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mesh denoising is a critical technology in geometry processing that aims to\nrecover high-fidelity 3D mesh models of objects from their noise-corrupted\nversions. In this work, we propose a learning-based normal filtering scheme for\nmesh denoising called NormalNet, which maps the guided normal filtering (GNF)\ninto a deep network. The scheme follows the iterative framework of\nfiltering-based mesh denoising. During each iteration, first, the voxelization\nstrategy is applied on each face in a mesh to transform the irregular local\nstructure into the regular volumetric representation, therefore, both the\nstructure and face normal information are preserved and the convolution\noperations in CNN(Convolutional Neural Network) can be easily performed.\nSecond, instead of the guidance normal generation and the guided filtering in\nGNF, a deep CNN is designed, which takes the volumetric representation as\ninput, and outputs the learned filtered normals. At last, the vertex positions\nare updated according to the filtered normals. Specifically, the iterative\ntraining framework is proposed, in which the generation of training data and\nthe network training are alternately performed, whereas the ground truth\nnormals are taken as the guidance normals in GNF to get the target normals.\nCompared to state-of-the-art works, NormalNet can effectively remove noise\nwhile preserving the original features and avoiding pseudo-features.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 16:04:38 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 11:56:27 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Zhao", "Wenbo", ""], ["Liu", "Xianming", ""], ["Zhao", "Yongsen", ""], ["Fan", "Xiaopeng", ""], ["Zhao", "Debin", ""]]}, {"id": "1903.04194", "submitter": "Yong Sheng Soh", "authors": "Yong Sheng Soh and Venkat Chandrasekaran", "title": "Fitting Tractable Convex Sets to Support Function Evaluations", "comments": "35 pages, 80 figures", "journal-ref": null, "doi": "10.1007/s00454-020-00258-0", "report-no": null, "categories": "math.ST cs.CG math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The geometric problem of estimating an unknown compact convex set from\nevaluations of its support function arises in a range of scientific and\nengineering applications. Traditional approaches typically rely on estimators\nthat minimize the error over all possible compact convex sets; in particular,\nthese methods do not allow for the incorporation of prior structural\ninformation about the underlying set and the resulting estimates become\nincreasingly more complicated to describe as the number of measurements\navailable grows. We address both of these shortcomings by describing a\nframework for estimating tractably specified convex sets from support function\nevaluations. Building on the literature in convex optimization, our approach is\nbased on estimators that minimize the error over structured families of convex\nsets that are specified as linear images of concisely described sets -- such as\nthe simplex or the spectraplex -- in a higher-dimensional space that is not\nmuch larger than the ambient space. Convex sets parametrized in this manner are\nsignificant from a computational perspective as one can optimize linear\nfunctionals over such sets efficiently; they serve a different purpose in the\ninferential context of the present paper, namely, that of incorporating\nregularization in the reconstruction while still offering considerable\nexpressive power. We provide a geometric characterization of the asymptotic\nbehavior of our estimators, and our analysis relies on the property that\ncertain sets which admit semialgebraic descriptions are Vapnik-Chervonenkis\n(VC) classes. Our numerical experiments highlight the utility of our framework\nover previous approaches in settings in which the measurements available are\nnoisy or small in number as well as those in which the underlying set to be\nreconstructed is non-polyhedral.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 10:01:00 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 08:22:31 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Soh", "Yong Sheng", ""], ["Chandrasekaran", "Venkat", ""]]}, {"id": "1903.04737", "submitter": "David Eppstein", "authors": "David Eppstein", "title": "Counting Polygon Triangulations is Hard", "comments": "24 pages, 11 figures. Expanded version of a paper from Proc. 35th\n  International Symposium on Computational Geometry", "journal-ref": "Discrete Comput. Geom. 64 (4): 1210-1234, 2020", "doi": "10.1007/s00454-020-00251-7", "report-no": null, "categories": "cs.CG cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We prove that it is $\\#\\mathsf{P}$-complete to count the triangulations of a\n(non-simple) polygon.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 05:43:54 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 00:38:55 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Eppstein", "David", ""]]}, {"id": "1903.05048", "submitter": "Benjamin Niedermann", "authors": "Benjamin Niedermann, Ignaz Rutter, Matthias Wolf", "title": "Efficient Algorithms for Ortho-Radial Graph Drawing", "comments": "27 pages; SoCG 2019 (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orthogonal drawings, i.e., embeddings of graphs into grids, are a classic\ntopic in Graph Drawing. Often the goal is to find a drawing that minimizes the\nnumber of bends on the edges. A key ingredient for bend minimization algorithms\nis the existence of an orthogonal representation that allows to describe such\ndrawings purely combinatorially by only listing the angles between the edges\naround each vertex and the directions of bends on the edges, but neglecting any\nkind of geometric information such as vertex coordinates or edge lengths. Barth\net al. [2] have established the existence of an analogous ortho-radial\nrepresentation for ortho-radial drawings, which are embeddings into an\northo-radial grid, whose gridlines are concentric circles around the origin and\nstraight-line spokes emanating from the origin but excluding the origin itself.\nWhile any orthogonal representation admits an orthogonal drawing, it is the\ncircularity of the ortho-radial grid that makes the problem of characterizing\nvalid ortho-radial representations all the more complex and interesting. Barth\net al. prove such a characterization. However, the proof is existential and\ndoes not provide an efficient algorithm for testing whether a given\northo-radial representation is valid, let alone actually obtaining a drawing\nfrom an ortho-radial representation. In this paper we give quadratic-time\nalgorithms for both of these tasks. They are based on a suitably constrained\nleft-first DFS in planar graphs and several new insights on ortho-radial\nrepresentations. Our validity check requires quadratic time, and a naive\napplication of it would yield a quartic algorithm for constructing a drawing\nfrom a valid ortho-radial representation. Using further structural insights we\nspeed up the drawing algorithm to quadratic running time.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 16:55:46 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Niedermann", "Benjamin", ""], ["Rutter", "Ignaz", ""], ["Wolf", "Matthias", ""]]}, {"id": "1903.05214", "submitter": "Sadra Sadraddini", "authors": "Sadra Sadraddini and Russ Tedrake", "title": "Linear Encodings for Polytope Containment Problems", "comments": "Shorter version submitted to CDC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The polytope containment problem is deciding whether a polytope is a\ncontained within another polytope. This problem is rooted in computational\nconvexity, and arises in applications such as verification and control of\ndynamical systems. The complexity heavily depends on how the polytopes are\nrepresented. Describing polytopes by their hyperplanes (H-polytopes) is a\npopular representation. In many applications we use affine transformations of\nH-polytopes, which we refer to as AH-polytopes. Zonotopes, orthogonal\nprojections of H-polytopes, and convex hulls/Minkowski sums of multiple\nH-polytopes can be efficiently represented as AH-polytopes. While there exists\nefficient necessary and sufficient conditions for AH-polytope in H-polytope\ncontainment, the case of AH-polytope in AH-polytope is known to be NP-complete.\nIn this paper, we provide a sufficient condition for this problem that is cast\nas a linear program with size that grows linearly with the number of\nhyperplanes of each polytope. Special cases on zonotopes, Minkowski sums,\nconvex hulls, and disjunctions of H-polytopes are studied. These efficient\nencodings enable us to designate certain components of polytopes as decision\nvariables, and incorporate them into a convex optimization problem. We present\nexamples on the zonotope containment problem, polytopic Hausdorff distances,\nzonotope order reduction, inner approximations of orthogonal projections, and\ndemonstrate the usefulness of our results on formal controller verification and\nsynthesis for hybrid systems.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 20:58:53 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Sadraddini", "Sadra", ""], ["Tedrake", "Russ", ""]]}, {"id": "1903.05255", "submitter": "Jie Xue", "authors": "Haitao Wang, Jie Xue", "title": "Near-Optimal Algorithms for Shortest Paths in Weighted Unit-Disk Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit a classical graph-theoretic problem, the \\textit{single-source\nshortest-path} (SSSP) problem, in weighted unit-disk graphs. We first propose\nan exact (and deterministic) algorithm which solves the problem in $O(n \\log^2\nn)$ time using linear space, where $n$ is the number of the vertices of the\ngraph. This significantly improves the previous deterministic algorithm by\nCabello and Jej\\v{c}i\\v{c} [CGTA'15] which uses $O(n^{1+\\delta})$ time and\n$O(n^{1+\\delta})$ space (for any small constant $\\delta>0$) and the previous\nrandomized algorithm by Kaplan et al. [SODA'17] which uses $O(n \\log^{12+o(1)}\nn)$ expected time and $O(n \\log^3 n)$ space. More specifically, we show that if\nthe 2D offline insertion-only (additively-)weighted nearest-neighbor problem\nwith $k$ operations (i.e., insertions and queries) can be solved in $f(k)$\ntime, then the SSSP problem in weighted unit-disk graphs can be solved in $O(n\n\\log n+f(n))$ time. Using the same framework with some new ideas, we also\nobtain a $(1+\\varepsilon)$-approximate algorithm for the problem, using $O(n\n\\log n + n \\log^2(1/\\varepsilon))$ time and linear space. This improves the\nprevious $(1+\\varepsilon)$-approximate algorithm by Chan and Skrepetos\n[SoCG'18] which uses $O((1/\\varepsilon)^2 n \\log n)$ time and\n$O((1/\\varepsilon)^2 n)$ space. More specifically, we show that if the 2D\noffline insertion-only weighted nearest-neighbor problem with $k_1$ operations\nin which at most $k_2$ operations are insertions can be solved in $f(k_1,k_2)$\ntime, then the $(1+\\varepsilon)$-approximate SSSP problem in weighted unit-disk\ngraphs can be solved in $O(n \\log n+f(n,O(\\varepsilon^{-2})))$ time. Because of\nthe $\\Omega(n \\log n)$-time lower bound of the problem (even when approximation\nis allowed), both of our algorithms are almost optimal.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 23:11:02 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Wang", "Haitao", ""], ["Xue", "Jie", ""]]}, {"id": "1903.05256", "submitter": "David Eppstein", "authors": "David Eppstein", "title": "Cubic Planar Graphs that cannot be Drawn on few Lines", "comments": "15 pages, 10 figures. To appear in Proceedings of the 35th\n  International Symposium on Computational Geometry (SoCG 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For every integer $\\ell$, we construct a cubic 3-vertex-connected planar\nbipartite graph $G$ with $O(\\ell^3)$ vertices such that there is no planar\nstraight-line drawing of $G$ whose vertices all lie on $\\ell$ lines. This\nstrengthens previous results on graphs that cannot be drawn on few lines, which\nconstructed significantly larger maximal planar graphs. We also find apex-trees\nand cubic bipartite series-parallel graphs that cannot be drawn on a bounded\nnumber of lines.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 23:23:06 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Eppstein", "David", ""]]}, {"id": "1903.05363", "submitter": "Petr Hlin\\v{e}n\\'y", "authors": "Drago Bokal, Zden\\v{e}k Dvo\\v{r}\\'ak, Petr Hlin\\v{e}n\\'y, Jes\\'us\n  Lea\\~nos, Bojan Mohar and Tilo Wiedera", "title": "Bounded maximum degree conjecture holds precisely for\n  $c$-crossing-critical graphs with $c \\leq 12$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study $c$-crossing-critical graphs, which are the minimal graphs that\nrequire at least $c$ edge-crossings when drawn in the plane. For every fixed\npair of integers with $c\\ge 13$ and $d\\ge 1$, we give first explicit\nconstructions of $c$-crossing-critical graphs containing a vertex of degree\ngreater than $d$. We also show that such unbounded degree constructions do not\nexist for $c\\le 12$, precisely, that there exists a constant $D$ such that\nevery $c$-crossing-critical graph with $c\\le 12$ has maximum degree at most\n$D$. Hence, the bounded maximum degree conjecture of $c$-crossing-critical\ngraphs, which was generally disproved in 2010 by Dvo\\v{r}\\'ak and Mohar\n(without an explicit construction), holds true, surprisingly, exactly for the\nvalues $c\\le 12.$\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 08:51:43 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 11:26:21 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Bokal", "Drago", ""], ["Dvo\u0159\u00e1k", "Zden\u011bk", ""], ["Hlin\u011bn\u00fd", "Petr", ""], ["Lea\u00f1os", "Jes\u00fas", ""], ["Mohar", "Bojan", ""], ["Wiedera", "Tilo", ""]]}, {"id": "1903.05774", "submitter": "Matthew Patitz", "authors": "Daniel Hader and Matthew J. Patitz", "title": "Geometric Tiles and Powers and Limitations of Geometric Hindrance in\n  Self-Assembly", "comments": "Extended version of the 14-page paper to appear in the proceedings of\n  UCNC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tile-based self-assembly systems are capable of universal computation and\nalgorithmically-directed growth. Systems capable of such behavior typically\nmake use of \"glue cooperation\" in which the glues on at least $2$ sides of a\ntile must match and bind to those exposed on the perimeter of an assembly for\nthat tile to attach. However, several models have been developed which utilize\n\"weak cooperation\", where only a single glue needs to bind but other\npreventative forces (such as geometric, or steric, hindrance) provide\nadditional selection for which tiles may attach, and where this allows for\nalgorithmic behavior. In this paper we first work in a model where tiles are\nallowed to have geometric bumps and dents on their edges. We show how such\ntiles can simulate systems of square tiles with complex glue functions (using\nasymptotically optimal sizes of bumps and dents), and also how they can\nsimulate weakly cooperative systems in a model which allows for duples (i.e.\ntiles either twice as long or twice as tall as square tiles). We then show that\nwith only weak cooperation via geometric hindrance, no system in any model can\nsimulate even a class of tightly constrained, deterministic cooperative\nsystems, further defining the boundary of what is possible using this tool.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 00:36:05 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Hader", "Daniel", ""], ["Patitz", "Matthew J.", ""]]}, {"id": "1903.06260", "submitter": "Riddhish Bhalodia", "authors": "Tim Sodergren and Riddhish Bhalodia and Ross Whitaker and Joshua Cates\n  and Nassir Marrouche and Shireen Elhabian", "title": "Mixture Modeling of Global Shape Priors and Autoencoding Local Intensity\n  Priors for Left Atrium Segmentation", "comments": "Statistical Atlases and Computational Models of the Heart. Atrial\n  Segmentation and LV Quantification Challenges 2019", "journal-ref": "Statistical Atlases and Computational Models of the Heart. Atrial\n  Segmentation and LV Quantification Challenges, 2019, Springer International\n  Publishing, Cham 357--367,", "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Difficult image segmentation problems, for instance left atrium MRI, can be\naddressed by incorporating shape priors to find solutions that are consistent\nwith known objects. Nonetheless, a single multivariate Gaussian is not an\nadequate model in cases with significant nonlinear shape variation or where the\nprior distribution is multimodal. Nonparametric density estimation is more\ngeneral, but has a ravenous appetite for training samples and poses serious\nchallenges in optimization, especially in high dimensional spaces. Here, we\npropose a maximum-a-posteriori formulation that relies on a generative image\nmodel by incorporating both local intensity and global shape priors. We use\ndeep autoencoders to capture the complex intensity distribution while avoiding\nthe careful selection of hand-crafted features. We formulate the shape prior as\na mixture of Gaussians and learn the corresponding parameters in a\nhigh-dimensional shape space rather than pre-projecting onto a low-dimensional\nsubspace. In segmentation, we treat the identity of the mixture component as a\nlatent variable and marginalize it within a generalized\nexpectation-maximization framework. We present a conditional maximization-based\nscheme that alternates between a closed-form solution for component-specific\nshape parameters that provides a global update-based optimization strategy, and\nan intensity-based energy minimization that translates the global notion of a\nnonlinear shape prior into a set of local penalties. We demonstrate our\napproach on the left atrial segmentation from gadolinium-enhanced MRI, which is\nuseful in quantifying the atrial geometry in patients with atrial fibrillation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 23:24:08 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Sodergren", "Tim", ""], ["Bhalodia", "Riddhish", ""], ["Whitaker", "Ross", ""], ["Cates", "Joshua", ""], ["Marrouche", "Nassir", ""], ["Elhabian", "Shireen", ""]]}, {"id": "1903.06617", "submitter": "Yufei Tao", "authors": "Yufei Tao and Yu Wang", "title": "Distribution-Sensitive Bounds on Relative Approximations of Geometric\n  Ranges", "comments": "SoCG'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A family $\\mathcal{R}$ of ranges and a set $X$ of points together define a\nrange space $(X, \\mathcal{R}|_X)$, where $\\mathcal{R}|_X = \\{X \\cap h \\mid h\n\\in \\mathcal{R}\\}$. We want to find a structure to estimate the quantity $|X\n\\cap h|/|X|$ for any range $h \\in \\mathcal{R}$ with the $(\\rho,\n\\epsilon)$-guarantee: (i) if $|X \\cap h|/|X| > \\rho$, the estimate must have a\nrelative error $\\epsilon$; (ii) otherwise, the estimate must have an absolute\nerror $\\rho \\epsilon$. The objective is to minimize the size of the structure.\nCurrently, the dominant solution is to compute a relative $(\\rho,\n\\epsilon)$-approximation, which is a subset of $X$ with\n$\\tilde{O}(\\lambda/(\\rho \\epsilon^2))$ points, where $\\lambda$ is the\nVC-dimension of $(X, \\mathcal{R}|_X)$, and $\\tilde{O}$ hides polylog factors.\n  This paper shows a more general bound sensitive to the content of $X$. We\ngive a structure that stores $O(\\log (1/\\rho))$ integers plus $\\tilde{O}(\\theta\n\\cdot (\\lambda/\\epsilon^2))$ points of $X$, where $\\theta$ - called the\ndisagreement coefficient - measures how much the ranges differ from each other\nin their intersections with $X$. The value of $\\theta$ is between 1 and\n$1/\\rho$, such that our space bound is never worse than that of relative\n$(\\rho, \\epsilon)$-approximations, but we improve the latter's $1/\\rho$ term\nwhenever $\\theta = o(\\frac{1}{\\rho \\log (1/\\rho)})$. We also prove that, in the\nworst case, summaries with the $(\\rho, 1/2)$-guarantee must consume\n$\\Omega(\\theta)$ words even for $d = 2$ and $\\lambda \\le 3$.\n  We then constrain $\\mathcal{R}$ to be the set of halfspaces in $\\mathbb{R}^d$\nfor a constant $d$, and prove the existence of structures with $o(1/(\\rho\n\\epsilon^2))$ size offering $(\\rho,\\epsilon)$-guarantees, when $X$ is generated\nfrom various stochastic distributions. This is the first formal justification\non why the term $1/\\rho$ is not compulsory for \"realistic\" inputs.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 16:00:48 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Tao", "Yufei", ""], ["Wang", "Yu", ""]]}, {"id": "1903.06785", "submitter": "Sariel Har-Peled", "authors": "Timothy M. Chan, Sariel Har-Peled", "title": "Smallest k-Enclosing Rectangle Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of $n$ points in the plane, and a parameter $k$, we consider the\nproblem of computing the minimum (perimeter or area) axis-aligned rectangle\nenclosing $k$ points. We present the first near quadratic time algorithm for\nthis problem, improving over the previous near-$O(n^{5/2})$-time algorithm by\nKaplan etal [KRS17]. We provide an almost matching conditional lower bound,\nunder the assumption that $(\\min,+)$-convolution cannot be solved in truly\nsubquadratic time. Furthermore, we present a new reduction (for either\nperimeter or area) that can make the time bound sensitive to $k$, giving near\n$O(n k) $ time. We also present a near linear time\n$(1+\\varepsilon)$-approximation algorithm to the minimum area of the optimal\nrectangle containing $k$ points. In addition, we study related problems\nincluding the $3$-sided, arbitrarily oriented, weighted, and subset sum\nversions of the problem.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 20:16:15 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Chan", "Timothy M.", ""], ["Har-Peled", "Sariel", ""]]}, {"id": "1903.06904", "submitter": "Yair Marom", "authors": "Yair Marom and Dan Feldman", "title": "k-Means Clustering of Lines for Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The input to the $k$-median for lines problem is a set $L$ of $n$ lines in\n$\\mathbb{R}^d$, and the goal is to compute a set of $k$ centers (points) in\n$\\mathbb{R}^d$ that minimizes the sum of squared distances over every line in\n$L$ and its nearest center. This is a straightforward generalization of the\n$k$-median problem where the input is a set of $n$ points instead of lines.\n  We suggest the first PTAS that computes a $(1+\\epsilon)$-approximation to\nthis problem in time $O(n \\log n)$ for any constant approximation error\n$\\epsilon \\in (0, 1)$, and constant integers $k, d \\geq 1$. This is by proving\nthat there is always a weighted subset (called coreset) of $dk^{O(k)}\\log\n(n)/\\epsilon^2$ lines in $L$ that approximates the sum of squared distances\nfrom $L$ to any given set of $k$ points.\n  Using traditional merge-and-reduce technique, this coreset implies results\nfor a streaming set (possibly infinite) of lines to $M$ machines in one pass\n(e.g. cloud) using memory, update time and communication that is\nnear-logarithmic in $n$, as well as deletion of any line but using linear\nspace. These results generalized for other distance functions such as\n$k$-median (sum of distances) or ignoring farthest $m$ lines from the given\ncenters to handle outliers.\n  Experimental results on 10 machines on Amazon EC2 cloud show that the\nalgorithm performs well in practice. Open source code for all the algorithms\nand experiments is also provided.\n  This thesis is an extension of the following accepted paper: \"$k$-Means\nClustering of Lines for Big Data\", by Yair Marom & Dan Feldman, Proceedings of\nNeurIPS 2019 conference, to appear on December 2019.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 10:34:52 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 16:53:54 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 13:35:04 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Marom", "Yair", ""], ["Feldman", "Dan", ""]]}, {"id": "1903.06955", "submitter": "Jisu Kim", "authors": "Jisu Kim, Jaehyeok Shin, Fr\\'ed\\'eric Chazal, Alessandro Rinaldo,\n  Larry Wasserman", "title": "Homotopy Reconstruction via the Cech Complex and the Vietoris-Rips\n  Complex", "comments": "60 pages, 7 figures, to appear in the 36th International Symposium on\n  Computational Geometry (SoCG 2020), the code is available at\n  https://github.com/jisuk1/nerveshape", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive conditions under which the reconstruction of a target space is\ntopologically correct via the \\v{C}ech complex or the Vietoris-Rips complex\nobtained from possibly noisy point cloud data. We provide two novel theoretical\nresults. First, we describe sufficient conditions under which any non-empty\nintersection of finitely many Euclidean balls intersected with a positive reach\nset is contractible, so that the Nerve theorem applies for the restricted\n\\v{C}ech complex. Second, we demonstrate the homotopy equivalence of a positive\n$\\mu$-reach set and its offsets. Applying these results to the restricted\n\\v{C}ech complex and using the interleaving relations with the \\v{C}ech complex\n(or the Vietoris-Rips complex), we formulate conditions guaranteeing that the\ntarget space is homotopy equivalent to the \\v{C}ech complex (or the\nVietoris-Rips complex), in terms of the $\\mu$-reach. Our results sharpen\nexisting results.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 16:37:13 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 22:46:42 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 12:42:47 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Kim", "Jisu", ""], ["Shin", "Jaehyeok", ""], ["Chazal", "Fr\u00e9d\u00e9ric", ""], ["Rinaldo", "Alessandro", ""], ["Wasserman", "Larry", ""]]}, {"id": "1903.06983", "submitter": "Laureano Gonzalez-Vega", "authors": "Alexandre Trocado and Laureano Gonzalez-Vega", "title": "Computing the intersection of two quadrics through projection and\n  lifting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to presenting a new approach to determine the\nintersection of two quadrics based on the detailed analysis of its projection\nin the plane (the so called cutcurve) allowing to perform the corresponding\nlifting correctly. This approach is based on a new computational\ncharacterisation of the singular points of the cutcurve and on how this curve\nis located with respect to the projection of the considered quadrics (whose\nboundaries are the so called silhouette curves).\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 20:43:20 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 16:16:08 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Trocado", "Alexandre", ""], ["Gonzalez-Vega", "Laureano", ""]]}, {"id": "1903.07019", "submitter": "Hugo Akitaya", "authors": "Hugo A. Akitaya, Matias Korman, Oliver Korten, Mikhail Rudoy, Diane L.\n  Souvaine, Csaba D. T\\'oth", "title": "Circumscribing Polygons and Polygonizations for Disjoint Line Segments", "comments": "Extended version (preliminary abstract accepted in the proceedings of\n  SoCG 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a planar straight-line graph $G=(V,E)$ in $\\mathbb{R}^2$, a\n\\emph{circumscribing polygon} of $G$ is a simple polygon $P$ whose vertex set\nis $V$, and every edge in $E$ is either an edge or an internal diagonal of $P$.\nA circumscribing polygon is a \\emph{polygonization} for $G$ if every edge in\n$E$ is an edge of $P$.\n  We prove that every arrangement of $n$ disjoint line segments in the plane\nhas a subset of size $\\Omega(\\sqrt{n})$ that admits a circumscribing polygon,\nwhich is the first improvement on this bound in 20 years. We explore relations\nbetween circumscribing polygons and other problems in combinatorial geometry,\nand generalizations to $\\mathbb{R}^3$.\n  We show that it is NP-complete to decide whether a given graph $G$ admits a\ncircumscribing polygon, even if $G$ is 2-regular. Settling a 30-year old\nconjecture by Rappaport, we also show that it is NP-complete to determine\nwhether a geometric matching admits a polygonization.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 02:52:46 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 00:07:30 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 14:43:36 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Akitaya", "Hugo A.", ""], ["Korman", "Matias", ""], ["Korten", "Oliver", ""], ["Rudoy", "Mikhail", ""], ["Souvaine", "Diane L.", ""], ["T\u00f3th", "Csaba D.", ""]]}, {"id": "1903.07024", "submitter": "Saeed Mehrabi", "authors": "Prosenjit Bose, Paz Carmi, J. Mark Keil, Anil Maheshwari, Saeed\n  Mehrabi, Debajyoti Mondal, and Michiel Smid", "title": "Computing Maximum Independent Set on Outerstring Graphs and Their\n  Relatives", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graph $G$ with $n$ vertices is called an outerstring graph if it has an\nintersection representation of a set of $n$ curves inside a disk such that one\nendpoint of every curve is attached to the boundary of the disk. Given an\nouterstring graph representation, the Maximum Independent Set (MIS) problem of\nthe underlying graph can be solved in $O(s^3)$ time, where $s$ is the number of\nsegments in the representation (Keil et al., Comput. Geom., 60:19--25, 2017).\nIf the strings are of constant size (e.g., line segments, L-shapes, etc.), then\nthe algorithm takes $O(n^3)$ time.\n  In this paper, we examine the fine-grained complexity of the MIS problem on\nsome well-known outerstring representations. We show that solving the MIS\nproblem on grounded segment and grounded square-L representations is at least\nas hard as solving MIS on circle graph representations. Note that no\n$O(n^{2-\\delta})$-time algorithm, $\\delta>0$, is known for the MIS problem on\ncircle graphs. For the grounded string representations where the strings are\n$y$-monotone simple polygonal paths of constant length with segments at\nintegral coordinates, we solve MIS in $O(n^2)$ time and show this to be the\nbest possible under the strong exponential time hypothesis (SETH). For the\nintersection graph of $n$ L-shapes in the plane, we give a $(4\\cdot \\log\nOPT)$-approximation algorithm for MIS (where $OPT$ denotes the size of an\noptimal solution), improving the previously best-known $(4\\cdot \\log\nn)$-approximation algorithm of Biedl and Derka (WADS 2017).\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 04:06:26 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Bose", "Prosenjit", ""], ["Carmi", "Paz", ""], ["Keil", "J. Mark", ""], ["Maheshwari", "Anil", ""], ["Mehrabi", "Saeed", ""], ["Mondal", "Debajyoti", ""], ["Smid", "Michiel", ""]]}, {"id": "1903.07047", "submitter": "Dror Aiger", "authors": "Dror Aiger, Haim Kaplan, Efi Kokiopoulou, Micha Sharir, Bernhard Zeisl", "title": "General techniques for approximate incidences and their application to\n  the camera posing problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical camera pose estimation problem that arises in many\ncomputer vision applications, in which we are given n 2D-3D correspondences\nbetween points in the scene and points in the camera image (some of which are\nincorrect associations), and where we aim to determine the camera pose (the\nposition and orientation of the camera in the scene) from this data. We\ndemonstrate that this posing problem can be reduced to the problem of computing\n{\\epsilon}-approximate incidences between two-dimensional surfaces (derived\nfrom the input correspondences) and points (on a grid) in a four-dimensional\npose space. Similar reductions can be applied to other camera pose problems, as\nwell as to similar problems in related application areas. We describe and\nanalyze three techniques for solving the resulting {\\epsilon}-approximate\nincidences problem in the context of our camera posing application. The first\nis a straightforward assignment of surfaces to the cells of a grid (of\nside-length {\\epsilon}) that they intersect. The second is a variant of a\nprimal-dual technique, recently introduced by a subset of the authors [2] for\ndifferent (and simpler) applications. The third is a non-trivial generalization\nof a data structure Fonseca and Mount [3], originally designed for the case of\nhyperplanes. We present and analyze this technique in full generality, and then\napply it to the camera posing problem at hand. We compare our methods\nexperimentally on real and synthetic data. Our experiments show that for the\ntypical values of n and {\\epsilon}, the primal-dual method is the fastest, also\nin practice.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 09:44:23 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Aiger", "Dror", ""], ["Kaplan", "Haim", ""], ["Kokiopoulou", "Efi", ""], ["Sharir", "Micha", ""], ["Zeisl", "Bernhard", ""]]}, {"id": "1903.07172", "submitter": "Konrad Swanepoel", "authors": "Alastair Maxwell and Konrad J. Swanepoel", "title": "Shortest directed networks in the plane", "comments": "21 pages, 14 figures. To appear in Graphs and Combinatorics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.CG math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of sources and a set of sinks as points in the Euclidean plane, a\ndirected network is a directed graph drawn in the plane with a directed path\nfrom each source to each sink. Such a network may contain nodes other than the\ngiven sources and sinks, called Steiner points. We characterize the local\nstructure of the Steiner points in all shortest-length directed networks in the\nEuclidean plane. This characterization implies that these networks are\nconstructible by straightedge and compass. Our results build on unpublished\nwork of Alfaro, Campbell, Sher, and Soto from 1989 and 1990. Part of the proof\nis based on a new method that uses other norms in the plane. This approach\ngives more conceptual proofs of some of their results, and as a consequence, we\nalso obtain results on shortest directed networks for these norms.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 21:14:18 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 19:32:37 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Maxwell", "Alastair", ""], ["Swanepoel", "Konrad J.", ""]]}, {"id": "1903.07196", "submitter": "Chen Ziv", "authors": "M. Sharir and C. Ziv", "title": "On the Complexity of the k-Level in Arrangements of Pseudoplanes", "comments": "23 pages, 13 figures", "journal-ref": "35th International Symposium on Computational Geometry (SoCG),\n  62:1-15, 2019", "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical open problem in combinatorial geometry is to obtain tight\nasymptotic bounds on the maximum number of k-level vertices in an arrangement\nof n hyperplanes in d dimensions (vertices with exactly k of the hyperplanes\npassing below them). This is a dual version of the k-set problem, which, in a\nprimal setting, seeks bounds for the maximum number of k-sets determined by n\npoints in d dimensions, where a k-set is a subset of size k that can be\nseparated from its complement by a hyperplane. The k-set problem is still wide\nopen even in the plane, with a substantial gap between the best known upper and\nlower bounds. The gap gets larger as the dimension grows. In three dimensions,\nthe best known upper bound is O(nk^(3/2)).\n  In its dual version, the problem can be generalized by replacing hyperplanes\nby other families of surfaces (or curves in the planes). Reasonably sharp\nbounds have been obtained for curves in the plane, but the known upper bounds\nare rather weak for more general surfaces, already in three dimensions, except\nfor the case of triangles. The best known general bound, due to Chan is\nO(n^2.997), for families of surfaces that satisfy certain (fairly weak)\nproperties.\n  In this paper we consider the case of pseudoplanes in 3 dimensions (defined\nin detail in the introduction), and establish the upper bound O(nk^(5/3)) for\nthe number of k-level vertices in an arrangement of n pseudoplanes. The bound\nis obtained by establishing suitable (and nontrivial) extensions of dual\nversions of classical tools that have been used in studying the primal k-set\nproblem, such as the Lova'sz Lemma and the Crossing Lemma.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 23:14:53 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 11:08:44 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Sharir", "M.", ""], ["Ziv", "C.", ""]]}, {"id": "1903.07595", "submitter": "Giordano Da Lozzo", "authors": "Patrizio Angelini, Steven Chaplick, Sabine Cornelsen, Giordano Da\n  Lozzo, and Vincenzo Roselli", "title": "Morphing Contact Representations of Graphs", "comments": "Extended version of \"Morphing Contact Representations of Graphs\", to\n  appear in Proceedings of the 35th International Symposium on Computational\n  Geometry (SoCG 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of morphing between contact representations of a\nplane graph. In an $\\mathcal F$-contact representation of a plane graph $G$,\nvertices are realized by internally disjoint elements from a family $\\mathcal\nF$ of connected geometric objects. Two such elements touch if and only if their\ncorresponding vertices are adjacent. These touchings also induce the same\nembedding as in $G$. In a morph between two $\\mathcal F$-contact\nrepresentations we insist that at each time step (continuously throughout the\nmorph) we have an $\\mathcal F$-contact representation.\n  We focus on the case when $\\mathcal{F}$ is the family of triangles in\n$\\mathbb{R}^2$ that are the lower-right half of axis-parallel rectangles. Such\nRT-representations exist for every plane graph and right triangles are one of\nthe simplest families of shapes supporting this property. Thus, they provide a\nnatural case to study regarding morphs of contact representations of plane\ngraphs.\n  We study piecewise linear morphs, where each step is a linear morph moving\nthe endpoints of each triangle at constant speed along straight-line\ntrajectories. We provide a polynomial-time algorithm that decides whether there\nis a piecewise linear morph between two RT-representations of an $n$-vertex\nplane triangulation, and, if so, computes a morph with $\\mathcal O(n^2)$ linear\nmorphs. As a direct consequence, we obtain that for $4$-connected plane\ntriangulations there is a morph between every pair of RT-representations where\nthe ``top-most'' triangle in both representations corresponds to the same\nvertex. This shows that the realization space of such RT-representations of any\n$4$-connected plane triangulation forms a connected set.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 17:42:52 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Angelini", "Patrizio", ""], ["Chaplick", "Steven", ""], ["Cornelsen", "Sabine", ""], ["Da Lozzo", "Giordano", ""], ["Roselli", "Vincenzo", ""]]}, {"id": "1903.07770", "submitter": "Qiang Zou", "authors": "Qiang Zou, Hsi-Yung Feng", "title": "Variational B-rep Model Analysis for Direct Modeling using Geometric\n  Perturbation", "comments": "16 pages; journal paper", "journal-ref": null, "doi": "10.1016/j.jcde.2019.03.002", "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The very recent CAD paradigm of direct modeling gives rise to the need of\nprocessing 3D geometric constraint systems defined on boundary representation\n(B-rep) models. The major issue of processing such variational B-rep models (in\nthe STEP format) is that free motions of a well-constrained model involve more\nthan just rigid-body motions. The fundamental difficulty lies in having a\nsystematic description of what pattern these free motions follow. This paper\nproposes a geometric perturbation method to study these free motions. This\nmethod is a generalization of the witness method, allowing it to directly deal\nwith variational B-rep models represented with the standard STEP scheme. This\ngeneralization is essentially achieved by using a direct, geometric\nrepresentation of the free motions, and then expressing the free motions in\nterms of composites of several basis motions. To demonstrate the effectiveness\nof the proposed method, a series of comparisons and case studies are presented.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 23:50:58 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Zou", "Qiang", ""], ["Feng", "Hsi-Yung", ""]]}, {"id": "1903.07908", "submitter": "Christian Scheffer", "authors": "S\\'andor P. Fekete, Phillip Keldenich, Christian Scheffer", "title": "Packing Disks into Disks with Optimal Worst-Case Density", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a tight result for a fundamental problem arising from packing\ndisks into a circular container: The critical density of packing disks in a\ndisk is 0.5. This implies that any set of (not necessarily equal) disks of\ntotal area $\\delta\\leq 1/2$ can always be packed into a disk of area 1; on the\nother hand, for any $\\varepsilon>0$ there are sets of disks of area\n$1/2+\\varepsilon$ that cannot be packed. The proof uses a careful manual\nanalysis, complemented by a minor automatic part that is based on interval\narithmetic. Beyond the basic mathematical importance, our result is also useful\nas a blackbox lemma for the analysis of recursive packing algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 09:59:06 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Fekete", "S\u00e1ndor P.", ""], ["Keldenich", "Phillip", ""], ["Scheffer", "Christian", ""]]}, {"id": "1903.07966", "submitter": "Giordano Da Lozzo", "authors": "Carla Binucci, Giordano Da Lozzo, Emilio Di Giacomo, Walter Didimo,\n  Tamara Mchedlidze, and Maurizio Patrignani", "title": "Upward Book Embeddings of st-Graphs", "comments": "Extended version of \"Upward Book Embeddings of st-Graphs\", to appear\n  in Proceedings of the 35th International Symposium on Computational Geometry\n  (SoCG 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study $k$-page upward book embeddings ($k$UBEs) of $st$-graphs, that is,\nbook embeddings of single-source single-sink directed acyclic graphs on $k$\npages with the additional requirement that the vertices of the graph appear in\na topological ordering along the spine of the book. We show that testing\nwhether a graph admits a $k$UBE is NP-complete for $k\\geq 3$. A hardness result\nfor this problem was previously known only for $k = 6$ [Heath and Pemmaraju,\n1999]. Motivated by this negative result, we focus our attention on $k=2$. On\nthe algorithmic side, we present polynomial-time algorithms for testing the\nexistence of $2$UBEs of planar $st$-graphs with branchwidth $\\beta$ and of\nplane $st$-graphs whose faces have a special structure. These algorithms run in\n$O(f(\\beta)\\cdot n+n^3)$ time and $O(n)$ time, respectively, where $f$ is a\nsingly-exponential function on $\\beta$. Moreover, on the combinatorial side, we\npresent two notable families of plane $st$-graphs that always admit an\nembedding-preserving $2$UBE.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 12:45:04 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Binucci", "Carla", ""], ["Da Lozzo", "Giordano", ""], ["Di Giacomo", "Emilio", ""], ["Didimo", "Walter", ""], ["Mchedlidze", "Tamara", ""], ["Patrignani", "Maurizio", ""]]}, {"id": "1903.08014", "submitter": "Peyman Afshani", "authors": "Peyman Afshani and Jeff M. Phillips", "title": "Independent Range Sampling, Revisited Again", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the range sampling problem: the input is a set of points where\neach point is associated with a real-valued weight. The goal is to store them\nin a structure such that given a query range and an integer $k$, we can extract\n$k$ independent random samples from the points inside the query range, where\nthe probability of sampling a point is proportional to its weight.\n  This line of work was initiated in 2014 by Hu, Qiao, and Tao and it was later\nfollowed up by Afshani and Wei. The first line of work mostly studied\nunweighted but dynamic version of the problem in one dimension whereas the\nsecond result considered the static weighted problem in one dimension as well\nas the unweighted problem in 3D for halfspace queries.\n  We offer three main results and some interesting insights that were missed by\nthe previous work: We show that it is possible to build efficient data\nstructures for range sampling queries if we allow the query time to hold in\nexpectation (the first result), or obtain efficient worst-case query bounds by\nallowing the sampling probability to be approximately proportional to the\nweight (the second result). The third result is a conditional lower bound that\nshows essentially one of the previous two concessions is needed. For instance,\nfor the 3D range sampling queries, the first two results give efficient data\nstructures with near-linear space and polylogarithmic query time whereas the\nlower bound shows with near-linear space the worst-case query time must be\nclose to $n^{2/3}$, ignoring polylogarithmic factors. Up to our knowledge, this\nis the first such major gap between the expected and worst-case query time of a\nrange searching problem.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 14:21:43 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Afshani", "Peyman", ""], ["Phillips", "Jeff M.", ""]]}, {"id": "1903.08280", "submitter": "Ivor Hoog v.d.", "authors": "Ivor van der Hoog, Irina Kostitsyna, Maarten L\\\"offler, Bettina\n  Speckmann", "title": "Preprocessing Ambiguous Imprecise Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let ${R} = \\{R_1, R_2, ..., R_n\\}$ be a set of regions and let $ X = \\{x_1,\nx_2, ..., x_n\\}$ be an (unknown) point set with $x_i \\in R_i$. Region $R_i$\nrepresents the uncertainty region of $x_i$. We consider the following question:\nhow fast can we establish order if we are allowed to preprocess the regions in\n$R$? The preprocessing model of uncertainty uses two consecutive phases: a\npreprocessing phase which has access only to ${R}$ followed by a reconstruction\nphase during which a desired structure on $X$ is computed. Recent results in\nthis model parametrize the reconstruction time by the ply of ${R}$, which is\nthe maximum overlap between the regions in ${R}$. We introduce the ambiguity\n$A({R})$ as a more fine-grained measure of the degree of overlap in ${R}$. We\nshow how to preprocess a set of $d$-dimensional disks in $O(n \\log n)$ time\nsuch that we can sort $X$ (if $d=1$) and reconstruct a quadtree on $X$ (if\n$d\\geq 1$ but constant) in $O(A({R}))$ time. If $A({R})$ is sub-linear, then\nreporting the result dominates the running time of the reconstruction phase.\nHowever, we can still return a suitable data structure representing the result\nin $O(A({R}))$ time.\n  In one dimension, ${R}$ is a set of intervals and the ambiguity is linked to\ninterval entropy, which in turn relates to the well-studied problem of sorting\nunder partial information. The number of comparisons necessary to find the\nlinear order underlying a poset $P$ is lower-bounded by the graph entropy of\n$P$. We show that if $P$ is an interval order, then the ambiguity provides a\nconstant-factor approximation of the graph entropy. This gives a lower bound of\n$\\Omega(A({R}))$ in all dimensions for the reconstruction phase (sorting or any\nproximity structure), independent of any preprocessing; hence our result is\ntight.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 22:54:46 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["van der Hoog", "Ivor", ""], ["Kostitsyna", "Irina", ""], ["L\u00f6ffler", "Maarten", ""], ["Speckmann", "Bettina", ""]]}, {"id": "1903.08298", "submitter": "Emilie Purvine", "authors": "Ellen Gasparovic, Maria Gommel, Emilie Purvine, Radmila Sazdanovic,\n  Bei Wang, Yusu Wang, Lori Ziegelmeier", "title": "Local Versus Global Distances for Zigzag Persistence Modules", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short note establishes explicit and broadly applicable relationships\nbetween persistence-based distances computed locally and globally. In\nparticular, we show that the bottleneck distance between two zigzag persistence\nmodules restricted to an interval is always bounded above by the distance\nbetween the unrestricted versions. While this result is not surprising, it\ncould have different practical implications. We give two related applications\nfor metric graph distances, as well as an extension for the matching distance\nbetween multiparameter persistence modules.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 00:59:32 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Gasparovic", "Ellen", ""], ["Gommel", "Maria", ""], ["Purvine", "Emilie", ""], ["Sazdanovic", "Radmila", ""], ["Wang", "Bei", ""], ["Wang", "Yusu", ""], ["Ziegelmeier", "Lori", ""]]}, {"id": "1903.08387", "submitter": "Timothy M. Chan", "authors": "Timothy M. Chan", "title": "Dynamic Geometric Data Structures via Shallow Cuttings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new results on a number of fundamental problems about dynamic\ngeometric data structures:\n  1. We describe the first fully dynamic data structures with sublinear\namortized update time for maintaining (i) the number of vertices or the volume\nof the convex hull of a 3D point set, (ii) the largest empty circle for a 2D\npoint set, (iii) the Hausdorff distance between two 2D point sets, (iv) the\ndiscrete 1-center of a 2D point set, (v)the number of maximal (i.e., skyline)\npoints in a 3D point set. The update times are near $n^{11/12}$ for (i) and\n(ii), $n^{7/8}$ for (iii) and (iv), and $n^{2/3}$ for (v). Previously,\nsublinear bounds were known only for restricted `semi-online' settings [Chan,\nSODA 2002].\n  2. We slightly improve previous fully dynamic data structures for answering\nextreme point queries for the convex hull of a 3D point set and nearest\nneighbor search for a 2D point set. The query time is $O(\\log^2n)$, and the\namortized update time is $O(\\log^4n)$ instead of $O(\\log^5n)$ [Chan, SODA 2006;\nKaplan et al., SODA 2017].\n  3. We also improve previous fully dynamic data structures for maintaining the\nbichromatic closest pair between two 2D point sets and the diameter of a 2D\npoint set. The amortized update time is $O(\\log^4n)$ instead of $O(\\log^7n)$\n[Eppstein 1995; Chan, SODA 2006; Kaplan et al., SODA 2017].\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 08:35:19 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Chan", "Timothy M.", ""]]}, {"id": "1903.08496", "submitter": "Philipp Kindermann", "authors": "Philipp Kindermann and Tamara Mchedlidze and Thomas Schneck and\n  Antonios Symvonis", "title": "Drawing planar graphs with few segments on a polynomial grid", "comments": "Appears in the Proceedings of the 27th International Symposium on\n  Graph Drawing and Network Visualization (GD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visual complexity of a graph drawing can be measured by the number of\ngeometric objects used for the representation of its elements. In this paper,\nwe study planar graph drawings where edges are represented by few segments. In\nsuch a drawing, one segment may represent multiple edges forming a path.\nDrawings of planar graphs with few segments were intensively studied in the\npast years. However, the area requirements were only considered for limited\nsubclasses of planar graphs. In this paper, we show that trees have drawings\nwith $3n/4-1$ segments and $n^2$ area, improving the previous result of\n$O(n^{3.58})$. We also show that 3-connected planar graphs and biconnected\nouterplanar graphs have a drawing with $8n/3-O(1)$ and $3n/2-O(1)$ segments,\nrespectively, and $O(n^3)$ area.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 13:19:44 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 16:24:59 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Kindermann", "Philipp", ""], ["Mchedlidze", "Tamara", ""], ["Schneck", "Thomas", ""], ["Symvonis", "Antonios", ""]]}, {"id": "1903.08510", "submitter": "Hubert Wagner", "authors": "Herbert Edelsbrunner, Ziga Virk, Hubert Wagner", "title": "Topological Data Analysis in Information Space", "comments": null, "journal-ref": "Full version of the paper published in proceedings of the 35th\n  International Symposium on Computational Geometry (SoCG 2019)", "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various kinds of data are routinely represented as discrete probability\ndistributions. Examples include text documents summarized by histograms of word\noccurrences and images represented as histograms of oriented gradients. Viewing\na discrete probability distribution as a point in the standard simplex of the\nappropriate dimension, we can understand collections of such objects in\ngeometric and topological terms. Importantly, instead of using the standard\nEuclidean distance, we look into dissimilarity measures with\ninformation-theoretic justification, and we develop the theory needed for\napplying topological data analysis in this setting. In doing so, we emphasize\nconstructions that enable usage of existing computational topology software in\nthis context.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 14:07:06 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 10:06:53 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Edelsbrunner", "Herbert", ""], ["Virk", "Ziga", ""], ["Wagner", "Hubert", ""]]}, {"id": "1903.08603", "submitter": "Arnaud de Mesmay", "authors": "Vincent Cohen-Addad, \\'Eric Colin de Verdi\\`ere, Daniel Marx and\n  Arnaud de Mesmay", "title": "Almost Tight Lower Bounds for Hard Cutting Problems in Embedded Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove essentially tight lower bounds, conditionally to the Exponential\nTime Hypothesis, for two fundamental but seemingly very different cutting\nproblems on surface-embedded graphs: the Shortest Cut Graph problem and the\nMultiway Cut problem. A cut graph of a graph $G$ embedded on a surface $S$ is a\nsubgraph of $G$ whose removal from $S$ leaves a disk. We consider the problem\nof deciding whether an unweighted graph embedded on a surface of genus $g$ has\na cut graph of length at most a given value. We prove a time lower bound for\nthis problem of $n^{\\Omega(g/\\log g)}$ conditionally to ETH. In other words,\nthe first $n^{O(g)}$-time algorithm by Erickson and Har-Peled [SoCG 2002,\nDiscr.\\ Comput.\\ Geom.\\ 2004] is essentially optimal. We also prove that the\nproblem is W[1]-hard when parameterized by the genus, answering a 17-year old\nquestion of these authors. A multiway cut of an undirected graph $G$ with $t$\ndistinguished vertices, called terminals, is a set of edges whose removal\ndisconnects all pairs of terminals. We consider the problem of deciding whether\nan unweighted graph $G$ has a multiway cut of weight at most a given value. We\nprove a time lower bound for this problem of $n^{\\Omega(\\sqrt{gt +\ng^2+t}/\\log(g+t))}$, conditionally to ETH, for any choice of the genus $g\\ge0$\nof the graph and the number of terminals $t\\ge4$. In other words, the algorithm\nby the second author [Algorithmica 2017] (for the more general multicut\nproblem) is essentially optimal; this extends the lower bound by the third\nauthor [ICALP 2012] (for the planar case). Reductions to planar problems\nusually involve a grid-like structure. The main novel idea for our results is\nto understand what structures instead of grids are needed if we want to exploit\noptimally a certain value $g$ of the genus.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 16:36:08 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 15:56:29 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2021 13:06:12 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Cohen-Addad", "Vincent", ""], ["de Verdi\u00e8re", "\u00c9ric Colin", ""], ["Marx", "Daniel", ""], ["de Mesmay", "Arnaud", ""]]}, {"id": "1903.08637", "submitter": "Radoslav Fulek", "authors": "Radoslav Fulek and Jan Kyn\\v{c}l", "title": "Z_2-genus of graphs and minimum rank of partial symmetric matrices", "comments": "Extended version (preliminary abstract accepted in the proceedings of\n  SoCG 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \\emph{genus} $\\mathrm{g}(G)$ of a graph $G$ is the minimum $g$ such that\n$G$ has an embedding on the orientable surface $M_g$ of genus $g$.\n  A drawing of a graph on a surface is \\emph{independently even} if every pair\nof nonadjacent edges in the drawing crosses an even number of times. The\n\\emph{$\\mathbb{Z}_2$-genus} of a graph $G$, denoted by $\\mathrm{g}_0(G)$, is\nthe minimum $g$ such that $G$ has an independently even drawing on $M_g$.\n  By a result of Battle, Harary, Kodama and Youngs from 1962, the graph genus\nis additive over 2-connected blocks.\n  In 2013, Schaefer and \\v{S}tefankovi\\v{c} proved that the\n$\\mathbb{Z}_2$-genus of a graph is additive over 2-connected blocks as well,\nand asked whether this result can be extended to so-called 2-amalgamations, as\nan analogue of results by Decker, Glover, Huneke, and Stahl for the genus. We\ngive the following partial answer. If $G=G_1\\cup G_2$, $G_1$ and $G_2$\nintersect in two vertices $u$ and $v$, and $G-u-v$ has $k$ connected components\n(among which we count the edge $uv$ if present), then\n$|\\mathrm{g}_0(G)-(\\mathrm{g}_0(G_1)+\\mathrm{g}_0(G_2))|\\le k+1$.\n  For complete bipartite graphs $K_{m,n}$, with $n\\ge m\\ge 3$, we prove that\n$\\frac{\\mathrm{g}_0(K_{m,n})}{\\mathrm{g}(K_{m,n})}=1-O(\\frac{1}{n})$. Similar\nresults are proved also for the Euler $\\mathbb{Z}_2$-genus.\n  We express the $\\mathbb{Z}_2$-genus of a graph using the minimum rank of\npartial symmetric matrices over $\\mathbb{Z}_2$; a problem that might be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 17:50:59 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Fulek", "Radoslav", ""], ["Kyn\u010dl", "Jan", ""]]}, {"id": "1903.08907", "submitter": "Pablo Antolin", "authors": "Fady Massarwi, Pablo Antolin and Gershon Elber", "title": "Volumetric Untrimming: Precise decomposition of trimmed trivariates into\n  tensor products", "comments": "18 pages, 32 figures. Contribution accepted in International\n  Conference on Geometric Modeling and Processing (GMP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D objects, modeled using Computer Aided Geometric Design tools, are\ntraditionally represented using a boundary representation (B-rep), and\ntypically use spline functions to parameterize these boundary surfaces.\nHowever, recent development in physical analysis, in isogeometric analysis\n(IGA) in specific, necessitates a volumetric parametrization of the interior of\nthe object. IGA is performed directly by integrating over the spline spaces of\nthe volumetric spline representation of the object. Typically, tensor-product\nB-spline trivariates are used to parameterize the volumetric domain. A general\n3D object, that can be modeled in contemporary B-rep CAD tools, is typically\nrepresented using trimmed B-spline surfaces. In order to capture the generality\nof the contemporary B-rep modeling space, while supporting IGA needs, Massarwi\nand Elber (2016) proposed the use of trimmed trivariates volumetric elements.\nHowever, the use of trimmed geometry makes the integration process more\ndifficult since integration over trimmed B-spline basis functions is a highly\nchallenging task. In this work, we propose an algorithm that precisely\ndecomposes a trimmed B-spline trivariate into a set of (singular only on the\nboundary) tensor-product B-spline trivariates, that can be utilized to simplify\nthe integration process in IGA. The trimmed B-spline trivariate is first\nsubdivided into a set of trimmed B\\'ezier trivariates, at all its internal\nknots. Then, each trimmed B\\'ezier trivariate, is decomposed into a set of\nmutually exclusive tensor-product B-spline trivariates, that precisely cover\nthe entire trimmed domain. This process, denoted untrimming, can be performed\nin either the Euclidean space or the parametric space of the trivariate. We\npresent examples on complex trimmed trivariates' based geometry, and we\ndemonstrate the effectiveness of the method by applying IGA over the\n(untrimmed) results.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 10:16:51 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Massarwi", "Fady", ""], ["Antolin", "Pablo", ""], ["Elber", "Gershon", ""]]}, {"id": "1903.09358", "submitter": "Hsien-Chih Chang", "authors": "Pankaj K. Agarwal, Hsien-Chih Chang, Allen Xiao", "title": "Efficient Algorithms for Geometric Partial Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $A$ and $B$ be two point sets in the plane of sizes $r$ and $n$\nrespectively (assume $r \\leq n$), and let $k$ be a parameter. A matching\nbetween $A$ and $B$ is a family of pairs in $A \\times B$ so that any point of\n$A \\cup B$ appears in at most one pair. Given two positive integers $p$ and\n$q$, we define the cost of matching $M$ to be $c(M) = \\sum_{(a, b) \\in\nM}\\|{a-b}\\|_p^q$ where $\\|{\\cdot}\\|_p$ is the $L_p$-norm. The geometric partial\nmatching problem asks to find the minimum-cost size-$k$ matching between $A$\nand $B$.\n  We present efficient algorithms for geometric partial matching problem that\nwork for any powers of $L_p$-norm matching objective: An exact algorithm that\nruns in $O((n + k^2) {\\mathop{\\mathrm{polylog}}} n)$ time, and a $(1 +\n\\varepsilon)$-approximation algorithm that runs in $O((n + k\\sqrt{k})\n{\\mathop{\\mathrm{polylog}}} n \\cdot \\log\\varepsilon^{-1})$ time. Both\nalgorithms are based on the primal-dual flow augmentation scheme; the main\nimprovements involve using dynamic data structures to achieve efficient flow\naugmentations. With similar techniques, we give an exact algorithm for the\nplanar transportation problem running in $O(\\min\\{n^2, rn^{3/2}\\}\n{\\mathop{\\mathrm{polylog}}} n)$ time.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 05:03:14 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Agarwal", "Pankaj K.", ""], ["Chang", "Hsien-Chih", ""], ["Xiao", "Allen", ""]]}, {"id": "1903.09416", "submitter": "Yi-Jen Chiang", "authors": "Ching-Hsiang Hsu and Yi-Jen Chiang and Chee Yap", "title": "Rods and Rings: Soft Subdivision Planner for R^3 x S^2", "comments": "Conference version to appear in Proc. Symposium on Computational\n  Geometry (SoCG '19), June, 2019. This is the full version, 25 pages. Some\n  typo regarding a reference to an appendix section was fixed. References were\n  further revised/corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider path planning for a rigid spatial robot moving amidst polyhedral\nobstacles. Our robot is either a rod or a ring. Being axially-symmetric, their\nconfiguration space is R^3 x S^2 with 5 degrees of freedom (DOF). Correct,\ncomplete and practical path planning for such robots is a long standing\nchallenge in robotics. While the rod is one of the most widely studied spatial\nrobots in path planning, the ring seems to be new, and a rare example of a\nnon-simply-connected robot. This work provides rigorous and complete algorithms\nfor these robots with theoretical guarantees. We implemented the algorithms in\nour open-source Core Library. Experiments show that they are practical,\nachieving near real-time performance. We compared our planner to\nstate-of-the-art sampling planners in OMPL.\n  Our subdivision path planner is based on the twin foundations of\n\\epsilon-exactness and soft predicates. Correct implementation is relatively\neasy. The technical innovations include subdivision atlases for S^2,\nintroduction of \\Sigma_2 representations for footprints, and extensions of our\nfeature-based technique for \"opening up the blackbox of collision detection\".\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 09:28:08 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 05:45:53 GMT"}, {"version": "v3", "created": "Fri, 7 Jun 2019 02:34:28 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Hsu", "Ching-Hsiang", ""], ["Chiang", "Yi-Jen", ""], ["Yap", "Chee", ""]]}, {"id": "1903.10445", "submitter": "Sharath Raghvendra", "authors": "Nathaniel Lahn and Sharath Raghvendra", "title": "A Weighted Approach to the Maximum Cardinality Bipartite Matching\n  Problem with Applications in Geometric Settings", "comments": "Appears in SoCG 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a weighted approach to compute a maximum cardinality matching in\nan arbitrary bipartite graph. Our main result is a new algorithm that takes as\ninput a weighted bipartite graph $G(A\\cup B,E)$ with edge weights of $0$ or\n$1$. Let $w \\leq n$ be an upper bound on the weight of any matching in $G$.\nConsider the subgraph induced by all the edges of $G$ with a weight $0$.\nSuppose every connected component in this subgraph has $\\mathcal{O}(r)$\nvertices and $\\mathcal{O}(mr/n)$ edges. We present an algorithm to compute a\nmaximum cardinality matching in $G$ in $\\tilde{\\mathcal{O}}( m(\\sqrt{w}+\n\\sqrt{r}+\\frac{wr}{n}))$ time.\n  When all the edge weights are $1$ (symmetrically when all weights are $0$),\nour algorithm will be identical to the well-known Hopcroft-Karp (HK) algorithm,\nwhich runs in $\\mathcal{O}(m\\sqrt{n})$ time. However, if we can carefully\nassign weights of $0$ and $1$ on its edges such that both $w$ and $r$ are\nsub-linear in $n$ and $wr=\\mathcal{O}(n^{\\gamma})$ for $\\gamma < 3/2$, then we\ncan compute maximum cardinality matching in $G$ in $o(m\\sqrt{n})$ time. Using\nour algorithm, we obtain a new $\\tilde{\\mathcal{O}}(n^{4/3}/\\varepsilon^4)$\ntime algorithm to compute an $\\varepsilon$-approximate bottleneck matching of\n$A,B\\subset\\mathbb{R}^2$ and an\n$\\frac{1}{\\varepsilon^{\\mathcal{O}(d)}}n^{1+\\frac{d-1}{2d-1}}\\mathrm{poly}\\log\nn$ time algorithm for computing $\\varepsilon$-approximate bottleneck matching\nin $d$-dimensions. All previous algorithms take $\\Omega(n^{3/2})$ time. Given\nany graph $G(A \\cup B,E)$ that has an easily computable balanced vertex\nseparator for every subgraph $G'(V',E')$ of size $|V'|^{\\delta}$, for\n$\\delta\\in [1/2,1)$, we can apply our algorithm to compute a maximum matching\nin $\\tilde{\\mathcal{O}}(mn^{\\frac{\\delta}{1+\\delta}})$ time improving upon the\n$\\mathcal{O}(m\\sqrt{n})$ time taken by the HK-Algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 16:27:04 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Lahn", "Nathaniel", ""], ["Raghvendra", "Sharath", ""]]}, {"id": "1903.10710", "submitter": "Josue Tonelli-Cueto", "authors": "Peter B\\\"urgisser, Felipe Cucker, Josu\\'e Tonelli-Cueto", "title": "Computing the Homology of Semialgebraic Sets. II: General formulas", "comments": "33 pages, 4 figures", "journal-ref": null, "doi": "10.1007/s10208-020-09483-8", "report-no": null, "categories": "cs.CG math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and analyze a numerical algorithm for computing the homology\n(Betti numbers and torsion coefficients) of semialgebraic sets given by Boolean\nformulas. The algorithm works in weak exponential time. This means that outside\na subset of data having exponentially small measure, the cost of the algorithm\nis single exponential in the size of the data. This extends the previous work\nof the authors in arXiv:1807.06435 to arbitrary semialgebraic sets.\n  All previous algorithms proposed for this problem have doubly exponential\ncomplexity.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 07:13:21 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 08:31:13 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["B\u00fcrgisser", "Peter", ""], ["Cucker", "Felipe", ""], ["Tonelli-Cueto", "Josu\u00e9", ""]]}, {"id": "1903.10942", "submitter": "Helene Svane", "authors": "Helene Svane and Andrew du Plessis", "title": "Reconstruction of r-Regular Objects from Trinary Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study digital images of r-regular objects where a pixel is black if it is\ncompletely inside the object, white if it is completely inside the complement\nof the object, and grey otherwise. We call such images trinary. We discuss\npossible configurations of pixels in trinary images of r-regular objects at\ncertain resolutions and propose a method for reconstructing objects from such\nimages. We show that the reconstructed object is close to the original object\nin Hausdorff norm, and that there is a homeomorphism of the plane taking the\nreconstructed set to the original.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 15:05:00 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Svane", "Helene", ""], ["Plessis", "Andrew du", ""]]}, {"id": "1903.10943", "submitter": "Ravid Cohen", "authors": "Pankaj K. Agarwal, Ravid Cohen, Dan Halperin and Wolfgang Mulzer", "title": "Maintaining the Union of Unit Discs under Insertions with Near-Optimal\n  Overhead", "comments": "This article is an extension of our previous work arXiv:1902.09565", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present efficient data structures for problems on unit discs and arcs of\ntheir boundary in the plane. (i) We give an output-sensitive algorithm for the\ndynamic maintenance of the union of $n$ unit discs under insertions in $O(k\n\\log^2 n)$ update time and $O(n)$ space, where $k$ is the combinatorial\ncomplexity of the structural change in the union due to the insertion of the\nnew disc. (ii) As part of the solution of (i) we devise a fully dynamic data\nstructure for the maintenance of lower envelopes of pseudo-lines, which we\nbelieve is of independent interest. The structure has $O(\\log^2 n)$ update time\nand $O(\\log n)$ vertical ray shooting query time. To achieve this performance,\nwe devise a new algorithm for finding the intersection between two lower\nenvelopes of pseudo-lines in $O(\\log n)$ time, using \\emph{tentative} binary\nsearch; the lower envelopes are special in that at $x=-\\infty$ any pseudo-line\ncontributing to the first envelope lies below every pseudo-line contributing to\nthe second envelope. (iii) We also present a dynamic range searching structure\nfor a set of circular arcs of unit radius (not necessarily on the boundary of\nthe union of the corresponding discs), where the ranges are unit discs, with\n$O(n \\log n)$ preprocessing time, $O(n^{1/2+\\varepsilon} + \\ell)$ query time\nand $O(\\log^2 n)$ amortized update time, where $\\ell$ is the size of the output\nand for any $\\varepsilon>0$. The structure requires $O(n)$ storage space.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 06:08:27 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Agarwal", "Pankaj K.", ""], ["Cohen", "Ravid", ""], ["Halperin", "Dan", ""], ["Mulzer", "Wolfgang", ""]]}, {"id": "1903.11131", "submitter": "Benjamin Berger", "authors": "Franz-Erich Wolter, Benjamin Berger", "title": "Differential Geometric Foundations for Power Flow Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DG cs.CE cs.CG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to systematically and comprehensively initiate a foundation\nfor using concepts from computational differential geometry as instruments for\npower flow computing and research. At this point we focus our discussion on the\nstatic case, with power flow equations given by quadratic functions defined on\nvoltage space with values in power space; both spaces have real Euclidean\ncoordinates. Central issue is a differential geometric analysis of the power\nflow solution space boundary (SSB) both in voltage and in power space. We\npresent different methods for computing tangent vectors, tangent planes and\nnormals of the SSB and the normals' derivatives. Using the latter we compute\nnormal and principal curvatures. All this is needed for tracing the orthogonal\nprojection of curves in voltage and power space onto the SSB for points on the\nSSB cosest to given points on the curves, thus obtaining estimates for the\ndistance to the SSB. Furthermore, we present a new high precision continuation\nmethod for power flow solutions. We also compute geodesics on the SSB or an\nimplicitly defined submanfold thereof and, used to define geodesic coordinates\ntogether with their Jacobians on the manifolds. These computations might be the\nmost innovative and most significant contribution of this paper, because this\nconcept provides a comprehensive coordinate system for sub many folds defined\nby implicit equations. Therefore while moving on geodesics described by the\ngeodesic coordinates of the sub manifold at hand we get, via systematic\nnavigation guided by geodesic coordinates, access to all feasible operation\npoints of the system. We propose some applications and show some properties of\nthe Jacobian of the power flow map.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 19:43:48 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 19:52:39 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Wolter", "Franz-Erich", ""], ["Berger", "Benjamin", ""]]}, {"id": "1903.11139", "submitter": "Pedro Rocha Filipe Monteiro", "authors": "Pedro Rocha", "title": "Robust NFP generation for Nesting problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cutting and packing problems arise in a large variety of industrial\napplications, where there is a need to cut pieces from a large object, or\nplacing them inside a containers, without overlap. When the pieces or the\ncontainers have irregular outline, the problem is classified as a Nesting\nproblem. The geometrical challenges of the Nesting problem are addressed by\nfocusing on the geometric aspect of the 2D pieces and containers involved. The\nchallenges of the geometrical component are mainly derived from the complexity\nof the pieces, due to high number of vertices, which is common when dealing\nwith real world scenarios. This complexity is challenging for current\nalgorithms to process efficiently and effectively, leading to high\ncomputational cost and less satisfactory results, particularly when dealing\nwith overlap verification operations. Usually, when tackling Nesting problems,\nthe overlap verification process between two objects is done through the use of\na structure known as No-Fit-Polygon (NFP).\n  In this work, the generation of the NFP is achieved through a simple\nalgorithm which produces a simplified shape while reducing numerical precision\nerrors and fully representing the region that forms the NFP including positions\nwith perfect fits.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 20:09:32 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Rocha", "Pedro", ""]]}, {"id": "1903.11287", "submitter": "Mateusz Skomra", "authors": "Mateusz Skomra and St\\'ephan Thomass\\'e", "title": "Convexly independent subsets of Minkowski sums of convex polygons", "comments": "v1: 9 pages, 3 figures; v2: minor revision, 10 pages, 5 figures", "journal-ref": "Discrete Mathematics, Volume 344, Issue 8, August 2021, 112472", "doi": "10.1016/j.disc.2021.112472", "report-no": null, "categories": "math.CO cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that there exist convex $n$-gons $P$ and $Q$ such that the largest\nconvex polygon in the Minkowski sum $P+Q$ has size $\\Theta(n\\log n)$. This\nmatches an upper bound of Tiwary.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 08:24:37 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 17:49:35 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Skomra", "Mateusz", ""], ["Thomass\u00e9", "St\u00e9phan", ""]]}, {"id": "1903.11445", "submitter": "Jules Wulms", "authors": "Wouter Meulemans, Kevin Verbeek, Jules Wulms", "title": "Stability analysis of kinetic orientation-based shape descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study three orientation-based shape descriptors on a set of continuously\nmoving points: the first principal component, the smallest oriented bounding\nbox and the thinnest strip. Each of these shape descriptors essentially defines\na cost capturing the quality of the descriptor and uses the orientation that\nminimizes the cost. This optimal orientation may be very unstable as the points\nare moving, which is undesirable in many practical scenarios. If we bound the\nspeed with which the orientation of the descriptor may change, this may lower\nthe quality of the resulting shape descriptor. In this paper we study the\ntrade-off between stability and quality of these shape descriptors. We first\nshow that there is no stateless algorithm, an algorithm that keeps no state\nover time, that both approximates the minimum cost of a shape descriptor and\nachieves continuous motion for the shape descriptor. On the other hand, if we\ncan use the previous state of the shape descriptor to compute the new state, we\ncan define \"chasing\" algorithms that attempt to follow the optimal orientation\nwith bounded speed. We show that, under mild conditions, chasing algorithms\nwith sufficient bounded speed approximate the optimal cost at all times for\noriented bounding boxes and strips. The analysis of such chasing algorithms is\nchallenging and has received little attention in literature, hence we believe\nthat our methods used in this analysis are of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 14:25:08 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 10:05:12 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Meulemans", "Wouter", ""], ["Verbeek", "Kevin", ""], ["Wulms", "Jules", ""]]}, {"id": "1903.12359", "submitter": "Gary Pui-Tung Choi", "authors": "Gary P. T. Choi, Yusan Leung-Liu, Xianfeng Gu, Lok Ming Lui", "title": "Parallelizable global conformal parameterization of simply-connected\n  surfaces via partial welding", "comments": null, "journal-ref": "SIAM Journal on Imaging Sciences 13(3), 1049-1083 (2020)", "doi": "10.1137/19M125337X", "report-no": null, "categories": "cs.CG cs.DC cs.GR math.CV math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal surface parameterization is useful in graphics, imaging and\nvisualization, with applications to texture mapping, atlas construction,\nregistration, remeshing and so on. With the increasing capability in scanning\nand storing data, dense 3D surface meshes are common nowadays. While meshes\nwith higher resolution better resemble smooth surfaces, they pose computational\ndifficulties for the existing parameterization algorithms. In this work, we\npropose a novel parallelizable algorithm for computing the global conformal\nparameterization of simply-connected surfaces via partial welding maps. A given\nsimply-connected surface is first partitioned into smaller subdomains. The\nlocal conformal parameterizations of all subdomains are then computed in\nparallel. The boundaries of the parameterized subdomains are subsequently\nintegrated consistently using a novel technique called partial welding, which\nis developed based on conformal welding theory. Finally, by solving the Laplace\nequation for each subdomain using the updated boundary conditions, we obtain a\nglobal conformal parameterization of the given surface, with bijectivity\nguaranteed by quasi-conformal theory. By including additional shape\nconstraints, our method can be easily extended to achieve disk conformal\nparameterization for simply-connected open surfaces and spherical conformal\nparameterization for genus-0 closed surfaces. Experimental results are\npresented to demonstrate the effectiveness of our proposed algorithm. When\ncompared to the state-of-the-art conformal parameterization methods, our method\nachieves a significant improvement in both computational time and accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 05:43:33 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 05:56:11 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Choi", "Gary P. T.", ""], ["Leung-Liu", "Yusan", ""], ["Gu", "Xianfeng", ""], ["Lui", "Lok Ming", ""]]}, {"id": "1903.12516", "submitter": "Patrick Schnider", "authors": "Patrick Schnider", "title": "Ham-Sandwich cuts and center transversals in subspaces", "comments": "In proceedings of the 35th International Symposium on Computational\n  Geometry (SoCG 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ham-Sandwich theorem is a well-known result in geometry. It states that\nany $d$ mass distributions in $\\mathbb{R}^d$ can be simultaneously bisected by\na hyperplane. The result is tight, that is, there are examples of $d+1$ mass\ndistributions that cannot be simultaneously bisected by a single hyperplane. In\nthis abstract we will study the following question: given a continuous\nassignment of mass distributions to certain subsets of $\\mathbb{R}^d$, is there\na subset on which we can bisect more masses than what is guaranteed by the\nHam-Sandwich theorem?\n  We investigate two types of subsets. The first type are linear subspaces of\n$\\mathbb{R}^d$, i.e., $k$-dimensional flats containing the origin. We show that\nfor any continuous assignment of $d$ mass distributions to the $k$-dimensional\nlinear subspaces of $\\mathbb{R}^d$, there is always a subspace on which we can\nsimultaneously bisect the images of all $d$ assignments. We extend this result\nto center transversals, a generalization of Ham-Sandwich cuts. As for\nHam-Sandwich cuts, we further show that for $d-k+2$ masses, we can choose $k-1$\nof the vectors defining the $k$-dimensional subspace in which the solution\nlies.\n  The second type of subsets we consider are subsets that are determined by\nfamilies of $n$ hyperplanes in $\\mathbb{R}^d$. Also in this case, we find a\nHam-Sandwich-type result. In an attempt to solve a conjecture by Langerman\nabout bisections with several cuts, we show that our underlying topological\nresult can be used to prove this conjecture in a relaxed setting.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 13:31:41 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Schnider", "Patrick", ""]]}]