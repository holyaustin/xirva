[{"id": "1901.00238", "submitter": "Ran Ling", "authors": "Gang Xu, Ran Ling, Jessica Zhang, Zhoufang Xiao, Zhongping Ji and\n  Timon Rabczuk", "title": "Singularity Structure Simplification of Hexahedral Mesh via Weighted\n  Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an improved singularity structure simplification\nmethod for hexahedral (hex) meshes using a weighted ranking approach. In\nprevious work, the selection of to-be-collapsed base complex sheets/chords is\nonly based on their thickness, which will introduce a few closed-loops and\ncause an early termination of simplification and a slow convergence rate. In\nthis paper, a new weighted ranking function is proposed by combining the\nvalence prediction function of local singularity structure, shape quality\nmetric of elements and the width of base complex sheets/chords together.\nAdaptive refinement and local optimization are also introduced to improve the\nuniformity and aspect ratio of mesh elements. Compared to thickness ranking\nmethods, our weighted ranking approach can yield a simpler singularity\nstructure with fewer base-complex components, while achieving comparable\nHausdorff distance ratio and better mesh quality. Comparisons on a hex-mesh\ndataset are performed to demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 01:54:47 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 07:44:23 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Xu", "Gang", ""], ["Ling", "Ran", ""], ["Zhang", "Jessica", ""], ["Xiao", "Zhoufang", ""], ["Ji", "Zhongping", ""], ["Rabczuk", "Timon", ""]]}, {"id": "1901.00990", "submitter": "Julian Marcon", "authors": "Julian Marcon, Michael Turner, David Moxey, Spencer J. Sherwin,\n  Joaquim Peir\\'o", "title": "A variational approach to high-order r-adaptation", "comments": "Pre-print accepted to the 26th International Meshing Roundtable", "journal-ref": "26th International Meshing Roundtable (2017)", "doi": null, "report-no": null, "categories": "cs.CG physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variational framework, initially developed for high-order mesh\noptimisation, is being extended for r-adaptation. The method is based on the\nminimisation of a functional of the mesh deformation. To achieve adaptation,\nelements of the initial mesh are manipulated using metric tensors to obtain\ntarget elements. The nonlinear optimisation in turns adapts the final\nhigh-order mesh to best fit the description of the target elements by\nminimising the element distortion. Encouraging preliminary results prove that\nthe method behaves well and can be used in the future for more extensive work\nwhich shall include the use of error indicators from CFD simulations.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 06:27:31 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Marcon", "Julian", ""], ["Turner", "Michael", ""], ["Moxey", "David", ""], ["Sherwin", "Spencer J.", ""], ["Peir\u00f3", "Joaquim", ""]]}, {"id": "1901.00992", "submitter": "Julian Marcon", "authors": "Julian Marcon, Michael Turner, Joaquim Peir\\'o, David Moxey, Claire R.\n  Pollard, Henry Bucklow, Mark Gammon", "title": "High-order curvilinear hybrid mesh generation for CFD simulations", "comments": "Pre-print accepted to the 2018 AIAA Aerospace Sciences Meeting", "journal-ref": "AIAA SciTech Forum (AIAA 2018-1403)", "doi": "10.2514/6.2018-1403", "report-no": null, "categories": "cs.GR cs.CG physics.comp-ph physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a semi-structured method for the generation of high-order hybrid\nmeshes suited for the simulation of high Reynolds number flows. This is\nachieved through the use of highly stretched elements in the viscous boundary\nlayers near the wall surfaces. CADfix is used to first repair any possible\ndefects in the CAD geometry and then generate a medial object based\ndecomposition of the domain that wraps the wall boundaries with partitions\nsuitable for the generation of either prismatic or hexahedral elements. The\nlatter is a novel distinctive feature of the method that permits to obtain\nwell-shaped hexahedral meshes at corners or junctions in the boundary layer.\nThe medial object approach allows greater control on the \"thickness\" of the\nboundary-layer mesh than is generally achievable with advancing layer\ntechniques. CADfix subsequently generates a hybrid straight sided mesh of\nprismatic and hexahedral elements in the near-field region modelling the\nboundary layer, and tetrahedral elements in the far-field region covering the\nrest of the domain. The mesh in the near-field region provides a framework that\nfacilitates the generation, via an isoparametric technique, of layers of highly\nstretched elements with a distribution of points in the direction normal to the\nwall tailored to efficiently and accurately capture the flow in the boundary\nlayer. The final step is the generation of a high-order mesh using NekMesh, a\nhigh-order mesh generator within the Nektar++ framework. NekMesh uses the\nCADfix API as a geometry engine that handles all the geometrical queries to the\nCAD geometry required during the high-order mesh generation process. We will\ndescribe in some detail the methodology using a simple geometry, a NACA wing\ntip, for illustrative purposes. Finally, we will present two examples of\napplication to reasonably complex geometries proposed by NASA as CFD validation\ncases.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 06:31:38 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Marcon", "Julian", ""], ["Turner", "Michael", ""], ["Peir\u00f3", "Joaquim", ""], ["Moxey", "David", ""], ["Pollard", "Claire R.", ""], ["Bucklow", "Henry", ""], ["Gammon", "Mark", ""]]}, {"id": "1901.01255", "submitter": "Tolga Birdal", "authors": "Tolga Birdal and Benjamin Busam and Nassir Navab and Slobodan Ilic and\n  Peter Sturm", "title": "Generic Primitive Detection in Point Clouds Using Novel Minimal Quadric\n  Fits", "comments": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI). arXiv admin note: substantial text overlap with\n  arXiv:1803.07191", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel and effective method for detecting 3D primitives in\ncluttered, unorganized point clouds, without axillary segmentation or type\nspecification. We consider the quadric surfaces for encapsulating the basic\nbuilding blocks of our environments - planes, spheres, ellipsoids, cones or\ncylinders, in a unified fashion. Moreover, quadrics allow us to model higher\ndegree of freedom shapes, such as hyperboloids or paraboloids that could be\nused in non-rigid settings.\n  We begin by contributing two novel quadric fits targeting 3D point sets that\nare endowed with tangent space information. Based upon the idea of aligning the\nquadric gradients with the surface normals, our first formulation is exact and\nrequires as low as four oriented points. The second fit approximates the first,\nand reduces the computational effort. We theoretically analyze these fits with\nrigor, and give algebraic and geometric arguments. Next, by re-parameterizing\nthe solution, we devise a new local Hough voting scheme on the null-space\ncoefficients that is combined with RANSAC, reducing the complexity from\n$O(N^4)$ to $O(N^3)$ (three points). To the best of our knowledge, this is the\nfirst method capable of performing a generic cross-type multi-object primitive\ndetection in difficult scenes without segmentation. Our extensive qualitative\nand quantitative results show that our method is efficient and flexible, as\nwell as being accurate.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 12:09:50 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Birdal", "Tolga", ""], ["Busam", "Benjamin", ""], ["Navab", "Nassir", ""], ["Ilic", "Slobodan", ""], ["Sturm", "Peter", ""]]}, {"id": "1901.01476", "submitter": "Philipp Kindermann", "authors": "Therese Biedl, Ahmad Biniaz, Veronika Irvine, Kshitij Jain, Philipp\n  Kindermann, Anna Lubiw", "title": "Maximum Matchings and Minimum Blocking Sets in $\\Theta_6$-Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $\\Theta_6$-Graphs graphs are important geometric graphs that have many\napplications especially in wireless sensor networks. They are equivalent to\nDelaunay graphs where empty equilateral triangles take the place of empty\ncircles. We investigate lower bounds on the size of maximum matchings in these\ngraphs. The best known lower bound is $n/3$, where $n$ is the number of\nvertices of the graph. Babu et al. (2014) conjectured that any $\\Theta_6$-graph\nhas a (near-)perfect matching (as is true for standard Delaunay graphs).\nAlthough this conjecture remains open, we improve the lower bound to\n$(3n-8)/7$.\n  We also relate the size of maximum matchings in $\\Theta_6$-graphs to the\nminimum size of a blocking set. Every edge of a $\\Theta_6$-graph on point set\n$P$ corresponds to an empty triangle that contains the endpoints of the edge\nbut no other point of $P$. A blocking set has at least one point in each such\ntriangle. We prove that the size of a maximum matching is at least $\\beta(n)/2$\nwhere $\\beta(n)$ is the minimum, over all theta-six graphs with $n$ vertices,\nof the minimum size of a blocking set. In the other direction, lower bounds on\nmatchings can be used to prove bounds on $\\beta$, allowing us to show that\n$\\beta(n)\\geq 3n/4-2$.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 00:22:04 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 23:00:11 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Biedl", "Therese", ""], ["Biniaz", "Ahmad", ""], ["Irvine", "Veronika", ""], ["Jain", "Kshitij", ""], ["Kindermann", "Philipp", ""], ["Lubiw", "Anna", ""]]}, {"id": "1901.01504", "submitter": "Andr\\'e Nusser", "authors": "Karl Bringmann, Marvin K\\\"unnemann, Andr\\'e Nusser", "title": "Walking the Dog Fast in Practice: Algorithm Engineering of the Fr\\'echet\n  Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Fr\\'echet distance provides a natural and intuitive measure for the\npopular task of computing the similarity of two (polygonal) curves. While a\nsimple algorithm computes it in near-quadratic time, a strongly subquadratic\nalgorithm cannot exist unless the Strong Exponential Time Hypothesis fails.\nStill, fast practical implementations of the Fr\\'echet distance, in particular\nfor realistic input curves, are highly desirable. This has even lead to a\ndesignated competition, the ACM SIGSPATIAL GIS Cup 2017: Here, the challenge\nwas to implement a near-neighbor data structure under the Fr\\'echet distance.\nThe bottleneck of the top three implementations turned out to be precisely the\ndecision procedure for the Fr\\'echet distance.\n  In this work, we present a fast, certifying implementation for deciding the\nFr\\'echet distance, in order to (1) complement its pessimistic worst-case\nhardness by an empirical analysis on realistic input data and to (2) improve\nthe state of the art for the GIS Cup challenge. We experimentally evaluate our\nimplementation on a large benchmark consisting of several data sets (including\nhandwritten characters and GPS trajectories). Compared to the winning\nimplementation of the GIS Cup, we obtain running time improvements of up to\nmore than two orders of magnitude for the decision procedure and of up to a\nfactor of 30 for queries to the near-neighbor data structure.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 06:16:48 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Bringmann", "Karl", ""], ["K\u00fcnnemann", "Marvin", ""], ["Nusser", "Andr\u00e9", ""]]}, {"id": "1901.01651", "submitter": "Gary Pui-Tung Choi", "authors": "Gary P. T. Choi, Hei Long Chan, Robin Yong, Sarbin Ranjitkar, Alan\n  Brook, Grant Townsend, Ke Chen, Lok Ming Lui", "title": "Tooth morphometry using quasi-conformal theory", "comments": null, "journal-ref": "Pattern Recognition 99, 107064 (2020)", "doi": "10.1016/j.patcog.2019.107064", "report-no": null, "categories": "cs.CV cs.CG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape analysis is important in anthropology, bioarchaeology and forensic\nscience for interpreting useful information from human remains. In particular,\nteeth are morphologically stable and hence well-suited for shape analysis. In\nthis work, we propose a framework for tooth morphometry using quasi-conformal\ntheory. Landmark-matching Teichm\\\"uller maps are used for establishing a 1-1\ncorrespondence between tooth surfaces with prescribed anatomical landmarks.\nThen, a quasi-conformal statistical shape analysis model based on the\nTeichm\\\"uller mapping results is proposed for building a tooth classification\nscheme. We deploy our framework on a dataset of human premolars to analyze the\ntooth shape variation among genders and ancestries. Experimental results show\nthat our method achieves much higher classification accuracy with respect to\nboth gender and ancestry when compared to the existing methods. Furthermore,\nour model reveals the underlying tooth shape difference between different\ngenders and ancestries in terms of the local geometric distortion and\ncurvatures.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 03:00:12 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Choi", "Gary P. T.", ""], ["Chan", "Hei Long", ""], ["Yong", "Robin", ""], ["Ranjitkar", "Sarbin", ""], ["Brook", "Alan", ""], ["Townsend", "Grant", ""], ["Chen", "Ke", ""], ["Lui", "Lok Ming", ""]]}, {"id": "1901.01763", "submitter": "Ali Gholami Rudi", "authors": "Ali Gholami Rudi", "title": "Approximate Discontinuous Trajectory Hotspots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hotspot is an axis-aligned square of fixed side length $s$, the duration of\nthe presence of an entity moving in the plane in which is maximised. An exact\nhotspot of a polygonal trajectory with $n$ edges can be found in $O(n^2)$.\nDefining a $c$-approximate hotspot as an axis-aligned square of side length\n$cs$, in which the duration of the entity's presence is no less than that of an\nexact hotspot, in this paper we present an algorithm to find a $(1 +\n\\epsilon)$-approximate hotspot of a polygonal trajectory with the time\ncomplexity $O({n\\phi \\over \\epsilon} \\log {n\\phi \\over \\epsilon})$, where\n$\\phi$ is the ratio of average trajectory edge length to $s$.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 12:01:26 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Rudi", "Ali Gholami", ""]]}, {"id": "1901.01870", "submitter": "Dennis Rohde", "authors": "Dennis Rohde", "title": "Coresets for $(k,l)$-Clustering under the Fr\\'echet Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is the task of partitioning a given set of geometric objects. This\nis thoroughly studied when the objects are points in the euclidean space. There\nare also several approaches for points in general metric spaces. In this thesis\nwe consider clustering polygonal curves, i.e., curves composed of line\nsegments, under the Fr\\'echet distance. We obtain clusterings by minimizing an\nobjective function, which yields a set of centers that induces a partition of\nthe input.\n  The objective functions we consider is the so called $(k,l)$-\\textsc{center},\nwhere we are to find the $k$ center-curves that minimize the maximum distance\nbetween any input-curve and a nearest center-curve and the so called\n$k$-\\textsc{median}, where we are to find the $k$ center-curves that minimize\nthe sum of the distances between the input-curves and a nearest center-curve.\n  Given a set of $n$ polygonal curves, we are interested in reducing this set\nto an $\\epsilon$-coreset, i.e., a notably smaller set of curves that has a very\nsimilar clustering-behavior. We develop a construction method for such\n$\\epsilon$-coresets for the $(k,l)$-\\textsc{center}, that yields\n$\\epsilon$-coresets of size of a polynomial of $\\frac{1}{\\epsilon}$, in time\nlinear in $n$ and a polynomial of $\\frac{1}{\\epsilon}$, for line segments.\nAlso, we develop a construction technique for the $(k,l)$-\\textsc{center} that\nyields $\\epsilon$-coresets of size exponential in $m$ with basis\n$\\frac{1}{\\epsilon}$, in time sub-quadratic in $n$ and exponential in $m$ with\nbasis $\\frac{1}{\\epsilon}$, for general polygonal curves. Finally, we develop a\nconstruction method for the $k$-\\textsc{median}, that yields\n$\\epsilon$-coresets of size polylogarithmic in $n$ and a polynomial of\n$\\frac{1}{\\epsilon}$, in time linear in $n$ and a polynomial of\n$\\frac{1}{\\epsilon}$.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 15:07:23 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 11:11:02 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 16:16:36 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Rohde", "Dennis", ""]]}, {"id": "1901.01968", "submitter": "Julian Marcon", "authors": "Julian Marcon, Joaquim Peir\\'o, David Moxey, Nico Bergemann, Henry\n  Bucklow, Mark Gammon", "title": "A semi-structured approach to curvilinear mesh generation around\n  streamlined bodies", "comments": "Preprint accepted to the 2019 AIAA Aerospace Sciences Meeting", "journal-ref": "AIAA SciTech Forum (AIAA 2019-1725)", "doi": "10.2514/6.2019-1725", "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for robust high-order mesh generation specially\ntailored to streamlined bodies. The method is based on a semi-sructured\napproach which combines the high quality of structured meshes in the near-field\nwith the flexibility of unstructured meshes in the far-field. We utilise medial\naxis technology to robustly partition the near-field into blocks which can be\nmeshed coarsely with a linear swept mesher. A high-order mesh of the near-field\nis then generated and split using an isoparametric approach which allows us to\nobtain highly stretched elements aligned with the flow field. Special treatment\nof the partition is performed on the wing root juntion and the trailing edge\n--- into the wake --- to obtain an H-type mesh configuration with anisotropic\nhexahedra ideal for the strong shear of high Reynolds number simulations. We\nthen proceed to discretise the far-field using traditional robust tetrahedral\nmeshing tools. This workflow is made possible by two sets of tools: CADfix,\nfocused on CAD system, the block partitioning of the near-field and the\ngeneration of a linear mesh; and NekMesh, focused on the curving of the\nhigh-order mesh and the generation of highly-stretched boundary layer elements.\nWe demonstrate this approach on a NACA0012 wing attached to a wall and show\nthat a gap between the wake partition and the wall can be inserted to remove\nthe dependency of the partitioning procedure on the local geometry.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 18:51:03 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Marcon", "Julian", ""], ["Peir\u00f3", "Joaquim", ""], ["Moxey", "David", ""], ["Bergemann", "Nico", ""], ["Bucklow", "Henry", ""], ["Gammon", "Mark", ""]]}, {"id": "1901.02070", "submitter": "Chiyu Jiang", "authors": "Chiyu \"Max\" Jiang, Dequan Wang, Jingwei Huang, Philip Marcus, Matthias\n  Nie{\\ss}ner", "title": "Convolutional Neural Networks on non-uniform geometrical signals using\n  Euclidean spectral transformation", "comments": "Accepted as a conference paper at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CG cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNN) have been successful in processing data\nsignals that are uniformly sampled in the spatial domain (e.g., images).\nHowever, most data signals do not natively exist on a grid, and in the process\nof being sampled onto a uniform physical grid suffer significant aliasing error\nand information loss. Moreover, signals can exist in different topological\nstructures as, for example, points, lines, surfaces and volumes. It has been\nchallenging to analyze signals with mixed topologies (for example, point cloud\nwith surface mesh). To this end, we develop mathematical formulations for\nNon-Uniform Fourier Transforms (NUFT) to directly, and optimally, sample\nnonuniform data signals of different topologies defined on a simplex mesh into\nthe spectral domain with no spatial sampling error. The spectral transform is\nperformed in the Euclidean space, which removes the translation ambiguity from\nworks on the graph spectrum. Our representation has four distinct advantages:\n(1) the process causes no spatial sampling error during the initial sampling,\n(2) the generality of this approach provides a unified framework for using CNNs\nto analyze signals of mixed topologies, (3) it allows us to leverage\nstate-of-the-art backbone CNN architectures for effective learning without\nhaving to design a particular architecture for a particular data structure in\nan ad-hoc fashion, and (4) the representation allows weighted meshes where each\nelement has a different weight (i.e., texture) indicating local properties. We\nachieve results on par with the state-of-the-art for the 3D shape retrieval\ntask, and a new state-of-the-art for the point cloud to surface reconstruction\ntask.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 21:23:33 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Jiang", "Chiyu \"Max\"", ""], ["Wang", "Dequan", ""], ["Huang", "Jingwei", ""], ["Marcus", "Philip", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1901.02405", "submitter": "Julian Marcon", "authors": "Julian Marcon, David A. Kopriva, Spencer J. Sherwin, Joaquim Peir\\'o", "title": "A High Resolution PDE Approach to Quadrilateral Mesh Generation", "comments": "31 pages, 21 figures, accepted for publication in Journal of\n  Computational Physics", "journal-ref": "Journal of Computational Physics 399C (2019) 108918", "doi": "10.1016/j.jcp.2019.108918", "report-no": null, "categories": "math.NA cs.CG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a high order technique to generate quadrilateral decompositions\nand meshes for complex two dimensional domains using spectral elements in a\nfield guided procedure. Inspired by cross field methods, we never actually\ncompute crosses. Instead, we compute a high order accurate guiding field using\na continuous Galerkin (CG) or discontinuous Galerkin (DG) spectral element\nmethod to solve a Laplace equation for each of the field variables using the\nopen source code Nektar++. The spectral method provides spectral convergence\nand sub-element resolution of the fields. The DG approximation allows meshing\nof corners that are not multiples of $\\pi/2$ in a discretization consistent\nmanner, when needed. The high order field can then be exploited to accurately\nfind irregular nodes, and can be accurately integrated using a high order\nseparatrix integration method to avoid features like limit cycles. The result\nis a mesh with naturally curved quadrilateral elements that do not need to be\ncurved a posteriori to eliminate invalid elements. The mesh generation\nprocedure is implemented in the open source mesh generation program NekMesh.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 17:03:21 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 13:44:37 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Marcon", "Julian", ""], ["Kopriva", "David A.", ""], ["Sherwin", "Spencer J.", ""], ["Peir\u00f3", "Joaquim", ""]]}, {"id": "1901.02508", "submitter": "Fereshteh Sadat Bashiri", "authors": "Fereshteh S. Bashiri, Reihaneh Rostami, Peggy Peissig, Roshan M.\n  D'Souza, Zeyun Yu", "title": "An Application of Manifold Learning in Global Shape Descriptors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid expansion of applied 3D computational vision, shape\ndescriptors have become increasingly important for a wide variety of\napplications and objects from molecules to planets. Appropriate shape\ndescriptors are critical for accurate (and efficient) shape retrieval and 3D\nmodel classification. Several spectral-based shape descriptors have been\nintroduced by solving various physical equations over a 3D surface model. In\nthis paper, for the first time, we incorporate a specific group of techniques\nin statistics and machine learning, known as manifold learning, to develop a\nglobal shape descriptor in the computer graphics domain. The proposed\ndescriptor utilizes the Laplacian Eigenmap technique in which the Laplacian\neigenvalue problem is discretized using an exponential weighting scheme. As a\nresult, our descriptor eliminates the limitations tied to the existing spectral\ndescriptors, namely dependency on triangular mesh representation and high\nintra-class quality of 3D models. We also present a straightforward\nnormalization method to obtain a scale-invariant descriptor. The extensive\nexperiments performed in this study show that the present contribution provides\na highly discriminative and robust shape descriptor under the presence of a\nhigh level of noise, random scale variations, and low sampling rate, in\naddition to the known isometric-invariance property of the Laplace-Beltrami\noperator. The proposed method significantly outperforms state-of-the-art\nalgorithms on several non-rigid shape retrieval benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 20:41:49 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Bashiri", "Fereshteh S.", ""], ["Rostami", "Reihaneh", ""], ["Peissig", "Peggy", ""], ["D'Souza", "Roshan M.", ""], ["Yu", "Zeyun", ""]]}, {"id": "1901.02823", "submitter": "Jozsef Molnar", "authors": "Jozsef Molnar, Michael Barbier, Winnok H. De Vos, Peter Horvath", "title": "An Elastic Energy Minimization Framework for Mean Contour Calculation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a contour mean calculation and interpolation method\ndesigned for averaging manual delineations of objects performed by experts and\ninterpolate 3D layer stack images. The proposed method retains all visible\ninformation of the input contour set: the relative positions, orientations and\nsize, but allows invisible quantities - parameterization and the centroid - to\nbe changed. The chosen representation space - the position vector rescaled by\nsquare root velocity - is a real valued vector space on which the imposed L2\nmetric is used to define the distance function. With respect to this\nrepresentation the re-parameterization group acts by isometries and the\ndistance has well defined meaning: the sum of the central second moments of the\ncoordinate functions. To identify the optimal re-parameterization system and\nproper centroid we use double energy minimization realized in a variational\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 16:51:30 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Molnar", "Jozsef", ""], ["Barbier", "Michael", ""], ["De Vos", "Winnok H.", ""], ["Horvath", "Peter", ""]]}, {"id": "1901.03048", "submitter": "Theo Lacombe", "authors": "Vincent Divol (DATASHAPE), Th\\'eo Lacombe (DATASHAPE)", "title": "Understanding the Topology and the Geometry of the Space of Persistence\n  Diagrams via Optimal Partial Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the obvious similarities between the metrics used in topological data\nanalysis and those of optimal transport, an optimal-transport based formalism\nto study persistence diagrams and similar topological descriptors has yet to\ncome. In this article, by considering the space of persistence diagrams as a\nspace of discrete measures, and by observing that its metrics can be expressed\nas optimal partial transport problems, we introduce a generalization of\npersistence diagrams, namely Radon measures supported on the upper half plane.\nSuch measures naturally appear in topological data analysis when considering\ncontinuous representations of persistence diagrams (e.g.\\ persistence surfaces)\nbut also as limits for laws of large numbers on persistence diagrams or as\nexpectations of probability distributions on the persistence diagrams space. We\nexplore topological properties of this new space, which will also hold for the\nclosed subspace of persistence diagrams. New results include a characterization\nof convergence with respect to Wasserstein metrics, a geometric description of\nbarycenters (Fr\\'echet means) for any distribution of diagrams, and an\nexhaustive description of continuous linear representations of persistence\ndiagrams. We also showcase the strength of this framework to study random\npersistence diagrams by providing several statistical results made meaningful\nthanks to this new formalism.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 07:54:44 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 13:10:27 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 15:12:09 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Divol", "Vincent", "", "DATASHAPE"], ["Lacombe", "Th\u00e9o", "", "DATASHAPE"]]}, {"id": "1901.03319", "submitter": "Philip Smith", "authors": "Vitaliy Kurlin, Philip Smith", "title": "Skeletonisation Algorithms with Theoretical Guarantees for Unorganised\n  Point Clouds with High Levels of Noise", "comments": "This paper has been published in the journal Pattern Recognition", "journal-ref": null, "doi": "10.1016/j.patcog.2021.107902", "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Science aims to extract meaningful knowledge from unorganised data. Real\ndatasets usually come in the form of a cloud of points with only pairwise\ndistances. Numerous applications require to visualise an overall shape of a\nnoisy cloud of points sampled from a non-linear object that is more complicated\nthan a union of disjoint clusters. The skeletonisation problem in its hardest\nform is to find a 1-dimensional skeleton that correctly represents a shape of\nthe cloud. This paper compares several algorithms that solve the above\nskeletonisation problem for any point cloud and guarantee a successful\nreconstruction. For example, given a highly noisy point sample of an unknown\nunderlying graph, a reconstructed skeleton should be geometrically close and\nhomotopy equivalent to (has the same number of independent cycles as) the\nunderlying graph. One of these algorithm produces a Homologically Persistent\nSkeleton (HoPeS) for any cloud without extra parameters. This universal\nskeleton contains sub-graphs that provably represent the 1-dimensional shape of\nthe cloud at any scale. Other subgraphs of HoPeS reconstruct an unknown graph\nfrom its noisy point sample with a correct homotopy type and within a small\noffset of the sample. The extensive experiments on synthetic and real data\nreveal for the first time the maximum level of noise that allows successful\ngraph reconstructions.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 18:52:02 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 16:16:11 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2021 13:34:40 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Kurlin", "Vitaliy", ""], ["Smith", "Philip", ""]]}, {"id": "1901.04738", "submitter": "Loic Crombez", "authors": "Lo\\\"ic Crombez, Guilherme D. da Fonseca, Yan G\\'erard", "title": "Efficient Algorithms to Test Digital Convexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A set $S \\subset \\mathbb{Z}^d$ is digital convex if $conv(S) \\cap\n\\mathbb{Z}^d = S$, where $conv(S)$ denotes the convex hull of $S$. In this\npaper, we consider the algorithmic problem of testing whether a given set $S$\nof $n$ lattice points is digital convex. Although convex hull computation\nrequires $\\Omega(n \\log n)$ time even for dimension $d = 2$, we provide an\nalgorithm for testing the digital convexity of $S\\subset \\mathbb{Z}^2$ in $O(n\n+ h \\log r)$ time, where $h$ is the number of edges of the convex hull and $r$\nis the diameter of $S$. This main result is obtained by proving that if $S$ is\ndigital convex, then the well-known quickhull algorithm computes the convex\nhull of $S$ in linear time. In fixed dimension $d$, we present the first\npolynomial algorithm to test digital convexity, as well as a simpler and more\npractical algorithm whose running time may not be polynomial in $n$ for certain\ninputs.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 10:02:33 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Crombez", "Lo\u00efc", ""], ["da Fonseca", "Guilherme D.", ""], ["G\u00e9rard", "Yan", ""]]}, {"id": "1901.04944", "submitter": "Jean-Emmanuel Deschaud", "authors": "Hassan Bouchiba, Simon Santoso, Jean-Emmanuel Deschaud, Luisa\n  Rocha-Da-Silva, Fran\\c{c}ois Goulette, Thierry Coupez", "title": "Computational Fluid Dynamics on 3D Point Set Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CE cs.CG physics.comp-ph physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational fluid dynamics (CFD) in many cases requires designing 3D models\nmanually, which is a tedious task that requires specific skills. In this paper,\nwe present a novel method for performing CFD directly on scanned 3D point\nclouds. The proposed method builds an anisotropic volumetric tetrahedral mesh\nadapted around a point-sampled surface, without an explicit surface\nreconstruction step. The surface is represented by a new extended implicit\nmoving least squares (EIMLS) scalar representation that extends the definition\nof the function to the entire computational domain, which makes it possible for\nuse in immersed boundary flow simulations. The workflow we present allows us to\ncompute flows around point-sampled geometries automatically. It also gives a\nbetter control of the precision around the surface with a limited number of\ncomputational nodes, which is a critical issue in CFD.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 13:14:19 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Bouchiba", "Hassan", ""], ["Santoso", "Simon", ""], ["Deschaud", "Jean-Emmanuel", ""], ["Rocha-Da-Silva", "Luisa", ""], ["Goulette", "Fran\u00e7ois", ""], ["Coupez", "Thierry", ""]]}, {"id": "1901.05726", "submitter": "Hanan ElNaghy", "authors": "Hanan ElNaghy and Leo Dorst", "title": "Complementarity-Preserving Fracture Morphology for Archaeological\n  Fragments", "comments": null, "journal-ref": "International Symposium on Mathematical Morphology 2019 (ISMM'19)", "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to employ scale spaces of mathematical morphology to\nhierarchically simplify fracture surfaces of complementarity fitting\narchaeological fragments. This representation preserves complementarity and is\ninsensitive to different kinds of abrasion affecting the exact fitting of the\noriginal fragments. We present a pipeline for morphologically simplifying\nfracture surfaces, based on their Lipschitz nature; its core is a new embedding\nof fracture surfaces to simultaneously compute both closing and opening\nmorphological operations, using distance transforms.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 10:54:51 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 10:45:48 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["ElNaghy", "Hanan", ""], ["Dorst", "Leo", ""]]}, {"id": "1901.06551", "submitter": "Gil Shamai", "authors": "Gil Shamai, Ron Slossberg, Ron Kimmel", "title": "Synthesizing facial photometries and corresponding geometries using\n  generative adversarial networks", "comments": "23 pages, 16 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial data synthesis is currently a well studied topic with useful\napplications in data science, computer vision, graphics and many other fields.\nGenerating realistic data is especially challenging since human perception is\nhighly sensitive to non realistic appearance. In recent times, new levels of\nrealism have been achieved by advances in GAN training procedures and\narchitectures. These successful models, however, are tuned mostly for use with\nregularly sampled data such as images, audio and video. Despite the successful\napplication of the architecture on these types of media, applying the same\ntools to geometric data poses a far greater challenge. The study of geometric\ndeep learning is still a debated issue within the academic community as the\nlack of intrinsic parametrization inherent to geometric objects prohibits the\ndirect use of convolutional filters, a main building block of today's machine\nlearning systems. In this paper we propose a new method for generating\nrealistic human facial geometries coupled with overlayed textures. We\ncircumvent the parametrization issue by imposing a global mapping from our data\nto the unit rectangle. We further discuss how to design such a mapping to\ncontrol the mapping distortion and conserve area within the mapped image. By\nrepresenting geometric textures and geometries as images, we are able to use\nadvanced GAN methodologies to generate new geometries. We address the often\nneglected topic of relation between texture and geometry and propose to use\nthis correlation to match between generated textures and their corresponding\ngeometries. We offer a new method for training GAN models on partially\ncorrupted data. Finally, we provide empirical evidence demonstrating our\ngenerative model's ability to produce examples of new identities independent\nfrom the training data while maintaining a high level of realism, two traits\nthat are often at odds.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 16:36:49 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Shamai", "Gil", ""], ["Slossberg", "Ron", ""], ["Kimmel", "Ron", ""]]}, {"id": "1901.06885", "submitter": "Georg Muntingh PhD", "authors": "Tom Lyche and Georg Muntingh", "title": "B-spline-like bases for $C^2$ cubics on the Powell-Sabin 12-split", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For spaces of constant, linear, and quadratic splines of maximal smoothness\non the Powell-Sabin 12-split of a triangle, the so-called S-bases were recently\nintroduced. These are simplex spline bases with B-spline-like properties on the\n12-split of a single triangle, which are tied together across triangles in a\nB\\'ezier-like manner.\n  In this paper we give a formal definition of an S-basis in terms of certain\nbasic properties. We proceed to investigate the existence of S-bases for the\naforementioned spaces and additionally the cubic case, resulting in an\nexhaustive list. From their nature as simplex splines, we derive simple\ndifferentiation and recurrence formulas to other S-bases. We establish a\nMarsden identity that gives rise to various quasi-interpolants and domain\npoints forming an intuitive control net, in terms of which conditions for\n$C^0$-, $C^1$-, and $C^2$-smoothness are derived.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 11:33:25 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 16:53:02 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Lyche", "Tom", ""], ["Muntingh", "Georg", ""]]}, {"id": "1901.08219", "submitter": "Hu Ding", "authors": "Hu Ding, Haikuo Yu, Zixiu Wang", "title": "Greedy Strategy Works for $k$-Center Clustering with Outliers and\n  Coreset Construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of $k$-center clustering with outliers in arbitrary\nmetrics and Euclidean space. Though a number of methods have been developed in\nthe past decades, it is still quite challenging to design quality guaranteed\nalgorithm with low complexity for this problem. Our idea is inspired by the\ngreedy method, Gonzalez's algorithm, for solving the problem of ordinary\n$k$-center clustering. Based on some novel observations, we show that this\ngreedy strategy actually can handle $k$-center clustering with outliers\nefficiently, in terms of clustering quality and time complexity. We further\nshow that the greedy approach yields small coreset for the problem in doubling\nmetrics, so as to reduce the time complexity significantly. Our algorithms are\neasy to implement in practice. We test our method on both synthetic and real\ndatasets. The experimental results suggest that our algorithms can achieve near\noptimal solutions and yield lower running times comparing with existing\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 03:45:54 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2019 11:00:29 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Ding", "Hu", ""], ["Yu", "Haikuo", ""], ["Wang", "Zixiu", ""]]}, {"id": "1901.08419", "submitter": "Michal Mackiewicz", "authors": "Michal Mackiewicz, Hans Jakob Rivertz, Graham D. Finlayson", "title": "Spherical sampling methods for the calculation of metamer mismatch\n  volumes", "comments": "One print or electronic copy may be made for personal use only.\n  Systematic reproduction and distribution, duplication of any material in this\n  paper for a fee or for commercial purposes, or modifications of this paper\n  are prohibited. Optical Society of America", "journal-ref": "Vol. 36, No. 1 / Jan 2019 / Journal of the Optical Society of\n  America A", "doi": "10.1364/JOSAA.36.000096", "report-no": null, "categories": "cs.CG cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose two methods of calculating theoretically maximal\nmetamer mismatch volumes. Unlike prior art techniques, our methods do not make\nany assumptions on the shape of spectra on the boundary of the mismatch\nvolumes. Both methods utilize a spherical sampling approach, but they calculate\nmismatch volumes in two different ways. The first method uses a linear\nprogramming optimization, while the second is a computational geometry approach\nbased on half-space intersection. We show that under certain conditions the\ntheoretically maximal metamer mismatch volume is significantly larger than the\none approximated using a prior art method.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 18:33:05 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Mackiewicz", "Michal", ""], ["Rivertz", "Hans Jakob", ""], ["Finlayson", "Graham D.", ""]]}, {"id": "1901.08544", "submitter": "Tal Wagner", "authors": "Yihe Dong and Piotr Indyk and Ilya Razenshteyn and Tal Wagner", "title": "Learning Space Partitions for Nearest Neighbor Search", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Space partitions of $\\mathbb{R}^d$ underlie a vast and important class of\nfast nearest neighbor search (NNS) algorithms. Inspired by recent theoretical\nwork on NNS for general metric spaces [Andoni, Naor, Nikolov, Razenshteyn,\nWaingarten STOC 2018, FOCS 2018], we develop a new framework for building space\npartitions reducing the problem to balanced graph partitioning followed by\nsupervised classification. We instantiate this general approach with the KaHIP\ngraph partitioner [Sanders, Schulz SEA 2013] and neural networks, respectively,\nto obtain a new partitioning procedure called Neural Locality-Sensitive Hashing\n(Neural LSH). On several standard benchmarks for NNS, our experiments show that\nthe partitions obtained by Neural LSH consistently outperform partitions found\nby quantization-based and tree-based methods as well as classic, data-oblivious\nLSH.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 18:07:59 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 04:48:38 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 19:22:44 GMT"}, {"version": "v4", "created": "Tue, 29 Sep 2020 02:50:54 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Dong", "Yihe", ""], ["Indyk", "Piotr", ""], ["Razenshteyn", "Ilya", ""], ["Wagner", "Tal", ""]]}, {"id": "1901.08564", "submitter": "Hugo Akitaya", "authors": "Hugo A. Akitaya, Cordelia Avery, Joseph Bergeron, Erik D. Demaine,\n  Justin Kopinsky, Jason Ku", "title": "Infinite All-Layers Simple Foldability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of deciding whether a crease pattern can be folded by\nsimple folds (folding along one line at a time) under the infinite all-layers\nmodel introduced by [Akitaya et al., 2017], in which each simple fold is\ndefined by an infinite line and must fold all layers of paper that intersect\nthis line. This model is motivated by folding in manufacturing such as\nsheet-metal bending. We improve on [Arkin et al., 2004] by giving a\ndeterministic $O(n)$-time algorithm to decide simple foldability of 1D crease\npatterns in the all-layers model. Then we extend this 1D result to 2D, showing\nthat simple foldability in this model can be decided in linear time for\nunassigned axis-aligned orthogonal crease patterns on axis-aligned 2D\northogonal paper. On the other hand, we show that simple foldability is\nstrongly NP-complete if a subset of the creases have a mountain-valley\nassignment, even for an axis-aligned rectangle of paper.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 18:31:28 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Akitaya", "Hugo A.", ""], ["Avery", "Cordelia", ""], ["Bergeron", "Joseph", ""], ["Demaine", "Erik D.", ""], ["Kopinsky", "Justin", ""], ["Ku", "Jason", ""]]}, {"id": "1901.08575", "submitter": "J\\'er\\^ome Durand-Lose", "authors": "J\\'er\\^ome Durand-Lose, Hendrik Jan Hoogeboom, Nata\\v{s}a Jonoska", "title": "Deterministic 2-Dimensional Temperature-1 Tile Assembly Systems Cannot\n  Compute", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider non cooperative binding in so called `temperature 1', in\ndeterministic (here called {\\it confluent}) tile self-assembly systems (1-TAS)\nand prove the standing conjecture that such systems do not have universal\ncomputational power. We call a TAS whose maximal assemblies contain at least\none ultimately periodic assembly path {\\it para-periodic}. We observe that a\nconfluent 1-TAS has at most one maximal producible assembly, $\\alpha_{max}$,\nthat can be considered a union of path assemblies, and we show that such a\nsystem is always para-periodic. This result is obtained through a superposition\nand a combination of two paths that produce a new path with desired properties,\na technique that we call \\emph{co-grow} of two paths. Moreover we provide a\ncharacterization of an $\\alpha_{max}$ of a confluent 1-TAS as one of two\npossible cases, so called, a grid or a disjoint union of combs. To a given\n$\\alpha_{max}$ we can associate a finite labeled graph, called \\emph{quipu},\nsuch that the union of all labels of paths in the quipu equals $\\alpha_{max}$,\ntherefore giving a finite description for $\\alpha_{max}$. This finite\ndescription implies that $\\alpha_{max}$ is a union of semi-affine subsets of\n$\\mathbb{Z}^2$ and since such a finite description can be algorithmicly\ngenerated from any 1-TAS, 1-TAS cannot have universal computational power.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 18:47:33 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Durand-Lose", "J\u00e9r\u00f4me", ""], ["Hoogeboom", "Hendrik Jan", ""], ["Jonoska", "Nata\u0161a", ""]]}, {"id": "1901.08805", "submitter": "Arnur Nigmetov", "authors": "Michael Kerber and Arnur Nigmetov", "title": "Metric Spaces with Expensive Distances", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In algorithms for finite metric spaces, it is common to assume that the\ndistance between two points can be computed in constant time, and complexity\nbounds are expressed only in terms of the number of points of the metric space.\nWe introduce a different model where we assume that the computation of a single\ndistance is an expensive operation and consequently, the goal is to minimize\nthe number of such distance queries. This model is motivated by metric spaces\nthat appear in the context of topological data analysis.\n  We consider two standard operations on metric spaces, namely the construction\nof a $1+\\varepsilon$-spanner and the computation of an approximate nearest\nneighbor for a given query point. In both cases, we partially explore the\nmetric space through distance queries and infer lower and upper bounds for yet\nunexplored distances through triangle inequality. For spanners, we evaluate\nseveral exploration strategies through extensive experimental evaluation. For\napproximate nearest neighbors, we prove that our strategy returns an\napproximate nearest neighbor after a logarithmic number of distance queries.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 10:01:27 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Kerber", "Michael", ""], ["Nigmetov", "Arnur", ""]]}, {"id": "1901.09234", "submitter": "Josue Tonelli-Cueto", "authors": "Felipe Cucker, Alperen A. Erg\\\"ur, Josue Tonelli-Cueto", "title": "Plantinga-Vegter algorithm takes average polynomial time", "comments": "8 pages, correction of typos", "journal-ref": "ACM Symposium on Symbolic and Algebraic Computation (ISSAC), 2019", "doi": "10.1145/3326229.3326252", "report-no": null, "categories": "cs.CG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exhibit a condition-based analysis of the adaptive subdivision algorithm\ndue to Plantinga and Vegter. The first complexity analysis of the PV Algorithm\nis due to Burr, Gao and Tsigaridas who proved a $O\\big(2^{\\tau d^{4}\\log\nd}\\big)$ worst-case cost bound for degree $d$ plane curves with maximum\ncoefficient bit-size $\\tau$. This exponential bound, it was observed, is in\nstark contrast with the good performance of the algorithm in practice. More in\nline with this performance, we show that, with respect to a broad family of\nmeasures, the expected time complexity of the PV Algorithm is bounded by\n$O(d^7)$ for real, degree $d$, plane curves. We also exhibit a smoothed\nanalysis of the PV Algorithm that yields similar complexity estimates. To\nobtain these results we combine robust probabilistic techniques coming from\ngeometric functional analysis with condition numbers and the continuous\namortization paradigm introduced by Burr, Krahmer and Yap. We hope this will\nmotivate a fruitful exchange of ideas between the different approaches to\nnumerical computation.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 15:38:32 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 18:25:46 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Cucker", "Felipe", ""], ["Erg\u00fcr", "Alperen A.", ""], ["Tonelli-Cueto", "Josue", ""]]}, {"id": "1901.10759", "submitter": "Fehmi Cirak", "authors": "Qiaoling Zhang, Thomas Takacs and Fehmi Cirak", "title": "Manifold-based B-splines on unstructured meshes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new manifold-based splines that are able to exactly reproduce\nB-splines on unstructured surface meshes. Such splines can be used in\nisogeometric analysis (IGA) to represent smooth surfaces of arbitrary topology.\nSince prevalent computer-aided design (CAD) models are composed of\ntensor-product B-spline patches, any IGA suitable construction should be able\nto reproduce B-splines. To achieve this goal, we focus on univariate\nmanifold-based constructions that can reproduce B-splines. The manifold-based\nsplines are constructed by smoothly blending together polynomial interpolants\ndefined on overlapping charts. The proposed constructions automatically\nreproduce B-splines in regular parts of the mesh, with no extraordinary\nvertices, and polynomial basis functions in the remaining parts of the mesh. We\nstudy and compare analytically and numerically the finite element convergence\nof several univariate constructions. The obtained results directly carry over\nto the tensor-product case.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 10:43:36 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Zhang", "Qiaoling", ""], ["Takacs", "Thomas", ""], ["Cirak", "Fehmi", ""]]}, {"id": "1901.10888", "submitter": "Yoel Shkolnisky", "authors": "Gabi Pragier and Yoel Shkolnisky", "title": "A common lines approach for ab-initio modeling of cyclically-symmetric\n  molecules", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/ab2fb2", "report-no": null, "categories": "cs.CG cs.NA math.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges in single particle reconstruction in cryo-electron\nmicroscopy is to find a three-dimensional model of a molecule using its\ntwo-dimensional noisy projection-images. In this paper, we propose a robust\n\"angular reconstitution\" algorithm for molecules with $n$-fold cyclic symmetry,\nthat estimates the orientation parameters of the projections-images. Our\nsuggested method utilizes self common lines which induce identical lines within\nthe Fourier transform of each projection-image. We show that the location of\nself common lines admits quite a few favorable geometrical constraints, thus\nallowing to detect them even in a noisy setting. In addition, for molecules\nwith higher order rotational symmetry, our proposed method exploits the fact\nthat there exist numerous common lines between any two Fourier transformed\nprojection-images of such molecules, thus allowing to determine their relative\norientation even under high levels of noise. The efficacy of our proposed\nmethod is demonstrated using numerical experiments conducted on simulated and\nexperimental data.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2019 21:05:19 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 18:59:27 GMT"}, {"version": "v3", "created": "Mon, 24 Jun 2019 08:30:10 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Pragier", "Gabi", ""], ["Shkolnisky", "Yoel", ""]]}, {"id": "1901.11082", "submitter": "Chiyu Jiang", "authors": "Chiyu \"Max\" Jiang, Dana Lynn Ona Lansigan, Philip Marcus, Matthias\n  Nie{\\ss}ner", "title": "DDSL: Deep Differentiable Simplex Layer for Learning Geometric Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Deep Differentiable Simplex Layer (DDSL) for neural networks for\ngeometric deep learning. The DDSL is a differentiable layer compatible with\ndeep neural networks for bridging simplex mesh-based geometry representations\n(point clouds, line mesh, triangular mesh, tetrahedral mesh) with raster images\n(e.g., 2D/3D grids). The DDSL uses Non-Uniform Fourier Transform (NUFT) to\nperform differentiable, efficient, anti-aliased rasterization of simplex-based\nsignals. We present a complete theoretical framework for the process as well as\nan efficient backpropagation algorithm. Compared to previous differentiable\nrenderers and rasterizers, the DDSL generalizes to arbitrary simplex degrees\nand dimensions. In particular, we explore its applications to 2D shapes and\nillustrate two applications of this method: (1) mesh editing and optimization\nguided by neural network outputs, and (2) using DDSL for a differentiable\nrasterization loss to facilitate end-to-end training of polygon generators. We\nare able to validate the effectiveness of gradient-based shape optimization\nwith the example of airfoil optimization, and using the differentiable\nrasterization loss to facilitate end-to-end training, we surpass state of the\nart for polygonal image segmentation given ground-truth bounding boxes.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 20:17:50 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 23:17:43 GMT"}, {"version": "v3", "created": "Wed, 14 Aug 2019 22:28:26 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Jiang", "Chiyu \"Max\"", ""], ["Lansigan", "Dana Lynn Ona", ""], ["Marcus", "Philip", ""], ["Nie\u00dfner", "Matthias", ""]]}]