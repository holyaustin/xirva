[{"id": "1605.00313", "submitter": "Konstantin Kobylkin S.", "authors": "Konstantin Kobylkin", "title": "Stabbing line segments with disks: complexity and approximation\n  algorithms", "comments": "12 pages, 1 appendix, 15 bibliography items, 6th International\n  Conference on Analysis of Images, Social Networks and Texts (AIST-2017)", "journal-ref": "Kobylkin K.Stabbing Line Segments with Disks: Complexity and\n  Approximation Algorithms. // Lecture Notes in Computer Science, 2018. vol\n  10716. pp 356-367 Springer", "doi": "10.1007/978-3-319-73013-4_33", "report-no": "Eng21", "categories": "cs.CG cs.CC cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computational complexity and approximation algorithms are reported for a\nproblem of stabbing a set of straight line segments with the least cardinality\nset of disks of fixed radii $r>0$ where the set of segments forms a straight\nline drawing $G=(V,E)$ of a planar graph without edge crossings. Close\ngeometric problems arise in network security applications. We give strong\nNP-hardness of the problem for edge sets of Delaunay triangulations, Gabriel\ngraphs and other subgraphs (which are often used in network design) for $r\\in\n[d_{\\min},\\eta d_{\\max}]$ and some constant $\\eta$ where $d_{\\max}$ and\n$d_{\\min}$ are Euclidean lengths of the longest and shortest graph edges\nrespectively. Fast $O(|E|\\log|E|)$-time $O(1)$-approximation algorithm is\nproposed within the class of straight line drawings of planar graphs for which\nthe inequality $r\\geq \\eta d_{\\max}$ holds uniformly for some constant\n$\\eta>0,$ i.e. when lengths of edges of $G$ are uniformly bounded from above by\nsome linear function of $r.$\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2016 21:54:15 GMT"}, {"version": "v2", "created": "Wed, 4 May 2016 14:06:50 GMT"}, {"version": "v3", "created": "Tue, 26 Jul 2016 09:32:56 GMT"}, {"version": "v4", "created": "Thu, 20 Jul 2017 08:56:24 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Kobylkin", "Konstantin", ""]]}, {"id": "1605.00967", "submitter": "Olivier Guye", "authors": "Olivier Guye", "title": "Hierarchical Modeling of Multidimensional Data in Regularly Decomposed\n  Spaces: Implementation on Computer", "comments": "214 pages, 22 figures, research report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The described works have been carried out in the framework of a mid-term\nstudy initiated by the Centre Electronique de l'Armement, then by an advanced\nstudy launched by the Direction de la Recherche et des Etudes Technologiques in\nFrance in the aim to develop new techniques for multidimensional hierarchical\nmodeling and to port them on parallel architecture computers for satisfying the\nfuture needs in processing huge numerical data bases. Following the first tome\ndescribing the modeling principles, the second tome details the way used for\ndeveloping the modeling software and for porting it on different computers,\nespecially on parallel architecture computers. In addition to these works, it\nis gone through new algorithms that have been developed after those that have\nbeen presented in the former tome and that are described in pseudo-code in\nannex of the present document: - operators for constructive geometry (building\nsimple shapes, Boolean operators, slice handling); - integral transformations\n(epigraph, hypograph, convex hull) ; - homotopic transformations (boundary,\nerosion, dilation, opening, closing) ; - median transformations (median\nfiltering, thinning, median set, intrinsic dimension) ; - transformations of\n(hyper-)surface manifolds (median filtering, extension, polynomial fitting of a\nsimple function). The present publication is ending with the software porting\non two distributed memory parallel computers: - a thin-grained synchronous\ncomputer ; - a coarse-grained asynchronous computer.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 16:15:57 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Guye", "Olivier", ""]]}, {"id": "1605.01011", "submitter": "Jisu Kim", "authors": "Jisu Kim, Alessandro Rinaldo, Larry Wasserman", "title": "Minimax Rates for Estimating the Dimension of a Manifold", "comments": "54 pages, 11 figures, to be published in Journal of Computational\n  Geometry, Volume 10, Number 1", "journal-ref": null, "doi": "10.20382/jocg.v10i1a3", "report-no": null, "categories": "math.ST cs.CG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many algorithms in machine learning and computational geometry require, as\ninput, the intrinsic dimension of the manifold that supports the probability\ndistribution of the data. This parameter is rarely known and therefore has to\nbe estimated. We characterize the statistical difficulty of this problem by\nderiving upper and lower bounds on the minimax rate for estimating the\ndimension. First, we consider the problem of testing the hypothesis that the\nsupport of the data-generating probability distribution is a well-behaved\nmanifold of intrinsic dimension $d_1$ versus the alternative that it is of\ndimension $d_2$, with $d_{1}<d_{2}$. With an i.i.d. sample of size $n$, we\nprovide an upper bound on the probability of choosing the wrong dimension of\n$O\\left( n^{-\\left(d_{2}/d_{1}-1-\\epsilon\\right)n} \\right)$, where $\\epsilon$\nis an arbitrarily small positive number. The proof is based on bounding the\nlength of the traveling salesman path through the data points. We also\ndemonstrate a lower bound of $\\Omega \\left( n^{-(2d_{2}-2d_{1}+\\epsilon)n}\n\\right)$, by applying Le Cam's lemma with a specific set of $d_{1}$-dimensional\nprobability distributions. We then extend these results to get minimax rates\nfor estimating the dimension of well-behaved manifolds. We obtain an upper\nbound of order $O \\left( n^{-(\\frac{1}{m-1}-\\epsilon)n} \\right)$ and a lower\nbound of order $\\Omega \\left( n^{-(2+\\epsilon)n} \\right)$, where $m$ is the\nembedding dimension.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 18:21:37 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 16:16:14 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2019 01:17:41 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Kim", "Jisu", ""], ["Rinaldo", "Alessandro", ""], ["Wasserman", "Larry", ""]]}, {"id": "1605.02245", "submitter": "Claudio Paglia", "authors": "Claudio Paglia", "title": "Real-time collision detection method for deformable bodies", "comments": "Computer-Aided Design, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a real-time solution for collision detection between\nobjects based on the physics properties. Traditional approaches on collision\ndetection often rely on the geometric relationships that computing the\nintersections between polygons. Such technique is very computationally\nexpensive when applied for deformable objects. As an alternative, we\napproximate the 3D mesh in an spherical surface implicitly. This allows us to\nperform a coarse-level collision detection at extremely fast speed. Then a\ndynamic programming based procedure is applied to identify the collision in\nfine details. Our method demonstrates better prevention to collision tunnelling\nand works more efficiently than the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2016 20:41:58 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Paglia", "Claudio", ""]]}, {"id": "1605.02499", "submitter": "Minati De", "authors": "Minati De and Abhiruk Lahiri", "title": "Geometric Dominating Set and Set Cover via Local Search", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study two classic optimization problems: minimum geometric\ndominating set and set cover. Both the problems have been studied for different\ntypes of objects for a long time. These problems become APX-hard when the\nobjects are axis-parallel rectangles, ellipses, $\\alpha$-fat objects of\nconstant description complexity, and convex polygons. On the other hand, PTAS\n(polynomial time approximation scheme) is known for them when the objects are\ndisks or unit squares. Surprisingly, PTAS was unknown even for arbitrary\nsquares. For homothetic set of convex objects, an $O(k^4)$ approximation\nalgorithm is known for dominating set problem, where $k$ is the number of\ncorners in a convex object. On the other hand, QPTAS (quasi polynomial time\napproximation scheme) is known very recently for the covering problem when the\nobjects are pseudodisks. For both problems obtaining a PTAS remains open for a\nlarge class of objects.\n  For the dominating set problems, we prove that the popular local search\nalgorithm leads to an $(1+\\varepsilon)$ approximation when objects are\nhomothetic set of convex objects (which includes arbitrary squares, $k$-regular\npolygons, translated and scaled copies of a convex set etc.) in\n$n^{O(1/\\varepsilon^2)}$ time. On the other hand, the same technique leads to a\nPTAS for geometric covering problem when the objects are convex pseudodisks\n(which includes disks, unit height rectangles, homothetic convex objects etc.).\nAs a consequence, we obtain an easy to implement approximation algorithm for\nboth problems for a large class of objects, significantly improving the best\nknown approximation guarantees.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 09:55:33 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["De", "Minati", ""], ["Lahiri", "Abhiruk", ""]]}, {"id": "1605.02626", "submitter": "Maxence Reberol", "authors": "Maxence Reberol (ALICE), Bruno L\\'evy (ALICE)", "title": "Low-order continuous finite element spaces on hybrid non-conforming\n  hexahedral-tetrahedral meshes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with solving partial differential equations with the\nfinite element method on hybrid non-conforming hexahedral-tetrahedral meshes.\nBy non-conforming, we mean that a quadrangular face of a hexahedron can be\nconnected to two triangular faces of tetrahedra. We introduce a set of\nlow-order continuous (C0) finite element spaces defined on these meshes. They\nare built from standard tri-linear and quadratic Lagrange finite elements with\nan extra set of constraints at non-conforming hexahedra-tetrahedra junctions to\nrecover continuity. We consider both the continuity of the geometry and the\ncontinuity of the function basis as follows: the continuity of the geometry is\nachieved by using quadratic mappings for tetrahedra connected to tri-affine\nhexahedra and the continuity of interpolating functions is enforced in a\nsimilar manner by using quadratic Lagrange basis on tetrahedra with constraints\nat non-conforming junctions to match tri-linear hexahedra. The so-defined\nfunction spaces are validated numerically on simple Poisson and linear\nelasticity problems for which an analytical solution is known. We observe that\nusing a hybrid mesh with the proposed function spaces results in an accuracy\nsignificantly better than when using linear tetrahedra and slightly worse than\nwhen solely using tri-linear hexahedra. As a consequence, the proposed function\nspaces may be a promising alternative for complex geometries that are out of\nreach of existing full hexahedral meshing methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 15:42:30 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Reberol", "Maxence", "", "ALICE"], ["L\u00e9vy", "Bruno", "", "ALICE"]]}, {"id": "1605.02701", "submitter": "Ilya Razenshteyn", "authors": "Alexandr Andoni, Thijs Laarhoven, Ilya Razenshteyn, Erik Waingarten", "title": "Lower Bounds on Time-Space Trade-Offs for Approximate Near Neighbors", "comments": "47 pages, 2 figures; v2: substantially revised introduction, lots of\n  small corrections; subsumed by arXiv:1608.03580 [cs.DS] (along with\n  arXiv:1511.07527 [cs.DS])", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show tight lower bounds for the entire trade-off between space and query\ntime for the Approximate Near Neighbor search problem. Our lower bounds hold in\na restricted model of computation, which captures all hashing-based approaches.\nIn articular, our lower bound matches the upper bound recently shown in\n[Laarhoven 2015] for the random instance on a Euclidean sphere (which we show\nin fact extends to the entire space $\\mathbb{R}^d$ using the techniques from\n[Andoni, Razenshteyn 2015]).\n  We also show tight, unconditional cell-probe lower bounds for one and two\nprobes, improving upon the best known bounds from [Panigrahy, Talwar, Wieder\n2010]. In particular, this is the first space lower bound (for any static data\nstructure) for two probes which is not polynomially smaller than for one probe.\nTo show the result for two probes, we establish and exploit a connection to\nlocally-decodable codes.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 19:13:06 GMT"}, {"version": "v2", "created": "Sat, 25 Jun 2016 22:23:46 GMT"}, {"version": "v3", "created": "Thu, 18 Aug 2016 22:03:33 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Andoni", "Alexandr", ""], ["Laarhoven", "Thijs", ""], ["Razenshteyn", "Ilya", ""], ["Waingarten", "Erik", ""]]}, {"id": "1605.03271", "submitter": "Yangdi Lyu", "authors": "Yangdi Lyu, Alper \\\"Ung\\\"or", "title": "A Fast 2-Approximation Algorithm for Guarding Orthogonal Terrains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Terrain Guarding Problem(TGP), which is known to be NP-complete, asks to find\na smallest set of guard locations on a terrain $T$ such that every point on $T$\nis visible by a guard. Here, we study this problem on 1.5D orthogonal terrains\nwhere the edges are bound to be horizontal or vertical. We propose a\n2-approximation algorithm that runs in O($n \\log m$) time, where $n$ and $m$\nare the sizes of input and output, respectively. This is an improvement over\nthe previous best algorithm, which is a 2-approximation with O($n^2$) running\ntime.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 03:35:00 GMT"}, {"version": "v2", "created": "Tue, 17 May 2016 23:15:47 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Lyu", "Yangdi", ""], ["\u00dcng\u00f6r", "Alper", ""]]}, {"id": "1605.03514", "submitter": "Mark Bell", "authors": "Mark C. Bell, Richard C. H. Webb", "title": "Applications of fast triangulation simplification", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GT cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new algorithm to compute the geometric intersection number\nbetween two curves, given as edge vectors on an ideal triangulation. Most\nimportantly, this algorithm runs in polynomial time in the bit-size of the two\nedge vectors.\n  In its simplest instances, this algorithm works by finding the minimal\nposition of the two curves. We achieve this by phrasing the problem as a\ncollection of linear programming problems. We describe how to reduce the more\ngeneral case down to one of these simplest instances in polynomial time. This\nreduction relies on an algorithm by the first author to quickly switch to a new\ntriangulation in which an edge vector is significantly smaller.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 16:59:03 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Bell", "Mark C.", ""], ["Webb", "Richard C. H.", ""]]}, {"id": "1605.03542", "submitter": "Sharareh Alipour", "authors": "Sharareh Alipour and Mohammad Ghodsi and Amir Jafari", "title": "An improved Constant-Factor Approximation Algorithm for Planar\n  Visibility Counting Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $S$ of $n$ disjoint line segments in $\\mathbb{R}^{2}$, the\nvisibility counting problem (VCP) is to preprocess $S$ such that the number of\nsegments in $S$ visible from any query point $p$ can be computed quickly. This\nproblem can trivially be solved in logarithmic query time using $O(n^{4})$\npreprocessing time and space. Gudmundsson and Morin proposed a 2-approximation\nalgorithm for this problem with a tradeoff between the space and the query\ntime. They answer any query in $O_{\\epsilon}(n^{1-\\alpha})$ with\n$O_{\\epsilon}(n^{2+2\\alpha})$ of preprocessing time and space, where $\\alpha$\nis a constant $0\\leq \\alpha\\leq 1$, $\\epsilon > 0$ is another constant that can\nbe made arbitrarily small, and $O_{\\epsilon}(f(n))=O(f(n)n^{\\epsilon})$.\n  In this paper, we propose a randomized approximation algorithm for VCP with a\ntradeoff between the space and the query time. We will show that for an\narbitrary constants $0\\leq \\beta\\leq \\frac{2}{3}$ and $0<\\delta <1$, the\nexpected preprocessing time, the expected space, and the query time of our\nalgorithm are $O(n^{4-3\\beta}\\log n)$, $O(n^{4-3\\beta})$, and\n$O(\\frac{1}{\\delta^3}n^{\\beta}\\log n)$, respectively. The algorithm computes\nthe number of visible segments from $p$, or $m_p$, exactly if $m_p\\leq\n\\frac{1}{\\delta^3}n^{\\beta}\\log n$. Otherwise, it computes a\n$(1+\\delta)$-approximation $m'_p$ with the probability of at least\n$1-\\frac{1}{\\log n}$, where $m_p\\leq m'_p\\leq (1+\\delta)m_p$.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 18:40:58 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Alipour", "Sharareh", ""], ["Ghodsi", "Mohammad", ""], ["Jafari", "Amir", ""]]}, {"id": "1605.03753", "submitter": "Maurice Margenstern", "authors": "Maurice Margenstern", "title": "A new system of coordinates for the tilings $\\{p,3\\}$ and\n  $\\{p$$-$$2,4\\}$", "comments": "33 pages, 9 figures; this second version improves one figure by\n  inserting a new picture; it also improves two algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we indicate a new way to define coordinates for the tiles of\nthe tilings $\\{p,3\\}$ and $\\{p$$-$$2,4\\}$ where the natural number $p$\nsatisfies $p\\geq 7$.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 10:47:50 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 15:40:34 GMT"}, {"version": "v3", "created": "Thu, 21 Jul 2016 16:40:34 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Margenstern", "Maurice", ""]]}, {"id": "1605.04156", "submitter": "Mao Shi PhD", "authors": "Mao Shi", "title": "Rational B\\'ezier Curves Approximated by Bernstein-Jacobi Hybrid\n  Polynomial Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a linear method for $C^{(r,s)}$ approximation of\nrational B\\'{e}zier curve with arbitrary degree polynomial curve. Based on\nweighted least-squares, the problem be converted to an approximation between\ntwo polynomial curves. Then applying Bernstein-Jacobi hybrid polynomials, we\nobtain the resulting curve. In order to reduce error, degree reduction method\nfor B\\'{e}zier curve is used. A error bound between rational B\\'{e}zier curve\nand B\\'{e}zier curve is presented. Finally, some examples and figures were\noffered to demonstrate the efficiency, simplicity, and stability of our\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 12:34:01 GMT"}, {"version": "v2", "created": "Sun, 4 Aug 2019 11:39:21 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Shi", "Mao", ""]]}, {"id": "1605.04265", "submitter": "Benjamin Niedermann", "authors": "Benjamin Niedermann, Martin N\\\"ollenburg", "title": "An Algorithmic Framework for Labeling Road Maps", "comments": "extended version of a paper to appear at GIScience 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an unlabeled road map, we consider, from an algorithmic perspective,\nthe cartographic problem to place non-overlapping road labels embedded in their\nroads. We first decompose the road network into logically coherent road\nsections, e.g., parts of roads between two junctions. Based on this\ndecomposition, we present and implement a new and versatile framework for\nplacing labels in road maps such that the number of labeled road sections is\nmaximized. In an experimental evaluation with road maps of 11 major cities we\nshow that our proposed labeling algorithm is both fast in practice and that it\nreaches near-optimal solution quality, where optimal solutions are obtained by\nmixed-integer linear programming. In comparison to the standard OpenStreetMap\nrenderer Mapnik, our algorithm labels 31% more road sections in average.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 17:34:02 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Niedermann", "Benjamin", ""], ["N\u00f6llenburg", "Martin", ""]]}, {"id": "1605.04323", "submitter": "Michael Burr", "authors": "Michael Burr and Robert Fabrizio", "title": "Error Probabilities for Halfspace Depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CG math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data depth functions are a generalization of one-dimensional order statistics\nand medians to real spaces of dimension greater than one; in particular, a data\ndepth function quantifies the centrality of a point with respect to a data set\nor a probability distribution. One of the most commonly studied data depth\nfunctions is halfspace depth. It is of interest to computational geometers\nbecause it is highly geometric, and it is of interest to statisticians because\nit shares many desirable theoretical properties with the one-dimensional\nmedian. As the sample size increases, the halfspace depth for a sample\nconverges to the halfspace depth for the underlying distribution, almost\nsurely. In this paper, we use the geometry of halfspace depth to improve the\nexplicit bounds on the rate of convergence.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 20:23:30 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Burr", "Michael", ""], ["Fabrizio", "Robert", ""]]}, {"id": "1605.04552", "submitter": "Hayato Waki", "authors": "Hayato Waki and Florin Nae", "title": "Boundary modeling in model-based calibration for automotive engines via\n  the vertex representation of the convex hulls", "comments": "We change only the license statement from the first version, MI\n  Preprint Series MI 2016-6", "journal-ref": "Pacific Journal of Mathematics for Industry, 9:1, 2017", "doi": "10.1186/s40736-016-0027-7", "report-no": "MI Preprint Series Mathematics for Industry Kyushu University, MI\n  2016-6", "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When using the convex hull approach in the boundary modeling process,\nModel-Based Calibration (MBC) software suites -- such as Model-Based\nCalibration Toolbox from MathWorks -- can be computationally intensive\ndepending on the amount of data modeled. The reason for this is that the\nhalf-space representation of the convex hull is used. We discuss here another\nrepresentation of the convex hull, the vertex representation, which proves\ncapable to reduce the computational cost. Numerical comparisons in this article\nare executed in MATLAB by using MBC Toolbox commands, and show that for certain\nconditions, the vertex representation outperforms the half-space\nrepresentation.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2016 13:51:30 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 12:16:10 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Waki", "Hayato", ""], ["Nae", "Florin", ""]]}, {"id": "1605.04986", "submitter": "Dennis Wei", "authors": "Dennis Wei", "title": "A Constant-Factor Bi-Criteria Approximation Guarantee for $k$-means++", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the $k$-means++ algorithm for clustering as well as the\nclass of $D^\\ell$ sampling algorithms to which $k$-means++ belongs. It is shown\nthat for any constant factor $\\beta > 1$, selecting $\\beta k$ cluster centers\nby $D^\\ell$ sampling yields a constant-factor approximation to the optimal\nclustering with $k$ centers, in expectation and without conditions on the\ndataset. This result extends the previously known $O(\\log k)$ guarantee for the\ncase $\\beta = 1$ to the constant-factor bi-criteria regime. It also improves\nupon an existing constant-factor bi-criteria result that holds only with\nconstant probability.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 23:41:55 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Wei", "Dennis", ""]]}, {"id": "1605.05141", "submitter": "Arkadiy Skopenkov", "authors": "A. Skopenkov", "title": "A user's guide to topological Tverberg conjecture", "comments": "20 pages, 7 figures, exposition improved, simple proof of the\n  Mabillard-Wagner Theorem added, appendix updated", "journal-ref": "Russian Math. Surveys, 73:2 (2018), 323-353", "doi": "10.1070/RM9774", "report-no": null, "categories": "math.CO cs.CG math.AT math.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The well-known topological Tverberg conjecture was considered a central\nunsolved problem of topological combinatorics. The conjecture asserts that for\neach integers $r,d>1$ and each continuous map $f\\colon\\Delta\\to\\mathbb R^d$ of\nthe $(d+1)(r-1)$-dimensional simplex $\\Delta$ there are pairwise disjoint\nsubsimplices $\\sigma_1,\\ldots,\\sigma_r\\subset\\Delta$ such that $f(\\sigma_1)\\cap\n\\ldots \\cap f(\\sigma_r)\\ne\\emptyset$.\n  A proof for a prime power $r$ was given by I. Barany, S. Shlosman, A. Szucs,\nM. Ozaydin and A. Volovikov in 1981-1996. A counterexample for other $r$ was\nfound in a series of papers by M. Ozaydin, M. Gromov, P. Blagojevic, F. Frick,\nG. Ziegler, I. Mabillard and U. Wagner, most of them recent. The arguments form\na beautiful and fruitful interplay between combinatorics, algebra and topology.\nIn this expository paper we present a simplified explanation of easier parts of\nthe arguments, accessible to non-specialists in the area, and give reference to\nmore complicated parts.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 12:46:34 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 15:23:34 GMT"}, {"version": "v3", "created": "Tue, 27 Sep 2016 09:57:15 GMT"}, {"version": "v4", "created": "Mon, 20 Feb 2017 07:08:53 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Skopenkov", "A.", ""]]}, {"id": "1605.05546", "submitter": "Bodhayan Roy", "authors": "Ajit Arvind Diwan and Bodhayan Roy", "title": "Partitions of planar point sets into polygons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we characterize planar point sets that can be partitioned into\ndisjoint polygons of arbitrarily specified sizes. We provide an algorithm to\nconstruct such a partition, if it exists, in polynomial time. We show that this\nproblem is equivalent to finding a specified $2$-factor in the visibility graph\nof the point set. The characterization for the case where all cycles have\nlength $3$ also translates to finding a $K_3$-factor of the visibility graph of\nthe point set. We show that the generalized problem of finding a $K_k$-factor\nof the visibility graph of a given point set for $k \\geq 5$ is NP-hard.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 12:21:39 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Diwan", "Ajit Arvind", ""], ["Roy", "Bodhayan", ""]]}, {"id": "1605.05629", "submitter": "Michael Deakin", "authors": "Michael Deakin, Jack Snoeyink", "title": "On the Precision to Sort Line-Quadric Intersections", "comments": "CCCG 2016 submission, 6 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To support exactly tracking a neutron moving along a given line segment\nthrough a CAD model with quadric surfaces, this paper considers the arithmetic\nprecision required to compute the order of intersection points of two quadrics\nalong the line segment. When the orders of all but one pair of intersections\nare known, we show that a resultant can resolve the order of the remaining pair\nusing only half the precision that may be required to eliminate radicals by\nrepeated squaring. We compare the time and accuracy of our technique with\nconverting to extended precision to calculate roots.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 15:51:22 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Deakin", "Michael", ""], ["Snoeyink", "Jack", ""]]}, {"id": "1605.05944", "submitter": "Kimmo Fredriksson", "authors": "Kimmo Fredriksson", "title": "Geometric Near-neighbor Access Tree (GNAT) revisited", "comments": "Minor changes, submitted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric Near-neighbor Access Tree (GNAT) is a metric space indexing method\nbased on hierarchical hyperplane partitioning of the space. While GNAT is very\nefficient in proximity searching, it has a bad reputation of being a memory\nhog. We show that this is partially based on too coarse analysis, and that the\nmemory requirements can be lowered while at the same time improving the search\nefficiency. We also show how to make GNAT memory adaptive in a smooth way, and\nthat the hyperplane partitioning can be replaced with ball partitioning, which\ncan further improve the search performance. We conclude with experimental\nresults showing the new methods can give significant performance boost.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 13:31:36 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 07:35:48 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Fredriksson", "Kimmo", ""]]}, {"id": "1605.06093", "submitter": "Giovanni Viglietta", "authors": "Marcello Mamino and Giovanni Viglietta", "title": "Square Formation by Asynchronous Oblivious Robots", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem in Distributed Computing is the Pattern Formation\nproblem, where some independent mobile entities, called robots, have to\nrearrange themselves in such a way as to form a given figure from every\npossible (non-degenerate) initial configuration.\n  In the present paper, we consider robots that operate in the Euclidean plane\nand are dimensionless, anonymous, oblivious, silent, asynchronous, disoriented,\nnon-chiral, and non-rigid. For this very elementary type of robots, the\nfeasibility of the Pattern Formation problem has been settled, either in the\npositive or in the negative, for every possible pattern, except for one case:\nthe Square Formation problem by a team of four robots.\n  Here we solve this last case by giving a Square Formation algorithm and\nproving its correctness. Our contribution represents the concluding chapter in\na long thread of research. Our results imply that in the context of the Pattern\nFormation problem for mobile robots, features such as synchronicity, chirality,\nand rigidity are computationally irrelevant.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 19:40:36 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 08:18:07 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Mamino", "Marcello", ""], ["Viglietta", "Giovanni", ""]]}, {"id": "1605.06215", "submitter": "Gary Pui-Tung Choi", "authors": "Chun Pang Yung, Gary P. T. Choi, Ke Chen, Lok Ming Lui", "title": "Efficient Feature-based Image Registration by Mapping Sparsified\n  Surfaces", "comments": null, "journal-ref": "Journal of Visual Communication and Image Representation 55,\n  561-571 (2018)", "doi": "10.1016/j.jvcir.2018.07.005", "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancement in the digital camera technology, the use of high\nresolution images and videos has been widespread in the modern society. In\nparticular, image and video frame registration is frequently applied in\ncomputer graphics and film production. However, conventional registration\napproaches usually require long computational time for high resolution images\nand video frames. This hinders the application of the registration approaches\nin the modern industries. In this work, we first propose a new image\nrepresentation method to accelerate the registration process by triangulating\nthe images effectively. For each high resolution image or video frame, we\ncompute an optimal coarse triangulation which captures the important features\nof the image. Then, we apply a surface registration algorithm to obtain a\nregistration map which is used to compute the registration of the high\nresolution image. Experimental results suggest that our overall algorithm is\nefficient and capable to achieve a high compression rate while the accuracy of\nthe registration is well retained when compared with the conventional\ngrid-based approach. Also, the computational time of the registration is\nsignificantly reduced using our triangulation-based approach.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 05:42:12 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 03:32:25 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Yung", "Chun Pang", ""], ["Choi", "Gary P. T.", ""], ["Chen", "Ke", ""], ["Lui", "Lok Ming", ""]]}, {"id": "1605.06362", "submitter": "Julia Schulte", "authors": "Astrid Kousholt and Julia Schulte", "title": "Reconstruction of convex bodies from moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.CG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate how much information about a convex body can be retrieved from\na finite number of its geometric moments. We give a sufficient condition for a\nconvex body to be uniquely determined by a finite number of its geometric\nmoments, and we show that among all convex bodies, those which are uniquely\ndetermined by a finite number of moments form a dense set. Further, we derive a\nstability result for convex bodies based on geometric moments. It turns out\nthat the stability result is improved considerably by using another set of\nmoments, namely Legendre moments. We present a reconstruction algorithm that\napproximates a convex body using a finite number of its Legendre moments. The\nconsistency of the algorithm is established using the stability result for\nLegendre moments. When only noisy measurements of Legendre moments are\navailable, the consistency of the algorithm is established under certain\nassumptions on the variance of the noise variables.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 14:05:40 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 13:54:24 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 09:18:20 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Kousholt", "Astrid", ""], ["Schulte", "Julia", ""]]}, {"id": "1605.07829", "submitter": "Marco Attene", "authors": "Marco Attene", "title": "As-exact-as-possible repair of unprintable STL files", "comments": null, "journal-ref": "Rapid Prototyping Journal (2018)", "doi": "10.1108/RPJ-11-2016-0185", "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: The class of models that can be represented by STL files is larger\nthan the class of models that can be printed using additive manufacturing\ntechnologies. Stated differently, there exist well-formed STL files that cannot\nbe printed. In this paper such a gap is formalized and a fully automatic\nprocedure is described to turn any such file into a printable model.\n  Approach: Based on well-established concepts from combinatorial topology, we\nprovide an unambiguous description of all the mathematical entities involved in\nthe modeling-printing pipeline. Specifically, we formally define the conditions\nthat an STL file must satisfy to be printable and, based on these, we design an\nas-exact-as-possible repairing algorithm.\n  Findings: We have found that, in order to cope with all the possible triangle\nconfigurations, the algorithm must distinguish between triangles that bound\nsolid parts and triangles that constitute zero-thickness sheets. Only the\nformer set can be fixed without distortion.\n  Originality: Previous methods that are guaranteed to fix all the possible\nconfigurations provide only approximate solutions with an unnecessary\ndistortion. Conversely, our procedure is as exact as possible, meaning that no\nvisible distortion is introduced unless it is strictly imposed by limitations\nof the printing device. Thanks to such an unprecedented flexibility and\naccuracy, this algorithm is expected to significantly simplify the\nmodeling-printing process, in particular within the continuously emerging\nnon-professional \"maker\" communities.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 11:16:00 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 11:53:20 GMT"}, {"version": "v3", "created": "Thu, 21 Jun 2018 14:41:52 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Attene", "Marco", ""]]}, {"id": "1605.08107", "submitter": "Omer Gold", "authors": "Omer Gold and Micha Sharir", "title": "Dominance Product and High-Dimensional Closest Pair under $L_\\infty$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $S$ of $n$ points in $\\mathbb{R}^d$, the Closest Pair problem is\nto find a pair of distinct points in $S$ at minimum distance. When $d$ is\nconstant, there are efficient algorithms that solve this problem, and fast\napproximate solutions for general $d$. However, obtaining an exact solution in\nvery high dimensions seems to be much less understood. We consider the\nhigh-dimensional $L_\\infty$ Closest Pair problem, where $d=n^r$ for some $r >\n0$, and the underlying metric is $L_\\infty$.\n  We improve and simplify previous results for $L_\\infty$ Closest Pair, showing\nthat it can be solved by a deterministic strongly-polynomial algorithm that\nruns in $O(DP(n,d)\\log n)$ time, and by a randomized algorithm that runs in\n$O(DP(n,d))$ expected time, where $DP(n,d)$ is the time bound for computing the\n{\\em dominance product} for $n$ points in $\\mathbb{R}^d$. That is a matrix $D$,\nsuch that $D[i,j] = \\bigl| \\{k \\mid p_i[k] \\leq p_j[k]\\} \\bigr|$; this is the\nnumber of coordinates at which $p_j$ dominates $p_i$. For integer coordinates\nfrom some interval $[-M, M]$, we obtain an algorithm that runs in\n$\\tilde{O}\\left(\\min\\{Mn^{\\omega(1,r,1)},\\, DP(n,d)\\}\\right)$ time, where\n$\\omega(1,r,1)$ is the exponent of multiplying an $n \\times n^r$ matrix by an\n$n^r \\times n$ matrix.\n  We also give slightly better bounds for $DP(n,d)$, by using more recent\nrectangular matrix multiplication bounds. Computing the dominance product\nitself is an important task, since it is applied in many algorithms as a major\nblack-box ingredient, such as algorithms for APBP (all pairs bottleneck paths),\nand variants of APSP (all pairs shortest paths).\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 00:23:34 GMT"}, {"version": "v2", "created": "Sat, 24 Jun 2017 05:13:36 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Gold", "Omer", ""], ["Sharir", "Micha", ""]]}, {"id": "1605.08638", "submitter": "Oliver Joseph David Barrowclough", "authors": "Oliver J. D. Barrowclough and Tor Dokken", "title": "Approximate implicitization using linear algebra", "comments": "25 pages, Article ID 293746", "journal-ref": "Journal of Applied Mathematics, Vol. 2012 (2012) 25 pages", "doi": "10.1155/2012/293746", "report-no": null, "categories": "math.NA cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a family of algorithms for approximate\nimplicitization of rational parametric curves and surfaces. The main\napproximation tool in all of the approaches is the singular value\ndecomposition, and they are therefore well suited to floating point\nimplementation in computer aided geometric design (CAGD) systems. We unify the\napproaches under the names of commonly known polynomial basis functions, and\nconsider various theoretical and practical aspects of the algorithms. We offer\nnew methods for a least squares approach to approximate implicitization using\northogonal polynomials, which tend to be faster and more numerically stable\nthan some existing algorithms. We propose several simple propositions relating\nthe properties of the polynomial bases to their implicit approximation\nproperties.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 13:45:27 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Barrowclough", "Oliver J. D.", ""], ["Dokken", "Tor", ""]]}, {"id": "1605.08669", "submitter": "Oliver Joseph David Barrowclough", "authors": "Oliver J. D. Barrowclough", "title": "A basis for the implicit representation of planar rational cubic\n  B\\'ezier curves", "comments": null, "journal-ref": "Computer Aided Geometric Design, 31(3-4) (2014) 148-167", "doi": "10.1016/j.cagd.2014.02.007", "report-no": null, "categories": "math.NA cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to finding the implicit equation of a planar rational\nparametric cubic curve, by defining a new basis for the representation. The\nbasis, which contains only four cubic bivariate polynomials, is defined in\nterms of the B\\'ezier control points of the curve. An explicit formula for the\ncoefficients of the implicit curve is given. Moreover, these coefficients lead\nto simple expressions which describe aspects of the geometric behaviour of the\ncurve. In particular, we present an explicit barycentric formula for the\nposition of the double point, in terms of the B\\'ezier control points of the\ncurve. We also give conditions for when an unwanted singularity occurs in the\nregion of interest. Special cases in which the method fails, such as when three\nof the control points are collinear, or when two points coincide, will be\ndiscussed separately.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 14:34:25 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Barrowclough", "Oliver J. D.", ""]]}, {"id": "1605.08912", "submitter": "Rushil Anirudh", "authors": "Rushil Anirudh, Vinay Venkataraman, Karthikeyan Natesan Ramamurthy,\n  Pavan Turaga", "title": "A Riemannian Framework for Statistical Analysis of Topological\n  Persistence Diagrams", "comments": "Accepted at DiffCVML 2016 (CVPR 2016 Workshops)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG cs.CV math.DG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological data analysis is becoming a popular way to study high dimensional\nfeature spaces without any contextual clues or assumptions. This paper concerns\nitself with one popular topological feature, which is the number of\n$d-$dimensional holes in the dataset, also known as the Betti$-d$ number. The\npersistence of the Betti numbers over various scales is encoded into a\npersistence diagram (PD), which indicates the birth and death times of these\nholes as scale varies. A common way to compare PDs is by a point-to-point\nmatching, which is given by the $n$-Wasserstein metric. However, a big drawback\nof this approach is the need to solve correspondence between points before\ncomputing the distance; for $n$ points, the complexity grows according to\n$\\mathcal{O}($n$^3)$. Instead, we propose to use an entirely new framework\nbuilt on Riemannian geometry, that models PDs as 2D probability density\nfunctions that are represented in the square-root framework on a Hilbert\nSphere. The resulting space is much more intuitive with closed form expressions\nfor common operations. The distance metric is 1) correspondence-free and also\n2) independent of the number of points in the dataset. The complexity of\ncomputing distance between PDs now grows according to $\\mathcal{O}(K^2)$, for a\n$K \\times K$ discretization of $[0,1]^2$. This also enables the use of existing\nmachinery in differential geometry towards statistical analysis of PDs such as\ncomputing the mean, geodesics, classification etc. We report competitive\nresults with the Wasserstein metric, at a much lower computational load,\nindicating the favorable properties of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 16:55:40 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Anirudh", "Rushil", ""], ["Venkataraman", "Vinay", ""], ["Ramamurthy", "Karthikeyan Natesan", ""], ["Turaga", "Pavan", ""]]}, {"id": "1605.09153", "submitter": "Zolt\\'an Kov\\'acs", "authors": "Francisco Botana and Zolt\\'an Kov\\'acs", "title": "New tools in GeoGebra offering novel opportunities to teach loci and\n  envelopes", "comments": "21 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.HO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GeoGebra is an open source mathematics education software tool being used in\nthousands of schools worldwide. Since version 4.2 (December 2012) it supports\nsymbolic computation of locus equations as a result of joint effort of\nmathematicians and programmers helping the GeoGebra developer team. The joint\nwork, based on former researches, started in 2010 and continued until present\ndays, now enables fast locus and envelope computations even in a web browser in\nfull HTML5 mode. Thus, classroom demonstrations and deeper investigations of\ndynamic analytical geometry are ready to use on tablets or smartphones as well.\n  In our paper we consider some typical secondary school topics where\ninvestigating loci is a natural way of defining mathematical objects. We\ndiscuss the technical possibilities in GeoGebra by using the new commands\nLocusEquation and Envelope, showing through different examples how these\ncommands can enrich the learning of mathematics. The covered school topics\ninclude definition of a parabola and other conics in different situations like\nsynthetic definitions or points and curves associated with a triangle. Despite\nthe fact that in most secondary schools, no other than quadratic curves are\ndiscussed, simple generalization of some exercises, and also every day\nproblems, will smoothly introduce higher order algebraic curves. Thus our paper\nmentions the cubic curve \"strophoid\" as locus of the orthocenter of a triangle\nwhen one of the vertices moves on a circle. Also quartic \"cardioid\" and sextic\n\"nephroid\" can be of every day interest when investigating mathematics in, say,\na coffee cup.\n  We also focus on GeoGebra specific tips and tricks when constructing a\ngeometric figure to be available for getting the locus equation. Among others,\nsimplification and synthetization (via the intercept theorem) are mentioned.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 09:37:28 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Botana", "Francisco", ""], ["Kov\u00e1cs", "Zolt\u00e1n", ""]]}, {"id": "1605.09244", "submitter": "Roman Prutkin", "authors": "Roman Prutkin", "title": "A Note on the Area Requirement of Euclidean Greedy Embeddings of\n  Christmas Cactus Graphs", "comments": "This problem has been stated by Ankur Moitra in his presentation at\n  the 49th Annual IEEE Symposium on Foundations of Computer Science (FOCS'08):\n  http://people.csail.mit.edu/moitra/docs/ftl.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An Euclidean greedy embedding of a graph is a straight-line embedding in the\nplane, such that for every pair of vertices $s$ and $t$, the vertex $s$ has a\nneighbor $v$ with smaller distance to $t$ than $s$. This drawing style is\nmotivated by greedy geometric routing in wireless sensor networks.\n  A Christmas cactus is a connected graph in which every two simple cycles have\nat most one vertex in common and in which every cutvertex is part of at most\ntwo biconnected blocks. It has been proved that Christmas cactus graphs have an\nEuclidean greedy embedding. This fact has played a crucial role in proving that\nevery 3-connected planar graph has an Euclidean greedy embedding. The proofs\nconstruct greedy embeddings of Christmas cactuses of exponential size, and it\nhas been an open question whether exponential area is necessary in the worst\ncase for greedy embeddings of Christmas cactuses. We prove that this is indeed\nthe case.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 14:15:31 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Prutkin", "Roman", ""]]}, {"id": "1605.09250", "submitter": "Julia Portl", "authors": "Julia Portl and Heike Leitte", "title": "Feature Extraction from Segmentations of Neuromuscular Junctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentations are often necessary for the analysis of image data. They are\nused to identify different objects, for example cell nuclei, mitochondria, or\ncomplete cells in microscopic images. There might be features in the data, that\ncannot be detected by segmentation approaches directly, because they are not\ncharacterized by their texture of boundaries, which are properties most\nsegmentation techniques rely on, but morphologically. In this report we will\nintroduce our algorithm for the extraction of suchlike morphological features\nof segmented objects from segmentations of neuromuscular junctions and its\ninterface for informed parameter tuning.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 14:28:31 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Portl", "Julia", ""], ["Leitte", "Heike", ""]]}]