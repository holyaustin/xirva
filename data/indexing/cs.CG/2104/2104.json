[{"id": "2104.00207", "submitter": "Manjanna Basappa", "authors": "Monith S. Reyunuru, Kriti Jethlia and Manjanna Basappa", "title": "The $k$-Colorable Unit Disk Cover Problem", "comments": "25 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we consider colorable variations of the Unit Disk Cover\n({\\it UDC}) problem as follows.\n  {\\it $k$-Colorable Discrete Unit Disk Cover ({\\it $k$-CDUDC})}: Given a set\n$P$ of $n$ points, and a set $D$ of $m$ unit disks (of radius=1), both lying in\nthe plane, and a parameter $k$, the objective is to compute a set $D'\\subseteq\nD$ such that every point in $P$ is covered by at least one disk in $D'$ and\nthere exists a function $\\chi:D'\\rightarrow C$ that assigns colors to disks in\n$D'$ such that for any $d$ and $d'$ in $D'$ if $d\\cap d'\\neq\\emptyset$, then\n$\\chi(d)\\neq\\chi(d')$, where $C$ denotes a set containing $k$ distinct colors.\n  For the {\\it $k$-CDUDC} problem, our proposed algorithms approximate the\nnumber of colors used in the coloring if there exists a $k$-colorable cover. We\nfirst propose a 4-approximation algorithm in $O(m^{7k}n\\log k)$ time for this\nproblem and then show that the running time can be improved by a multiplicative\nfactor of $m^k$, where a positive integer $k$ denotes the cardinality of a\ncolor-set. The previous best known result for the problem when $k=3$ is due to\nthe recent work of Biedl et al., (2021), who proposed a 2-approximation\nalgorithm in $O(m^{25}n)$ time. For $k=3$, our algorithm runs in $O(m^{18}n)$\ntime, faster than the previous best algorithm, but gives a 4-approximate\nresult. We then generalize our approach to exhibit a\n$O((1+\\lceil\\frac{2}{\\tau}\\rceil)^2)$-approximation algorithm in\n$O(m^{(\\lfloor\\frac{4\\pi+8\\tau+\\tau^2}{\\sqrt{12}}\\rfloor)k}n\\log k)$ time for a\ngiven $1 \\leq \\tau \\leq 2$. We also extend our algorithm to solve the {\\it\n$k$-Colorable Line Segment Disk Cover ({\\it $k$-CLSDC})} and {\\it $k$-Colorable\nRectangular Region Cover ({\\it $k$-CRRC})} problems, in which instead of the\nset $P$ of $n$ points, we are given a set $S$ of $n$ line segments, and a\nrectangular region $\\cal R$, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 02:23:34 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 14:32:42 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Reyunuru", "Monith S.", ""], ["Jethlia", "Kriti", ""], ["Basappa", "Manjanna", ""]]}, {"id": "2104.00514", "submitter": "Luc Moschella", "authors": "Luca Moschella, Simone Melzi, Luca Cosmo, Filippo Maggioli, Or Litany,\n  Maks Ovsjanikov, Leonidas Guibas, Emanuele Rodol\\`a", "title": "Spectral Unions of Partial Deformable 3D Shapes", "comments": "17 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral geometric methods have brought revolutionary changes to the field of\ngeometry processing -- however, when the data to be processed exhibits severe\npartiality, such methods fail to generalize. As a result, there exists a big\nperformance gap between methods dealing with complete shapes, and methods that\naddress missing geometry. In this paper, we propose a possible way to fill this\ngap. We introduce the first method to compute compositions of non-rigidly\ndeforming shapes, without requiring to solve first for a dense correspondence\nbetween the given partial shapes. We do so by operating in a purely spectral\ndomain, where we define a union operation between short sequences of\neigenvalues. Working with eigenvalues allows to deal with unknown\ncorrespondence, different sampling, and different discretization (point clouds\nand meshes alike), making this operation especially robust and general. Our\napproach is data-driven, and can generalize to isometric and non-isometric\ndeformations of the surface, as long as these stay within the same semantic\nclass (e.g., human bodies), as well as to partiality artifacts not seen at\ntraining time.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:19:18 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Moschella", "Luca", ""], ["Melzi", "Simone", ""], ["Cosmo", "Luca", ""], ["Maggioli", "Filippo", ""], ["Litany", "Or", ""], ["Ovsjanikov", "Maks", ""], ["Guibas", "Leonidas", ""], ["Rodol\u00e0", "Emanuele", ""]]}, {"id": "2104.00720", "submitter": "Mason A. Porter", "authors": "Michelle Feng, Abigail Hickok, and Mason A. Porter", "title": "Topological Data Analysis of Spatial Systems", "comments": "draft of book chapter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CG math.AT nlin.AO physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter, we discuss applications of topological data analysis (TDA)\nto spatial systems. We briefly review the recently proposed level-set\nconstruction of filtered simplicial complexes, and we then examine persistent\nhomology in two cases studies: street networks in Shanghai and hotspots of\nCOVID-19 infections. We then summarize our results and provide an outlook on\nTDA in spatial systems.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 18:58:18 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Feng", "Michelle", ""], ["Hickok", "Abigail", ""], ["Porter", "Mason A.", ""]]}, {"id": "2104.00944", "submitter": "Wanyue Xu", "authors": "Jiang Che, Xu Wanyue, Zhou Xiaotian, Zhang Zhongzhi, and Kan Haibin", "title": "Some Combinatorial Problems in Power-law Graphs", "comments": null, "journal-ref": "published in the Computer Journal, 2021", "doi": "10.1093/comjnl/bxab007", "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power-law behavior is ubiquitous in a majority of real-world networks,\nand it was shown to have a strong effect on various combinatorial, structural,\nand dynamical properties of graphs. For example, it has been shown that in\nreal-life power-law networks, both the matching number and the domination\nnumber are relatively smaller, compared with homogeneous graphs. In this paper,\nwe study analytically several combinatorial problems for two power-law graphs\nwith the same number of vertices, edges, and the same power exponent. For both\ngraphs, we determine exactly or recursively their matching number, independence\nnumber, domination number, the number of maximum matchings, the number of\nmaximum independent sets, and the number of minimum dominating sets. We show\nthat power-law behavior itself cannot characterize the combinatorial properties\nof a heterogenous graph. Since the combinatorial properties studied here have\nfound wide applications in different fields, such as structural controllability\nof complex networks, our work offers insight in the applications of these\ncombinatorial problems in power-law graphs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 08:48:48 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Che", "Jiang", ""], ["Wanyue", "Xu", ""], ["Xiaotian", "Zhou", ""], ["Zhongzhi", "Zhang", ""], ["Haibin", "Kan", ""]]}, {"id": "2104.02876", "submitter": "Dmitry Berdinsky", "authors": "Dmitry Berdinsky and Prohrak Kruengthomya", "title": "Representing Polynomial Splines over Infinite Hierarchical Meshes by\n  Finite Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data structure based on finite automata for representing\npolynomial splines over infinite hierarchical meshes. It allows to store and\noperate such splines using only finite amount of memory. This naturally extends\na classical framework of hierarchical tensor product B-splines for infinite\nmeshes in a way suitable for computing.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 02:59:43 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Berdinsky", "Dmitry", ""], ["Kruengthomya", "Prohrak", ""]]}, {"id": "2104.03484", "submitter": "Yair Bartal", "authors": "Yair Bartal", "title": "Advances in Metric Ramsey Theory and its Applications", "comments": "This is paper is still in stages of preparation, this version is not\n  intended for distribution. A preliminary version of this article was written\n  by the author in 2006, and was presented in the 2007 ICMS Workshop on\n  Geometry and Algorithms. The basic result on constructive metric Ramsey\n  decomposition and metric Ramsey theorem has also appeared in the author's\n  lectures notes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metric Ramsey theory is concerned with finding large well-structured subsets\nof more complex metric spaces. For finite metric spaces this problem was first\nstudies by Bourgain, Figiel and Milman \\cite{bfm}, and studied further in depth\nby Bartal et. al \\cite{BLMN03}. In this paper we provide deterministic\nconstructions for this problem via a novel notion of \\emph{metric Ramsey\ndecomposition}. This method yields several more applications, reflecting on\nsome basic results in metric embedding theory.\n  The applications include various results in metric Ramsey theory including\nthe first deterministic construction yielding Ramsey theorems with tight\nbounds, a well as stronger theorems and properties, implying appropriate\ndistance oracle applications.\n  In addition, this decomposition provides the first deterministic\nBourgain-type embedding of finite metric spaces into Euclidean space, and an\noptimal multi-embedding into ultrametrics, thus improving its applications in\napproximation and online algorithms.\n  The decomposition presented here, the techniques and its consequences have\nalready been used in recent research in the field of metric embedding for\nvarious applications.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 02:52:16 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Bartal", "Yair", ""]]}, {"id": "2104.03900", "submitter": "Xianghao Xu", "authors": "Xianghao Xu, Wenzhe Peng, Chin-Yi Cheng, Karl D.D. Willis, Daniel\n  Ritchie", "title": "Inferring CAD Modeling Sequences Using Zone Graphs", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In computer-aided design (CAD), the ability to \"reverse engineer\" the\nmodeling steps used to create 3D shapes is a long-sought-after goal. This\nprocess can be decomposed into two sub-problems: converting an input mesh or\npoint cloud into a boundary representation (or B-rep), and then inferring\nmodeling operations which construct this B-rep. In this paper, we present a new\nsystem for solving the second sub-problem. Central to our approach is a new\ngeometric representation: the zone graph. Zones are the set of solid regions\nformed by extending all B-Rep faces and partitioning space with them; a zone\ngraph has these zones as its nodes, with edges denoting geometric adjacencies\nbetween them. Zone graphs allow us to tractably work with industry-standard CAD\noperations, unlike prior work using CSG with parametric primitives. We focus on\nCAD programs consisting of sketch + extrude + Boolean operations, which are\ncommon in CAD practice. We phrase our problem as search in the space of such\nextrusions permitted by the zone graph, and we train a graph neural network to\nscore potential extrusions in order to accelerate the search. We show that our\napproach outperforms an existing CSG inference baseline in terms of geometric\nreconstruction accuracy and reconstruction time, while also creating more\nplausible modeling sequences.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 05:55:59 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 19:24:39 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Xu", "Xianghao", ""], ["Peng", "Wenzhe", ""], ["Cheng", "Chin-Yi", ""], ["Willis", "Karl D. D.", ""], ["Ritchie", "Daniel", ""]]}, {"id": "2104.04229", "submitter": "Sanjana Dey", "authors": "Sanjana Dey, Ramesh K. Jallu and Subhas C. Nandy", "title": "On the minimum spanning tree problem in imprecise set-up", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we study the Euclidean minimum spanning tree problem in an\nimprecise setup. The problem is known as the \\emph{Minimum Spanning Tree\nProblem with Neighborhoods} in the literature. We study the problem where the\nneighborhoods are represented as non-crossing line segments. Given a set ${\\cal\nS}$ of $n$ disjoint line segments in $I\\!\\!R^2$, the objective is to find a\nminimum spanning tree (MST) that contains exactly one end-point from each\nsegment in $\\cal S$ and the cost of the MST is minimum among $2^n$ possible\nMSTs. We show that finding such an MST is NP-hard in general, and propose a\n$2\\alpha$-factor approximation algorithm for the same, where $\\alpha$ is the\napproximation factor of the best-known approximation algorithm to compute a\nminimum cost Steiner tree in an undirected graph with non-negative edge\nweights. As an implication of our reduction, we can show that the unrestricted\nversion of the problem (i.e., one point must be chosen from each segment such\nthat the cost of MST is as minimum as possible) is also NP-hard. We also\npropose a parameterized algorithm for the problem based on the \"separability\"\nparameter defined for segments.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 07:42:21 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Dey", "Sanjana", ""], ["Jallu", "Ramesh K.", ""], ["Nandy", "Subhas C.", ""]]}, {"id": "2104.04525", "submitter": "Shunji Umetani", "authors": "Shunji Umetani and Shohei Murakami", "title": "Coordinate descent heuristics for the irregular strip packing problem of\n  rasterized shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.AI math.OC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the irregular strip packing problem of rasterized shapes, where a\ngiven set of pieces of irregular shapes represented in pixels should be placed\ninto a rectangular container without overlap. The rasterized shapes enable us\nto check overlap without any exceptional handling due to geometric issues,\nwhile they often require much memory and computational effort in\nhigh-resolution. We develop an efficient algorithm to check overlap using a\npair of scanlines that reduces the complexity of rasterized shapes by merging\nconsecutive pixels in each row and column into strips with unit width,\nrespectively. Based on this, we develop coordinate descent heuristics that\nrepeat a line search in the horizontal and vertical directions alternately.\nComputational results for test instances show that the proposed algorithm\nobtains sufficiently dense layouts of rasterized shapes in high-resolution\nwithin a reasonable computation time.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 08:55:52 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Umetani", "Shunji", ""], ["Murakami", "Shohei", ""]]}, {"id": "2104.04614", "submitter": "Marcel Campen", "authors": "Marcel Campen, Ryan Capouellez, Hanxiao Shen, Leyi Zhu, Daniele\n  Panozzo, Denis Zorin", "title": "Efficient and Robust Discrete Conformal Equivalence with Boundary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an efficient algorithm to compute a conformally equivalent metric\nfor a discrete surface, possibly with boundary, exhibiting prescribed Gaussian\ncurvature at all interior vertices and prescribed geodesic curvature along the\nboundary. Our construction is based on the theory developed in [Gu et al. 2018;\nSpringborn 2020], and in particular relies on results on hyperbolic Delaunay\ntriangulations. Generality is achieved by considering the surface's intrinsic\ntriangulation as a degree of freedom, and particular attention is paid to the\nproper treatment of surface boundaries. While via a double cover approach the\nboundary case can be reduced to the closed case quite naturally, the implied\nsymmetry of the setting causes additional challenges related to stable\nDelaunay-critical configurations that we address explicitly in this work.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 21:32:47 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Campen", "Marcel", ""], ["Capouellez", "Ryan", ""], ["Shen", "Hanxiao", ""], ["Zhu", "Leyi", ""], ["Panozzo", "Daniele", ""], ["Zorin", "Denis", ""]]}, {"id": "2104.04940", "submitter": "Gerardo Lauro Maldonado Mart\\'inez", "authors": "Gerardo L. Maldonado and Edgardo Rold\\'an-Pensado", "title": "Dissecting the square into seven or nine congruent parts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a computer-based proof of the following fact: If a square is divided\ninto seven or nine convex polygons, congruent among themselves, then the tiles\nare rectangles.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 07:21:57 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 18:19:51 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 15:49:40 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Maldonado", "Gerardo L.", ""], ["Rold\u00e1n-Pensado", "Edgardo", ""]]}, {"id": "2104.05550", "submitter": "Tim Felle Olsen", "authors": "Florian Cyril Stutz (1), Tim Felle Olsen (1), Jeroen Peter Groen (1),\n  Niels Aage (1), Ole Sigmund (1), Justin Solomon (2) and Jakob Andreas\n  B{\\ae}rentzen (1) ((1) Technical University of Denmark, (2) Massachusetts\n  Institute of Technology)", "title": "Synthesis of Frame Field-Aligned Multi-Laminar Structures", "comments": "19 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of topology optimization, the homogenization approach has been\nrevived as an important alternative to the established, density-based methods\nbecause it can represent the microstructural design at a much finer\nlength-scale than the computational grid. The optimal microstructure for a\nsingle load case is an orthogonal rank-3 laminate. A rank-3 laminate can be\ndescribed in terms of frame fields, which are also an important tool for mesh\ngeneration in both 2D and 3D.\n  We propose a method for generating multi-laminar structures from frame\nfields. Rather than relying on integrative approaches that find a\nparametrization based on the frame field, we find stream surfaces, represented\nas point clouds aligned with frame vectors, and we solve an optimization\nproblem to find well-spaced collections of such stream surfaces. The stream\nsurface tracing is unaffected by the presence of singularities outside the\nregion of interest. Neither stream surface tracing nor selecting well-spaced\nsurface rely on combed frame fields.\n  In addition to stream surface tracing and selection, we provide two methods\nfor generating structures from stream surface collections. One of these methods\nproduces volumetric solids by summing basis functions associated with each\npoint of the stream surface collection. The other method reinterprets point\nsampled stream surfaces as a spatial twist continuum and produces a\nhexahedralization by dualizing a graph representing the structure.\n  We demonstrate our methods on several frame fields produced using the\nhomogenization approach for topology optimization, boundary-aligned, algebraic\nframe fields, and frame fields computed from closed-form expressions.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 15:19:36 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Stutz", "Florian Cyril", ""], ["Olsen", "Tim Felle", ""], ["Groen", "Jeroen Peter", ""], ["Aage", "Niels", ""], ["Sigmund", "Ole", ""], ["Solomon", "Justin", ""], ["B\u00e6rentzen", "Jakob Andreas", ""]]}, {"id": "2104.05628", "submitter": "Maciej Skorski", "authors": "Maciej Skorski", "title": "Confidence-Optimal Random Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seminal result of Johnson and Lindenstrauss on random embeddings has been\nintensively studied in applied and theoretical computer science. Despite that\nvast body of literature, we still lack of complete understanding of statistical\nproperties of random projections; a particularly intriguing question is: why\nare the theoretical bounds that far behind the empirically observed\nperformance? Motivated by this question, this work develops\nJohnson-Lindenstrauss distributions with optimal, data-oblivious, statistical\nconfidence bounds. These bounds are numerically best possible, for any given\ndata dimension, embedding dimension, and distortion tolerance. They improve\nupon prior works in terms of statistical accuracy, as well as exactly determine\nthe no-go regimes for data-oblivious approaches. Furthermore, the corresponding\nprojection matrices are efficiently samplable. The construction relies on\northogonal matrices, and the proof uses certain elegant properties of the unit\nsphere. The following techniques introduced in this work are of independent\ninterest: a) a compact expression for distortion in terms of singular\neigenvalues of the projection matrix, b) a parametrization linking the unit\nsphere and the Dirichlet distribution and c) anti-concentration bounds for the\nDirichlet distribution.\n  Besides the technical contribution, the paper presents applications and\nnumerical evaluation along with working implementation in Python.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 18:00:02 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Skorski", "Maciej", ""]]}, {"id": "2104.06575", "submitter": "Sterling Baird", "authors": "Sterling G. Baird, Eric R. Homer, David T. Fullwood, Oliver K. Johnson", "title": "Five Degree-of-Freedom Property Interpolation of Arbitrary Grain\n  Boundaries via Voronoi Fundamental Zone Octonion Framework", "comments": "main: 22 pages, 10 figures; appendices: 5 pages, 3 figures; supp: 13\n  pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the Voronoi fundamental zone octonion interpolation framework\nfor grain boundary (GB) structure-property models and surrogates. The VFZO\nframework offers an advantage over other five degree-of-freedom based property\ninterpolation methods because it is constructed as a point set in a manifold.\nThis means that directly computed Euclidean distances approximate the original\noctonion distance with significantly reduced computation runtime (~7 CPU\nminutes vs. 153 CPU days for a 50000x50000 pairwise-distance matrix). This\nincreased efficiency facilitates lower interpolation error through the use of\nsignificantly more input data. We demonstrate grain boundary energy\ninterpolation results for a non-smooth validation function and simulated\nbi-crystal datasets for Fe and Ni using four interpolation methods: barycentric\ninterpolation, Gaussian process regression (GPR), inverse-distance weighting,\nand nearest-neighbor interpolation. These are evaluated for 50000 random input\nGBs and 10 000 random prediction GBs. The best performance was achieved with\nGPR, which resulted in a reduction of the root mean square error (RMSE) by\n83.0% relative to RMSE of a constant, average model. Likewise, interpolation on\na large, noisy, molecular statics Fe simulation dataset improves performance by\n34.4% compared to 21.2% in prior work. Interpolation on a small, low-noise MS\nNi simulation dataset is similar to interpolation results for the original\noctonion metric (57.6% vs. 56.4%). A vectorized, parallelized, MATLAB\ninterpolation function (interp5DOF.m) and related routines are available in our\nVFZO repository (github.com/sgbaird-5dof/interp) which can be applied to other\ncrystallographic point groups. The VFZO framework offers advantages for\ncomputing distances between GBs, estimating property values for arbitrary GBs,\nand modeling surrogates of computationally expensive 5DOF functions and\nsimulations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 01:33:20 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Baird", "Sterling G.", ""], ["Homer", "Eric R.", ""], ["Fullwood", "David T.", ""], ["Johnson", "Oliver K.", ""]]}, {"id": "2104.06787", "submitter": "Boris Zolotov", "authors": "Stefan Langerman, Nicolas Potvin, Boris Zolotov", "title": "Enumerating All Convex Polyhedra Glued from Squares in Polynomial Time", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm that enumerates and classifies all edge-to-edge\ngluings of unit squares that correspond to convex polyhedra. We show that the\nnumber of such gluings of $n$ squares is polynomial in $n$, and the algorithm\nruns in time polynomial in $n$ (pseudopolynomial if $n$ is considered the only\ninput). Our technique can be applied in several similar settings, including\ngluings of regular hexagons and triangles.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 11:24:54 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Langerman", "Stefan", ""], ["Potvin", "Nicolas", ""], ["Zolotov", "Boris", ""]]}, {"id": "2104.07011", "submitter": "Tzvika Geft", "authors": "Tzvika Geft, Dan Halperin", "title": "On the Complexity of a Family of Decoupled Multi-Robot Motion Planning\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-robot motion planning (MRMP) the aim is to plan the motion of\nseveral robots operating in a common workspace, while avoiding collisions with\nobstacles or with fellow robots. The main contribution of this paper is a\nsimple construction that serves as a lower bound for the computational cost of\na variety of prevalent MRMP problems. In particular we show that optimal\ndecoupling of multi-robot motion -- decoupling being a standard approach to\npractically addressing MRMP -- is NP-hard. The basic problem for which we\npresent our construction is monotone MRMP, a restricted and natural MRMP\nvariant, where robots move one by one to their targets with no intermediate\nstops. Observing the hardness of these restricted versions of MRMP is\nsignificant as it guides the search for efficient solutions towards techniques\nthat can cope with intractable problems. Furthermore, our construction\nhighlights structural properties of difficult instances, such as the need of\nrobots to pass through many start and target positions of other robots. These\ninsights can lead to useful problem relaxations resulting in efficient\nsolutions and to suitable engineering of workspaces.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:51:24 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Geft", "Tzvika", ""], ["Halperin", "Dan", ""]]}, {"id": "2104.07097", "submitter": "Mario Vazquez Corte MSc", "authors": "Mario Vazquez Corte, Luis V. Montiel", "title": "Novel Matrix Hit and Run for Sampling Polytopes and Its GPU\n  Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a new Markov Chain Monte Carlo algorithm that\ngenerates a uniform sample over full and non-full dimensional polytopes. This\nalgorithm, termed \"Matrix Hit and Run\" (MHAR), is a modification of the Hit and\nRun framework. For the regime $n^{1+\\frac{1}{3}} \\ll m$, MHAR has a lower\nasymptotic cost per sample in terms of soft-O notation ($\\SO$) than do existing\nsampling algorithms after a \\textit{warm start}. MHAR is designed to take\nadvantage of matrix multiplication routines that require less computational and\nmemory resources. Our tests show this implementation to be substantially faster\nthan the \\textit{hitandrun} R package, especially for higher dimensions.\nFinally, we provide a python library based on Pytorch and a Colab notebook with\nthe implementation ready for deployment in architectures with GPU or just CPU.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 19:55:04 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Corte", "Mario Vazquez", ""], ["Montiel", "Luis V.", ""]]}, {"id": "2104.07563", "submitter": "Luis Scoccola", "authors": "Luis Scoccola and Jose A. Perea", "title": "Approximate and discrete Euclidean vector bundles", "comments": "56 pages, 9 figures; v2: improvements to exposition", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce $\\varepsilon$-approximate versions of the notion of Euclidean\nvector bundle for $\\varepsilon \\geq 0$, which recover the classical notion of\nEuclidean vector bundle when $\\varepsilon = 0$. In particular, we study\n\\v{C}ech cochains with coefficients in the orthogonal group that satisfy an\napproximate cocycle condition. We show that $\\varepsilon$-approximate vector\nbundles can be used to represent classical vector bundles when $\\varepsilon >\n0$ is sufficiently small. We also introduce distances between approximate\nvector bundles and use them to prove that sufficiently similar approximate\nvector bundles represent the same classical vector bundle. This gives a way of\nspecifying vector bundles over finite simplicial complexes using a finite\namount of data, and also allows for some tolerance to noise when working with\nvector bundles in an applied setting. As an example, we prove a reconstruction\ntheorem for vector bundles from finite samples. We give algorithms for the\neffective computation of low-dimensional characteristic classes of vector\nbundles directly from discrete and approximate representations and illustrate\nthe usage of these algorithms with computational examples.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:19:56 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 17:28:42 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Scoccola", "Luis", ""], ["Perea", "Jose A.", ""]]}, {"id": "2104.07710", "submitter": "Samantha Chen", "authors": "Samantha Chen, Yusu Wang", "title": "Approximation algorithms for 1-Wasserstein distance between persistence\n  diagrams", "comments": "To be published in LIPIcs, Volume 190, SEA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have witnessed a tremendous growth using topological summaries,\nespecially the persistence diagrams (encoding the so-called persistent\nhomology) for analyzing complex shapes. Intuitively, persistent homology maps a\npotentially complex input object (be it a graph, an image, or a point set and\nso on) to a unified type of feature summary, called the persistence diagrams.\nOne can then carry out downstream data analysis tasks using such persistence\ndiagram representations. A key problem is to compute the distance between two\npersistence diagrams efficiently. In particular, a persistence diagram is\nessentially a multiset of points in the plane, and one popular distance is the\nso-called 1-Wasserstein distance between persistence diagrams. In this paper,\nwe present two algorithms to approximate the 1-Wasserstein distance for\npersistence diagrams in near-linear time. These algorithms primarily follow the\nsame ideas as two existing algorithms to approximate optimal transport between\ntwo finite point-sets in Euclidean spaces via randomly shifted quadtrees. We\nshow how these algorithms can be effectively adapted for the case of\npersistence diagrams. Our algorithms are much more efficient than previous\nexact and approximate algorithms, both in theory and in practice, and we\ndemonstrate its efficiency via extensive experiments. They are conceptually\nsimple and easy to implement, and the code is publicly available in github.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 18:27:58 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Chen", "Samantha", ""], ["Wang", "Yusu", ""]]}, {"id": "2104.08136", "submitter": "Jordi Vermeulen", "authors": "Marc van Kreveld, Frank Staals, Amir Vaxman, Jordi Vermeulen", "title": "Approximating the Earth Mover's Distance between sets of geometric\n  objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given two distributions $P$ and $S$ of equal total mass, the Earth Mover's\nDistance measures the cost of transforming one distribution into the other,\nwhere the cost of moving a unit of mass is equal to the distance over which it\nis moved.\n  We give approximation algorithms for the Earth Mover's Distance between\nvarious sets of geometric objects. We give a $(1 + \\varepsilon)$-approximation\nwhen $P$ is a set of weighted points and $S$ is a set of line segments,\ntriangles or $d$-dimensional simplices. When $P$ and $S$ are both sets of line\nsegments, sets of triangles or sets of simplices, we give a $(1 +\n\\varepsilon)$-approximation with a small additive term. All algorithms run in\ntime polynomial in the size of $P$ and $S$, and actually calculate the\ntransport plan (that is, a specification of how to move the mass), rather than\njust the cost. To our knowledge, these are the first combinatorial algorithms\nwith a provable approximation ratio for the Earth Mover's Distance when the\nobjects are continuous rather than discrete points.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 14:34:17 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["van Kreveld", "Marc", ""], ["Staals", "Frank", ""], ["Vaxman", "Amir", ""], ["Vermeulen", "Jordi", ""]]}, {"id": "2104.09392", "submitter": "Dennis Rohde", "authors": "Maike Buchin and Dennis Rohde", "title": "Coresets for $(k, \\ell)$-Median Clustering under the Fr\\'echet Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for computing $\\epsilon$-coresets for $(k,\n\\ell)$-median clustering of polygonal curves under the Fr\\'echet distance. This\ntype of clustering, which has recently drawn increasing popularity due to a\ngrowing number of applications, is an adaption of Euclidean $k$-median\nclustering: we are given a set of polygonal curves and want to compute $k$\ncenter curves such that the sum of distances from the given curves to their\nnearest center curve is minimal. Additionally, we restrict the complexity,\ni.e., number of vertices, of the center curves to be at most $\\ell$ each, to\nsuppress overfitting. We achieve this result by applying the improved\n$\\epsilon$-coreset framework by Braverman et al. to a generalized $k$-median\nproblem over an arbitrary metric space. Later we combine this result with the\nrecent result by Driemel et al. on the VC dimension of metric balls under the\nFr\\'echet distance. Finally, we show that $\\epsilon$-coresets can be used to\nimprove an existing approximation algorithm for $(1,\\ell)$-median clustering\nunder the Fr\\'echet distance, taking a further step in the direction of\npractical $(k,\\ell)$-median clustering algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:32:26 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 09:55:53 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Buchin", "Maike", ""], ["Rohde", "Dennis", ""]]}, {"id": "2104.09976", "submitter": "Dibyayan Chakraborty", "authors": "Dibyayan Chakraborty, Kshitij Gajjar", "title": "Finding Geometric Representations of Apex Graphs is NP-Hard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Planar graphs can be represented as intersection graphs of different types of\ngeometric objects in the plane, e.g., circles (Koebe, 1936), line segments\n(Chalopin \\& Gon{\\c{c}}alves, 2009), \\textsc{L}-shapes (Gon{\\c{c}}alves et al,\n2018). For general graphs, however, even deciding whether such representations\nexist is often $NP$-hard. We consider apex graphs, i.e., graphs that can be\nmade planar by removing one vertex from them. We show, somewhat surprisingly,\nthat deciding whether geometric representations exist for apex graphs is\n$NP$-hard.\n  More precisely, we show that for every positive integer $k$, recognizing\nevery graph class $\\mathcal{G}$ which satisfies $\\textsc{PURE-2-DIR} \\subseteq\n\\mathcal{G} \\subseteq \\textsc{1-STRING}$ is $NP$-hard, even when the input\ngraphs are apex graphs of girth at least $k$. Here, $PURE-2-DIR$ is the class\nof intersection graphs of axis-parallel line segments (where intersections are\nallowed only between horizontal and vertical segments) and \\textsc{1-STRING} is\nthe class of intersection graphs of simple curves (where two curves share at\nmost one point) in the plane. This partially answers an open question raised by\nKratochv{\\'\\i}l \\& Pergel (2007).\n  Most known $NP$-hardness reductions for these problems are from variants of\n3-SAT. We reduce from the \\textsc{PLANAR HAMILTONIAN PATH COMPLETION} problem,\nwhich uses the more intuitive notion of planarity. As a result, our proof is\nmuch simpler and encapsulates several classes of geometric graphs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 14:04:48 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 03:34:11 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Chakraborty", "Dibyayan", ""], ["Gajjar", "Kshitij", ""]]}, {"id": "2104.09982", "submitter": "David Naccache", "authors": "Leon M\\\"achler, David Naccache", "title": "Explaining the Entombed Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In \\cite{entombed}, John Aycock and Tara Copplestone pose an open question,\nnamely the explanation of the mysterious lookup table used in the Entombed\nGame's Algorithm for two dimensional maze generation. The question attracted\nmedia attention (BBC etc) and was open until today. This paper answers this\nquestion, explains the algorithm and even extends it to three dimensions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 16:07:19 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 22:38:43 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["M\u00e4chler", "Leon", ""], ["Naccache", "David", ""]]}, {"id": "2104.10152", "submitter": "Luk\\'a\\v{s} Vok\\v{r}\\'inek", "authors": "Marek Filakovsk\\'y, Luk\\'a\\v{s} Vok\\v{r}\\'inek", "title": "Computing homotopy classes for diagrams", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm that, given finite simplicial sets $X$, $A$, $Y$ with\nan action of a finite group $G$, computes the set $[X,Y]^A_G$ of homotopy\nclasses of equivariant maps $\\ell \\colon X \\to Y$ extending a given equivariant\nmap $f \\colon A \\to Y$ under the stability assumption $\\dim X^H \\leq 2\n\\operatorname{conn} Y^H$ and $\\operatorname{conn} Y^H \\geq 1$, for all\nsubgroups $H\\leq G$. For fixed $n = \\operatorname{dim} X$, the algorithm runs\nin polynomial time. When the stability condition is dropped, the problem is\nundecidable already in the non-equivariant setting.\n  The algorithm is obtained as a special case of a more general result: For\nfinite diagrams of simplicial sets $X$, $A$, $Y$, i.e. functors\n$\\mathcal{I}^\\mathrm{op} \\to \\mathsf{sSet}$, in the stable range\n$\\operatorname{dim} X \\leq 2 \\operatorname{conn} Y$ and $\\operatorname{conn} Y\n> 1$, we give an algorithm that computes the set $[X, Y]^A$ of homotopy classes\nof maps of diagrams $\\ell \\colon X \\to Y$ extending a given $f \\colon A \\to Y$.\nAgain, for fixed $n = \\dim X$, the running time of the algorithm is polynomial.\n  The algorithm can be utilized to compute homotopy invariants in the\nequivariant setting -- for example, one can algorithmically compute equivariant\nstable homotopy groups. Further, one can apply the result to solve problems\nfrom computational topology, which we showcase on the following Tverberg-type\nproblem: Given a $k$-dimensional simplicial complex $K$, is there a map $K \\to\n\\mathbb{R}^{d}$ without $r$-tuple intersection points? In the metastable range\nof dimensions, $rd \\geq (r+1)k +3$, the result of Mabillard and Wagner shows\nthis problem equivalent to the existence of a particular equivariant map. In\nthis range, our algorithm is applicable and, thus, the $r$-Tverberg problem is\nalgorithmically decidable (in polynomial time when $k$, $d$ and $r$ are fixed).\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:52:25 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Filakovsk\u00fd", "Marek", ""], ["Vok\u0159\u00ednek", "Luk\u00e1\u0161", ""]]}, {"id": "2104.10622", "submitter": "Chenlei Lv", "authors": "Chenlei Lv, Weisi Lin, Baoquan Zhao", "title": "Voxel Structure-based Mesh Reconstruction from a 3D Point Cloud", "comments": "Accepted by IEEE Transactions on Multimedia", "journal-ref": null, "doi": "10.1109/TMM.2021.3073265", "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mesh reconstruction from a 3D point cloud is an important topic in the fields\nof computer graphic, computer vision, and multimedia analysis. In this paper,\nwe propose a voxel structure-based mesh reconstruction framework. It provides\nthe intrinsic metric to improve the accuracy of local region detection. Based\non the detected local regions, an initial reconstructed mesh can be obtained.\nWith the mesh optimization in our framework, the initial reconstructed mesh is\noptimized into an isotropic one with the important geometric features such as\nexternal and internal edges. The experimental results indicate that our\nframework shows great advantages over peer ones in terms of mesh quality,\ngeometric feature keeping, and processing speed.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 16:31:49 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 12:50:41 GMT"}, {"version": "v3", "created": "Fri, 23 Apr 2021 16:55:39 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Lv", "Chenlei", ""], ["Lin", "Weisi", ""], ["Zhao", "Baoquan", ""]]}, {"id": "2104.11046", "submitter": "Teresa Heiss", "authors": "Herbert Edelsbrunner, Teresa Heiss, Vitaliy Kurlin, Philip Smith,\n  Mathijs Wintraecken", "title": "The Density Fingerprint of a Periodic Point Set", "comments": "accepted for SoCG 2021", "journal-ref": "SoCG 2021, 32:1-16", "doi": "10.4230/LIPIcs.SoCG.2021.32", "report-no": null, "categories": "cs.CG math.MG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modeling a crystal as a periodic point set, we present a fingerprint\nconsisting of density functions that facilitates the efficient search for new\nmaterials and material properties. We prove invariance under isometries,\ncontinuity, and completeness in the generic case, which are necessary features\nfor the reliable comparison of crystals. The proof of continuity integrates\nmethods from discrete geometry and lattice theory, while the proof of generic\ncompleteness combines techniques from geometry with analysis. The fingerprint\nhas a fast algorithm based on Brillouin zones and related inclusion-exclusion\nformulae. We have implemented the algorithm and describe its application to\ncrystal structure prediction.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 13:23:32 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Edelsbrunner", "Herbert", ""], ["Heiss", "Teresa", ""], ["Kurlin", "Vitaliy", ""], ["Smith", "Philip", ""], ["Wintraecken", "Mathijs", ""]]}, {"id": "2104.11214", "submitter": "Bei Wang", "authors": "Youjia Zhou, Archit Rathore, Emilie Purvine, Bei Wang", "title": "Topological Simplifications of Hypergraphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study hypergraph visualization via its topological simplification. We\nexplore both vertex simplification and hyperedge simplification of hypergraphs\nusing tools from topological data analysis. In particular, we transform a\nhypergraph to its graph representations known as the line graph and clique\nexpansion. A topological simplification of such a graph representation induces\na simplification of the hypergraph. In simplifying a hypergraph, we allow\nvertices to be combined if they belong to almost the same set of hyperedges,\nand hyperedges to be merged if they share almost the same set of vertices. Our\nproposed approaches are general, mathematically justifiable, and they put\nvertex simplification and hyperedge simplification in a unifying framework.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:49:36 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Zhou", "Youjia", ""], ["Rathore", "Archit", ""], ["Purvine", "Emilie", ""], ["Wang", "Bei", ""]]}, {"id": "2104.11420", "submitter": "Arun Kumar Das", "authors": "Sergio Cabello and Arun Kumar Das and Sandip Das and Joydeep Mukherjee", "title": "Finding a Largest-Area Triangle in a Terrain in Near-Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A terrain is an $x$-monotone polygon whose lower boundary is a single line\nsegment. We present an algorithm to find in a terrain a triangle of largest\narea in $O(n \\log n)$ time, where $n$ is the number of vertices defining the\nterrain. The best previous algorithm for this problem has a running time of\n$O(n^2)$.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 05:35:47 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Cabello", "Sergio", ""], ["Das", "Arun Kumar", ""], ["Das", "Sandip", ""], ["Mukherjee", "Joydeep", ""]]}, {"id": "2104.11618", "submitter": "Pascal Lenzner", "authors": "Wilhelm Friedemann, Tobias Friedrich, Hans Gawendowicz, Pascal\n  Lenzner, Anna Melnichenko, Jannik Peters, Daniel Stephan, Michael Vaichenker", "title": "Efficiency and Stability in Euclidean Network Design", "comments": "To appear at the 33rd ACM Symposium on Paralellism in Algorithms and\n  Architectures (SPAA), full version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network Design problems typically ask for a minimum cost sub-network from a\ngiven host network. This classical point-of-view assumes a central authority\nenforcing the optimum solution. But how should networks be designed to cope\nwith selfish agents that own parts of the network? In this setting, agents will\ndeviate from a minimum cost network if this decreases their individual cost.\nHence, designed networks should be both efficient in terms of total cost and\nstable in terms of the agents' willingness to accept the network.\n  We study this novel type of Network Design problem by investigating the\ncreation of $(\\beta,\\gamma)$-networks, that are in $\\beta$-approximate Nash\nequilibrium and have a total cost of at most $\\gamma$ times the optimal cost,\nfor the recently proposed Euclidean Generalized Network Creation Game by Bil\\`o\net al. [SPAA 2019]. There, $n$ agents corresponding to points in Euclidean\nspace create costly edges among themselves to optimize their centrality in the\ncreated network. Our main result is a simple $\\mathcal{O}(n^2)$-time algorithm\nthat computes a $(\\beta,\\beta)$-network with low $\\beta$ for any given set of\npoints. Moreover, on integer grid point sets or random point sets our algorithm\nachieves a low constant $\\beta$. Besides these results, we discuss a\ngeneralization of our algorithm to instances with arbitrary, even non-metric,\nedge lengths. Moreover, we show that no such positive results are possible when\nfocusing on either optimal networks or perfectly stable networks as in both\ncases NP-hard problems arise, there exist instances with very unstable optimal\nnetworks, and there are instances for perfectly stable networks with high total\ncost. Along the way, we significantly improve several results from Bil\\`o et\nal. and we asymptotically resolve their conjecture about the Price of Anarchy\nby providing a tight bound.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 14:12:13 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Friedemann", "Wilhelm", ""], ["Friedrich", "Tobias", ""], ["Gawendowicz", "Hans", ""], ["Lenzner", "Pascal", ""], ["Melnichenko", "Anna", ""], ["Peters", "Jannik", ""], ["Stephan", "Daniel", ""], ["Vaichenker", "Michael", ""]]}, {"id": "2104.12141", "submitter": "Abhinandan Nath", "authors": "Abhinandan Nath", "title": "Coresets for $k$-median clustering under Fr\\'{e}chet and Hausdorff\n  distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give algorithms for computing coresets for $(1+\\varepsilon)$-approximate\n$k$-median clustering of polygonal curves (under the discrete and continuous\nFr\\'{e}chet distance) and point sets (under the Hausdorff distance), when the\ncluster centers are restricted to be of low complexity. Ours is the first such\nresult, where the size of the coreset is independent of the number of input\ncurves/point sets to be clustered (although it still depends on the maximum\ncomplexity of each input object). Specifically, the size of the coreset is\n$\\Theta\\left(\\frac{k^3lm^{\\delta}d}{\\varepsilon^2}\\log\\left(\n\\frac{kl}{\\varepsilon}\\right)\\right)$ for any $\\delta > 0$, where $d$ is the\nambient dimension, $m$ is the maximum number of points in an input curve/point\nset, and $l$ is the maximum number of points allowed in a cluster center. We\nformally characterize a general condition on the restricted space of cluster\ncenters -- this helps us to generalize and apply the importance sampling\nframework, that was used by Langberg and Schulman for computing coresets for\n$k$-median clustering of $d$-dimensional points on normed spaces in\n$\\mathbb{R}^d$, to the problem of clustering curves and point sets using the\nFr\\'{e}chet and Hausdorff metrics. Roughly, the condition places an upper bound\non the number of different combinations of metric balls that the restricted\nspace of cluster centers can hit. We also derive lower bounds on the size of\nthe coreset, given the restriction that the coreset must be a subset of the\ninput objects.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 12:27:05 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Nath", "Abhinandan", ""]]}, {"id": "2104.12285", "submitter": "Matthew Piekenbrock", "authors": "Matt Piekenbrock and Jose A. Perea", "title": "Move Schedules: Fast persistence computations in sparse dynamic settings", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.AT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The standard procedure for computing the persistent homology of a filtered\nsimplicial complex is the matrix reduction algorithm. Its output is a\nparticular decomposition of the total boundary matrix, from which the\npersistence diagrams and generating cycles can be derived. Persistence diagrams\nare known to vary continuously with respect to their input; this motivates the\nalgorithmic study of persistence computations for time-varying filtered\ncomplexes. Computationally, simulating persistence dynamically can be reduced\nto maintaining a valid decomposition under adjacent transpositions in the\nfiltration order. In practice, the quadratic scaling in the number of\ntranspositions often makes this maintenance procedure slower than simply\ncomputing the decomposition from scratch, effectively limiting the application\nof dynamic persistence to relatively small data sets. In this work, we propose\na coarser strategy for maintaining the decomposition over a discrete\n1-parameter family of filtrations. Our first result is an analysis of a simple\nlinear-time strategy for reducing the number of column operations needed to\nsimulate persistence across a fixed homotopy by at most a factor of 2. We then\nshow a modification of this technique which maintains only a sublinear number\nof valid states, as opposed to a quadratic number of states, and we provide\ntight lower bounds for this technique. Finally, we provide empirical results\nsuggesting that the decrease in operations needed to compute diagrams across a\nfamily of filtrations is proportional to the difference between the expected\nquadratic number of states, and the proposed sublinear coarsening.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 23:06:57 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Piekenbrock", "Matt", ""], ["Perea", "Jose A.", ""]]}, {"id": "2104.12458", "submitter": "Thomas Fernique", "authors": "Thomas Fernique and Daria Pchelina", "title": "Compact Packings are not always the Densest", "comments": "2 pages, SageMath code included in source (file 110.sage)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We provide a counterexample to a conjecture by B. Connelly about density of\ncircle packings\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 10:41:54 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 16:02:25 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Fernique", "Thomas", ""], ["Pchelina", "Daria", ""]]}, {"id": "2104.12654", "submitter": "Natan Rubin", "authors": "Natan Rubin", "title": "Stronger Bounds for Weak Epsilon-Nets in Higher Dimensions", "comments": "Preliminary version accepted to STOC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a finite point set $P$ in ${\\mathbb R}^d$, and $\\epsilon>0$ we say that\n$N\\subseteq {\\mathbb R}^d$ is a weak $\\epsilon$-net if it pierces every convex\nset $K$ with $|K\\cap P|\\geq \\epsilon |P|$.\n  Let $d\\geq 3$. We show that for any finite point set in ${\\mathbb R}^d$, and\nany $\\epsilon>0$, there exist a weak $\\epsilon$-net of cardinality\n$\\displaystyle O\\left(\\frac{1}{\\epsilon^{d-1/2+\\gamma}}\\right)$, where\n$\\gamma>0$ is an arbitrary small constant.\n  This is the first improvement of the bound of $\\displaystyle\nO^*\\left(\\frac{1}{\\epsilon^d}\\right)$ that was obtained in 1994 by Chazelle,\nEdelsbrunner, Grigni, Guibas, Sharir, and Welzl for general point sets in\ndimension $d\\geq 3$.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 15:31:08 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Rubin", "Natan", ""]]}, {"id": "2104.13174", "submitter": "Dan Reznik", "authors": "Daniel Jaud and Dan Reznik and Ronaldo Garcia", "title": "Poncelet Plectra: Harmonious Properties of Cosine Space", "comments": "13 pages, 11 figures, 5 video links", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.CG cs.RO math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poncelet N-periodics in a confocal pair (elliptic billiard) conserve the same\nsum of cosines as their affine image with fixed incircle. For N=3, the vector\nof cosines in either family sweep the same planar curve: an equilateral cubic\nresembling a plectrum (guitar pick). We also show that the family of excentral\ntriangles to the confocal family conserves the same product of cosines as its\naffine image with fixed circumcircle. In cosine space cosine triples in either\nfamily sweep the same spherical curve. The associated planar curve in log\ncosine space is also plectrum-shaped, though rounder than the one swept by its\nparent confocal family.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 13:33:05 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 17:42:29 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Jaud", "Daniel", ""], ["Reznik", "Dan", ""], ["Garcia", "Ronaldo", ""]]}, {"id": "2104.13430", "submitter": "Tao Hou", "authors": "Anand V. Patel, Tao Hou, Juan D. Beltran Rodriguez, Tamal K. Dey,\n  Dunbar P. Birnie III", "title": "3D Microstructure Segmentation by Topological Persistence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tomography is a widely used tool for analyzing microstructures in three\ndimensions (3D). The analysis, however, faces difficulty because the\nconstituent materials produce similar grey-scale values. Sometimes, this\nprompts the image segmentation process to assign a pixel/voxel to the wrong\nphase (active material or pore). Consequently, errors are introduced in the\nmicrostructure characteristics calculation. In this work we develop a filtering\nalgorithm based on topological persistence, a technique used in topological\ndata analysis. One problem faced when evaluating filtering algorithms is that\nreal image data in general are not equipped with the 'ground truth' information\nabout the microstructure characteristics. For this study, we construct\nsynthetic images for which the ground truth values are known. Specifically, we\ncompare interconnected pore tortuosity and phase fraction. Experimental results\nshow that our filtering algorithm provides a significant improvement in\nreproducing tortuosity close to the ground truth, even when the grey-scale\nvalues of the phases are similar.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 19:02:03 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Patel", "Anand V.", ""], ["Hou", "Tao", ""], ["Rodriguez", "Juan D. Beltran", ""], ["Dey", "Tamal K.", ""], ["Birnie", "Dunbar P.", "III"]]}, {"id": "2104.13478", "submitter": "Petar Veli\\v{c}kovi\\'c", "authors": "Michael M. Bronstein, Joan Bruna, Taco Cohen, Petar Veli\\v{c}kovi\\'c", "title": "Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges", "comments": "156 pages. Work in progress -- comments welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has witnessed an experimental revolution in data science and\nmachine learning, epitomised by deep learning methods. Indeed, many\nhigh-dimensional learning tasks previously thought to be beyond reach -- such\nas computer vision, playing Go, or protein folding -- are in fact feasible with\nappropriate computational scale. Remarkably, the essence of deep learning is\nbuilt from two simple algorithmic principles: first, the notion of\nrepresentation or feature learning, whereby adapted, often hierarchical,\nfeatures capture the appropriate notion of regularity for each task, and\nsecond, learning by local gradient-descent type methods, typically implemented\nas backpropagation.\n  While learning generic functions in high dimensions is a cursed estimation\nproblem, most tasks of interest are not generic, and come with essential\npre-defined regularities arising from the underlying low-dimensionality and\nstructure of the physical world. This text is concerned with exposing these\nregularities through unified geometric principles that can be applied\nthroughout a wide spectrum of applications.\n  Such a 'geometric unification' endeavour, in the spirit of Felix Klein's\nErlangen Program, serves a dual purpose: on one hand, it provides a common\nmathematical framework to study the most successful neural network\narchitectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand,\nit gives a constructive procedure to incorporate prior physical knowledge into\nneural architectures and provide principled way to build future architectures\nyet to be invented.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 21:09:51 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 16:16:03 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Bronstein", "Michael M.", ""], ["Bruna", "Joan", ""], ["Cohen", "Taco", ""], ["Veli\u010dkovi\u0107", "Petar", ""]]}, {"id": "2104.13499", "submitter": "Sima Hajiaghaei Shanjani", "authors": "Sima Hajiaghaei Shanjani and Valerie King", "title": "Communication Costs in a Geometric Communication Network", "comments": "A version of this work was submitted to ICDCN 2021", "journal-ref": null, "doi": "10.1145/3427796.3427800", "report-no": null, "categories": "cs.DC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A communication network is a graph in which each node has only local\ninformation about the graph and nodes communicate by passing messages along its\nedges. Here, we consider the {\\it geometric communication network} where the\nnodes also occupy points in space and the distance between points is the\nEuclidean distance. Our goal is to understand the communication cost needed to\nsolve several fundamental geometry problems, including Convex Hull, Diameter,\nClosest Pair, and approximations of these problems, in the asynchronous CONGEST\nKT1 model. This extends the 2011 result of Rajsbaum and Urrutia for finding a\nconvex hull of a planar geometric communication network to networks of\narbitrary topology.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 22:53:03 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Shanjani", "Sima Hajiaghaei", ""], ["King", "Valerie", ""]]}, {"id": "2104.14476", "submitter": "Yiming Zhao", "authors": "Haitao Wang and Yiming Zhao", "title": "Reverse Shortest Path Problem for Unit-Disk Graphs", "comments": "A preliminary version to appear in WADS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set P of n points in the plane, a unit-disk graph G_{r}(P) with\nrespect to a radius r is an undirected graph whose vertex set is P such that an\nedge connects two points p, q \\in P if the Euclidean distance between p and q\nis at most r. The length of any path in G_r(P) is the number of edges of the\npath. Given a value \\lambda>0 and two points s and t of P, we consider the\nfollowing reverse shortest path problem: finding the smallest r such that the\nshortest path length between s and t in G_r(P) is at most \\lambda. It was known\npreviously that the problem can be solved in O(n^{4/3} \\log^3 n) time. In this\npaper, we present an algorithm of O(\\lfloor \\lambda \\rfloor \\cdot n \\log n)\ntime and another algorithm of O(n^{5/4} \\log^2 n) time.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 16:38:35 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Wang", "Haitao", ""], ["Zhao", "Yiming", ""]]}, {"id": "2104.14680", "submitter": "Haitao Wang", "authors": "Logan Pedersen and Haitao Wang", "title": "Algorithms for the Line-Constrained Disk Coverage and Related Problems", "comments": "A preliminary version to appear in WADS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $P$ of $n$ points and a set $S$ of $m$ weighted disks in the\nplane, the disk coverage problem asks for a subset of disks of minimum total\nweight that cover all points of $P$. The problem is NP-hard. In this paper, we\nconsider a line-constrained version in which all disks are centered on a line\n$L$ (while points of $P$ can be anywhere in the plane). We present an\n$O((m+n)\\log(m+n)+\\kappa\\log m)$ time algorithm for the problem, where $\\kappa$\nis the number of pairs of disks that intersect. Alternatively, we can also\nsolve the problem in $O(nm\\log(m+n))$ time. For the unit-disk case where all\ndisks have the same radius, the running time can be reduced to\n$O((n+m)\\log(m+n))$. In addition, we solve in $O((m+n)\\log(m+n))$ time the\n$L_{\\infty}$ and $L_1$ cases of the problem, in which the disks are squares and\ndiamonds, respectively. As a by-product, the 1D version of the problem where\nall points of $P$ are on $L$ and the disks are line segments on $L$ is also\nsolved in $O((m+n)\\log(m+n))$ time. We also show that the problem has an\n$\\Omega((m+n)\\log (m+n))$ time lower bound even for the 1D case.\n  We further demonstrate that our techniques can also be used to solve other\ngeometric coverage problems. For example, given in the plane a set $P$ of $n$\npoints and a set $S$ of $n$ weighted half-planes, we solve in $O(n^4\\log n)$\ntime the problem of finding a subset of half-planes to cover $P$ so that their\ntotal weight is minimized. This improves the previous best algorithm of\n$O(n^5)$ time by almost a linear factor. If all half-planes are lower ones,\nthen our algorithm runs in $O(n^2\\log n)$ time, which improves the previous\nbest algorithm of $O(n^4)$ time by almost a quadratic factor.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 22:19:33 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Pedersen", "Logan", ""], ["Wang", "Haitao", ""]]}]