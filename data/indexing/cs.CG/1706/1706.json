[{"id": "1706.00091", "submitter": "Roel Apfelbaum", "authors": "Roel Apfelbaum", "title": "The Constant of Proportionality in Lower Bound Constructions of\n  Point-Line Incidences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $I(n,l)$ denote the maximum possible number of incidences between $n$\npoints and $l$ lines. It is well known that $I(n,l) = \\Theta(n^{2/3}l^{2/3} + n\n+ l)$. Let $c_{\\mathrm{SzTr}}$ denote the lower bound on the constant of\nproportionality of the $n^{2/3}l^{2/3}$ term. The known lower bound, due to\nElekes, is $c_{\\mathrm{SzTr}} \\ge 2^{-2/3} = 0.63$. With a slight modification\nof Elekes' construction, we show that it can give a better lower bound of\n$c_{\\mathrm{SzTr}} \\ge 1$, i.e., $I(n,l) \\ge n^{2/3}l^{2/3}$. Furthermore, we\nanalyze a different construction given by Erd{\\H o}s, and show its constant of\nproportionality to be even better, $c_{\\mathrm{SzTr}} \\ge 3/(2^{1/3}\\pi^{2/3})\n\\approx 1.11$.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 21:14:15 GMT"}, {"version": "v2", "created": "Sat, 15 Jul 2017 23:47:13 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Apfelbaum", "Roel", ""]]}, {"id": "1706.00204", "submitter": "Mathieu Carri\\`ere", "authors": "Mathieu Carri\\`ere and Bertrand Michel and Steve Oudot", "title": "Statistical Analysis and Parameter Selection for Mapper", "comments": "Minor modifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.AT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study the question of the statistical convergence of the\n1-dimensional Mapper to its continuous analogue, the Reeb graph. We show that\nthe Mapper is an optimal estimator of the Reeb graph, which gives, as a\nbyproduct, a method to automatically tune its parameters and compute confidence\nregions on its topological features, such as its loops and flares. This allows\nto circumvent the issue of testing a large grid of parameters and keeping the\nmost stable ones in the brute-force setting, which is widely used in\nvisualization, clustering and feature selection with the Mapper.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 08:34:01 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 13:36:23 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Carri\u00e8re", "Mathieu", ""], ["Michel", "Bertrand", ""], ["Oudot", "Steve", ""]]}, {"id": "1706.00380", "submitter": "Peter Franek", "authors": "Marek Filakovsky, Peter Franek, Uli Wagner, Stephan Zhechev", "title": "Computing simplicial representatives of homotopy group elements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central problem of algebraic topology is to understand the homotopy groups\n$\\pi_d(X)$ of a topological space $X$. For the computational version of the\nproblem, it is well known that there is no algorithm to decide whether the\nfundamental group $\\pi_1(X)$ of a given finite simplicial complex $X$ is\ntrivial. On the other hand, there are several algorithms that, given a finite\nsimplicial complex $X$ that is simply connected (i.e., with $\\pi_1(X)$\ntrivial), compute the higher homotopy group $\\pi_d(X)$ for any given $d\\geq 2$.\n%The first such algorithm was given by Brown, and more recently, \\v{C}adek et\nal.\n  However, these algorithms come with a caveat: They compute the isomorphism\ntype of $\\pi_d(X)$, $d\\geq 2$ as an \\emph{abstract} finitely generated abelian\ngroup given by generators and relations, but they work with very implicit\nrepresentations of the elements of $\\pi_d(X)$. Converting elements of this\nabstract group into explicit geometric maps from the $d$-dimensional sphere\n$S^d$ to $X$ has been one of the main unsolved problems in the emerging field\nof computational homotopy theory.\n  Here we present an algorithm that, given a~simply connected space $X$,\ncomputes $\\pi_d(X)$ and represents its elements as simplicial maps from a\nsuitable triangulation of the $d$-sphere $S^d$ to $X$. For fixed $d$, the\nalgorithm runs in time exponential in $size(X)$, the number of simplices of\n$X$. Moreover, we prove that this is optimal: For every fixed $d\\geq 2$, we\nconstruct a family of simply connected spaces $X$ such that for any simplicial\nmap representing a generator of $\\pi_d(X)$, the size of the triangulation of\n$S^d$ on which the map is defined, is exponential in $size(X)$.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 16:35:12 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 13:50:45 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Filakovsky", "Marek", ""], ["Franek", "Peter", ""], ["Wagner", "Uli", ""], ["Zhechev", "Stephan", ""]]}, {"id": "1706.01344", "submitter": "Pierre-Alexandre Beaufort Ir", "authors": "Pierre-Alexandre Beaufort, Christos Georgiadis Jonathan Lambrechts,\n  Fran\\c{c}ois Henrotte, Christophe Geuzaine, Jean-Fran\\c{c}ois Remacle", "title": "Computing cross fields -- A PDE approach based on the Ginzburg-Landau\n  theory", "comments": "Promoted version of: Proceeding for the 26th International Meshing\n  Roundtable, IMR26, 18-21 September 2017, Barcelona, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method to compute crossfields based on the\nGinzburg-Landau theory. The Ginzburg-Landau functional has two terms: the\nDirichlet energy of the distribution and a term penalizing the mismatch between\nthe fixed and actual norm of the distribution. Directional fields on surfaces\nare known to have a number of critical points, which are properly identified\nwith the Ginzburg-Landau approach: the asymptotic behavior of Ginzburg-Landau\nproblem provides well-distributed critical points over the 2-manifold, whose\nindices are as low as possible. The central idea in this paper is to exploit\nthis theoretical background for crossfield computation on arbitrary surfaces.\nSuch crossfields are instrumental in the generation of meshes with quadrangular\nelements. The relation between the topological properties of quadrangular\nmeshes and crossfields are hence first recalled. It is then shown that a\ncrossfield on a surface can be represented by a complex function of unit norm\nwith a number of critical points, i.e., a nearly everywhere smooth function\ntaking its values in the unit circle of the complex plane. As maximal\nsmoothness of the crossfield is equivalent with minimal energy, the crossfield\nproblem is equivalent to an optimization problem based on Ginzburg-Landau\nfunctional. A discretization scheme with Crouzeix-Raviart elements is applied\nand the correctness of the resulting finite element formulation is validated on\nthe unit disk by comparison with an analytical solution. The method is also\napplied to the 2-sphere where, surprisingly but rightly, the computed critical\npoints are not located at the vertices of a cube, but at those of an anticube.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 13:49:39 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 08:07:02 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 14:59:02 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Beaufort", "Pierre-Alexandre", ""], ["Lambrechts", "Christos Georgiadis Jonathan", ""], ["Henrotte", "Fran\u00e7ois", ""], ["Geuzaine", "Christophe", ""], ["Remacle", "Jean-Fran\u00e7ois", ""]]}, {"id": "1706.01577", "submitter": "Luiz C. B. da Silva Dr.", "authors": "Luiz C. B. da Silva", "title": "Characterization of Spherical and Plane Curves Using Rotation Minimizing\n  Frames", "comments": "8 pages. This version is an improvement of the previous one. In\n  addition to a study of some properties of plane and spherical curves, it\n  contains a characterization of Bertrand curves in terms of the so-called\n  natural mates", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DG cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study plane and spherical curves in Euclidean and\nLorentz-Minkowski 3-spaces by employing rotation minimizing (RM) frames. By\nconveniently writing the curvature and torsion for a curve on a sphere, we show\nhow to find the angle between the principal normal and an RM vector field for\nspherical curves. Later, we characterize plane and spherical curves as curves\nwhose position vector lies, up to a translation, on a moving plane spanned by\ntheir unit tangent and an RM vector field. Finally, as an application, we\ncharacterize Bertrand curves as curves whose so-called natural mates are\nspherical.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 01:28:15 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 23:22:22 GMT"}, {"version": "v3", "created": "Sat, 24 Jun 2017 23:53:44 GMT"}, {"version": "v4", "created": "Mon, 5 Aug 2019 17:39:07 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["da Silva", "Luiz C. B.", ""]]}, {"id": "1706.01613", "submitter": "Amaury Johnen", "authors": "Amaury Johnen, Jean-Christophe Weill, Jean-Fran\\c{c}ois Remacle", "title": "Robust and efficient validation of the linear hexahedral element", "comments": "13 pages, 7 figures. Submitted to the 26th International Meshing\n  Roundtable conference. V2: removed Appendix \"Derivatives of the Jacobian\n  determinant of a linear hexahedron\" and update acknowledgements. V3:\n  modifications in abstract, introduction and conclusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Checking mesh validity is a mandatory step before doing any finite element\nanalysis. If checking the validity of tetrahedra is trivial, checking the\nvalidity of hexahedral elements is far from being obvious. In this paper, a\nmethod that robustly and efficiently compute the validity of standard linear\nhexahedral elements is presented. This method is a significant improvement of a\nprevious work on the validity of curvilinear elements. The new implementation\nis simple and computationally efficient. The key of the algorithm is still to\ncompute B\\'ezier coefficients of the Jacobian determinant. We show that only 20\nJacobian determinants are necessary to compute the 27 B\\'ezier coefficients.\nThose 20 Jacobians can be efficiently computed by calculating the volume of 20\ntetrahedra. The new implementation is able to check the validity of about 6\nmillion hexahedra per second on one core of a personal computer. Through the\npaper, all the necessary information is provided that allow to easily reproduce\nthe results, \\ie write a simple code that takes the coordinates of 8 points as\ninput and outputs the validity of the hexahedron.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 06:14:10 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 09:07:35 GMT"}, {"version": "v3", "created": "Mon, 7 Aug 2017 13:49:16 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Johnen", "Amaury", ""], ["Weill", "Jean-Christophe", ""], ["Remacle", "Jean-Fran\u00e7ois", ""]]}, {"id": "1706.02004", "submitter": "Sariel Har-Peled", "authors": "Sariel Har-Peled and Mitchell Jones", "title": "On Separating Points by Lines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $P$ of $n$ points in the plane, its separability is the minimum\nnumber of lines needed to separate all its pairs of points from each other. We\nshow that the minimum number of lines needed to separate $n$ points, picked\nrandomly (and uniformly) in the unit square, is $\\Theta( n^{2/3})$, where\n$\\Theta$ hides polylogarithmic factors. In addition, we provide a fast\napproximation algorithm for computing the separability of a given point set in\nthe plane. Finally, we point out the connection between separability and\npartitions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 23:16:28 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Har-Peled", "Sariel", ""], ["Jones", "Mitchell", ""]]}, {"id": "1706.02028", "submitter": "Saeed Mehrabi", "authors": "Therese Biedl and Saeed Mehrabi", "title": "On Guarding Orthogonal Polygons with Bounded Treewidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exist many variants of guarding an orthogonal polygon in an orthogonal\nfashion: sometimes a guard can see an entire rectangle, or along a staircase,\nor along an orthogonal path with at most $k$ bends. In this paper, we study all\nthese guarding models in the special case of orthogonal polygons that have\nbounded treewidth in some sense. Exploiting algorithms for graphs of bounded\ntreewidth, we show that the problem of finding the minimum number of guards in\nthese models becomes linear-time solvable in orthogonal polygons of bounded\ntreewidth.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 02:16:49 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Biedl", "Therese", ""], ["Mehrabi", "Saeed", ""]]}, {"id": "1706.02236", "submitter": "Christos Georgiadis Mr", "authors": "Christos Georgiadis, Pierre-Alexandre Beaufort, Jonathan Lambrechts,\n  Jean-Fran\\c{c}ois Remacle", "title": "High quality mesh generation using cross and asterisk fields:\n  Application on coastal domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method to generate high quality triangular or\nquadrilateral meshes that uses direction fields and a frontal point insertion\nstrategy. Two types of direction fields are considered: asterisk fields and\ncross fields. With asterisk fields we generate high quality triangulations,\nwhile with cross fields we generate right-angled triangulations that are\noptimal for transformation to quadrilateral meshes. The input of our algorithm\nis an initial triangular mesh and a direction field calculated on it. The goal\nis to compute the vertices of the final mesh by an advancing front strategy\nalong the direction field. We present an algorithm that enables to efficiently\ngenerate the points using solely information from the base mesh. A\nmulti-threaded implementation of our algorithm is presented, allowing us to\nachieve significant speedup of the point generation. Regarding the\nquadrangulation process, we develop a quality criterion for right-angled\ntriangles with respect to the local cross field and an optimization process\nbased on it. Thus we are able to further improve the quality of the output\nquadrilaterals. The algorithm is demonstrated on the sphere and examples of\nhigh quality triangular and quadrilateral meshes of coastal domains are\npresented.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 16:10:19 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Georgiadis", "Christos", ""], ["Beaufort", "Pierre-Alexandre", ""], ["Lambrechts", "Jonathan", ""], ["Remacle", "Jean-Fran\u00e7ois", ""]]}, {"id": "1706.02619", "submitter": "Tam\\'as R\\'obert Mezei", "authors": "Ervin Gy\\H{o}ri and Tam\\'as R\\'obert Mezei", "title": "Mobile vs. point guards", "comments": "This version covers a previously missing case in both Phase 2 & 3", "journal-ref": "Discrete & Computational Geometry, March 2019, Volume 61, Issue 2,\n  pp 421-451", "doi": "10.1007/s00454-018-9996-x", "report-no": null, "categories": "math.CO cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of guarding orthogonal art galleries with horizontal\nmobile guards (alternatively, vertical) and point guards, using \"rectangular\nvision\". We prove a sharp bound on the minimum number of point guards required\nto cover the gallery in terms of the minimum number of vertical mobile guards\nand the minimum number of horizontal mobile guards required to cover the\ngallery. Furthermore, we show that the latter two numbers can be calculated in\nlinear time.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 14:51:19 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 16:37:32 GMT"}, {"version": "v3", "created": "Fri, 23 Nov 2018 15:52:58 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Gy\u0151ri", "Ervin", ""], ["Mezei", "Tam\u00e1s R\u00f3bert", ""]]}, {"id": "1706.02939", "submitter": "Kyle Fox", "authors": "Pankaj K. Agarwal, Kyle Fox, Oren Salzman", "title": "An Efficient Algorithm for Computing High-Quality Paths amid Polygonal\n  Obstacles", "comments": "A preliminary version of this work appear in the Proceedings of the\n  27th Annual ACM-SIAM Symposium on Discrete Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a path-planning problem amid a set $\\mathcal{O}$ of obstacles in\n$\\mathbb{R}^2$, in which we wish to compute a short path between two points\nwhile also maintaining a high clearance from $\\mathcal{O}$; the clearance of a\npoint is its distance from a nearest obstacle in $\\mathcal{O}$. Specifically,\nthe problem asks for a path minimizing the reciprocal of the clearance\nintegrated over the length of the path. We present the first polynomial-time\napproximation scheme for this problem. Let $n$ be the total number of obstacle\nvertices and let $\\varepsilon \\in (0,1]$. Our algorithm computes in time\n$O(\\frac{n^2}{\\varepsilon ^2} \\log \\frac{n}{\\varepsilon})$ a path of total cost\nat most $(1+\\varepsilon)$ times the cost of the optimal path.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 13:11:37 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Agarwal", "Pankaj K.", ""], ["Fox", "Kyle", ""], ["Salzman", "Oren", ""]]}, {"id": "1706.03049", "submitter": "Yoav Kallus", "authors": "Yoav Kallus", "title": "A linear-time algorithm for the maximum-area inscribed triangle in a\n  convex polygon", "comments": "Eprint source includes C++ implementation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the n vertices of a convex polygon in cyclic order, can the triangle of\nmaximum area inscribed in P be determined by an algorithm with O(n) time\ncomplexity? A purported linear-time algorithm by Dobkin and Snyder from 1979\nhas recently been shown to be incorrect by Keikha, L\\\"offler, Urhausen, and van\nder Hoog. These authors give an alternative algorithm with O(n log n) time\ncomplexity. Here we give an algorithm with linear time complexity.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 17:17:47 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Kallus", "Yoav", ""]]}, {"id": "1706.03263", "submitter": "Sharath Raghvendra", "authors": "Sharath Raghvendra and Mari\\\"ette C. Wessels", "title": "A Grid-Based Approximation Algorithm for the Minimum Weight\n  Triangulation Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of $n$ points on a plane, in the Minimum Weight Triangulation\nproblem, we wish to find a triangulation that minimizes the sum of Euclidean\nlength of its edges. This incredibly challenging problem has been studied for\nmore than four decades and has been only recently shown to be NP-Hard. In this\npaper we present a novel polynomial-time algorithm that computes a\n$14$-approximation of the minimum weight triangulation -- a constant that is\nsignificantly smaller than what has been previously known.\n  In our algorithm, we use grids to partition the edges into levels where\nshorter edges appear at smaller levels and edges with similar lengths appear at\nthe same level. We then triangulate the point set incrementally by introducing\nedges in increasing order of their levels. We introduce the edges of any level\n$i+1$ in two steps. In the first step, we add edges using a variant of the\nwell-known ring heuristic to generate a partial triangulation\n$\\hat{\\mathcal{A}}_i$. In the second step, we greedily add non-intersecting\nlevel $i+1$ edges to $\\hat{\\mathcal{A}}_i$ in increasing order of their length\nand obtain a partial triangulation $\\mathcal{A}_{ i+1}$. The ring heuristic is\nknown to yield only an $O(\\log n)$-approximation even for a convex polygon and\nthe greedy heuristic achieves only a $\\Theta(\\sqrt{n})$-approximation.\nTherefore, it is surprising that their combination leads to an improved\napproximation ratio of $14$.\n  For the proof, we identify several useful properties of $\\hat{\\mathcal{A}}_i$\nand combine it with a new Euler characteristic based technique to show that\n$\\hat{\\mathcal{A}}_i$ has more edges than $\\mathcal{T}_i$; here $\\mathcal{T}_i$\nis the partial triangulation consisting of level $\\le i$ edges of some minimum\nweight triangulation. We then use a simple greedy stays ahead proof strategy to\nbound the approximation ratio.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 18:03:48 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Raghvendra", "Sharath", ""], ["Wessels", "Mari\u00ebtte C.", ""]]}, {"id": "1706.03268", "submitter": "Bogdan Armaselu", "authors": "Bogdan Armaselu, Ovidiu Daescu", "title": "Maximum Area Rectangle Separating Red and Blue Points", "comments": "14 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a set R of n red points and a set B of m blue points, we study the\nproblem of finding a rectangle that contains all the red points, the minimum\nnumber of blue points and has the largest area. We call such rectangle a\nmaximum separating rectangle. We address the planar, axis-aligned (2D) version,\nand present an O(mlogm+n) time, O(m+n) space algorithm. The running time\nreduces to O(m + n) if the points are pre-sorted by one of the coordinates. We\nfurther prove that our algorithm is optimal in the decision model of\ncomputation.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 18:48:18 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Armaselu", "Bogdan", ""], ["Daescu", "Ovidiu", ""]]}, {"id": "1706.03358", "submitter": "Mathieu Carri\\`ere", "authors": "Mathieu Carri\\`ere and Marco Cuturi and Steve Oudot", "title": "Sliced Wasserstein Kernel for Persistence Diagrams", "comments": "Minor modifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistence diagrams (PDs) play a key role in topological data analysis\n(TDA), in which they are routinely used to describe topological properties of\ncomplicated shapes. PDs enjoy strong stability properties and have proven their\nutility in various learning contexts. They do not, however, live in a space\nnaturally endowed with a Hilbert structure and are usually compared with\nspecific distances, such as the bottleneck distance. To incorporate PDs in a\nlearning pipeline, several kernels have been proposed for PDs with a strong\nemphasis on the stability of the RKHS distance w.r.t. perturbations of the PDs.\nIn this article, we use the Sliced Wasserstein approximation SW of the\nWasserstein distance to define a new kernel for PDs, which is not only provably\nstable but also provably discriminative (depending on the number of points in\nthe PDs) w.r.t. the Wasserstein distance $d_1$ between PDs. We also demonstrate\nits practicality, by developing an approximation technique to reduce kernel\ncomputation time, and show that our proposal compares favorably to existing\nkernels for PDs on several benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 14:47:19 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 08:44:30 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 14:47:06 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Carri\u00e8re", "Mathieu", ""], ["Cuturi", "Marco", ""], ["Oudot", "Steve", ""]]}, {"id": "1706.04549", "submitter": "James  Peters Ph.D.", "authors": "M.Z. Ahmad, J.F. Peters", "title": "Delta Complexes in Digital Images. Approximating Image Object Shapes", "comments": "20 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a computational topology of digital images, simplexes are replaced by\nDelta sets in approximating image object shapes. For simplicity, simplexes and\nDelta sets are restricted to the Euclidean plane. A planar simplex is either a\nvertex, a line segment or a filled triangle. In this study of image shapes, a\nplanar Delta set is a sequence of ordered simplicial complexes. The basic\napproach is to approximate an image shape by decomposing an image region\ncontaining the shape into combinations of Delta sets called Delta complexes.\nThis approach to image shapes is motivated by the ease with which shapes\ncovered by Delta complexes can be measured and compared. A number of basic\nresults directly related to shape analysis are also given in the context of\nDelta complex proximities.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 15:49:17 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Ahmad", "M. Z.", ""], ["Peters", "J. F.", ""]]}, {"id": "1706.04708", "submitter": "Matias Korman", "authors": "Jean-Fran\\c{c}ois Baffier and Yago Diez and Matias Korman", "title": "Experimental Study of Compressed Stack Algorithms in Limited Memory\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The {\\em compressed stack} is a data structure designed by Barba {\\em et al.}\n(Algorithmica 2015) that allows to reduce the amount of memory needed by an\nalgorithm (at the cost of increasing its runtime). In this paper we introduce\nthe first implementation of this data structure and make its source code\npublicly available.\n  Together with the implementation we analyze the performance of the compressed\nstack. In our synthetic experiments, considering different test scenarios and\nusing data sizes ranging up to $2^{30}$ elements, we compare it with the\nclassic (uncompressed) stack, both in terms of runtime and memory used.\n  Our experiments show that the compressed stack needs significantly less\nmemory than the usual stack (this difference is significant for inputs\ncontaining $2000$ or more elements). Overall, with a proper choice of\nparameters, we can save a significant amount of space (from two to four orders\nof magnitude) with a small increase in the runtime ($2.32$ times slower on\naverage than the classic stack). These results holds even in test scenarios\nspecifically designed to be challenging for the compressed stack.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 00:47:55 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Baffier", "Jean-Fran\u00e7ois", ""], ["Diez", "Yago", ""], ["Korman", "Matias", ""]]}, {"id": "1706.05118", "submitter": "Joshua Zahl", "authors": "Joshua Zahl", "title": "Breaking the 3/2 barrier for unit distances in three dimensions", "comments": "43 pages, 0 figures. Final version, to appear in IMRN", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.CG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that every set of $n$ points in $\\mathbb{R}^3$ spans\n$O(n^{295/197+\\epsilon})$ unit distances. This is an improvement over the\nprevious bound of $O(n^{3/2})$. A key ingredient in the proof is a new result\nfor cutting circles in $\\mathbb{R}^3$ into pseudo-segments.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 00:34:58 GMT"}, {"version": "v2", "created": "Sun, 24 Dec 2017 19:36:15 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Zahl", "Joshua", ""]]}, {"id": "1706.05263", "submitter": "Sebastien Tixeuil", "authors": "Jordan Adamek, Mikhail Nesterenko, James Robinson, S\\'ebastien Tixeuil\n  (NPA, IUF, LINCS)", "title": "Concurrent Geometric Multicasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CG cs.DM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MCFR, a multicasting concurrent face routing algorithm that uses\ngeometric routing to deliver a message from source to multiple targets. We\ndescribe the algorithm's operation, prove it correct, estimate its performance\nbounds and evaluate its performance using simulation. Our estimate shows that\nMCFR is the first geometric multicast routing algorithm whose message delivery\nlatency is independent of network size and only proportional to the distance\nbetween the source and the targets. Our simulation indicates that MCFR has\nsignificantly better reliability than existing algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 13:16:54 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Adamek", "Jordan", "", "NPA, IUF, LINCS"], ["Nesterenko", "Mikhail", "", "NPA, IUF, LINCS"], ["Robinson", "James", "", "NPA, IUF, LINCS"], ["Tixeuil", "S\u00e9bastien", "", "NPA, IUF, LINCS"]]}, {"id": "1706.05412", "submitter": "Ali Gholami Rudi", "authors": "Ali Gholami Rudi, Raimi Ayinde Rufai", "title": "A Practical Algorithm for Enumerating Collinear Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of enumerating all maximal collinear subsets\nof size at least three in a given set of $n$ points. An algorithm for this\nproblem, besides solving degeneracy testing and the exact fitting problem, can\nalso help with other problems, such as point line cover and general position\nsubset selection. The classic \\emph{topological sweeping} algorithm of\nEdelsbrunner and Guibas can find these subsets in $O(n^2)$ time in the dual\nplane. We present an alternative algorithm that, although asymptotically slower\nthan their algorithm in the worst case, is simpler to implement and more\namenable to parallelization. If the input points are decomposed into $m$ convex\npolygons, our algorithm has time complexity $O(n^2 \\log m)$ and space\ncomplexity $O(n)$. Our algorithm can be parallelized on the CREW PRAM with time\ncomplexity $O(n \\log m)$ using $n$ processors.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 19:41:12 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Rudi", "Ali Gholami", ""], ["Rufai", "Raimi Ayinde", ""]]}, {"id": "1706.05906", "submitter": "Markus Schmid", "authors": "Katrin Casel, Henning Fernau, Alexander Grigoriev, Markus L. Schmid\n  and Sue Whitesides", "title": "Combinatorial Properties and Recognition of Unit Square Visibility\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unit square (grid) visibility graphs (USV and USGV, resp.) are described by\naxis-parallel visibility between unit squares placed (on integer grid\ncoordinates) in the plane. We investigate combinatorial properties of these\ngraph classes and the hardness of variants of the recognition problem, i.e.,\nthe problem of representing USGV with fixed visibilities within small area and,\nfor USV, the general recognition problem.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 12:48:09 GMT"}, {"version": "v2", "created": "Fri, 20 Oct 2017 09:57:39 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Casel", "Katrin", ""], ["Fernau", "Henning", ""], ["Grigoriev", "Alexander", ""], ["Schmid", "Markus L.", ""], ["Whitesides", "Sue", ""]]}, {"id": "1706.05999", "submitter": "Sascha Wirges", "authors": "Sascha Wirges, Bj\\\"orn Roxin, Eike Rehder, Tilman K\\\"uhner and Martin\n  Lauer", "title": "Guided Depth Upsampling for Precise Mapping of Urban Environments", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an improved model for MRF-based depth upsampling, guided by image-\nas well as 3D surface normal features. By exploiting the underlying camera\nmodel we define a novel regularization term that implicitly evaluates the\nplanarity of arbitrary oriented surfaces. Our method improves upsampling\nquality in scenes composed of predominantly planar surfaces, such as urban\nareas. We use a synthetic dataset to demonstrate that our approach outperforms\nrecent methods that implement distance-based regularization terms. Finally, we\nvalidate our approach for mapping applications on our experimental vehicle.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 15:04:41 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Wirges", "Sascha", ""], ["Roxin", "Bj\u00f6rn", ""], ["Rehder", "Eike", ""], ["K\u00fchner", "Tilman", ""], ["Lauer", "Martin", ""]]}, {"id": "1706.06019", "submitter": "Francisco Belch\\'i Guillam\\'on", "authors": "Francisco Belch\\'i", "title": "Optimising the topological information of the $A_\\infty$-persistence\n  groups", "comments": "26 pages, 3 figures", "journal-ref": "Discrete and Computational Geometry, Volume 62(1) (2019), pages\n  29-54", "doi": "10.1007/s00454-019-00094-x", "report-no": null, "categories": "math.AT cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent homology typically studies the evolution of homology groups\n$H_p(X)$ (with coefficients in a field) along a filtration of topological\nspaces. $A_\\infty$-persistence extends this theory by analysing the evolution\nof subspaces such as $V := \\text{Ker}\\, {\\Delta_n}_{| H_p(X)} \\subseteq\nH_p(X)$, where $\\{\\Delta_m\\}_{m\\geq1}$ denotes a structure of\n$A_\\infty$-coalgebra on $H_*(X)$. In this paper we illustrate how\n$A_\\infty$-persistence can be useful beyond persistent homology by discussing\nthe topological meaning of $V$, which is the most basic form of\n$A_\\infty$-persistence group. In addition, we explore how to choose\n$A_\\infty$-coalgebras along a filtration to make the $A_\\infty$-persistence\ngroups carry more faithful information.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 15:37:53 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Belch\u00ed", "Francisco", ""]]}, {"id": "1706.06287", "submitter": "Abolfazl Poureidi", "authors": "Abolfazl Poureidi and Mohammad Farshi", "title": "The well-separated pair decomposition for balls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a real number $t>1$, a geometric $t$-spanner is a geometric graph for a\npoint set in $\\mathbb{R}^d$ with straight lines between vertices such that the\nratio of the shortest-path distance between every pair of vertices in the graph\n(with Euclidean edge lengths) to their actual Euclidean distance is at most\n$t$. An imprecise point set is modeled by a set $R$ of regions in\n$\\mathbb{R}^d$. If one chooses a point in each region of $R$, then the\nresulting point set is called a precise instance of~$R$. An imprecise\n$t$-spanner for an imprecise point set $R$ is a graph $G=(R,E)$ such that for\neach precise instance $S$ of $R$, graph $G_S=(S,E_S)$, where $E_S$ is the set\nof edges corresponding to $E$, is a $t$-spanner.\n  In this paper, we show that, given a real number $t>1$, there is an imprecise\npoint set $R$ of $n$ straight-line segments in the plane such that any\nimprecise $t$-spanner for $R$ has $\\Omega(n^2)$ edges. Then, we propose an\nalgorithm that computes a Well-Separated Pair Decomposition (WSPD) of size\n${\\cal O}(n)$ for a set of $n$ pairwise disjoint $d$-dimensional balls with\narbitrary sizes. Given a real number $t>1$ and given a set of $n$ pairwise\ndisjoint $d$-balls with arbitrary sizes, we use this WSPD to compute in ${\\cal\nO}(n\\log n+n/(t-1)^d)$ time an imprecise $t$-spanner with ${\\cal O}(n/(t-1)^d)$\nedges for balls.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 06:49:16 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Poureidi", "Abolfazl", ""], ["Farshi", "Mohammad", ""]]}, {"id": "1706.06682", "submitter": "Jian Cheng", "authors": "Jian Cheng, Dinggang Shen, Pew-Thian Yap and Peter J. Basser", "title": "Single- and Multiple-Shell Uniform Sampling Schemes for Diffusion MRI\n  Using Spherical Codes", "comments": "Accepted by IEEE transactions on Medical Imaging. Codes have been\n  released in dmritool\n  https://diffusionmritool.github.io/tutorial_qspacesampling.html", "journal-ref": null, "doi": "10.1109/TMI.2017.2756072", "report-no": null, "categories": "physics.med-ph cs.CG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In diffusion MRI (dMRI), a good sampling scheme is important for efficient\nacquisition and robust reconstruction. Diffusion weighted signal is normally\nacquired on single or multiple shells in q-space. Signal samples are typically\ndistributed uniformly on different shells to make them invariant to the\norientation of structures within tissue, or the laboratory coordinate frame.\nThe Electrostatic Energy Minimization (EEM) method, originally proposed for\nsingle shell sampling scheme in dMRI, was recently generalized to multi-shell\nschemes, called Generalized EEM (GEEM). GEEM has been successfully used in the\nHuman Connectome Project (HCP). However, EEM does not directly address the goal\nof optimal sampling, i.e., achieving large angular separation between sampling\npoints. In this paper, we propose a more natural formulation, called Spherical\nCode (SC), to directly maximize the minimal angle between different samples in\nsingle or multiple shells. We consider not only continuous problems to design\nsingle or multiple shell sampling schemes, but also discrete problems to\nuniformly extract sub-sampled schemes from an existing single or multiple shell\nscheme, and to order samples in an existing scheme. We propose five algorithms\nto solve the above problems, including an incremental SC (ISC), a sophisticated\ngreedy algorithm called Iterative Maximum Overlap Construction (IMOC), an 1-Opt\ngreedy method, a Mixed Integer Linear Programming (MILP) method, and a\nConstrained Non-Linear Optimization (CNLO) method. To our knowledge, this is\nthe first work to use the SC formulation for single or multiple shell sampling\nschemes in dMRI. Experimental results indicate that SC methods obtain larger\nangular separation and better rotational invariance than the state-of-the-art\nEEM and GEEM. The related codes and a tutorial have been released in DMRITool.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 22:13:53 GMT"}, {"version": "v2", "created": "Sat, 23 Sep 2017 16:23:30 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Cheng", "Jian", ""], ["Shen", "Dinggang", ""], ["Yap", "Pew-Thian", ""], ["Basser", "Peter J.", ""]]}, {"id": "1706.06708", "submitter": "Mikhail Rudoy", "authors": "Erik D. Demaine, Sarah Eisenstat, Mikhail Rudoy", "title": "Solving the Rubik's Cube Optimally is NP-complete", "comments": "35 pages, 8 figures", "journal-ref": null, "doi": "10.4230/LIPIcs.STACS.2018.24", "report-no": null, "categories": "cs.CC cs.CG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove that optimally solving an $n \\times n \\times n$\nRubik's Cube is NP-complete by reducing from the Hamiltonian Cycle problem in\nsquare grid graphs. This improves the previous result that optimally solving an\n$n \\times n \\times n$ Rubik's Cube with missing stickers is NP-complete. We\nprove this result first for the simpler case of the Rubik's Square---an $n\n\\times n \\times 1$ generalization of the Rubik's Cube---and then proceed with a\nsimilar but more complicated proof for the Rubik's Cube case.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 00:02:45 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 07:33:24 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Demaine", "Erik D.", ""], ["Eisenstat", "Sarah", ""], ["Rudoy", "Mikhail", ""]]}, {"id": "1706.06938", "submitter": "Alina Shaikhet", "authors": "Prosenjit Bose, Jean-Lou De Carufel, Alina Shaikhet and Michiel Smid", "title": "Art Gallery Localization", "comments": "21 pages, 34 figures, submitted to the Journal of Computational\n  Geometry: Theory and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of placing a set $T$ of broadcast towers in a simple\npolygon $P$ in order for any point to locate itself in the interior of $P$. Let\n$V(p)$ denote the visibility polygon of a point $p$, as the set of all points\n$q \\in P$ that are visible to $p$. For any point $p \\in P$: for each tower $t\n\\in T \\cap V(p)$ the point $p$ receives the coordinates of $t$ and the\nEuclidean distance between $t$ and $p$. From this information $p$ can determine\nits coordinates. We show a tower-positioning algorithm that computes such a set\n$T$ of size at most $\\lfloor 2n/3\\rfloor$, where $n$ is the size of $P$. This\nimproves the previous upper bound of $\\lfloor 8n/9\\rfloor$ towers. We also show\nthat $\\lfloor 2n/3\\rfloor$ towers are sometimes necessary.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 14:46:53 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 16:17:51 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Bose", "Prosenjit", ""], ["De Carufel", "Jean-Lou", ""], ["Shaikhet", "Alina", ""], ["Smid", "Michiel", ""]]}, {"id": "1706.07357", "submitter": "Yin Tat Lee", "authors": "Yin Tat Lee, Aaron Sidford, Santosh S. Vempala", "title": "Efficient Convex Optimization with Membership Oracles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing a convex function over a convex set\ngiven access only to an evaluation oracle for the function and a membership\noracle for the set. We give a simple algorithm which solves this problem with\n$\\tilde{O}(n^2)$ oracle calls and $\\tilde{O}(n^3)$ additional arithmetic\noperations. Using this result, we obtain more efficient reductions among the\nfive basic oracles for convex sets and functions defined by Gr\\\"otschel, Lovasz\nand Schrijver.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 15:07:24 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Lee", "Yin Tat", ""], ["Sidford", "Aaron", ""], ["Vempala", "Santosh S.", ""]]}, {"id": "1706.07399", "submitter": "Aruni Choudhary", "authors": "Aruni Choudhary, Michael Kerber, Sharath Raghvendra", "title": "Improved Approximate Rips Filtrations with Shifted Integer Lattices", "comments": "To appear in ESA 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rips complexes are important structures for analyzing topological features of\nmetric spaces. Unfortunately, generating these complexes constitutes an\nexpensive task because of a combinatorial explosion in the complex size. For\n$n$ points in $\\mathbb{R}^d$, we present a scheme to construct a\n$3\\sqrt{2}$-approximation of the multi-scale filtration of the $L_\\infty$-Rips\ncomplex, which extends to a $O(d^{0.25})$-approximation of the Rips filtration\nfor the Euclidean case. The $k$-skeleton of the resulting approximation has a\ntotal size of $n2^{O(d\\log k)}$. The scheme is based on the integer lattice and\non the barycentric subdivision of the $d$-cube.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 17:01:46 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Choudhary", "Aruni", ""], ["Kerber", "Michael", ""], ["Raghvendra", "Sharath", ""]]}, {"id": "1706.07473", "submitter": "Pierre Lairez", "authors": "Peter B\\\"urgisser, Felipe Cucker, Pierre Lairez", "title": "Computing the homology of basic semialgebraic sets in weak exponential\n  time", "comments": null, "journal-ref": "Journal of the ACM 66(1), Article 5, 2018", "doi": "10.1145/3275242", "report-no": null, "categories": "cs.CG math.AG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and analyze an algorithm for computing the homology (Betti\nnumbers and torsion coefficients) of basic semialgebraic sets which works in\nweak exponential time. That is, out of a set of exponentially small measure in\nthe space of data the cost of the algorithm is exponential in the size of the\ndata. All algorithms previously proposed for this problem have a complexity\nwhich is doubly exponential (and this is so for almost all data).\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 19:44:28 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 14:50:16 GMT"}, {"version": "v3", "created": "Sat, 28 Sep 2019 19:27:19 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["B\u00fcrgisser", "Peter", ""], ["Cucker", "Felipe", ""], ["Lairez", "Pierre", ""]]}, {"id": "1706.07900", "submitter": "Mikhail Rudoy", "authors": "Erik D. Demaine, Mikhail Rudoy", "title": "Tree-Residue Vertex-Breaking: a new tool for proving hardness", "comments": "37 pages, 28 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new problem called Tree-Residue Vertex-Breaking\n(TRVB): given a multigraph $G$ some of whose vertices are marked \"breakable,\"\nis it possible to convert $G$ into a tree via a sequence of \"vertex-breaking\"\noperations (replacing a degree-$k$ breakable vertex by $k$ degree-$1$ vertices,\ndisconnecting the $k$ incident edges)?\n  We characterize the computational complexity of TRVB with any combination of\nthe following additional constraints: $G$ must be planar, $G$ must be a simple\ngraph, the degree of every breakable vertex must belong to an allowed list $B$,\nand the degree of every unbreakable vertex must belong to an allowed list $U$.\nThe two results which we expect to be most generally applicable are that (1)\nTRVB is polynomially solvable when breakable vertices are restricted to have\ndegree at most $3$; and (2) for any $k \\ge 4$, TRVB is NP-complete when the\ngiven multigraph is restricted to be planar and to consist entirely of\ndegree-$k$ breakable vertices. To demonstrate the use of TRVB, we give a simple\nproof of the known result that Hamiltonicity in max-degree-$3$ square grid\ngraphs is NP-hard.\n  We also demonstrate a connection between TRVB and the Hypergraph Spanning\nTree problem. This connection allows us to show that the Hypergraph Spanning\nTree problem in $k$-uniform $2$-regular hypergraphs is NP-complete for any $k\n\\ge 4$, even when the incidence graph of the hypergraph is planar.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 03:32:42 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 03:45:50 GMT"}, {"version": "v3", "created": "Thu, 3 May 2018 05:17:03 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Demaine", "Erik D.", ""], ["Rudoy", "Mikhail", ""]]}, {"id": "1706.08016", "submitter": "Alina Shaikhet", "authors": "Prosenjit Bose, Jean-Lou De Carufel, Alina Shaikhet and Michiel Smid", "title": "Optimal Art Gallery Localization is NP-hard", "comments": "12 pages; 13 figures; submitted to the Journal of Computational\n  Geometry: Theory and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Art Gallery Localization (AGL) is the problem of placing a set $T$ of\nbroadcast towers in a simple polygon $P$ in order for a point to locate itself\nin the interior. For any point $p \\in P$: for each tower $t \\in T \\cap V(p)$\n(where $V(p)$ denotes the visibility polygon of $p$) the point $p$ receives the\ncoordinates of $t$ and the Euclidean distance between $t$ and $p$. From this\ninformation $p$ can determine its coordinates. We study the computational\ncomplexity of AGL problem. We show that the problem of determining the minimum\nnumber of broadcast towers that can localize a point anywhere in a simple\npolygon $P$ is NP-hard. We show a reduction from Boolean Three Satisfiability\nproblem to our problem and give a proof that the reduction takes polynomial\ntime.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 00:51:07 GMT"}, {"version": "v2", "created": "Sun, 23 Jul 2017 01:10:34 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 16:17:19 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Bose", "Prosenjit", ""], ["De Carufel", "Jean-Lou", ""], ["Shaikhet", "Alina", ""], ["Smid", "Michiel", ""]]}, {"id": "1706.08105", "submitter": "Ahmad Biniaz", "authors": "Ahmad Biniaz, Anil Maheshwari, and Michiel Smid", "title": "Compatible 4-Holes in Point Sets", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting interior-disjoint empty convex polygons in a point set is a typical\nErd\\H{o}s-Szekeres-type problem. We study this problem for 4-gons. Let $P$ be a\nset of $n$ points in the plane and in general position. A subset $Q$ of $P$,\nwith four points, is called a $4$-hole in $P$ if $Q$ is in convex position and\nits convex hull does not contain any point of $P$ in its interior. Two 4-holes\nin $P$ are compatible if their interiors are disjoint. We show that $P$\ncontains at least $\\lfloor 5n/11\\rfloor {-} 1$ pairwise compatible 4-holes.\nThis improves the lower bound of $2\\lfloor(n-2)/5\\rfloor$ which is implied by a\nresult of Sakai and Urrutia (2007).\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 13:51:16 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 01:28:57 GMT"}, {"version": "v3", "created": "Thu, 19 Apr 2018 14:15:11 GMT"}, {"version": "v4", "created": "Thu, 26 Jul 2018 13:47:31 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Biniaz", "Ahmad", ""], ["Maheshwari", "Anil", ""], ["Smid", "Michiel", ""]]}, {"id": "1706.09086", "submitter": "Debajyoti Mondal", "authors": "Anna Lubiw and Debajyoti Mondal", "title": "On Compatible Triangulations with a Minimum Number of Steiner Points", "comments": "A preliminary version appeared at the 29th Canadian Conference on\n  Computational Geometry (CCCG 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two vertex-labelled polygons are \\emph{compatible} if they have the same\nclockwise cyclic ordering of vertices. The definition extends to polygonal\nregions (polygons with holes) and to triangulations---for every face, the\nclockwise cyclic order of vertices on the boundary must be the same. It is\nknown that every pair of compatible $n$-vertex polygonal regions can be\nextended to compatible triangulations by adding $O(n^2)$ Steiner points.\nFurthermore, $\\Omega(n^2)$ Steiner points are sometimes necessary, even for a\npair of polygons. Compatible triangulations provide piecewise linear\nhomeomorphisms and are also a crucial first step in morphing planar graph\ndrawings, aka \"2D shape animation\". An intriguing open question, first posed by\nAronov, Seidel, and Souvaine in 1993, is to decide if two compatible polygons\nhave compatible triangulations with at most $k$ Steiner points. In this paper\nwe prove the problem to be NP-hard for polygons with holes. The question\nremains open for simple polygons.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 00:33:18 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Lubiw", "Anna", ""], ["Mondal", "Debajyoti", ""]]}, {"id": "1706.09441", "submitter": "Ery Arias-Castro", "authors": "Ery Arias-Castro and Thibaut Le Gouic", "title": "Unconstrained and Curvature-Constrained Shortest-Path Distances and\n  their Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study shortest paths and their distances on a subset of a Euclidean space,\nand their approximation by their equivalents in a neighborhood graph defined on\na sample from that subset. In particular, we recover and extend the results of\nBernstein et al. (2000). We do the same with curvature-constrained shortest\npaths and their distances, establishing what we believe are the first\napproximation bounds for them.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 18:38:39 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 18:45:38 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Gouic", "Thibaut Le", ""]]}, {"id": "1706.09533", "submitter": "Christopher Liaw", "authors": "Christopher Liaw, Paul Liu, Robert Reiss", "title": "Approximation Schemes for Covering and Packing in the Streaming Model", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shifting strategy, introduced by Hochbaum and Maass, and independently by\nBaker, is a unified framework for devising polynomial approximation schemes to\nNP-Hard problems. This strategy has been used to great success within the\ncomputational geometry community in a plethora of different applications; most\nnotably covering, packing, and clustering problems. In this paper, we revisit\nthe shifting strategy in the context of the streaming model and develop a\nstreaming-friendly shifting strategy. When combined with the shifting coreset\nmethod introduced by Fonseca et al., we obtain streaming algorithms for various\ngraph properties of unit disc graphs. As a further application, we present\nnovel approximation algorithms and lower bounds for the unit disc cover (UDC)\nproblem in the streaming model, for which currently no algorithms are known.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 01:15:50 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Liaw", "Christopher", ""], ["Liu", "Paul", ""], ["Reiss", "Robert", ""]]}, {"id": "1706.10046", "submitter": "Mikhail Rudoy", "authors": "Erik D. Demaine, Mikhail Rudoy", "title": "Hamiltonicity is Hard in Thin or Polygonal Grid Graphs, but Easy in Thin\n  Polygonal Grid Graphs", "comments": "25 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2007, Arkin et al. initiated a systematic study of the complexity of the\nHamiltonian cycle problem on square, triangular, or hexagonal grid graphs,\nrestricted to polygonal, thin, superthin, degree-bounded, or solid grid graphs.\nThey solved many combinations of these problems, proving them either\npolynomially solvable or NP-complete, but left three combinations open. In this\npaper, we prove two of these unsolved combinations to be NP-complete:\nHamiltonicity of Square Polygonal Grid Graphs and Hamiltonicity of Hexagonal\nThin Grid Graphs. We also consider a new restriction, where the grid graph is\nboth thin and polygonal, and prove that Hamiltonicity then becomes polynomially\nsolvable for square, triangular, and hexagonal grid graphs.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 07:37:53 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Demaine", "Erik D.", ""], ["Rudoy", "Mikhail", ""]]}, {"id": "1706.10191", "submitter": "Adalat Jabrayilov", "authors": "Adalat Jabrayilov and Petra Mutzel", "title": "New Integer Linear Programming Models for the Vertex Coloring Problem", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vertex coloring problem asks for the minimum number of colors that can be\nassigned to the vertices of a given graph such that for all vertices v the\ncolor of v is different from the color of any of its neighbors. The problem is\nNP-hard. Here, we introduce new integer linear programming formulations based\non partial orderings. They have the advantage that they are as simple to work\nwith as the classical assignment formulation, since they can be fed directly\ninto a standard integer linear programming solver. We evaluate our new models\nusing Gurobi and show that our new simple approach is a good alternative to the\nbest state-of-the-art approaches for the vertex coloring problem. In our\ncomputational experiments, we compare our formulations with the classical\nassignment formulation and the representatives formulation on a large set of\nbenchmark graphs as well as randomly generated graphs of varying size and\ndensity. The evaluation shows that one of the new models dominates both\nformulations for sparse graphs, while the representatives formulation is the\nbest for dense graphs.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 13:37:16 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 16:00:15 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Jabrayilov", "Adalat", ""], ["Mutzel", "Petra", ""]]}, {"id": "1706.10193", "submitter": "Pat Morin", "authors": "Boris Aronov, Vida Dujmovi\\'c, Pat Morin, Aur\\'elien Ooms, and Lu\\'is\n  Fernando Schultz Xavier da Silveira", "title": "More Tur\\'an-Type Theorems for Triangles in Convex Point Sets", "comments": "25 pages, 14 figures, 16 graphics. This version corrects one theorem\n  statement from the original version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following family of problems: Given a set of $n$ points in\nconvex position, what is the maximum number triangles one can create having\nthese points as vertices while avoiding certain sets of forbidden\nconfigurations. As forbidden configurations we consider all 8 ways in which a\npair of triangles in such a point set can interact. This leads to 256 extremal\nTur\\'an-type questions. We give nearly tight (within a $\\log n$ factor) bounds\nfor 248 of these questions and show that the remaining 8 questions are all\nasymptotically equivalent to Stein's longstanding tripod packing problem.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 13:40:11 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 13:52:52 GMT"}, {"version": "v3", "created": "Fri, 7 Dec 2018 14:37:18 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Aronov", "Boris", ""], ["Dujmovi\u0107", "Vida", ""], ["Morin", "Pat", ""], ["Ooms", "Aur\u00e9lien", ""], ["da Silveira", "Lu\u00eds Fernando Schultz Xavier", ""]]}, {"id": "1706.10195", "submitter": "Thom Castermans", "authors": "Thom Castermans and Bettina Speckmann and Frank Staals and Kevin\n  Verbeek", "title": "Agglomerative Clustering of Growing Squares", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study an agglomerative clustering problem motivated by interactive glyphs\nin geo-visualization. Consider a set of disjoint square glyphs on an\ninteractive map. When the user zooms out, the glyphs grow in size relative to\nthe map, possibly with different speeds. When two glyphs intersect, we wish to\nreplace them by a new glyph that captures the information of the intersecting\nglyphs.\n  We present a fully dynamic kinetic data structure that maintains a set of $n$\ndisjoint growing squares. Our data structure uses $O(n (\\log n \\log\\log n)^2)$\nspace, supports queries in worst case $O(\\log^3 n)$ time, and updates in\n$O(\\log^7 n)$ amortized time. This leads to an $O(n\\alpha(n)\\log^7 n)$ time\nalgorithm to solve the agglomerative clustering problem. This is a significant\nimprovement over the current best $O(n^2)$ time algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 13:43:33 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 09:40:55 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Castermans", "Thom", ""], ["Speckmann", "Bettina", ""], ["Staals", "Frank", ""], ["Verbeek", "Kevin", ""]]}]