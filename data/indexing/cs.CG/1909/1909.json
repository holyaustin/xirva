[{"id": "1909.00223", "submitter": "Fabrizio Montecchiani", "authors": "Patrizio Angelini, Michael A. Bekos, Franz J. Brandenburg, Giordano Da\n  Lozzo, Giuseppe Di Battista, Walter Didimo, Michael Hoffmann, Giuseppe\n  Liotta, Fabrizio Montecchiani, Ignaz Rutter, Csaba D. T\\'oth", "title": "Simple $k$-Planar Graphs are Simple $(k+1)$-Quasiplanar", "comments": "arXiv admin note: substantial text overlap with arXiv:1705.05569", "journal-ref": null, "doi": "10.1016/j.jctb.2019.08.006", "report-no": null, "categories": "cs.CG cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple topological graph is $k$-quasiplanar ($k\\geq 2$) if it contains no\n$k$ pairwise crossing edges, and $k$-planar if no edge is crossed more than $k$\ntimes. In this paper, we explore the relationship between $k$-planarity and\n$k$-quasiplanarity to show that, for $k \\geq 2$, every $k$-planar simple\ntopological graph can be transformed into a $(k+1)$-quasiplanar simple\ntopological graph.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 14:20:01 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Angelini", "Patrizio", ""], ["Bekos", "Michael A.", ""], ["Brandenburg", "Franz J.", ""], ["Da Lozzo", "Giordano", ""], ["Di Battista", "Giuseppe", ""], ["Didimo", "Walter", ""], ["Hoffmann", "Michael", ""], ["Liotta", "Giuseppe", ""], ["Montecchiani", "Fabrizio", ""], ["Rutter", "Ignaz", ""], ["T\u00f3th", "Csaba D.", ""]]}, {"id": "1909.00263", "submitter": "Gabriel Nivasch", "authors": "Sergey Avvakumov and Gabriel Nivasch", "title": "Homotopic curve shortening and the affine curve-shortening flow", "comments": "Minor changes. 28 pages, 22 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define and study a discrete process that generalizes the convex-layer\ndecomposition of a planar point set. Our process, which we call \"homotopic\ncurve shortening\" (HCS), starts with a closed curve (which might\nself-intersect) in the presence of a set $P\\subset \\mathbb R^2$ of point\nobstacles, and evolves in discrete steps, where each step consists of (1)\ntaking shortcuts around the obstacles, and (2) reducing the curve to its\nshortest homotopic equivalent.\n  We find experimentally that, if the initial curve is held fixed and $P$ is\nchosen to be either a very fine regular grid or a uniformly random point set,\nthen HCS behaves at the limit like the affine curve-shortening flow (ACSF).\nThis connection between ACSF and HCS generalizes the link between ACSF and\nconvex-layer decomposition (Eppstein et al., 2017; Calder and Smart, 2020),\nwhich is restricted to convex curves.\n  We prove that HCS satisfies some properties analogous to those of ACSF: HCS\nis invariant under affine transformations, preserves convexity, and does not\nincrease the total absolute curvature. Furthermore, the number of\nself-intersections of a curve, or intersections between two curves\n(appropriately defined), does not increase. Finally, if the initial curve is\nsimple, then the number of inflection points (appropriately defined) does not\nincrease.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 18:56:21 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 08:07:55 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 19:41:19 GMT"}, {"version": "v4", "created": "Sat, 2 Jan 2021 21:31:58 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Avvakumov", "Sergey", ""], ["Nivasch", "Gabriel", ""]]}, {"id": "1909.00852", "submitter": "Mikkel Abrahamsen", "authors": "Anders Aamand, Mikkel Abrahamsen, Mikkel Thorup", "title": "Disks in Curves of Bounded Convex Curvature", "comments": "Accepted for publication in The American Mathematical Monthly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We say that a simple, closed curve $\\gamma$ in the plane has bounded convex\ncurvature if for every point $x$ on $\\gamma$, there is an open unit disk $U_x$\nand $\\varepsilon_x>0$ such that $x\\in\\partial U_x$ and\n$B_{\\varepsilon_x}(x)\\cap U_x\\subset\\text{Int}\\;\\gamma$. We prove that the\ninterior of every curve of bounded convex curvature contains an open unit disk.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 20:08:08 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Aamand", "Anders", ""], ["Abrahamsen", "Mikkel", ""], ["Thorup", "Mikkel", ""]]}, {"id": "1909.00922", "submitter": "Dmitry Golovaty", "authors": "Dmitry Golovaty, Jose Alberto Montero, Daniel Spirn", "title": "A variational method for generating $n$-cross fields using higher-order\n  $Q$-tensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $n$-cross field is a locally-defined orthogonal coordinate system\ninvariant with respect to the cubic symmetry group. Cross fields are finding\nwide-spread use in mesh generation, computer graphics, and materials science\namong many applications. It was recently by other authors that $3$-cross fields\ncan be embedded into the set of symmetric $4$th-order tensors. Another\nconcurrent work further develops a relaxation of this tensor field via a\ncertain set of varieties. In this paper, we consider the problem of generating\nan arbitrary $n$-cross field using a fourth-order $Q$-tensor theory that is\nconstructed out of tensored projection matrices. We establish that by a\nGinzburg-Landau relaxation towards a global projection, one can reliably\ngenerate an $n$-cross field on arbitrary Lipschitz domains. Our work provides a\nrigorous approach that offers several new results including porting the tensor\nframework to arbitrary dimensions, providing a new relaxation method that\nembeds the problem into a global steepest descent, and offering a relaxation\nscheme for aligning the cross field with the boundary. Our approach is designed\nto fit within the classical Ginzburg-Landau PDE theory, offering a concrete\nroad map for the future careful study of singularities of energy minimizers.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 02:29:38 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 14:34:00 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 14:52:00 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Golovaty", "Dmitry", ""], ["Montero", "Jose Alberto", ""], ["Spirn", "Daniel", ""]]}, {"id": "1909.01106", "submitter": "Yida Wang", "authors": "Yida Wang, David Joseph Tan, Nassir Navab, Federico Tombari", "title": "ForkNet: Multi-branch Volumetric Semantic Completion from a Single Depth\n  Image", "comments": "Accepted in International Conference on Computer Vision 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel model for 3D semantic completion from a single depth\nimage, based on a single encoder and three separate generators used to\nreconstruct different geometric and semantic representations of the original\nand completed scene, all sharing the same latent space. To transfer information\nbetween the geometric and semantic branches of the network, we introduce paths\nbetween them concatenating features at corresponding network layers. Motivated\nby the limited amount of training samples from real scenes, an interesting\nattribute of our architecture is the capacity to supplement the existing\ndataset by generating a new training dataset with high quality, realistic\nscenes that even includes occlusion and real noise. We build the new dataset by\nsampling the features directly from latent space which generates a pair of\npartial volumetric surface and completed volumetric semantic surface. Moreover,\nwe utilize multiple discriminators to increase the accuracy and realism of the\nreconstructions. We demonstrate the benefits of our approach on standard\nbenchmarks for the two most common completion tasks: semantic 3D scene\ncompletion and 3D object completion.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 12:04:39 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Wang", "Yida", ""], ["Tan", "David Joseph", ""], ["Navab", "Nassir", ""], ["Tombari", "Federico", ""]]}, {"id": "1909.01456", "submitter": "Paul Rosen", "authors": "Junyi Tu, Paul Rosen", "title": "Topologically-Guided Color Image Enhancement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enhancement is an important step in post-processing digital images for\npersonal use, in medical imaging, and for object recognition. Most existing\nmanual techniques rely on region selection, similarity, and/or thresholding for\nediting, never really considering the topological structure of the image. In\nthis paper, we leverage the contour tree to extract a hierarchical\nrepresentation of the topology of an image. We propose 4 topology-aware\ntransfer functions for editing features of the image using local topological\nproperties, instead of global image properties. Finally, we evaluate our\napproach with grayscale and color images.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 21:15:24 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Tu", "Junyi", ""], ["Rosen", "Paul", ""]]}, {"id": "1909.01513", "submitter": "Paul Rosen", "authors": "Junyi Tu, Mustafa Hajij, Paul Rosen", "title": "Propagate and Pair: A Single-Pass Approach to Critical Point Pairing in\n  Reeb Graphs", "comments": null, "journal-ref": "Advances in Visual Computing. ISVC 2019. Lecture Notes in Computer\n  Science, vol 11844. Springer, Cham", "doi": "10.1007/978-3-030-33720-9_8", "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the popularization of Topological Data Analysis, the Reeb graph has\nfound new applications as a summarization technique in the analysis and\nvisualization of large and complex data, whose usefulness extends beyond just\nthe graph itself. Pairing critical points enables forming topological\nfingerprints, known as persistence diagrams, that provides insights into the\nstructure and noise in data. Although the body of work addressing the efficient\ncalculation of Reeb graphs is large, the literature on pairing is limited. In\nthis paper, we discuss two algorithmic approaches for pairing critical points\nin Reeb graphs, first a multipass approach, followed by a new single-pass\nalgorithm, called Propagate and Pair.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 01:22:23 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 15:14:36 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Tu", "Junyi", ""], ["Hajij", "Mustafa", ""], ["Rosen", "Paul", ""]]}, {"id": "1909.02083", "submitter": "Haolin Liu", "authors": "Yuxuan Yu, Haolin Liu, Kuanren Qian, Humphrey Yang, Matthew McGehee,\n  Jianzhe Gu, Danli Luo, Lining Yao, Yongjie Jessica Zhang", "title": "Material characterization and precise finite element analysis of fiber\n  reinforced thermoplastic composites for 4D printing", "comments": "The first two authors contributed equally. 18 figures, 11 pages", "journal-ref": "Journal of Computer-aided Design, Volume 122, May 2020, 102817", "doi": null, "report-no": null, "categories": "cs.CG physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Four-dimensional (4D) printing, a new technology emerged from additive\nmanufacturing (3D printing), is widely known for its capability of programming\npost-fabrication shape-changing into artifacts. Fused deposition modeling\n(FDM)-based 4D printing, in particular, uses thermoplastics to produce\nartifacts and requires computational analysis to assist the design processes of\ncomplex geometries. However, these artifacts are weak against structural loads,\nand the design quality can be limited by less accurate material models and\nnumerical simulations. To address these issues, this paper propounds a\ncomposite structure design made of two materials - polylactic acid (PLA) and\ncarbon fiber reinforced PLA (CFPLA) - to increase the structural strength of 4D\nprinted artifacts and a workflow composed of several physical experiments and\nseries of dynamic mechanical analysis (DMA) to characterize materials. We apply\nthis workflow to 3D printed samples fabricated with different printed\nparameters to accurately characterize the materials and implement a sequential\nfinite element analysis (FEA) to achieve accurate simulations. The accuracy of\ndeformation induced by the triggering process is both computationally and\nexperimentally verified with several creative design examples, and the 95%\nconfidence interval of the accuracy is (0.972, 0.985). We believe the presented\nworkflow is essential to the combination of geometry, material mechanism and\ndesign, and has various potential applications.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 20:14:43 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 19:28:53 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Yu", "Yuxuan", ""], ["Liu", "Haolin", ""], ["Qian", "Kuanren", ""], ["Yang", "Humphrey", ""], ["McGehee", "Matthew", ""], ["Gu", "Jianzhe", ""], ["Luo", "Danli", ""], ["Yao", "Lining", ""], ["Zhang", "Yongjie Jessica", ""]]}, {"id": "1909.03488", "submitter": "Adam Brown", "authors": "Adam Brown, Omer Bobrowski, Elizabeth Munch, Bei Wang", "title": "Probabilistic Convergence and Stability of Random Mapper Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the probabilistic convergence between the mapper graph and the Reeb\ngraph of a topological space $\\mathbb{X}$ equipped with a continuous function\n$f: \\mathbb{X} \\rightarrow \\mathbb{R}$. We first give a categorification of the\nmapper graph and the Reeb graph by interpreting them in terms of cosheaves and\nstratified covers of the real line $\\mathbb{R}$. We then introduce a variant of\nthe classic mapper graph of Singh et al.~(2007), referred to as the enhanced\nmapper graph, and demonstrate that such a construction approximates the Reeb\ngraph of $(\\mathbb{X}, f)$ when it is applied to points randomly sampled from a\nprobability density function concentrated on $(\\mathbb{X}, f)$.\n  Our techniques are based on the interleaving distance of constructible\ncosheaves and topological estimation via kernel density estimates. Following\nMunch and Wang (2018), we first show that the mapper graph of $(\\mathbb{X},\nf)$, a constructible $\\mathbb{R}$-space (with a fixed open cover), approximates\nthe Reeb graph of the same space. We then construct an isomorphism between the\nmapper of $(\\mathbb{X},f)$ to the mapper of a super-level set of a probability\ndensity function concentrated on $(\\mathbb{X}, f)$. Finally, building on the\napproach of Bobrowski et al.~(2017), we show that, with high probability, we\ncan recover the mapper of the super-level set given a sufficiently large\nsample. Our work is the first to consider the mapper construction using the\ntheory of cosheaves in a probabilistic setting. It is part of an ongoing effort\nto combine sheaf theory, probability, and statistics, to support topological\ndata analysis with random data.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 16:02:11 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 21:33:58 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 15:13:28 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Brown", "Adam", ""], ["Bobrowski", "Omer", ""], ["Munch", "Elizabeth", ""], ["Wang", "Bei", ""]]}, {"id": "1909.03542", "submitter": "L\\^e Th\\`anh D\\~ung Nguy\\^en", "authors": "L\\^e Th\\`anh D\\~ung Nguy\\^en", "title": "When lattice cubes meet affine subspaces: a short note", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give short and simple proofs of what seem to be folklore results: * the\nmaximum cardinality of the intersection of a lattice cube with an affine\nsubspace; * the minimum number of affine subspaces needed to cover a lattice\ncube.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 20:39:25 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 16:03:03 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Nguy\u00ean", "L\u00ea Th\u00e0nh D\u0169ng", ""]]}, {"id": "1909.03547", "submitter": "Shay Moran", "authors": "Mark Braverman and Gillat Kol and Shay Moran and Raghuvansh R. Saxena", "title": "Convex Set Disjointness, Distributed Learning of Halfspaces, and LP\n  Feasibility", "comments": "37 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.CG cs.DM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Convex Set Disjointness (CSD) problem, where two players have\ninput sets taken from an arbitrary fixed domain~$U\\subseteq \\mathbb{R}^d$ of\nsize $\\lvert U\\rvert = n$. Their mutual goal is to decide using minimum\ncommunication whether the convex hulls of their sets intersect (equivalently,\nwhether their sets can be separated by a hyperplane).\n  Different forms of this problem naturally arise in distributed learning and\noptimization: it is equivalent to {\\em Distributed Linear Program (LP)\nFeasibility} -- a basic task in distributed optimization, and it is tightly\nlinked to {\\it Distributed Learning of Halfdpaces in $\\mathbb{R}^d$}. In\n{communication complexity theory}, CSD can be viewed as a geometric\ninterpolation between the classical problems of {Set Disjointness} (when~$d\\geq\nn-1$) and {Greater-Than} (when $d=1$).\n  We establish a nearly tight bound of $\\tilde \\Theta(d\\log n)$ on the\ncommunication complexity of learning halfspaces in $\\mathbb{R}^d$. For Convex\nSet Disjointness (and the equivalent task of distributed LP feasibility) we\nderive upper and lower bounds of $\\tilde O(d^2\\log n)$ and~$\\Omega(d\\log n)$.\nThese results improve upon several previous works in distributed learning and\noptimization.\n  Unlike typical works in communication complexity, the main technical\ncontribution of this work lies in the upper bounds. In particular, our\nprotocols are based on a {\\it Container Lemma for Halfspaces} and on two\nvariants of {\\it Carath\\'eodory's Theorem}, which may be of independent\ninterest. These geometric statements are used by our protocols to provide a\ncompressed summary of the players' input.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 21:19:34 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Braverman", "Mark", ""], ["Kol", "Gillat", ""], ["Moran", "Shay", ""], ["Saxena", "Raghuvansh R.", ""]]}, {"id": "1909.03613", "submitter": "Aakanksha Rana", "authors": "S. M. Iman Zolanvari, Susana Ruano, Aakanksha Rana, Alan Cummins,\n  Rogerio Eduardo da Silva, Morteza Rahbar, Aljosa Smolic", "title": "DublinCity: Annotated LiDAR Point Cloud and its Applications", "comments": "Accepted to the 30th British Machine Vision Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CG cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene understanding of full-scale 3D models of an urban area remains a\nchallenging task. While advanced computer vision techniques offer\ncost-effective approaches to analyse 3D urban elements, a precise and densely\nlabelled dataset is quintessential. The paper presents the first-ever labelled\ndataset for a highly dense Aerial Laser Scanning (ALS) point cloud at\ncity-scale. This work introduces a novel benchmark dataset that includes a\nmanually annotated point cloud for over 260 million laser scanning points into\n100'000 (approx.) assets from Dublin LiDAR point cloud [12] in 2015. Objects\nare labelled into 13 classes using hierarchical levels of detail from large\n(i.e., building, vegetation and ground) to refined (i.e., window, door and\ntree) elements. To validate the performance of our dataset, two different\napplications are showcased. Firstly, the labelled point cloud is employed for\ntraining Convolutional Neural Networks (CNNs) to classify urban elements. The\ndataset is tested on the well-known state-of-the-art CNNs (i.e., PointNet,\nPointNet++ and So-Net). Secondly, the complete ALS dataset is applied as\ndetailed ground truth for city-scale image-based 3D reconstruction.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 13:47:31 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Zolanvari", "S. M. Iman", ""], ["Ruano", "Susana", ""], ["Rana", "Aakanksha", ""], ["Cummins", "Alan", ""], ["da Silva", "Rogerio Eduardo", ""], ["Rahbar", "Morteza", ""], ["Smolic", "Aljosa", ""]]}, {"id": "1909.03880", "submitter": "Christian Scheffer", "authors": "S\\'andor P. Fekete, Eike Niehs, Christian Scheffer, Arne Schmidt", "title": "Connected Assembly and Reconfiguration by Finite Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider methods for connected reconfigurations by finite automate in the\nso-called \\emph{hybrid} or \\emph{Robot-on-Tiles} model of programmable matter,\nin which a number of simple robots move on and rearrange an arrangement of\npassive tiles in the plane that form \\emph{polyomino} shapes, making use of a\nsupply of additional tiles that can be placed. We investigate the problem of\nreconfiguration under the constraint of maintaining connectivity of the tile\narrangement; this reflects scenarios in which disconnected subarrangements may\ndrift apart, e.g., in the absence of gravity in space. We show that two finite\nautomata suffice to mark a bounding box, which can then be used as a stepping\nstone for more complex operations, such as scaling a tile arrangement by a\ngiven factor, rotating arrangements, or copying arrangements to a different\nlocation. We also describe an algorithm for scaling monotone polyominoes\nwithout the help of a bounding box.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 14:18:22 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Fekete", "S\u00e1ndor P.", ""], ["Niehs", "Eike", ""], ["Scheffer", "Christian", ""], ["Schmidt", "Arne", ""]]}, {"id": "1909.03991", "submitter": "Changlin Wan", "authors": "Changlin Wan, Wennan Chang, Tong Zhao, Mengya Li, Sha Cao, Chi Zhang", "title": "Fast And Efficient Boolean Matrix Factorization By Geometric\n  Segmentation", "comments": "Accepted at AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boolean matrix has been used to represent digital information in many fields,\nincluding bank transaction, crime records, natural language processing,\nprotein-protein interaction, etc. Boolean matrix factorization (BMF) aims to\nfind an approximation of a binary matrix as the Boolean product of two low rank\nBoolean matrices, which could generate vast amount of information for the\npatterns of relationships between the features and samples. Inspired by binary\nmatrix permutation theories and geometric segmentation, we developed a fast and\nefficient BMF approach called MEBF (Median Expansion for Boolean\nFactorization). Overall, MEBF adopted a heuristic approach to locate binary\npatterns presented as submatrices that are dense in 1's. At each iteration,\nMEBF permutates the rows and columns such that the permutated matrix is\napproximately Upper Triangular-Like (UTL) with so-called Simultaneous\nConsecutive-ones Property (SC1P). The largest submatrix dense in 1 would lies\non the upper triangular area of the permutated matrix, and its location was\ndetermined based on a geometric segmentation of a triangular. We compared MEBF\nwith other state of the art approaches on data scenarios with different\nsparsity and noise levels. MEBF demonstrated superior performances in lower\nreconstruction error, and higher computational efficiency, as well as more\naccurate sparse patterns than popular methods such as ASSO, PANDA and MP. We\ndemonstrated the application of MEBF on both binary and non-binary data sets,\nand revealed its further potential in knowledge retrieving and data denoising.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 17:02:57 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 19:17:30 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Wan", "Changlin", ""], ["Chang", "Wennan", ""], ["Zhao", "Tong", ""], ["Li", "Mengya", ""], ["Cao", "Sha", ""], ["Zhang", "Chi", ""]]}, {"id": "1909.04419", "submitter": "Patrick Schnider", "authors": "Alexander Pilz and Patrick Schnider", "title": "Bisecting three classes of lines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following problem: Let $\\mathcal{L}$ be an arrangement of $n$\nlines in $\\mathbb{R}^3$ colored red, green, and blue. Does there exist a\nvertical plane $P$ such that a line on $P$ simultaneously bisects all three\nclasses of points in the cross-section $\\mathcal{L} \\cap P$? Recently, Schnider\n[SoCG 2019] used topological methods to prove that such a cross-section always\nexists. In this work, we give an alternative proof of this fact, using only\nmethods from discrete geometry. With this combinatorial proof at hand, we\ndevise an $O(n^2\\log^2(n))$ time algorithm to find such a plane and the\nbisector of the induced cross-section. We do this by providing a general\nframework, from which we expect that it can be applied to solve similar\nproblems on cross-sections and kinetic points.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 11:37:34 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Pilz", "Alexander", ""], ["Schnider", "Patrick", ""]]}, {"id": "1909.05953", "submitter": "Efi Fogel", "authors": "Tom Tsabar, Efi Fogel, and Dan Halperin", "title": "Optimized Synthesis of Snapping Fixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fixtures for constraining the movement of parts have been extensively\ninvestigated in robotics, since they are essential for using robots in\nautomated manufacturing. This paper deals with the design and optimized\nsynthesis of a special type of fixtures, which we call \\emph{snapping\nfixtures}. Given a polyhedral workpiece $P$ with $n$ vertices and of constant\ngenus, which we need to hold, a snapping fixture is a semi-rigid polyhedron\n$G$, made of a palm and several fingers, such that when $P$ and $G$ are well\nseparated, we can push $P$ toward $G$, slightly bending the fingers of $G$ on\nthe way (exploiting its mild flexibility), and obtain a configuration, where\n$G$ is back in its original shape and $P$ and $G$ are inseparable as rigid\nbodies. We prove the minimal closure conditions under which such fixtures can\nhold parts, using Helly's theorem. We then introduce an algorithm running in\n$O(n^3)$ time that produces a snapping fixture, minimizing the number of\nfingers and optimizing additional objectives, if a snapping fixture exists. We\nalso provide an efficient and robust implementation of a simpler version of the\nalgorithm, which produces the fixture model to be 3D printed and runs in\n$O(n^4)$ time. We describe two applications with different optimization\ncriteria: Fixtures to hold add-ons for drones, where we aim to make the fixture\nas lightweight as possible, and small-scale fixtures to hold precious stones in\njewelry, where we aim to maximize the exposure of the stones, namely minimize\nthe obscuring of the workpiece by the fixture.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 21:17:26 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 11:18:26 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Tsabar", "Tom", ""], ["Fogel", "Efi", ""], ["Halperin", "Dan", ""]]}, {"id": "1909.06457", "submitter": "Satyabrata Jana", "authors": "Satyabrata Jana, Anil Maheshwari, Sasanka Roy", "title": "Linear Size Planar Manhattan Network for Convex Point Sets", "comments": "31 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $G = (V, E)$ be an edge-weighted geometric graph such that every edge is\nhorizontal or vertical. The weight of an edge $uv \\in E$ is its length. Let $\nW_G (u,v)$ denote the length of a shortest path between a pair of vertices $u$\nand $v$ in $G$. The graph $G$ is said to be a Manhattan network for a given\npoint set $ P $ in the plane if $P \\subseteq V$ and $\\forall p,q \\in P$, $ W_G\n(p,q)=|pq|_1$. In addition to $ P$, graph $G$ may also include a set $T$ of\nSteiner points in its vertex set $V$. In the Manhattan network problem, the\nobjective is to construct a Manhattan network of small size for a set of $ n $\npoints. This problem was first considered by Gudmundsson et\nal.\\cite{gudmundsson2007small}. They give a construction of a Manhattan network\nof size $\\Theta(n \\log n)$ for general point set in the plane. We say a\nManhattan network is planar if it can be embedded in the plane without any edge\ncrossings. In this paper, we construct a linear size planar Manhattan network\nfor convex point set in linear time using $\\mathcal{ O}(n)$ Steiner points. We\nalso show that, even for convex point set, the construction in Gudmundsson et\nal. \\cite{gudmundsson2007small} needs $\\Omega (n \\log n)$ Steiner points and\nthe network may not be planar.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 21:26:56 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Jana", "Satyabrata", ""], ["Maheshwari", "Anil", ""], ["Roy", "Sasanka", ""]]}, {"id": "1909.06463", "submitter": "Parameswaran Raman", "authors": "Parameswaran Raman, Jiasen Yang", "title": "Optimization on the Surface of the (Hyper)-Sphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thomson problem is a classical problem in physics to study how $n$ number of\ncharged particles distribute themselves on the surface of a sphere of $k$\ndimensions. When $k=2$, i.e. a 2-sphere (a circle), the particles appear at\nequally spaced points. Such a configuration can be computed analytically.\nHowever, for higher dimensions such as $k \\ge 3$, i.e. the case of 3-sphere\n(standard sphere), there is not much that is understood analytically. Finding\nglobal minimum of the problem under these settings is particularly tough since\nthe optimization problem becomes increasingly computationally intensive with\nlarger values of $k$ and $n$. In this work, we explore a wide variety of\nnumerical optimization methods to solve the Thomson problem. In our empirical\nstudy, we find stochastic gradient based methods (SGD) to be a compelling\nchoice for this problem as it scales well with the number of points.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 21:50:57 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Raman", "Parameswaran", ""], ["Yang", "Jiasen", ""]]}, {"id": "1909.07013", "submitter": "Csaba D. Toth", "authors": "Daniel Archambault, Csaba D. T\\'oth", "title": "Proceedings of the 27th International Symposium on Graph Drawing and\n  Network Visualization (GD 2019)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS cs.HC cs.SI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the arXiv index for the electronic proceedings of GD 2019, which\ncontains the peer-reviewed and revised accepted papers with an optional\nappendix. Proceedings (without appendices) are also to be published by Springer\nin the Lecture Notes in Computer Science series.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 06:29:50 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Archambault", "Daniel", ""], ["T\u00f3th", "Csaba D.", ""]]}, {"id": "1909.07093", "submitter": "Fabrizio Frati", "authors": "Fidel Barrera-Cruz, Manuel Borrazzo, Giordano Da Lozzo, Giuseppe Di\n  Battista, Fabrizio Frati, Maurizio Patrignani, and Vincenzo Roselli", "title": "How to Morph a Tree on a Small Grid", "comments": "A preliminary version of this paper appears in WADS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study planar morphs between straight-line planar grid\ndrawings of trees. A morph consists of a sequence of morphing steps, where in a\nmorphing step vertices move along straight-line trajectories at constant speed.\nWe show how to construct planar morphs that simultaneously achieve a reduced\nnumber of morphing steps and a polynomially-bounded resolution. We assume that\nboth the initial and final drawings lie on the grid and we ensure that each\nmorphing step produces a grid drawing; further, we consider both upward\ndrawings of rooted trees and drawings of arbitrary trees.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 09:54:20 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 13:19:14 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Barrera-Cruz", "Fidel", ""], ["Borrazzo", "Manuel", ""], ["Da Lozzo", "Giordano", ""], ["Di Battista", "Giuseppe", ""], ["Frati", "Fabrizio", ""], ["Patrignani", "Maurizio", ""], ["Roselli", "Vincenzo", ""]]}, {"id": "1909.07141", "submitter": "Bhargav Narayanan", "authors": "Logan Crew, Bhargav Narayanan, Sophie Spirkl", "title": "Disproportionate division", "comments": "8 pages, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the disproportionate version of the classical cake-cutting problem:\nhow efficiently can we divide a cake, here $[0,1]$, among $n$ agents with\ndifferent demands $\\alpha_1, \\alpha_2, \\dots, \\alpha_n$ summing to $1$? When\nall the agents have equal demands of $\\alpha_1 = \\alpha_2 = \\dots = \\alpha_n =\n1/n$, it is well-known that there exists a fair division with $n-1$ cuts, and\nthis is optimal. For arbitrary demands on the other hand, folklore arguments\nfrom algebraic topology show that $O(n\\log n)$ cuts suffice, and this has been\nthe state of the art for decades. Here, we improve the state of affairs in two\nways: we prove that disproportionate division may always be achieved with\n$3n-4$ cuts, and give an effective combinatorial procedure to construct such a\ndivision. We also offer a topological conjecture that implies that $2n-2$ cuts\nsuffice in general, which would be optimal.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 12:05:26 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Crew", "Logan", ""], ["Narayanan", "Bhargav", ""], ["Spirkl", "Sophie", ""]]}, {"id": "1909.07347", "submitter": "Fabian Klute", "authors": "Alan Arroyo, Fabian Klute, Irene Parada, Raimund Seidel, Birgit\n  Vogtenhuber, Tilo Wiedera", "title": "Inserting one edge into a simple drawing is hard", "comments": "Full version of the preliminary version published in the proceedings\n  of the 46th International Workshop on Graph-Theoretic Concepts in Computer\n  Science (WG'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A {\\em simple drawing} $D(G)$ of a graph $G$ is one where each pair of edges\nshare at most one point: either a common endpoint or a proper crossing. An edge\n$e$ in the complement of $G$ can be {\\em inserted} into $D(G)$ if there exists\na simple drawing of $G+e$ extending $D(G)$. As a result of Levi's Enlargement\nLemma, if a drawing is rectilinear (pseudolinear), that is, the edges can be\nextended into an arrangement of lines (pseudolines), then any edge in the\ncomplement of $G$ can be inserted. In contrast, we show that it is NP -complete\nto decide whether one edge can be inserted into a simple drawing. This remains\ntrue even if we assume that the drawing is pseudocircular, that is, the edges\ncan be extended to an arrangement of pseudocircles. On the positive side, we\nshow that, given an arrangement of pseudocircles $\\mathcal{A}$ and a\npseudosegment $\\sigma$, it can be decided in polynomial time whether there\nexists a pseudocircle $\\Phi_\\sigma$ extending $\\sigma$ for which\n$\\mathcal{A}\\cup\\{\\Phi_\\sigma\\}$ is again an arrangement of pseudocircles.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 17:24:51 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 21:12:46 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Arroyo", "Alan", ""], ["Klute", "Fabian", ""], ["Parada", "Irene", ""], ["Seidel", "Raimund", ""], ["Vogtenhuber", "Birgit", ""], ["Wiedera", "Tilo", ""]]}, {"id": "1909.08417", "submitter": "Hongwei Lin", "authors": "Zhetong Dong, Hongwei Lin, Chi Zhou", "title": "Persistence B-Spline Grids: Stable Vector Representation of Persistence\n  Diagrams Based on Data Fitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decades, many attempts have been made to optimally integrate\nmachine learning (ML) and topological data analysis. A prominent problem in\napplying persistent homology to ML tasks is finding a vector representation of\na persistence diagram (PD), which is a summary diagram for representing\ntopological features. From the perspective of data fitting, a stable vector\nrepresentation, persistence B-spline grid (PB), is proposed based on the\nefficient technique of progressive-iterative approximation for least-squares\nB-spline surface fitting. Meanwhile, we theoretically prove that the PB method\nis stable with respect to the metrics defined on the PD space, i.e., the\n$p$-Wasserstein distance and the bottleneck distance. The proposed method was\ntested on a synthetic dataset, datasets of randomly generated PDs, data of a\ndynamical system, and 3D CAD models.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 09:09:43 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Dong", "Zhetong", ""], ["Lin", "Hongwei", ""], ["Zhou", "Chi", ""]]}, {"id": "1909.08544", "submitter": "Leo Liberti", "authors": "Leo Liberti", "title": "Distance Geometry and Data Science", "comments": "This invited survey will appear in the journal TOP\n  <https://link.springer.com/journal/11750>, in 2020 issue 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data are often represented as graphs. Many common tasks in data science are\nbased on distances between entities. While some data science methodologies\nnatively take graphs as their input, there are many more that take their input\nin vectorial form. In this survey we discuss the fundamental problem of mapping\ngraphs to vectors, and its relation with mathematical programming. We discuss\napplications, solution methods, dimensional reduction techniques and some of\ntheir limits. We then present an application of some of these ideas to neural\nnetworks, showing that distance geometry techniques can give competitive\nperformance with respect to more traditional graph-to-vector mappings.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 16:11:33 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Liberti", "Leo", ""]]}, {"id": "1909.08901", "submitter": "Ankan Pal", "authors": "Daniele Di Tullio and Ankan Pal", "title": "A New Method for Geometric Interpretation of Elliptic Curve Discrete\n  Logarithm Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CG math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we intend to study the geometric meaning of the discrete\nlogarithm problem defined over an Elliptic Curve. The key idea is to reduce the\nElliptic Curve Discrete Logarithm Problem (EC-DLP) into a system of equations.\nThese equations arise from the interesection of quadric hypersurfaces in an\naffine space of lower dimension. In cryptography, this interpretation can be\nused to design attacks on EC-DLP. Presently, the best known attack algorithm\nhaving a sub-exponential time complexity is through the implementation of\nSummation Polynomials and Weil Descent. It is expected that the proposed\ngeometric interpretation can result in faster reduction of the problem into a\nsystem of equations. These overdetermined system of equations are hard to\nsolve. We have used F4 (Faugere) algorithms and got results for primes less\nthan 500,000. Quantum Algorithms can expedite the process of solving these\nover-determined system of equations. In the absence of fast algorithms for\ncomputing summation polynomials, we expect that this could be an alternative.\nWe do not claim that the proposed algorithm would be faster than Shor's\nalgorithm for breaking EC-DLP but this interpretation could be a candidate as\nan alternative to the 'summation polynomial attack' in the post-quantum era.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 10:13:07 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Di Tullio", "Daniele", ""], ["Pal", "Ankan", ""]]}, {"id": "1909.09043", "submitter": "Eryk Lipka", "authors": "Eryk Lipka", "title": "A note on minimal art galleries", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We will consider some extensions of the polygonal art gallery problem. In a\nrecent paper Morrison has shown the smallest (9 sides) example of an art\ngallery that cannot be observed by guards placed in every third corner. Author\nalso mentioned two related problems, for which the minimal examples are not\nknown. We will show that a polygonal fortress such that its exterior cannot be\nguarded by sentries placed in every second vertex has at least 12 sides. Also,\nwe will show an example of three-dimensional polyhedron such that its inside\ncannot be covered by placing guard in every vertex which has both fewer\nvertices and faces than previously known.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 15:17:43 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Lipka", "Eryk", ""]]}, {"id": "1909.09217", "submitter": "Andreas Tillmann", "authors": "Andreas M. Tillmann and Leif Kobbelt", "title": "Structured Discrete Shape Approximation: Theoretical Complexity and\n  Practical Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximating a two-dimensional shape contour (or\ncurve segment) using discrete assembly systems, which allow to build geometric\nstructures based on limited sets of node and edge types subject to edge length\nand orientation restrictions. We show that already deciding feasibility of such\napproximation problems is NP-hard, and remains intractable even for very simple\nsetups. We then devise an algorithmic framework that combines shape sampling\nwith exact cardinality-minimization to obtain good approximations using few\ncomponents. As a particular application and showcase example, we discuss\napproximating shape contours using the classical Zometool construction kit and\nprovide promising computational results, demonstrating that our algorithm is\ncapable of obtaining good shape representations within reasonable time, in\nspite of the problem's general intractability. We conclude the paper with an\noutlook on possible extensions of the developed methodology, in particular\nregarding 3D shape approximation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 19:52:07 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 19:22:31 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Tillmann", "Andreas M.", ""], ["Kobbelt", "Leif", ""]]}, {"id": "1909.09445", "submitter": "Sanjib Sadhu Mr", "authors": "Sanjib Sadhu and Xiaozhou He and Sasanka Roy and Subhas C. Nandy and\n  Suchismita Roy", "title": "Corrigendum to: \"Linear time algorithm to cover and hit a set of line\n  segments optimally by two axis-parallel squares\", Theoretical Computer\n  Science 769 (2019) 63--74", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper \"Linear time algorithm to cover and hit a set of line segments\noptimally by two axis-parallel squares\", TCS Volume 769 (2019), pages 63--74,\nthe LHIT problem is proposed as follows:\n  For a given set of non-intersecting line segments ${\\cal L} = \\{\\ell_1,\n\\ell_2, \\ldots, \\ell_n\\}$ in $I\\!\\!R^2$, compute two axis-parallel congruent\nsquares ${\\cal S}_1$ and ${\\cal S}_2$ of minimum size whose union hits all the\nline segments in $\\cal L$, and a linear time algorithm was proposed. Later it\nwas observed that the algorithm has a bug. In this corrigendum, we corrected\nthe algorithm. The time complexity of the corrected algorithm is $O(n^2)$.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 12:14:11 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 04:18:39 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 07:23:29 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Sadhu", "Sanjib", ""], ["He", "Xiaozhou", ""], ["Roy", "Sasanka", ""], ["Nandy", "Subhas C.", ""], ["Roy", "Suchismita", ""]]}, {"id": "1909.09772", "submitter": "Vladyslav Oles", "authors": "Vladyslav Oles, Nathan Lemons, Alexander Panchenko", "title": "Efficient estimation of a Gromov--Hausdorff distance between unweighted\n  graphs", "comments": "Fixed a type in the proof of Claim 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.GT math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gromov-Hausdorff distances measure shape difference between the objects\nrepresentable as compact metric spaces, e.g. point clouds, manifolds, or\ngraphs. Computing any Gromov-Hausdorff distance is equivalent to solving an\nNP-Hard optimization problem, deeming the notion impractical for applications.\nIn this paper we propose polynomial algorithm for estimating the so-called\nmodified Gromov-Hausdorff (mGH) distance, whose topological equivalence with\nthe standard Gromov-Hausdorff (GH) distance was established in \\cite{memoli12}\n(M\\'emoli, F, \\textit{Discrete \\& Computational Geometry, 48}(2) 416-440,\n2012). We implement the algorithm for the case of compact metric spaces induced\nby unweighted graphs as part of Python library \\verb|scikit-tda|, and\ndemonstrate its performance on real-world and synthetic networks. The algorithm\nfinds the mGH distances exactly on most graphs with the scale-free property. We\nuse the computed mGH distances to successfully detect outliers in real-world\nsocial and computer networks.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 04:38:53 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 03:25:09 GMT"}, {"version": "v3", "created": "Sun, 29 Sep 2019 01:16:23 GMT"}, {"version": "v4", "created": "Sat, 12 Oct 2019 10:27:18 GMT"}, {"version": "v5", "created": "Tue, 26 Nov 2019 06:48:05 GMT"}, {"version": "v6", "created": "Wed, 27 Nov 2019 11:00:12 GMT"}, {"version": "v7", "created": "Sun, 15 Dec 2019 00:18:00 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Oles", "Vladyslav", ""], ["Lemons", "Nathan", ""], ["Panchenko", "Alexander", ""]]}, {"id": "1909.10215", "submitter": "Andr\\'e van Renssen", "authors": "Vikrant Ashvinkumar, Joachim Gudmundsson, Christos Levcopoulos, Bengt\n  J. Nilsson, Andr\\'e van Renssen", "title": "Local Routing in Sparse and Lightweight Geometric Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online routing in a planar embedded graph is central to a number of fields\nand has been studied extensively in the literature. For most planar graphs no\n$O(1)$-competitive online routing algorithm exists. A notable exception is the\nDelaunay triangulation for which Bose and Morin [Online routing in\ntriangulations. SIAM Journal on Computing, 33(4):937-951, 2004] showed that\nthere exists an online routing algorithm that is $O(1)$-competitive. However, a\nDelaunay triangulation can have $\\Omega(n)$ vertex degree and a total weight\nthat is a linear factor greater than the weight of a minimum spanning tree.\n  We show a simple construction, given a set $V$ of $n$ points in the Euclidean\nplane, of a planar geometric graph on $V$ that has small weight (within a\nconstant factor of the weight of a minimum spanning tree on $V$), constant\ndegree, and that admits a local routing strategy that is $O(1)$-competitive.\nMoreover, the technique used to bound the weight works generally for any planar\ngeometric graph whilst preserving the admission of an $O(1)$-competitive\nrouting strategy.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 08:30:16 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Ashvinkumar", "Vikrant", ""], ["Gudmundsson", "Joachim", ""], ["Levcopoulos", "Christos", ""], ["Nilsson", "Bengt J.", ""], ["van Renssen", "Andr\u00e9", ""]]}, {"id": "1909.10958", "submitter": "Karthik C. S.", "authors": "Anat Ganor, Karthik C. S., and D\\\"om\\\"ot\\\"or P\\'alv\\\"olgyi", "title": "On Communication Complexity of Fixed Point Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CG cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brouwer's fixed point theorem states that any continuous function from a\ncompact convex space to itself has a fixed point. Roughgarden and Weinstein\n(FOCS 2016) initiated the study of fixed point computation in the two-player\ncommunication model, where each player gets a function from $[0,1]^n$ to\n$[0,1]^n$, and their goal is to find an approximate fixed point of the\ncomposition of the two functions. They left it as an open question to show a\nlower bound of $2^{\\Omega(n)}$ for the (randomized) communication complexity of\nthis problem, in the range of parameters which make it a total search problem.\nWe answer this question affirmatively.\n  Additionally, we introduce two natural fixed point problems in the two-player\ncommunication model.\n  $\\bullet$ Each player is given a function from $[0,1]^n$ to $[0,1]^{n/2}$,\nand their goal is to find an approximate fixed point of the concatenation of\nthe functions.\n  $\\bullet$ Each player is given a function from $[0,1]^n$ to $[0,1]^{n}$, and\ntheir goal is to find an approximate fixed point of the interpolation of the\nfunctions.\n  We show a randomized communication complexity lower bound of $2^{\\Omega(n)}$\nfor these problems (for some constant approximation factor).\n  Finally, we initiate the study of finding a panchromatic simplex in a\nSperner-coloring of a triangulation (guaranteed by Sperner's lemma) in the\ntwo-player communication model: A triangulation $T$ of the $d$-simplex is\npublicly known and one player is given a set $S_A\\subset T$ and a coloring\nfunction from $S_A$ to $\\{0,\\ldots ,d/2\\}$, and the other player is given a set\n$S_B\\subset T$ and a coloring function from $S_B$ to $\\{d/2+1,\\ldots ,d\\}$,\nsuch that $S_A\\dot\\cup S_B=T$, and their goal is to find a panchromatic\nsimplex. We show a randomized communication complexity lower bound of\n$|T|^{\\Omega(1)}$ for the aforementioned problem as well (when $d$ is large).\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 14:34:41 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 22:56:35 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Ganor", "Anat", ""], ["S.", "Karthik C.", ""], ["P\u00e1lv\u00f6lgyi", "D\u00f6m\u00f6t\u00f6r", ""]]}, {"id": "1909.10973", "submitter": "Julian Marcon", "authors": "Julian Marcon, Giacomo Castiglioni, David Moxey, Spencer J. Sherwin,\n  Joaquim Peir\\'o", "title": "$rp$-adaptation for compressible flows", "comments": "23 pages, 11 figures, accepted for publication in International\n  Journal for Numerical Methods in Engineering", "journal-ref": "International Journal for Numerical Methods in Engineering 121\n  (2020) 5405-5425", "doi": "10.1002/nme.6529", "report-no": null, "categories": "physics.comp-ph cs.CE cs.CG cs.NA math.NA physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an $rp$-adaptation strategy for high-fidelity simulation of\ncompressible inviscid flows with shocks. The mesh resolution in regions of flow\ndiscontinuities is increased by using a variational optimiser to $r$-adapt the\nmesh and cluster degrees of freedom there. In regions of smooth flow, we\nlocally increase or decrease the local resolution through increasing or\ndecreasing the polynomial order of the elements, respectively. This dual\napproach allows us to take advantage of the strengths of both methods for best\ncomputational performance, thereby reducing the overall cost of the simulation.\nThe adaptation workflow uses a sensor for both discontinuities and smooth\nregions that is cheap to calculate, but the framework is general and could be\nused in conjunction with other feature-based sensors or error estimators. We\ndemonstrate this proof-of-concept using two geometries at transonic and\nsupersonic flow regimes. The method has been implemented in the open-source\nspectral/$hp$ element framework $Nektar++$, and its dedicated high-order mesh\ngeneration tool $NekMesh$. The results show that the proposed $rp$-adaptation\nmethodology is a reasonably cost-effective way of improving accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 14:56:21 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 17:39:23 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Marcon", "Julian", ""], ["Castiglioni", "Giacomo", ""], ["Moxey", "David", ""], ["Sherwin", "Spencer J.", ""], ["Peir\u00f3", "Joaquim", ""]]}, {"id": "1909.11152", "submitter": "V\\'aclav Bla\\v{z}ej", "authors": "V\\'aclav Bla\\v{z}ej, Ji\\v{r}\\'i Fiala, Giuseppe Liotta", "title": "On the edge-length ratio of 2-trees", "comments": "Appears in the Proceedings of the 28th International Symposium on\n  Graph Drawing and Network Visualization (GD 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study planar straight-line drawings of graphs that minimize the ratio\nbetween the length of the longest and the shortest edge. We answer a question\nof Lazard et al. [Theor. Comput. Sci. 770 (2019), 88--94] and, for any given\nconstant $r$, we provide a $2$-tree which does not admit a planar straight-line\ndrawing with a ratio bounded by $r$. When the ratio is restricted to adjacent\nedges only, we prove that any $2$-tree admits a planar straight-line drawing\nwhose edge-length ratio is at most $4 + \\varepsilon$ for any arbitrarily small\n$\\varepsilon > 0$, hence the upper bound on the local edge-length ratio of\npartial $2$-trees is $4$.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 19:42:59 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 09:46:00 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 07:44:16 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Bla\u017eej", "V\u00e1clav", ""], ["Fiala", "Ji\u0159\u00ed", ""], ["Liotta", "Giuseppe", ""]]}, {"id": "1909.11620", "submitter": "Erva Ulu", "authors": "Erva Ulu, Nurcan Gecer Ulu, Walter Hsiao, Saigopal Nelaturi", "title": "Manufacturability Oriented Model Correction and Build Direction\n  Optimization for Additive Manufacturing", "comments": "Accepted to Journal of Mechanical Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to analyze and modify a shape to make it manufacturable\nfor a given additive manufacturing (AM) process. Different AM technologies,\nprocess parameters or materials introduce geometric constraints on what is\nmanufacturable or not. Given an input 3D model and minimum printable feature\nsize dictated by the manufacturing process characteristics and parameters, our\nalgorithm generates a corrected geometry that is printable with the intended AM\nprocess. A key issue in model correction for manufacturability is the\nidentification of critical features that are affected by the printing process.\nTo address this challenge, we propose a topology aware approach to construct\nthe allowable space for a print head to traverse during the 3D printing\nprocess. Combined with our build orientation optimization algorithm, the amount\nof modifications performed on the shape is kept at minimum while providing an\naccurate approximation of the as-manufactured part. We demonstrate our method\non a variety of 3D models and validate it by 3D printing the results.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 20:44:37 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Ulu", "Erva", ""], ["Ulu", "Nurcan Gecer", ""], ["Hsiao", "Walter", ""], ["Nelaturi", "Saigopal", ""]]}, {"id": "1909.12044", "submitter": "Sandor Kisfaludi-Bak", "authors": "S\\'andor Kisfaludi-Bak, D\\'aniel Marx, Tom C. van der Zanden", "title": "How does object fatness impact the complexity of packing in d\n  dimensions?", "comments": "Short version appears in ISAAC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Packing is a classical problem where one is given a set of subsets of\nEuclidean space called objects, and the goal is to find a maximum size subset\nof objects that are pairwise non-intersecting. The problem is also known as the\nIndependent Set problem on the intersection graph defined by the objects.\nAlthough the problem is NP-complete, there are several subexponential\nalgorithms in the literature. One of the key assumptions of such algorithms has\nbeen that the objects are fat, with a few exceptions in two dimensions; for\nexample, the packing problem of a set of polygons in the plane surprisingly\nadmits a subexponential algorithm. In this paper we give tight running time\nbounds for packing similarly-sized non-fat objects in higher dimensions.\n  We propose an alternative and very weak measure of fatness called the\nstabbing number, and show that the packing problem in Euclidean space of\nconstant dimension $d \\geq 3$ for a family of similarly sized objects with\nstabbing number $\\alpha$ can be solved in $2^{O(n^{1-1/d}\\alpha)}$ time. We\nprove that even in the case of axis-parallel boxes of fixed shape, there is no\n$2^{o(n^{1-1/d}\\alpha)}$ algorithm under ETH. This result smoothly bridges the\nwhole range of having constant-fat objects on one extreme ($\\alpha=1$) and a\nsubexponential algorithm of the usual running time, and having very \"skinny\"\nobjects on the other extreme ($\\alpha=n^{1/d}$), where we cannot hope to\nimprove upon the brute force running time of $2^{O(n)}$, and thereby\ncharacterizes the impact of fatness on the complexity of packing in case of\nsimilarly sized objects. We also study the same problem when parameterized by\nthe solution size $k$, and give a $n^{O(k^{1-1/d}\\alpha)}$ algorithm, with an\nalmost matching lower bound.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 12:08:31 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Kisfaludi-Bak", "S\u00e1ndor", ""], ["Marx", "D\u00e1niel", ""], ["van der Zanden", "Tom C.", ""]]}, {"id": "1909.12252", "submitter": "Chandrakana Nandi", "authors": "Chandrakana Nandi, Max Willsey, Adam Anderson, James R. Wilcox, Eva\n  Darulova, Dan Grossman, Zachary Tatlock", "title": "Synthesizing Structured CAD Models with Equality Saturation and Inverse\n  Transformations", "comments": "14 pages", "journal-ref": "PLDI 2020", "doi": "10.1145/3385412.3386012", "report-no": null, "categories": "cs.PL cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent program synthesis techniques help users customize CAD models(e.g., for\n3D printing) by decompiling low-level triangle meshes to Constructive Solid\nGeometry (CSG) expressions. Without loops or functions, editing CSG can require\nmany coordinated changes, and existing mesh decompilers use heuristics that can\nobfuscate high-level structure.\n  This paper proposes a second decompilation stage to robustly \"shrink\"\nunstructured CSG expressions into more editable programs with map and fold\noperators. We present Szalinski, a tool that uses Equality Saturation with\nsemantics-preserving CAD rewrites to efficiently search for smaller equivalent\nprograms. Szalinski relies on inverse transformations, a novel way for solvers\nto speculatively add equivalences to an E-graph. We qualitatively evaluate\nSzalinski in case studies, show how it composes with an existing mesh\ndecompiler, and demonstrate that Szalinski can shrink large models in seconds.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 16:58:08 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 15:05:08 GMT"}, {"version": "v3", "created": "Sun, 12 Apr 2020 18:15:51 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Nandi", "Chandrakana", ""], ["Willsey", "Max", ""], ["Anderson", "Adam", ""], ["Wilcox", "James R.", ""], ["Darulova", "Eva", ""], ["Grossman", "Dan", ""], ["Tatlock", "Zachary", ""]]}, {"id": "1909.12474", "submitter": "Yossi Bokor", "authors": "Yossi Bokor, Daniel Grixti-Cheng, Markus Hegland, Stephen Roberts,\n  Katharine Turner", "title": "Stratified Space Learning: Reconstructing Embedded Graphs", "comments": "7 pages, 3 figures, accepted for MODSIM 2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG math.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data-rich industries are interested in the efficient discovery and\nmodelling of structures underlying large data sets, as it allows for the fast\ntriage and dimension reduction of large volumes of data embedded in high\ndimensional spaces. The modelling of these underlying structures is also\nbeneficial for the creation of simulated data that better represents real data.\nIn particular, for systems testing in cases where the use of real data streams\nmight prove impractical or otherwise undesirable. We seek to discover and model\nthe structure by combining methods from topological data analysis with\nnumerical modelling. As a first step in combining these two areas, we examine\nthe recovery of the abstract graph $G$ structure, and model a linear embedding\n$|G|$ given only a noisy point cloud sample $X$ of $|G|$.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 02:41:19 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Bokor", "Yossi", ""], ["Grixti-Cheng", "Daniel", ""], ["Hegland", "Markus", ""], ["Roberts", "Stephen", ""], ["Turner", "Katharine", ""]]}, {"id": "1909.12945", "submitter": "Zhaobing Kang", "authors": "Zhaobing Kang, Wei Zou, Zheng Zhu, Chi Zhang and Hongxuan Ma", "title": "EPOSIT: An Absolute Pose Estimation Method for Pinhole and Fish-Eye\n  Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a generic 6DOF camera pose estimation method, which can\nbe used for both the pinhole camera and the fish-eye camera. Different from\nexisting methods, relative positions of 3D points rather than absolute\ncoordinates in the world coordinate system are employed in our method, and it\nhas a unique solution. The application scope of POSIT (Pose from Orthography\nand Scaling with Iteration) algorithm is generalized to fish-eye cameras by\ncombining with the radially symmetric projection model. The image point\nrelationship between the pinhole camera and the fish-eye camera is derived\nbased on their projection model. The general pose expression which fits for\ndifferent cameras can be acquired by four noncoplanar object points and their\ncorresponding image points. Accurate estimation results are calculated\niteratively. Experimental results on synthetic and real data show that the pose\nestimation results of our method are more stable and accurate than\nstate-of-the-art methods. The source code is available at\nhttps://github.com/k032131/EPOSIT.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 01:11:43 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Kang", "Zhaobing", ""], ["Zou", "Wei", ""], ["Zhu", "Zheng", ""], ["Zhang", "Chi", ""], ["Ma", "Hongxuan", ""]]}, {"id": "1909.13356", "submitter": "Michael O'Neil", "authors": "Felipe Vico, Leslie Greengard, Michael O'Neil, Manas Rachh", "title": "A fast boundary integral method for high-order multiscale mesh\n  generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present an algorithm to construct an infinitely\ndifferentiable smooth surface from an input consisting of a (rectilinear)\ntriangulation of a surface of arbitrary shape. The original surface can have\nnon-trivial genus and multiscale features, and our algorithm has computational\ncomplexity which is linear in the number of input triangles. We use a smoothing\nkernel to define a function $\\Phi$ whose level set defines the surface of\ninterest. Charts are subsequently generated as maps from the original\nuser-specified triangles to $\\mathbb R^3$. The degree of smoothness is\ncontrolled locally by the kernel to be commensurate with the fineness of the\ninput triangulation. The expression for~$\\Phi$ can be transformed into a\nboundary integral, whose evaluation can be accelerated using a fast multipole\nmethod. We demonstrate the effectiveness and cost of the algorithm with\npolyhedral and quadratic skeleton surfaces obtained from CAD and meshing\nsoftware.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 20:15:03 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Vico", "Felipe", ""], ["Greengard", "Leslie", ""], ["O'Neil", "Michael", ""], ["Rachh", "Manas", ""]]}, {"id": "1909.13472", "submitter": "Martin Royer", "authors": "Martin Royer (DATASHAPE), Fr\\'ed\\'eric Chazal (DATASHAPE), Cl\\'ement\n  Levrard (LPSM (UMR\\_8001)), Umeda Yuhei, Ike Yuichi", "title": "ATOL: Measure Vectorization for Automatic Topologically-Oriented\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust topological information commonly comes in the form of a set of\npersistence diagrams, finite measures that are in nature uneasy to affix to\ngeneric machine learning frameworks. We introduce a fast, learnt, unsupervised\nvectorization method for measures in Euclidean spaces and use it for reflecting\nunderlying changes in topological behaviour in machine learning contexts. The\nalgorithm is simple and efficiently discriminates important space regions where\nmeaningful differences to the mean measure arise. It is proven to be able to\nseparate clusters of persistence diagrams. We showcase the strength and\nrobustness of our approach on a number of applications, from emulous and modern\ngraph collections where the method reaches state-of-the-art performance to a\ngeometric synthetic dynamical orbits problem. The proposed methodology comes\nwith a single high level tuning parameter: the total measure encoding budget.\nWe provide a completely open access software.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 06:30:33 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 16:21:18 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 08:13:12 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Royer", "Martin", "", "DATASHAPE"], ["Chazal", "Fr\u00e9d\u00e9ric", "", "DATASHAPE"], ["Levrard", "Cl\u00e9ment", "", "LPSM"], ["Yuhei", "Umeda", ""], ["Yuichi", "Ike", ""]]}, {"id": "1909.13755", "submitter": "Jayson Lynch", "authors": "Divya Gopinath, Rohan Kodialam, Kevin Lu, Jayson Lynch, Santiago\n  Ospina", "title": "Hamiltonicity in Semi-Regular Tessellation Dual Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows NP-completeness for finding Hamiltonian cycles in induced\nsubgraphs of the dual graphs of semi-regular tessilations. It also shows\nNP-hardness for a new, wide class of graphs called augmented square grids. This\nwork follows up on prior studies of the complexity of finding Hamiltonian\ncycles in regular and semi-regular grid graphs.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 14:43:42 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Gopinath", "Divya", ""], ["Kodialam", "Rohan", ""], ["Lu", "Kevin", ""], ["Lynch", "Jayson", ""], ["Ospina", "Santiago", ""]]}]