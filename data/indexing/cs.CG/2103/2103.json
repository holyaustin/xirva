[{"id": "2103.00076", "submitter": "Haitao Wang", "authors": "Haitao Wang", "title": "An Optimal Deterministic Algorithm for Geodesic Farthest-Point Voronoi\n  Diagrams in Simple Polygons", "comments": "To appear in SoCG 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $S$ of $m$ point sites in a simple polygon $P$ of $n$ vertices,\nwe consider the problem of computing the geodesic farthest-point Voronoi\ndiagram for $S$ in $P$. It is known that the problem has an $\\Omega(n+m\\log m)$\ntime lower bound. Previously, a randomized algorithm was proposed [Barba, SoCG\n2019] that can solve the problem in $O(n+m\\log m)$ expected time. The previous\nbest deterministic algorithms solve the problem in $O(n\\log \\log n+ m\\log m)$\ntime [Oh, Barba, and Ahn, SoCG 2016] or in $O(n+m\\log m+m\\log^2 n)$ time [Oh\nand Ahn, SoCG 2017]. In this paper, we present a deterministic algorithm of\n$O(n+m\\log m)$ time, which is optimal. This answers an open question posed by\nMitchell in the Handbook of Computational Geometry two decades ago.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 22:45:00 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 03:32:13 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Wang", "Haitao", ""]]}, {"id": "2103.00151", "submitter": "Georg Muntingh PhD", "authors": "Juan Gerardo Alc\\'azar, Georg Muntingh", "title": "Affine equivalences of rational surfaces of translation, and\n  applications to rational minimal surfaces", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.CG cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an algorithm for determining whether two rational surfaces of\ntranslation are affinely equivalent. In turn, this also provides an algorithm\nfor determining whether two rational minimal surfaces are affinely equivalent.\nThis algorithm is applied to determine the symmetries of rational minimal\nsurfaces, in particular the higher-order Enneper surfaces. Finally certain\nparity-like conditions in the Weierstrass form of minimal surfaces are used to\nconstruct minimal surfaces with prescribed symmetries.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 07:36:21 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Alc\u00e1zar", "Juan Gerardo", ""], ["Muntingh", "Georg", ""]]}, {"id": "2103.00164", "submitter": "Menglin Yang", "authors": "Menglin Yang, Ziqiao Meng, Irwin King", "title": "FeatureNorm: L2 Feature Normalization for Dynamic Graph Embedding", "comments": "ICDM 2020;", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Dynamic graphs arise in a plethora of practical scenarios such as social\nnetworks, communication networks, and financial transaction networks. Given a\ndynamic graph, it is fundamental and essential to learn a graph representation\nthat is expected not only to preserve structural proximity but also jointly\ncapture the time-evolving patterns. Recently, graph convolutional network (GCN)\nhas been widely explored and used in non-Euclidean application domains. The\nmain success of GCN, especially in handling dependencies and passing messages\nwithin nodes, lies in its approximation to Laplacian smoothing. As a matter of\nfact, this smoothing technique can not only encourage must-link node pairs to\nget closer but also push cannot-link pairs to shrink together, which\npotentially cause serious feature shrink or oversmoothing problem, especially\nwhen stacking graph convolution in multiple layers or steps. For learning\ntime-evolving patterns, a natural solution is to preserve historical state and\ncombine it with the current interactions to obtain the most recent\nrepresentation. Then the serious feature shrink or oversmoothing problem could\nhappen when stacking graph convolution explicitly or implicitly according to\ncurrent prevalent methods, which would make nodes too similar to distinguish\neach other. To solve this problem in dynamic graph embedding, we analyze the\nshrinking properties in the node embedding space at first, and then design a\nsimple yet versatile method, which exploits L2 feature normalization constraint\nto rescale all nodes to hypersphere of a unit ball so that nodes would not\nshrink together, and yet similar nodes can still get closer. Extensive\nexperiments on four real-world dynamic graph datasets compared with competitive\nbaseline models demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 09:13:47 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 08:12:50 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Yang", "Menglin", ""], ["Meng", "Ziqiao", ""], ["King", "Irwin", ""]]}, {"id": "2103.00558", "submitter": "Jiawei Huang", "authors": "Hu Ding and Jiawei Huang", "title": "Is Simple Uniform Sampling Efficient for Center-Based Clustering With\n  Outliers: When and Why?", "comments": "arXiv admin note: text overlap with arXiv:1905.10143", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering has many important applications in computer science, but\nreal-world datasets often contain outliers. The presence of outliers can make\nthe clustering problems to be much more challenging. In this paper, we propose\na framework for solving three representative center-based clustering with\noutliers problems: $k$-center/median/means clustering with outliers. The\nframework actually is very simple, where we just need to take a small uniform\nsample from the input and run an existing approximation algorithm on the\nsample. However, our analysis is fundamentally different from the previous\n(uniform and non-uniform) sampling based ideas. To explain the effectiveness of\nuniform sampling in theory, we introduce a \"significance\" criterion and prove\nthat the performance of our framework depends on the significance degree of the\ngiven instance. In particular, the sample size can be independent of the input\ndata size $n$ and the dimensionality $d$, if we assume the given instance is\nsufficiently \"significant\", which is in fact a fairly appropriate assumption in\npractice. Due to its simplicity, the uniform sampling approach also enjoys\nseveral significant advantages over the non-uniform sampling approaches. The\nexperiments suggest that our framework can achieve comparable clustering\nresults with existing methods, but is much easier to implement and can greatly\nreduce the running times. To the best of our knowledge, this is the first work\nthat systematically studies the effectiveness of uniform sampling from both\ntheoretical and experimental aspects.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 16:43:37 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 05:40:40 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Ding", "Hu", ""], ["Huang", "Jiawei", ""]]}, {"id": "2103.01660", "submitter": "Vahideh Keikha", "authors": "Vahideh Keikha", "title": "On Optimal $w$-gons in Convex Polygons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $P$ be a set of $n$ points in $\\mathbb{R}^2$. For a given positive\ninteger $w<n$, our objective is to find a set $C \\subset P$ of points, such\nthat $CH(P\\setminus C)$ has the smallest number of vertices and $C$ has at most\n$n-w$ points. We discuss the $O(wn^3)$ time dynamic programming algorithm for\nmonotone decomposable functions (MDF) introduced for finding a class of optimal\nconvex $w$-gons, with vertices chosen from $P$, and improve it to $O(n^3 \\log\nw)$ time, which gives an improvement to the existing algorithm for MDFs if\ntheir input is a convex polygon.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 11:45:10 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Keikha", "Vahideh", ""]]}, {"id": "2103.02486", "submitter": "Julius Nehring-Wirxel", "authors": "Julius Nehring-Wirxel and Philip Trettner and Leif Kobbelt", "title": "Fast Exact Booleans for Iterated CSG using Octree-Embedded BSPs", "comments": null, "journal-ref": null, "doi": "10.1016/j.cad.2021.103015", "report-no": null, "categories": "cs.CG cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present octree-embedded BSPs, a volumetric mesh data structure suited for\nperforming a sequence of Boolean operations (iterated CSG) efficiently. At its\ncore, our data structure leverages a plane-based geometry representation and\ninteger arithmetics to guarantee unconditionally robust operations. These\ntypically present considerable performance challenges which we overcome by\nusing custom-tailored fixed-precision operations and an efficient algorithm for\ncutting a convex mesh against a plane. Consequently, BSP Booleans and mesh\nextraction are formulated in terms of mesh cutting. The octree is used as a\nglobal acceleration structure to keep modifications local and bound the BSP\ncomplexity. With our optimizations, we can perform up to 2.5 million mesh-plane\ncuts per second on a single core, which creates roughly 40-50 million output\nBSP nodes for CSG. We demonstrate our system in two iterated CSG settings:\nsweep volumes and a milling simulation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 15:54:50 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Nehring-Wirxel", "Julius", ""], ["Trettner", "Philip", ""], ["Kobbelt", "Leif", ""]]}, {"id": "2103.02749", "submitter": "Vitaliy Kurlin", "authors": "Olga Anosova and Vitaliy Kurlin", "title": "Introduction to Periodic Geometry and Topology", "comments": "40 pages, 21 figures. The second version contains minor amendments,\n  especially in algorithmic section 8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This monograph introduces key concepts and problems in the new research area\nof Periodic Geometry and Topology for materials applications.Periodic\nstructures such as solid crystalline materials or textiles were previously\nclassified in discrete and coarse ways that depend on manual choices or are\nunstable under perturbations. Since crystal structures are determined in a\nrigid form, their finest natural equivalence is defined by rigid motion or\nisometry, which preserves inter-point distances. Due to atomic vibrations,\nisometry classes of periodic point sets form a continuous space whose geometry\nand topology were unknown. The key new problem in Periodic Geometry is to\nunambiguously parameterize this space of isometry classes by continuous\ncoordinates that allow a complete reconstruction of any crystal. The major part\nof this manuscript reviews the recently developed isometry invariants to\nresolve the above problem: (1) density functions computed from higher order\nVoronoi zones, (2) distance-based invariants that allow ultra-fast\nvisualizations of huge crystal datasets, and (3) the complete invariant isoset\n(a DNA-type code) with a first continuous metric on all periodic crystals. The\nmain goal of Periodic Topology is to classify textiles up to periodic isotopy,\nwhich is a continuous deformation of a thickened plane without a fixed lattice\nbasis. This practical problem substantially differs from past research focused\non links in a fixed thickened torus.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 23:18:43 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 23:01:42 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Anosova", "Olga", ""], ["Kurlin", "Vitaliy", ""]]}, {"id": "2103.02939", "submitter": "Jovana Jezdimirovi\\'c", "authors": "Jovana Jezdimirovi\\' c, Alexandre Chemin, Maxence Reberol,\n  Fran\\c{c}ois Henrotte, Jean Fran\\c{c}ois Remacle", "title": "Quad layouts with high valence singularities for flexible quad meshing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CG cs.NA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A novel algorithm that produces a quad layout based on imposed set of\nsingularities is proposed. In this paper, we either use singularities that\nappear naturally, e.g., by minimizing Ginzburg-Landau energy, or use as an\ninput user-defined singularity pattern, possibly with high valence\nsingularities that do not appear naturally in cross-field computations. The\nfirst contribution of the paper is the development of a formulation that allows\ncomputing a cross-field from a given set of singularities through the\nresolution of two linear PDEs. A specific mesh refinement is applied at the\nvicinity of singularities to accommodate the large gradients of cross\ndirections that appear in the vicinity of singularities of high valence. The\nsecond contribution of the paper is a correction scheme that repairs limit\ncycles and/or non-quadrilateral patches. Finally, a high quality\nblock-structured quad mesh is generated from the quad layout and per-partition\nparameterization.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 10:41:54 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["c", "Jovana Jezdimirovi\\'", ""], ["Chemin", "Alexandre", ""], ["Reberol", "Maxence", ""], ["Henrotte", "Fran\u00e7ois", ""], ["Remacle", "Jean Fran\u00e7ois", ""]]}, {"id": "2103.02980", "submitter": "Thomas Takacs", "authors": "Pascal Weinm\\\"uller, Thomas Takacs", "title": "Construction of approximate $C^1$ bases for isogeometric analysis on\n  two-patch domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop and study approximately smooth basis constructions\nfor isogeometric analysis over two-patch domains. One key element of\nisogeometric analysis is that it allows high order smoothness within one patch.\nHowever, for representing complex geometries, a multi-patch construction is\nneeded. In this case, a $C^0$-smooth basis is easy to obtain, whereas\n$C^1$-smooth isogeometric functions require a special construction. Such spaces\nare of interest when solving numerically fourth-order PDE problems, such as the\nbiharmonic equation and the Kirchhoff-Love plate or shell formulation, using an\nisogeometric Galerkin method.\n  With the construction of so-called analysis-suitable $G^1$ (in short,\nAS-$G^1$) parametrizations, as introduced in (Collin, Sangalli, Takacs; CAGD,\n2016), it is possible to construct $C^1$ isogeometric spaces which possess\noptimal approximation properties. These geometries need to satisfy certain\nconstraints along the interfaces and additionally require that the regularity\n$r$ and degree $p$ of the underlying spline space satisfy $1 \\leq r \\leq p-2$.\nThe problem is that most complex geometries are not AS-$G^1$ geometries.\nTherefore, we define basis functions for isogeometric spaces by enforcing\napproximate $C^1$ conditions following the basis construction from (Kapl,\nSangalli, Takacs; CAGD, 2017). For this reason, the defined function spaces are\nnot exactly $C^1$ but only approximately.\n  We study the convergence behavior and define function spaces that converge\noptimally under $h$-refinement, by locally introducing functions of higher\npolynomial degree and lower regularity. The convergence rate is optimal in\nseveral numerical tests performed on domains with non-trivial interfaces. While\nan extension to more general multi-patch domains is possible, we restrict\nourselves to the two-patch case and focus on the construction over a single\ninterface.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 11:58:55 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Weinm\u00fcller", "Pascal", ""], ["Takacs", "Thomas", ""]]}, {"id": "2103.03394", "submitter": "Farzan Erlik Nowruzi", "authors": "Farzan Erlik Nowruzi, Dhanvin Kolhatkar, Prince Kapoor, Robert\n  Laganiere", "title": "Point Cloud based Hierarchical Deep Odometry Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Processing point clouds using deep neural networks is still a challenging\ntask. Most existing models focus on object detection and registration with deep\nneural networks using point clouds. In this paper, we propose a deep model that\nlearns to estimate odometry in driving scenarios using point cloud data. The\nproposed model consumes raw point clouds in order to extract frame-to-frame\nodometry estimation through a hierarchical model architecture. Also, a local\nbundle adjustment variation of this model using LSTM layers is implemented.\nThese two approaches are comprehensively evaluated and are compared against the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 00:17:58 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Nowruzi", "Farzan Erlik", ""], ["Kolhatkar", "Dhanvin", ""], ["Kapoor", "Prince", ""], ["Laganiere", "Robert", ""]]}, {"id": "2103.03862", "submitter": "Jacob Whitehill", "authors": "Anand Ramakrishnan, Minh Pham, and Jacob Whitehill", "title": "Harnessing Geometric Constraints from Emotion Labels to improve Face\n  Verification", "comments": "8 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  For the task of face verification, we explore the utility of harnessing\nauxiliary facial emotion labels to impose explicit geometric constraints on the\nembedding space when training deep embedding models. We introduce several novel\nloss functions that, in conjunction with a standard Triplet Loss [43], or\nArcFace loss [10], provide geometric constraints on the embedding space; the\nlabels for our loss functions can be provided using either manually annotated\nor automatically detected auxiliary emotion labels. Our method is implemented\npurely in terms of the loss function and does not require any changes to the\nneural network backbone of the embedding function.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 18:27:38 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 14:17:43 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 15:45:31 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Ramakrishnan", "Anand", ""], ["Pham", "Minh", ""], ["Whitehill", "Jacob", ""]]}, {"id": "2103.04046", "submitter": "Mustafa Hajij", "authors": "Mustafa Hajij, Ghada Zamzmi, Xuanting Cai", "title": "Simplicial Complex Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CG cs.CV math.AT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simplicial complexes form an important class of topological spaces that are\nfrequently used to in many applications areas such as computer-aided design,\ncomputer graphics, and simulation. The representation learning on graphs, which\nare just 1-d simplicial complexes, has witnessed a great attention and success\nin the past few years. Due to the additional complexity higher dimensional\nsimplicial hold, there has not been enough effort to extend representation\nlearning to these objects especially when it comes to learn entire-simplicial\ncomplex representation. In this work, we propose a method for simplicial\ncomplex-level representation learning that embeds a simplicial complex to a\nuniversal embedding space in a way that complex-to-complex proximity is\npreserved. Our method utilizes a simplex-level embedding induced by a\npre-trained simplicial autoencoder to learn an entire simplicial complex\nrepresentation. To the best of our knowledge, this work presents the first\nmethod for learning simplicial complex-level representation.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 06:33:04 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 07:18:25 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 22:01:33 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Hajij", "Mustafa", ""], ["Zamzmi", "Ghada", ""], ["Cai", "Xuanting", ""]]}, {"id": "2103.04183", "submitter": "Yuxuan Yu", "authors": "Yuxuan Yu, Jialei Ginny Liu and Yongjie Jessica Zhang", "title": "HexDom: Polycube-Based Hexahedral-Dominant Mesh Generation", "comments": "arXiv admin note: substantial text overlap with arXiv:2011.14213", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend our earlier polycube-based all-hexahedral mesh\ngeneration method to hexahedral-dominant mesh generation, and present the\nHexDom software package. Given the boundary representation of a solid model,\nHexDom creates a hex-dominant mesh by using a semi-automated polycube-based\nmesh generation method. The resulting hexahedral dominant mesh includes\nhexahedra, tetrahedra, and triangular prisms. By adding non-hexahedral\nelements, we are able to generate better quality hexahedral elements than in\nall-hexahedral meshes. We explain the underlying algorithms in four modules\nincluding segmentation, polycube construction, hex-dominant mesh generation and\nquality improvement, and use a rockerarm model to explain how to run the\nsoftware. We also apply our software to a number of other complex models to\ntest their robustness. The software package and all tested models are availabe\nin github (https://github.com/CMU-CBML/HexDom).\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 19:51:07 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 15:19:27 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Yu", "Yuxuan", ""], ["Liu", "Jialei Ginny", ""], ["Zhang", "Yongjie Jessica", ""]]}, {"id": "2103.04540", "submitter": "Hitesh Gakhar", "authors": "Hitesh Gakhar and Jose A. Perea", "title": "Sliding Window Persistence of Quasiperiodic Functions", "comments": "32 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG math.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A function is called quasiperiodic if its fundamental frequencies are\nlinearly independent over the rationals. With appropriate parameters, the\nsliding window point clouds of such functions can be shown to be dense in tori\nwith dimension equal to the number of independent frequencies. In this paper,\nwe develop theoretical and computational techniques to study the persistent\nhomology of such sets. Specifically, we provide parameter optimization schemes\nfor sliding windows of quasiperiodic functions, and present theoretical lower\nbounds on their Rips persistent homology. The latter leverages a recent\npersistent K\\\"{u}nneth formula. The theory is illustrated via computational\nexamples and an application to dissonance detection in music audio samples.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 04:15:16 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 16:03:56 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Gakhar", "Hitesh", ""], ["Perea", "Jose A.", ""]]}, {"id": "2103.04652", "submitter": "Maxence Reberol", "authors": "Maxence Reberol, Christos Georgiadis, Jean-Fran\\c{c}ois Remacle", "title": "Quasi-structured quadrilateral meshing in Gmsh -- a robust pipeline for\n  complex CAD models", "comments": "33 pages, 10 figures, supplemental at\n  https://www.hextreme.eu/data/quadqs2021_supplemental.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.CG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an end-to-end pipeline to robustly generate high-quality\nquadrilateral meshes for complex CAD models. An initial quad-dominant mesh is\ngenerated with frontal point insertion guided by a locally integrable cross\nfield and a scalar size map adapted to the small CAD features. After triangle\ncombination and midpoint-subdivision into an all-quadrilateral mesh, the\ntopology of the mesh is modified to reduce the number of irregular vertices.\nThe idea is to preserve the irregular vertices matching cross-field\nsingularities and to eliminate the others. The topological modifications are\neither local and based on disk quadrangulations, or more global with the\nremeshing of patches of quads according to predefined patterns. Validity of the\nquad mesh is guaranteed by monitoring element quality during all operations and\nreverting the changes when necessary. Advantages of our approach include\nrobustness, strict respect of the CAD features and support for user-prescribed\nsize constraints. The quad mesher, which is available in Gmsh, is validated and\nillustrated on two datasets of CAD models.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 10:23:55 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Reberol", "Maxence", ""], ["Georgiadis", "Christos", ""], ["Remacle", "Jean-Fran\u00e7ois", ""]]}, {"id": "2103.04799", "submitter": "Tiago Guerreiro", "authors": "Tiago Guerreiro, Zijia Li, Josef Schicho", "title": "Classification of Higher Mobility Linkages", "comments": "18 pp. Comments are welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.RO cs.SC math.AG math.RA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We provide a complete classification of paradoxical $n$-linkages, $n\\geq6$\nwhose mobility is $n-4$ or higher containing $R$, $P$ or $H$ joints. We also\nexplicitly write down strong necessary conditions for $nR$-linkages of mobility\n$n-5$.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 14:49:03 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Guerreiro", "Tiago", ""], ["Li", "Zijia", ""], ["Schicho", "Josef", ""]]}, {"id": "2103.04971", "submitter": "Loic Crombez", "authors": "Crombez Lo\\\"ic", "title": "Digital Convex + Unimodular Mapping =8-Connected (All Points but One\n  4-Connected)", "comments": "13 pages + appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In two dimensional digital geometry, two lattice points are 4-connected\n(resp. 8-connected) if their Euclidean distance is at most one (resp.\n$\\sqrt{2}$). A set $S \\subset Z^2$ is 4-connected (resp. 8-connected) if for\nall pair of points $p_1, p_2$ in $S$ there is a path connecting $p_1$ to $p_2$\nsuch that every edge consists of a 4-connected (resp. 8-connected) pair of\npoints. The original definition of digital convexity which states that a set $S\n\\subset Z^d$ is digital convex if $\\conv(S) \\cap Z^d= S$, where $\\conv(S)$\ndenotes the convex hull of $S$ does not guarantee connectivity. However,\nmultiple algorithms assume connectivity. In this paper, we show that in two\ndimensional space, any digital convex set $S$ of $n$ points is unimodularly\nequivalent to a 8-connected digital convex set $C$. In fact, the resulting\ndigital convex set $C$ is 4-connected except for at most one point which is\n8-connected to the rest of the set. The matrix of $SL_2(Z)$ defining the affine\nisomorphism of $Z^2$ between the two unimodularly equivalent lattice polytopes\n$S$ and $C$ can be computed in roughly $O(n)$ time. We also show that no\nsimilar result is possible in higher dimension.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 18:48:35 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Lo\u00efc", "Crombez", ""]]}, {"id": "2103.05931", "submitter": "R Inkulu", "authors": "R. Inkulu, A. Singh", "title": "Vertex Fault-Tolerant Spanners for Weighted Points in Polygonal Domains", "comments": "arXiv admin note: substantial text overlap with arXiv:2011.03354", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a set $S$ of $n$ points, a weight function $w$ to associate a\nnon-negative weight to each point in $S$, a positive integer $k \\ge 1$, and a\nreal number $\\epsilon > 0$, we devise the following algorithms to compute a\n$k$-vertex fault-tolerant spanner network $G(S, E)$ for the metric space\ninduced by the weighted points in $S$: (1) When the points in $S$ are located\nin a simple polygon, we present an algorithm to compute $G$ with multiplicative\nstretch $\\sqrt{10}+\\epsilon$, and the number of edges in $G$ (size of $G$) is\n$O(k n (\\lg{n})^2)$. (2) When the points in $S$ are located in the free space\nof a polygonal domain $\\cal P$ with $h$ number of obstacles, we present an\nalgorithm to compute $G$ with multiplicative stretch $6+\\epsilon$ and size\n$O(\\sqrt{h} k n(\\lg{n})^2)$. (3) When the points in $S$ are located on a\npolyhedral terrain, we devise an algorithm to compute $G$ with multiplicative\nstretch $6+\\epsilon$ and size $O(k n (\\lg{n})^2)$.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 08:38:45 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Inkulu", "R.", ""], ["Singh", "A.", ""]]}, {"id": "2103.05960", "submitter": "Matthijs Ebbens", "authors": "Matthijs Ebbens, Iordan Iordanov, Monique Teillaud, Gert Vegter", "title": "Delaunay triangulations of generalized Bolza surfaces", "comments": "50 pages, 28 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.GT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Bolza surface can be seen as the quotient of the hyperbolic plane,\nrepresented by the Poincar\\'e disk model, under the action of the group\ngenerated by the hyperbolic isometries identifying opposite sides of a regular\noctagon centered at the origin. We consider generalized Bolza surfaces\n$\\mathbb{M}_g$, where the octagon is replaced by a regular $4g$-gon, leading to\na genus $g$ surface. We propose an extension of Bowyer's algorithm to these\nsurfaces. In particular, we compute the value of the systole of $\\mathbb{M}_g$.\nWe also propose algorithms computing small sets of points on $\\mathbb{M}_g$\nthat are used to initialize Bowyer's algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 09:33:09 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Ebbens", "Matthijs", ""], ["Iordanov", "Iordan", ""], ["Teillaud", "Monique", ""], ["Vegter", "Gert", ""]]}, {"id": "2103.06040", "submitter": "Frederik Br\\\"uning", "authors": "Hugo A. Akitaya, Frederik Br\\\"uning, Erin Chambers, Anne Driemel", "title": "Covering a Curve with Subtrajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study subtrajectory clustering under the Fr\\'echet distance. Given a\npolygonal curve $P$ with $n$ vertices, and parameters $k$ and $\\ell$, the goal\nis to find $k$ center curves of complexity at most $\\ell$ such that every point\non $P$ is covered by a subtrajectory that has small Fr\\'echet distance to one\nof the $k$ center curves. We suggest a new approach to solving this problem\nbased on a set cover formulation leading to polynomial-time approximation\nalgorithms. Our solutions rely on carefully designed set system oracles for\nsystems of subtrajectories.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 13:19:57 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Akitaya", "Hugo A.", ""], ["Br\u00fcning", "Frederik", ""], ["Chambers", "Erin", ""], ["Driemel", "Anne", ""]]}, {"id": "2103.06139", "submitter": "Pierre-Alain Fayolle", "authors": "Markus Friedrich and Pierre-Alain Fayolle", "title": "On the Complexity of the CSG Tree Extraction Problem", "comments": "Add references for the programming language based approaches and the\n  construction of the intersection graph", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note, we discuss the complexity of the search space for the\nproblem of finding a CSG expression (or CSG tree) corresponding to an input\npoint-cloud and a list of fitted solid primitives.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 10:52:51 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 04:41:26 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Friedrich", "Markus", ""], ["Fayolle", "Pierre-Alain", ""]]}, {"id": "2103.06408", "submitter": "Henry Adams", "authors": "Henry Adams and Baris Coskunuzer", "title": "Geometric Approaches on Persistent Homology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG math.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce several geometric notions, including thick-thin decompositions\nand the width of a homology class, to the theory of persistent homology. These\nideas provide geometric interpretations of persistence diagrams. Indeed, we\ngive quantitative and geometric descriptions of the ``size'' or ``persistence''\nof a homology class. As a case study, we analyze the power filtration on\nunweighted graphs, and provide explicit bounds for the life spans of homology\nclasses in persistence diagrams in all dimensions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 01:46:13 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 14:38:46 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Adams", "Henry", ""], ["Coskunuzer", "Baris", ""]]}, {"id": "2103.06597", "submitter": "Meike Neuwohner", "authors": "Meike Neuwohner", "title": "Reducing Moser's Square Packing Problem to a Bounded Number of Squares", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The problem widely known as Moser's Square Packing Problem asks for the\nsmallest area $A$ such that for any set $S$ of squares of total area $1$, there\nexists a rectangle $R$ of area $A$ into which the squares in $S$ permit an\ninternally-disjoint, axis-parallel packing. It was formulated by Moser in 1966\nand remains unsolved so far. The best known lower bound of\n$\\frac{2+\\sqrt{3}}{3}\\leq A$ is due to Novotn\\'y and has been shown to be\nsufficient for up to $11$ squares by Platz, while Hougardy and Ilhan have\nestablished that $A < 1.37$. In this paper, we reduce Moser's Square Packing\nProblem to a problem on a finite set of squares in the following sense: We show\nhow to compute a natural number $N$ such that it is enough to determine the\nvalue of $A$ for sets containing at most $N$ squares with total area $1$.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 10:52:22 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Neuwohner", "Meike", ""]]}, {"id": "2103.06614", "submitter": "Ignasi Sau", "authors": "Julien Baste, Ignasi Sau, Dimitrios M. Thilikos", "title": "Hitting minors on bounded treewidth graphs. III. Lower bounds", "comments": "41 pages, 20 figures. arXiv admin note: substantial text overlap with\n  arXiv:1907.04442, arXiv:1704.07284", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a finite collection of graphs ${\\cal F}$, the ${\\cal F}$-M-DELETION\nproblem consists in, given a graph $G$ and an integer $k$, decide whether there\nexists $S \\subseteq V(G)$ with $|S| \\leq k$ such that $G \\setminus S$ does not\ncontain any of the graphs in ${\\cal F}$ as a minor. We are interested in the\nparameterized complexity of ${\\cal F}$-M-DELETION when the parameter is the\ntreewidth of $G$, denoted by $tw$. Our objective is to determine, for a fixed\n${\\cal F}$, the smallest function $f_{{\\cal F}}$ such that ${\\cal\nF}$-M-DELETION can be solved in time $f_{{\\cal F}}(tw) \\cdot n^{O(1)}$ on\n$n$-vertex graphs. We provide lower bounds under the ETH on $f_{{\\cal F}}$ for\nseveral collections ${\\cal F}$. We first prove that for any ${\\cal F}$\ncontaining connected graphs of size at least two, $f_{{\\cal F}}(tw)=\n2^{\\Omega(tw)}$, even if the input graph $G$ is planar. Our main contribution\nconsists of superexponential lower bounds for a number of collections ${\\cal\nF}$, inspired by a reduction of Bonnet et al.~[IPEC, 2017]. In particular, we\nprove that when ${\\cal F}$ contains a single connected graph $H$ that is either\n$P_5$ or is not a minor of the banner (that is, the graph consisting of a $C_4$\nplus a pendent edge), then $f_{{\\cal F}}(tw)= 2^{\\Omega(tw \\cdot \\log tw)}$.\nThis is the third of a series of articles on this topic, and the results given\nhere together with other ones allow us, in particular, to provide a tight\ndichotomy on the complexity of $\\{H\\}$-M-DELETION, in terms of $H$, when $H$ is\nconnected.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 11:34:12 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Baste", "Julien", ""], ["Sau", "Ignasi", ""], ["Thilikos", "Dimitrios M.", ""]]}, {"id": "2103.06620", "submitter": "Mai Lan Tran", "authors": "Mai Lan Tran, Changbom Park, Jae-Hun Jung", "title": "Topological Data Analysis of Korean Music in Jeongganbo: A Cycle\n  Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CG eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Jeongganbo is a unique music representation invented by Sejong the Great.\nContrary to the western music notation, the pitch of each note is encrypted and\nthe length is visualized directly in a matrix form in Jeongganbo. We use\ntopological data analysis (TDA) to analyze the Korean music written in\nJeongganbo for Suyeonjang, Songuyeo, and Taryong, those well-known pieces\nplayed at the palace and among noble community. We are particularly interested\nin the cycle structure. We first define and determine the node elements of each\nmusic, characterized uniquely with its pitch and length. Then we transform the\nmusic into a graph and define the distance between the nodes as their adjacent\noccurrence rate. The graph is used as a point cloud whose homological structure\nis investigated by measuring the hole structure in each dimension. We identify\ncycles of each music, match those in Jeongganbo, and show how those cycles are\ninterconnected. The main discovery of this work is that the cycles of\nSuyeonjang and Songuyeo, categorized as a special type of cyclic music known as\nDodeuri, frequently overlap each other when appearing in the music while the\ncycles found in Taryong, which does not belong to Dodeuri class, appear\nindividually.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 11:42:02 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 18:19:03 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Tran", "Mai Lan", ""], ["Park", "Changbom", ""], ["Jung", "Jae-Hun", ""]]}, {"id": "2103.06696", "submitter": "Maria Saumell", "authors": "Ankush Acharyya, Ramesh K. Jallu, Maarten L\\\"offler, Gert G.T. Meijer,\n  Maria Saumell, Rodrigo I. Silveira, Frank Staals, Hans Raj Tiwary", "title": "Terrain prickliness: theoretical grounds for low complexity viewsheds", "comments": "13 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task when working with terrain models is computing viewsheds:\nthe parts of the terrain visible from a given viewpoint. When the terrain is\nmodeled as a polyhedral terrain, the viewshed is composed of the union of all\nthe triangle parts that are visible from the viewpoint. The complexity of a\nviewshed can vary significantly, from constant to quadratic in the number of\nterrain vertices, depending on the terrain topography and the viewpoint\nposition.\n  In this work we study a new topographic attribute, the \\emph{prickliness},\nthat measures the number of local maxima in a terrain from all possible\nperspectives. We show that the prickliness effectively captures the potential\nof 2.5D terrains to have high complexity viewsheds, and we present near-optimal\nalgorithms to compute the prickliness of 1.5D and 2.5D terrains. We also report\non some experiments relating the prickliness of real word 2.5D terrains to the\nsize of the terrains and to their viewshed complexity.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 14:35:10 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Acharyya", "Ankush", ""], ["Jallu", "Ramesh K.", ""], ["L\u00f6ffler", "Maarten", ""], ["Meijer", "Gert G. T.", ""], ["Saumell", "Maria", ""], ["Silveira", "Rodrigo I.", ""], ["Staals", "Frank", ""], ["Tiwary", "Hans Raj", ""]]}, {"id": "2103.06801", "submitter": "Johannes Zink", "authors": "Jonathan Klawitter and Johannes Zink", "title": "Upward Planar Drawings with Three Slopes", "comments": "Preliminary work to appear at EuroCG'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study upward planar straight-line drawings that use only three different\nslopes. We show that deciding whether a digraph admits such a drawing is\nNP-hard already for embedded outerplanar digraphs, though linear-time solvable\nfor trees with and without given embedding.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 17:13:15 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Klawitter", "Jonathan", ""], ["Zink", "Johannes", ""]]}, {"id": "2103.06916", "submitter": "Tim Ophelders", "authors": "Tim Ophelders, Ignaz Rutter, Bettina Speckmann, Kevin Verbeek", "title": "Polygon-Universal Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study a fundamental question from graph drawing: given a pair $(G,C)$ of a\ngraph $G$ and a cycle $C$ in $G$ together with a simple polygon $P$, is there a\nstraight-line drawing of $G$ inside $P$ which maps $C$ to $P$? We say that such\na drawing of $(G,C)$ respects $P$. We fully characterize those instances\n$(G,C)$ which are polygon-universal, that is, they have a drawing that respects\n$P$ for any simple (not necessarily convex) polygon $P$. Specifically, we\nidentify two necessary conditions for an instance to be polygon-universal. Both\nconditions are based purely on graph and cycle distances and are easy to check.\nWe show that these two conditions are also sufficient. Furthermore, if an\ninstance $(G,C)$ is planar, that is, if there exists a planar drawing of $G$\nwith $C$ on the outer face, we show that the same conditions guarantee for\nevery simple polygon $P$ the existence of a planar drawing of $(G,C)$ that\nrespects $P$. If $(G,C)$ is polygon-universal, then our proofs directly imply a\nlinear-time algorithm to construct a drawing that respects a given polygon $P$.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 19:26:58 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Ophelders", "Tim", ""], ["Rutter", "Ignaz", ""], ["Speckmann", "Bettina", ""], ["Verbeek", "Kevin", ""]]}, {"id": "2103.07258", "submitter": "Phillip Keldenich", "authors": "S\\'andor P. Fekete and Vijaykrishna Gurunathan and Kushagra Juneja and\n  Phillip Keldenich and Linda Kleist and Christian Scheffer", "title": "Packing Squares into a Disk with Optimal Worst-Case Density", "comments": "23 pages, 15 figures. Full version of a SoCG 2021 paper with the same\n  title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a tight result for a fundamental problem arising from packing\nsquares into a circular container: The critical density of packing squares into\na disk is $\\delta=\\frac{8}{5\\pi}\\approx 0.509$. This implies that any set of\n(not necessarily equal) squares of total area $A \\leq \\frac{8}{5}$ can always\nbe packed into a disk with radius 1; in contrast, for any $\\varepsilon>0$ there\nare sets of squares of total area $\\frac{8}{5}+\\varepsilon$ that cannot be\npacked, even if squares may be rotated. This settles the last (and arguably,\nmost elusive) case of packing circular or square objects into a circular or\nsquare container: The critical densities for squares in a square\n$\\left(\\frac{1}{2}\\right)$, circles in a square\n$\\left(\\frac{\\pi}{(3+2\\sqrt{2})}\\approx 0.539\\right)$ and circles in a circle\n$\\left(\\frac{1}{2}\\right)$ have already been established, making use of\nrecursive subdivisions of a square container into pieces bounded by straight\nlines, or the ability to use recursive arguments based on similarity of objects\nand container; neither of these approaches can be applied when packing squares\ninto a circular container. Our proof uses a careful manual analysis,\ncomplemented by a computer-assisted part that is based on interval arithmetic.\nBeyond the basic mathematical importance, our result is also useful as a\nblackbox lemma for the analysis of recursive packing algorithms. At the same\ntime, our approach showcases the power of a general framework for\ncomputer-assisted proofs, based on interval arithmetic.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 13:29:23 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 11:40:12 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Fekete", "S\u00e1ndor P.", ""], ["Gurunathan", "Vijaykrishna", ""], ["Juneja", "Kushagra", ""], ["Keldenich", "Phillip", ""], ["Kleist", "Linda", ""], ["Scheffer", "Christian", ""]]}, {"id": "2103.07344", "submitter": "Yuri Malykhin", "authors": "Yuri Malykhin, Evgeny Shchepin", "title": "Search of fractal space-filling curves with minimal dilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an algorithm for a search of extremal fractal curves in large\ncurve classes. It heavily uses SAT-solvers -- heuristic algorithms that find\nmodels for CNF boolean formulas. Our algorithm was implemented and applied to\nthe search of fractal surjective curves $\\gamma\\colon[0,1]\\to[0,1]^d$ with\nminimal dilation\n$$\\sup_{t_1<t_2}\\frac{\\|\\gamma(t_2)-\\gamma(t_1)\\|^d}{t_2-t_1}.$$\n  We report new results of that search in the case of Euclidean norm.\n  One of the results is a new curve that we call \"YE\", a self-similar\n(monofractal) plain curve of genus $5\\times 5$ with dilation\n$5\\frac{43}{73}=5.5890\\ldots$, which is best-known among plain monofractals.\nMoreover, the YE-curve is the unique minimal curve among monofractals of genus\n$\\le 6\\times 6$. We give a proof of minimality, which relies both on\ncomputations and theoretical results.\n  We notice that the classes of facet-gated multifractals are rigid enough to\nallow an efficient search, and contain many curves with small dilation. In\ndimension $3$ we have found facet-gated bifractals (that we call \"Spring\") of\ngenus $2\\times2\\times 2$ with dilation $<17$. In dimension $4$ we obtained that\nthere is a curve with dilation $<62$.\n  Some lower bounds on the dilation for wider classes of cubically decomposable\ncurves are proved.}\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 15:23:29 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Malykhin", "Yuri", ""], ["Shchepin", "Evgeny", ""]]}, {"id": "2103.07353", "submitter": "Tao Hou", "authors": "Tamal K. Dey, Tao Hou", "title": "Computing Zigzag Persistence on Graphs in Near-Linear Time", "comments": "The full version of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs model real-world circumstances in many applications where they may\nconstantly change to capture the dynamic behavior of the phenomena. Topological\npersistence which provides a set of birth and death pairs for the topological\nfeatures is one instrument for analyzing such changing graph data. However,\nstandard persistent homology defined over a growing space cannot always capture\nsuch a dynamic process unless shrinking with deletions is also allowed. Hence,\nzigzag persistence which incorporates both insertions and deletions of\nsimplices is more appropriate in such a setting. Unlike standard persistence\nwhich admits nearly linear-time algorithms for graphs, such results for the\nzigzag version improving the general $O(m^\\omega)$ time complexity are not\nknown, where $\\omega< 2.37286$ is the matrix multiplication exponent. In this\npaper, we propose algorithms for zigzag persistence on graphs which run in\nnear-linear time. Specifically, given a filtration with $m$ additions and\ndeletions on a graph with $n$ vertices and edges, the algorithm for\n$0$-dimension runs in $O(m\\log^2 n+m\\log m)$ time and the algorithm for\n1-dimension runs in $O(m\\log^4 n)$ time. The algorithm for $0$-dimension draws\nupon another algorithm designed originally for pairing critical points of Morse\nfunctions on $2$-manifolds. The algorithm for $1$-dimension pairs a negative\nedge with the earliest positive edge so that a $1$-cycle containing both edges\nresides in all intermediate graphs. Both algorithms achieve the claimed time\ncomplexity via dynamic graph data structures proposed by Holm et al. In the\nend, using Alexander duality, we extend the algorithm for $0$-dimension to\ncompute the $(p-1)$-dimensional zigzag persistence for $\\mathbb{R}^p$-embedded\ncomplexes in $O(m\\log^2 n+m\\log m+n\\log n)$ time.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 15:35:33 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Dey", "Tamal K.", ""], ["Hou", "Tao", ""]]}, {"id": "2103.07584", "submitter": "Shizuo Kaji", "authors": "Shizuo Kaji, Jingyao Zhang", "title": "Free-form Design of Discrete Architectural Surfaces by use of Circle\n  Packing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an efficient approach for the conceptual design of\narchitectural surfaces which are composed of triangular panels. Given an\ninitial design, the proposed method finds a triangulated surface with\nuser-specified Gaussian curvatures (not limited to constant Gaussian\ncurvatures) while keeping some of the vertices fixed. In addition, the\nconformal class of the final design can be specified; that is, the user has\ncontrol over the shape (the corner angles) of each triangular panel. The panels\ncould be encouraged to form a regular tessellation or kept close to those of\nthe initial design. This allows the free-form design of discrete architectural\nsurfaces that achive curvature requirements posed by stiffness and\nconstructability. Furthermore, controllability on the conformal class\nsuppresses possible distortion of the panels, resulting in higher structural\nperformance and aesthetics. Our method relies on the idea in discrete\ndifferential geometry called circle packing. In this line of research, the\ndiscrete Ricci flow has been widely used for surface modelling. However, it is\nnot trivial to incorporate constraints such as boundary locations and convexity\nof the spanned surface, which are essential to architectural applications. Due\nto this difficulty, few concrete applications of the discrete Ricci flow have\nbeen reported which specifically aims at creation of architectural surfaces. We\npropose a perturbation of the discrete Ricci energy and develop a\nleast-squares-based optimisation scheme to address these problems with a\nworking implementation available online.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 00:31:50 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Kaji", "Shizuo", ""], ["Zhang", "Jingyao", ""]]}, {"id": "2103.07803", "submitter": "Bartosz Walczak", "authors": "James Davies, Tomasz Krawczyk, Rose McCarty, Bartosz Walczak", "title": "Colouring polygon visibility graphs and their generalizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Curve pseudo-visibility graphs generalize polygon and pseudo-polygon\nvisibility graphs and form a hereditary class of graphs. We prove that every\ncurve pseudo-visibility graph with clique number $\\omega$ has chromatic number\nat most $3\\cdot 4^{\\omega-1}$. The proof is carried through in the setting of\nordered graphs; we identify two conditions satisfied by every curve\npseudo-visibility graph (considered as an ordered graph) and prove that they\nare sufficient for the claimed bound. The proof is algorithmic: both the clique\nnumber and a colouring with the claimed number of colours can be computed in\npolynomial time.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 22:15:57 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Davies", "James", ""], ["Krawczyk", "Tomasz", ""], ["McCarty", "Rose", ""], ["Walczak", "Bartosz", ""]]}, {"id": "2103.07823", "submitter": "Ren\\'e Corbet", "authors": "Ren\\'e Corbet, Michael Kerber, Michael Lesnick, Georg Osang", "title": "Computing the Multicover Bifiltration", "comments": "25 pages, 8 figures, 4 tables. Extended version of a paper accepted\n  to the 2021 Symposium on Computational Geometry", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.AT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a finite set $A\\subset\\mathbb{R}^d$, let Cov$_{r,k}$ denote the set of\nall points within distance $r$ to at least $k$ points of $A$. Allowing $r$ and\n$k$ to vary, we obtain a 2-parameter family of spaces that grow larger when $r$\nincreases or $k$ decreases, called the \\emph{multicover bifiltration}.\nMotivated by the problem of computing the homology of this bifiltration, we\nintroduce two closely related combinatorial bifiltrations, one polyhedral and\nthe other simplicial, which are both topologically equivalent to the multicover\nbifiltration and far smaller than a \\v Cech-based model considered in prior\nwork of Sheehy. Our polyhedral construction is a bifiltration of the rhomboid\ntiling of Edelsbrunner and Osang, and can be efficiently computed using a\nvariant of an algorithm given by these authors. Using an implementation for\ndimension 2 and 3, we provide experimental results. Our simplicial construction\nis useful for understanding the polyhedral construction and proving its\ncorrectness.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 01:19:24 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 22:20:36 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Corbet", "Ren\u00e9", ""], ["Kerber", "Michael", ""], ["Lesnick", "Michael", ""], ["Osang", "Georg", ""]]}, {"id": "2103.07857", "submitter": "Qizheng He", "authors": "Timothy M. Chan, Qizheng He", "title": "More Dynamic Data Structures for Geometric Set Cover with Sublinear\n  Update Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study geometric set cover problems in dynamic settings, allowing\ninsertions and deletions of points and objects. We present the first dynamic\ndata structure that can maintain an $O(1)$-approximation in sublinear update\ntime for set cover for axis-aligned squares in 2D. More precisely, we obtain\nrandomized update time $O(n^{2/3+\\delta})$ for an arbitrarily small constant\n$\\delta>0$. Previously, a dynamic geometric set cover data structure with\nsublinear update time was known only for unit squares by Agarwal, Chang, Suri,\nXiao, and Xue [SoCG 2020]. If only an approximate size of the solution is\nneeded, then we can also obtain sublinear amortized update time for disks in 2D\nand halfspaces in 3D. As a byproduct, our techniques for dynamic set cover also\nyield an optimal randomized $O(n\\log n)$-time algorithm for static set cover\nfor 2D disks and 3D halfspaces, improving our earlier $O(n\\log n(\\log\\log\nn)^{O(1)})$ result [SoCG 2020].\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 06:49:19 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Chan", "Timothy M.", ""], ["He", "Qizheng", ""]]}, {"id": "2103.08043", "submitter": "Timothy M. Chan", "authors": "Timothy M. Chan", "title": "Faster Algorithms for Largest Empty Rectangles and Boxes", "comments": "full version of a SoCG 2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit a classical problem in computational geometry: finding the\nlargest-volume axis-aligned empty box (inside a given bounding box) amidst $n$\ngiven points in $d$ dimensions. Previously, the best algorithms known have\nrunning time $O(n\\log^2n)$ for $d=2$ (by Aggarwal and Suri [SoCG'87]) and near\n$n^d$ for $d\\ge 3$. We describe faster algorithms with running time (i)\n$O(n2^{O(\\log^*n)}\\log n)$ for $d=2$, (ii) $O(n^{2.5+o(1)})$ time for $d=3$,\nand (iii) $\\widetilde{O}(n^{(5d+2)/6})$ time for any constant $d\\ge 4$.\n  To obtain the higher-dimensional result, we adapt and extend previous\ntechniques for Klee's measure problem to optimize certain objective functions\nover the complement of a union of orthants.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 21:27:09 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Chan", "Timothy M.", ""]]}, {"id": "2103.08058", "submitter": "Sharareh Alipour", "authors": "Sharareh Alipour", "title": "On Planar Visibility Counting Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For a set $S$ of $n$ disjoint line segments in $\\mathbb{R}^{2}$, the\nvisibility counting problem is to preprocess $S$ such that the number of\nvisible segments in $S$ from any query point $p$ can be computed quickly. There\nhave been approximation algorithms for this problem with trade off between\nspace and query time. We propose a new randomized algorithm to compute the\nexact answer of the problem. For any $0<\\alpha<1$, the space, preprocessing\ntime and query time are $O_{\\epsilon}(n^{4-4\\alpha})$,\n$O_{\\epsilon}(n^{4-2\\alpha})$ and $O_{\\epsilon}(n^{2\\alpha})$, respectively.\nWhere $O_{\\epsilon}(f(n)) = O(f(n)n^{\\epsilon})$ and $\\epsilon>0$ is an\narbitrary constant number.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 22:50:43 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Alipour", "Sharareh", ""]]}, {"id": "2103.08354", "submitter": "Henk Alkema", "authors": "Henk Alkema, Mark de Berg", "title": "Rectilinear Steiner Trees in Narrow Strips", "comments": "21 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A rectilinear Steiner tree for a set $P$ of points in $\\mathbb{R}^2$ is a\ntree that connects the points in $P$ using horizontal and vertical line\nsegments. The goal of Minimal Rectilinear Steiner Tree is to find a rectilinear\nSteiner tree with minimal total length. We investigate how the complexity of\nMinimal Rectilinear Steiner Tree for point sets $P$ inside the strip\n$(-\\infty,+\\infty)\\times [0,\\delta]$ depends on the strip width $\\delta$. We\nobtain two main results. 1) We present an algorithm with running time\n$n^{O(\\sqrt{\\delta})}$ for sparse point sets, that is, point sets where each\n$1\\times\\delta$ rectangle inside the strip contains $O(1)$ points. 2) For\nrandom point sets, where the points are chosen randomly inside a rectangle of\nheight $\\delta$ and expected width $n$, we present an algorithm that is\nfixed-parameter tractable with respect to $\\delta$ and linear in $n$. It has an\nexpected running time of $2^{O(\\delta \\sqrt{\\delta})} n$.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 12:50:35 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Alkema", "Henk", ""], ["de Berg", "Mark", ""]]}, {"id": "2103.08416", "submitter": "Soeren Nickel", "authors": "Sujoy Bhore, Soeren Nickel, Martin N\\\"ollenburg", "title": "Recognition of Unit Disk Graphs for Caterpillars, Embedded Trees, and\n  Outerplanar Graphs", "comments": "17 Pages, 12 Figures, preliminary version of this paper accepted at\n  EuroCG21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unit disk graphs are graphs that have a unit disk intersection representation\n(UDR). In the recognition problem the objective is to decide whether a given\ngraph is a unit disk graph, which is known to be NP-hard, even for planar\ngraphs. In this work, we show that the recognition of unit disk graphs remains\nNP-hard for outerplanar graphs and for embedded trees. We also show that given\na caterpillar graph, we can decide in linear time whether it is a unit disk\ngraph.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 14:40:20 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Bhore", "Sujoy", ""], ["Nickel", "Soeren", ""], ["N\u00f6llenburg", "Martin", ""]]}, {"id": "2103.08421", "submitter": "Patrick Schnider", "authors": "Patrick Schnider", "title": "Enclosing Depth and other Depth Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": "CPH-GEOTOP-DNRF151", "categories": "math.CO cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study families of depth measures defined by natural sets of axioms. We\nshow that any such depth measure is a constant factor approximation of Tukey\ndepth. We further investigate the dimensions of depth regions, showing that the\nCascade conjecture, introduced by Kalai for Tverberg depth, holds for all depth\nmeasures which satisfy our most restrictive set of axioms, which includes Tukey\ndepth. Along the way, we introduce and study a new depth measure called\nenclosing depth, which we believe to be of independent interest, and show its\nrelation to a constant-fraction Radon theorem on certain two-colored point\nsets.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 14:46:14 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Schnider", "Patrick", ""]]}, {"id": "2103.08432", "submitter": "Goran Malic", "authors": "Goran Mali\\'c and Ileana Streinu", "title": "Combinatorial Resultants in the Algebraic Rigidity Matroid", "comments": "32 pages, 9 figures. This is the full paper accompanying the extended\n  abstract of the same title (to appear in SoCG 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CG math.AG math.MG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by a rigidity-theoretic perspective on the Localization Problem in\n2D, we develop an algorithm for computing circuit polynomials in the algebraic\nrigidity matroid associated to the Cayley-Menger ideal for $n$ points in 2D. We\nintroduce combinatorial resultants, a new operation on graphs that captures\nproperties of the Sylvester resultant of two polynomials in the algebraic\nrigidity matroid. We show that every rigidity circuit has a construction tree\nfrom $K_4$ graphs based on this operation. Our algorithm performs an algebraic\nelimination guided by the construction tree, and uses classical resultants,\nfactorization and ideal membership. To demonstrate its effectiveness, we\nimplemented our algorithm in Mathematica: it took less than 15 seconds on an\nexample where a Groebner Basis calculation took 5 days and 6 hrs.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 15:00:02 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Mali\u0107", "Goran", ""], ["Streinu", "Ileana", ""]]}, {"id": "2103.08665", "submitter": "Patrick Schnider", "authors": "Daniel Bertschinger, Jonas Passweg, Patrick Schnider", "title": "Tukey Depth Histograms", "comments": null, "journal-ref": null, "doi": null, "report-no": "CPH-GEOTOP-DNRF151", "categories": "cs.CG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Tukey depth of a flat with respect to a point set is a concept that\nappears in many areas of discrete and computational geometry. In particular,\nthe study of centerpoints, center transversals, Ham Sandwich cuts, or $k$-edges\ncan all be phrased in terms of depths of certain flats with respect to one or\nmore point sets. In this work, we introduce the Tukey depth histogram of\n$k$-flats in $\\mathbb{R}^d$ with respect to a point set $P$, which is a vector\n$D^{k,d}(P)$, whose $i$'th entry $D^{k,d}_i(P)$ denotes the number of $k$-flats\nspanned by $k+1$ points of $P$ that have Tukey depth $i$ with respect to $P$.\nAs our main result, we give a complete characterization of the depth histograms\nof points, that is, for any dimension $d$ we give a description of all possible\nhistograms $D^{0,d}(P)$. This then allows us to compute the exact number of\npossible such histograms.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 19:16:54 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Bertschinger", "Daniel", ""], ["Passweg", "Jonas", ""], ["Schnider", "Patrick", ""]]}, {"id": "2103.08719", "submitter": "Andrew Nathenson", "authors": "Andrew Nathenson", "title": "Axis-Aligned Square Contact Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new class $\\mathcal{G}$ of bipartite plane graphs and prove\nthat each graph in $\\mathcal{G}$ admits a proper square contact representation.\nA contact between two squares is \\emph{proper} if they intersect in a line\nsegment of positive length. The class $\\mathcal{G}$ is the family of\nquadrangulations obtained from the 4-cycle $C_4$ by successively inserting a\nsingle vertex or a 4-cycle of vertices into a face.\n  For every graph $G\\in \\mathcal{G}$, we construct a proper square contact\nrepresentation. The key parameter of the recursive construction is the aspect\nratio of the rectangle bounded by the four outer squares. We show that this\naspect ratio may continuously vary in an interval $I_G$. The interval $I_G$\ncannot be replaced by a fixed aspect ratio, however, as we show, the feasible\ninterval $I_G$ may be an arbitrarily small neighborhood of any positive real.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 21:04:28 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 19:56:25 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Nathenson", "Andrew", ""]]}, {"id": "2103.08739", "submitter": "Sahar Chehrazad", "authors": "Sahar Chehrazad, Dirk Roose and Tony Wauters", "title": "A fast and scalable algorithm to solve nesting problems using a\n  semi-discrete representation", "comments": "24 pages, 23 figures, submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast algorithm to solve nesting problems based on a\nsemi-discrete representation of both the 2D non-convex pieces and the strip.\nThe pieces and the strip are represented by a set of equidistant vertical line\nsegments. The discretization algorithm uses a sweep-line method and applies\nminimal extensions to the line segments of a piece to ensure that\nnon-overlapping placement of the segments, representing two pieces, cannot\ncause overlap of the original pieces. We implemented a bottom-left-fill greedy\nplacement procedure, using an optimized ordering of the segment overlap tests.\nThe C++ implementation of our algorithm uses appropriate data structures that\nallow fast execution. It executes the bottom-left-fill algorithm for typical\nESICUPdata sets in a few milliseconds, even when the rotation of the pieces is\nconsidered, and thus provides a suitable basis for integration in\nmetaheuristics. Moreover, we show that the algorithm scales well when the\nnumber of pieces increases.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 22:00:27 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Chehrazad", "Sahar", ""], ["Roose", "Dirk", ""], ["Wauters", "Tony", ""]]}, {"id": "2103.08995", "submitter": "Leonie Selbach", "authors": "Maike Buchin and Leonie Selbach", "title": "Decomposing Polygons into Fat Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of decomposing (i.e. partitioning and covering) polygons\ninto components that are $\\alpha$-fat, which means that the aspect ratio of\neach subpolygon is at most $\\alpha$. We consider decompositions without Steiner\npoints. We present a polynomial-time algorithm for simple polygons that finds\nthe minimum $\\alpha$ such that an $\\alpha$-fat partition exists. Furthermore,\nwe show that finding an $\\alpha$-fat partition or covering with minimum\ncardinality is NP-hard for polygons with holes.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 11:45:20 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Buchin", "Maike", ""], ["Selbach", "Leonie", ""]]}, {"id": "2103.09223", "submitter": "Aleksandr Popov", "authors": "Kevin Buchin, Maarten L\\\"offler, Aleksandr Popov, Marcel Roeloffzen", "title": "Uncertain Curve Simplification", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of polygonal curve simplification under uncertainty,\nwhere instead of a sequence of exact points, each uncertain point is\nrepresented by a region, which contains the (unknown) true location of the\nvertex. The regions we consider are disks, line segments, convex polygons, and\ndiscrete sets of points. We are interested in finding the shortest subsequence\nof uncertain points such that no matter what the true location of each\nuncertain point is, the resulting polygonal curve is a valid simplification of\nthe original polygonal curve under the Hausdorff or the Fr\\'echet distance. For\nboth these distance measures, we present polynomial-time algorithms for this\nproblem.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 17:51:09 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Buchin", "Kevin", ""], ["L\u00f6ffler", "Maarten", ""], ["Popov", "Aleksandr", ""], ["Roeloffzen", "Marcel", ""]]}, {"id": "2103.09286", "submitter": "Xavier Goaoc", "authors": "Xavier Goaoc, Andreas F. Holmsen and Zuzana Pat\\'akov\\'a", "title": "A Stepping-Up Lemma for Topological Set Systems", "comments": "37th International Symposium on Computational Geometry (SoCG'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intersection patterns of convex sets in $\\mathbb{R}^d$ have the remarkable\nproperty that for $d+1 \\le k \\le \\ell$, in any sufficiently large family of\nconvex sets in $\\mathbb{R}^d$, if a constant fraction of the $k$-element\nsubfamilies have nonempty intersection, then a constant fraction of the\n$\\ell$-element subfamilies must also have nonempty intersection. Here, we prove\nthat a similar phenomenon holds for any topological set system $\\mathcal{F}$ in\n$\\mathbb{R}^d$. Quantitatively, our bounds depend on how complicated the\nintersection of $\\ell$ elements of $\\mathcal{F}$ can be, as measured by the sum\nof the $\\lceil\\frac{d}2\\rceil$ first Betti numbers. As an application, we\nimprove the fractional Helly number of set systems with bounded topological\ncomplexity due to the third author, from a Ramsey number down to $d+1$. We also\nshed some light on a conjecture of Kalai and Meshulam on intersection patterns\nof sets with bounded homological VC dimension. A key ingredient in our proof is\nthe use of the stair convexity of Bukh, Matou\\v{s}ek and Nivash to recast a\nsimplicial complex as a homological minor of a cubical complex.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 19:05:54 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 08:41:56 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Goaoc", "Xavier", ""], ["Holmsen", "Andreas F.", ""], ["Pat\u00e1kov\u00e1", "Zuzana", ""]]}, {"id": "2103.09308", "submitter": "Sariel Har-Peled", "authors": "Stav Ashur and Sariel Har-Peled", "title": "On Undecided LP, Clustering and Active Learning", "comments": "To appear in SoCG 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study colored coverage and clustering problems. Here, we are given a\ncolored point set where the points are covered by (unknown) $k$ clusters, which\nare monochromatic (i.e., all the points covered by the same cluster, have the\nsame color). The access to the colors of the points (or even the points\nthemselves) is provided indirectly via various queries (such as nearest\nneighbor, or separation queries). We show that if the number of clusters is a\nconstant, then one can correctly deduce the color of all the points (i.e.,\ncompute a monochromatic clustering of the points) using a polylogarithmic\nnumber of queries.\n  We investigate several variants of this problem, including Undecided Linear\nProgramming, covering of points by $k$ monochromatic balls, covering by $k$\ntriangles/simplices, and terrain simplification. For the later problem, we\npresent the first near linear time approximation algorithm. While our\napproximation is slightly worse than previous work, this is the first algorithm\nto have subquadratic complexity if the terrain has \"small\" complexity.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 20:33:50 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 16:28:28 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Ashur", "Stav", ""], ["Har-Peled", "Sariel", ""]]}, {"id": "2103.09684", "submitter": "Adam Karczmarz", "authors": "Adam Karczmarz, Jakub Pawlewicz, Piotr Sankowski", "title": "Sublinear Average-Case Shortest Paths in Weighted Unit-Disk Graphs", "comments": "Full version of a SoCG'21 paper. Abstract truncated to meet arxiv\n  requirements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of computing shortest paths in weighted unit-disk\ngraphs in constant dimension $d$. Although the single-source and all-pairs\nvariants of this problem are well-studied in the plane case, no non-trivial\nexact distance oracles for unit-disk graphs have been known to date, even for\n$d=2$.\n  The classical result of Sedgewick and Vitter [Algorithmica '86] shows that\nfor weighted unit-disk graphs in the plane the $A^*$ search has average-case\nperformance superior to that of a standard shortest path algorithm, e.g.,\nDijkstra's algorithm. Specifically, if the $n$ corresponding points of a\nweighted unit-disk graph $G$ are picked from a unit square uniformly at random,\nand the connectivity radius is $r\\in (0,1)$, $A^*$ finds a shortest path in $G$\nin $O(n)$ expected time when $r=\\Omega(\\sqrt{\\log n/n})$, even though $G$ has\n$\\Theta((nr)^2)$ edges in expectation. In other words, the work done by the\nalgorithm is in expectation proportional to the number of vertices and not the\nnumber of edges.\n  In this paper, we break this natural barrier and show even stronger sublinear\ntime results. We propose a new heuristic approach to computing point-to-point\nexact shortest paths in unit-disk graphs. We analyze the average-case behavior\nof our heuristic using the same random graph model as used by Sedgewick and\nVitter and prove it superior to $A^*$. Specifically, we show that, if we are\nable to report the set of all $k$ points of $G$ from an arbitrary rectangular\nregion of the plane in $O(k + t(n))$ time, then a shortest path between\narbitrary two points of such a random graph on the plane can be found in\n$O(1/r^2 + t(n))$ expected time. In particular, the state-of-the-art range\nreporting data structures imply a sublinear expected bound for all\n$r=\\Omega(\\sqrt{\\log n/n})$ and $O(\\sqrt{n})$ expected bound for\n$r=\\Omega(n^{-1/4})$ after only near-linear preprocessing of the point set.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 14:31:56 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Karczmarz", "Adam", ""], ["Pawlewicz", "Jakub", ""], ["Sankowski", "Piotr", ""]]}, {"id": "2103.09735", "submitter": "Arnab Maiti", "authors": "Arindam Khan, Arnab Maiti, Amatya Sharma, Andreas Wiese", "title": "On Guillotine Separable Packings for the Two-dimensional Geometric\n  Knapsack Problem", "comments": "39 pages, 13 figures, To appear in SOCG 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In two-dimensional geometric knapsack problem, we are given a set of n\naxis-aligned rectangular items and an axis-aligned square-shaped knapsack. Each\nitem has integral width, integral height and an associated integral profit. The\ngoal is to find a (non-overlapping axis-aligned) packing of a maximum profit\nsubset of rectangles into the knapsack. A well-studied and frequently used\nconstraint in practice is to allow only packings that are guillotine separable,\ni.e., every rectangle in the packing can be obtained by recursively applying a\nsequence of edge-to-edge axis-parallel cuts that do not intersect any item of\nthe solution. In this paper we study approximation algorithms for the geometric\nknapsack problem under guillotine cut constraints. We present polynomial time\n(1 + {\\epsilon})-approximation algorithms for the cases with and without\nallowing rotations by 90 degrees, assuming that all input numeric data are\npolynomially bounded in n. In comparison, the best-known approximation factor\nfor this setting is 3 + {\\epsilon} [Jansen-Zhang, SODA 2004], even in the\ncardinality case where all items have the same profit. Our main technical\ncontribution is a structural lemma which shows that any guillotine packing can\nbe converted into another structured guillotine packing with almost the same\nprofit. In this packing, each item is completely contained in one of a constant\nnumber of boxes and L-shaped regions, inside which the items are placed by a\nsimple greedy routine. In particular, we provide a clean sufficient condition\nwhen such a packing obeys the guillotine cut constraints which might be useful\nfor other settings where these constraints are imposed.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 15:47:42 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Khan", "Arindam", ""], ["Maiti", "Arnab", ""], ["Sharma", "Amatya", ""], ["Wiese", "Andreas", ""]]}, {"id": "2103.09803", "submitter": "Alexander Wolff", "authors": "Elena Arseneva, Linda Kleist, Boris Klemz, Maarten L\\\"offler, Andr\\'e\n  Schulz, Birgit Vogtenhuber, Alexander Wolff", "title": "Adjacency Graphs of Polyhedral Surfaces", "comments": "To appear in Proc. SoCG 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study whether a given graph can be realized as an adjacency graph of the\npolygonal cells of a polyhedral surface in $\\mathbb{R}^3$. We show that every\ngraph is realizable as a polyhedral surface with arbitrary polygonal cells, and\nthat this is not true if we require the cells to be convex. In particular, if\nthe given graph contains $K_5$, $K_{5,81}$, or any nonplanar $3$-tree as a\nsubgraph, no such realization exists. On the other hand, all planar graphs,\n$K_{4,4}$, and $K_{3,5}$ can be realized with convex cells. The same holds for\nany subdivision of any graph where each edge is subdivided at least once, and,\nby a result from McMullen et al. (1983), for any hypercube.\n  Our results have implications on the maximum density of graphs describing\npolyhedral surfaces with convex cells: The realizability of hypercubes shows\nthat the maximum number of edges over all realizable $n$-vertex graphs is in\n$\\Omega(n \\log n)$. From the non-realizability of $K_{5,81}$, we obtain that\nany realizable $n$-vertex graph has $O(n^{9/5})$ edges. As such, these graphs\ncan be considerably denser than planar graphs, but not arbitrarily dense.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 17:41:13 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Arseneva", "Elena", ""], ["Kleist", "Linda", ""], ["Klemz", "Boris", ""], ["L\u00f6ffler", "Maarten", ""], ["Schulz", "Andr\u00e9", ""], ["Vogtenhuber", "Birgit", ""], ["Wolff", "Alexander", ""]]}, {"id": "2103.09811", "submitter": "Jeff Erickson", "authors": "Mikkel Abrahamsen, Jeff Erickson, Irina Kostitsyna, Maarten L\\\"offler,\n  Tillmann Miltzow, J\\'er\\^ome Urhausen, Jordi Vermeulen, and Giovanni\n  Viglietta", "title": "Chasing Puppies: Mobile Beacon Routing on Closed Curves", "comments": "Full version of a SOCG 2021 paper, 28 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve an open problem posed by Michael Biro at CCCG 2013 that was inspired\nby his and others' work on beacon-based routing. Consider a human and a puppy\non a simple closed curve in the plane. The human can walk along the curve at\nbounded speed and change direction as desired. The puppy runs with unbounded\nspeed along the curve as long as the Euclidean straight-line distance to the\nhuman is decreasing, so that it is always at a point on the curve where the\ndistance is locally minimal. Assuming that the curve is smooth (with some mild\ngenericity constraints) or a simple polygon, we prove that the human can always\ncatch the puppy in finite time.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 17:54:20 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Abrahamsen", "Mikkel", ""], ["Erickson", "Jeff", ""], ["Kostitsyna", "Irina", ""], ["L\u00f6ffler", "Maarten", ""], ["Miltzow", "Tillmann", ""], ["Urhausen", "J\u00e9r\u00f4me", ""], ["Vermeulen", "Jordi", ""], ["Viglietta", "Giovanni", ""]]}, {"id": "2103.09992", "submitter": "Xiaoling Hu Mr", "authors": "Xiaoling Hu, Yusu Wang, Li Fuxin, Dimitris Samaras, Chao Chen", "title": "Topology-Aware Segmentation Using Discrete Morse Theory", "comments": "19 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the segmentation of fine-scale structures from natural and biomedical\nimages, per-pixel accuracy is not the only metric of concern. Topological\ncorrectness, such as vessel connectivity and membrane closure, is crucial for\ndownstream analysis tasks. In this paper, we propose a new approach to train\ndeep image segmentation networks for better topological accuracy. In\nparticular, leveraging the power of discrete Morse theory (DMT), we identify\nglobal structures, including 1D skeletons and 2D patches, which are important\nfor topological accuracy. Trained with a novel loss based on these global\nstructures, the network performance is significantly improved especially near\ntopologically challenging locations (such as weak spots of connections and\nmembranes). On diverse datasets, our method achieves superior performance on\nboth the DICE score and topological metrics.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 02:47:21 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Hu", "Xiaoling", ""], ["Wang", "Yusu", ""], ["Fuxin", "Li", ""], ["Samaras", "Dimitris", ""], ["Chen", "Chao", ""]]}, {"id": "2103.10406", "submitter": "Diego Ram\\'irez-Romero", "authors": "Waldo G\\'alvez, Fabrizio Grandoni, Arindam Khan, Diego\n  Ram\\'irez-Romero, Andreas Wiese", "title": "Improved Approximation Algorithms for 2-Dimensional Knapsack: Packing\n  into Multiple L-Shapes, Spirals, and More", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the \\textsc{2-Dimensional Knapsack} problem (2DK) we are given a square\nknapsack and a collection of $n$ rectangular items with integer sizes and\nprofits. Our goal is to find the most profitable subset of items that can be\npacked non-overlappingly into the knapsack. The currently best known\npolynomial-time approximation factor for 2DK is $17/9+\\varepsilon<1.89$ and\nthere is a $(3/2+\\varepsilon)$-approximation algorithm if we are allowed to\nrotate items by 90 degrees~{[}G\\'alvez et al., FOCS 2017{]}. In this paper, we\ngive $(4/3+\\varepsilon)$-approximation algorithms in polynomial time for both\ncases, assuming that all input data are {integers polynomially bounded in $n$}.\n  G\\'alvez et al.'s algorithm for 2DK partitions the knapsack into a constant\nnumber of rectangular regions plus \\emph{one} L-shaped region and packs items\ninto those {in a structured way}. We generalize this approach by allowing up to\na \\emph{constant} number of {\\emph{more general}} regions that can have the\nshape of an L, a U, a Z, a spiral, and more, and therefore obtain an improved\napproximation ratio. {In particular, we present an algorithm that computes the\nessentially optimal structured packing into these regions. }\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:42:20 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["G\u00e1lvez", "Waldo", ""], ["Grandoni", "Fabrizio", ""], ["Khan", "Arindam", ""], ["Ram\u00edrez-Romero", "Diego", ""], ["Wiese", "Andreas", ""]]}, {"id": "2103.10830", "submitter": "Herbert Edelsbrunner", "authors": "Herbert Edelsbrunner and Katharina \\\"Olsb\\\"ock", "title": "Tri-Partitions and Bases of an Ordered Complex", "comments": "15 pages, 3 figures", "journal-ref": "Discrete and Computational Geometry (DCG) 64 (2020), 759-775", "doi": "10.1007/s00454-020-00188-x", "report-no": null, "categories": "math.CO cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalizing the decomposition of a connected planar graph into a tree and a\ndual tree, we prove a combinatorial analog of the classic Helmholz-Hodge\ndecomposition of a smooth vector field. Specifically, we show that for every\npolyhedral complex, $K$, and every dimension, $p$, there is a partition of the\nset of $p$-cells into a maximal $p$-tree, a maximal $p$-cotree, and a\ncollection of $p$-cells whose cardinality is the $p$-th Betti number of $K$.\nGiven an ordering of the $p$-cells, this tri-partition is unique, and it can be\ncomputed by a matrix reduction algorithm that also constructs canonical bases\nof cycle and boundary groups.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 14:24:26 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Edelsbrunner", "Herbert", ""], ["\u00d6lsb\u00f6ck", "Katharina", ""]]}, {"id": "2103.11107", "submitter": "Rameshwar Pratap", "authors": "Amit Deshpande and Rameshwar Pratap", "title": "On Subspace Approximation and Subset Selection in Fewer Passes by MCMC\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.LG math.SP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of subset selection for $\\ell_{p}$ subspace\napproximation, i.e., given $n$ points in $d$ dimensions, we need to pick a\nsmall, representative subset of the given points such that its span gives\n$(1+\\epsilon)$ approximation to the best $k$-dimensional subspace that\nminimizes the sum of $p$-th powers of distances of all the points to this\nsubspace. Sampling-based subset selection techniques require adaptive sampling\niterations with multiple passes over the data. Matrix sketching techniques give\na single-pass $(1+\\epsilon)$ approximation for $\\ell_{p}$ subspace\napproximation but require additional passes for subset selection.\n  In this work, we propose an MCMC algorithm to reduce the number of passes\nrequired by previous subset selection algorithms based on adaptive sampling.\nFor $p=2$, our algorithm gives subset selection of nearly optimal size in only\n$2$ passes, whereas the number of passes required in previous work depend on\n$k$. Our algorithm picks a subset of size $\\mathrm{poly}(k/\\epsilon)$ that\ngives $(1+\\epsilon)$ approximation to the optimal subspace. The running time of\nthe algorithm is $nd + d~\\mathrm{poly}(k/\\epsilon)$. We extend our results to\nthe case when outliers are present in the datasets, and suggest a two pass\nalgorithm for the same. Our ideas also extend to give a reduction in the number\nof passes required by adaptive sampling algorithms for $\\ell_{p}$ subspace\napproximation and subset selection, for $p \\geq 2$.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 06:07:30 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Deshpande", "Amit", ""], ["Pratap", "Rameshwar", ""]]}, {"id": "2103.11142", "submitter": "Bal\\'azs Keszegh", "authors": "Bal\\'azs Keszegh", "title": "Discrete Helly-type theorems for pseudohalfplanes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove discrete Helly-type theorems for pseudohalfplanes, which extend\nrecent results of Jensen, Joshi and Ray about halfplanes. Among others we show\nthat given a family of pseudohalfplanes $\\cal H$ and a set of points $P$, if\nevery triple of pseudohalfplanes has a common point in $P$ then there exists a\nset of at most two points that hits every pseudohalfplane of $\\cal H$. We also\nprove that if every triple of points of $P$ is contained in a pseudohalfplane\nof $\\cal H$ then there are two pseudohalfplanes of $\\cal H$ that cover all\npoints of $P$.\n  To prove our results we regard pseudohalfplane hypergraphs, define their\nextremal vertices and show that these behave in many ways as points on the\nboundary of the convex hull of a set of points. Our methods are purely\ncombinatorial.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 09:41:35 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Keszegh", "Bal\u00e1zs", ""]]}, {"id": "2103.11216", "submitter": "Steven Damelin Dr", "authors": "Gurpreet S. Kalsi and Steven B. Damelin", "title": "Preprocessing power weighted shortest path data using a s-Well Separated\n  Pair Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CG cs.DS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For $s$ $>$ 0, we consider an algorithm that computes all $s$-well separated\npairs in certain point sets in $\\mathbb{R}^{n}$, $n$ $>1$. For an integer $K$\n$>1$, we also consider an algorithm that is a permutation of Dijkstra's\nalgorithm, that computes $K$-nearest neighbors using a certain power weighted\nshortest path metric in $\\mathbb{R}^{n}$, $n$ $>$ $1$. We describe each\nalgorithm and their respective dependencies on the input data. We introduce a\nway to combine both algorithms into a fused algorithm. Several open problems\nare given for future research.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 17:38:13 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 20:27:09 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Kalsi", "Gurpreet S.", ""], ["Damelin", "Steven B.", ""]]}, {"id": "2103.11260", "submitter": "Dan Reznik", "authors": "Pedro Roitman, Ronaldo Garcia, Dan Reznik", "title": "New Invariants of Poncelet-Jacobi Bicentric Polygons", "comments": "17 pages, 6 figures, 1 table with 18 video links", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.CG math-ph math.MG math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 1d family of Poncelet polygons interscribed between two circles is known\nas the Bicentric family. Using elliptic functions and Liouville's theorem, we\nshow (i) that this family has invariant sum of internal angle cosines and (ii)\nthat the pedal polygons with respect to the family's limiting points have\ninvariant perimeter. Interestingly, both (i) and (ii) are also properties of\nelliptic billiard N-periodics. Furthermore, since the pedal polygons in (ii)\nare identical to inversions of elliptic billiard N-periodics with respect to a\nfocus-centered circle, an important corollary is that (iii) elliptic billiard\nfocus-inversive N-gons have constant perimeter. Interestingly, these also\nconserve their sum of cosines (except for the N=4 case).\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 22:58:00 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 12:03:30 GMT"}, {"version": "v3", "created": "Tue, 13 Jul 2021 17:07:08 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Roitman", "Pedro", ""], ["Garcia", "Ronaldo", ""], ["Reznik", "Dan", ""]]}, {"id": "2103.11310", "submitter": "Bolun Wang", "authors": "Bolun Wang, Xin Jiang, Guanying Huo, Danlei Ye, Cheng Su, Zehong Lu,\n  Dongming Yan, Zhiming Zheng", "title": "KPI Method for Dynamic Surface Reconstruction With B-splines based on\n  Sparse Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional B-splines are catching tremendous attentions in fields of\nIso-geometry Analysis, dynamic surface reconstruction and so on. However, the\nactual measured data are usually sparse and nonuniform, which might not meet\nthe requirement of traditional B-spline algorithms. In this paper, we present a\nnovel dynamic surface reconstruction approach, which is a 3-dimensional key\npoints interpolation method (KPI) based on B-spline, aimed at dealing with\nsparse distributed data. This method includes two stages: a data set generation\nalgorithm based on Kriging and a control point solving method based on key\npoints interpolation. The data set generation method is designed to construct a\ngrided dataset which can meet the requirement of B-spline interpolation, while\npromisingly catching the trend of sparse data, and it also includes a parameter\nreduction method which can significantly reduce the number of control points of\nthe result surface. The control points solving method ensures the 3-dimensional\nB-spline function to interpolate the sparse data points precisely while\napproximating the data points generated by Kriging. We apply the method into a\ntemperature data interpolation problem. It is shown that the generated dynamic\nsurface accurately interpolates the sparsely distributed temperature data,\npreserves the dynamic characteristics, with fewer control points than those of\ntraditional B-spline surface interpolation algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 05:31:24 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Wang", "Bolun", ""], ["Jiang", "Xin", ""], ["Huo", "Guanying", ""], ["Ye", "Danlei", ""], ["Su", "Cheng", ""], ["Lu", "Zehong", ""], ["Yan", "Dongming", ""], ["Zheng", "Zhiming", ""]]}, {"id": "2103.11676", "submitter": "Rodrigo Silveira", "authors": "Delia Garijo, Alberto M\\'arquez, Rodrigo I. Silveira", "title": "Continuous mean distance of a weighted graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the concept of the continuous mean distance of a weighted graph. For\nconnected unweighted graphs, the mean distance can be defined as the arithmetic\nmean of the distances between all pairs of vertices. This parameter provides a\nnatural measure of the compactness of the graph, and has been intensively\nstudied, together with several variants, including its version for weighted\ngraphs. The continuous analog of the (discrete) mean distance is the mean of\nthe distances between all pairs of points on the edges of the graph. Despite\nbeing a very natural generalization, to the best of our knowledge this concept\nhas been barely studied, since the jump from discrete to continuous implies\nhaving to deal with an infinite number of distances, something that increases\nthe difficulty of the parameter. In this paper we show that the continuous mean\ndistance of a weighted graph can be computed in time quadratic in the number of\nedges, by two different methods that apply fundamental concepts in discrete\nalgorithms and computational geometry. We also present structural results that\nallow a faster computation of this continuous parameter for several classes of\nweighted graphs. Finally, we study the relation between the (discrete) mean\ndistance and its continuous counterpart, mainly focusing on the relevant\nquestion of the convergence when iteratively subdividing the edges of the\nweighted graph.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 09:15:09 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Garijo", "Delia", ""], ["M\u00e1rquez", "Alberto", ""], ["Silveira", "Rodrigo I.", ""]]}, {"id": "2103.11688", "submitter": "Jingjing Liu", "authors": "Jingjing Liu, Fang Deng, Jiansong Deng", "title": "Space Mapping of Spline Spaces over Hierarchical T-meshes", "comments": "31 pages,20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we construct a bijective mapping between a biquadratic spline\nspace over the hierarchical T-mesh and the piecewise constant space over the\ncorresponding crossing-vertex-relationship graph (CVR graph). We propose a\nnovel structure, by which we offer an effective and easy operative method for\nconstructing the basis functions of the biquadratic spline space. The mapping\nwe construct is an isomorphism. The basis functions of the biquadratic spline\nspace hold the properties such as linearly independent, completeness and the\nproperty of partition of unity, which are the same with the properties for the\nbasis functions of piecewise constant space over the CVR graph. To demonstrate\nthat the new basis functions are efficient, we apply the basis functions to fit\nsome open surfaces.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 09:41:42 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Liu", "Jingjing", ""], ["Deng", "Fang", ""], ["Deng", "Jiansong", ""]]}, {"id": "2103.12769", "submitter": "S\\'andor Boz\\'oki", "authors": "S\\'andor Boz\\'oki and G\\'abor Domokos and Fl\\'ori\\'an Kov\\'acs and\n  Krisztina Reg\\H{o}s", "title": "Mono-monostatic polyhedra with uniform point masses have at least 8\n  vertices", "comments": "45 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The monostatic property of convex polyhedra (i.e. the property of having just\none stable or unstable static equilibrium point) has been in the focus of\nresearch ever since Conway and Guy published the proof of the existence of the\nfirst such object, followed by the constructions of Bezdek and Reshetov. These\nexamples establish $F\\leq 14, V\\leq 18$ as the respective \\emph{upper bounds}\nfor the minimal number of faces and vertices for a homogeneous mono-stable\npolyhedron. By proving that no mono-stable homogeneous tetrahedron existed,\nConway and Guy established for the same problem the lower bounds for the number\nof faces and vertices as $F, V \\geq 5$ and the same lower bounds were also\nestablished for the mono-unstable case. It is also clear that the $F,V \\geq 5$\nbounds also apply for convex, homogeneous point sets with unit masses at each\npoint (also called polyhedral 0-skeletons) and they are also valid for\nmono-monostatic polyhedra with exactly on stable and one unstable equilibrium\npoint (both homogeneous and 0-skeletons). Here we present an algorithm by which\nwe improve the lower bound to $V\\geq 8$ vertices (implying $f \\geq 6$ faces) on\nmono-unstable and mono-monostable 0-skeletons. Our algorithm appears to be less\nwell suited to compute the lower bounds for mono-stability. We point out these\ndifficulties in connection with the work of Dawson and Finbow who explored the\nmonostatic property of simplices in higher dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 18:11:25 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Boz\u00f3ki", "S\u00e1ndor", ""], ["Domokos", "G\u00e1bor", ""], ["Kov\u00e1cs", "Fl\u00f3ri\u00e1n", ""], ["Reg\u0151s", "Krisztina", ""]]}, {"id": "2103.13013", "submitter": "Chuan-Shen Hu", "authors": "Yu-Min Chung, Sarah Day, Chuan-Shen Hu", "title": "A Multi-parameter Persistence Framework for Mathematical Morphology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV math.AT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The field of mathematical morphology offers well-studied techniques for image\nprocessing. In this work, we view morphological operations through the lens of\npersistent homology, a tool at the heart of the field of topological data\nanalysis. We demonstrate that morphological operations naturally form a\nmultiparameter filtration and that persistent homology can then be used to\nextract information about both topology and geometry in the images as well as\nto automate methods for optimizing the study and rendering of structure in\nimages. For illustration, we apply this framework to analyze noisy binary,\ngrayscale, and color images.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 06:46:00 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Chung", "Yu-Min", ""], ["Day", "Sarah", ""], ["Hu", "Chuan-Shen", ""]]}, {"id": "2103.13179", "submitter": "Peyman Lahe", "authors": "Peyman Lahe Motlagh, Ipek Basdogana", "title": "Multimode piezoelectric shunt damping of thin plates with arrays of\n  separately shunted patches, method, and experimental validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Two-dimensional thin plates are widely used in many applications. Shunt\ndamping is a promising way for the attenuation of vibration of these\nelectromechanical systems. It enables a compact vibration damping method\nwithout adding significant mass and volumetric occupancy. Analyzing the\ndynamics of such electromechanical systems requires precise modeling tools that\nproperly consider the coupling between the piezoelectric elements and the host\nstructure. Although the concept of shunt damping has been studied extensively\nin the literature, most of the studies do not provide a formulation for\nmodeling the multiple piezoelectric patches that are scattered on the host\nstructure and shunted separately. This paper presents a methodology and a\nformulation for separately shunted piezoelectric patches for achieving higher\nperformance on vibration attenuation. The Rayleigh-Ritz method is used for\nperforming modal analysis and obtaining the frequency response functions of the\nelectro-mechanical system. The developed model includes mass and stiffness\ncontribution of the piezoelectric patches as well as the electromechanical\ncoupling effect. In this study, the piezoelectric patches are shunted via\nseparate electrical circuits and compared with the ones those are shunted via\ninterconnected electrical circuits. For verification, system-level finite\nelement simulations are performed in ANSYS software and compared with the\nanalytical model results. An experimental setup is also built to validate the\nperformance of the separately shunted piezoelectric patches. The effectiveness\nof the method is investigated for a broader range of frequencies and it was\nshown that separately shunted piezoelectric patches are more effective compared\nto connected for a wide range of frequencies.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 13:31:38 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Motlagh", "Peyman Lahe", ""], ["Basdogana", "Ipek", ""]]}, {"id": "2103.13294", "submitter": "Apostolos Chalkis", "authors": "Apostolos Chalkis, Emmanouil Christoforou, Theodore Dalamagkas,\n  Ioannis Z. Emiris", "title": "Modeling of crisis periods in stock markets", "comments": "11 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exploit a recent computational framework to model and detect financial\ncrises in stock markets, as well as shock events in cryptocurrency markets,\nwhich are characterized by a sudden or severe drop in prices. Our method\nmanages to detect all past crises in the French industrial stock market\nstarting with the crash of 1929, including financial crises after 1990 (e.g.\ndot-com bubble burst of 2000, stock market downturn of 2002), and all past\ncrashes in the cryptocurrency market, namely in 2018, and also in 2020 due to\ncovid-19. We leverage copulae clustering, based on the distance between\nprobability distributions, in order to validate the reliability of the\nframework; we show that clusters contain copulae from similar market states\nsuch as normal states, or crises. Moreover, we propose a novel regression model\nthat can detect successfully all past events using less than 10% of the\ninformation that the previous framework requires. We train our model by\nhistorical data on the industry assets, and we are able to detect all past\nshock events in the cryptocurrency market. Our tools provide the essential\ncomponents of our software framework that offers fast and reliable detection,\nor even prediction, of shock events in stock and cryptocurrency markets of\nhundreds of assets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 23:22:14 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Chalkis", "Apostolos", ""], ["Christoforou", "Emmanouil", ""], ["Dalamagkas", "Theodore", ""], ["Emiris", "Ioannis Z.", ""]]}, {"id": "2103.13735", "submitter": "Huu Phuoc Le", "authors": "Huu Phuoc Le, Mohab Safey El Din", "title": "Faster One Block Quantifier Elimination for Regular Polynomial Systems\n  of Equations", "comments": "International Symposium on Symbolic and Algebraic Computation 2021,\n  Jul. 2021, Saint-Petersbourg, Russia", "journal-ref": null, "doi": "10.1145/3452143.3465546", "report-no": null, "categories": "cs.SC cs.CG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Quantifier elimination over the reals is a central problem in computational\nreal algebraic geometry, polynomial system solving and symbolic computation.\nGiven a semi-algebraic formula (whose atoms are polynomial constraints) with\nquantifiers on some variables, it consists in computing a logically equivalent\nformula involving only unquantified variables. When there is no alternation of\nquantifiers, one has a one block quantifier elimination problem.\n  This paper studies a variant of the one block quantifier elimination in which\nwe compute an almost equivalent formula of the input. We design a new\nprobabilistic efficient algorithm for solving this variant when the input is a\nsystem of polynomial equations satisfying some regularity assumptions. When the\ninput is generic, involves $s$ polynomials of degree bounded by $D$ with $n$\nquantified variables and $t$ unquantified ones, we prove that this algorithm\noutputs semi-algebraic formulas of degree bounded by $\\mathcal{D}$ using $O\\\n{\\widetilde{~}}\\left ((n-s+1)\\ 8^{t}\\ \\mathcal{D}^{3t+2}\n\\binom{t+\\mathcal{D}}{t} \\right )$ arithmetic operations in the ground field\nwhere $\\mathcal{D} = 2(n+s)\\ D^s(D-1)^{n-s+1}\\ \\binom{n}{s}$. In practice, it\nallows us to solve quantifier elimination problems which are out of reach of\nthe state-of-the-art (up to $8$ variables).\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 10:26:51 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 20:56:27 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 11:59:07 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Le", "Huu Phuoc", ""], ["Din", "Mohab Safey El", ""]]}, {"id": "2103.13794", "submitter": "Maurilio Matracia", "authors": "Maurilio Matracia, Mustafa A. Kishk, Mohamed-Slim Alouini", "title": "Coverage Analysis for UAV-Assisted Cellular Networks in Rural Areas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite coverage enhancement in rural areas is one of the main requirements\nin next generations of wireless networks (i.e., 5G and 6G), the low expected\nprofit prevents telecommunication providers from investing in such sparsely\npopulated areas. Hence, it is required to design and deploy cost efficient\nalternatives for extending the cellular infrastructure to these regions. A\nconcrete mathematical model that characterizes and clearly captures the\naforementioned problem might be a key-enabler for studying the efficiency of\nany potential solution. Unfortunately, the commonly used mathematical tools\nthat model large scale wireless networks are not designed to capture the\nunfairness, in terms of cellular coverage, suffered by exurban and rural areas.\nIn big cities, in fact, cellular deployment is essentially capacity driven and\nthus cellular base station densities are maximum in the town centers and\ndecline when getting far from them. In this paper, a new stochastic\ngeometry-based model is implemented in order to show the coverage spatial\nvariation among urban, suburban, and exurban settlements. Indeed, by\nimplementing inhomogeneous Poisson point processes (PPPs) it is possible to\nstudy the performance metrics in a realistic scenario where terrestrial base\nstations (TBSs) are clustered around the urban center while outer aerial base\nstations (ABSs) are uniformly distributed outside an urban exclusion zone.\nBased on this, our simulation results can quantify the improvement, in terms of\ncoverage probability, that even a surprisingly low density of ABSs can bring to\nperipheral regions depending on the extension of the exclusion zone, enabling\nus to draw insightful considerations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 12:43:43 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Matracia", "Maurilio", ""], ["Kishk", "Mustafa A.", ""], ["Alouini", "Mohamed-Slim", ""]]}, {"id": "2103.13882", "submitter": "Benjamin Holmgren", "authors": "Benjamin Holmgren, Bradley McCoy, Brittany Fasy, and David Millman", "title": "If You Must Choose Among Your Children, Pick the Right One", "comments": "Proceedings of the Canadian Conference on Computational Geometry,\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a simplicial complex $K$ and an injective function $f$ from the\nvertices of $K$ to $\\mathbb{R}$, we consider algorithms that extend $f$ to a\ndiscrete Morse function on $K$. We show that an algorithm of King, Knudson and\nMramor can be described on the directed Hasse diagram of $K$. Our description\nhas a faster runtime for high dimensional data with no increase in space.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 20:04:06 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Holmgren", "Benjamin", ""], ["McCoy", "Bradley", ""], ["Fasy", "Brittany", ""], ["Millman", "David", ""]]}, {"id": "2103.13956", "submitter": "Guilherme D. da Fonseca", "authors": "Lo\\\"ic Crombez, Guilherme D. da Fonseca, Yan Gerard, Aldo\n  Gonzalez-Lorenzo, Pascal Lafourcade, and Luc Libralesso", "title": "Shadoks Approach to Low-Makespan Coordinated Motion Planning", "comments": null, "journal-ref": "SoCG 2021, 63:1-9", "doi": "10.4230/LIPIcs.SoCG.2021.63", "report-no": null, "categories": "cs.CG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the heuristics used by the Shadoks team for the CG:SHOP\n2021 challenge. This year's problem is to coordinate the motion of multiple\nrobots in order to reach their targets without collisions and minimizing the\nmakespan. Using the heuristics outlined in this paper, our team won first place\nwith the best solution to 202 out of 203 instances and optimal solutions to at\nleast 105 of them.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 16:24:49 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 17:21:46 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 09:58:44 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Crombez", "Lo\u00efc", ""], ["da Fonseca", "Guilherme D.", ""], ["Gerard", "Yan", ""], ["Gonzalez-Lorenzo", "Aldo", ""], ["Lafourcade", "Pascal", ""], ["Libralesso", "Luc", ""]]}, {"id": "2103.14092", "submitter": "Onur \\c{C}a\\u{g}{\\i}r{\\i}c{\\i} M.Sc.", "authors": "Onur \\c{C}a\\u{g}{\\i}r{\\i}c{\\i} and Deniz A\\u{g}ao\\u{g}lu", "title": "Unit Disk Visibility Graphs", "comments": "16 pages, full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study unit disk visibility graphs, where the visibility relation between a\npair of geometric entities is defined by not only obstacles, but also the\ndistance between them. That is, two entities are not mutually visible if they\nare too far apart, regardless of having an obstacle between them. This\nparticular graph class models real world scenarios more accurately compared to\nthe conventional visibility graphs. We first define and classify the unit disk\nvisibility graphs, and then show that the 3-coloring problem is NP-complete\nwhen unit disk visibility model is used for a set of line segments (which\napplies to a set of points) and for a polygon with holes.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 19:17:38 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["\u00c7a\u011f\u0131r\u0131c\u0131", "Onur", ""], ["A\u011fao\u011flu", "Deniz", ""]]}, {"id": "2103.14599", "submitter": "Dominik Michael Krupke", "authors": "Kevin Buchin, S\\'andor P. Fekete, Alexander Hill, Linda Kleist, Irina\n  Kostitsyna, Dominik Krupke, Roel Lambers, Martijn Struijs", "title": "Minimum Scan Cover and Variants -- Theory and Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a spectrum of geometric optimization problems motivated by\ncontexts such as satellite communication and astrophysics. In the problem\nMinimum Scan Cover with Angular Costs, we are given a graph $G$ that is\nembedded in Euclidean space. The edges of $G$ need to be scanned, i.e., probed\nfrom both of their vertices. In order to scan their edge, two vertices need to\nface each other; changing the heading of a vertex incurs some cost in terms of\nenergy or rotation time that is proportional to the corresponding rotation\nangle. Our goal is to compute schedules that minimize the following objective\nfunctions: (i) in Minimum Makespan Scan Cover (MSC-MS), this is the time until\nall edges are scanned; (ii) in Minimum Total Energy Scan Cover (MSC-TE), the\nsum of all rotation angles; (iii) in Minimum Bottleneck Energy Scan Cover\n(MSC-BE), the maximum total rotation angle at one vertex.\n  Previous theoretical work on MSC-MS revealed a close connection to graph\ncoloring and the cut cover problem, leading to hardness and approximability\nresults. In this paper, we present polynomial-time algorithms for 1D instances\nof MSC-TE and MSC-BE, but NP-hardness proofs for bipartite 2D instances. For\nbipartite graphs in 2D, we also give 2-approximation algorithms for both MSC-TE\nand MSC-BE. Most importantly, we provide a comprehensive study of practical\nmethods for all three problems. We compare three different mixed-integer\nprogramming and two constraint programming approaches, and show how to compute\nprovably optimal solutions for geometric instances with up to 300 edges.\nAdditionally, we compare the performance of different meta-heuristics for even\nlarger instances.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:02:10 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Buchin", "Kevin", ""], ["Fekete", "S\u00e1ndor P.", ""], ["Hill", "Alexander", ""], ["Kleist", "Linda", ""], ["Kostitsyna", "Irina", ""], ["Krupke", "Dominik", ""], ["Lambers", "Roel", ""], ["Struijs", "Martijn", ""]]}, {"id": "2103.14627", "submitter": "Marco Cavallo", "authors": "Marco Cavallo", "title": "Higher Dimensional Graphics: Conceiving Worlds in Four Spatial\n  Dimensions and Beyond", "comments": "Eurographics 2021 / Computer Graphics Forum", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CG cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While the interpretation of high-dimensional datasets has become a necessity\nin most industries, and is supported by continuous advances in data science and\nmachine learning, the spatial visualization of higher-dimensional geometry has\nmostly remained a niche research topic for mathematicians and physicists.\nIntermittent contributions to this field date back more than a century, and\nhave had a non-negligible influence on contemporary art and philosophy.\nHowever, most contributions have focused on the understanding of specific\nmathematical shapes, with few concrete applications. In this work, we attempt\nto revive the community's interest in visualizing higher dimensional geometry\nby shifting the focus from the visualization of abstract shapes to the design\nof a broader hyper-universe concept, wherein 3D and 4D objects can coexist and\ninteract with each other. Specifically, we discuss the content definition,\nauthoring patterns, and technical implementations associated with the process\nof extending standard 3D applications as to support 4D mechanics. We\noperationalize our ideas through the introduction of a new hybrid 3D/4D\nvideogame called Across Dimensions, which we developed in Unity3D through the\nintegration of our own 4D plugin.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:41:25 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Cavallo", "Marco", ""]]}, {"id": "2103.15037", "submitter": "Debajyoti Mondal", "authors": "Jared Espenant and Debajyoti Mondal", "title": "StreamTable: An Area Proportional Visualization for Tables with Flowing\n  Streams", "comments": "Preliminary results appeared at the 37th European Workshop on\n  Computational Geometry (EuroCG 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $M$ be an $r\\times c$ table with each cell weighted by a nonzero positive\nnumber. A StreamTable visualization of $M$ represents the columns as\nnon-overlapping vertical streams and the rows as horizontal stripes such that\nthe area of intersection between a column and a row is equal to the weight of\nthe corresponding cell. To avoid large wiggle of the streams, it is desirable\nto keep the consecutive cells in a stream to be adjacent. Let $B$ be the\nsmallest axis-aligned bounding box containing the StreamTable. Then the\ndifference between the area of $B$ and the sum of the weights is referred to as\nthe excess area.\n  We examine the complexity of optimizing various table aesthetics (minimizing\nexcess area, or maximizing cell adjacencies in streams) in a StreamTable\nvisualization. (A) If the row permutation is fixed and the row heights are\ngiven as a part of the input, then we provide an $O(rc)$-time algorithm that\noptimizes these aesthetics. (B) If the row permutation is fixed but the row\nheights can be chosen, then we discuss a technique to compute an aesthetic\nStreamTable by solving a quadratically constrained quadratic program, followed\nby iterative improvements. (C) If row permutations can be chosen, then we show\nthat it is NP-hard to find a row permutation that optimizes the area or\nadjacency aesthetics.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 03:38:09 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Espenant", "Jared", ""], ["Mondal", "Debajyoti", ""]]}, {"id": "2103.15062", "submitter": "Paul Liu", "authors": "Paul Liu, Jack Spalding-Jamieson, Brandon Zhang, Da Wei Zheng", "title": "Coordinated Motion Planning Through Randomized k-Opt", "comments": "To appear in SoCG 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper examines the approach taken by team gitastrophe in the CG:SHOP\n2021 challenge. The challenge was to find a sequence of simultaneous moves of\nsquare robots between two given configurations that minimized either total\ndistance travelled or makespan (total time). Our winning approach has two main\ncomponents: an initialization phase that finds a good initial solution, and a\n$k$-opt local search phase which optimizes this solution. This led to a first\nplace finish in the distance category and a third place finish in the makespan\ncategory.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 06:37:57 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Paul", ""], ["Spalding-Jamieson", "Jack", ""], ["Zhang", "Brandon", ""], ["Zheng", "Da Wei", ""]]}, {"id": "2103.15381", "submitter": "Sandor P. Fekete", "authors": "S\\'andor P. Fekete and Phillip Keldenich and Dominik Krupke and Joseph\n  S. B. Mitchell", "title": "Computing Coordinated Motion Plans for Robot Swarms: The CG:SHOP\n  Challenge 2021", "comments": "13 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an overview of the 2021 Computational Geometry Challenge, which\ntargeted the problem of optimally coordinating a set of robots by computing a\nfamily of collision-free trajectories for a set set S of n pixel-shaped objects\nfrom a given start configuration into a desired target configuration.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 07:18:51 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Fekete", "S\u00e1ndor P.", ""], ["Keldenich", "Phillip", ""], ["Krupke", "Dominik", ""], ["Mitchell", "Joseph S. B.", ""]]}, {"id": "2103.15712", "submitter": "Benjamin Doerr", "authors": "Benjamin Doerr", "title": "A Sharp Discrepancy Bound for Jittered Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CG cs.DM cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For $m, d \\in {\\mathbb N}$, a jittered sampling point set $P$ having $N =\nm^d$ points in $[0,1)^d$ is constructed by partitioning the unit cube $[0,1)^d$\ninto $m^d$ axis-aligned cubes of equal size and then placing one point\nindependently and uniformly at random in each cube. We show that there are\nconstants $c \\ge 0$ and $C$ such that for all $d$ and all $m \\ge d$ the\nexpected non-normalized star discrepancy of a jittered sampling point set\nsatisfies \\[c \\,dm^{\\frac{d-1}{2}} \\sqrt{1 + \\log(\\tfrac md)} \\le {\\mathbb E}\nD^*(P) \\le C\\, dm^{\\frac{d-1}{2}} \\sqrt{1 + \\log(\\tfrac md)}.\\]\n  This discrepancy is thus smaller by a factor of\n$\\Theta\\big(\\sqrt{\\frac{1+\\log(m/d)}{m/d}}\\,\\big)$ than the one of a uniformly\ndistributed random point set of $m^d$ points. This result improves both the\nupper and the lower bound for the discrepancy of jittered sampling given by\nPausinger and Steinerberger (Journal of Complexity (2016)). It also removes the\nasymptotic requirement that $m$ is sufficiently large compared to $d$.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 15:53:53 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Doerr", "Benjamin", ""]]}, {"id": "2103.16071", "submitter": "Ahmed Abdelkader", "authors": "Ahmed Abdelkader, David M. Mount", "title": "Approximate Nearest-Neighbor Search for Line Segments", "comments": "20 pages (including appendix), 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Approximate nearest-neighbor search is a fundamental algorithmic problem that\ncontinues to inspire study due its essential role in numerous contexts. In\ncontrast to most prior work, which has focused on point sets, we consider\nnearest-neighbor queries against a set of line segments in $\\mathbb{R}^d$, for\nconstant dimension $d$. Given a set $S$ of $n$ disjoint line segments in\n$\\mathbb{R}^d$ and an error parameter $\\varepsilon > 0$, the objective is to\nbuild a data structure such that for any query point $q$, it is possible to\nreturn a line segment whose Euclidean distance from $q$ is at most\n$(1+\\varepsilon)$ times the distance from $q$ to its nearest line segment. We\npresent a data structure for this problem with storage $O((n^2/\\varepsilon^{d})\n\\log (\\Delta/\\varepsilon))$ and query time $O(\\log\n(\\max(n,\\Delta)/\\varepsilon))$, where $\\Delta$ is the spread of the set of\nsegments $S$. Our approach is based on a covering of space by anisotropic\nelements, which align themselves according to the orientations of nearby\nsegments.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 04:39:46 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 14:33:30 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Abdelkader", "Ahmed", ""], ["Mount", "David M.", ""]]}, {"id": "2103.17094", "submitter": "Torsten Ueckerdt", "authors": "Zden\\v{e}k Dvo\\v{r}\\'ak, Jakub Pek\\'arek, Torsten Ueckerdt, Yelena\n  Yuditsky", "title": "Weak Coloring Numbers of Intersection Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weak and strong coloring numbers are generalizations of the degeneracy of a\ngraph, where for each natural number $k$, we seek a vertex ordering such every\nvertex can (weakly respectively strongly) reach in $k$ steps only few vertices\nwith lower index in the ordering. Both notions capture the sparsity of a graph\nor a graph class, and have interesting applications in the structural and\nalgorithmic graph theory. Recently, the first author together with McCarty and\nNorin observed a natural volume-based upper bound for the strong coloring\nnumbers of intersection graphs of well-behaved objects in $\\mathbb{R}^d$, such\nas homothets of a centrally symmetric compact convex object, or comparable\naxis-aligned boxes.\n  In this paper, we prove upper and lower bounds for the $k$-th weak coloring\nnumbers of these classes of intersection graphs. As a consequence, we describe\na natural graph class whose strong coloring numbers are polynomial in $k$, but\nthe weak coloring numbers are exponential. We also observe a surprising\ndifference in terms of the dependence of the weak coloring numbers on the\ndimension between touching graphs of balls (single-exponential) and hypercubes\n(double-exponential).\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:04:42 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 10:13:07 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Dvo\u0159\u00e1k", "Zden\u011bk", ""], ["Pek\u00e1rek", "Jakub", ""], ["Ueckerdt", "Torsten", ""], ["Yuditsky", "Yelena", ""]]}, {"id": "2103.17251", "submitter": "Marc Glisse", "authors": "Marc Glisse", "title": "Lower bound on the Voronoi diagram of lines in $\\mathbb{R}^d$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note gives a lower bound of $\\Omega(n^{\\lceil 2d/3\\rceil})$ on the\nmaximal complexity of the Euclidean Voronoi diagram of $n$ non-intersecting\nlines in $\\mathbb{R}^d$ for $d>2$.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:52:51 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Glisse", "Marc", ""]]}]