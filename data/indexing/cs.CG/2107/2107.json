[{"id": "2107.00684", "submitter": "Csaba D. Toth", "authors": "Sujoy Bhore and Csaba D. T\\'oth", "title": "Online Euclidean Spanners", "comments": "22 pages, 8 figures. An extended abstract of this paper will appear\n  in the Proceedings of ESA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the online Euclidean spanners problem for points in\n$\\mathbb{R}^d$. Suppose we are given a sequence of $n$ points $(s_1,s_2,\\ldots,\ns_n)$ in $\\mathbb{R}^d$, where point $s_i$ is presented in step~$i$ for\n$i=1,\\ldots, n$. The objective of an online algorithm is to maintain a\ngeometric $t$-spanner on $S_i=\\{s_1,\\ldots, s_i\\}$ for each step~$i$.\n  First, we establish a lower bound of $\\Omega(\\varepsilon^{-1}\\log n / \\log\n\\varepsilon^{-1})$ for the competitive ratio of any online\n$(1+\\varepsilon)$-spanner algorithm, for a sequence of $n$ points in\n1-dimension. We show that this bound is tight, and there is an online algorithm\nthat can maintain a $(1+\\varepsilon)$-spanner with competitive ratio\n$O(\\varepsilon^{-1}\\log n / \\log \\varepsilon^{-1})$. Next, we design online\nalgorithms for sequences of points in $\\mathbb{R}^d$, for any constant $d\\ge\n2$, under the $L_2$ norm. We show that previously known incremental algorithms\nachieve a competitive ratio $O(\\varepsilon^{-(d+1)}\\log n)$. However, if the\nalgorithm is allowed to use additional points (Steiner points), then it is\npossible to substantially improve the competitive ratio in terms of\n$\\varepsilon$. We describe an online Steiner $(1+\\varepsilon)$-spanner\nalgorithm with competitive ratio $O(\\varepsilon^{(1-d)/2} \\log n)$. As a\ncounterpart, we show that the dependence on $n$ cannot be eliminated in\ndimensions $d \\ge 2$. In particular, we prove that any online spanner algorithm\nfor a sequence of $n$ points in $\\mathbb{R}^d$ under the $L_2$ norm has\ncompetitive ratio $\\Omega(f(n))$, where $\\lim_{n\\rightarrow\n\\infty}f(n)=\\infty$. Finally, we provide improved lower bounds under the $L_1$\nnorm: $\\Omega(\\varepsilon^{-2}/\\log \\varepsilon^{-1})$ in the plane and\n$\\Omega(\\varepsilon^{-d})$ in $\\mathbb{R}^d$ for $d\\geq 3$.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 18:18:52 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Bhore", "Sujoy", ""], ["T\u00f3th", "Csaba D.", ""]]}, {"id": "2107.01553", "submitter": "Ling Zhou", "authors": "Marco Contessoto, Facundo M\\'emoli, Anastasios Stefanou and Ling Zhou", "title": "Persistent Cup-Length", "comments": "32 pages, 13 figures. This version includes update of bibliography,\n  restructuring of section 3 and section 4, updated algorithms that work for\n  general filtration, and more", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cohomological ideas have recently been injected into persistent homology and\nhave been utilized for both enriching and accelerating the calculation of\npersistence diagrams. For instance, the software Ripser fundamentally exploits\nthe computational advantages offered by cohomological ideas. The cup product\noperation which is available at cohomology level gives rise to a graded ring\nstructure which extends the natural vector space structure and is therefore\nable to extract and encode additional rich information. The maximum number of\ncocycles having non-zero cup product yields an invariant, the Cup-Length, which\nis efficient at discriminating spaces.\n  In this paper, we lift the cup-length into the Persistent Cup-Length\ninvariant for the purpose of extracting non-trivial information about the\nevolution of the cohomology ring structure across a filtration. We show that\nthe Persistent Cup-Length can be computed from a family of representative\ncocycles and devise a polynomial time algorithm for the computation of the\nPersistent Cup-Length invariant. We furthermore show that this invariant is\nstable under suitable interleaving-type distances. Along the way, we identify\nan invariant which we call the Cup-Length Diagram, which is stronger than\npersistent cup-length but can still be computed efficiently.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 06:17:35 GMT"}, {"version": "v2", "created": "Sun, 18 Jul 2021 18:31:32 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Contessoto", "Marco", ""], ["M\u00e9moli", "Facundo", ""], ["Stefanou", "Anastasios", ""], ["Zhou", "Ling", ""]]}, {"id": "2107.01759", "submitter": "Jibum Kim", "authors": "Jaeseung Lee, Woojin Choi, Jibum Kim", "title": "Learning Delaunay Triangulation using Self-attention and Domain\n  Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Delaunay triangulation is a well-known geometric combinatorial optimization\nproblem with various applications. Many algorithms can generate Delaunay\ntriangulation given an input point set, but most are nontrivial algorithms\nrequiring an understanding of geometry or the performance of additional\ngeometric operations, such as the edge flip. Deep learning has been used to\nsolve various combinatorial optimization problems; however, generating Delaunay\ntriangulation based on deep learning remains a difficult problem, and very few\nresearch has been conducted due to its complexity. In this paper, we propose a\nnovel deep-learning-based approach for learning Delaunay triangulation using a\nnew attention mechanism based on self-attention and domain knowledge. The\nproposed model is designed such that the model efficiently learns\npoint-to-point relationships using self-attention in the encoder. In the\ndecoder, a new attention score function using domain knowledge is proposed to\nprovide a high penalty when the geometric requirement is not satisfied. The\nstrength of the proposed attention score function lies in its ability to extend\nits application to solving other combinatorial optimization problems involving\ngeometry. When the proposed neural net model is well trained, it is simple and\nefficient because it automatically predicts the Delaunay triangulation for an\ninput point set without requiring any additional geometric operations. We\nconduct experiments to demonstrate the effectiveness of the proposed model and\nconclude that it exhibits better performance compared with other\ndeep-learning-based approaches.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 01:56:37 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Lee", "Jaeseung", ""], ["Choi", "Woojin", ""], ["Kim", "Jibum", ""]]}, {"id": "2107.02115", "submitter": "Ryan Slechta", "authors": "Tamal K. Dey and Marian Mrozek and Ryan Slechta", "title": "Persistence of Conley-Morse Graphs in Combinatorial Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.CG math.AT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multivector fields provide an avenue for studying continuous dynamical\nsystems in a combinatorial framework. There are currently two approaches in the\nliterature which use persistent homology to capture changes in combinatorial\ndynamical systems. The first captures changes in the Conley index, while the\nsecond captures changes in the Morse decomposition. However, such approaches\nhave limitations. The former approach only describes how the Conley index\nchanges across a selected isolated invariant set though the dynamics can be\nmuch more complicated than the behavior of a single isolated invariant set.\nLikewise, considering a Morse decomposition omits much information about the\nindividual Morse sets. In this paper, we propose a method to summarize changes\nin combinatorial dynamical systems by capturing changes in the so-called\nConley-Morse graphs. A Conley-Morse graph contains information about both the\nstructure of a selected Morse decomposition and about the Conley index at each\nMorse set in the decomposition. Hence, our method summarizes the changing\nstructure of a sequence of dynamical systems at a finer granularity than\nprevious approaches.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 16:12:59 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 02:38:30 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Dey", "Tamal K.", ""], ["Mrozek", "Marian", ""], ["Slechta", "Ryan", ""]]}, {"id": "2107.02570", "submitter": "Svend Christian Svendsen", "authors": "Svend C. Svendsen", "title": "Practical I/O-Efficient Multiway Separators", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We revisit the fundamental problem of I/O-efficiently computing $r$-way\nseparators on planar graphs. An $r$-way separator divides a planar graph with\n$N$ vertices into $O(r)$ regions of size $O(N/r)$ and $O(\\sqrt {Nr})$ boundary\nvertices in total, where boundary vertices are vertices that are adjacent to\nmore than one region. Such separators are used in I/O-efficient solutions to\nmany fundamental problems on planar graphs such as breadth-first search,\nfinding single-source shortest paths, topological sorting, and finding strongly\nconnected components. Our main result is an I/O-efficient sampling-based\nalgorithm that, given a Koebe-embedding of a graph with $N$ vertices and a\nparameter $r$, computes an $r$-way separator for the graph under certain\nassumptions on the size of internal memory. Computing a Koebe-embedding of a\nplanar graph is difficult in practice and no known I/O-efficient algorithm\ncurrently exists. Therefore, we show how our algorithm can be generalized and\napplied directly to Delaunay triangulations without relying on a\nKoebe-embedding. This adaptation can produce many boundary vertices in the\nworst-case, however, to our knowledge our result is the first to be implemented\nin practice due to the many non-trivial and complex techniques used in previous\nresults. Furthermore, we show that our algorithm performs well on real-world\ndata and that the number of boundary vertices is small in practice.\n  Motivated by applications in geometric information systems, we show how our\nalgorithm for Delaunay triangulations can be applied to compute the flow\naccumulation over a terrain, which models how much water flows over the\nvertices of a terrain. When given an $r$-way separator, our implementation of\nthe algorithm outperforms traditional sweep-line-based algorithms on the\npublicly available digital elevation model of Denmark.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 12:14:26 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Svendsen", "Svend C.", ""]]}, {"id": "2107.02787", "submitter": "Younan Gao", "authors": "Younan Gao and Meng He", "title": "Space Efficient Two-Dimensional Orthogonal Colored Range Counting", "comments": "full version of an ESA 2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the two-dimensional orthogonal colored range counting problem, we\npreprocess a set, $P$, of $n$ colored points on the plane, such that given an\northogonal query rectangle, the number of distinct colors of the points\ncontained in this rectangle can be computed efficiently.\n  For this problem, we design three new solutions, and the bounds of each can\nbe expressed in some form of time-space tradeoff.\n  By setting appropriate parameter values for these solutions, we can achieve\nnew specific results with (the space are in words and $\\epsilon$ is an\narbitrary constant in $(0,1)$):\n  ** $O(n\\lg^3 n)$ space and $O(\\sqrt{n}\\lg^{5/2} n \\lg \\lg n)$ query time;\n  ** $O(n\\lg^2 n)$ space and $O(\\sqrt{n}\\lg^{4+\\epsilon} n)$ query time;\n  ** $O(n\\frac{\\lg^2 n}{\\lg \\lg n})$ space and $O(\\sqrt{n}\\lg^{5+\\epsilon} n)$\nquery time;\n  ** $O(n\\lg n)$ space and $O(n^{1/2+\\epsilon})$ query time.\n  A known conditional lower bound to this problem based on Boolean matrix\nmultiplication gives some evidence on the difficulty of achieving near-linear\nspace solutions with query time better than $\\sqrt{n}$ by more than a\npolylogarithmic factor using purely combinatorial approaches. Thus the time and\nspace bounds in all these results are efficient.\n  Previously, among solutions with similar query times, the most\nspace-efficient solution uses $O(n\\lg^4 n)$ space to answer queries in\n$O(\\sqrt{n}\\lg^8 n)$ time (SIAM. J. Comp.~2008).\n  Thus the new results listed above all achieve improvements in space\nefficiency, while all but the last result achieve speed-up in query time as\nwell.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 17:50:19 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Gao", "Younan", ""], ["He", "Meng", ""]]}, {"id": "2107.03140", "submitter": "Bahram Sadeghi Bigham", "authors": "Bahram Sadeghi Bigham", "title": "Minimum Constraint Removal Problem for Line Segments is NP-hard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the minimum constraint removal ($MCR$), there is no feasible path to move\nfrom the starting point towards the goal and, the minimum constraints should be\nremoved in order to find a collision-free path. It has been proved that $MCR$\nproblem is $NP-hard$ when constraints have arbitrary shapes or even they are in\nshape of convex polygons. However, it has a simple linear solution when\nconstraints are lines and the problem is open for other cases yet. In this\npaper, using a reduction from Subset Sum problem, in three steps, we show that\nthe problem is NP-hard for both weighted and unweighted line segments.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 10:57:22 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Bigham", "Bahram Sadeghi", ""]]}, {"id": "2107.03153", "submitter": "Joseph O'Rourke", "authors": "Joseph O'Rourke and Costin Vilcu", "title": "Reshaping Convex Polyhedra", "comments": "Research monograph. 234 pages, 105 figures, 55 references. arXiv\n  admin note: text overlap with arXiv:2008.01759", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given a convex polyhedral surface P, we define a tailoring as excising from P\na simple polygonal domain that contains one vertex v, and whose boundary can be\nsutured closed to a new convex polyhedron via Alexandrov's Gluing Theorem. In\nparticular, a digon-tailoring cuts off from P a digon containing v, a subset of\nP bounded by two equal-length geodesic segments that share endpoints, and can\nthen zip closed.\n  In the first part of this monograph, we primarily study properties of the\ntailoring operation on convex polyhedra. We show that P can be reshaped to any\npolyhedral convex surface Q a subset of conv(P) by a sequence of tailorings.\nThis investigation uncovered previously unexplored topics, including a notion\nof unfolding of Q onto P--cutting up Q into pieces pasted non-overlapping onto\nP.\n  In the second part of this monograph, we study vertex-merging processes on\nconvex polyhedra (each vertex-merge being in a sense the reverse of a\ndigon-tailoring), creating embeddings of P into enlarged surfaces. We aim to\nproduce non-overlapping polyhedral and planar unfoldings, which led us to\ndevelop an apparently new theory of convex sets, and of minimal length\nenclosing polygons, on convex polyhedra.\n  All our theorem proofs are constructive, implying polynomial-time algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 11:29:46 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["O'Rourke", "Joseph", ""], ["Vilcu", "Costin", ""]]}, {"id": "2107.03193", "submitter": "Thore Thie{\\ss}en", "authors": "Thore Thie{\\ss}en and Jan Vahrenhold", "title": "Oblivious Median Slope Selection", "comments": "14 pages, to appear in Proceedings of CCCG 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CR cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the median slope selection problem in the oblivious RAM model. In\nthis model memory accesses have to be independent of the data processed, i.e.,\nan adversary cannot use observed access patterns to derive additional\ninformation about the input. We show how to modify the randomized algorithm of\nMatou\\v{s}ek (1991) to obtain an oblivious version with O(n log^2 n) expected\ntime for n points in R^2. This complexity matches a theoretical upper bound\nthat can be obtained through general oblivious transformation. In addition,\nresults from a proof-of-concept implementation show that our algorithm is also\npractically efficient.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 13:10:13 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Thie\u00dfen", "Thore", ""], ["Vahrenhold", "Jan", ""]]}, {"id": "2107.03460", "submitter": "Thomas Weighill", "authors": "Tom Needham and Thomas Weighill", "title": "Geometric averages of partitioned datasets", "comments": "31 pages, Supplemental Material included as an Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for jointly registering ensembles of partitioned\ndatasets in a way which is both geometrically coherent and partition-aware.\nOnce such a registration has been defined, one can group partition blocks\nacross datasets in order to extract summary statistics, generalizing the\ncommonly used order statistics for scalar-valued data. By modeling a\npartitioned dataset as an unordered $k$-tuple of points in a Wasserstein space,\nwe are able to draw from techniques in optimal transport. More generally, our\nmethod is developed using the formalism of local Fr\\'{e}chet means in symmetric\nproducts of metric spaces. We establish basic theory in this general setting,\nincluding Alexandrov curvature bounds and a verifiable characterization of\nlocal means. Our method is demonstrated on ensembles of political redistricting\nplans to extract and visualize basic properties of the space of plans for a\nparticular state, using North Carolina as our main example.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 19:52:08 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Needham", "Tom", ""], ["Weighill", "Thomas", ""]]}, {"id": "2107.03615", "submitter": "Daniel Frishberg", "authors": "David Eppstein, Daniel Frishberg, and Martha C. Osegueda", "title": "Angles of Arc-Polygons and Lombardi Drawings of Cacti", "comments": "12 pages, 8 figures. To be published in Proc. 33rd Canadian\n  Conference on Computational Geometry, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We characterize the triples of interior angles that are possible in\nnon-self-crossing triangles with circular-arc sides, and we prove that a given\ncyclic sequence of angles can be realized by a non-self-crossing polygon with\ncircular-arc sides whenever all angles are at most pi. As a consequence of\nthese results, we prove that every cactus has a planar Lombardi drawing (a\ndrawing with edges depicted as circular arcs, meeting at equal angles at each\nvertex) for its natural embedding in which every cycle of the cactus is a face\nof the drawing. However, there exist planar embeddings of cacti that do not\nhave planar Lombardi drawings.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 05:35:56 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Eppstein", "David", ""], ["Frishberg", "Daniel", ""], ["Osegueda", "Martha C.", ""]]}, {"id": "2107.03861", "submitter": "Eunjin Oh", "authors": "Shinwoo An and Eunjin Oh", "title": "Feedback Vertex Set on Geometric Intersection Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present an algorithm for computing a feedback vertex set of\na unit disk graph of size $k$, if it exists, which runs in time\n$2^{O(\\sqrt{k})}(n+m)$, where $n$ and $m$ denote the numbers of vertices and\nedges, respectively. This improves the $2^{O(\\sqrt{k}\\log k)}n^{O(1)}$-time\nalgorithm for this problem on unit disk graphs by Fomin et al. [ICALP 2017].\nMoreover, our algorithm is optimal assuming the exponential-time hypothesis.\nAlso, our algorithm can be extended to handle geometric intersection graphs of\nsimilarly sized fat objects without increasing the running time.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 14:17:34 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["An", "Shinwoo", ""], ["Oh", "Eunjin", ""]]}, {"id": "2107.04112", "submitter": "Florestan Brunck", "authors": "Florestan Brunck", "title": "Iterated Medial Triangle Subdivision in Surfaces of Constant Curvature", "comments": "27 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a geodesic triangle on a surface of constant curvature and subdivide\nit recursively into 4 triangles by joining the midpoints of its edges. We show\nthe existence of a uniform $\\delta>0$ such that, at any step of the\nsubdivision, all the triangle angles lie in the interval $(\\delta, \\pi\n-\\delta)$. Additionally, we exhibit stabilising behaviours for both angles and\nlengths as this subdivision progresses.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 21:32:51 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Brunck", "Florestan", ""]]}, {"id": "2107.04321", "submitter": "Pascal Kunz", "authors": "Pascal Kunz, Till Fluschnik, Rolf Niedermeier, Malte Renken", "title": "Most Classic Problems Remain NP-hard on Relative Neighborhood Graphs and\n  their Relatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proximity graphs have been studied for several decades, motivated by\napplications in computational geometry, geography, data mining, and many other\nfields. However, the computational complexity of classic graph problems on\nproximity graphs mostly remained open. We now study 3-Colorability, Dominating\nSet, Feedback Vertex Set, Hamiltonian Cycle, and Independent Set on the\nproximity graph classes relative neighborhood graphs, Gabriel graphs, and\nrelatively closest graphs. We prove that all of the problems remain NP-hard on\nthese graphs, except for 3-Colorability and Hamiltonian Cycle on relatively\nclosest graphs, where the former is trivial and the latter is left open.\nMoreover, for every NP-hard case we additionally show that no\n$2^{o(n^{1/4})}$-time algorithm exists unless the ETH fails, where n denotes\nthe number of vertices.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 09:20:30 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Kunz", "Pascal", ""], ["Fluschnik", "Till", ""], ["Niedermeier", "Rolf", ""], ["Renken", "Malte", ""]]}, {"id": "2107.04519", "submitter": "Manuel Soriano-Trigueros", "authors": "R. Gonzalez Diaz, M. Soriano Trigueros", "title": "Persistence Partial Matchings Induced by Morphisms between Persistence\n  Modules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of persistence partial matching, as a generalization of partial\nmatchings between persistence modules, is introduced. We study how to obtain a\npersistence partial matching $\\mathcal{G}_f$, and a partial matching\n$\\mathcal{M}_f$, induced by a morphism $f$ between persistence modules, both\nbeing linear with respect to direct sums of morphisms. Some of their properties\nare also provided, including their stability after a perturbation of the\nmorphism $f$, and their relationship with other induced partial matchings\nalready defined in TDA.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 16:08:00 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Diaz", "R. Gonzalez", ""], ["Trigueros", "M. Soriano", ""]]}, {"id": "2107.04654", "submitter": "Elizabeth Munch", "authors": "Rehab Alharbi and Erin Wolf Chambers and Elizabeth Munch", "title": "Realizable piecewise linear paths of persistence diagrams with Reeb\n  graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reeb graphs are widely used in a range of fields for the purposes of\nanalyzing and comparing complex spaces via a simpler combinatorial object.\nFurther, they are closely related to extended persistence diagrams, which\nlargely but not completely encode the information of the Reeb graph. In this\npaper, we investigate the effect on the persistence diagram of a particular\ncontinuous operation on Reeb graphs; namely the (truncated) smoothing\noperation. This construction arises in the context of the Reeb graph\ninterleaving distance, but separately from that viewpoint provides a\nsimplification of the Reeb graph which continuously shrinks small loops. We\nthen use this characterization to initiate the study of inverse problems for\nReeb graphs using smoothing by showing which paths in persistence diagram space\n(commonly known as vineyards) can be realized by a path in the space of Reeb\ngraphs via these simple operations. This allows us to solve the inverse problem\non a certain family of piecewise linear vineyards when fixing an initial Reeb\ngraph.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 19:56:55 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Alharbi", "Rehab", ""], ["Chambers", "Erin Wolf", ""], ["Munch", "Elizabeth", ""]]}, {"id": "2107.04657", "submitter": "Darryl Hill", "authors": "Jean-Lou De Carufel, Darryl Hill, Anil Maheshwari, Sasanka Roy, Lu\\'is\n  Fernando Schultz Xavier da Silveira", "title": "Constant Delay Lattice Train Schedules", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The following geometric vehicle scheduling problem has been considered: given\ncontinuous curves $f_1, \\ldots, f_n : \\mathbb{R} \\rightarrow \\mathbb{R}^2$,\nfind non-negative delays $t_1, \\ldots, t_n$ minimizing $\\max \\{ t_1, \\ldots,\nt_n \\}$ such that, for every distinct $i$ {and $j$} and every time $t$, $| f_j\n(t - t_j) - f_i (t - t_i) | > \\ell$, where~$\\ell$ is a given safety distance.\nWe study a variant of this problem where we consider trains (rods) of fixed\nlength $\\ell$ that move at constant speed and sets of train lines (tracks),\neach of which consisting of an axis-parallel line-segment with endpoints in the\ninteger lattice $\\mathbb{Z}^d$ and of a direction of movement (towards $\\infty$\n{or $- \\infty$}). We are interested in upper bounds on the maximum delay we\nneed to introduce on any line to avoid collisions, but more specifically on\nuniversal upper bounds that apply no matter the set of train lines. We show\nsmall universal constant upper bounds for $d = 2$ and any given $\\ell$ and also\nfor $d = 3$ and $\\ell = 1$. Through clique searching, we are also able to show\nthat several of these upper bounds are tight.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 20:17:00 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["De Carufel", "Jean-Lou", ""], ["Hill", "Darryl", ""], ["Maheshwari", "Anil", ""], ["Roy", "Sasanka", ""], ["da Silveira", "Lu\u00eds Fernando Schultz Xavier", ""]]}, {"id": "2107.05036", "submitter": "Jonathan Klawitter", "authors": "Jonathan Klawitter, Felix Klesen, Alexander Wolff", "title": "Algorithms for Floor Planning with Proximity Requirements", "comments": "Appears in the Proceedings of the CAAD Futures 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Floor planning is an important and difficult task in architecture. When\nplanning office buildings, rooms that belong to the same organisational unit\nshould be placed close to each other. This leads to the following NP-hard\nmathematical optimization problem. Given the outline of each floor, a list of\nroom sizes, and, for each room, the unit to which it belongs, the aim is to\ncompute floor plans such that each room is placed on some floor and the total\ndistance of the rooms within each unit is minimized.\n  The problem can be formulated as an integer linear program (ILP). Commercial\nILP solvers exist, but due to the difficulty of the problem, only small to\nmedium instances can be solved to (near-) optimality. For solving larger\ninstances, we propose to split the problem into two subproblems; floor\nassignment and planning single floors. We formulate both subproblems as ILPs\nand solve realistic problem instances. Our experimental study shows that the\nproblem helps to reduce the computation time considerably. Where we were able\nto compute the global optimum, the solution cost of the combined approach\nincreased very little.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 12:39:44 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Klawitter", "Jonathan", ""], ["Klesen", "Felix", ""], ["Wolff", "Alexander", ""]]}, {"id": "2107.05198", "submitter": "Debajyoti Mondal", "authors": "J. Mark Keil, Debajyoti Mondal, Ehsan Moradi, Yakov Nekrich", "title": "Finding a Maximum Clique in a Grounded 1-Bend String Graph", "comments": "A preliminary version of the paper was presented at the 32nd Canadian\n  Conference on Computational Geometry (CCCG 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A grounded 1-bend string graph is an intersection graph of a set of polygonal\nlines, each with one bend, such that the lines lie above a common horizontal\nline $\\ell$ and have exactly one endpoint on $\\ell$. We show that the problem\nof finding a maximum clique in a grounded 1-bend string graph is APX-hard, even\nfor strictly $y$-monotone strings. For general 1-bend strings, the problem\nremains APX-hard even if we restrict the position of the bends and end-points\nto lie on at most three parallel horizontal lines. We give fast algorithms to\ncompute a maximum clique for different subclasses of grounded segment graphs,\nwhich are formed by restricting the strings to various forms of $L$-shapes.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 05:17:35 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Keil", "J. Mark", ""], ["Mondal", "Debajyoti", ""], ["Moradi", "Ehsan", ""], ["Nekrich", "Yakov", ""]]}, {"id": "2107.05412", "submitter": "Matteo Caorsi", "authors": "Juli\\'an Burella P\\'erez, Sydney Hauke, Umberto Lupo, Matteo Caorsi,\n  Alberto Dassatti", "title": "Giotto-ph: A Python Library for High-Performance Computation of\n  Persistent Homology of Vietoris--Rips Filtrations", "comments": "18 apges, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce giotto-ph, a high-performance, open-source software package for\nthe computation of Vietoris--Rips barcodes. giotto-ph is based on Morozov and\nNigmetov's lockfree (multicore) implementation of Ulrich Bauer's Ripser\npackage. It also contains a re-implementation of Boissonnat and Pritam's \"Edge\nCollapser\", implemented so far only in the GUDHI library. Our contribution is\ntwofold: on the one hand, we integrate existing state-of-the-art ideas\ncoherently in a single library and provide Python bindings to the C++ code. On\nthe other hand, we increase parallelization opportunities and improve overall\nperformance by adopting higher performance data structures. The final\nimplementation of our persistent homology backend establishes a new state of\nthe art, surpassing even GPU-accelerated implementations such as Ripser++ when\nusing as few as 5--10 CPU cores. Furthermore, our implementation of the edge\ncollapser algorithm has reduced dependencies and significantly improved\nrun-times.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 13:30:45 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["P\u00e9rez", "Juli\u00e1n Burella", ""], ["Hauke", "Sydney", ""], ["Lupo", "Umberto", ""], ["Caorsi", "Matteo", ""], ["Dassatti", "Alberto", ""]]}, {"id": "2107.05540", "submitter": "Bahram Sadeghi Bigham", "authors": "Bahram Sadeghi Bigham, Sahar Badri and Nazanin Padkan", "title": "A new metaheuristic approach for the art gallery problem", "comments": "A metaheuristic approach for an old NP-hard problem applicable in\n  Robotics and Telecommunication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the problem Localization and trilateration with minimum number of\nlandmarks, we faced 3-guard and classic Art Gallery Problem. The goal of the\nart gallery problem is to find the minimum number of guards within a simple\npolygon to observe and protect the entire of it. It has many applications in\nRobotics, Telecommunication and so on and there are some approaches to handle\nthe art gallery problem which is theoretically NP-hard. This paper offers an\nefficient method based on the Particle Filter algorithm which solves the most\nfundamental state of the problem near optimal. The experimental results on the\nrandomly polygons generated by Bottino shows that the new method is more\naccurate with less or equal guards. Furthermore, we discuss the resampling and\nparticle numbers to minimize the running time.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 10:29:55 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Bigham", "Bahram Sadeghi", ""], ["Badri", "Sahar", ""], ["Padkan", "Nazanin", ""]]}, {"id": "2107.05895", "submitter": "Giovanni Viglietta", "authors": "Giovanni Viglietta", "title": "A Theory of Spherical Diagrams", "comments": "8 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the axiomatic theory of Spherical Occlusion Diagrams as a tool\nto study certain combinatorial properties of polyhedra in $\\mathbb R^3$, which\nare of central interest in the context Art Gallery problems for polyhedra and\nother visibility-related problems in discrete and computational geometry.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 07:49:15 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Viglietta", "Giovanni", ""]]}, {"id": "2107.06130", "submitter": "Raphael Sulzer", "authors": "Raphael Sulzer, Loic Landrieu, Renaud Marlet, Bruno Vallet", "title": "Scalable Surface Reconstruction with Delaunay-Graph Neural Networks", "comments": "The presentation of this work at SGP 2021 is available at\n  https://youtu.be/KIrCDGhS10o", "journal-ref": "Computer Graphics Forum 2021", "doi": "10.1111/cgf.14364", "report-no": "40-Issue 5", "categories": "cs.CV cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel learning-based, visibility-aware, surface reconstruction\nmethod for large-scale, defect-laden point clouds. Our approach can cope with\nthe scale and variety of point cloud defects encountered in real-life\nMulti-View Stereo (MVS) acquisitions. Our method relies on a 3D Delaunay\ntetrahedralization whose cells are classified as inside or outside the surface\nby a graph neural network and an energy model solvable with a graph cut. Our\nmodel, making use of both local geometric attributes and line-of-sight\nvisibility information, is able to learn a visibility model from a small amount\nof synthetic training data and generalizes to real-life acquisitions. Combining\nthe efficiency of deep learning methods and the scalability of energy based\nmodels, our approach outperforms both learning and non learning-based\nreconstruction algorithms on two publicly available reconstruction benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 14:30:32 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 16:01:59 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Sulzer", "Raphael", ""], ["Landrieu", "Loic", ""], ["Marlet", "Renaud", ""], ["Vallet", "Bruno", ""]]}, {"id": "2107.06236", "submitter": "\\'Eric Colin De Verdi\\`ere", "authors": "\\'Eric Colin de Verdi\\`ere and Thomas Magnard", "title": "An FPT algorithm for the embeddability of graphs into two-dimensional\n  simplicial complexes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the embeddability problem of a graph G into a two-dimensional\nsimplicial complex C: Given G and C, decide whether G admits a topological\nembedding into C. The problem is NP-hard, even in the restricted case where C\nis homeomorphic to a surface.\n  It is known that the problem admits an algorithm with running time\nf(c).n^{O(c)}, where n is the size of the graph G and c is the size of the\ntwo-dimensional complex C. In other words, that algorithm is polynomial when C\nis fixed, but the degree of the polynomial depends on C. We prove that the\nproblem is fixed-parameter tractable in the size of the two-dimensional\ncomplex, by providing a deterministic f(c).n^3-time algorithm. We also provide\na randomized algorithm with expected running time 2^{c^{O(1)}}.n^{O(1)}.\n  Our approach is to reduce to the case where G has bounded branchwidth via an\nirrelevant vertex method, and to apply dynamic programming. We do not rely on\nany component of the existing linear-time algorithms for embedding graphs on a\nfixed surface; the only elaborated tool that we use is an algorithm to compute\ngrid minors.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 16:43:49 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["de Verdi\u00e8re", "\u00c9ric Colin", ""], ["Magnard", "Thomas", ""]]}, {"id": "2107.06490", "submitter": "Cuong Than", "authors": "Hung Le and Cuong Than", "title": "Greedy Spanners in Euclidean Spaces Admit Sublinear Separators", "comments": "Abstract shorted to meet Arxiv character limit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The greedy spanner in a low dimensional Euclidean space is a fundamental\ngeometric construction that has been extensively studied over three decades as\nit possesses the two most basic properties of a good spanner: constant maximum\ndegree and constant lightness. Recently, Eppstein and Khodabandeh showed that\nthe greedy spanner in $\\mathbb{R}^2$ admits a sublinear separator in a strong\nsense: any subgraph of $k$ vertices of the greedy spanner in $\\mathbb{R}^2$ has\na separator of size $O(\\sqrt{k})$. Their technique is inherently planar and is\nnot extensible to higher dimensions. They left showing the existence of a small\nseparator for the greedy spanner in $\\mathbb{R}^d$ for any constant $d\\geq 3$\nas an open problem. In this paper, we resolve the problem of Eppstein and\nKhodabandeh by showing that any subgraph of $k$ vertices of the greedy spanner\nin $\\mathbb{R}^d$ has a separator of size $O(k^{1-1/d})$. We introduce a new\ntechnique that gives a simple characterization for any geometric graph to have\na sublinear separator that we dub $\\tau$-lanky: a geometric graph is\n$\\tau$-lanky if any ball of radius $r$ cuts at most $\\tau$ edges of length at\nleast $r$ in the graph. We show that any $\\tau$-lanky geometric graph of $n$\nvertices in $\\mathbb{R}^d$ has a separator of size $O(\\tau n^{1-1/d})$. We then\nderive our main result by showing that the greedy spanner is $O(1)$-lanky. We\nindeed obtain a more general result that applies to unit ball graphs and point\nsets of low fractal dimensions in $\\mathbb{R}^d$. Our technique naturally\nextends to doubling metrics. We use the $\\tau$-lanky characterization to show\nthat there exists a $(1+\\epsilon)$-spanner for doubling metrics of dimension\n$d$ with a constant maximum degree and a separator of size\n$O(n^{1-\\frac{1}{d}})$; this result resolves an open problem posed by Abam and\nHar-Peled a decade ago.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 05:24:40 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Le", "Hung", ""], ["Than", "Cuong", ""]]}, {"id": "2107.06571", "submitter": "Martina Gallato", "authors": "Friedrich Eisenbrand, Martina Gallato, Ola Svensson, Moritz Venzin", "title": "A QPTAS for stabbing rectangles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the following geometric optimization problem: Given $ n $\naxis-aligned rectangles in the plane, the goal is to find a set of horizontal\nsegments of minimum total length such that each rectangle is stabbed. A segment\nstabs a rectangle if it intersects both its left and right edge. As such, this\nstabbing problem falls into the category of weighted geometric set cover\nproblems for which techniques that improve upon the general ${\\Theta}(\\log\nn)$-approximation guarantee have received a lot of attention in the literature.\nChan at al. (2018) have shown that rectangle stabbing is NP-hard and that it\nadmits a constant-factor approximation algorithm based on Varadarajan's\nquasi-uniform sampling method. In this work we make progress on rectangle\nstabbing on two fronts. First, we present a quasi-polynomial time approximation\nscheme (QPTAS) for rectangle stabbing. Furthermore, we provide a simple\n$8$-approximation algorithm that avoids the framework of Varadarajan. This\nsettles two open problems raised by Chan et al. (2018).\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 09:22:38 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Eisenbrand", "Friedrich", ""], ["Gallato", "Martina", ""], ["Svensson", "Ola", ""], ["Venzin", "Moritz", ""]]}, {"id": "2107.06626", "submitter": "Ora Nova Fandina", "authors": "Yair Bartal and Ora Nova Fandina and Kasper Green Larsen", "title": "Optimality of the Johnson-Lindenstrauss Dimensionality Reduction for\n  Practical Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is well known that the Johnson-Lindenstrauss dimensionality reduction\nmethod is optimal for worst case distortion. While in practice many other\nmethods and heuristics are used, not much is known in terms of bounds on their\nperformance. The question of whether the JL method is optimal for practical\nmeasures of distortion was recently raised in \\cite{BFN19} (NeurIPS'19). They\nprovided upper bounds on its quality for a wide range of practical measures and\nshowed that indeed these are best possible in many cases. Yet, some of the most\nimportant cases, including the fundamental case of average distortion were left\nopen. In particular, they show that the JL transform has $1+\\epsilon$ average\ndistortion for embedding into $k$-dimensional Euclidean space, where\n$k=O(1/\\eps^2)$, and for more general $q$-norms of distortion, $k =\nO(\\max\\{1/\\eps^2,q/\\eps\\})$, whereas tight lower bounds were established only\nfor large values of $q$ via reduction to the worst case.\n  In this paper we prove that these bounds are best possible for any\ndimensionality reduction method, for any $1 \\leq q \\leq O(\\frac{\\log (2\\eps^2\nn)}{\\eps})$ and $\\epsilon \\geq \\frac{1}{\\sqrt{n}}$, where $n$ is the size of\nthe subset of Euclidean space.\n  Our results imply that the JL method is optimal for various distortion\nmeasures commonly used in practice, such as {\\it stress, energy} and {\\it\nrelative error}. We prove that if any of these measures is bounded by $\\eps$\nthen $k=\\Omega(1/\\eps^2)$, for any $\\epsilon \\geq \\frac{1}{\\sqrt{n}}$, matching\nthe upper bounds of \\cite{BFN19} and extending their tightness results for the\nfull range moment analysis.\n  Our results may indicate that the JL dimensionality reduction method should\nbe considered more often in practical applications, and the bounds we provide\nfor its quality should be served as a measure for comparison when evaluating\nthe performance of other methods and heuristics.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 12:00:46 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Bartal", "Yair", ""], ["Fandina", "Ora Nova", ""], ["Larsen", "Kasper Green", ""]]}, {"id": "2107.06800", "submitter": "Steve Oudot", "authors": "Magnus Bakke Botnan, Steffen Oppermann, Steve Oudot", "title": "Signed Barcodes for Multi-Parameter Persistence via Rank Decompositions\n  and Rank-Exact Resolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.CG math.RT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the signed barcode, a new visual representation of\nthe global structure of the rank invariant of a multi-parameter persistence\nmodule or, more generally, of a poset representation. Like its unsigned\ncounterpart in one-parameter persistence, the signed barcode encodes the rank\ninvariant as a $\\mathbb{Z}$-linear combination of rank invariants of indicator\nmodules supported on segments in the poset. It can also be enriched to encode\nthe generalized rank invariant as a $\\mathbb{Z}$-linear combination of\ngeneralized rank invariants in fixed classes of interval modules. In the paper\nwe develop the theory behind these rank invariant decompositions, showing under\nwhat conditions they exist and are unique -- so the signed barcode is\ncanonically defined. We also connect them to the line of work on generalized\npersistence diagrams via M\\\"obius inversions, deriving explicit formulas to\ncompute a rank decomposition and its associated signed barcode. Finally, we\nshow that, similarly to its unsigned counterpart, the signed barcode has its\nroots in algebra, coming from a projective resolution of the module in some\nexact category. To complete the picture, we show some experimental results that\nillustrate the contribution of the signed barcode in the exploration of\nmulti-parameter persistence modules.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 15:59:28 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Botnan", "Magnus Bakke", ""], ["Oppermann", "Steffen", ""], ["Oudot", "Steve", ""]]}, {"id": "2107.07358", "submitter": "Fabrizio Grandoni", "authors": "Fabrizio Grandoni, Rafail Ostrovsky, Yuval Rabani, Leonard J.\n  Schulman, Rakesh Venkat", "title": "A Refined Approximation for Euclidean k-Means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the Euclidean $k$-Means problem we are given a collection of $n$ points\n$D$ in an Euclidean space and a positive integer $k$. Our goal is to identify a\ncollection of $k$ points in the same space (centers) so as to minimize the sum\nof the squared Euclidean distances between each point in $D$ and the closest\ncenter. This problem is known to be APX-hard and the current best approximation\nratio is a primal-dual $6.357$ approximation based on a standard LP for the\nproblem [Ahmadian et al. FOCS'17, SICOMP'20].\n  In this note we show how a minor modification of Ahmadian et al.'s analysis\nleads to a slightly improved $6.12903$ approximation. As a related result, we\nalso show that the mentioned LP has integrality gap at least\n$\\frac{16+\\sqrt{5}}{15}>1.2157$.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 14:35:04 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Grandoni", "Fabrizio", ""], ["Ostrovsky", "Rafail", ""], ["Rabani", "Yuval", ""], ["Schulman", "Leonard J.", ""], ["Venkat", "Rakesh", ""]]}, {"id": "2107.07379", "submitter": "Gerasimos Arvanitis", "authors": "Gerasimos Arvanitis", "title": "Spectral Processing and Optimization of Static and Dynamic 3D Geometries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Geometry processing of 3D objects is of primary interest in many areas of\ncomputer vision and graphics, including robot navigation, 3D object\nrecognition, classification, feature extraction, etc. The recent introduction\nof cheap range sensors has created a great interest in many new areas, driving\nthe need for developing efficient algorithms for 3D object processing.\nPreviously, in order to capture a 3D object, expensive specialized sensors were\nused, such as lasers or dedicated range images, but now this limitation has\nchanged. The current approaches of 3D object processing require a significant\namount of manual intervention and they are still time-consuming making them\nunavailable for use in real-time applications. The aim of this thesis is to\npresent algorithms, mainly inspired by the spectral analysis, subspace\ntracking, etc, that can be used and facilitate many areas of low-level 3D\ngeometry processing (i.e., reconstruction, outliers removal, denoising,\ncompression), pattern recognition tasks (i.e., significant features extraction)\nand high-level applications (i.e., registration and identification of 3D\nobjects in partially scanned and cluttered scenes), taking into consideration\ndifferent types of 3D models (i.e., static and dynamic point clouds, static and\ndynamic 3D meshes).\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 15:09:20 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Arvanitis", "Gerasimos", ""]]}, {"id": "2107.07686", "submitter": "Amir M. Mirzendehdel", "authors": "Amir M. Mirzendehdel, Morad Behandish, and Saigopal Nelaturi", "title": "Optimizing Build Orientation for Support Removal using Multi-Axis\n  Machining", "comments": "Special issue on Computational Fabrication", "journal-ref": "Computers and Graphics (2021)", "doi": "10.1016/j.cag.2021.07.011", "report-no": null, "categories": "cs.CE cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Parts fabricated by additive manufacturing (AM) are often fabricated first as\na near-net shape, a combination of intended nominal geometry and sacrificial\nsupport structures, which need to be removed in a subsequent post-processing\nstage using subtractive manufacturing (SM). In this paper, we present a\nframework for optimizing the build orientation with respect to removability of\nsupport structures. In particular, given a general multi-axis machining setup\nand sampled build orientations, we define a Pareto-optimality criterion based\non the total support volume and the \"secluded\" support volume defined as the\nsupport volume that is not accessible by a given set of machining tools. Since\ntotal support volume mainly depends on the build orientation and the secluded\nvolume is dictated by the machining setup, in many cases the two objectives are\ncompeting and their trade-off needs to be taken into account. The accessibility\nanalysis relies on the inaccessibility measure field (IMF), which is a\ncontinuous field in the Euclidean space that quantifies the inaccessibility of\neach point given a collection of tools and fixturing devices. The value of IMF\nat each point indicates the minimum possible volumetric collision between\nobjects in relative motion including the part, fixtures, and the tools, over\nall possible tool orientations and sharp points on the tool. We also propose an\nautomated support removal planning algorithm based on IMF, where a sequence of\nactions are provided in terms of the fixturing devices, cutting tools, and tool\norientation at each step. In our approach, each step is chosen based on the\nmaximal removable volume to iteratively remove accessible supports. The\neffectiveness of the proposed approach is demonstrated through benchmark\nexamples in 2D and realistic examples in 3D.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 03:24:51 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Mirzendehdel", "Amir M.", ""], ["Behandish", "Morad", ""], ["Nelaturi", "Saigopal", ""]]}, {"id": "2107.07789", "submitter": "Julien Tierny", "authors": "Mathieu Pont, Jules Vidal, Julie Delon and Julien Tierny", "title": "Wasserstein Distances, Geodesics and Barycenters of Merge Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CG cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a unified computational framework for the estimation of\ndistances, geodesics and barycenters of merge trees. We extend recent work on\nthe edit distance [106] and introduce a new metric, called the Wasserstein\ndistance between merge trees, which is purposely designed to enable efficient\ncomputations of geodesics and barycenters. Specifically, our new distance is\nstrictly equivalent to the L2-Wasserstein distance between extremum persistence\ndiagrams, but it is restricted to a smaller solution space, namely, the space\nof rooted partial isomorphisms between branch decomposition trees. This enables\na simple extension of existing optimization frameworks [112] for geodesics and\nbarycenters from persistence diagrams to merge trees. We introduce a task-based\nalgorithm which can be generically applied to distance, geodesic, barycenter or\ncluster computation. The task-based nature of our approach enables further\naccelerations with shared-memory parallelism. Extensive experiments on public\nensembles and SciVis contest benchmarks demonstrate the efficiency of our\napproach -- with barycenter computations in the orders of minutes for the\nlargest examples -- as well as its qualitative ability to generate\nrepresentative barycenter merge trees, visually summarizing the features of\ninterest found in the ensemble. We show the utility of our contributions with\ndedicated visualization applications: feature tracking, temporal reduction and\nensemble clustering. We provide a lightweight C++ implementation that can be\nused to reproduce our results.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 09:27:49 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Pont", "Mathieu", ""], ["Vidal", "Jules", ""], ["Delon", "Julie", ""], ["Tierny", "Julien", ""]]}, {"id": "2107.07792", "submitter": "Andr\\'e Nusser", "authors": "Karl Bringmann, Anne Driemel, Andr\\'e Nusser, Ioannis Psarros", "title": "Tight Bounds for Approximate Near Neighbor Searching for Time Series\n  under the Fr\\'echet Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CC cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the $c$-approximate near neighbor problem under the continuous\nFr\\'echet distance: Given a set of $n$ polygonal curves with $m$ vertices, a\nradius $\\delta > 0$, and a parameter $k \\leq m$, we want to preprocess the\ncurves into a data structure that, given a query curve $q$ with $k$ vertices,\neither returns an input curve with Fr\\'echet distance at most $c\\cdot \\delta$\nto $q$, or returns that there exists no input curve with Fr\\'echet distance at\nmost $\\delta$ to $q$. We focus on the case where the input and the queries are\none-dimensional polygonal curves -- also called time series -- and we give a\ncomprehensive analysis for this case. We obtain new upper bounds that provide\ndifferent tradeoffs between approximation factor, preprocessing time, and query\ntime.\n  Our data structures improve upon the state of the art in several ways. We\nshow that for any $0 < \\varepsilon \\leq 1$ an approximation factor of\n$(1+\\varepsilon)$ can be achieved within the same asymptotic time bounds as the\npreviously best result for $(2+\\varepsilon)$. Moreover, we show that an\napproximation factor of $(2+\\varepsilon)$ can be obtained by using\npreprocessing time and space $O(nm)$, which is linear in the input size, and\nquery time in $O(\\frac{1}{\\varepsilon})^{k+2}$, where the previously best\nresult used preprocessing time in $n \\cdot O(\\frac{m}{\\varepsilon k})^k$ and\nquery time in $O(1)^k$. We complement our upper bounds with matching\nconditional lower bounds based on the Orthogonal Vectors Hypothesis.\nInterestingly, some of our lower bounds already hold for any super-constant\nvalue of $k$. This is achieved by proving hardness of a one-sided sparse\nversion of the Orthogonal Vectors problem as an intermediate problem, which we\nbelieve to be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 09:35:16 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Bringmann", "Karl", ""], ["Driemel", "Anne", ""], ["Nusser", "Andr\u00e9", ""], ["Psarros", "Ioannis", ""]]}, {"id": "2107.07914", "submitter": "Nirman Kumar", "authors": "Marzieh Eskandari and Bhavika B. Khare and Nirman Kumar", "title": "Separated Red Blue Center Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study a generalization of $k$-center clustering, first introduced by\nKavand et. al., where instead of one set of centers, we have two types of\ncenters, $p$ red and $q$ blue, and where each red center is at least $\\alpha$\ndistant from each blue center. The goal is to minimize the covering radius. We\nprovide an approximation algorithm for this problem, and a polynomial time\nalgorithm for the constrained problem, where all the centers must lie on a line\n$\\ell$.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 14:10:41 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Eskandari", "Marzieh", ""], ["Khare", "Bhavika B.", ""], ["Kumar", "Nirman", ""]]}, {"id": "2107.08175", "submitter": "Marzieh Eskandari Eskandari", "authors": "Marzieh Eskandari and Bahram Sadeghi Bigham", "title": "Lower Bound for Sculpture Garden Problem", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The purpose of the current study is to investigate a special case of art\ngallery problem, namely Sculpture Garden Problem. In the said problem, for a\ngiven polygon $P$, the ultimate goal is to place the minimum number of guards\nto define the interior polygon $P$ by applying a monotone Boolean formula\ncomposed of the guards.\n  As the findings indicate, the conjecture about the issue that in the worst\ncase, $n-2$ guards are required to describe any $n$-gon (Eppstein et al. 2007)\ncan be conclusively proved.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 03:47:04 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Eskandari", "Marzieh", ""], ["Bigham", "Bahram Sadeghi", ""]]}, {"id": "2107.08444", "submitter": "Shay Moran", "authors": "Noga Alon and Steve Hanneke and Ron Holzman and Shay Moran", "title": "A Theory of PAC Learnability of Partial Concept Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC cs.CG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We extend the theory of PAC learning in a way which allows to model a rich\nvariety of learning tasks where the data satisfy special properties that ease\nthe learning process. For example, tasks where the distance of the data from\nthe decision boundary is bounded away from zero. The basic and simple idea is\nto consider partial concepts: these are functions that can be undefined on\ncertain parts of the space. When learning a partial concept, we assume that the\nsource distribution is supported only on points where the partial concept is\ndefined.\n  This way, one can naturally express assumptions on the data such as lying on\na lower dimensional surface or margin conditions. In contrast, it is not at all\nclear that such assumptions can be expressed by the traditional PAC theory. In\nfact we exhibit easy-to-learn partial concept classes which provably cannot be\ncaptured by the traditional PAC theory. This also resolves a question posed by\nAttias, Kontorovich, and Mansour 2019.\n  We characterize PAC learnability of partial concept classes and reveal an\nalgorithmic landscape which is fundamentally different than the classical one.\nFor example, in the classical PAC model, learning boils down to Empirical Risk\nMinimization (ERM). In stark contrast, we show that the ERM principle fails in\nexplaining learnability of partial concept classes. In fact, we demonstrate\nclasses that are incredibly easy to learn, but such that any algorithm that\nlearns them must use an hypothesis space with unbounded VC dimension. We also\nfind that the sample compression conjecture fails in this setting.\n  Thus, this theory features problems that cannot be represented nor solved in\nthe traditional way. We view this as evidence that it might provide insights on\nthe nature of learnability in realistic scenarios which the classical theory\nfails to explain.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 13:29:26 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 19:25:35 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Alon", "Noga", ""], ["Hanneke", "Steve", ""], ["Holzman", "Ron", ""], ["Moran", "Shay", ""]]}, {"id": "2107.09188", "submitter": "Abigail Hickok", "authors": "Abigail Hickok, Deanna Needell, and Mason A. Porter", "title": "Analysis of Spatiotemporal Anomalies Using Persistent Homology: Case\n  Studies with COVID-19 Data", "comments": "31 pages; submitted; methods paper; keywords: topological data\n  analysis, persistent homology, spatiotemporal data, COVID-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.AT physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a method for analyzing spatiotemporal anomalies in geospatial data\nusing topological data analysis (TDA). To do this, we use persistent homology\n(PH), a tool from TDA that allows one to algorithmically detect geometric voids\nin a data set and quantify the persistence of these voids. We construct an\nefficient filtered simplicial complex (FSC) such that the voids in our FSC are\nin one-to-one correspondence with the anomalies. Our approach goes beyond\nsimply identifying anomalies; it also encodes information about the\nrelationships between anomalies. We use vineyards, which one can interpret as\ntime-varying persistence diagrams (an approach for visualizing PH), to track\nhow the locations of the anomalies change over time. We conduct two case\nstudies using spatially heterogeneous COVID-19 data. First, we examine\nvaccination rates in New York City by zip code. Second, we study a year-long\ndata set of COVID-19 case rates in neighborhoods in the city of Los Angeles.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 22:59:37 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 23:39:33 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Hickok", "Abigail", ""], ["Needell", "Deanna", ""], ["Porter", "Mason A.", ""]]}, {"id": "2107.09481", "submitter": "Sayan Bandyapadhyay", "authors": "Sayan Bandyapadhyay, Fedor V. Fomin, Petr A. Golovach, Nidhi Purohit,\n  Kirill Simonov", "title": "FPT Approximation for Fair Minimum-Load Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider the Minimum-Load $k$-Clustering/Facility Location\n(MLkC) problem where we are given a set $P$ of $n$ points in a metric space\nthat we have to cluster and an integer $k$ that denotes the number of clusters.\nAdditionally, we are given a set $F$ of cluster centers in the same metric\nspace. The goal is to select a set $C\\subseteq F$ of $k$ centers and assign\neach point in $P$ to a center in $C$, such that the maximum load over all\ncenters is minimized. Here the load of a center is the sum of the distances\nbetween it and the points assigned to it.\n  Although clustering/facility location problems have a rich literature, the\nminimum-load objective is not studied substantially, and hence MLkC has\nremained a poorly understood problem. More interestingly, the problem is\nnotoriously hard even in some special cases including the one in line metrics\nas shown by Ahmadian et al. [ACM Trans. Algo. 2018]. They also show\nAPX-hardness of the problem in the plane. On the other hand, the best-known\napproximation factor for MLkC is $O(k)$, even in the plane.\n  In this work, we study a fair version of MLkC inspired by the work of\nChierichetti et al. [NeurIPS, 2017], which generalizes MLkC. Here the input\npoints are colored by one of the $\\ell$ colors denoting the group they belong\nto. MLkC is the special case with $\\ell=1$. Considering this problem, we are\nable to obtain a $3$-approximation in $f(k,\\ell)\\cdot n^{O(1)}$ time. Also, our\nscheme leads to an improved $(1 + \\epsilon)$-approximation in case of Euclidean\nnorm, and in this case, the running time depends only polynomially on the\ndimension $d$. Our results imply the same approximations for MLkC with running\ntime $f(k)\\cdot n^{O(1)}$, achieving the first constant approximations for this\nproblem in general and Euclidean metric spaces.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 13:34:13 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Bandyapadhyay", "Sayan", ""], ["Fomin", "Fedor V.", ""], ["Golovach", "Petr A.", ""], ["Purohit", "Nidhi", ""], ["Simonov", "Kirill", ""]]}, {"id": "2107.09489", "submitter": "Victor Parque", "authors": "Victor Parque", "title": "On a Class of Polar Log-Aesthetic Curves", "comments": "Accepted for presentation in (and publication in the proceedings of)\n  The ASME 2021 International Design Engineering Technical Conferences &\n  Computers and Information in Engineering Conference (IDETC/CIE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CE cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Curves are essential concepts that enable compounded aesthetic curves, e.g.,\nto assemble complex silhouettes, match a specific curvature profile in\nindustrial design, and construct smooth, comfortable, and safe trajectories in\nvehicle-robot navigation systems. New mechanisms able to encode, generate,\nevaluate, and deform aesthetic curves are expected to improve the throughput\nand the quality of industrial design. In recent years, the study of (log)\naesthetic curves have attracted the community's attention due to its ubiquity\nin natural phenomena such as bird eggs, butterfly wings, falcon flights, and\nmanufactured products such as Japanese swords and automobiles.\n  A (log) aesthetic curve renders a logarithmic curvature graph approximated by\na straight line, and polar aesthetic curves enable to mode user-defined\ndynamics of the polar tangential angle in the polar coordinate system. As such,\nthe curvature profile often becomes a by-product of the tangential angle.\n  In this paper, we extend the concept of polar aesthetic curves and establish\nthe analytical formulations to construct aesthetic curves with user-defined\ncriteria. In particular, we propose the closed-form analytic characterizations\nof polar log-aesthetic curves meeting user-defined criteria of curvature\nprofiles and dynamics of polar tangential angles. We present numerical examples\nportraying the feasibility of rendering the logarithmic curvature graphs\nrepresented by a straight line. Our approach enables the seamless\ncharacterization of aesthetic curves in the polar coordinate system, which can\nmodel aesthetic shapes with desirable aesthetic curvature profiles.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 06:54:15 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Parque", "Victor", ""]]}, {"id": "2107.09821", "submitter": "Sima Hajiaghaei Shanjani", "authors": "Navid Assadian, Sima Hajiaghaei Shanjani, Alireza Zarei", "title": "Separating Colored Points with Minimum Number of Rectangles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper we study the following problem: Given $k$ disjoint sets of\npoints, $P_1, \\ldots, P_k$ on the plane, find a minimum cardinality set\n$\\mathcal{T}$ of arbitrary rectangles such that each rectangle contains points\nof just one set $P_i$ but not the others. We prove the NP-hardness of this\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 00:41:40 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Assadian", "Navid", ""], ["Shanjani", "Sima Hajiaghaei", ""], ["Zarei", "Alireza", ""]]}, {"id": "2107.09862", "submitter": "Bruno Benedetti", "authors": "Bruno Benedetti, Crystal Lai, Davide Lofano, and Frank H. Lutz", "title": "Random Simple-Homotopy Theory", "comments": "22 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.AT math.CO math.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We implement an algorithm RSHT (Random Simple-Homotopy) to study the\nsimple-homotopy types of simplicial complexes, with a particular focus on\ncontractible spaces and on finding substructures in higher-dimensional\ncomplexes. The algorithm combines elementary simplicial collapses with pure\nelementary expansions. For triangulated d-manifolds with d < 7, we show that\nRSHT reduces to (random) bistellar flips.\n  Among the many examples on which we test RSHT, we describe an explicit\n15-vertex triangulation of the Abalone, and more generally, (14k+1)-vertex\ntriangulations of Bing's houses with k rooms, which all can be deformed to a\npoint using only six pure elementary expansions.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 03:05:11 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Benedetti", "Bruno", ""], ["Lai", "Crystal", ""], ["Lofano", "Davide", ""], ["Lutz", "Frank H.", ""]]}, {"id": "2107.10339", "submitter": "Mitchell Black", "authors": "Mitchell Black and Amir Nayyeri", "title": "Finding surfaces in simplicial complexes with bounded-treewidth\n  1-skeleton", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem 2-Dim-Bounding-Surface. 2-Dim-Bounded-Surface asks\nwhether or not there is a subcomplex $S$ of a simplicial complex $K$\nhomeomorphic to a given compact, connected surface bounded by a given\nsubcomplex $B\\subset K$. 2-Dim-Bounding-Surface is NP-hard. We show it is\nfixed-parameter tractable with respect to the treewidth of the 1-skeleton of\nthe simplicial complex $K$. Using some of the techniques we developed for the\n2-Dim-Bounded-Surface problem, we obtain fixed parameter tractable algorithms\nfor other topological problems such as computing an optimal chain with a given\nboundary and computing an optimal chain in a given homology class.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 20:09:35 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Black", "Mitchell", ""], ["Nayyeri", "Amir", ""]]}, {"id": "2107.11265", "submitter": "Brian Hamilton", "authors": "Brian Hamilton", "title": "Generating $N$-point spherical configurations with low mesh ratios using\n  spherical area coordinates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short contribution presents a method for generating $N$-point spherical\nconfigurations with low mesh ratios. The method extends Caspar-Klug icosahedral\npoint-grids to non-icosahedral nets through the use of planar barycentric\ncoordinates, which are subsequently interpreted as spherical area coordinates\nfor spherical point sets. The proposed procedure may be applied iteratively and\nis parameterised by a sequence of integer pairs. For well-chosen input\nparameters, the proposed method is able to generate point sets with mesh ratios\nthat are lower than previously reported for $N<10^6$.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 14:25:30 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Hamilton", "Brian", ""]]}, {"id": "2107.12411", "submitter": "Marzieh Eskandari", "authors": "M. Eskandari, B. B. Khare, N. Kumar", "title": "Red blue $k$-center clustering with distance constraints", "comments": "12 pages. arXiv admin note: text overlap with arXiv:2107.07914", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We consider a variant of the $k$-center clustering problem in $\\Re^d$, where\nthe centers can be divided into two subsets, one, the red centers of size $p$,\nand the other, the blue centers of size $q$, where $p+q=k$, and such that each\nred center and each blue center must be apart a distance of at least some given\n$\\alpha \\geq 0$, with the aim of minimizing the covering radius. We provide a\nbi-criteria approximation algorithm for the problem and a polynomial time\nalgorithm for the constrained problem where all centers must lie on a given\nline $\\ell$.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 18:05:58 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Eskandari", "M.", ""], ["Khare", "B. B.", ""], ["Kumar", "N.", ""]]}, {"id": "2107.14079", "submitter": "Thomas Fernique", "authors": "Thomas Fernique", "title": "Density of Binary Disc Packings:Lower and Upper Bounds", "comments": "C++ code in ancillary files. Relies on results provided in\n  arXiv:2002.07168", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.MG cs.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We provide, for any $r\\in (0,1)$, lower and upper bounds on the maximal\ndensity of a packing in the Euclidean plane of discs of radius $1$ and $r$. The\nlower bounds are mostly folk, but the upper bounds improve the best previously\nknown ones for any $r\\in[0.11,0.74]$. For many values of $r$, this gives a\nfairly good idea of the exact maximum density. In particular, we get new\nintervals for $r$ which does not allow any packing more dense that the\nhexagonal packing of equal discs.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 15:11:29 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Fernique", "Thomas", ""]]}, {"id": "2107.14221", "submitter": "Lazar Milenkovic", "authors": "Omri Kahalon, Hung Le, Lazar Milenkovic, Shay Solomon", "title": "Can't See The Forest for the Trees: Navigating Metric Spaces by Bounded\n  Hop-Diameter Spanners", "comments": "Abstract truncated to fit arXiv limits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spanners for metric spaces have been extensively studied, both in general\nmetrics and in restricted classes, perhaps most notably in low-dimensional\nEuclidean spaces -- due to their numerous applications. Euclidean spanners can\nbe viewed as means of compressing the $\\binom{n}{2}$ pairwise distances of a\n$d$-dimensional Euclidean space into $O(n) = O_{\\epsilon,d}(n)$ spanner edges,\nso that the spanner distances preserve the original distances to within a\nfactor of $1+\\epsilon$, for any $\\epsilon > 0$. Moreover, one can compute such\nspanners in optimal $O(n \\log n)$ time. Once the spanner has been computed, it\nserves as a \"proxy\" overlay network, on which the computation can proceed,\nwhich gives rise to huge savings in space and other important quality measures.\n  On the negative side, by working on the spanner rather than the original\nmetric, one loses the key property of being able to efficiently \"navigate\"\nbetween pairs of points. While in the original metric, one can go from any\npoint to any other via a direct edge, it is unclear how to efficiently navigate\nin the spanner: How can we translate the existence of a \"good\" path into an\nefficient algorithm finding it? Moreover, usually by \"good\" path we mean a path\nwhose weight approximates the original distance between its endpoints -- but a\npriori the number of edges (or \"hops\") in the path could be huge. To control\nthe hop-length of paths, one can try to upper bound the spanner's hop-diameter,\nbut naturally bounded hop-diameter spanners are more complex than spanners with\nunbounded hop-diameter, which might render the algorithmic task of efficiently\nfinding good paths more challenging.\n  The original metric enables us to navigate optimally -- a single hop (for any\ntwo points) with the exact distance, but the price is high -- $\\Theta(n^2)$\nedges. [...]\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 17:54:30 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Kahalon", "Omri", ""], ["Le", "Hung", ""], ["Milenkovic", "Lazar", ""], ["Solomon", "Shay", ""]]}]