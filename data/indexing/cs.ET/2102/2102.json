[{"id": "2102.00371", "submitter": "Christopher Monroe", "authors": "S. Blinov, B. Wu, and C. Monroe", "title": "Comparison of Cloud-Based Ion Trap and Superconducting Quantum Computer\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum computing represents a radical departure from conventional approaches\nto information processing, offering the potential for solving problems that can\nnever be approached classically. While large scale quantum computer hardware is\nstill in development, several quantum computing systems have recently become\navailable as commercial cloud services. We compare the performance of these\nsystems on several simple quantum circuits and algorithms, and examine\ncomponent performance in the context of each system's architecture.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 04:00:48 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Blinov", "S.", ""], ["Wu", "B.", ""], ["Monroe", "C.", ""]]}, {"id": "2102.01153", "submitter": "Tirthak Patel", "authors": "Tirthak Patel and Devesh Tiwari", "title": "DisQ: A Novel Quantum Output State Classification Method on IBM Quantum\n  Computers using OpenPulse", "comments": null, "journal-ref": "In Proceedings of the 39th International Conference on\n  Computer-Aided Design (pp. 1-9) (2020)", "doi": null, "report-no": null, "categories": "quant-ph cs.ET", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Superconducting quantum computing technology has ushered in a new era of\ncomputational possibilities. While a considerable research effort has been\ngeared toward improving the quantum technology and building the software stack\nto efficiently execute quantum algorithms with reduced error rate, effort\ntoward optimizing how quantum output states are defined and classified for the\npurpose of reducing the error rate is still limited. To this end, this paper\nproposes DisQ, a quantum output state classification approach which reduces\nerror rates of quantum programs on NISQ devices.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 20:43:53 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Patel", "Tirthak", ""], ["Tiwari", "Devesh", ""]]}, {"id": "2102.01442", "submitter": "Guodong Yin", "authors": "Guodong Yin, Yi Cai, Juejian Wu, Zhengyang Duan, Zhenhua Zhu, Yongpan\n  Liu, Yu Wang, Huazhong Yang, and Xueqing Li", "title": "Enabling Lower-Power Charge-Domain Nonvolatile In-Memory Computing with\n  Ferroelectric FETs", "comments": "Accepted by IEEE Transactions on Circuits and Systems II in 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compute-in-memory (CiM) is a promising approach to alleviating the memory\nwall problem for domain-specific applications. Compared to current-domain CiM\nsolutions, charge-domain CiM shows the opportunity for higher energy efficiency\nand resistance to device variations. However, the area occupation and standby\nleakage power of existing SRAMbased charge-domain CiM (CD-CiM) are high. This\npaper proposes the first concept and analysis of CD-CiM using nonvolatile\nmemory (NVM) devices. The design implementation and performance evaluation are\nbased on a proposed 2-transistor-1-capacitor (2T1C) CiM macro using\nferroelectric field-effect-transistors (FeFETs), which is free from leakage\npower and much denser than the SRAM solution. With the supply voltage between\n0.45V and 0.90V, operating frequency between 100MHz to 1.0GHz, binary neural\nnetwork application simulations show over 47%, 60%, and 64% energy consumption\nreduction from existing SRAM-based CD-CiM, SRAM-based current-domain CiM, and\nRRAM-based current-domain CiM, respectively. For classifications in MNIST and\nCIFAR-10 data sets, the proposed FeFETbased CD-CiM achieves an accuracy over\n95% and 80%, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 11:32:21 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Yin", "Guodong", ""], ["Cai", "Yi", ""], ["Wu", "Juejian", ""], ["Duan", "Zhengyang", ""], ["Zhu", "Zhenhua", ""], ["Liu", "Yongpan", ""], ["Wang", "Yu", ""], ["Yang", "Huazhong", ""], ["Li", "Xueqing", ""]]}, {"id": "2102.01453", "submitter": "Panjin Kim Mr", "authors": "Panjin Kim and Daewan Han", "title": "Measurement-based Uncomputation Applied to Controlled Modular\n  Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.ET", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This is a brief report on a particular use of measurement-based\nuncomputation. Though not appealing in performance, it may shed light on\noptimization techniques in various quantum circuits.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 11:58:15 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Kim", "Panjin", ""], ["Han", "Daewan", ""]]}, {"id": "2102.02064", "submitter": "Hassnaa El-Derhalli", "authors": "Hassnaa El-Derhalli, Lea Constans, Sebastien Le Beux, Alfredo De\n  Rossi, Fabrice Raineri, Sofiene Tahar", "title": "Optical Stochastic Computing Architectures Using Photonic Crystal\n  Nanocavities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic computing allows a drastic reduction in hardware complexity using\nserial processing of bit streams. While the induced high computing latency can\nbe overcome using integrated optics technology, the design of realistic optical\nstochastic computing architectures calls for energy efficient switching\ndevices. Photonics Crystal (PhC) nanocavities are $\\mu m^2$ scale devices\noffering 100fJ switching operation under picoseconds-scale switching speed.\nFabrication process allows controlling the Quality factor of each nanocavity\nresonance, leading to opportunities to implement architectures involving\ncascaded gates and multi-wavelength signaling. In this report, we investigate\nthe design of cascaded gates architecture using nanocavities in the context of\nstochastic computing. We propose a transmission model considering key\nnanocavity device parameters, such as Quality factors, resonance wavelength and\nswitching efficiency. The model is calibrated with experimental measurements.\nWe propose the design of XOR gate and multiplexer. We illustrate the use of the\ngates to design an edge detection filter. System-level exploration of laser\npower, bit-stream length and bit-error rate is carried out for the processing\nof gray-scale images. The results show that the proposed architecture leads to\n8.5nJ/pixel energy consumption and 512ns/pixel processing time.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 13:42:58 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["El-Derhalli", "Hassnaa", ""], ["Constans", "Lea", ""], ["Beux", "Sebastien Le", ""], ["De Rossi", "Alfredo", ""], ["Raineri", "Fabrice", ""], ["Tahar", "Sofiene", ""]]}, {"id": "2102.03415", "submitter": "Piotr Rzeszut", "authors": "Piotr Rzeszut, Jakub Ch\\k{e}ci\\'nski, Ireneusz Brzozowski, S{\\l}awomir\n  Zi\\k{e}tek, Witold Skowro\\'nski and Tomasz Stobiecki", "title": "Multi-state MRAM cells for hardware neuromorphic computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET physics.app-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Magnetic tunnel junctions (MTJ) have been successfully applied in various\nsensing application and digital information storage technologies. Currently, a\nnumber of new potential applications of MTJs are being actively studied,\nincluding high-frequency electronics, energy harvesting or random number\ngenerators. Recently, MTJs have been also proposed in designs of a new\nplatforms for unconventional or bio-inspired computing. In the present work, it\nis shown that serially connected MTJs forming a multi-state memory cell can be\nused in a hardware implementation of a neural computing device. A behavioral\nmodel of the multi-cell is proposed based on the experimentally determined MTJ\nparameters. The main purpose of the mutli-cell is the formation of the\nquantized weights of the network, which can be programmed using the proposed\nelectronic circuit. Mutli-cells are connected to CMOS-based summing amplifier\nand sigmoid function generator, forming an artificial neuron. The operation of\nthe designed network is tested using a recognition of the hand-written digits\nin 20x20 pixel matrix and shows detection ratio comparable to the software\nalgorithm, using the weight stored in a multi-cell consisting of four MTJs or\nmore.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 20:37:24 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Rzeszut", "Piotr", ""], ["Ch\u0119ci\u0144ski", "Jakub", ""], ["Brzozowski", "Ireneusz", ""], ["Zi\u0119tek", "S\u0142awomir", ""], ["Skowro\u0144ski", "Witold", ""], ["Stobiecki", "Tomasz", ""]]}, {"id": "2102.03547", "submitter": "Yuan-Hang Zhang", "authors": "Yuan-Hang Zhang, Massimiliano Di Ventra", "title": "Directed percolation and numerical stability of simulations of digital\n  memcomputing machines", "comments": "12 pages, 11 figures", "journal-ref": null, "doi": "10.1063/5.0045375", "report-no": null, "categories": "cs.ET cond-mat.stat-mech cs.NE nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital memcomputing machines (DMMs) are a novel, non-Turing class of\nmachines designed to solve combinatorial optimization problems. They can be\nphysically realized with continuous-time, non-quantum dynamical systems with\nmemory (time non-locality), whose ordinary differential equations (ODEs) can be\nnumerically integrated on modern computers. Solutions of many hard problems\nhave been reported by numerically integrating the ODEs of DMMs, showing\nsubstantial advantages over state-of-the-art solvers. To investigate the\nreasons behind the robustness and effectiveness of this method, we employ three\nexplicit integration schemes (forward Euler, trapezoid and Runge-Kutta 4th\norder) with a constant time step, to solve 3-SAT instances with planted\nsolutions. We show that, (i) even if most of the trajectories in the phase\nspace are destroyed by numerical noise, the solution can still be achieved;\n(ii) the forward Euler method, although having the largest numerical error,\nsolves the instances in the least amount of function evaluations; and (iii)\nwhen increasing the integration time step, the system undergoes a\n\"solvable-unsolvable transition\" at a critical threshold, which needs to decay\nat most as a power law with the problem size, to control the numerical errors.\nTo explain these results, we model the dynamical behavior of DMMs as directed\npercolation of the state trajectory in the phase space in the presence of\nnoise. This viewpoint clarifies the reasons behind their numerical robustness\nand provides an analytical understanding of the unsolvable-solvable transition.\nThese results land further support to the usefulness of DMMs in the solution of\nhard combinatorial optimization problems.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 09:44:28 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 16:41:46 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zhang", "Yuan-Hang", ""], ["Di Ventra", "Massimiliano", ""]]}, {"id": "2102.04464", "submitter": "Arka Majumdar", "authors": "Albert Ryou, James Whitehead, Maksym Zhelyeznyakov, Paul Anderson, Cem\n  Keskin, Michal Bajcsy, and Arka Majumdar", "title": "Free-space optical neural network based on thermal atomic nonlinearity", "comments": null, "journal-ref": null, "doi": "10.1364/PRJ.415964", "report-no": null, "categories": "cs.ET physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As artificial neural networks (ANNs) continue to make strides in wide-ranging\nand diverse fields of technology, the search for more efficient hardware\nimplementations beyond conventional electronics is gaining traction. In\nparticular, optical implementations potentially offer extraordinary gains in\nterms of speed and reduced energy consumption due to intrinsic parallelism of\nfree-space optics. At the same time, a physical nonlinearity, a crucial\ningredient of an ANN, is not easy to realize in free-space optics, which\nrestricts the potential of this platform. This problem is further exacerbated\nby the need to perform the nonlinear activation also in parallel for each data\npoint to preserve the benefit of linear free-space optics. Here, we present a\nfree-space optical ANN with diffraction-based linear weight summation and\nnonlinear activation enabled by the saturable absorption of thermal atoms. We\ndemonstrate, via both simulation and experiment, image classification of\nhandwritten digits using only a single layer and observed 6-percent improvement\nin classification accuracy due to the optical nonlinearity compared to a linear\nmodel. Our platform preserves the massive parallelism of free-space optics even\nwith physical nonlinearity, and thus opens the way for novel designs and wider\ndeployment of optical ANNs.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 17:52:00 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Ryou", "Albert", ""], ["Whitehead", "James", ""], ["Zhelyeznyakov", "Maksym", ""], ["Anderson", "Paul", ""], ["Keskin", "Cem", ""], ["Bajcsy", "Michal", ""], ["Majumdar", "Arka", ""]]}, {"id": "2102.05137", "submitter": "Jan Kaiser", "authors": "Jan Kaiser, William A. Borders, Kerem Y. Camsari, Shunsuke Fukami,\n  Hideo Ohno, and Supriyo Datta", "title": "Hardware-aware in-situ Boltzmann machine learning using stochastic\n  magnetic tunnel junctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mes-hall cond-mat.dis-nn cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the big challenges of current electronics is the design and\nimplementation of hardware neural networks that perform fast and\nenergy-efficient machine learning. Spintronics is a promising catalyst for this\nfield with the capabilities of nanosecond operation and compatibility with\nexisting microelectronics. Considering large-scale, viable neuromorphic systems\nhowever, variability of device properties is a serious concern. In this paper,\nwe show an autonomously operating circuit that performs hardware-aware machine\nlearning utilizing probabilistic neurons built with stochastic magnetic tunnel\njunctions. We show that in-situ learning of weights and biases in a Boltzmann\nmachine can counter device-to-device variations and learn the probability\ndistribution of meaningful operations such as a full adder. This scalable\nautonomously operating learning circuit using spintronics-based neurons could\nbe especially of interest for standalone artificial-intelligence devices\ncapable of fast and efficient learning at the edge.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 21:26:21 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Kaiser", "Jan", ""], ["Borders", "William A.", ""], ["Camsari", "Kerem Y.", ""], ["Fukami", "Shunsuke", ""], ["Ohno", "Hideo", ""], ["Datta", "Supriyo", ""]]}, {"id": "2102.05271", "submitter": "Vinay Joshi", "authors": "Vinay Joshi, Wangxin He, Jae-sun Seo and Bipin Rajendran", "title": "Hybrid In-memory Computing Architecture for the Training of Deep Neural\n  Networks", "comments": "Accepted at ISCAS 2021 for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cost involved in training deep neural networks (DNNs) on von-Neumann\narchitectures has motivated the development of novel solutions for efficient\nDNN training accelerators. We propose a hybrid in-memory computing (HIC)\narchitecture for the training of DNNs on hardware accelerators that results in\nmemory-efficient inference and outperforms baseline software accuracy in\nbenchmark tasks. We introduce a weight representation technique that exploits\nboth binary and multi-level phase-change memory (PCM) devices, and this leads\nto a memory-efficient inference accelerator. Unlike previous in-memory\ncomputing-based implementations, we use a low precision weight update\naccumulator that results in more memory savings. We trained the ResNet-32\nnetwork to classify CIFAR-10 images using HIC. For a comparable model size,\nHIC-based training outperforms baseline network, trained in floating-point\n32-bit (FP32) precision, by leveraging appropriate network width multiplier.\nFurthermore, we observe that HIC-based training results in about 50% less\ninference model size to achieve baseline comparable accuracy. We also show that\nthe temporal drift in PCM devices has a negligible effect on post-training\ninference accuracy for extended periods (year). Finally, our simulations\nindicate HIC-based training naturally ensures that the number of write-erase\ncycles seen by the devices is a small fraction of the endurance limit of PCM,\ndemonstrating the feasibility of this architecture for achieving hardware\nplatforms that can learn in the field.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 05:26:27 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Joshi", "Vinay", ""], ["He", "Wangxin", ""], ["Seo", "Jae-sun", ""], ["Rajendran", "Bipin", ""]]}, {"id": "2102.06365", "submitter": "Sahaj Garg", "authors": "Sahaj Garg, Joe Lou, Anirudh Jain, Mitchell Nahmias", "title": "Dynamic Precision Analog Computing for Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analog electronic and optical computing exhibit tremendous advantages over\ndigital computing for accelerating deep learning when operations are executed\nat low precision. In this work, we derive a relationship between analog\nprecision, which is limited by noise, and digital bit precision. We propose\nextending analog computing architectures to support varying levels of precision\nby repeating operations and averaging the result, decreasing the impact of\nnoise. Such architectures enable programmable tradeoffs between precision and\nother desirable performance metrics such as energy efficiency or throughput. To\nutilize dynamic precision, we propose a method for learning the precision of\neach layer of a pre-trained model without retraining network weights. We\nevaluate this method on analog architectures subject to a variety of noise\nsources such as shot noise, thermal noise, and weight noise and find that\nemploying dynamic precision reduces energy consumption by up to 89% for\ncomputer vision models such as Resnet50 and by 24% for natural language\nprocessing models such as BERT. In one example, we apply dynamic precision to a\nshot-noise limited homodyne optical neural network and simulate inference at an\noptical energy consumption of 2.7 aJ/MAC for Resnet50 and 1.6 aJ/MAC for BERT\nwith <2% accuracy degradation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 06:56:56 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Garg", "Sahaj", ""], ["Lou", "Joe", ""], ["Jain", "Anirudh", ""], ["Nahmias", "Mitchell", ""]]}, {"id": "2102.06960", "submitter": "Sudeep Pasricha", "authors": "Febin Sunny, Asif Mirza, Mahdi Nikdast, and Sudeep Pasricha", "title": "CrossLight: A Cross-Layer Optimized Silicon Photonic Neural Network\n  Accelerator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.ET cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Domain-specific neural network accelerators have seen growing interest in\nrecent years due to their improved energy efficiency and inference performance\ncompared to CPUs and GPUs. In this paper, we propose a novel cross-layer\noptimized neural network accelerator called CrossLight that leverages silicon\nphotonics. CrossLight includes device-level engineering for resilience to\nprocess variations and thermal crosstalk, circuit-level tuning enhancements for\ninference latency reduction, and architecture-level optimization to enable\nhigher resolution, better energy-efficiency, and improved throughput. On\naverage, CrossLight offers 9.5x lower energy-per-bit and 15.9x higher\nperformance-per-watt at 16-bit resolution than state-of-the-art photonic deep\nlearning accelerators.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 17:08:06 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Sunny", "Febin", ""], ["Mirza", "Asif", ""], ["Nikdast", "Mahdi", ""], ["Pasricha", "Sudeep", ""]]}, {"id": "2102.07260", "submitter": "Yigit Demirag", "authors": "Yigit Demirag, Filippo Moro, Thomas Dalgaty, Gabriele Navarro,\n  Charlotte Frenkel, Giacomo Indiveri, Elisa Vianello, Melika Payvand", "title": "PCM-trace: Scalable Synaptic Eligibility Traces with Resistivity Drift\n  of Phase-Change Materials", "comments": "Typos are fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dedicated hardware implementations of spiking neural networks that combine\nthe advantages of mixed-signal neuromorphic circuits with those of emerging\nmemory technologies have the potential of enabling ultra-low power pervasive\nsensory processing. To endow these systems with additional flexibility and the\nability to learn to solve specific tasks, it is important to develop\nappropriate on-chip learning mechanisms.Recently, a new class of three-factor\nspike-based learning rules have been proposed that can solve the temporal\ncredit assignment problem and approximate the error back-propagation algorithm\non complex tasks. However, the efficient implementation of these rules on\nhybrid CMOS/memristive architectures is still an open challenge. Here we\npresent a new neuromorphic building block,called PCM-trace, which exploits the\ndrift behavior of phase-change materials to implement long lasting eligibility\ntraces, a critical ingredient of three-factor learning rules. We demonstrate\nhow the proposed approach improves the area efficiency by >10X compared to\nexisting solutions and demonstrates a techno-logically plausible learning\nalgorithm supported by experimental data from device measurements\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 22:35:22 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 09:32:53 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Demirag", "Yigit", ""], ["Moro", "Filippo", ""], ["Dalgaty", "Thomas", ""], ["Navarro", "Gabriele", ""], ["Frenkel", "Charlotte", ""], ["Indiveri", "Giacomo", ""], ["Vianello", "Elisa", ""], ["Payvand", "Melika", ""]]}, {"id": "2102.07268", "submitter": "Sven K\\\"oppel", "authors": "Sven K\\\"oppel, Bernd Ulmann, Lars Heimann, Dirk Killat", "title": "Using analog computers in today's largest computational challenges", "comments": "12 pages, 2 figures, accepted paper towards a Special Issue of the\n  open-access journal \"Advances in Radio Science\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.ET cs.NA math.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Analog computers can be revived as a feasible technology platform for low\nprecision, energy efficient and fast computing. We justify this statement by\nmeasuring the performance of a modern analog computer and comparing it with\nthat of traditional digital processors. General statements are made about the\nsolution of ordinary and partial differential equations. Computational fluid\ndynamics are discussed as an example of large scale scientific computing\napplications. Several models are proposed which demonstrate the benefits of\nanalog and digital-analog hybrid computing.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 23:34:35 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 09:04:55 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["K\u00f6ppel", "Sven", ""], ["Ulmann", "Bernd", ""], ["Heimann", "Lars", ""], ["Killat", "Dirk", ""]]}, {"id": "2102.07959", "submitter": "Aqeeb Iqbal Arka", "authors": "Aqeeb Iqbal Arka, Biresh Kumar Joardar, Janardhan Rao Doppa, Partha\n  Pratim Pande, Krishnendu Chakrabarty", "title": "ReGraphX: NoC-enabled 3D Heterogeneous ReRAM Architecture for Training\n  Graph Neural Networks", "comments": "This paper has been accepted and presented at Design Automation and\n  Test in Europe (DATE) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph Neural Network (GNN) is a variant of Deep Neural Networks (DNNs)\noperating on graphs. However, GNNs are more complex compared to traditional\nDNNs as they simultaneously exhibit features of both DNN and graph\napplications. As a result, architectures specifically optimized for either DNNs\nor graph applications are not suited for GNN training. In this work, we propose\na 3D heterogeneous manycore architecture for on-chip GNN training to address\nthis problem. The proposed architecture, ReGraphX, involves heterogeneous ReRAM\ncrossbars to fulfill the disparate requirements of both DNN and graph\ncomputations simultaneously. The ReRAM-based architecture is complemented with\na multicast-enabled 3D NoC to improve the overall achievable performance. We\ndemonstrate that ReGraphX outperforms conventional GPUs by up to 3.5X (on an\naverage 3X) in terms of execution time, while reducing energy consumption by as\nmuch as 11X.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 04:59:17 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Arka", "Aqeeb Iqbal", ""], ["Joardar", "Biresh Kumar", ""], ["Doppa", "Janardhan Rao", ""], ["Pande", "Partha Pratim", ""], ["Chakrabarty", "Krishnendu", ""]]}, {"id": "2102.08451", "submitter": "Casey Duckering", "authors": "Casey Duckering and Jonathan M. Baker and Andrew Litteken and Frederic\n  T. Chong", "title": "Orchestrated Trios: Compiling for Efficient Communication in Quantum\n  Programs with 3-Qubit Gates", "comments": "In ASPLOS '21: 26th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems, 12 pages, 12 figures", "journal-ref": null, "doi": "10.1145/3445814.3446718", "report-no": null, "categories": "quant-ph cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current quantum computers are especially error prone and require high levels\nof optimization to reduce operation counts and maximize the probability the\ncompiled program will succeed. These computers only support operations\ndecomposed into one- and two-qubit gates and only two-qubit gates between\nphysically connected pairs of qubits. Typical compilers first decompose\noperations, then route data to connected qubits. We propose a new compiler\nstructure, Orchestrated Trios, that first decomposes to the three-qubit\nToffoli, routes the inputs of the higher-level Toffoli operations to groups of\nnearby qubits, then finishes decomposition to hardware-supported gates.\n  This significantly reduces communication overhead by giving the routing pass\naccess to the higher-level structure of the circuit instead of discarding it. A\nsecond benefit is the ability to now select an architecture-tuned Toffoli\ndecomposition such as the 8-CNOT Toffoli for the specific hardware qubits now\nknown after the routing pass. We perform real experiments on IBM Johannesburg\nshowing an average 35% decrease in two-qubit gate count and 23% increase in\nsuccess rate of a single Toffoli over Qiskit. We additionally compile many\nnear-term benchmark algorithms showing an average 344% increase in (or 4.44x)\nsimulated success rate on the Johannesburg architecture and compare with other\narchitecture types.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 21:06:58 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Duckering", "Casey", ""], ["Baker", "Jonathan M.", ""], ["Litteken", "Andrew", ""], ["Chong", "Frederic T.", ""]]}, {"id": "2102.08555", "submitter": "Corey Lammie", "authors": "Corey Lammie, Wei Xiang, Mostafa Rahimi Azghadi", "title": "Towards Memristive Deep Learning Systems for Real-time Mobile Epileptic\n  Seizure Prediction", "comments": "Accepted at 2021 IEEE International Symposium on Circuits and Systems\n  (ISCAS)", "journal-ref": "2021 IEEE International Symposium on Circuits and Systems (ISCAS)", "doi": null, "report-no": null, "categories": "cs.ET cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unpredictability of seizures continues to distress many people with\ndrug-resistant epilepsy. On account of recent technological advances,\nconsiderable efforts have been made using different hardware technologies to\nrealize smart devices for the real-time detection and prediction of seizures.\nIn this paper, we investigate the feasibility of using Memristive Deep Learning\nSystems (MDLSs) to perform real-time epileptic seizure prediction on the edge.\nUsing the MemTorch simulation framework and the Children's Hospital Boston\n(CHB)-Massachusetts Institute of Technology (MIT) dataset we determine the\nperformance of various simulated MDLS configurations. An average sensitivity of\n77.4% and a Area Under the Receiver Operating Characteristic Curve (AUROC) of\n0.85 are reported for the optimal configuration that can process\nElectroencephalogram (EEG) spectrograms with 7,680 samples in 1.408ms while\nconsuming 0.0133W and occupying an area of 0.1269mm$^2$ in a 65nm Complementary\nMetal-Oxide-Semiconductor (CMOS) process.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 03:49:25 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Lammie", "Corey", ""], ["Xiang", "Wei", ""], ["Azghadi", "Mostafa Rahimi", ""]]}, {"id": "2102.08739", "submitter": "Qingqing Wu", "authors": "Qingqing Wu, Xiaobo Zhou, Robert Schober", "title": "IRS-Assisted Wireless Powered NOMA: Do We Really Need Different Phase\n  Shifts in DL and UL?", "comments": "We unveiled in the paper that dynamic IRS beamforming is not needed\n  for wireless powered communication networks (WPCNs) employing NOMA. The TDMA\n  based IRS-aided WPCN is ready soon as a sister work. My online tutorials for\n  more information on IRS. 1)\n  https://www.bilibili.com/video/BV1wZ4y1N7J4?from=search&seid=17353402214239402263\n  2) https://www.youtube.com/watch?v=J9CpUIdpfeo&t=306s", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AR cs.ET cs.NI math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intelligent reflecting surface (IRS) is a promising technology to improve the\nperformance of wireless powered communication networks (WPCNs) due to its\ncapability to reconfigure signal propagation environments via smart reflection.\nIn particular, the high passive beamforming gain promised by IRS can\nsignificantly enhance the efficiency of both downlink wireless power transfer\n(DL WPT) and uplink wireless information transmission (UL WIT) in WPCNs.\nAlthough adopting different IRS phase shifts for DL WPT and UL WIT, i.e.,\ndynamic IRS beamforming, is in principle possible but incurs additional\nsignaling overhead and computational complexity, it is an open problem whether\nit is actually beneficial. To answer this question, we consider an IRS-assisted\nWPCN where multiple devices employ a hybrid access point (HAP) to first harvest\nenergy and then transmit information using non-orthogonal multiple access\n(NOMA). Specifically, we aim to maximize the sum throughput of all devices by\njointly optimizing the IRS phase shifts and the resource allocation. To this\nend, we first prove that dynamic IRS beamforming is not needed for the\nconsidered system, which helps reduce the number of IRS phase shifts to be\noptimized. Then, we propose both joint and alternating optimization based\nalgorithms to solve the resulting problem. Simulation results demonstrate the\neffectiveness of our proposed designs over benchmark schemes and also provide\nuseful insights into the importance of IRS for realizing spectrally and energy\nefficient WPCNs.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 13:06:08 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2021 02:28:47 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2021 04:09:37 GMT"}, {"version": "v4", "created": "Mon, 15 Mar 2021 11:21:51 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Wu", "Qingqing", ""], ["Zhou", "Xiaobo", ""], ["Schober", "Robert", ""]]}, {"id": "2102.09200", "submitter": "Harideep Nair", "authors": "Shreyas Chaudhari, Harideep Nair, Jos\\'e M.F. Moura and John Paul Shen", "title": "Unsupervised Clustering of Time Series Signals using Neuromorphic\n  Energy-Efficient Temporal Neural Networks", "comments": "Accepted for publication at ICASSP 2021", "journal-ref": null, "doi": "10.1109/ICASSP39728.2021.9414882", "report-no": null, "categories": "cs.LG cs.AI cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised time series clustering is a challenging problem with diverse\nindustrial applications such as anomaly detection, bio-wearables, etc. These\napplications typically involve small, low-power devices on the edge that\ncollect and process real-time sensory signals. State-of-the-art time-series\nclustering methods perform some form of loss minimization that is extremely\ncomputationally intensive from the perspective of edge devices. In this work,\nwe propose a neuromorphic approach to unsupervised time series clustering based\non Temporal Neural Networks that is capable of ultra low-power, continuous\nonline learning. We demonstrate its clustering performance on a subset of UCR\nTime Series Archive datasets. Our results show that the proposed approach\neither outperforms or performs similarly to most of the existing algorithms\nwhile being far more amenable for efficient hardware implementation. Our\nhardware assessment analysis shows that in 7 nm CMOS the proposed architecture,\non average, consumes only about 0.005 mm^2 die area and 22 uW power and can\nprocess each signal with about 5 ns latency.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 07:47:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chaudhari", "Shreyas", ""], ["Nair", "Harideep", ""], ["Moura", "Jos\u00e9 M. F.", ""], ["Shen", "John Paul", ""]]}, {"id": "2102.09360", "submitter": "Wolfram Pernice", "authors": "J. Feldmann, N. Youngblood, C.D. Wright, H. Bhaskaran and W.H.P.\n  Pernice", "title": "All-optical spiking neurosynaptic networks with self-learning\n  capabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Software-implementation, via neural networks, of brain-inspired computing\napproaches underlie many important modern-day computational tasks, from image\nprocessing to speech recognition, artificial intelligence and deep learning\napplications. Yet, differing from real neural tissue, traditional computing\narchitectures physically separate the core computing functions of memory and\nprocessing, making fast, efficient and low-energy brain-like computing\ndifficult to achieve. To overcome such limitations, an attractive and\nalternative goal is to design direct hardware mimics of brain neurons and\nsynapses which, when connected in appropriate networks (or neuromorphic\nsystems), process information in a way more fundamentally analogous to that of\nreal brains. Here we present an all-optical approach to achieving such a goal.\nSpecifically, we demonstrate an all-optical spiking neuron device and connect\nit, via an integrated photonics network, to photonic synapses to deliver a\nsmall-scale all-optical neurosynaptic system capable of supervised and\nunsupervised learning. Moreover, we exploit wavelength division multiplexing\ntechniques to implement a scalable circuit architecture for photonic neural\nnetworks, successfully demonstrating pattern recognition directly in the\noptical domain using a photonic system comprising 140 elements. Such optical\nimplementations of neurosynaptic networks promise access to the high speed and\nbandwidth inherent to optical systems, which would be very attractive for the\ndirect processing of telecommunication and visual data in the optical domain.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 14:12:24 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Feldmann", "J.", ""], ["Youngblood", "N.", ""], ["Wright", "C. D.", ""], ["Bhaskaran", "H.", ""], ["Pernice", "W. H. P.", ""]]}, {"id": "2102.09561", "submitter": "Wenjia Zhang", "authors": "Yue Jiang, Wenjia Zhang, Fan Yang and Zuyuan He", "title": "Photonic Convolution Neural Network Based on Interleaved Time-Wavelength\n  Modulation", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET physics.optics", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Convolution neural network (CNN), as one of the most powerful and popular\ntechnologies, has achieved remarkable progress for image and video\nclassification since its invention in 1989. However, with the high definition\nvideo-data explosion, convolution layers in the CNN architecture will occupy a\ngreat amount of computing time and memory resources due to high computation\ncomplexity of matrix multiply accumulate operation. In this paper, a novel\nintegrated photonic CNN is proposed based on double correlation operations\nthrough interleaved time-wavelength modulation. Micro-ring based\nmulti-wavelength manipulation and single dispersion medium are utilized to\nrealize convolution operation and replace the conventional optical delay lines.\n200 images are tested in MNIST datasets with accuracy of 85.5% in our photonic\nCNN versus 86.5% in 64-bit computer.We also analyze the computing error of\nphotonic CNN caused by various micro-ring parameters, operation baud rates and\nthe characteristics of micro-ring weighting bank. Furthermore, a tensor\nprocessing unit based on 4x4 mesh with 1.2 TOPS (operation per second when 100%\nutilization) computing capability at 20G baud rate is proposed and analyzed to\nform a paralleled photonic CNN.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 04:17:35 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Jiang", "Yue", ""], ["Zhang", "Wenjia", ""], ["Yang", "Fan", ""], ["He", "Zuyuan", ""]]}, {"id": "2102.09630", "submitter": "Renate Krause", "authors": "Elisa Donati, Renate Krause and Giacomo Indiveri", "title": "Neuromorphic Pattern Generation Circuits for Bioelectronic Medicine", "comments": "4 pages; 7 figures; to be published in 10th International IEEE EMBS\n  Conference On Neural Engineering (NER'21), Special Session on Bioelectronics\n  medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chronic diseases can greatly benefit from bioelectronic medicine approaches.\nNeuromorphic electronic circuits present ideal characteristics for the\ndevelopment of brain-inspired low-power implantable processing systems that can\nbe interfaced with biological systems. These circuits, therefore, represent a\npromising additional tool in the tool-set of bioelectronic medicine. In this\npaper, we describe the main features of neuromorphic circuits that are ideally\nsuited for continuously monitoring the physiological parameters of the body and\ninteract with them in real-time. We propose examples of computational\nprimitives that can be used for real-time pattern generation and present a\nneuromorphic implementation of neural oscillators for the generation of\nsequence activation patterns. We demonstrate the features of such systems with\nan implementation of a three-phase network that models the dynamics of the\nrespiratory Central Pattern Generator (CPG) and the heart chambers rhythm, and\nthat could be used to build an adaptive pacemaker.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 21:46:05 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Donati", "Elisa", ""], ["Krause", "Renate", ""], ["Indiveri", "Giacomo", ""]]}, {"id": "2102.09722", "submitter": "Ying Zuo", "authors": "Ying Zuo, Zhao Yujun, You-Chiuan Chen, Shengwang Du and Junwei Liu", "title": "Scalability of all-optical neural networks based on spatial light\n  modulators", "comments": null, "journal-ref": "Phys. Rev. Applied 15, 054034 (2021)", "doi": "10.1103/PhysRevApplied.15.054034", "report-no": null, "categories": "physics.optics cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical implementation of artificial neural networks has been attracting\ngreat attention due to its potential in parallel computation at speed of light.\nAlthough all-optical deep neural networks (AODNNs) with a few neurons have been\nexperimentally demonstrated with acceptable errors recently, the feasibility of\nlarge scale AODNNs remains unknown because error might accumulate inevitably\nwith increasing number of neurons and connections. Here, we demonstrate a\nscalable AODNN with programmable linear operations and tunable nonlinear\nactivation functions. We verify its scalability by measuring and analyzing\nerrors propagating from a single neuron to the entire network. The feasibility\nof AODNNs is further confirmed by recognizing handwritten digits and fashions\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 03:43:18 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Zuo", "Ying", ""], ["Yujun", "Zhao", ""], ["Chen", "You-Chiuan", ""], ["Du", "Shengwang", ""], ["Liu", "Junwei", ""]]}, {"id": "2102.10140", "submitter": "Sudeep Pasricha", "authors": "D. Dang, S. V. R. Chittamuru, S. Pasricha, R. Mahapatra, D. Sahoo", "title": "BPLight-CNN: A Photonics-based Backpropagation Accelerator for Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": "epic-21-01", "categories": "cs.LG cs.AR cs.ET", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Training deep learning networks involves continuous weight updates across the\nvarious layers of the deep network while using a backpropagation algorithm\n(BP). This results in expensive computation overheads during training.\nConsequently, most deep learning accelerators today employ pre-trained weights\nand focus only on improving the design of the inference phase. The recent trend\nis to build a complete deep learning accelerator by incorporating the training\nmodule. Such efforts require an ultra-fast chip architecture for executing the\nBP algorithm. In this article, we propose a novel photonics-based\nbackpropagation accelerator for high performance deep learning training. We\npresent the design for a convolutional neural network, BPLight-CNN, which\nincorporates the silicon photonics-based backpropagation accelerator.\nBPLight-CNN is a first-of-its-kind photonic and memristor-based CNN\narchitecture for end-to-end training and prediction. We evaluate BPLight-CNN\nusing a photonic CAD framework (IPKISS) on deep learning benchmark models\nincluding LeNet and VGG-Net. The proposed design achieves (i) at least 34x\nspeedup, 34x improvement in computational efficiency, and 38.5x energy savings,\nduring training; and (ii) 29x speedup, 31x improvement in computational\nefficiency, and 38.7x improvement in energy savings, during inference compared\nto the state-of-the-art designs. All these comparisons are done at a 16-bit\nresolution; and BPLight-CNN achieves these improvements at a cost of\napproximately 6% lower accuracy compared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 20:00:21 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Dang", "D.", ""], ["Chittamuru", "S. V. R.", ""], ["Pasricha", "S.", ""], ["Mahapatra", "R.", ""], ["Sahoo", "D.", ""]]}, {"id": "2102.10398", "submitter": "Volker Sorger", "authors": "Ting Yu, Xiaoxuan Ma, Ernest Pastor, Jonathan K. George, Simon Wall,\n  Mario Miscuglio, Robert E. Simpson, Volker J. Sorger", "title": "All-Chalcogenide Programmable All-Optical Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.optics cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deeplearning algorithms are revolutionising many aspects of modern life.\nTypically, they are implemented in CMOS-based hardware with severely limited\nmemory access times and inefficient data-routing. All-optical neural networks\nwithout any electro-optic conversions could alleviate these shortcomings.\nHowever, an all-optical nonlinear activation function, which is a vital\nbuilding block for optical neural networks, needs to be developed efficiently\non-chip. Here, we introduce and demonstrate both optical synapse weighting and\nall-optical nonlinear thresholding using two different effects in a\nchalcogenide material photonic platform. We show how the structural phase\ntransitions in a wide-bandgap phase-change material enables storing the neural\nnetwork weights via non-volatile photonic memory, whilst resonant bond\ndestabilisation is used as a nonlinear activation threshold without changing\nthe material. These two different transitions within chalcogenides enable\nprogrammable neural networks with near-zero static power consumption once\ntrained, in addition to picosecond delays performing inference tasks not\nlimited by wire charging that limit electrical circuits; for instance, we show\nthat nanosecond-order weight programming and near-instantaneous weight updates\nenable accurate inference tasks within 20 picoseconds in a 3-layer all-optical\nneural network. Optical neural networks that bypass electro-optic conversion\naltogether hold promise for network-edge machine learning applications where\ndecision-making in real-time are critical, such as for autonomous vehicles or\nnavigation systems such as signal pre-processing of LIDAR systems.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 17:35:23 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 15:31:23 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 00:47:42 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Yu", "Ting", ""], ["Ma", "Xiaoxuan", ""], ["Pastor", "Ernest", ""], ["George", "Jonathan K.", ""], ["Wall", "Simon", ""], ["Miscuglio", "Mario", ""], ["Simpson", "Robert E.", ""], ["Sorger", "Volker J.", ""]]}, {"id": "2102.10648", "submitter": "Herbert Jaeger", "authors": "Herbert Jaeger, Dirk Doorakkers, Celestine Lawrence, Giacomo Indiveri", "title": "Dimensions of Timescales in Neuromorphic Computing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This article is a public deliverable of the EU project \"Memory technologies\nwith multi-scale time constants for neuromorphic architectures\" (MeMScales,\nhttps://memscales.eu, Call ICT-06-2019 Unconventional Nanoelectronics, project\nnumber 871371). This arXiv version is a verbatim copy of the deliverable\nreport, with administrative information stripped. It collects a wide and varied\nassortment of phenomena, models, research themes and algorithmic techniques\nthat are connected with timescale phenomena in the fields of computational\nneuroscience, mathematics, machine learning and computer science, with a bias\ntoward aspects that are relevant for neuromorphic engineering. It turns out\nthat this theme is very rich indeed and spreads out in many directions which\ndefy a unified treatment. We collected several dozens of sub-themes, each of\nwhich has been investigated in specialized settings (in the neurosciences,\nmathematics, computer science and machine learning) and has been documented in\nits own body of literature. The more we dived into this diversity, the more it\nbecame clear that our first effort to compose a survey must remain sketchy and\npartial. We conclude with a list of insights distilled from this survey which\ngive general guidelines for the design of future neuromorphic systems.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 17:15:43 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Jaeger", "Herbert", ""], ["Doorakkers", "Dirk", ""], ["Lawrence", "Celestine", ""], ["Indiveri", "Giacomo", ""]]}, {"id": "2102.11764", "submitter": "Mohammad-Ali Javidian", "authors": "Mohammad Ali Javidian, Vaneet Aggarwal, Fanglin Bao, Zubin Jacob", "title": "Quantum Entropic Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AI cs.IT math.IT quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As quantum computing and networking nodes scale-up, important open questions\narise on the causal influence of various sub-systems on the total system\nperformance. These questions are related to the tomographic reconstruction of\nthe macroscopic wavefunction and optimizing connectivity of large engineered\nqubit systems, the reliable broadcasting of information across quantum networks\nas well as speed-up of classical causal inference algorithms on quantum\ncomputers. A direct generalization of the existing causal inference techniques\nto the quantum domain is not possible due to superposition and entanglement. We\nput forth a new theoretical framework for merging quantum information science\nand causal inference by exploiting entropic principles. First, we build the\nfundamental connection between the celebrated quantum marginal problem and\nentropic causal inference. Second, inspired by the definition of geometric\nquantum discord, we fill the gap between classical conditional probabilities\nand quantum conditional density matrices. These fundamental theoretical\nadvances are exploited to develop a scalable algorithmic approach for quantum\nentropic causal inference. We apply our proposed framework to an experimentally\nrelevant scenario of identifying message senders on quantum noisy links. This\nsuccessful inference on a synthetic quantum dataset can lay the foundations of\nidentifying originators of malicious activity on future multi-node quantum\nnetworks. We unify classical and quantum causal inference in a principled way\npaving the way for future applications in quantum computing and networking.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 15:51:34 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 22:40:47 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Javidian", "Mohammad Ali", ""], ["Aggarwal", "Vaneet", ""], ["Bao", "Fanglin", ""], ["Jacob", "Zubin", ""]]}, {"id": "2102.11963", "submitter": "Yuriy Pershin", "authors": "Y. V. Pershin, J. Kim, T. Datta, M. Di Ventra", "title": "An experimental demonstration of the memristor test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mes-hall cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple and unambiguous test has been recently suggested [J. Phys. D:\nApplied Physics, 52, 01LT01 (2018)] to check experimentally if a resistor with\nmemory is indeed a memristor, namely a resistor whose resistance depends only\non the charge that flows through it, or on the history of the voltage across\nit. However, although such a test would represent the litmus test for claims\nabout memristors (in the ideal sense), it has yet to be applied widely to\nactual physical devices. In this paper, we experimentally apply it to a\ncurrent-carrying wire interacting with a magnetic core, which was recently\nclaimed to be a memristor (so-called `$\\Phi$ memristor') [J. Appl. Phys. 125,\n054504 (2019)]. The results of our experiment demonstrate unambiguously that\nthis `$\\Phi$ memristor' is not a memristor: it is simply an inductor with\nmemory. This demonstration casts further doubts that ideal memristors do\nactually exist in nature or may be easily created in the lab.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 22:09:13 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Pershin", "Y. V.", ""], ["Kim", "J.", ""], ["Datta", "T.", ""], ["Di Ventra", "M.", ""]]}, {"id": "2102.13288", "submitter": "Junde Li", "authors": "Junde Li, Mahabubul Alam, Swaroop Ghosh", "title": "Large-scale Quantum Approximate Optimization via Divide-and-Conquer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantum Approximate Optimization Algorithm (QAOA) is a promising hybrid\nquantum-classical algorithm for solving combinatorial optimization problems.\nHowever, it cannot overcome qubit limitation for large-scale problems.\nFurthermore, the execution time of QAOA scales exponentially with the problem\nsize. We propose a Divide-and-Conquer QAOA (DC-QAOA) to address the above\nchallenges for graph maximum cut (MaxCut) problem. The algorithm works by\nrecursively partitioning a larger graph into smaller ones whose MaxCut\nsolutions are obtained with small-size NISQ computers. The overall solution is\nretrieved from the sub-solutions by applying the combination policy of quantum\nstate reconstruction. Multiple partitioning and reconstruction methods are\nproposed/ compared. DC-QAOA achieves 97.14% approximation ratio (20.32% higher\nthan classical counterpart), and 94.79% expectation value (15.80% higher than\nquantum annealing). DC-QAOA also reduces the time complexity of conventional\nQAOA from exponential to quadratic.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 03:10:30 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Li", "Junde", ""], ["Alam", "Mahabubul", ""], ["Ghosh", "Swaroop", ""]]}, {"id": "2102.13323", "submitter": "Eli Shlizerman", "authors": "Jinlin Xiang, Shane Colburn, Arka Majumdar, Eli Shlizerman", "title": "Knowledge Distillation Circumvents Nonlinearity for Optical\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Convolutional Neural Networks (CNNs) have enabled ubiquitous\nimage processing applications. As such, CNNs require fast runtime (forward\npropagation) to process high-resolution visual streams in real time. This is\nstill a challenging task even with state-of-the-art graphics and tensor\nprocessing units. The bottleneck in computational efficiency primarily occurs\nin the convolutional layers. Performing operations in the Fourier domain is a\npromising way to accelerate forward propagation since it transforms\nconvolutions into elementwise multiplications, which are considerably faster to\ncompute for large kernels. Furthermore, such computation could be implemented\nusing an optical 4f system with orders of magnitude faster operation. However,\na major challenge in using this spectral approach, as well as in an optical\nimplementation of CNNs, is the inclusion of a nonlinearity between each\nconvolutional layer, without which CNN performance drops dramatically. Here, we\npropose a Spectral CNN Linear Counterpart (SCLC) network architecture and\ndevelop a Knowledge Distillation (KD) approach to circumvent the need for a\nnonlinearity and successfully train such networks. While the KD approach is\nknown in machine learning as an effective process for network pruning, we adapt\nthe approach to transfer the knowledge from a nonlinear network (teacher) to a\nlinear counterpart (student). We show that the KD approach can achieve\nperformance that easily surpasses the standard linear version of a CNN and\ncould approach the performance of the nonlinear network. Our simulations show\nthat the possibility of increasing the resolution of the input image allows our\nproposed 4f optical linear network to perform more efficiently than a nonlinear\nnetwork with the same accuracy on two fundamental image processing tasks: (i)\nobject classification and (ii) semantic segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 06:35:34 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Xiang", "Jinlin", ""], ["Colburn", "Shane", ""], ["Majumdar", "Arka", ""], ["Shlizerman", "Eli", ""]]}]