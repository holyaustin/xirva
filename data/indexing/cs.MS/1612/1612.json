[{"id": "1612.00530", "submitter": "Jun Makino", "authors": "Naoki Yoshifuji, Ryo Sakamoto, Keigo Nitadori and Jun Makino", "title": "Implementation and evaluation of data-compression algorithms for\n  irregular-grid iterative methods on the PEZY-SC processor", "comments": "Talk given at IA3 2016 Sixth Workshop on Irregular Applications:\n  Architectures and Algorithms http://hpc.pnl.gov/IA3/IA3/Program.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative methods on irregular grids have been used widely in all areas of\ncomptational science and engineering for solving partial differential equations\nwith complex geometry. They provide the flexibility to express complex shapes\nwith relatively low computational cost. However, the direction of the evolution\nof high-performance processors in the last two decades have caused serious\ndegradation of the computational efficiency of iterative methods on irregular\ngrids, because of relatively low memory bandwidth. Data compression can in\nprinciple reduce the necessary memory memory bandwidth of iterative methods and\nthus improve the efficiency. We have implemented several data compression\nalgorithms on the PEZY-SC processor, using the matrix generated for the HPCG\nbenchmark as an example. For the SpMV (Sparse Matrix-Vector multiplication)\npart of the HPCG benchmark, the best implementation without data compression\nachieved 11.6Gflops/chip, close to the theoretical limit due to the memory\nbandwidth. Our implementation with data compression has achieved 32.4Gflops.\nThis is of course rather extreme case, since the grid used in HPCG is\ngeometrically regular and thus its compression efficiency is very high.\nHowever, in real applications, it is in many cases possible to make a large\npart of the grid to have regular geometry, in particular when the resolution is\nhigh. Note that we do not need to change the structure of the program, except\nfor the addition of the data compression/decompression subroutines. Thus, we\nbelieve the data compression will be very useful way to improve the performance\nof many applications which rely on the use of irregular grids.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 01:09:23 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Yoshifuji", "Naoki", ""], ["Sakamoto", "Ryo", ""], ["Nitadori", "Keigo", ""], ["Makino", "Jun", ""]]}, {"id": "1612.02495", "submitter": "Kyle Niemeyer", "authors": "Daniel Magee and Kyle E Niemeyer", "title": "An initial investigation of the performance of GPU-based swept\n  time-space decomposition", "comments": "14 pages; submitted to 2017 AIAA SciTech Forum", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.DC cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simulations of physical phenomena are essential to the expedient design of\nprecision components in aerospace and other high-tech industries. These\nphenomena are often described by mathematical models involving partial\ndifferential equations (PDEs) without exact solutions. Modern design problems\nrequire simulations with a level of resolution that is difficult to achieve in\na reasonable amount of time even in effectively parallelized solvers. Though\nthe scale of the problem relative to available computing power is the greatest\nimpediment to accelerating these applications, significant performance gains\ncan be achieved through careful attention to the details of memory accesses.\nParallelized PDE solvers are subject to a trade-off in memory management: store\nthe solution for each timestep in abundant, global memory with high access\ncosts or in a limited, private memory with low access costs that must be passed\nbetween nodes. The GPU implementation of swept time-space decomposition\npresented here mitigates this dilemma by using private (shared) memory,\navoiding internode communication, and overwriting unnecessary values. It shows\nsignificant improvement in the execution time of the PDE solvers in one\ndimension achieving speedups of 6-2x for large and small problem sizes\nrespectively compared to naive GPU versions and 7-300x compared to parallel CPU\nversions.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 00:19:54 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 21:27:02 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Magee", "Daniel", ""], ["Niemeyer", "Kyle E", ""]]}, {"id": "1612.03772", "submitter": "Hadi Fanaee-T", "authors": "Hadi Fanaee-T and Joao Gama", "title": "SimTensor: A synthetic tensor data generator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SimTensor is a multi-platform, open-source software for generating artificial\ntensor data (either with CP/PARAFAC or Tucker structure) for reproducible\nresearch on tensor factorization algorithms. SimTensor is a stand-alone\napplication based on MATALB. It provides a wide range of facilities for\ngenerating tensor data with various configurations. It comes with a\nuser-friendly graphical user interface, which enables the user to generate\ntensors with complicated settings in an easy way. It also has this facility to\nexport generated data to universal formats such as CSV and HDF5, which can be\nimported via a wide range of programming languages (C, C++, Java, R, Fortran,\nMATLAB, Perl, Python, and many more). The most innovative part of SimTensor is\nthis that can generate temporal tensors with periodic waves, seasonal effects\nand streaming structure. it can apply constraints such as non-negativity and\ndifferent kinds of sparsity to the data. SimTensor also provides this facility\nto simulate different kinds of change-points and inject various types of\nanomalies. The source code and binary versions of SimTensor is available for\ndownload in http://www.simtensor.org.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 19:13:03 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Fanaee-T", "Hadi", ""], ["Gama", "Joao", ""]]}, {"id": "1612.04470", "submitter": "Farhad Merchant", "authors": "Farhad Merchant, Tarun Vatwani, Anupam Chattopadhyay, Soumyendu Raha,\n  S K Nandy, and Ranjani Narayan", "title": "Efficient Realization of Householder Transform through\n  Algorithm-Architecture Co-design for Acceleration of QR Factorization", "comments": null, "journal-ref": null, "doi": "10.1109/TPDS.2018.2803820", "report-no": null, "categories": "cs.PF cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present efficient realization of Householder Transform (HT) based QR\nfactorization through algorithm-architecture co-design where we achieve\nperformance improvement of 3-90x in-terms of Gflops/watt over state-of-the-art\nmulticore, General Purpose Graphics Processing Units (GPGPUs), Field\nProgrammable Gate Arrays (FPGAs), and ClearSpeed CSX700. Theoretical and\nexperimental analysis of classical HT is performed for opportunities to exhibit\nhigher degree of parallelism where parallelism is quantified as a number of\nparallel operations per level in the Directed Acyclic Graph (DAG) of the\ntransform. Based on theoretical analysis of classical HT, an opportunity\nre-arrange computations in the classical HT is identified that results in\nModified HT (MHT) where it is shown that MHT exhibits 1.33x times higher\nparallelism than classical HT. Experiments in off-the-shelf multicore and\nGeneral Purpose Graphics Processing Units (GPGPUs) for HT and MHT suggest that\nMHT is capable of achieving slightly better or equal performance compared to\nclassical HT based QR factorization realizations in the optimized software\npackages for Dense Linear Algebra (DLA). We implement MHT on a customized\nplatform for Dense Linear Algebra (DLA) and show that MHT achieves 1.3x better\nperformance than native implementation of classical HT on the same accelerator.\nFor custom realization of HT and MHT based QR factorization, we also identify\nmacro operations in the DAGs of HT and MHT that are realized on a\nReconfigurable Data-path (RDP). We also observe that due to re-arrangement in\nthe computations in MHT, custom realization of MHT is capable of achieving 12%\nbetter performance improvement over multicore and GPGPUs than the performance\nimprovement reported by General Matrix Multiplication (GEMM) over highly tuned\nDLA software packages for multicore and GPGPUs which is counter-intuitive.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 03:22:44 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Merchant", "Farhad", ""], ["Vatwani", "Tarun", ""], ["Chattopadhyay", "Anupam", ""], ["Raha", "Soumyendu", ""], ["Nandy", "S K", ""], ["Narayan", "Ranjani", ""]]}, {"id": "1612.05313", "submitter": "Nathan Bliss", "authors": "Nathan Bliss and Jan Verschelde", "title": "The Method of Gauss-Newton to Compute Power Series Solutions of\n  Polynomial Homotopies", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.SC math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the extension of the method of Gauss-Newton from complex\nfloating-point arithmetic to the field of truncated power series with complex\nfloating-point coefficients. With linearization we formulate a linear system\nwhere the coefficient matrix is a series with matrix coefficients, and provide\na characterization for when the matrix series is regular based on the algebraic\nvariety of an augmented system. The structure of the linear system leads to a\nblock triangular system. In the regular case, solving the linear system is\nequivalent to solving a Hermite interpolation problem. We show that this\nsolution has cost cubic in the problem size. In general, at singular points, we\nrely on methods of tropical algebraic geometry to compute Puiseux series. With\na few illustrative examples, we demonstrate the application to polynomial\nhomotopy continuation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 23:52:29 GMT"}, {"version": "v2", "created": "Sat, 1 Jul 2017 22:23:17 GMT"}, {"version": "v3", "created": "Mon, 23 Oct 2017 04:14:25 GMT"}, {"version": "v4", "created": "Thu, 26 Oct 2017 00:54:59 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Bliss", "Nathan", ""], ["Verschelde", "Jan", ""]]}, {"id": "1612.05778", "submitter": "Ning Xie", "authors": "Changbo Chen, Svyatoslav Covanov, Farnam Mansouri, Marc Moreno Maza,\n  Ning Xie and Yuzhen Xie", "title": "Parallel Integer Polynomial Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for multiplying dense polynomials with integer\ncoefficients in a parallel fashion, targeting multi-core processor\narchitectures. Complexity estimates and experimental comparisons demonstrate\nthe advantages of this new approach.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2016 14:54:52 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Chen", "Changbo", ""], ["Covanov", "Svyatoslav", ""], ["Mansouri", "Farnam", ""], ["Maza", "Marc Moreno", ""], ["Xie", "Ning", ""], ["Xie", "Yuzhen", ""]]}, {"id": "1612.07526", "submitter": "Shengguo Li", "authors": "Shengguo Li, Francois-Henry Rouet, Jie Liu, Chun Huang, Xingyu Gao and\n  Xuebin Chi", "title": "An efficient hybrid tridiagonal divide-and-conquer algorithm on\n  distributed memory architectures", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an efficient divide-and-conquer (DC) algorithm is proposed for\nthe symmetric tridiagonal matrices based on ScaLAPACK and the hierarchically\nsemiseparable (HSS) matrices. HSS is an important type of rank-structured\nmatrices.Most time of the DC algorithm is cost by computing the eigenvectors\nvia the matrix-matrix multiplications (MMM). In our parallel hybrid DC (PHDC)\nalgorithm, MMM is accelerated by using the HSS matrix techniques when the\nintermediate matrix is large. All the HSS algorithms are done via the package\nSTRUMPACK. PHDC has been tested by using many different matrices. Compared with\nthe DC implementation in MKL, PHDC can be faster for some matrices with few\ndeflations when using hundreds of processes. However, the gains decrease as the\nnumber of processes increases. The comparisons of PHDC with ELPA (the\nEigenvalue soLvers for Petascale Applications library) are similar. PHDC is\nusually slower than MKL and ELPA when using 300 or more processes on Tianhe-2\nsupercomputer.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 10:19:09 GMT"}], "update_date": "2016-12-27", "authors_parsed": [["Li", "Shengguo", ""], ["Rouet", "Francois-Henry", ""], ["Liu", "Jie", ""], ["Huang", "Chun", ""], ["Gao", "Xingyu", ""], ["Chi", "Xuebin", ""]]}, {"id": "1612.07848", "submitter": "Meiyue Shao", "authors": "Meiyue Shao and Chao Yang", "title": "BSEPACK User's Guide", "comments": "The software is available at\n  https://sites.google.com/a/lbl.gov/bsepack/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the user manual for the software package BSEPACK (Bethe--Salpeter\nEigenvalue Solver Package).\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 01:16:54 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Shao", "Meiyue", ""], ["Yang", "Chao", ""]]}, {"id": "1612.08060", "submitter": "Luke Olson", "authors": "Amanda Bienz, William D. Gropp, Luke N. Olson", "title": "Node Aware Sparse Matrix-Vector Multiplication", "comments": "27 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sparse matrix-vector multiply (SpMV) operation is a key computational\nkernel in many simulations and linear solvers. The large communication\nrequirements associated with a reference implementation of a parallel SpMV\nresult in poor parallel scalability. The cost of communication depends on the\nphysical locations of the send and receive processes: messages injected into\nthe network are more costly than messages sent between processes on the same\nnode. In this paper, a node aware parallel SpMV (NAPSpMV) is introduced to\nexploit knowledge of the system topology, specifically the node-processor\nlayout, to reduce costs associated with communication. The values of the input\nvector are redistributed to minimize both the number and the size of messages\nthat are injected into the network during a SpMV, leading to a reduction in\ncommunication costs. A variety of computational experiments that highlight the\nefficiency of this approach are presented.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 18:40:46 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 19:33:50 GMT"}, {"version": "v3", "created": "Wed, 15 Nov 2017 16:22:24 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Bienz", "Amanda", ""], ["Gropp", "William D.", ""], ["Olson", "Luke N.", ""]]}]