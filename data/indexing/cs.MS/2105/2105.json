[{"id": "2105.00115", "submitter": "James Diffenderfer", "authors": "James Diffenderfer, Daniel Osei-Kuffuor, Harshitha Menon", "title": "QDOT: Quantized Dot Product Kernel for Approximate High-Performance\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Approximate computing techniques have been successful in reducing computation\nand power costs in several domains. However, error sensitive applications in\nhigh-performance computing are unable to benefit from existing approximate\ncomputing strategies that are not developed with guaranteed error bounds. While\napproximate computing techniques can be developed for individual\nhigh-performance computing applications by domain specialists, this often\nrequires additional theoretical analysis and potentially extensive software\nmodification. Hence, the development of low-level error-bounded approximate\ncomputing strategies that can be introduced into any high-performance computing\napplication without requiring additional analysis or significant software\nalterations is desirable. In this paper, we provide a contribution in this\ndirection by proposing a general framework for designing error-bounded\napproximate computing strategies and apply it to the dot product kernel to\ndevelop qdot -- an error-bounded approximate dot product kernel. Following the\nintroduction of qdot, we perform a theoretical analysis that yields a\ndeterministic bound on the relative approximation error introduced by qdot.\nEmpirical tests are performed to illustrate the tightness of the derived error\nbound and to demonstrate the effectiveness of qdot on a synthetic dataset, as\nwell as two scientific benchmarks -- Conjugate Gradient (CG) and the Power\nmethod. In particular, using qdot for the dot products in CG can result in a\nmajority of components being perforated or quantized to half precision without\nincreasing the iteration count required for convergence to the same solution as\nCG using a double precision dot product.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 22:41:17 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Diffenderfer", "James", ""], ["Osei-Kuffuor", "Daniel", ""], ["Menon", "Harshitha", ""]]}, {"id": "2105.00385", "submitter": "Zachary Pardos", "authors": "Anirudhan Badrinath, Frederic Wang, Zachary Pardos", "title": "pyBKT: An Accessible Python Library of Bayesian Knowledge Tracing Models", "comments": "Accepted to the 2021 Conference on Educational Data Mining (EDM '21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Knowledge Tracing, a model used for cognitive mastery estimation,\nhas been a hallmark of adaptive learning research and an integral component of\ndeployed intelligent tutoring systems (ITS). In this paper, we provide a brief\nhistory of knowledge tracing model research and introduce pyBKT, an accessible\nand computationally efficient library of model extensions from the literature.\nThe library provides data generation, fitting, prediction, and cross-validation\nroutines, as well as a simple to use data helper interface to ingest typical\ntutor log dataset formats. We evaluate the runtime with various dataset sizes\nand compare to past implementations. Additionally, we conduct sanity checks of\nthe model using experiments with simulated data to evaluate the accuracy of its\nEM parameter learning and use real-world data to validate its predictions,\ncomparing pyBKT's supported model variants with results from the papers in\nwhich they were originally introduced. The library is open source and open\nlicense for the purpose of making knowledge tracing more accessible to\ncommunities of research and practice and to facilitate progress in the field\nthrough easier replication of past approaches.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 03:08:53 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 04:20:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Badrinath", "Anirudhan", ""], ["Wang", "Frederic", ""], ["Pardos", "Zachary", ""]]}, {"id": "2105.00420", "submitter": "Johann Dreo", "authors": "Johann Dreo (Systems Biology Group, Department of Computational\n  Biology, USR 3756, Institut Pasteur and CNRS, Paris, France), Arnaud\n  Liefooghe (Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189 CRIStAL, Lille,\n  France), S\\'ebastien Verel (Univ. Littoral C\\^ote d'Opale, Calais, France),\n  Marc Schoenauer (TAU, Inria, CNRS and UPSaclay, LISN, Saclay, France), Juan\n  J. Merelo (University of Granada, Granada, Spain), Alexandre Quemy (Poznan\n  University of Technology, Poznan, Poland), Benjamin Bouvier, Jan Gmys (Inria,\n  Lille, France)", "title": "Paradiseo: From a Modular Framework for Evolutionary Computation to the\n  Automated Design of Metaheuristics ---22 Years of Paradiseo---", "comments": "12 pages, 6 figures, 3 listings, 1 table. To appear in 2021 Genetic\n  and Evolutionary Computation Conference Companion (GECCO'21 Companion), July\n  10--14, 2021, Lille, France. ACM, New York, NY, USA", "journal-ref": null, "doi": "10.1145/3449726.3463276", "report-no": null, "categories": "cs.NE cs.MS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The success of metaheuristic optimization methods has led to the development\nof a large variety of algorithm paradigms. However, no algorithm clearly\ndominates all its competitors on all problems. Instead, the underlying variety\nof landscapes of optimization problems calls for a variety of algorithms to\nsolve them efficiently. It is thus of prior importance to have access to mature\nand flexible software frameworks which allow for an efficient exploration of\nthe algorithm design space. Such frameworks should be flexible enough to\naccommodate any kind of metaheuristics, and open enough to connect with\nhigher-level optimization, monitoring and evaluation softwares. This article\nsummarizes the features of the ParadisEO framework, a comprehensive C++ free\nsoftware which targets the development of modular metaheuristics. ParadisEO\nprovides a highly modular architecture, a large set of components, speed of\nexecution and automated algorithm design features, which are key to modern\napproaches to metaheuristics development.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 08:45:33 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Dreo", "Johann", "", "Systems Biology Group, Department of Computational\n  Biology, USR 3756, Institut Pasteur and CNRS, Paris, France"], ["Liefooghe", "Arnaud", "", "Univ. Lille, CNRS, Inria, Centrale Lille, UMR 9189 CRIStAL, Lille,\n  France"], ["Verel", "S\u00e9bastien", "", "Univ. Littoral C\u00f4te d'Opale, Calais, France"], ["Schoenauer", "Marc", "", "TAU, Inria, CNRS and UPSaclay, LISN, Saclay, France"], ["Merelo", "Juan J.", "", "University of Granada, Granada, Spain"], ["Quemy", "Alexandre", "", "Poznan\n  University of Technology, Poznan, Poland"], ["Bouvier", "Benjamin", "", "Inria,\n  Lille, France"], ["Gmys", "Jan", "", "Inria,\n  Lille, France"]]}, {"id": "2105.00578", "submitter": "Erik Boman", "authors": "Seher Acer, Erik G Boman, Christian A Glusa, and Sivasankaran\n  Rajamanickam", "title": "Sphynx: a parallel multi-GPU graph partitioner for distributed-memory\n  systems", "comments": "To appear in Parallel Computing", "journal-ref": null, "doi": null, "report-no": "SAND2021-0352-O", "categories": "cs.DC cs.DM cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph partitioning has been an important tool to partition the work among\nseveral processors to minimize the communication cost and balance the workload.\nWhile accelerator-based supercomputers are emerging to be the standard, the use\nof graph partitioning becomes even more important as applications are rapidly\nmoving to these architectures. However, there is no distributed-memory\nparallel, multi-GPU graph partitioner available for applications. We developed\na spectral graph partitioner, Sphynx, using the portable, accelerator-friendly\nstack of the Trilinos framework. In Sphynx, we allow using different\npreconditioners and exploit their unique advantages. We use Sphynx to\nsystematically evaluate the various algorithmic choices in spectral\npartitioning with a focus on the GPU performance. We perform those evaluations\non two distinct classes of graphs: regular (such as meshes, matrices from\nfinite element methods) and irregular (such as social networks and web graphs),\nand show that different settings and preconditioners are needed for these graph\nclasses. The experimental results on the Summit supercomputer show that Sphynx\nis the fastest alternative on irregular graphs in an application-friendly\nsetting and obtains a partitioning quality close to ParMETIS on regular graphs.\nWhen compared to nvGRAPH on a single GPU, Sphynx is faster and obtains better\nbalance and better quality partitions. Sphynx provides a good and robust\npartitioning method across a wide range of graphs for applications looking for\na GPU-based partitioner.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 23:47:28 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Acer", "Seher", ""], ["Boman", "Erik G", ""], ["Glusa", "Christian A", ""], ["Rajamanickam", "Sivasankaran", ""]]}, {"id": "2105.02936", "submitter": "Edward Raff", "authors": "Edward Raff", "title": "Exact Acceleration of K-Means++ and K-Means$\\|$", "comments": "to appear in the 30th International Joint Conference on Artificial\n  Intelligence (IJCAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-Means++ and its distributed variant K-Means$\\|$ have become de facto tools\nfor selecting the initial seeds of K-means. While alternatives have been\ndeveloped, the effectiveness, ease of implementation, and theoretical grounding\nof the K-means++ and $\\|$ methods have made them difficult to \"best\" from a\nholistic perspective. By considering the limited opportunities within seed\nselection to perform pruning, we develop specialized triangle inequality\npruning strategies and a dynamic priority queue to show the first acceleration\nof K-Means++ and K-Means$\\|$ that is faster in run-time while being\nalgorithmicly equivalent. For both algorithms we are able to reduce distance\ncomputations by over $500\\times$. For K-means++ this results in up to a\n17$\\times$ speedup in run-time and a $551\\times$ speedup for K-means$\\|$. We\nachieve this with simple, but carefully chosen, modifications to known\ntechniques which makes it easy to integrate our approach into existing\nimplementations of these algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 20:22:55 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Raff", "Edward", ""]]}, {"id": "2105.02991", "submitter": "Ryan Renslow", "authors": "Felicity F. Nielson, Sean M. Colby, Ryan S. Renslow, Thomas O. Metz", "title": "Similarity Downselection: A Python implementation of a heuristic search\n  algorithm for finding the set of the n most dissimilar items with an\n  application in conformer sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.MS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Finding the set of the n items most dissimilar from each other out of a\nlarger population becomes increasingly difficult and computationally expensive\nas either n or the population size grows large. Finding the set of the n most\ndissimilar items is different than simply sorting an array of numbers because\nthere exists a pairwise relationship between each item and all other items in\nthe population. For instance, if you have a set of the most dissimilar n=4\nitems, one or more of the items from n=4 might not be in the set n=5. An exact\nsolution would have to search all possible combinations of size n in the\npopulation, exhaustively. We present an open-source software called similarity\ndownselection (SDS), written in Python and freely available on GitHub. SDS\nimplements a heuristic algorithm for quickly finding the approximate set(s) of\nthe n most dissimilar items. We benchmark SDS against a Monte Carlo method,\nwhich attempts to find the exact solution through repeated random sampling. We\nshow that for SDS to find the set of n most dissimilar conformers, our method\nis not only orders of magnitude faster, but is also more accurate than running\nthe Monte Carlo for 1,000,000 iterations, each searching for set sizes n=3-7\nout of a population of 50,000. We also benchmark SDS against the exact solution\nfor example small populations, showing SDS produces a solution close to the\nexact solution in these instances.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 22:02:38 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Nielson", "Felicity F.", ""], ["Colby", "Sean M.", ""], ["Renslow", "Ryan S.", ""], ["Metz", "Thomas O.", ""]]}, {"id": "2105.03949", "submitter": "Shashi Gowda", "authors": "Shashi Gowda, Yingbo Ma, Alessandro Cheli, Maja Gwozdz, Viral B. Shah,\n  Alan Edelman, Christopher Rackauckas", "title": "High-performance symbolic-numerics via multiple dispatch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.MS cs.PL cs.SC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As mathematical computing becomes more democratized in high-level languages,\nhigh-performance symbolic-numeric systems are necessary for domain scientists\nand engineers to get the best performance out of their machine without deep\nknowledge of code optimization. Naturally, users need different term types\neither to have different algebraic properties for them, or to use efficient\ndata structures. To this end, we developed Symbolics.jl, an extendable symbolic\nsystem which uses dynamic multiple dispatch to change behavior depending on the\ndomain needs. In this work we detail an underlying abstract term interface\nwhich allows for speed without sacrificing generality. We show that by\nformalizing a generic API on actions independent of implementation, we can\nretroactively add optimized data structures to our system without changing the\npre-existing term rewriters. We showcase how this can be used to optimize term\nconstruction and give a 113x acceleration on general symbolic transformations.\nFurther, we show that such a generic API allows for complementary\nterm-rewriting implementations. We demonstrate the ability to swap between\nclassical term-rewriting simplifiers and e-graph-based term-rewriting\nsimplifiers. We showcase an e-graph ruleset which minimizes the number of CPU\ncycles during expression evaluation, and demonstrate how it simplifies a\nreal-world reaction-network simulation to halve the runtime. Additionally, we\nshow a reaction-diffusion partial differential equation solver which is able to\nbe automatically converted into symbolic expressions via multiple dispatch\ntracing, which is subsequently accelerated and parallelized to give a 157x\nsimulation speedup. Together, this presents Symbolics.jl as a next-generation\nsymbolic-numeric computing environment geared towards modeling and simulation.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 14:22:43 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 17:02:31 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Gowda", "Shashi", ""], ["Ma", "Yingbo", ""], ["Cheli", "Alessandro", ""], ["Gwozdz", "Maja", ""], ["Shah", "Viral B.", ""], ["Edelman", "Alan", ""], ["Rackauckas", "Christopher", ""]]}, {"id": "2105.04236", "submitter": "Deevashwer Rathee", "authors": "Deevashwer Rathee, Mayank Rathee, Rahul Kranti Kiran Goli, Divya\n  Gupta, Rahul Sharma, Nishanth Chandran, Aseem Rastogi", "title": "SIRNN: A Math Library for Secure RNN Inference", "comments": "IEEE Security and Privacy 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex machine learning (ML) inference algorithms like recurrent neural\nnetworks (RNNs) use standard functions from math libraries like exponentiation,\nsigmoid, tanh, and reciprocal of square root. Although prior work on secure\n2-party inference provides specialized protocols for convolutional neural\nnetworks (CNNs), existing secure implementations of these math operators rely\non generic 2-party computation (2PC) protocols that suffer from high\ncommunication. We provide new specialized 2PC protocols for math functions that\ncrucially rely on lookup-tables and mixed-bitwidths to address this performance\noverhead; our protocols for math functions communicate up to 423x less data\nthan prior work. Some of the mixed bitwidth operations used by our math\nimplementations are (zero and signed) extensions, different forms of\ntruncations, multiplication of operands of mixed-bitwidths, and digit\ndecomposition (a generalization of bit decomposition to larger digits). For\neach of these primitive operations, we construct specialized 2PC protocols that\nare more communication efficient than generic 2PC, and can be of independent\ninterest. Furthermore, our math implementations are numerically precise, which\nensures that the secure implementations preserve model accuracy of cleartext.\nWe build on top of our novel protocols to build SIRNN, a library for end-to-end\nsecure 2-party DNN inference, that provides the first secure implementations of\nan RNN operating on time series sensor data, an RNN operating on speech data,\nand a state-of-the-art ML architecture that combines CNNs and RNNs for\nidentifying all heads present in images. Our evaluation shows that SIRNN\nachieves up to three orders of magnitude of performance improvement when\ncompared to inference of these models using an existing state-of-the-art 2PC\nframework.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 10:04:46 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Rathee", "Deevashwer", ""], ["Rathee", "Mayank", ""], ["Goli", "Rahul Kranti Kiran", ""], ["Gupta", "Divya", ""], ["Sharma", "Rahul", ""], ["Chandran", "Nishanth", ""], ["Rastogi", "Aseem", ""]]}, {"id": "2105.04937", "submitter": "Takeshi Fukaya", "authors": "Takeshi Fukaya, Koki Ishida, Akie Miura, Takeshi Iwashita, Hiroshi\n  Nakashima", "title": "Accelerating the SpMV kernel on standard CPUs by exploiting the\n  partially diagonal structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Matrix Vector multiplication (SpMV) is one of basic building blocks in\nscientific computing, and acceleration of SpMV has been continuously required.\nIn this research, we aim for accelerating SpMV on recent CPUs for sparse\nmatrices that have a specific sparsity structure, namely a diagonally\nstructured sparsity pattern. We focus a hybrid storage format that combines the\nDIA and CSR formats, so-called the HDC format. First, we recall the importance\nof introducing cache blocking techniques into HDC-based SpMV kernels. Next,\nbased on the observation of the cache blocked kernel, we present a modified\nversion of the HDC formats, which we call the M-HDC format, in which partial\ndiagonal structures are expected to be more efficiently picked up. For these\nSpMV kernels, we theoretically analyze the expected performance improvement\nbased on performance models. Then, we conduct comprehensive experiments on\nstate-of-the-art multi-core CPUs. By the experiments using typical matrices, we\nclarify the detailed performance characteristics of each SpMV kernel. We also\nevaluate the performance for matrices appearing in practical applications and\ndemonstrate that our approach can accelerate SpMV for some of them. Through the\npresent paper, we demonstrate the effectiveness of exploiting partial diagonal\nstructures by the M-HDC format as a promising approach to accelerating SpMV on\nCPUs for a certain kind of practical sparse matrices.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 11:04:01 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Fukaya", "Takeshi", ""], ["Ishida", "Koki", ""], ["Miura", "Akie", ""], ["Iwashita", "Takeshi", ""], ["Nakashima", "Hiroshi", ""]]}, {"id": "2105.05969", "submitter": "Jonas \\v{S}ukys", "authors": "Jonas \\v{S}ukys and Marco Bacci", "title": "SPUX Framework: a Scalable Package for Bayesian Uncertainty\n  Quantification and Propagation", "comments": "Supplementary Material available as a PDF in submission package", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CE cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SPUX - a modular framework for Bayesian inference enabling\nuncertainty quantification and propagation in linear and nonlinear,\ndeterministic and stochastic models, and supporting Bayesian model selection.\nSPUX can be coupled to any serial or parallel application written in any\nprogramming language, (e.g. including Python, R, Julia, C/C++, Fortran, Java,\nor a binary executable), scales effortlessly from serial runs on a personal\ncomputer to parallel high performance computing clusters, and aims to provide a\nplatform particularly suited to support and foster reproducibility in\ncomputational science. We illustrate SPUX capabilities for a simple yet\nrepresentative random walk model, describe how to couple different types of\nuser applications, and showcase several readily available examples from\nenvironmental sciences. In addition to available state-of-the-art numerical\ninference algorithms including EMCEE, PMCMC (PF) and SABC, the open source\nnature of the SPUX framework and the explicit description of the hierarchical\nparallel SPUX executors should also greatly simplify the implementation and\nusage of other inference and optimization techniques.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 21:16:24 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["\u0160ukys", "Jonas", ""], ["Bacci", "Marco", ""]]}, {"id": "2105.07544", "submitter": "Jennifer Loe", "authors": "Jennifer A. Loe, Christian A. Glusa, Ichitaro Yamazaki, Erik G. Boman,\n  and Sivasankaran Rajamanickam", "title": "Experimental Evaluation of Multiprecision Strategies for GMRES on GPUs", "comments": "Accepted for publication in the IEEE IPDPS Accelerators and Hybrid\n  Emerging Systems (AsHES) 11th Workshop, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support for lower precision computation is becoming more common in\naccelerator hardware due to lower power usage, reduced data movement and\nincreased computational performance. However, computational science and\nengineering (CSE) problems require double precision accuracy in several\ndomains. This conflict between hardware trends and application needs has\nresulted in a need for multiprecision strategies at the linear algebra\nalgorithms level if we want to exploit the hardware to its full potential while\nmeeting the accuracy requirements. In this paper, we focus on preconditioned\nsparse iterative linear solvers, a key kernel in several CSE applications. We\npresent a study of multiprecision strategies for accelerating this kernel on\nGPUs. We seek the best methods for incorporating multiple precisions into the\nGMRES linear solver; these include iterative refinement and parallelizable\npreconditioners. Our work presents strategies to determine when multiprecision\nGMRES will be effective and to choose parameters for a multiprecision iterative\nrefinement solver to achieve better performance. We use an implementation that\nis based on the Trilinos library and employs Kokkos Kernels for performance\nportability of linear algebra kernels. Performance results demonstrate the\npromise of multiprecision approaches and demonstrate even further improvements\nare possible by optimizing low-level kernels.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 23:22:02 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Loe", "Jennifer A.", ""], ["Glusa", "Christian A.", ""], ["Yamazaki", "Ichitaro", ""], ["Boman", "Erik G.", ""], ["Rajamanickam", "Sivasankaran", ""]]}, {"id": "2105.09107", "submitter": "Tomas Pevny", "authors": "Simon Mandlik, Matej Racinsky, Viliam Lisy, Tomas Pevny", "title": "Mill.jl and JsonGrinder.jl: automated differentiable feature extraction\n  for learning from raw JSON data", "comments": "5 pages, 2 figures, 1 table, submitted to section on one-source\n  software of Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Learning from raw data input, thus limiting the need for manual feature\nengineering, is one of the key components of many successful applications of\nmachine learning methods. While machine learning problems are often formulated\non data that naturally translate into a vector representation suitable for\nclassifiers, there are data sources, for example in cybersecurity, that are\nnaturally represented in diverse files with a unifying hierarchical structure,\nsuch as XML, JSON, and Protocol Buffers. Converting this data to vector\n(tensor) representation is generally done by manual feature engineering, which\nis laborious, lossy, and prone to human bias about the importance of particular\nfeatures.\n  Mill and JsonGrinder is a tandem of libraries, which fully automates the\nconversion. Starting with an arbitrary set of JSON samples, they create a\ndifferentiable machine learning model capable of infer from further JSON\nsamples in their raw form.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 13:02:10 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Mandlik", "Simon", ""], ["Racinsky", "Matej", ""], ["Lisy", "Viliam", ""], ["Pevny", "Tomas", ""]]}, {"id": "2105.09512", "submitter": "Americo Cunha Jr", "authors": "A. Cunha Jr, R. Nasser, R. Sampaio, H. Lopes, and K. Breitman", "title": "Uncertainty quantification through Monte Carlo method in a cloud\n  computing setting", "comments": null, "journal-ref": "Computer Physics Communications, vol. 185, pp. 1355-1363, 2014", "doi": "10.1016/j.cpc.2014.01.006", "report-no": null, "categories": "stat.CO cs.MS math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Monte Carlo (MC) method is the most common technique used for uncertainty\nquantification, due to its simplicity and good statistical results. However,\nits computational cost is extremely high, and, in many cases, prohibitive.\nFortunately, the MC algorithm is easily parallelizable, which allows its use in\nsimulations where the computation of a single realization is very costly. This\nwork presents a methodology for the parallelization of the MC method, in the\ncontext of cloud computing. This strategy is based on the MapReduce paradigm,\nand allows an efficient distribution of tasks in the cloud. This methodology is\nillustrated on a problem of structural dynamics that is subject to\nuncertainties. The results show that the technique is capable of producing good\nresults concerning statistical moments of low order. It is shown that even a\nsimple problem may require many realizations for convergence of histograms,\nwhich makes the cloud computing strategy very attractive (due to its high\nscalability capacity and low-cost). Additionally, the results regarding the\ntime of processing and storage space usage allow one to qualify this new\nmethodology as a solution for simulations that require a number of MC\nrealizations beyond the standard.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 04:52:40 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Cunha", "A.", "Jr"], ["Nasser", "R.", ""], ["Sampaio", "R.", ""], ["Lopes", "H.", ""], ["Breitman", "K.", ""]]}, {"id": "2105.10332", "submitter": "Kyle Niemeyer", "authors": "Anthony S. Walker and Kyle E. Niemeyer", "title": "The Two-Dimensional Swept Rule Applied on Heterogeneous Architectures", "comments": "18 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The partial differential equations describing compressible fluid flows can be\nnotoriously difficult to resolve on a pragmatic scale and often require the use\nof high performance computing systems and/or accelerators. However, these\nsystems face scaling issues such as latency, the fixed cost of communicating\ninformation between devices in the system. The swept rule is a technique\ndesigned to minimize these costs by obtaining a solution to unsteady equations\nat as many possible spatial locations and times prior to communicating. In this\nstudy, we implemented and tested the swept rule for solving two-dimensional\nproblems on heterogeneous computing systems across two distinct systems. Our\nsolver showed a speedup range of 0.22-2.71 for the heat diffusion equation and\n0.52-1.46 for the compressible Euler equations. We can conclude from this study\nthat the swept rule offers both potential for speedups and slowdowns and that\ncare should be taken when designing such a solver to maximize benefits. These\nresults can help make decisions to maximize these benefits and inform designs.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 20:06:09 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Walker", "Anthony S.", ""], ["Niemeyer", "Kyle E.", ""]]}, {"id": "2105.10384", "submitter": "Leonid Sokolinsky", "authors": "Leonid B. Sokolinsky and Irina M. Sokolinskaya", "title": "FRaGenLP: A Generator of Random Linear Programming Problems for Cluster\n  Computing Systems", "comments": "Submitted to \"Communications in Computer and Information Science\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The article presents and evaluates a scalable FRaGenLP algorithm for\ngenerating random linear programming problems of large dimension $n$ on cluster\ncomputing systems. To ensure the consistency of the problem and the boundedness\nof the feasible region, the constraint system includes $2n+1$ standard\ninequalities, called support inequalities. New random inequalities are\ngenerated and added to the system in a manner that ensures the consistency of\nthe constraints. Furthermore, the algorithm uses two likeness metrics to\nprevent the addition of a new random inequality that is similar to one already\npresent in the constraint system. The algorithm also rejects random\ninequalities that cannot affect the solution of the linear programming problem\nbounded by the support inequalities. The parallel implementation of the\nFRaGenLP algorithm is performed in C++ through the parallel BSF-skeleton, which\nencapsulates all aspects related to the MPI-based parallelization of the\nprogram. We provide the results of large-scale computational experiments on a\ncluster computing system to study the scalability of the FRaGenLP algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 14:55:24 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Sokolinsky", "Leonid B.", ""], ["Sokolinskaya", "Irina M.", ""]]}, {"id": "2105.10798", "submitter": "Alexander Brandt", "authors": "Alexander Brandt and Marc Moreno Maza", "title": "On the Complexity and Parallel Implementation of Hensel's Lemma and\n  Weierstrass Preparation", "comments": "21 pages, 3 figures, submitted to Computer Algebra in Scientific\n  Computing CASC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.DC cs.MS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Hensel's lemma, combined with repeated applications of Weierstrass\npreparation theorem, allows for the factorization of polynomials with\nmultivariate power series coefficients. We present a complexity analysis for\nthis method and leverage those results to guide the load-balancing of a\nparallel implementation to concurrently update all factors. In particular, the\nfactorization creates a pipeline where the terms of degree k of the first\nfactor are computed simultaneously with the terms of degree k-1 of the second\nfactor, etc. An implementation challenge is the inherent irregularity of\ncomputational work between factors, as our complexity analysis reveals.\nAdditional resource utilization and load-balancing is achieved through the\nparallelization of Weierstrass preparation. Experimental results show the\nefficacy of this mixed parallel scheme, achieving up to 9x parallel speedup on\n12 cores.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 19:26:52 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 18:50:05 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Brandt", "Alexander", ""], ["Maza", "Marc Moreno", ""]]}, {"id": "2105.11534", "submitter": "Francesco Oliveri", "authors": "Francesco Oliveri", "title": "ReLie: a Reduce program for Lie group analysis of differential equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA math-ph math.MP math.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Lie symmetry analysis provides a general theoretical framework for\ninvestigating ordinary and partial differential equations. The theory is\ncompletely algorithmic even if it usually involves lengthy computations. For\nthis reason, many computer algebra packages have been developed along the years\nto automate the computation. In this paper, we describe the program ReLie,\nwritten in the Computer Algebra System Reduce, which since 2008 is an open\nsource program (http://www.reduce-algebra.com) and is available for all\nplatforms. \\relie is able to perform almost automatically the needed\ncomputations for Lie symmetry analysis of differential equations. Its source\ncode is freely available at the url http://mat521.unime.it/oliveri. The use of\nthe program is illustrated by means of some simple examples; nevertheless, it\nis to be underlined that it provides effective also for more complex\ncomputations where one has to deal with very large expressions.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 11:22:13 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Oliveri", "Francesco", ""]]}, {"id": "2105.12301", "submitter": "Keichi Takahashi", "authors": "Keichi Takahashi (1), Wassapon Watanakeesuntorn (1), Kohei Ichikawa\n  (1), Joseph Park (2), Ryousei Takano (3), Jason Haga (3), George Sugihara\n  (4), Gerald M. Pao (5) ((1) Nara Institute of Science and Technology, (2)\n  U.S. Department of the Interior, (3) National Institute of Advanced\n  Industrial Science and Technology, (4) University of California San Diego,\n  (5) Salk Institute for Biological Studies)", "title": "kEDM: A Performance-portable Implementation of Empirical Dynamic\n  Modeling using Kokkos", "comments": "8 pages, 9 figures, accepted at Practice & Experience in Advanced\n  Research Computing (PEARC'21), corresponding authors: Keichi Takahashi,\n  Gerald M. Pao", "journal-ref": null, "doi": "10.1145/3437359.3465571", "report-no": null, "categories": "cs.DC cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical Dynamic Modeling (EDM) is a state-of-the-art non-linear time-series\nanalysis framework. Despite its wide applicability, EDM was not scalable to\nlarge datasets due to its expensive computational cost. To overcome this\nobstacle, researchers have attempted and succeeded in accelerating EDM from\nboth algorithmic and implementational aspects. In previous work, we developed a\nmassively parallel implementation of EDM targeting HPC systems (mpEDM).\nHowever, mpEDM maintains different backends for different architectures. This\ndesign becomes a burden in the increasingly diversifying HPC systems, when\nporting to new hardware. In this paper, we design and develop a\nperformance-portable implementation of EDM based on the Kokkos performance\nportability framework (kEDM), which runs on both CPUs and GPUs while based on a\nsingle codebase. Furthermore, we optimize individual kernels specifically for\nEDM computation, and use real-world datasets to demonstrate up to $5.5\\times$\nspeedup compared to mpEDM in convergent cross mapping computation.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 02:21:55 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Takahashi", "Keichi", ""], ["Watanakeesuntorn", "Wassapon", ""], ["Ichikawa", "Kohei", ""], ["Park", "Joseph", ""], ["Takano", "Ryousei", ""], ["Haga", "Jason", ""], ["Sugihara", "George", ""], ["Pao", "Gerald M.", ""]]}, {"id": "2105.12415", "submitter": "Tobias Weinzierl", "authors": "Peter J. Noble, Tobias Weinzierl", "title": "A multiresolution Discrete Element Method for triangulated objects with\n  implicit timestepping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulations of many rigid bodies colliding with each other sometimes yield\nparticularly interesting results if the colliding objects differ significantly\nin size and are non-spherical. The most expensive part within such a simulation\ncode is the collision detection. We propose a family of novel multiscale\ncollision detection algorithms that can be applied to triangulated objects\nwithin explicit and implicit time stepping methods. They are well-suited to\nhandle objects that cannot be represented by analytical shapes or assemblies of\nanalytical objects. Inspired by multigrid methods and adaptive mesh refinement,\nwe determine collision points iteratively over a resolution hierarchy, and\ncombine a functional minimisation plus penalty parameters with the actual\ncomparision-based geometric distance calculation. Coarse surrogate geometry\nrepresentations identify \"no collision\" scenarios early on and otherwise yield\nan educated guess which triangle subsets of the next finer level potentially\nyield collisions. They prune the search tree, and furthermore feed conservative\ncontact force estimates into the iterative solve behind an implicit time\nstepping. Implicit time stepping and non-analytical shapes often yield\nprohibitive high compute cost for rigid body simulations. Our approach reduces\nthese cost algorithmically by one to two orders of magnitude. It also exhibits\nhigh vectorisation efficiency due to its iterative nature.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 09:11:33 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Noble", "Peter J.", ""], ["Weinzierl", "Tobias", ""]]}, {"id": "2105.12739", "submitter": "Holger Schulz", "authors": "Holger Schulz and Gonzalo Brito Gadeschi and Oleksandr Rudyy and\n  Tobias Weinzierl", "title": "Task inefficiency patterns for a wave equation solver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The orchestration of complex algorithms demands high levels of automation to\nuse modern hardware efficiently. Task-based programming with OpenMP 5.0 is a\nprominent candidate to accomplish this goal. We study OpenMP 5.0's tasking in\nthe context of a wave equation solver (ExaHyPE) using three different\narchitectures and runtimes. We describe several task-scheduling flaws present\nin currently available runtimes, demonstrate how they impact performance and\nshow how to work around them. Finally, we propose extensions to the OpenMP\nstandard.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:00:01 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 12:40:58 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Schulz", "Holger", ""], ["Gadeschi", "Gonzalo Brito", ""], ["Rudyy", "Oleksandr", ""], ["Weinzierl", "Tobias", ""]]}, {"id": "2105.13921", "submitter": "Oleg Smirnov", "authors": "Oleg Smirnov", "title": "TensorFlow RiemOpt: a library for optimization on Riemannian manifolds", "comments": "The library code is available at\n  https://github.com/master/tensorflow-riemopt", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The adoption of neural networks and deep learning in non-Euclidean domains\nhas been hindered until recently by the lack of scalable and efficient learning\nframeworks. Existing toolboxes in this space were mainly motivated by research\nand education use cases, whereas practical aspects, such as deploying and\nmaintaining machine learning models, were often overlooked.\n  We attempt to bridge this gap by proposing TensorFlow RiemOpt, a Python\nlibrary for optimization on Riemannian manifolds in TensorFlow. The library is\ndesigned with the aim for a seamless integration with the TensorFlow ecosystem,\ntargeting not only research, but also streamlining production machine learning\npipelines.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 10:42:09 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 18:50:43 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Smirnov", "Oleg", ""]]}]