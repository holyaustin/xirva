[{"id": "1509.00864", "submitter": "Jonathan Sorenson", "authors": "Jonathan P. Sorenson and Jonathan Webster", "title": "Strong Pseudoprimes to Twelve Prime Bases", "comments": null, "journal-ref": null, "doi": "10.1090/mcom/3134", "report-no": null, "categories": "math.NT cs.DS cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\psi_m$ be the smallest strong pseudoprime to the first $m$ prime bases.\nThis value is known for $1 \\leq m \\leq 11$. We extend this by finding\n$\\psi_{12}$ and $\\psi_{13}$. We also present an algorithm to find all integers\n$n\\le B$ that are strong pseudoprimes to the first $m$ prime bases; with a\nreasonable heuristic assumption we can show that it takes at most\n$B^{2/3+o(1)}$ time.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 20:23:07 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Sorenson", "Jonathan P.", ""], ["Webster", "Jonathan", ""]]}, {"id": "1509.01347", "submitter": "Pablo De Oliveira Castro", "authors": "Christophe Denis (CMLA), Pablo De Oliveira Castro (LI-PaRAD, UVSQ),\n  Eric Petit (UVSQ)", "title": "Verificarlo: checking floating point accuracy through Monte Carlo\n  Arithmetic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical accuracy of floating point computation is a well studied topic\nwhich has not made its way to the end-user in scientific computing. Yet, it has\nbecome a critical issue with the recent requirements for code modernization to\nharness new highly parallel hardware and perform higher resolution computation.\nTo democratize numerical accuracy analysis, it is important to propose tools\nand methodologies to study large use cases in a reliable and automatic way. In\nthis paper, we propose verificarlo, an extension to the LLVM compiler to\nautomatically use Monte Carlo Arithmetic in a transparent way for the end-user.\nIt supports all the major languages including C, C++, and Fortran. Unlike\nsource-to-source approaches, our implementation captures the influence of\ncompiler optimizations on the numerical accuracy. We illustrate how Monte Carlo\nArithmetic using the verificarlo tool outperforms the existing approaches on\nvarious use cases and is a step toward automatic numerical analysis.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 06:20:18 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2015 09:46:12 GMT"}, {"version": "v3", "created": "Wed, 4 Nov 2015 12:53:31 GMT"}, {"version": "v4", "created": "Fri, 9 Nov 2018 07:55:49 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Denis", "Christophe", "", "CMLA"], ["Castro", "Pablo De Oliveira", "", "LI-PaRAD, UVSQ"], ["Petit", "Eric", "", "UVSQ"]]}, {"id": "1509.02796", "submitter": "Johannes K\\\"oster", "authors": "Johannes K\\\"oster", "title": "Rust-Bio - a fast and safe bioinformatics library", "comments": null, "journal-ref": null, "doi": "10.1093/bioinformatics/btv573", "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Rust-Bio, the first general purpose bioinformatics library for the\ninnovative Rust programming language. Rust-Bio leverages the unique combination\nof speed, memory safety and high-level syntax offered by Rust to provide a fast\nand safe set of bioinformatics algorithms and data structures with a focus on\nsequence analysis.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 14:53:02 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["K\u00f6ster", "Johannes", ""]]}, {"id": "1509.03604", "submitter": "Kathryn Huff", "authors": "Kathryn D. Huff, Matthew J. Gidden, Robert W. Carlsen, Robert R.\n  Flanagan, Meghan B. McGarry, Arrielle C. Opotowsky, Erich A. Schneider,\n  Anthony M. Scopatz, Paul P.H. Wilson", "title": "Fundamental concepts in the Cyclus nuclear fuel cycle simulation\n  framework", "comments": null, "journal-ref": "Advances in Engineering Software, Volume 94, April 2016, Pages\n  46-59", "doi": "10.1016/j.advengsoft.2016.01.014", "report-no": null, "categories": "cs.SE cs.CE cs.MA cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As nuclear power expands, technical, economic, political, and environmental\nanalyses of nuclear fuel cycles by simulators increase in importance. To date,\nhowever, current tools are often fleet-based rather than discrete and\nrestrictively licensed rather than open source. Each of these choices presents\na challenge to modeling fidelity, generality, efficiency, robustness, and\nscientific transparency. The Cyclus nuclear fuel cycle simulator framework and\nits modeling ecosystem incorporate modern insights from simulation science and\nsoftware architecture to solve these problems so that challenges in nuclear\nfuel cycle analysis can be better addressed. A summary of the Cyclus fuel cycle\nsimulator framework and its modeling ecosystem are presented. Additionally, the\nimplementation of each is discussed in the context of motivating challenges in\nnuclear fuel cycle simulation. Finally, the current capabilities of Cyclus are\ndemonstrated for both open and closed fuel cycles.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 18:39:59 GMT"}, {"version": "v2", "created": "Fri, 11 Mar 2016 16:05:10 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Huff", "Kathryn D.", ""], ["Gidden", "Matthew J.", ""], ["Carlsen", "Robert W.", ""], ["Flanagan", "Robert R.", ""], ["McGarry", "Meghan B.", ""], ["Opotowsky", "Arrielle C.", ""], ["Schneider", "Erich A.", ""], ["Scopatz", "Anthony M.", ""], ["Wilson", "Paul P. H.", ""]]}, {"id": "1509.04518", "submitter": "Yaroslav Sergeyev", "authors": "Yaroslav D. Sergeyev, Dmitri E. Kvasov", "title": "A deterministic global optimization using smooth diagonal auxiliary\n  functions", "comments": "25 pages, 7 figures, 3 tables", "journal-ref": "Communications in Nonlinear Science and Numerical Simulation,\n  2015, 21, 99-111", "doi": "10.1016/j.cnsns.2014.08.026", "report-no": null, "categories": "math.OC cs.MS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many practical decision-making problems it happens that functions involved\nin optimization process are black-box with unknown analytical representations\nand hard to evaluate. In this paper, a global optimization problem is\nconsidered where both the goal function~$f(x)$ and its gradient $f'(x)$ are\nblack-box functions. It is supposed that $f'(x)$ satisfies the Lipschitz\ncondition over the search hyperinterval with an unknown Lipschitz constant~$K$.\nA new deterministic `Divide-the-Best' algorithm based on efficient diagonal\npartitions and smooth auxiliary functions is proposed in its basic version, its\nconvergence conditions are studied and numerical experiments executed on eight\nhundred test functions are presented.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 12:24:58 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Sergeyev", "Yaroslav D.", ""], ["Kvasov", "Dmitri E.", ""]]}, {"id": "1509.04627", "submitter": "Johannes Holke", "authors": "Carsten Burstedde, Johannes Holke", "title": "A tetrahedral space-filling curve for non-conforming adaptive meshes", "comments": "33 pages, 12 figures, 8 tables", "journal-ref": null, "doi": "10.1137/15M1040049", "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a space-filling curve for triangular and tetrahedral\nred-refinement that can be computed using bitwise interleaving operations\nsimilar to the well-known Z-order or Morton curve for cubical meshes. To store\nsufficient information for random access, we define a low-memory encoding using\n10 bytes per triangle and 14 bytes per tetrahedron. We present algorithms that\ncompute the parent, children, and face-neighbors of a mesh element in constant\ntime, as well as the next and previous element in the space-filling curve and\nwhether a given element is on the boundary of the root simplex or not. Our\npresentation concludes with a scalability demonstration that creates and adapts\nselected meshes on a large distributed-memory system.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 16:16:54 GMT"}, {"version": "v2", "created": "Fri, 21 Apr 2017 09:44:26 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Burstedde", "Carsten", ""], ["Holke", "Johannes", ""]]}, {"id": "1509.04692", "submitter": "M. Eric Irrgang II", "authors": "Joshua A. Anderson, M. Eric Irrgang, Sharon C. Glotzer", "title": "Scalable Metropolis Monte Carlo for simulation of hard shapes", "comments": "5 figures", "journal-ref": null, "doi": "10.1016/j.cpc.2016.02.024", "report-no": null, "categories": "cond-mat.soft cs.MS physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design and implement HPMC, a scalable hard particle Monte Carlo simulation\ntoolkit, and release it open source as part of HOOMD-blue. HPMC runs in\nparallel on many CPUs and many GPUs using domain decomposition. We employ BVH\ntrees instead of cell lists on the CPU for fast performance, especially with\nlarge particle size disparity, and optimize inner loops with SIMD vector\nintrinsics on the CPU. Our GPU kernel proposes many trial moves in parallel on\na checkerboard and uses a block-level queue to redistribute work among threads\nand avoid divergence. HPMC supports a wide variety of shape classes, including\nspheres / disks, unions of spheres, convex polygons, convex spheropolygons,\nconcave polygons, ellipsoids / ellipses, convex polyhedra, convex\nspheropolyhedra, spheres cut by planes, and concave polyhedra. NVT and NPT\nensembles can be run in 2D or 3D triclinic boxes. Additional integration\nschemes permit Frenkel-Ladd free energy computations and implicit depletant\nsimulations. In a benchmark system of a fluid of 4096 pentagons, HPMC performs\n10 million sweeps in 10 minutes on 96 CPU cores on XSEDE Comet. The same\nsimulation would take 7.6 hours in serial. HPMC also scales to large system\nsizes, and the same benchmark with 16.8 million particles runs in 1.4 hours on\n2048 GPUs on OLCF Titan.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 19:25:33 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Anderson", "Joshua A.", ""], ["Irrgang", "M. Eric", ""], ["Glotzer", "Sharon C.", ""]]}, {"id": "1509.04706", "submitter": "Daniil Kazantsev", "authors": "Daniil Kazantsev, Evgueni Ovtchinnikov, William R. B. Lionheart,\n  Philip J. Withers, Peter D. Lee", "title": "Direct high-order edge-preserving regularization for tomographic image\n  reconstruction", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.MS cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we present a new two-level iterative algorithm for tomographic\nimage reconstruction. The algorithm uses a regularization technique, which we\ncall edge-preserving Laplacian, that preserves sharp edges between objects\nwhile damping spurious oscillations in the areas where the reconstructed image\nis smooth. Our numerical simulations demonstrate that the proposed method\noutperforms total variation (TV) regularization and it is competitive with the\ncombined TV-L2 penalty. Obtained reconstructed images show increased\nsignal-to-noise ratio and visually appealing structural features. Computer\nimplementation and parameter control of the proposed technique is\nstraightforward, which increases the feasibility of it across many tomographic\napplications. In this paper, we applied our method to the under-sampled\ncomputed tomography (CT) projection data and also considered a case of\nreconstruction in emission tomography The MATLAB code is provided to support\nobtained results.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 18:23:56 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Kazantsev", "Daniil", ""], ["Ovtchinnikov", "Evgueni", ""], ["Lionheart", "William R. B.", ""], ["Withers", "Philip J.", ""], ["Lee", "Peter D.", ""]]}, {"id": "1509.06397", "submitter": "David Hallac", "authors": "David Hallac, Christopher Wong, Steven Diamond, Abhijit Sharang, Rok\n  Sosic, Stephen Boyd, Jure Leskovec", "title": "SnapVX: A Network-Based Convex Optimization Solver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.MS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SnapVX is a high-performance Python solver for convex optimization problems\ndefined on networks. For these problems, it provides a fast and scalable\nsolution with guaranteed global convergence. SnapVX combines the capabilities\nof two open source software packages: Snap.py and CVXPY. Snap.py is a large\nscale graph processing library, and CVXPY provides a general modeling framework\nfor small-scale subproblems. SnapVX offers a customizable yet easy-to-use\ninterface with out-of-the-box functionality. Based on the Alternating Direction\nMethod of Multipliers (ADMM), it is able to efficiently store, analyze, and\nsolve large optimization problems from a variety of different applications.\nDocumentation, examples, and more can be found on the SnapVX website at\nhttp://snap.stanford.edu/snapvx.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 20:44:12 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 05:50:05 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Hallac", "David", ""], ["Wong", "Christopher", ""], ["Diamond", "Steven", ""], ["Sharang", "Abhijit", ""], ["Sosic", "Rok", ""], ["Boyd", "Stephen", ""], ["Leskovec", "Jure", ""]]}, {"id": "1509.06935", "submitter": "Daniel Ruprecht", "authors": "Daniel Ruprecht", "title": "Shared Memory Pipelined Parareal", "comments": null, "journal-ref": "In: Rivera F., Pena T., Cabaleiro J. (eds) Euro-Par 2017: Parallel\n  Processing. Lecture Notes in Computer Science, vol 10417. Springer", "doi": "10.1007/978-3-319-64203-1_48", "report-no": null, "categories": "cs.MS cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the parallel-in-time integration method Parareal, pipelining can be used\nto hide some of the cost of the serial correction step and improve its\nefficiency. The paper introduces a basic OpenMP implementation of pipelined\nParareal and compares it to a standard MPI-based variant. Both versions yield\nalmost identical runtimes, but, depending on the compiler, the OpenMP variant\nconsumes about 7% less energy and has a significantly smaller memory footprint.\nHowever, its higher implementation complexity might make it difficult to use in\nlegacy codes and in combination with spatial parallelisation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 12:04:23 GMT"}, {"version": "v2", "created": "Wed, 20 Apr 2016 09:57:47 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2019 15:20:23 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Ruprecht", "Daniel", ""]]}, {"id": "1509.07164", "submitter": "Bob Carpenter", "authors": "Bob Carpenter, Matthew D. Hoffman, Marcus Brubaker, Daniel Lee, Peter\n  Li, Michael Betancourt", "title": "The Stan Math Library: Reverse-Mode Automatic Differentiation in C++", "comments": "96 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As computational challenges in optimization and statistical inference grow\never harder, algorithms that utilize derivatives are becoming increasingly more\nimportant. The implementation of the derivatives that make these algorithms so\npowerful, however, is a substantial user burden and the practicality of these\nalgorithms depends critically on tools like automatic differentiation that\nremove the implementation burden entirely. The Stan Math Library is a C++,\nreverse-mode automatic differentiation library designed to be usable, extensive\nand extensible, efficient, scalable, stable, portable, and redistributable in\norder to facilitate the construction and utilization of such algorithms.\n  Usability is achieved through a simple direct interface and a cleanly\nabstracted functional interface. The extensive built-in library includes\nfunctions for matrix operations, linear algebra, differential equation solving,\nand most common probability functions. Extensibility derives from a\nstraightforward object-oriented framework for expressions, allowing users to\neasily create custom functions. Efficiency is achieved through a combination of\ncustom memory management, subexpression caching, traits-based metaprogramming,\nand expression templates. Partial derivatives for compound functions are\nevaluated lazily for improved scalability. Stability is achieved by taking care\nwith arithmetic precision in algebraic expressions and providing stable,\ncompound functions where possible. For portability, the library is\nstandards-compliant C++ (03) and has been tested for all major compilers for\nWindows, Mac OS X, and Linux.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 21:34:46 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Carpenter", "Bob", ""], ["Hoffman", "Matthew D.", ""], ["Brubaker", "Marcus", ""], ["Lee", "Daniel", ""], ["Li", "Peter", ""], ["Betancourt", "Michael", ""]]}, {"id": "1509.07659", "submitter": "Laura Rebollo-Neira", "authors": "Laura Rebollo-Neira and Gagan Aggarwal", "title": "A dedicated greedy pursuit algorithm for sparse spectral representation\n  of music sound", "comments": "Routines for implementing the approach are available on\n  http://www.nonlinear-approx.info/examples/node02.html", "journal-ref": "Journal of the Acoustical Society of America, Vol.140, No.4,\n  2933-2943 (2016)", "doi": "10.1121/1.4964342", "report-no": null, "categories": "cs.SD cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dedicated algorithm for sparse spectral representation of music sound is\npresented. The goal is to enable the representation of a piece of music signal,\nas a linear superposition of as few spectral components as possible. A\nrepresentation of this nature is said to be sparse. In the present context\nsparsity is accomplished by greedy selection of the spectral components, from\nan overcomplete set called a dictionary. The proposed algorithm is tailored to\nbe applied with trigonometric dictionaries. Its distinctive feature being that\nit avoids the need for the actual construction of the whole dictionary, by\nimplementing the required operations via the Fast Fourier Transform. The\nachieved sparsity is theoretically equivalent to that rendered by the\nOrthogonal Matching Pursuit method. The contribution of the proposed dedicated\nimplementation is to extend the applicability of the standard Orthogonal\nMatching Pursuit algorithm, by reducing its storage and computational demands.\nThe suitability of the approach for producing sparse spectral models is\nillustrated by comparison with the traditional method, in the line of the Short\nTime Fourier Transform, involving only the corresponding orthonormal\ntrigonometric basis.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 10:03:17 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 13:48:34 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Rebollo-Neira", "Laura", ""], ["Aggarwal", "Gagan", ""]]}, {"id": "1509.07919", "submitter": "Ang Li", "authors": "Ang Li, Radu Serban, Dan Negrut", "title": "Analysis of A Splitting Approach for the Parallel Solution of Linear\n  Systems on GPU Cards", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss an approach for solving sparse or dense banded linear systems\n${\\bf A} {\\bf x} = {\\bf b}$ on a Graphics Processing Unit (GPU) card. The\nmatrix ${\\bf A} \\in {\\mathbb{R}}^{N \\times N}$ is possibly nonsymmetric and\nmoderately large; i.e., $10000 \\leq N \\leq 500000$. The ${\\it split\\ and\\\nparallelize}$ (${\\tt SaP}$) approach seeks to partition the matrix ${\\bf A}$\ninto diagonal sub-blocks ${\\bf A}_i$, $i=1,\\ldots,P$, which are independently\nfactored in parallel. The solution may choose to consider or to ignore the\nmatrices that couple the diagonal sub-blocks ${\\bf A}_i$. This approach, along\nwith the Krylov subspace-based iterative method that it preconditions, are\nimplemented in a solver called ${\\tt SaP::GPU}$, which is compared in terms of\nefficiency with three commonly used sparse direct solvers: ${\\tt PARDISO}$,\n${\\tt SuperLU}$, and ${\\tt MUMPS}$. ${\\tt SaP::GPU}$, which runs entirely on\nthe GPU except several stages involved in preliminary row-column permutations,\nis robust and compares well in terms of efficiency with the aforementioned\ndirect solvers. In a comparison against Intel's ${\\tt MKL}$, ${\\tt SaP::GPU}$\nalso fares well when used to solve dense banded systems that are close to being\ndiagonally dominant. ${\\tt SaP::GPU}$ is publicly available and distributed as\nopen source under a permissive BSD3 license.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 23:04:17 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Li", "Ang", ""], ["Serban", "Radu", ""], ["Negrut", "Dan", ""]]}]