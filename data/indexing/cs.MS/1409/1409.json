[{"id": "1409.0669", "submitter": "Karl Rupp", "authors": "Karl Rupp and Philippe Tillet and Florian Rudolf and Josef Weinbub and\n  Tibor Grasser and Ansgar J\\\"ungel", "title": "Performance Portability Study of Linear Algebra Kernels in OpenCL", "comments": "11 pages, 8 figures, 2 tables, International Workshop on OpenCL 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance portability of OpenCL kernel implementations for common\nmemory bandwidth limited linear algebra operations across different hardware\ngenerations of the same vendor as well as across vendors is studied. Certain\ncombinations of kernel implementations and work sizes are found to exhibit good\nperformance across compute kernels, hardware generations, and, to a lesser\ndegree, vendors. As a consequence, it is demonstrated that the optimization of\na single kernel is often sufficient to obtain good performance for a large\nclass of more complicated operations.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 11:21:13 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Rupp", "Karl", ""], ["Tillet", "Philippe", ""], ["Rudolf", "Florian", ""], ["Weinbub", "Josef", ""], ["Grasser", "Tibor", ""], ["J\u00fcngel", "Ansgar", ""]]}, {"id": "1409.1354", "submitter": "Ming-Hua Li", "authors": "Ming-Hua Li, Ping Wang, Zhe Chang, and Dong Zhao", "title": "CosmoMC Installation and Running Guidelines", "comments": "10 pages, 0 figures. Publicly distributed and available", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.CO cs.MS hep-th", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  CosmoMC is a Fortran 95 Markov-Chain Monte-Carlo (MCMC) engine to explore the\ncosmological parameter space, plus a Python suite for plotting and presenting\nresults (see http://cosmologist.info/cosmomc/). This document describes the\ninstallation of the CosmoMC on a Linux system (Ubuntu 14.04.1 LTS 64-bit\nversion). It is written for those who want to use it in their scientific\nresearch but without much training on Linux and the program. Besides a\nstep-by-step installation guide, we also give a brief introduction of how to\nrun the program on both a desktop and a cluster. We share our way to generate\nthe plots that are commonly used in the references of cosmology. For more\ninformation, one can refer to the CosmoCoffee forum\n(http://cosmocoffee.info/viewforum.php?f=11) or contact the authors of this\ndocument. Questions and comments would be much appreciated.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 08:21:16 GMT"}, {"version": "v2", "created": "Tue, 9 Sep 2014 09:35:56 GMT"}, {"version": "v3", "created": "Thu, 11 Sep 2014 02:24:35 GMT"}], "update_date": "2014-09-12", "authors_parsed": [["Li", "Ming-Hua", ""], ["Wang", "Ping", ""], ["Chang", "Zhe", ""], ["Zhao", "Dong", ""]]}, {"id": "1409.2008", "submitter": "Klaus Rohe", "authors": "Klaus Rohe", "title": "Computing the coefficients for the power series solution of the\n  Lane-Emden equation with the Python library SymPy", "comments": "14 pages, 4 figures, 2 source code listings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  It is shown how the Python library Sympy can be used to compute symbolically\nthe coefficients of the power series solution of the Lane-Emden equation (LEE).\nSympy is an open source Python library for symbolic mathematics. The power\nseries solutions are compared to the numerically computed solutions using\nmatplotlib. The results of a run time measurement of the implemented algorithm\nare discussed at the end.\n", "versions": [{"version": "v1", "created": "Sat, 6 Sep 2014 12:19:37 GMT"}, {"version": "v2", "created": "Sun, 25 Jan 2015 18:34:58 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Rohe", "Klaus", ""]]}, {"id": "1409.2908", "submitter": "Austin Benson", "authors": "Austin R. Benson and Grey Ballard", "title": "A Framework for Practical Parallel Fast Matrix Multiplication", "comments": null, "journal-ref": "Proceedings of the 20th ACM SIGPLAN Symposium on Principles and\n  Practice of Parallel Programming (PPoPP), 2015", "doi": "10.1145/2858788.2688513", "report-no": null, "categories": "cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix multiplication is a fundamental computation in many scientific\ndisciplines. In this paper, we show that novel fast matrix multiplication\nalgorithms can significantly outperform vendor implementations of the classical\nalgorithm and Strassen's fast algorithm on modest problem sizes and shapes.\nFurthermore, we show that the best choice of fast algorithm depends not only on\nthe size of the matrices but also the shape. We develop a code generation tool\nto automatically implement multiple sequential and shared-memory parallel\nvariants of each fast algorithm, including our novel parallelization scheme.\nThis allows us to rapidly benchmark over 20 fast algorithms on several problem\nsizes. Furthermore, we discuss a number of practical implementation issues for\nthese algorithms on shared-memory machines that can direct further research on\nmaking fast algorithms practical.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 22:28:36 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Benson", "Austin R.", ""], ["Ballard", "Grey", ""]]}, {"id": "1409.3144", "submitter": "Duncan Temple Lang", "authors": "Duncan Temple Lang", "title": "Enhancing R with Advanced Compilation Tools and Methods", "comments": "Published in at http://dx.doi.org/10.1214/13-STS462 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 2, 181-200", "doi": "10.1214/13-STS462", "report-no": "IMS-STS-STS462", "categories": "stat.CO cs.MS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I describe an approach to compiling common idioms in R code directly to\nnative machine code and illustrate it with several examples. Not only can this\nyield significant performance gains, but it allows us to use new approaches to\ncomputing in R. Importantly, the compilation requires no changes to R itself,\nbut is done entirely via R packages. This allows others to experiment with\ndifferent compilation strategies and even to define new domain-specific\nlanguages within R. We use the Low-Level Virtual Machine (LLVM) compiler\ntoolkit to create the native code and perform sophisticated optimizations on\nthe code. By adopting this widely used software within R, we leverage its\nability to generate code for different platforms such as CPUs and GPUs, and\nwill continue to benefit from its ongoing development. This approach\npotentially allows us to develop high-level R code that is also fast, that can\nbe compiled to work with different data representations and sources, and that\ncould even be run outside of R. The approach aims to both provide a compiler\nfor a limited subset of the R language and also to enable R programmers to\nwrite other compilers. This is another approach to help us write high-level\ndescriptions of what we want to compute, not how.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 10:37:20 GMT"}], "update_date": "2014-09-12", "authors_parsed": [["Lang", "Duncan Temple", ""]]}, {"id": "1409.4618", "submitter": "Immanuel Anjam", "authors": "Immanuel Anjam, Jan Valdman", "title": "Fast MATLAB assembly of FEM matrices in 2D and 3D: Edge elements", "comments": "12 pages, 5 figures, ESCO 2014 conference", "journal-ref": null, "doi": "10.1016/j.amc.2015.03.105", "report-no": null, "categories": "cs.MS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an effective and flexible way to assemble finite element stiffness\nand mass matrices in MATLAB. We apply this for problems discretized by edge\nfinite elements. Typical edge finite elements are Raviart-Thomas elements used\nin discretizations of H(div) spaces and Nedelec elements in discretizations of\nH(curl) spaces. We explain vectorization ideas and comment on a freely\navailable MATLAB code which is fast and scalable with respect to time.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 13:08:47 GMT"}, {"version": "v2", "created": "Mon, 11 May 2015 13:19:00 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Anjam", "Immanuel", ""], ["Valdman", "Jan", ""]]}, {"id": "1409.5757", "submitter": "Andrey Vladimirov", "authors": "Ryo Asai and Andrey Vladimirov", "title": "Intel Cilk Plus for Complex Parallel Algorithms: \"Enormous Fast Fourier\n  Transform\" (EFFT) Library", "comments": "17 pages. Submitted to Parallel Computing", "journal-ref": null, "doi": "10.1016/j.parco.2015.05.004", "report-no": null, "categories": "cs.MS cs.DC cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we demonstrate the methodology for parallelizing the\ncomputation of large one-dimensional discrete fast Fourier transforms (DFFTs)\non multi-core Intel Xeon processors. DFFTs based on the recursive Cooley-Tukey\nmethod have to control cache utilization, memory bandwidth and vector hardware\nusage, and at the same time scale across multiple threads or compute nodes. Our\nmethod builds on single-threaded Intel Math Kernel Library (MKL) implementation\nof DFFT, and uses the Intel Cilk Plus framework for thread parallelism. We\ndemonstrate the ability of Intel Cilk Plus to handle parallel recursion with\nnested loop-centric parallelism without tuning the code to the number of cores\nor cache metrics. The result of our work is a library called EFFT that performs\n1D DFTs of size 2^N for N>=21 faster than the corresponding Intel MKL parallel\nDFT implementation by up to 1.5x, and faster than FFTW by up to 2.5x. The code\nof EFFT is available for free download under the GPLv3 license. This work\nprovides a new efficient DFFT implementation, and at the same time demonstrates\nan educational example of how computer science problems with complex parallel\npatterns can be optimized for high performance using the Intel Cilk Plus\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 18:48:58 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Asai", "Ryo", ""], ["Vladimirov", "Andrey", ""]]}, {"id": "1409.7316", "submitter": "Atilim Gunes Baydin", "authors": "Atilim Gunes Baydin, Barak A. Pearlmutter", "title": "An Analysis of Publication Venues for Automatic Differentiation Research", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the results of our analysis of publication venues for papers on\nautomatic differentiation (AD), covering academic journals and conference\nproceedings. Our data are collected from the AD publications database\nmaintained by the autodiff.org community website. The database is purpose-built\nfor the AD field and is expanding via submissions by AD researchers. Therefore,\nit provides a relatively noise-free list of publications relating to the field.\nHowever, it does include noise in the form of variant spellings of journal and\nconference names. We handle this by manually correcting and merging these\nvariants under the official names of corresponding venues. We also share the\nraw data we get after these corrections.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 16:20:16 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Baydin", "Atilim Gunes", ""], ["Pearlmutter", "Barak A.", ""]]}, {"id": "1409.8186", "submitter": "Bertrand Thierry", "authors": "Bertrand Thierry and Xavier Antoine and Chokri Chniti and Hasan\n  Alzubaidi", "title": "$\\mu$-diff: an open-source Matlab toolbox for computing multiple\n  scattering problems by disks", "comments": "27 pages, 15 figures, associated code available online at\n  http://mu-diff.math.cnrs.fr", "journal-ref": null, "doi": "10.1016/j.cpc.2015.03.013", "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to describe a Matlab toolbox, called $\\mu$-diff, for\nmodeling and numerically solving two-dimensional complex multiple scattering by\na large collection of circular cylinders. The approximation methods in\n$\\mu$-diff are based on the Fourier series expansions of the four basic\nintegral operators arising in scattering theory. Based on these expressions, an\nefficient spectrally accurate finite-dimensional solution of multiple\nscattering problems can be simply obtained for complex media even when many\nscatterers are considered as well as large frequencies. The solution of the\nglobal linear system to solve can use either direct solvers or preconditioned\niterative Krylov subspace solvers for block Toeplitz matrices. Based on this\napproach, this paper explains how the code is built and organized. Some\ncomplete numerical examples of applications (direct and inverse scattering) are\nprovided to show that $\\mu$-diff is a flexible, efficient and robust toolbox\nfor solving some complex multiple scattering problems.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 16:48:16 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Thierry", "Bertrand", ""], ["Antoine", "Xavier", ""], ["Chniti", "Chokri", ""], ["Alzubaidi", "Hasan", ""]]}, {"id": "1409.8608", "submitter": "Elmar Peise", "authors": "Elmar Peise (1), Diego Fabregat-Traver (1), Paolo Bientinesi (1) ((1)\n  AICES, RWTH Aachen)", "title": "On the Performance Prediction of BLAS-based Tensor Contractions", "comments": "Submitted to PMBS14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor operations are surging as the computational building blocks for a\nvariety of scientific simulations and the development of high-performance\nkernels for such operations is known to be a challenging task. While for\noperations on one- and two-dimensional tensors there exist standardized\ninterfaces and highly-optimized libraries (BLAS), for higher dimensional\ntensors neither standards nor highly-tuned implementations exist yet. In this\npaper, we consider contractions between two tensors of arbitrary dimensionality\nand take on the challenge of generating high-performance implementations by\nresorting to sequences of BLAS kernels. The approach consists in breaking the\ncontraction down into operations that only involve matrices or vectors. Since\nin general there are many alternative ways of decomposing a contraction, we are\nable to methodically derive a large family of algorithms. The main contribution\nof this paper is a systematic methodology to accurately identify the fastest\nalgorithms in the bunch, without executing them. The goal is instead\naccomplished with the help of a set of cache-aware micro-benchmarks for the\nunderlying BLAS kernels. The predictions we construct from such benchmarks\nallow us to reliably single out the best-performing algorithms in a tiny\nfraction of the time taken by the direct execution of the algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 15:54:13 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Peise", "Elmar", ""], ["Fabregat-Traver", "Diego", ""], ["Bientinesi", "Paolo", ""]]}]