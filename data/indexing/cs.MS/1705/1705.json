[{"id": "1705.00720", "submitter": "Jan Verschelde", "authors": "Anders Jensen, Jeff Sommars, and Jan Verschelde", "title": "Computing Tropical Prevarieties in Parallel", "comments": "Accepted for publication in the proceedings of PASCO 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CG cs.DC math.AG math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation of the tropical prevariety is the first step in the\napplication of polyhedral methods to compute positive dimensional solution sets\nof polynomial systems. In particular, pretropisms are candidate leading\nexponents for the power series developments of the solutions. The computation\nof the power series may start as soon as one pretropism is available, so our\nparallel computation of the tropical prevariety has an application in a\npipelined solver.\n  We present a parallel implementation of dynamic enumeration. Our first\ndistributed memory implementation with forked processes achieved good speedups,\nbut quite often resulted in large variations in the execution times of the\nprocesses. The shared memory multithreaded version applies work stealing to\nreduce the variability of the run time. Our implementation applies the thread\nsafe Parma Polyhedral Library (PPL), in exact arithmetic with the GNU\nMultiprecision Arithmetic Library (GMP), aided by the fast memory allocations\nof TCMalloc.\n  Our parallel implementation is capable of computing the tropical prevariety\nof the cyclic 16-roots problem. We also report on computational experiments on\nthe $n$-body and $n$-vortex problems; our computational results compare\nfavorably with Gfan.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 21:34:00 GMT"}, {"version": "v2", "created": "Sat, 1 Jul 2017 23:04:37 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Jensen", "Anders", ""], ["Sommars", "Jeff", ""], ["Verschelde", "Jan", ""]]}, {"id": "1705.01221", "submitter": "Luca De Feo", "authors": "Ludovic Brieulle, Luca De Feo, Javad Doliskani, Jean-Pierre Flori and\n  \\'Eric Schost", "title": "Computing isomorphisms and embeddings of finite fields", "comments": null, "journal-ref": null, "doi": "10.1090/mcom/3363", "report-no": null, "categories": "cs.SC cs.MS math.NT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathbb{F}_q$ be a finite field. Given two irreducible polynomials $f,g$\nover $\\mathbb{F}_q$, with $\\mathrm{deg} f$ dividing $\\mathrm{deg} g$, the\nfinite field embedding problem asks to compute an explicit description of a\nfield embedding of $\\mathbb{F}_q[X]/f(X)$ into $\\mathbb{F}_q[Y]/g(Y)$. When\n$\\mathrm{deg} f = \\mathrm{deg} g$, this is also known as the isomorphism\nproblem.\n  This problem, a special instance of polynomial factorization, plays a central\nrole in computer algebra software. We review previous algorithms, due to\nLenstra, Allombert, Rains, and Narayanan, and propose improvements and\ngeneralizations. Our detailed complexity analysis shows that our newly proposed\nvariants are at least as efficient as previously known algorithms, and in many\ncases significantly better.\n  We also implement most of the presented algorithms, compare them with the\nstate of the art computer algebra software, and make the code available as open\nsource. Our experiments show that our new variants consistently outperform\navailable software.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 01:33:29 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Brieulle", "Ludovic", ""], ["De Feo", "Luca", ""], ["Doliskani", "Javad", ""], ["Flori", "Jean-Pierre", ""], ["Schost", "\u00c9ric", ""]]}, {"id": "1705.01598", "submitter": "Dmitry Liakh", "authors": "Antti-Pekka Hynninen, Dmitry I. Lyakh", "title": "cuTT: A High-Performance Tensor Transpose Library for CUDA Compatible\n  GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the CUDA Tensor Transpose (cuTT) library that implements\nhigh-performance tensor transposes for NVIDIA GPUs with Kepler and above\narchitectures. cuTT achieves high performance by (a) utilizing two\nGPU-optimized transpose algorithms that both use a shared memory buffer in\norder to reduce global memory access scatter, and by (b) computing memory\npositions of tensor elements using a thread-parallel algorithm. We evaluate the\nperformance of cuTT on a variety of benchmarks with tensor ranks ranging from 2\nto 12 and show that cuTT performance is independent of the tensor rank and that\nit performs no worse than an approach based on code generation. We develop a\nheuristic scheme for choosing the optimal parameters for tensor transpose\nalgorithms by implementing an analytical GPU performance model that can be used\nat runtime without need for performance measurements or profiling. Finally, by\nintegrating cuTT into the tensor algebra library TAL-SH, we significantly\nreduce the tensor transpose overhead in tensor contractions, achieving as low\nas just one percent overhead for arithmetically intensive tensor contractions.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 19:58:00 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Hynninen", "Antti-Pekka", ""], ["Lyakh", "Dmitry I.", ""]]}, {"id": "1705.03123", "submitter": "Alexei Sibidanov", "authors": "Alexei Sibidanov", "title": "A revision of the subtract-with-borrow random number generators", "comments": "13 pages", "journal-ref": null, "doi": "10.1016/j.cpc.2017.09.005", "report-no": null, "categories": "physics.comp-ph cs.MS hep-lat", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most popular and widely used subtract-with-borrow generator, also known\nas RANLUX, is reimplemented as a linear congruential generator using large\ninteger arithmetic with the modulus size of 576 bits. Modern computers, as well\nas the specific structure of the modulus inferred from RANLUX, allow for the\ndevelopment of a fast modular multiplication -- the core of the procedure. This\nwas previously believed to be slow and have too high cost in terms of computing\nresources. Our tests show a significant gain in generation speed which is\ncomparable with other fast, high quality random number generators. An\nadditional feature is the fast skipping of generator states leading to a\nseeding scheme which guarantees the uniqueness of random number sequences.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 23:42:29 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Sibidanov", "Alexei", ""]]}, {"id": "1705.03162", "submitter": "Kyle Niemeyer", "authors": "Daniel J Magee and Kyle E Niemeyer", "title": "Accelerating solutions of one-dimensional unsteady PDEs with GPU-based\n  swept time-space decomposition", "comments": "25 pages, 10 figures", "journal-ref": "J. Comput. Phys. 357 (2018) 338-352", "doi": "10.1016/j.jcp.2017.12.028", "report-no": null, "categories": "physics.comp-ph cs.DC cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The expedient design of precision components in aerospace and other high-tech\nindustries requires simulations of physical phenomena often described by\npartial differential equations (PDEs) without exact solutions. Modern design\nproblems require simulations with a level of resolution difficult to achieve in\nreasonable amounts of time---even in effectively parallelized solvers. Though\nthe scale of the problem relative to available computing power is the greatest\nimpediment to accelerating these applications, significant performance gains\ncan be achieved through careful attention to the details of memory\ncommunication and access. The swept time-space decomposition rule reduces\ncommunication between sub-domains by exhausting the domain of influence before\ncommunicating boundary values. Here we present a GPU implementation of the\nswept rule, which modifies the algorithm for improved performance on this\nprocessing architecture by prioritizing use of private (shared) memory,\navoiding interblock communication, and overwriting unnecessary values. It shows\nsignificant improvement in the execution time of finite-difference solvers for\none-dimensional unsteady PDEs, producing speedups of 2--9$\\times$ for a range\nof problem sizes, respectively, compared with simple GPU versions and\n7--300$\\times$ compared with parallel CPU versions. However, for a more\nsophisticated one-dimensional system of equations discretized with a\nsecond-order finite-volume scheme, the swept rule performs 1.2--1.9$\\times$\nworse than a standard implementation for all problem sizes.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 03:25:24 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 23:58:13 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Magee", "Daniel J", ""], ["Niemeyer", "Kyle E", ""]]}, {"id": "1705.03266", "submitter": "Fredrik Johansson", "authors": "Fredrik Johansson", "title": "Computing the Lambert W function in arbitrary-precision complex interval\n  arithmetic", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an algorithm to evaluate all the complex branches of the Lambert\nW function with rigorous error bounds in interval arithmetic, which has been\nimplemented in the Arb library. The classic 1996 paper on the Lambert W\nfunction by Corless et al. provides a thorough but partly heuristic numerical\nanalysis which needs to be complemented with some explicit inequalities and\npractical observations about managing precision and branch cuts.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 10:45:42 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Johansson", "Fredrik", ""]]}, {"id": "1705.03625", "submitter": "Kalyana Babu Nakshatrala", "authors": "J. Chang, K. B. Nakshatrala, M. G. Knepley and L. Johnsson", "title": "A performance spectrum for parallel computational frameworks that solve\n  PDEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Important computational physics problems are often large-scale in nature, and\nit is highly desirable to have robust and high performing computational\nframeworks that can quickly address these problems. However, it is no trivial\ntask to determine whether a computational framework is performing efficiently\nor is scalable. The aim of this paper is to present various strategies for\nbetter understanding the performance of any parallel computational frameworks\nfor solving PDEs. Important performance issues that negatively impact\ntime-to-solution are discussed, and we propose a performance spectrum analysis\nthat can enhance one's understanding of critical aforementioned performance\nissues. As proof of concept, we examine commonly used finite element simulation\npackages and software and apply the performance spectrum to quickly analyze the\nperformance and scalability across various hardware platforms, software\nimplementations, and numerical discretizations. It is shown that the proposed\nperformance spectrum is a versatile performance model that is not only\nextendable to more complex PDEs such as hydrostatic ice sheet flow equations,\nbut also useful for understanding hardware performance in a massively parallel\ncomputing environment. Potential applications and future extensions of this\nwork are also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 06:51:15 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 00:00:52 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Chang", "J.", ""], ["Nakshatrala", "K. B.", ""], ["Knepley", "M. G.", ""], ["Johnsson", "L.", ""]]}, {"id": "1705.03667", "submitter": "Mikl\\'os Homolya", "authors": "Mikl\\'os Homolya, Lawrence Mitchell, Fabio Luporini, David A. Ham", "title": "TSFC: a structure-preserving form compiler", "comments": "Accepted version. 28 pages plus 5 pages supplement", "journal-ref": "SIAM Journal on Scientific Computing, 40 (2018), pp. C401-C428", "doi": "10.1137/17M1130642", "report-no": null, "categories": "cs.MS cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A form compiler takes a high-level description of the weak form of partial\ndifferential equations and produces low-level code that carries out the finite\nelement assembly. In this paper we present the Two-Stage Form Compiler (TSFC),\na new form compiler with the main motivation to maintain the structure of the\ninput expression as long as possible. This facilitates the application of\noptimizations at the highest possible level of abstraction. TSFC features a\nnovel, structure-preserving method for separating the contributions of a form\nto the subblocks of the local tensor in discontinuous Galerkin problems. This\nenables us to preserve the tensor structure of expressions longer through the\ncompilation process than other form compilers. This is also achieved in part by\na two-stage approach that cleanly separates the lowering of finite element\nconstructs to tensor algebra in the first stage, from the scheduling of those\ntensor operations in the second stage. TSFC also efficiently traverses\ncomplicated expressions, and experimental evaluation demonstrates good\ncompile-time performance even for highly complex forms.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 09:21:24 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 13:51:11 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Homolya", "Mikl\u00f3s", ""], ["Mitchell", "Lawrence", ""], ["Luporini", "Fabio", ""], ["Ham", "David A.", ""]]}, {"id": "1705.05249", "submitter": "Cedric Nugteren", "authors": "Cedric Nugteren", "title": "CLBlast: A Tuned OpenCL BLAS Library", "comments": "Conference paper in: IWOCL '18, the International Workshop on OpenCL", "journal-ref": null, "doi": "10.1145/3204919.3204924", "report-no": null, "categories": "cs.MS cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces CLBlast, an open-source BLAS library providing optimized\nOpenCL routines to accelerate dense linear algebra for a wide variety of\ndevices. It is targeted at machine learning and HPC applications and thus\nprovides a fast matrix-multiplication routine (GEMM) to accelerate the core of\nmany applications (e.g. deep learning, iterative solvers, astrophysics,\ncomputational fluid dynamics, quantum chemistry). CLBlast has five main\nadvantages over other OpenCL BLAS libraries: 1) it is optimized for and tested\non a large variety of OpenCL devices including less commonly used devices such\nas embedded and low-power GPUs, 2) it can be explicitly tuned for specific\nproblem-sizes on specific hardware platforms, 3) it can perform operations in\nhalf-precision floating-point FP16 saving bandwidth, time and energy, 4) it has\nan optional CUDA back-end, 5) and it can combine multiple operations in a\nsingle batched routine, accelerating smaller problems significantly. This paper\ndescribes the library and demonstrates the advantages of CLBlast experimentally\nfor different use-cases on a wide variety of OpenCL hardware.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 17:16:59 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 09:10:16 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Nugteren", "Cedric", ""]]}, {"id": "1705.06134", "submitter": "Fredrik Johansson", "authors": "Claus Fieker, William Hart, Tommy Hofmann, Fredrik Johansson", "title": "Nemo/Hecke: Computer Algebra and Number Theory Packages for the Julia\n  Programming Language", "comments": "ISSAC '17, Kaiserslautern, Germany, July 25-28, 2017, 8 pages", "journal-ref": null, "doi": "10.1145/3087604.3087611", "report-no": null, "categories": "cs.MS cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two new packages, Nemo and Hecke, written in the Julia\nprogramming language for computer algebra and number theory. We demonstrate\nthat high performance generic algorithms can be implemented in Julia, without\nthe need to resort to a low-level C implementation. For specialised algorithms,\nwe use Julia's efficient native C interface to wrap existing C/C++ libraries\nsuch as Flint, Arb, Antic and Singular. We give examples of how to use Hecke\nand Nemo and discuss some algorithms that we have implemented to provide high\nperformance basic arithmetic.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 13:10:32 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Fieker", "Claus", ""], ["Hart", "William", ""], ["Hofmann", "Tommy", ""], ["Johansson", "Fredrik", ""]]}, {"id": "1705.06661", "submitter": "Paul Springer", "authors": "Paul Springer, Devin Matthews, Paolo Bientinesi", "title": "Spin Summations: A High-Performance Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Besides tensor contractions, one of the most pronounced computational\nbottlenecks in the non-orthogonally spin-adapted forms of the quantum chemistry\nmethods CCSDT and CCSDTQ, and their approximate forms---including CCSD(T) and\nCCSDT(Q)---are spin summations. At a first sight, spin summations are\noperations similar to tensor transpositions; a closer look instead reveals\nadditional challenges to high-performance calculations, including temporal\nlocality as well as scattered memory accesses. This publication explores a\nsequence of algorithmic solutions for spin summations, each exploiting\nindividual properties of either the underlying hardware (e.g. caches,\nvectorization), or the problem itself (e.g. factorizability). The final\nalgorithm combines the advantages of all the solutions, while avoiding their\ndrawbacks; this algorithm, achieves high-performance through parallelization,\nvectorization, and by exploiting the temporal locality inherent to spin\nsummations. Combined, these optimizations result in speedups between 2.4x and\n5.5x over the NCC quantum chemistry software package. In addition to such a\nperformance boost, our algorithm can perform the spin summations in-place, thus\nreducing the memory footprint by 2x over an out-of-place variant.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 15:48:46 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Springer", "Paul", ""], ["Matthews", "Devin", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1705.06668", "submitter": "Ahmad H. A. Eid", "authors": "Ahmad Hosny Eid", "title": "Introducing Geometric Algebra to Geometric Computing Software\n  Developers: A Computational Thinking Approach", "comments": "Tutorial, 43 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing software systems for Geometric Computing applications can be a\nchallenging task. Software engineers typically use software abstractions to\nhide and manage the high complexity of such systems. Without the presence of a\nunifying algebraic system to describe geometric models, the use of software\nabstractions alone can result in many design and maintenance problems.\nGeometric Algebra (GA) can be a universal abstract algebraic language for\nsoftware engineering geometric computing applications. Few sources, however,\nprovide enough information about GA-based software implementations targeting\nthe software engineering community. In particular, successfully introducing GA\nto software engineers requires quite different approaches from introducing GA\nto mathematicians or physicists. This article provides a high-level\nintroduction to the abstract concepts and algebraic representations behind the\nelegant GA mathematical structure. The article focuses on the conceptual and\nrepresentational abstraction levels behind GA mathematics with sufficient\nreferences for more details. In addition, the article strongly recommends\napplying the methods of Computational Thinking in both introducing GA to\nsoftware engineers, and in using GA as a mathematical language for developing\nGeometric Computing software systems.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 16:05:54 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Eid", "Ahmad Hosny", ""]]}, {"id": "1705.07282", "submitter": "Leonid Yavits PhD", "authors": "L. Yavits, A. Morad, R. Ginosar", "title": "Sparse Matrix Multiplication On An Associative Processor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse matrix multiplication is an important component of linear algebra\ncomputations. Implementing sparse matrix multiplication on an associative\nprocessor (AP) enables high level of parallelism, where a row of one matrix is\nmultiplied in parallel with the entire second matrix, and where the execution\ntime of vector dot product does not depend on the vector size. Four sparse\nmatrix multiplication algorithms are explored in this paper, combining AP and\nbaseline CPU processing to various levels. They are evaluated by simulation on\na large set of sparse matrices. The computational complexity of sparse matrix\nmultiplication on AP is shown to be an O(nnz) where nnz is the number of\nnonzero elements. The AP is found to be especially efficient in binary sparse\nmatrix multiplication. AP outperforms conventional solutions in power\nefficiency.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 09:07:04 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Yavits", "L.", ""], ["Morad", "A.", ""], ["Ginosar", "R.", ""]]}, {"id": "1705.08784", "submitter": "Volker John", "authors": "Ulrich Wilbrandt, Clemens Bartsch, Naveed Ahmed, Najib Alia, Felix\n  Anker, Laura Blank, Alfonso Caiazzo, Sashikumaar Ganesan, Swetlana Giere,\n  Gunar Matthies, Raviteja Meesala, Abdus Shamim, Jagannath Venkatesan, Volker\n  John", "title": "ParMooN - a modernized program package based on mapped finite elements", "comments": "partly supported by European Union (EU), Horizon 2020, Marie\n  Sk{\\l}odowska-Curie Innovative Training Networks (ITN-EID), MIMESIS, grant\n  number 675715", "journal-ref": "Comput. Math. Appl. 74(1), 74-88, 2017", "doi": "10.1016/j.camwa.2016.12.020", "report-no": null, "categories": "math.NA cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  {\\sc ParMooN} is a program package for the numerical solution of elliptic and\nparabolic partial differential equations. It inherits the distinct features of\nits predecessor {\\sc MooNMD} \\cite{JM04}: strict decoupling of geometry and\nfinite element spaces, implementation of mapped finite elements as their\ndefinition can be found in textbooks, and a geometric multigrid preconditioner\nwith the option to use different finite element spaces on different levels of\nthe multigrid hierarchy. After having presented some thoughts about in-house\nresearch codes, this paper focuses on aspects of the parallelization for a\ndistributed memory environment, which is the main novelty of {\\sc ParMooN}.\nNumerical studies, performed on compute servers, assess the efficiency of the\nparallelized geometric multigrid preconditioner in comparison with some\nparallel solvers that are available in the library {\\sc PETSc}. The results of\nthese studies give a first indication whether the cumbersome implementation of\nthe parallelized geometric multigrid method was worthwhile or not.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 14:17:57 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 16:40:58 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Wilbrandt", "Ulrich", ""], ["Bartsch", "Clemens", ""], ["Ahmed", "Naveed", ""], ["Alia", "Najib", ""], ["Anker", "Felix", ""], ["Blank", "Laura", ""], ["Caiazzo", "Alfonso", ""], ["Ganesan", "Sashikumaar", ""], ["Giere", "Swetlana", ""], ["Matthies", "Gunar", ""], ["Meesala", "Raviteja", ""], ["Shamim", "Abdus", ""], ["Venkatesan", "Jagannath", ""], ["John", "Volker", ""]]}, {"id": "1705.08849", "submitter": "Amir Geranmayeh Dr.-Ing.", "authors": "Amir Geranmayeh", "title": "Parallel Matrix-Free Implementation of Frequency-Domain Finite\n  Difference Methods for Cluster Computing", "comments": "7 pages, 10 figures including: Matrix-free 3D finite-difference\n  frequency-domain (FDFD) methods, Simultaneous reduction in memory usage and\n  computational costs of FDFD, Broadband impedance calculation of electrically\n  large interconnects, Ease of solver modification for mutual field coupling\n  simulation between many ports, Domain decomposition for passing the mesh\n  information to parallel machines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Full-wave 3D electromagnetic simulations of complex planar devices,\nmultilayer interconnects, and chip packages are presented for wide-band\nfrequency-domain analysis using the finite difference integration technique\ndeveloped in the PETSc software package. Initial reordering of the index\nassignment to the unknowns makes the resulting system matrix diagonally\ndominant. The rearrangement also facilitates the decomposition of large domain\ninto slices for passing the mesh information to different machines. Matrix-free\nmethods are then exploited to minimize the number of element-wise\nmultiplications and memory requirements in the construction of the system of\nlinear equations. Besides, the recipes provide extreme ease of modifications in\nthe kernel of the code. The applicability of different Krylov subspace solvers\nis investigated. The accuracy is checked through comparisons with CST MICROWAVE\nSTUDIO transient solver results. The parallel execution of the compiled code on\nspecific number of processors in multi-core distributed-memory architectures\ndemonstrate high scalability of the computational algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 16:45:20 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Geranmayeh", "Amir", ""]]}, {"id": "1705.09905", "submitter": "Bangtian Liu", "authors": "Bangtian Liu, Chengyao Wen, Anand D.Sarwate and Maryam Mehri Dehnavi", "title": "A Unified Optimization Approach for Sparse Tensor Operations on GPUs", "comments": null, "journal-ref": null, "doi": "10.1109/CLUSTER.2017.75", "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse tensors appear in many large-scale applications with multidimensional\nand sparse data. While multidimensional sparse data often need to be processed\non manycore processors, attempts to develop highly-optimized GPU-based\nimplementations of sparse tensor operations are rare. The irregular computation\npatterns and sparsity structures as well as the large memory footprints of\nsparse tensor operations make such implementations challenging. We leverage the\nfact that sparse tensor operations share similar computation patterns to\npropose a unified tensor representation called F-COO. Combined with\nGPU-specific optimizations, F-COO provides highly-optimized implementations of\nsparse tensor computations on GPUs. The performance of the proposed unified\napproach is demonstrated for tensor-based kernels such as the Sparse Matricized\nTensor- Times-Khatri-Rao Product (SpMTTKRP) and the Sparse Tensor- Times-Matrix\nMultiply (SpTTM) and is used in tensor decomposition algorithms. Compared to\nstate-of-the-art work we improve the performance of SpTTM and SpMTTKRP up to\n3.7 and 30.6 times respectively on NVIDIA Titan-X GPUs. We implement a\nCANDECOMP/PARAFAC (CP) decomposition and achieve up to 14.9 times speedup using\nthe unified method over state-of-the-art libraries on NVIDIA Titan-X GPUs.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 07:41:22 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Liu", "Bangtian", ""], ["Wen", "Chengyao", ""], ["Sarwate", "Anand D.", ""], ["Dehnavi", "Maryam Mehri", ""]]}, {"id": "1705.10218", "submitter": "Alfio Lazzaro", "authors": "Alfio Lazzaro, Joost VandeVondele, Juerg Hutter, Ole Schuett", "title": "Increasing the Efficiency of Sparse Matrix-Matrix Multiplication with a\n  2.5D Algorithm and One-Sided MPI", "comments": "In Proceedings of PASC '17, Lugano, Switzerland, June 26-28, 2017, 10\n  pages, 4 figures", "journal-ref": null, "doi": "10.1145/3093172.3093228", "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix-matrix multiplication is a basic operation in linear algebra and an\nessential building block for a wide range of algorithms in various scientific\nfields. Theory and implementation for the dense, square matrix case are\nwell-developed. If matrices are sparse, with application-specific sparsity\npatterns, the optimal implementation remains an open question. Here, we explore\nthe performance of communication reducing 2.5D algorithms and one-sided MPI\ncommunication in the context of linear scaling electronic structure theory. In\nparticular, we extend the DBCSR sparse matrix library, which is the basic\nbuilding block for linear scaling electronic structure theory and low scaling\ncorrelated methods in CP2K. The library is specifically designed to efficiently\nperform block-sparse matrix-matrix multiplication of matrices with a relatively\nlarge occupation. Here, we compare the performance of the original\nimplementation based on Cannon's algorithm and MPI point-to-point\ncommunication, with an implementation based on MPI one-sided communications\n(RMA), in both a 2D and a 2.5D approach. The 2.5D approach trades memory and\nauxiliary operations for reduced communication, which can lead to a speedup if\ncommunication is dominant. The 2.5D algorithm is somewhat easier to implement\nwith one-sided communications. A detailed description of the implementation is\nprovided, also for non ideal processor topologies, since this is important for\nactual applications. Given the importance of the precise sparsity pattern, and\neven the actual matrix data, which decides the effective fill-in upon\nmultiplication, the tests are performed within the CP2K package with\napplication benchmarks. Results show a substantial boost in performance for the\nRMA based 2.5D algorithm, up to 1.80x, which is observed to increase with the\nnumber of involved processes in the parallelization.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 14:42:14 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Lazzaro", "Alfio", ""], ["VandeVondele", "Joost", ""], ["Hutter", "Juerg", ""], ["Schuett", "Ole", ""]]}]