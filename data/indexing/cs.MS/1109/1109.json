[{"id": "1109.0783", "submitter": "EPTCS", "authors": "Jerzy Karczmarczuk (University of Caen, France)", "title": "Specific \"scientific\" data structures, and their processing", "comments": "In Proceedings DSL 2011, arXiv:1109.0323", "journal-ref": "EPTCS 66, 2011, pp. 195-209", "doi": "10.4204/EPTCS.66.10", "report-no": null, "categories": "cs.DS cs.MS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming physicists use, as all programmers, arrays, lists, tuples,\nrecords, etc., and this requires some change in their thought patterns while\nconverting their formulae into some code, since the \"data structures\" operated\nupon, while elaborating some theory and its consequences, are rather: power\nseries and Pad\\'e approximants, differential forms and other instances of\ndifferential algebras, functionals (for the variational calculus), trajectories\n(solutions of differential equations), Young diagrams and Feynman graphs, etc.\nSuch data is often used in a [semi-]numerical setting, not necessarily\n\"symbolic\", appropriate for the computer algebra packages. Modules adapted to\nsuch data may be \"just libraries\", but often they become specific, embedded\nsub-languages, typically mapped into object-oriented frameworks, with\noverloaded mathematical operations. Here we present a functional approach to\nthis philosophy. We show how the usage of Haskell datatypes and - fundamental\nfor our tutorial - the application of lazy evaluation makes it possible to\noperate upon such data (in particular: the \"infinite\" sequences) in a natural\nand comfortable manner.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2011 01:57:07 GMT"}], "update_date": "2011-09-06", "authors_parsed": [["Karczmarczuk", "Jerzy", "", "University of Caen, France"]]}, {"id": "1109.1264", "submitter": "Andreas Adelmann", "authors": "J. Progsch, Y. Ineichen and A. Adelmann", "title": "A New Vectorization Technique for Expression Templates in C++", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector operations play an important role in high performance computing and\nare typically provided by highly optimized libraries that implement the BLAS\n(Basic Linear Algebra Subprograms) interface. In C++ templates and operator\noverloading allow the implementation of these vector operations as expression\ntemplates which construct custom loops at compile time and providing a more\nabstract interface. Unfortunately existing expression template libraries lack\nthe performance of fast BLAS(Basic Linear Algebra Subprograms) implementations.\nThis paper presents a new approach - Statically Accelerated Loop Templates\n(SALT) - to close this performance gap by combining expression templates with\nan aggressive loop unrolling technique. Benchmarks were conducted using the\nIntel C++ compiler and GNU Compiler Collection to assess the performance of our\nlibrary relative to Intel's Math Kernel Library as well as the Eigen template\nlibrary. The results show that the approach is able to provide optimization\ncomparable to the fastest available BLAS implementations, while retaining the\nconvenience and flexibility of a template library.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 18:42:48 GMT"}], "update_date": "2011-09-07", "authors_parsed": [["Progsch", "J.", ""], ["Ineichen", "Y.", ""], ["Adelmann", "A.", ""]]}, {"id": "1109.3739", "submitter": "Aydin Buluc", "authors": "Aydin Buluc and John Gilbert", "title": "Parallel Sparse Matrix-Matrix Multiplication and Indexing:\n  Implementation and Experiments", "comments": null, "journal-ref": "SIAM J. Sci. Comput., 34(4), 170 - 191, 2012", "doi": "10.1137/110848244", "report-no": null, "categories": "cs.DC cs.MS cs.NA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized sparse matrix-matrix multiplication (or SpGEMM) is a key\nprimitive for many high performance graph algorithms as well as for some linear\nsolvers, such as algebraic multigrid. Here we show that SpGEMM also yields\nefficient algorithms for general sparse-matrix indexing in distributed memory,\nprovided that the underlying SpGEMM implementation is sufficiently flexible and\nscalable. We demonstrate that our parallel SpGEMM methods, which use\ntwo-dimensional block data distributions with serial hypersparse kernels, are\nindeed highly flexible, scalable, and memory-efficient in the general case.\nThis algorithm is the first to yield increasing speedup on an unbounded number\nof processors; our experiments show scaling up to thousands of processors in a\nvariety of test scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 16 Sep 2011 23:25:28 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2012 20:41:41 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Buluc", "Aydin", ""], ["Gilbert", "John", ""]]}, {"id": "1109.5981", "submitter": "Xiangrui Meng", "authors": "Xiangrui Meng and Michael A. Saunders, and Michael W. Mahoney", "title": "LSRN: A Parallel Iterative Solver for Strongly Over- or Under-Determined\n  Systems", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a parallel iterative least squares solver named \\texttt{LSRN}\nthat is based on random normal projection. \\texttt{LSRN} computes the\nmin-length solution to $\\min_{x \\in \\mathbb{R}^n} \\|A x - b\\|_2$, where $A \\in\n\\mathbb{R}^{m \\times n}$ with $m \\gg n$ or $m \\ll n$, and where $A$ may be\nrank-deficient. Tikhonov regularization may also be included. Since $A$ is only\ninvolved in matrix-matrix and matrix-vector multiplications, it can be a dense\nor sparse matrix or a linear operator, and \\texttt{LSRN} automatically speeds\nup when $A$ is sparse or a fast linear operator. The preconditioning phase\nconsists of a random normal projection, which is embarrassingly parallel, and a\nsingular value decomposition of size $\\lceil \\gamma \\min(m,n) \\rceil \\times\n\\min(m,n)$, where $\\gamma$ is moderately larger than 1, e.g., $\\gamma = 2$. We\nprove that the preconditioned system is well-conditioned, with a strong\nconcentration result on the extreme singular values, and hence that the number\nof iterations is fully predictable when we apply LSQR or the Chebyshev\nsemi-iterative method. As we demonstrate, the Chebyshev method is particularly\nefficient for solving large problems on clusters with high communication cost.\nNumerical results demonstrate that on a shared-memory machine, \\texttt{LSRN}\noutperforms LAPACK's DGELSD on large dense problems, and MATLAB's backslash\n(SuiteSparseQR) on sparse problems. Further experiments demonstrate that\n\\texttt{LSRN} scales well on an Amazon Elastic Compute Cloud cluster.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2011 18:06:44 GMT"}, {"version": "v2", "created": "Sat, 18 Feb 2012 20:39:18 GMT"}], "update_date": "2012-02-21", "authors_parsed": [["Meng", "Xiangrui", ""], ["Saunders", "Michael A.", ""], ["Mahoney", "Michael W.", ""]]}]