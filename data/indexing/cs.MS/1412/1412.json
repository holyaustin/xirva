[{"id": "1412.0436", "submitter": "Luis Torgo", "authors": "Luis Torgo", "title": "An Infra-Structure for Performance Estimation and Experimental\n  Comparison of Predictive Models in R", "comments": "Updated to version 1.0.2 of the R package. Added a small section on\n  package installation. Made explicit the reference to the R package version\n  number within the document", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG cs.SE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes an infra-structure provided by the R package\nperformanceEstimation that allows to estimate the predictive performance of\ndifferent approaches (workflows) to predictive tasks. The infra-structure is\ngeneric in the sense that it can be used to estimate the values of any\nperformance metrics, for any workflow on different predictive tasks, namely,\nclassification, regression and time series tasks. The package also includes\nseveral standard workflows that allow users to easily set up their experiments\nlimiting the amount of work and information they need to provide. The overall\ngoal of the infra-structure provided by our package is to facilitate the task\nof estimating the predictive performance of different modeling approaches to\npredictive tasks in the R environment.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 11:35:47 GMT"}, {"version": "v2", "created": "Sat, 6 Dec 2014 18:13:14 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2015 09:40:18 GMT"}, {"version": "v4", "created": "Mon, 7 Sep 2015 15:03:45 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Torgo", "Luis", ""]]}, {"id": "1412.2562", "submitter": "Vincent Delos", "authors": "Vincent Delos (I2M), Denis Teissandier (I2M)", "title": "Minkowski sum of HV-polytopes in Rn", "comments": "4th Annual International Conference on Computational Mathematics,\n  Computational Geometry and Statistics, Jan 2015, Singapore, Singapore", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.MS physics.class-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minkowski sums cover a wide range of applications in many different fields\nlike algebra, morphing, robotics, mechanical CAD/CAM systems ... This paper\ndeals with sums of polytopes in a n dimensional space provided that both\nH-representation and V-representation are available i.e. the polytopes are\ndescribed by both their half-spaces and vertices. The first method uses the\npolytope normal fans and relies on the ability to intersect dual polyhedral\ncones. Then we introduce another way of considering Minkowski sums of polytopes\nbased on the primal polyhedral cones attached to each vertex.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 13:56:17 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Delos", "Vincent", "", "I2M"], ["Teissandier", "Denis", "", "I2M"]]}, {"id": "1412.2564", "submitter": "Vincent Delos", "authors": "Vincent Delos (I2M), Denis Teissandier (I2M)", "title": "Minkowski Sum of Polytopes Defined by Their Vertices", "comments": null, "journal-ref": "Journal of Applied Mathematics and Physics (JAMP), Scientific\n  Research Publishing, 2015, 3 (1), pp.62-67", "doi": "10.4236/jamp.2015.31008", "report-no": null, "categories": "cs.CG cs.MS physics.class-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minkowski sums are of theoretical interest and have applications in fields\nrelated to industrial backgrounds. In this paper we focus on the specific case\nof summing polytopes as we want to solve the tolerance analysis problem\ndescribed in [1]. Our approach is based on the use of linear programming and is\nsolvable in polynomial time. The algorithm we developed can be implemented and\nparallelized in a very easy way.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 13:57:21 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 09:39:04 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Delos", "Vincent", "", "I2M"], ["Teissandier", "Denis", "", "I2M"]]}, {"id": "1412.3510", "submitter": "Mark Tygert", "authors": "Arthur Szlam, Yuval Kluger, and Mark Tygert", "title": "An implementation of a randomized algorithm for principal component\n  analysis", "comments": "13 pages, 4 figures", "journal-ref": "ACM TOMS, 43(3): 28:1-28:14, 2016", "doi": null, "report-no": null, "categories": "stat.CO cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed intense development of randomized methods for\nlow-rank approximation. These methods target principal component analysis (PCA)\nand the calculation of truncated singular value decompositions (SVD). The\npresent paper presents an essentially black-box, fool-proof implementation for\nMathworks' MATLAB, a popular software platform for numerical computation. As\nillustrated via several tests, the randomized algorithms for low-rank\napproximation outperform or at least match the classical techniques (such as\nLanczos iterations) in basically all respects: accuracy, computational\nefficiency (both speed and memory usage), ease-of-use, parallelizability, and\nreliability. However, the classical procedures remain the methods of choice for\nestimating spectral norms, and are far superior for calculating the least\nsingular values and corresponding singular vectors (or singular subspaces).\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 00:52:41 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Szlam", "Arthur", ""], ["Kluger", "Yuval", ""], ["Tygert", "Mark", ""]]}, {"id": "1412.4564", "submitter": "Karel Lenc", "authors": "Andrea Vedaldi, Karel Lenc", "title": "MatConvNet - Convolutional Neural Networks for MATLAB", "comments": "Updated for release v1.0-beta20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MatConvNet is an implementation of Convolutional Neural Networks (CNNs) for\nMATLAB. The toolbox is designed with an emphasis on simplicity and flexibility.\nIt exposes the building blocks of CNNs as easy-to-use MATLAB functions,\nproviding routines for computing linear convolutions with filter banks, feature\npooling, and many more. In this manner, MatConvNet allows fast prototyping of\nnew CNN architectures; at the same time, it supports efficient computation on\nCPU and GPU allowing to train complex models on large datasets such as ImageNet\nILSVRC. This document provides an overview of CNNs and how they are implemented\nin MatConvNet and gives the technical details of each computational block in\nthe toolbox.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 12:23:35 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2015 15:35:25 GMT"}, {"version": "v3", "created": "Thu, 5 May 2016 14:31:06 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Vedaldi", "Andrea", ""], ["Lenc", "Karel", ""]]}, {"id": "1412.4690", "submitter": "Dominic Searson", "authors": "Dominic P. Searson", "title": "GPTIPS 2: an open-source software platform for symbolic data mining", "comments": "26 pages, accepted for publication in the Springer Handbook of\n  Genetic Programming Applications (2015, in press)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPTIPS is a free, open source MATLAB based software platform for symbolic\ndata mining (SDM). It uses a multigene variant of the biologically inspired\nmachine learning method of genetic programming (MGGP) as the engine that drives\nthe automatic model discovery process. Symbolic data mining is the process of\nextracting hidden, meaningful relationships from data in the form of symbolic\nequations. In contrast to other data-mining methods, the structural\ntransparency of the generated predictive equations can give new insights into\nthe physical systems or processes that generated the data. Furthermore, this\ntransparency makes the models very easy to deploy outside of MATLAB. The\nrationale behind GPTIPS is to reduce the technical barriers to using,\nunderstanding, visualising and deploying GP based symbolic models of data,\nwhilst at the same time remaining highly customisable and delivering robust\nnumerical performance for power users. In this chapter, notable new features of\nthe latest version of the software are discussed with these aims in mind.\nAdditionally, a simplified variant of the MGGP high level gene crossover\nmechanism is proposed. It is demonstrated that the new functionality of GPTIPS\n2 (a) facilitates the discovery of compact symbolic relationships from data\nusing multiple approaches, e.g. using novel gene-centric visualisation analysis\nto mitigate horizontal bloat and reduce complexity in multigene symbolic\nregression models (b) provides numerous methods for visualising the properties\nof symbolic models (c) emphasises the generation of graphically navigable\nlibraries of models that are optimal in terms of the Pareto trade off surface\nof model performance and complexity and (d) expedites real world applications\nby the simple, rapid and robust deployment of symbolic models outside the\nsoftware environment they were developed in.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 17:36:45 GMT"}, {"version": "v2", "created": "Fri, 22 May 2015 08:46:46 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Searson", "Dominic P.", ""]]}, {"id": "1412.4825", "submitter": "Alireza Mahani", "authors": "Alireza S. Mahani, Mansour T.A. Sharabiani", "title": "Efficient SIMD RNG for Varying-Parameter Streams: C++ Class BatchRNG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-Instruction, Multiple-Data (SIMD) random number generators (RNGs) take\nadvantage of vector units to offer significant performance gain over\nnon-vectorized libraries, but they often rely on batch production of deviates\nfrom distributions with fixed parameters. In many statistical applications such\nas Gibbs sampling, parameters of sampled distributions change from one\niteration to the next, requiring that random deviates be generated\none-at-a-time. This situation can render vectorized RNGs inefficient, and even\ninferior to their scalar counterparts. The C++ class BatchRNG uses buffers of\nbase distributions such uniform, Gaussian and exponential to take advantage of\nvector units while allowing for sequences of deviates to be generated with\nvarying parameters. These small buffers are consumed and replenished as needed\nduring a program execution. Performance tests using Intel Vector Statistical\nLibrary (VSL) on various probability distributions illustrates the\neffectiveness of the proposed batching strategy.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 22:27:57 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Mahani", "Alireza S.", ""], ["Sharabiani", "Mansour T. A.", ""]]}, {"id": "1412.5316", "submitter": "Evgeny Latkin", "authors": "Evgeny Latkin", "title": "Twofolds in C and C++", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here I propose C and C++ interfaces and experimental implementation for\ntwofolds arithmetic. I introduce twofolds in my previous article entitled\n\"Twofold fast arithmetic\" for tracking floating-point inaccuracy. Testing\nshows, plain C enables high-performance computing with twofolds. C++ interface\nenables coding as easily as ordinary floating-point numbers. My goal is\nconvincing you to try twofolds; I think assuring accuracy of math computations\nis worth its cost. Code and use examples available at my web site, references\ninside.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 10:06:49 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Latkin", "Evgeny", ""]]}, {"id": "1412.5720", "submitter": "David Budden", "authors": "Madison Flannery, David M Budden and Alexandre Mendes", "title": "FlexDM: Enabling robust and reliable parallel data mining using WEKA", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing massive data mining experiments with multiple datasets and methods\nis a common task faced by most bioinformatics and computational biology\nlaboratories. WEKA is a machine learning package designed to facilitate this\ntask by providing tools that allow researchers to select from several\nclassification methods and specific test strategies. Despite its popularity,\nthe current WEKA environment for batch experiments, namely Experimenter, has\nfour limitations that impact its usability: the selection of value ranges for\nmethods options lacks flexibility and is not intuitive; there is no support for\nparallelisation when running large-scale data mining tasks; the XML schema is\ndifficult to read, necessitating the use of the Experimenter's graphical user\ninterface for generation and modification; and robustness is limited by the\nfact that results are not saved until the last test has concluded.\n  FlexDM implements an interface to WEKA to run batch processing tasks in a\nsimple and intuitive way. In a short and easy-to-understand XML file, one can\ndefine hundreds of tests to be performed on several datasets. FlexDM also\nallows those tests to be executed asynchronously in parallel to take advantage\nof multi-core processors, significantly increasing usability and productivity.\nResults are saved incrementally for better robustness and reliability.\n  FlexDM is implemented in Java and runs on Windows, Linux and OSX. As we\nencourage other researchers to explore and adopt our software, FlexDM is made\navailable as a pre-configured bootable reference environment. All code,\nsupporting documentation and usage examples are also available for download at\nhttp://sourceforge.net/projects/flexdm.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 05:07:44 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Flannery", "Madison", ""], ["Budden", "David M", ""], ["Mendes", "Alexandre", ""]]}, {"id": "1412.6367", "submitter": "Pierre de Buyl", "authors": "J\\'er\\^ome Kieffer, Giannis Ashiotis", "title": "PyFAI: a Python library for high performance azimuthal integration on\n  GPU", "comments": "Part of the Proceedings of the 7th European Conference on Python in\n  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2014-02", "categories": "astro-ph.IM cs.DC cs.MS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The pyFAI package has been designed to reduce X-ray diffraction images into\npowder diffraction curves to be further processed by scientists. This\ncontribution describes how to convert an image into a radial profile using the\nNumpy package, how the process was accelerated using Cython. The algorithm was\nparallelised, needing a complete re-design to benefit from massively parallel\ndevices like graphical processing units or accelerators like the Intel Xeon Phi\nusing the PyOpenCL library.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 15:06:50 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Kieffer", "J\u00e9r\u00f4me", ""], ["Ashiotis", "Giannis", ""]]}, {"id": "1412.6395", "submitter": "Pierre de Buyl", "authors": "Esteban Fuentes, Hector E. Martinez", "title": "SClib, a hack for straightforward embedded C functions in Python", "comments": "Part of the Proceedings of the 7th European Conference on Python in\n  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2014-08", "categories": "cs.MS physics.comp-ph", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We present SClib, a simple hack that allows easy and straightforward\nevaluation of C functions within Python code, boosting flexibility for better\ntrade-off between computation power and feature availability, such as\nvisualization and existing computation routines in SciPy. We also present two\ncases were SClib has been used. In the first set of applications we use SClib\nto write a port to Python of a Schr\\\"odinger equation solver that has been\nextensively used the literature, the resulting script presents a speed-up of\nabout 150x with respect to the original one. A review of the situations where\nthe speeded-up script has been used is presented. We also describe the solution\nto the related problem of solving a set of coupled Schr\\\"odinger-like equations\nwhere SClib is used to implement the speed-critical parts of the code. We argue\nthat when using SClib within IPython we can use NumPy and Matplotlib for the\nmanipulation and visualization of the solutions in an interactive environment\nwith no performance compromise. The second case is an engineering application.\nWe use SClib to evaluate the control and system derivatives in a feedback\ncontrol loop for electrical motors. With this and the integration routines\navailable in SciPy, we can run simulations of the control loop a la Simulink.\nThe use of C code not only boosts the speed of the simulations, but also\nenables to test the exact same code that we use in the test rig to get\nexperimental results. Again, integration with IPython gives us the flexibility\nto analyze and visualize the data.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 15:51:21 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Fuentes", "Esteban", ""], ["Martinez", "Hector E.", ""]]}, {"id": "1412.6407", "submitter": "Pierre de Buyl", "authors": "Robert Cimrman", "title": "Enhancing SfePy with Isogeometric Analysis", "comments": "Part of the Proceedings of the 7th European Conference on Python in\n  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2014-11", "categories": "cs.MS cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In the paper a recent enhancement to the open source package SfePy (Simple\nFinite Elements in Python, http://sfepy.org) is introduced, namely the addition\nof another numerical discretization scheme, the isogeometric analysis, to the\noriginal implementation based on the nowadays standard and well-established\nnumerical solution technique, the finite element method. The isogeometric\nremoves the need of the solution domain approximation by a piece-wise polygonal\ndomain covered by the finite element mesh, and allows approximation of unknown\nfields with a higher smoothness then the finite element method, which can be\nadvantageous in many applications. Basic numerical examples illustrating the\nimplementation and use of the isogeometric analysis in SfePy are shown.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 16:03:26 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Cimrman", "Robert", ""]]}, {"id": "1412.6890", "submitter": "Balasubramanian Narasimhan", "authors": "Balasubramanian Narasimhan, Daniel L. Rubin, Samuel M. Gross, Marina\n  Bendersky, Philip W. Lavori", "title": "Software for Distributed Computation on Medical Databases: A\n  Demonstration Project", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.MS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bringing together the information latent in distributed medical databases\npromises to personalize medical care by enabling reliable, stable modeling of\noutcomes with rich feature sets (including patient characteristics and\ntreatments received). However, there are barriers to aggregation of medical\ndata, due to lack of standardization of ontologies, privacy concerns,\nproprietary attitudes toward data, and a reluctance to give up control over end\nuse. Aggregation of data is not always necessary for model fitting. In models\nbased on maximizing a likelihood, the computations can be distributed, with\naggregation limited to the intermediate results of calculations on local data,\nrather than raw data. Distributed fitting is also possible for singular value\ndecomposition. There has been work on the technical aspects of shared\ncomputation for particular applications, but little has been published on the\nsoftware needed to support the \"social networking\" aspect of shared computing,\nto reduce the barriers to collaboration. We describe a set of software tools\nthat allow the rapid assembly of a collaborative computational project, based\non the flexible and extensible R statistical software and other open source\npackages, that can work across a heterogeneous collection of database\nenvironments, with full transparency to allow local officials concerned with\nprivacy protections to validate the safety of the method. We describe the\nprinciples, architecture, and successful test results for the site-stratified\nCox model and rank-k Singular Value Decomposition (SVD).\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 07:17:01 GMT"}, {"version": "v2", "created": "Fri, 10 Feb 2017 02:03:23 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Narasimhan", "Balasubramanian", ""], ["Rubin", "Daniel L.", ""], ["Gross", "Samuel M.", ""], ["Bendersky", "Marina", ""], ["Lavori", "Philip W.", ""]]}, {"id": "1412.7030", "submitter": "Pierre de Buyl", "authors": "Pierre de Buyl, Nelle Varoquaux", "title": "Proceedings of the 7th European Conference on Python in Science\n  (EuroSciPy 2014)", "comments": null, "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2014-00", "categories": "cs.CE cs.MS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  These are the proceedings of the 7th European Conference on Python in\nScience, EuroSciPy 2014, that was held in Cambridge, UK (27-30 August 2014).\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 15:47:51 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["de Buyl", "Pierre", ""], ["Varoquaux", "Nelle", ""]]}, {"id": "1412.7160", "submitter": "Marco Selig", "authors": "Marco Selig", "title": "The NIFTY way of Bayesian signal inference", "comments": "6 pages, 2 figures, refereed proceeding of the 33rd International\n  Workshop on Bayesian Inference and Maximum Entropy Methods in Science and\n  Engineering (MaxEnt 2013), software available at\n  http://www.mpa-garching.mpg.de/ift/nifty/ and\n  http://www.mpa-garching.mpg.de/ift/d3po/", "journal-ref": "AIP Conf. Proc. 1636, 68 (2014)", "doi": "10.1063/1.4903712", "report-no": null, "categories": "astro-ph.IM cs.IT cs.MS math.IT physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce NIFTY, \"Numerical Information Field Theory\", a software package\nfor the development of Bayesian signal inference algorithms that operate\nindependently from any underlying spatial grid and its resolution. A large\nnumber of Bayesian and Maximum Entropy methods for 1D signal reconstruction, 2D\nimaging, as well as 3D tomography, appear formally similar, but one often finds\nindividualized implementations that are neither flexible nor easily\ntransferable. Signal inference in the framework of NIFTY can be done in an\nabstract way, such that algorithms, prototyped in 1D, can be applied to real\nworld problems in higher-dimensional settings. NIFTY as a versatile library is\napplicable and already has been applied in 1D, 2D, 3D and spherical settings. A\nrecent application is the D3PO algorithm targeting the non-trivial task of\ndenoising, deconvolving, and decomposing photon observations in high energy\nastronomy.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 21:00:07 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Selig", "Marco", ""]]}]