[{"id": "2010.00257", "submitter": "Simon Heybrock", "authors": "Simon Heybrock, Owen Arnold, Igor Gudich, Daniel Nixon, and Neil\n  Vaytet", "title": "Scipp: Scientific data handling with labeled multi-dimensional arrays\n  for C++ and Python", "comments": "Proceedings of ICANS-XXIII, 13 pages, 5 figures", "journal-ref": "Journal of Neutron Research, vol. Pre-press, no. Pre-press, pp.\n  1-13, 2020", "doi": "10.3233/JNR-190131", "report-no": null, "categories": "cs.MS physics.data-an physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scipp is heavily inspired by the Python library xarray. It enriches raw\nNumPy-like multi-dimensional arrays of data by adding named dimensions and\nassociated coordinates. Multiple arrays are combined into datasets. On top of\nthis, scipp introduces (i) implicit handling of physical units, (ii) implicit\npropagation of uncertainties, (iii) support for histograms, i.e., bin-edge\ncoordinate axes, which exceed the data's dimension extent by one, and (iv)\nsupport for event data. In conjunction these features enable a more natural and\nmore concise user experience. The combination of named dimensions, coordinates,\nand units helps to drastically reduce the risk for programming errors. The core\nof scipp is written in C++ to open opportunities for performance improvements\nthat a Python-based solution would not allow for. On top of the C++ core,\nscipp's Python components provide functionality for plotting and content\nrepresentations, e.g., for use in Jupyter Notebooks. While none of scipp's\nconcepts in isolation is novel per-se, we are not aware of any project\ncombining all of these aspects in a single coherent software package.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 08:59:03 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Heybrock", "Simon", ""], ["Arnold", "Owen", ""], ["Gudich", "Igor", ""], ["Nixon", "Daniel", ""], ["Vaytet", "Neil", ""]]}, {"id": "2010.00724", "submitter": "Amir Shahmoradi", "authors": "Amir Shahmoradi, Fatemeh Bagheri, Joshua Alexander Osborne", "title": "Fast fully-reproducible serial/parallel Monte Carlo and MCMC simulations\n  and visualizations via ParaMonte::Python library", "comments": "to be submitted to JOSS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS astro-ph.IM q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ParaMonte::Python (standing for Parallel Monte Carlo in Python) is a serial\nand MPI-parallelized library of (Markov Chain) Monte Carlo (MCMC) routines for\nsampling mathematical objective functions, in particular, the posterior\ndistributions of parameters in Bayesian modeling and analysis in data science,\nMachine Learning, and scientific inference in general. In addition to providing\naccess to fast high-performance serial/parallel Monte Carlo and MCMC sampling\nroutines, the ParaMonte::Python library provides extensive post-processing and\nvisualization tools that aim to automate and streamline the process of model\ncalibration and uncertainty quantification in Bayesian data analysis.\nFurthermore, the automatically-enabled restart functionality of\nParaMonte::Python samplers ensure seamless fully-deterministic into-the-future\nrestart of Monte Carlo simulations, should any interruptions happen. The\nParaMonte::Python library is MIT-licensed and is permanently maintained on\nGitHub at\nhttps://github.com/cdslaborg/paramonte/tree/master/src/interface/Python.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 23:26:42 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Shahmoradi", "Amir", ""], ["Bagheri", "Fatemeh", ""], ["Osborne", "Joshua Alexander", ""]]}, {"id": "2010.00902", "submitter": "Carlos Palenzuela", "authors": "C.Palenzuela, B.Mi\\~nano, A.Arbona, C.Bona-Casas, C.Bona and J.Mass\\'o", "title": "Simflowny 3: An upgraded platform for scientific modelling and\n  simulation", "comments": "11 pages, 6 figures, accepted in Computer Physics Communications", "journal-ref": null, "doi": "10.1016/j.cpc.2020.107675", "report-no": null, "categories": "physics.comp-ph cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simflowny is an open platform which automatically generates efficient\nparallel code of scientific dynamical models for different simulation\nframeworks. Here we present major upgrades on this software to support\nsimultaneously a quite generic family of partial differential equations. These\nequations can be discretized using: (i) standard finite-difference for systems\nwith derivatives up to any order, (ii) High-Resolution-Shock-Capturing methods\nto deal with shocks and discontinuities of balance law equations, and (iii)\nparticle-based methods. We have improved the adaptive-mesh-refinement\nalgorithms to preserve the convergence order of the numerical methods, which is\na requirement for improving scalability. Finally, we have also extended our\ngraphical user interface (GUI) to accommodate these and future families of\nequations. This paper summarizes the formal representation and implementation\nof these new families, providing several validation results.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 10:04:30 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Palenzuela", "C.", ""], ["Mi\u00f1ano", "B.", ""], ["Arbona", "A.", ""], ["Bona-Casas", "C.", ""], ["Bona", "C.", ""], ["Mass\u00f3", "J.", ""]]}, {"id": "2010.01709", "submitter": "William S. Moses", "authors": "William S. Moses and Valentin Churavy", "title": "Instead of Rewriting Foreign Code for Machine Learning, Automatically\n  Synthesize Fast Gradients", "comments": "To be published in NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.AI cs.LG cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying differentiable programming techniques and machine learning\nalgorithms to foreign programs requires developers to either rewrite their code\nin a machine learning framework, or otherwise provide derivatives of the\nforeign code. This paper presents Enzyme, a high-performance automatic\ndifferentiation (AD) compiler plugin for the LLVM compiler framework capable of\nsynthesizing gradients of statically analyzable programs expressed in the LLVM\nintermediate representation (IR). Enzyme synthesizes gradients for programs\nwritten in any language whose compiler targets LLVM IR including C, C++,\nFortran, Julia, Rust, Swift, MLIR, etc., thereby providing native AD\ncapabilities in these languages. Unlike traditional source-to-source and\noperator-overloading tools, Enzyme performs AD on optimized IR. On a\nmachine-learning focused benchmark suite including Microsoft's ADBench, AD on\noptimized IR achieves a geometric mean speedup of 4.5x over AD on IR before\noptimization allowing Enzyme to achieve state-of-the-art performance. Packaging\nEnzyme for PyTorch and TensorFlow provides convenient access to gradients of\nforeign code with state-of-the art performance, enabling foreign code to be\ndirectly incorporated into existing machine learning workflows.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 22:32:51 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Moses", "William S.", ""], ["Churavy", "Valentin", ""]]}, {"id": "2010.03923", "submitter": "Peter Coveney", "authors": "D. Groen, H. Arabnejad, V. Jancauskas, W. N. Edeling, F. Jansson, R.\n  A. Richardson, J. Lakhlili, L. Veen, B. Bosak, P. Kopta, D. W. Wright, N.\n  Monnier, P. Karlshoefer, D. Suleimenova, R. Sinclair, M. Vassaux, A.\n  Nikishova, M. Bieniek, O. O. Luk, M. Kulczewski, E. Raffin, D. Crommelin, O.\n  Hoenen, D. P. Coster, T. Piontek and P. V. Coveney", "title": "VECMAtk: A Scalable Verification, Validation and Uncertainty\n  Quantification Toolkit for Scientific Simulations", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": "10.1098/rsta.2020.0221", "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the VECMA toolkit (VECMAtk), a flexible software environment for\nsingle and multiscale simulations that introduces directly applicable and\nreusable procedures for verification, validation (V&V), sensitivity analysis\n(SA) and uncertainty quantification (UQ). It enables users to verify key\naspects of their applications, systematically compare and validate the\nsimulation outputs against observational or benchmark data, and run simulations\nconveniently on any platform from the desktop to current multi-petascale\ncomputers. In this sequel to our paper on VECMAtk which we presented last year,\nwe focus on a range of functional and performance improvements that we have\nintroduced, cover newly introduced components, and applications examples from\nseven different domains such as conflict modelling and environmental sciences.\nWe also present several implemented patterns for UQ/SA and V&V, and guide the\nreader through one example concerning COVID-19 modelling in detail.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 12:08:33 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 22:07:38 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Groen", "D.", ""], ["Arabnejad", "H.", ""], ["Jancauskas", "V.", ""], ["Edeling", "W. N.", ""], ["Jansson", "F.", ""], ["Richardson", "R. A.", ""], ["Lakhlili", "J.", ""], ["Veen", "L.", ""], ["Bosak", "B.", ""], ["Kopta", "P.", ""], ["Wright", "D. W.", ""], ["Monnier", "N.", ""], ["Karlshoefer", "P.", ""], ["Suleimenova", "D.", ""], ["Sinclair", "R.", ""], ["Vassaux", "M.", ""], ["Nikishova", "A.", ""], ["Bieniek", "M.", ""], ["Luk", "O. O.", ""], ["Kulczewski", "M.", ""], ["Raffin", "E.", ""], ["Crommelin", "D.", ""], ["Hoenen", "O.", ""], ["Coster", "D. P.", ""], ["Piontek", "T.", ""], ["Coveney", "P. V.", ""]]}, {"id": "2010.03935", "submitter": "Alexander McCaskey", "authors": "Thien Nguyen, Anthony Santana, Tyler Kharazi, Daniel Claudino, Hal\n  Finkel, Alexander McCaskey", "title": "Extending C++ for Heterogeneous Quantum-Classical Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present qcor - a language extension to C++ and compiler implementation\nthat enables heterogeneous quantum-classical programming, compilation, and\nexecution in a single-source context. Our work provides a first-of-its-kind C++\ncompiler enabling high-level quantum kernel (function) expression in a\nquantum-language agnostic manner, as well as a hardware-agnostic, retargetable\ncompiler workflow targeting a number of physical and virtual quantum computing\nbackends. qcor leverages novel Clang plugin interfaces and builds upon the XACC\nsystem-level quantum programming framework to provide a state-of-the-art\nintegration mechanism for quantum-classical compilation that leverages the best\nfrom the community at-large. qcor translates quantum kernels ultimately to the\nXACC intermediate representation, and provides user-extensible hooks for\nquantum compilation routines like circuit optimization, analysis, and\nplacement. This work details the overall architecture and compiler workflow for\nqcor, and provides a number of illuminating programming examples demonstrating\nits utility for near-term variational tasks, quantum algorithm expression, and\nfeed-forward error correction schemes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 12:49:07 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Nguyen", "Thien", ""], ["Santana", "Anthony", ""], ["Kharazi", "Tyler", ""], ["Claudino", "Daniel", ""], ["Finkel", "Hal", ""], ["McCaskey", "Alexander", ""]]}, {"id": "2010.04678", "submitter": "Christos Psarras M.Sc.", "authors": "Christos Psarras, Lars Karlsson and Paolo Bientinesi", "title": "Concurrent Alternating Least Squares for multiple simultaneous Canonical\n  Polyadic Decompositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor decompositions, such as CANDECOMP/PARAFAC (CP), are widely used in a\nvariety of applications, such as chemometrics, signal processing, and machine\nlearning. A broadly used method for computing such decompositions relies on the\nAlternating Least Squares (ALS) algorithm. When the number of components is\nsmall, regardless of its implementation, ALS exhibits low arithmetic intensity,\nwhich severely hinders its performance and makes GPU offloading ineffective. We\nobserve that, in practice, experts often have to compute multiple\ndecompositions of the same tensor, each with a small number of components\n(typically fewer than 20), to ultimately find the best ones to use for the\napplication at hand. In this paper, we illustrate how multiple decompositions\nof the same tensor can be fused together at the algorithmic level to increase\nthe arithmetic intensity. Therefore, it becomes possible to make efficient use\nof GPUs for further speedups; at the same time the technique is compatible with\nmany enhancements typically used in ALS, such as line search, extrapolation,\nand non-negativity constraints. We introduce the Concurrent ALS algorithm and\nlibrary, which offers an interface to Matlab, and a mechanism to effectively\ndeal with the issue that decompositions complete at different times.\nExperimental results on artificial and real datasets demonstrate a shorter time\nto completion due to increased arithmetic intensity.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 16:55:46 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Psarras", "Christos", ""], ["Karlsson", "Lars", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "2010.04868", "submitter": "Liang Yuan", "authors": "Liang Yuan and Hang Cao and Yunquan Zhang and Kun Li and Pengqi Lu and\n  Yue Yue", "title": "Temporal Vectorization for Stencils", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stencil computations represent a very common class of nested loops in\nscientific and engineering applications. Exploiting vector units in modern CPUs\nis crucial to achieving peak performance. Previous vectorization approaches\noften consider the data space, in particular the innermost unit-strided loop.\nIt leads to the well-known data alignment conflict problem that vector loads\nare overlapped due to the data sharing between continuous stencil computations.\nThis paper proposes a novel temporal vectorization scheme for stencils. It\nvectorizes the stencil computation in the iteration space and assembles points\nwith different time coordinates in one vector. The temporal vectorization leads\nto a small fixed number of vector reorganizations that is irrelevant to the\nvector length, stencil order, and dimension. Furthermore, it is also applicable\nto Gauss-Seidel stencils, whose vectorization is not well-studied. The\neffectiveness of the temporal vectorization is demonstrated by various Jacobi\nand Gauss-Seidel stencils.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 01:47:34 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Yuan", "Liang", ""], ["Cao", "Hang", ""], ["Zhang", "Yunquan", ""], ["Li", "Kun", ""], ["Lu", "Pengqi", ""], ["Yue", "Yue", ""]]}, {"id": "2010.07097", "submitter": "Daniel Wilczak", "authors": "Tomasz Kapela, Marian Mrozek, Daniel Wilczak, Piotr Zgliczy\\'nski", "title": "CAPD::DynSys: a flexible C++ toolbox for rigorous numerical analysis of\n  dynamical systems", "comments": "25 pages, 4 figures, 11 full C++ examples", "journal-ref": null, "doi": "10.1016/j.cnsns.2020.105578", "report-no": null, "categories": "math.NA cs.MS cs.NA math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the CAPD::DynSys library for rigorous numerical analysis of\ndynamical systems. The basic interface is described together with several\ninteresting case studies illustrating how it can be used for computer-assisted\nproofs in dynamics of ODEs.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:53:54 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Kapela", "Tomasz", ""], ["Mrozek", "Marian", ""], ["Wilczak", "Daniel", ""], ["Zgliczy\u0144ski", "Piotr", ""]]}, {"id": "2010.07906", "submitter": "Sebastiano Vascon Mr", "authors": "Sebastiano Vascon, Samuel Rota Bul\\`o, Vittorio Murino, Marcello\n  Pelillo", "title": "DSLib: An open source library for the dominant set clustering method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  DSLib is an open-source implementation of the Dominant Set (DS) clustering\nalgorithm written entirely in Matlab. The DS method is a graph-based clustering\ntechnique rooted in the evolutionary game theory that starts gaining lots of\ninterest in the computer science community. Thanks to its duality with game\ntheory and its strict relation to the notion of maximal clique, has been\nexplored in several directions not only related to clustering problems.\nApplications in graph matching, segmentation, classification and medical\nimaging are common in literature. This package provides an implementation of\nthe original DS clustering algorithm since no code has been officially released\nyet, together with a still growing collection of methods and variants related\nto it. Our library is integrable into a Matlab pipeline without dependencies,\nit is simple to use and easily extendable for upcoming works. The latest source\ncode, the documentation and some examples can be downloaded from\nhttps://xwasco.github.io/DominantSetLibrary.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 17:36:48 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Vascon", "Sebastiano", ""], ["Bul\u00f2", "Samuel Rota", ""], ["Murino", "Vittorio", ""], ["Pelillo", "Marcello", ""]]}, {"id": "2010.09860", "submitter": "Matthew Roughan", "authors": "Matthew Roughan", "title": "The Polylogarithm Function in Julia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The polylogarithm function is one of the constellation of important\nmathematical functions. It has a long history, and many connections to other\nspecial functions and series, and many applications, for instance in\nstatistical physics. However, the practical aspects of its numerical evaluation\nhave not received the type of comprehensive treatments lavished on its\nsiblings. Only a handful of formal publications consider the evaluation of the\nfunction, and most focus on a specific domain and/or presume arbitrary\nprecision arithmetic will be used. And very little of the literature contains\nany formal validation of numerical performance. In this paper we present an\nalgorithm for calculating polylogarithms for both complex parameter and\nargument and evaluate it thoroughly in comparison to the arbitrary precision\nimplementation in Mathematica. The implementation was created in a new\nscientific computing language Julia, which is ideal for the purpose, but also\nallows us to write the code in a simple, natural manner so as to make it easy\nto port the implementation to other such languages.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 06:35:17 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Roughan", "Matthew", ""]]}, {"id": "2010.10248", "submitter": "George Bisbas", "authors": "George Bisbas, Fabio Luporini, Mathias Louboutin, Rhodri Nelson,\n  Gerard Gorman, Paul H. J. Kelly", "title": "Temporal blocking of finite-difference stencil operators with sparse\n  \"off-the-grid\" sources", "comments": "Accepted for publication at 35th IEEE International Parallel &\n  Distributed Processing Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stencil kernels dominate a range of scientific applications, including\nseismic and medical imaging, image processing, and neural networks. Temporal\nblocking is a performance optimization that aims to reduce the required memory\nbandwidth of stencil computations by re-using data from the cache for multiple\ntime steps. It has already been shown to be beneficial for this class of\nalgorithms. However, applying temporal blocking to practical applications'\nstencils remains challenging. These computations often consist of sparsely\nlocated operators not aligned with the computational grid (\"off-the-grid\"). Our\nwork is motivated by modeling problems in which source injections result in\nwavefields that must then be measured at receivers by interpolation from the\ngrided wavefield. The resulting data dependencies make the adoption of temporal\nblocking much more challenging. We propose a methodology to inspect these data\ndependencies and reorder the computation, leading to performance gains in\nstencil codes where temporal blocking has not been applicable. We implement\nthis novel scheme in the Devito domain-specific compiler toolchain. Devito\nimplements a domain-specific language embedded in Python to generate optimized\npartial differential equation solvers using the finite-difference method from\nhigh-level symbolic problem definitions. We evaluate our scheme using isotropic\nacoustic, anisotropic acoustic, and isotropic elastic wave propagators of\nindustrial significance. After auto-tuning, performance evaluation shows that\nthis enables substantial performance improvement through temporal blocking over\nhighly-optimized vectorized spatially-blocked code of up to 1.6x.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 13:20:07 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 14:26:45 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Bisbas", "George", ""], ["Luporini", "Fabio", ""], ["Louboutin", "Mathias", ""], ["Nelson", "Rhodri", ""], ["Gorman", "Gerard", ""], ["Kelly", "Paul H. J.", ""]]}, {"id": "2010.13513", "submitter": "Nils Kohl", "authors": "Nils Kohl, Ulrich R\\\"ude", "title": "Textbook efficiency: massively parallel matrix-free multigrid for the\n  Stokes system", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We employ textbook multigrid efficiency (TME), as introduced by Achi Brandt,\nto construct an asymptotically optimal monolithic multigrid solver for the\nStokes system. The geometric multigrid solver builds upon the concept of\nhierarchical hybrid grids (HHG), which is extended to higher-order\nfinite-element discretizations, and a corresponding matrix-free implementation.\nThe computational cost of the full multigrid (FMG) iteration is quantified, and\nthe solver is applied to multiple benchmark problems. Through a parameter\nstudy, we suggest configurations that achieve TME for both, stabilized\nequal-order, and Taylor-Hood discretizations. The excellent node-level\nperformance of the relevant compute kernels is presented via a roofline\nanalysis. Finally, we demonstrate the weak and strong scalability to up to\n$147,456$ parallel processes and solve Stokes systems with more than $3.6\n\\times 10^{12}$ (trillion) unknowns.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 12:11:55 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Kohl", "Nils", ""], ["R\u00fcde", "Ulrich", ""]]}, {"id": "2010.13887", "submitter": "Yang Wei", "authors": "Xiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang, Lei Li", "title": "LightSeq: A High Performance Inference Library for Transformers", "comments": "8 pages, 6 figures, accepted by NAACL 2021 Industry Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformer, BERT and their variants have achieved great success in natural\nlanguage processing. Since Transformer models are huge in size, serving these\nmodels is a challenge for real industrial applications. In this paper, we\npropose LightSeq, a highly efficient inference library for models in the\nTransformer family. LightSeq includes a series of GPU optimization techniques\nto to streamline the computation of neural layers and to reduce memory\nfootprint. LightSeq can easily import models trained using PyTorch and\nTensorflow. Experimental results on machine translation benchmarks show that\nLightSeq achieves up to 14x speedup compared with TensorFlow and 1.4x compared\nwith FasterTransformer, a concurrent CUDA implementation. The code is available\nat https://github.com/bytedance/lightseq.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 13:45:26 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 02:47:33 GMT"}, {"version": "v3", "created": "Tue, 30 Mar 2021 07:17:59 GMT"}, {"version": "v4", "created": "Thu, 22 Apr 2021 09:37:37 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Wang", "Xiaohui", ""], ["Xiong", "Ying", ""], ["Wei", "Yang", ""], ["Wang", "Mingxuan", ""], ["Li", "Lei", ""]]}, {"id": "2010.14132", "submitter": "Nick Brown", "authors": "Nick Brown", "title": "A comparison of techniques for solving the Poisson equation in CFD", "comments": "Pre-print of paper in the Journal of Civil Aircraft Design and\n  Research 2017-03", "journal-ref": "Journal of Civil Aircraft Design and Research pages 85-94 2017-03", "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CFD is a ubiquitous technique central to much of computational simulation\nsuch as that required by aircraft design. Solving of the Poisson equation\noccurs frequently in CFD and there are a number of possible approaches one may\nleverage. The dynamical core of the MONC atmospheric model is one example of\nCFD which requires the solving of the Poisson equation to determine pressure\nterms. Traditionally this aspect of the model has been very time consuming\nand-so it is important to consider how we might reduce the runtime cost.\n  In this paper we survey the different approaches implemented in MONC to\nperform the pressure solve. Designed to take advantage of large scale, modern,\nHPC machines, we are concerned with the computation and communication behaviour\nof the available techniques and in this text we focus on direct FFT and\nindirect iterative methods. In addition to describing the implementation of\nthese techniques we illustrate on up to 32768 processor cores of a Cray XC30\nboth the performance and scalability of our approaches. Raw runtime is not the\nonly measure so we also make some comments around the stability and accuracy of\nsolution. The result of this work are a number of techniques, optimised for\nlarge scale HPC systems, and an understanding of which is most appropriate in\ndifferent situations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 08:43:53 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Brown", "Nick", ""]]}, {"id": "2010.14734", "submitter": "Derek Beaton", "authors": "Derek Beaton (1) ((1) Rotman Research Institute, Baycrest Health\n  Sciences)", "title": "Generalized eigen, singular value, and partial least squares\n  decompositions: The GSVD package", "comments": "38 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The generalized singular value decomposition (GSVD, a.k.a. \"SVD triplet\",\n\"duality diagram\" approach) provides a unified strategy and basis to perform\nnearly all of the most common multivariate analyses (e.g., principal\ncomponents, correspondence analysis, multidimensional scaling, canonical\ncorrelation, partial least squares). Though the GSVD is ubiquitous, powerful,\nand flexible, it has very few implementations. Here I introduce the GSVD\npackage for R. The general goal of GSVD is to provide a small set of accessible\nfunctions to perform the GSVD and two other related decompositions (generalized\neigenvalue decomposition, generalized partial least squares-singular value\ndecomposition). Furthermore, GSVD helps provide a more unified conceptual\napproach and nomenclature to many techniques. I first introduce the concept of\nthe GSVD, followed by a formal definition of the generalized decompositions.\nNext I provide some key decisions made during development, and then a number of\nexamples of how to use GSVD to implement various statistical techniques. These\nexamples also illustrate one of the goals of GSVD: how others can (or should)\nbuild analysis packages that depend on GSVD. Finally, I discuss the possible\nfuture of GSVD.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 03:57:27 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 13:24:14 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 23:59:48 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Beaton", "Derek", ""]]}, {"id": "2010.14993", "submitter": "Radoslava Hristova", "authors": "I. Hristov, R. Hristova, S. Dimova, P. Armyanov, N. Shegunov, I.\n  Puzynin, T. Puzynina, Z. Sharipov, Z. Tukhliev", "title": "Parallelizing multiple precision Taylor series method for integrating\n  the Lorenz system", "comments": "arXiv admin note: text overlap with arXiv:1908.09301", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA math.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hybrid MPI+OpenMP strategy for parallelizing multiple precision Taylor\nseries method is proposed, realized and tested. To parallelize the algorithm we\ncombine MPI and OpenMP parallel technologies together with GMP library (GNU\nmiltiple precision libary) and the tiny MPIGMP library. The details of the\nparallelization are explained on the paradigmatic model of the Lorenz system.\nWe succeed to obtain a correct reference solution in the rather long time\ninterval - [0,7000]. The solution is verified by comparing the results for\n2700-th order Taylor series method and precision of ~ 3374 decimal digits, and\nthose with 2800-th order and precision of ~ 3510 decimal digits. With 192 CPU\ncores in Nestum cluster, Sofia, Bulgaria, the 2800-th order computation was ~\n145 hours with speedup ~ 105.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 09:28:08 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 17:41:26 GMT"}, {"version": "v3", "created": "Fri, 27 Nov 2020 17:42:38 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Hristov", "I.", ""], ["Hristova", "R.", ""], ["Dimova", "S.", ""], ["Armyanov", "P.", ""], ["Shegunov", "N.", ""], ["Puzynin", "I.", ""], ["Puzynina", "T.", ""], ["Sharipov", "Z.", ""], ["Tukhliev", "Z.", ""]]}, {"id": "2010.16114", "submitter": "Seyoon Ko", "authors": "Seyoon Ko, Hua Zhou, Jin Zhou, and Joong-Ho Won", "title": "DistStat.jl: Towards Unified Programming for High-Performance\n  Statistical Computing Environments in Julia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for high-performance computing (HPC) is ever-increasing for\neveryday statistical computing purposes. The downside is that we need to write\nspecialized code for each HPC environment. CPU-level parallelization needs to\nbe explicitly coded for effective use of multiple nodes in cluster\nsupercomputing environments. Acceleration via graphics processing units (GPUs)\nrequires to write kernel code. The Julia software package DistStat.jl\nimplements a data structure for distributed arrays that work on both multi-node\nCPU clusters and multi-GPU environments transparently. This package paves a way\nto developing high-performance statistical software in various HPC environments\nsimultaneously. As a demonstration of the transparency and scalability of the\npackage, we provide applications to large-scale nonnegative matrix\nfactorization, multidimensional scaling, and $\\ell_1$-regularized Cox\nproportional hazards model on an 8-GPU workstation and a 720-CPU-core virtual\ncluster in Amazon Web Services (AWS) cloud. As a case in point, we analyze the\non-set of type-2 diabetes from the UK Biobank with 400,000 subjects and 500,000\nsingle nucleotide polymorphisms using the $\\ell_1$-regularized Cox proportional\nhazards model. Fitting a half-million-variate regression model took less than\n50 minutes on AWS.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 08:16:47 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Ko", "Seyoon", ""], ["Zhou", "Hua", ""], ["Zhou", "Jin", ""], ["Won", "Joong-Ho", ""]]}]