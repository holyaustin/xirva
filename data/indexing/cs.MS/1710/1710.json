[{"id": "1710.01133", "submitter": "Cosmin Bonchis", "authors": "Cosmin Bonchis, Eva Kaslik and Florin Rosu", "title": "HPC optimal parallel communication algorithm for the simulation of\n  fractional-order systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A parallel numerical simulation algorithm is presented for fractional-order\nsystems involving Caputo-type derivatives, based on the Adams-Bashforth-Moulton\n(ABM) predictor-corrector scheme. The parallel algorithm is implemented using\nseveral different approaches: a pure MPI version, a combination of MPI with\nOpenMP optimization and a memory saving speedup approach. All tests run on a\nBlueGene/P cluster, and comparative improvement results for the running time\nare provided. As an applied experiment, the solutions of a fractional-order\nversion of a system describing a forced series LCR circuit are numerically\ncomputed, depicting cascades of period-doubling bifurcations which lead to the\nonset of chaotic behavior.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 14:36:55 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Bonchis", "Cosmin", ""], ["Kaslik", "Eva", ""], ["Rosu", "Florin", ""]]}, {"id": "1710.01839", "submitter": "Tomonori Kouya", "authors": "Tomonori Kouya", "title": "Tuning Technique for Multiple Precision Dense Matrix Multiplication\n  using Prediction of Computational Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although reliable long precision floating-point arithmetic libraries such as\nQD and MPFR/GMP are necessary to solve ill-conditioned problems in numerical\nsimulation, long precision BLAS-level computation such as matrix multiplication\nhas not been fully optimized because tuning costs are very high compared to\nIEEE float and double precision arithmetic. In this study, we develop a\ntechnique to shorten this tuning time by using prediction of computational\ntimes in several block sizes for the blocking algorithm, and then selecting the\nfastest matrix multiplication method for tuning multiple precision dense real\nmatrix multiplication in various precisions, matrix sizes, and degrees of\nparallelization.\n", "versions": [{"version": "v1", "created": "Thu, 5 Oct 2017 00:50:37 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Kouya", "Tomonori", ""]]}, {"id": "1710.03462", "submitter": "Holger Homann", "authors": "Holger Homann and Francois Laenen", "title": "SoAx: A generic C++ Structure of Arrays for handling Particles in HPC\n  Codes", "comments": null, "journal-ref": null, "doi": "10.1016/j.cpc.2017.11.015", "report-no": null, "categories": "physics.comp-ph cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The numerical study of physical problems often require integrating the\ndynamics of a large number of particles evolving according to a given set of\nequations. Particles are characterized by the information they are carrying\nsuch as an identity, a position other. There are generally speaking two\ndifferent possibilities for handling particles in high performance computing\n(HPC) codes. The concept of an Array of Structures (AoS) is in the spirit of\nthe object-oriented programming (OOP) paradigm in that the particle information\nis implemented as a structure. Here, an object (realization of the structure)\nrepresents one particle and a set of many particles is stored in an array. In\ncontrast, using the concept of a Structure of Arrays (SoA), a single structure\nholds several arrays each representing one property (such as the identity) of\nthe whole set of particles.\n  The AoS approach is often implemented in HPC codes due to its handiness and\nflexibility. For a class of problems, however, it is know that the performance\nof SoA is much better than that of AoS. We confirm this observation for our\nparticle problem. Using a benchmark we show that on modern Intel Xeon\nprocessors the SoA implementation is typically several times faster than the\nAoS one. On Intel's MIC co-processors the performance gap even attains a factor\nof ten. The same is true for GPU computing, using both computational and\nmulti-purpose GPUs.\n  Combining performance and handiness, we present the library SoAx that has\noptimal performance (on CPUs, MICs, and GPUs) while providing the same\nhandiness as AoS. For this, SoAx uses modern C++ design techniques such\ntemplate meta programming that allows to automatically generate code for user\ndefined heterogeneous data structures.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 09:13:54 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Homann", "Holger", ""], ["Laenen", "Francois", ""]]}, {"id": "1710.03940", "submitter": "Denis Demidov", "authors": "Denis Demidov and Riccardo Rossi", "title": "Subdomain Deflation Combined with Local AMG: a Case Study Using AMGCL\n  Library", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": "10.1134/S1995080220040071", "report-no": null, "categories": "cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a combination of the subdomain deflation method and local\nalgebraic multigrid as a scalable distributed memory preconditioner that is\nable to solve large linear systems of equations. The implementation of the\nalgorithm is made available for the community as part of an open source AMGCL\nlibrary. The solution targets both homogeneous (CPU-only) and heterogeneous\n(CPU/GPU) systems, employing hybrid MPI/OpenMP approach in the former and a\ncombination of MPI, OpenMP, and CUDA in the latter cases. The use of OpenMP\nminimizes the number of MPI processes, thus reducing the communication overhead\nof the deflation method and improving both weak and strong scalability of the\npreconditioner. The examples of scalar, Poisson-like, systems as well as\nnon-scalar problems, stemming out of the discretization of the Navier-Stokes\nequations, are considered in order to estimate performance of the implemented\nalgorithm. A comparison with a traditional global AMG preconditioner based on a\nwell-established Trilinos ML package is provided.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 07:25:16 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 10:54:14 GMT"}, {"version": "v3", "created": "Thu, 24 May 2018 08:19:38 GMT"}, {"version": "v4", "created": "Fri, 26 Oct 2018 06:20:18 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Demidov", "Denis", ""], ["Rossi", "Riccardo", ""]]}, {"id": "1710.04286", "submitter": "Devangi Parikh", "authors": "Devangi N. Parikh, Maggie E. Myers, Robert A. van de Geijn", "title": "Deriving Correct High-Performance Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": "FLAME Working Note #86, The University of Texas at Austin,\n  Department of Computer Science, Technical Report TR-17-07", "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dijkstra observed that verifying correctness of a program is difficult and\nconjectured that derivation of a program hand-in-hand with its proof of\ncorrectness was the answer. We illustrate this goal-oriented approach by\napplying it to the domain of dense linear algebra libraries for distributed\nmemory parallel computers. We show that algorithms that underlie the\nimplementation of most functionality for this domain can be systematically\nderived to be correct. The benefit is that an entire family of algorithms for\nan operation is discovered so that the best algorithm for a given architecture\ncan be chosen. This approach is very practical: Ideas inspired by it have been\nused to rewrite the dense linear algebra software stack starting below the\nBasic Linear Algebra Subprograms (BLAS) and reaching up through the Elemental\ndistributed memory library, and every level in between. The paper demonstrates\nhow formal methods and rigorous mathematical techniques for correctness impact\nHPC.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 20:04:43 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Parikh", "Devangi N.", ""], ["Myers", "Maggie E.", ""], ["van de Geijn", "Robert A.", ""]]}, {"id": "1710.04985", "submitter": "Ruipeng Li", "authors": "Ruipeng Li", "title": "On Parallel Solution of Sparse Triangular Linear Systems in CUDA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The acceleration of sparse matrix computations on modern many-core\nprocessors, such as the graphics processing units (GPUs), has been recognized\nand studied over a decade. Significant performance enhancements have been\nachieved for many sparse matrix computational kernels such as sparse\nmatrix-vector products and sparse matrix-matrix products. Solving linear\nsystems with sparse triangular structured matrices is another important sparse\nkernel as demanded by a variety of scientific and engineering applications such\nas sparse linear solvers. However, the development of efficient parallel\nalgorithms in CUDA for solving sparse triangular linear systems remains a\nchallenging task due to the inherently sequential nature of the computation. In\nthis paper, we will revisit this problem by reviewing the existing\nlevel-scheduling methods and proposing algorithms with self-scheduling\ntechniques. Numerical results have indicated that the CUDA implementations of\nthe proposed algorithms can outperform the state-of-the-art solvers in cuSPARSE\nby a factor of up to $2.6$ for structured model problems and general sparse\nmatrices.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 16:15:41 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Li", "Ruipeng", ""]]}, {"id": "1710.07226", "submitter": "Issaku Kanamori", "authors": "Issaku Kanamori and Hideo Matsufuru", "title": "Wilson and Domainwall Kernels on Oakforest-PACS", "comments": "8 pages, 9 figures, Proceedings for the 35th International Symposium\n  on Lattice Field Theory (Lattice 2017)", "journal-ref": null, "doi": "10.1051/epjconf/201817509002", "report-no": null, "categories": "hep-lat cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report the performance of Wilson and Domainwall Kernels on a new Intel\nXeon Phi Knights Landing based machine named Oakforest-PACS, which is co-hosted\nby University of Tokyo and Tsukuba University and is currently fastest in\nJapan. This machine uses Intel Omni-Path for the internode network. We compare\nperformance with several types of implementation including that makes use of\nthe Grid library. The code is incorporated with the code set Bridge++.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 06:24:50 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Kanamori", "Issaku", ""], ["Matsufuru", "Hideo", ""]]}, {"id": "1710.07819", "submitter": "Alberto Paoluzzi", "authors": "Francesco Furiani, Giulio Martella, Alberto Paoluzzi", "title": "Geometric Computing with Chain Complexes: Design and Features of a Julia\n  Package", "comments": "Submitted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geometric computing with chain complexes allows for the computation of the\nwhole chain of linear spaces and (co)boundary operators generated by a space\ndecomposition into a cell complex. The space decomposition is stored and\nhandled with LAR (Linear Algebraic Representation), i.e. with sparse integer\narrays, and allows for using cells of a very general type, even non convex and\nwith internal holes. In this paper we discuss the features and the merits of\nthis approach, and describe the goals and the implementation of a software\npackage aiming at providing for simple and efficient computational support of\ngeometric computing with any kind of meshes, using linear algebra tools with\nsparse matrices. The library is being written in Julia, the novel efficient and\nparallel language for scientific computing. This software, that is being ported\non hybrid architectures (CPU+GPU) of last generation, is yet under development.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 16:03:17 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 09:00:54 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Furiani", "Francesco", ""], ["Martella", "Giulio", ""], ["Paoluzzi", "Alberto", ""]]}, {"id": "1710.08259", "submitter": "Bal\\'azs T\\'oth", "authors": "Balazs Toth", "title": "Nauticle: a general-purpose particle-based simulation tool", "comments": "Submitted manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nauticle is a general-purpose simulation tool for the flexible and highly\nconfigurable application of particle-based methods of either discrete or\ncontinuum phenomena. It is presented that Nauticle has three distinct layers\nfor users and developers, then the top two layers are discussed in detail. The\npaper introduces the Symbolic Form Language (SFL) of Nauticle, which\nfacilitates the formulation of user-defined numerical models at the top level\nin text-based configuration files and provides simple application examples of\nuse. On the other hand, at the intermediate level, it is shown that the SFL can\nbe intuitively extended with new particle methods without tedious recoding or\neven the knowledge of the bottom level. Finally, the efficiency of the code is\nalso tested through a performance benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 13:27:36 GMT"}, {"version": "v2", "created": "Sat, 14 Apr 2018 16:41:41 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Toth", "Balazs", ""]]}, {"id": "1710.08471", "submitter": "Edward Hutter", "authors": "Edward Hutter, Edgar Solomonik", "title": "Communication-avoiding Cholesky-QR2 for rectangular matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable QR factorization algorithms for solving least squares and eigenvalue\nproblems are critical given the increasing parallelism within modern machines.\nWe introduce a more general parallelization of the CholeskyQR2 algorithm and\nshow its effectiveness for a wide range of matrix sizes. Our algorithm executes\nover a 3D processor grid, the dimensions of which can be tuned to trade-off\ncosts in synchronization, interprocessor communication, computational work, and\nmemory footprint. We implement this algorithm, yielding a code that can achieve\na factor of $\\Theta(P^{1/6})$ less interprocessor communication on $P$\nprocessors than any previous parallel QR implementation. Our performance study\non Intel Knights-Landing and Cray XE supercomputers demonstrates the\neffectiveness of this CholeskyQR2 parallelization on a large number of nodes.\nSpecifically, relative to ScaLAPACK's QR, on 1024 nodes of Stampede2, our\nCholeskyQR2 implementation is faster by 2.6x-3.3x in strong scaling tests and\nby 1.1x-1.9x in weak scaling tests.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 19:35:07 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 03:36:15 GMT"}, {"version": "v3", "created": "Sat, 25 Aug 2018 23:36:20 GMT"}, {"version": "v4", "created": "Thu, 18 Oct 2018 00:54:26 GMT"}, {"version": "v5", "created": "Wed, 9 Jan 2019 03:16:52 GMT"}, {"version": "v6", "created": "Sat, 15 Jun 2019 18:52:13 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Hutter", "Edward", ""], ["Solomonik", "Edgar", ""]]}, {"id": "1710.08679", "submitter": "Takuma Yamaguchi", "authors": "Takuma Yamaguchi (1), Kohei Fujita (1 and 2), Tsuyoshi Ichimura (1 and\n  2), Muneo Hori (1 and 2), Maddegedara Lalith (1 and 2) and Kengo Nakajima (3)\n  ((1) Earthquake Research Institute and Department of Civil Engineering, The\n  University of Tokyo, (2) Advanced Institute for Computational Science, RIKEN,\n  (3) Information Technology Center, The University of Tokyo)", "title": "Implicit Low-Order Unstructured Finite-Element Multiple Simulation\n  Enhanced by Dense Computation using OpenACC", "comments": "18 pages, 10 figures, accepted for WACCPD2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a low-order three-dimensional finite-element solver\nfor fast multiple-case crust deformation analysis on GPU-based systems. Based\non a high-performance solver designed for massively parallel CPU based systems,\nwe modify the algorithm to reduce random data access, and then insert OpenACC\ndirectives. The developed solver on ten Reedbush-H nodes (20 P100 GPUs)\nattained speedup of 14.2 times from 20 K computer nodes, which is high\nconsidering the peak memory bandwidth ratio of 11.4 between the two systems. On\nthe newest Volta generation V100 GPUs, the solver attained a further 2.45 times\nspeedup from P100 GPUs. As a demonstrative example, we computed 368 cases of\ncrustal deformation analyses of northeast Japan with 400 million degrees of\nfreedom. The total procedure of algorithm modification and porting\nimplementation took only two weeks; we can see that high performance\nimprovement was achieved with low development cost. With the developed solver,\nwe can expect improvement in reliability of crust-deformation analyses by\nmany-case analyses on a wide range of GPU-based systems.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 09:51:31 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Yamaguchi", "Takuma", "", "1 and 2"], ["Fujita", "Kohei", "", "1 and 2"], ["Ichimura", "Tsuyoshi", "", "1 and\n  2"], ["Hori", "Muneo", "", "1 and 2"], ["Lalith", "Maddegedara", "", "1 and 2"], ["Nakajima", "Kengo", ""]]}, {"id": "1710.08717", "submitter": "Zhenwen Dai", "authors": "Matthias Seeger, Asmus Hetzel, Zhenwen Dai, Eric Meissner, Neil D.\n  Lawrence", "title": "Auto-Differentiating Linear Algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Development systems for deep learning (DL), such as Theano, Torch,\nTensorFlow, or MXNet, are easy-to-use tools for creating complex neural network\nmodels. Since gradient computations are automatically baked in, and execution\nis mapped to high performance hardware, these models can be trained end-to-end\non large amounts of data. However, it is currently not easy to implement many\nbasic machine learning primitives in these systems (such as Gaussian processes,\nleast squares estimation, principal components analysis, Kalman smoothing),\nmainly because they lack efficient support of linear algebra primitives as\ndifferentiable operators. We detail how a number of matrix decompositions\n(Cholesky, LQ, symmetric eigen) can be implemented as differentiable operators.\nWe have implemented these primitives in MXNet, running on CPU and GPU in single\nand double precision. We sketch use cases of these new operators, learning\nGaussian process and Bayesian linear regression models, where we demonstrate\nvery substantial reductions in implementation complexity and running time\ncompared to previous codes. Our MXNet extension allows end-to-end learning of\nhybrid models, which combine deep neural networks (DNNs) with Bayesian\nconcepts, with applications in advanced Gaussian process models, scalable\nBayesian optimization, and Bayesian active learning.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 11:58:45 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 15:13:38 GMT"}, {"version": "v3", "created": "Fri, 6 Jul 2018 13:48:27 GMT"}, {"version": "v4", "created": "Wed, 31 Oct 2018 09:40:13 GMT"}, {"version": "v5", "created": "Wed, 14 Aug 2019 13:08:25 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Seeger", "Matthias", ""], ["Hetzel", "Asmus", ""], ["Dai", "Zhenwen", ""], ["Meissner", "Eric", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1710.08826", "submitter": "Henry Schreiner Iii", "authors": "Henry Schreiner and Christoph Hasse and Bradley Hittle and Himadri\n  Pandey and Michael Sokoloff and Karen Tomko", "title": "GooFit 2.0", "comments": "Submitted to the ACAT 2017 proceedings, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS hep-ex physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The GooFit package provides physicists a simple, familiar syntax for\nmanipulating probability density functions and performing fits, and is highly\noptimized for data analysis on NVIDIA GPUs and multithreaded CPU backends.\nGooFit was updated to version 2.0, bringing a host of new features. A\ncompletely revamped and redesigned build system makes GooFit easier to install,\ndevelop with, and run on virtually any system. Unit testing, continuous\nintegration, and advanced logging options are improving the stability and\nreliability of the system. Developing new PDFs now uses standard CUDA\nterminology and provides a lower barrier for new users. The system now has\nbuilt-in support for multiple graphics cards or nodes using MPI, and is being\ntested on a wide range of different systems. GooFit also has significant\nimprovements in performance on some GPU architectures due to optimized memory\naccess. Support for time-dependent four-body amplitude analyses has also been\nadded.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 11:43:33 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Schreiner", "Henry", ""], ["Hasse", "Christoph", ""], ["Hittle", "Bradley", ""], ["Pandey", "Himadri", ""], ["Sokoloff", "Michael", ""], ["Tomko", "Karen", ""]]}, {"id": "1710.09409", "submitter": "Meifeng Lin", "authors": "Peter A. Boyle, M. A. Clark, Carleton DeTar, Meifeng Lin, Verinder\n  Rana, Alejandro Vaquero Avil\\'es-Casco", "title": "Performance Portability Strategies for Grid C++ Expression Templates", "comments": "8 pages, 4 figures. Talk presented at the 35th International\n  Symposium on Lattice Field Theory, 18-24 June 2017, Granada, Spain", "journal-ref": null, "doi": "10.1051/epjconf/201817509006", "report-no": null, "categories": "hep-lat cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key requirements for the Lattice QCD Application Development as\npart of the US Exascale Computing Project is performance portability across\nmultiple architectures. Using the Grid C++ expression template as a starting\npoint, we report on the progress made with regards to the Grid GPU offloading\nstrategies. We present both the successes and issues encountered in using CUDA,\nOpenACC and Just-In-Time compilation. Experimentation and performance on GPUs\nwith a SU(3)$\\times$SU(3) streaming test will be reported. We will also report\non the challenges of using current OpenMP 4.x for GPU offloading in the same\ncode.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 18:23:19 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Boyle", "Peter A.", ""], ["Clark", "M. A.", ""], ["DeTar", "Carleton", ""], ["Lin", "Meifeng", ""], ["Rana", "Verinder", ""], ["Avil\u00e9s-Casco", "Alejandro Vaquero", ""]]}, {"id": "1710.09578", "submitter": "Sebastian Semper", "authors": "Christoph Wagner, Sebastian Semper", "title": "Fast Linear Transformations in Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new free library for the Python programming language,\nwhich provides a collection of structured linear transforms, that are not\nrepresented as explicit two dimensional arrays but in a more efficient way by\nexploiting the structural knowledge.\n  This allows fast and memory savy forward and backward transformations while\nalso provding a clean but still flexible interface to these effcient\nalgorithms, thus making code more readable, scable and adaptable.\n  We first outline the goals of this library, then how they were achieved and\nlastly we demonstrate the performance compared to current state of the art\npackages available for Python.\n  This library is released and distributed under a free license.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 08:16:04 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Wagner", "Christoph", ""], ["Semper", "Sebastian", ""]]}, {"id": "1710.10899", "submitter": "Michael Lass", "authors": "Michael Lass and Stephan Mohr and Hendrik Wiebeler and Thomas D.\n  K\\\"uhne and Christian Plessl", "title": "A Massively Parallel Algorithm for the Approximate Calculation of\n  Inverse p-th Roots of Large Sparse Matrices", "comments": null, "journal-ref": null, "doi": "10.1145/3218176.3218231", "report-no": null, "categories": "cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the submatrix method, a highly parallelizable method for the\napproximate calculation of inverse p-th roots of large sparse symmetric\nmatrices which are required in different scientific applications. We follow the\nidea of Approximate Computing, allowing imprecision in the final result in\norder to be able to utilize the sparsity of the input matrix and to allow\nmassively parallel execution. For an n x n matrix, the proposed algorithm\nallows to distribute the calculations over n nodes with only little\ncommunication overhead. The approximate result matrix exhibits the same\nsparsity pattern as the input matrix, allowing for efficient reuse of allocated\ndata structures.\n  We evaluate the algorithm with respect to the error that it introduces into\ncalculated results, as well as its performance and scalability. We demonstrate\nthat the error is relatively limited for well-conditioned matrices and that\nresults are still valuable for error-resilient applications like\npreconditioning even for ill-conditioned matrices. We discuss the execution\ntime and scaling of the algorithm on a theoretical level and present a\ndistributed implementation of the algorithm using MPI and OpenMP. We\ndemonstrate the scalability of this implementation by running it on a\nhigh-performance compute cluster comprised of 1024 CPU cores, showing a speedup\nof 665x compared to single-threaded execution.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 12:33:35 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 13:18:53 GMT"}, {"version": "v3", "created": "Thu, 12 Apr 2018 13:50:40 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Lass", "Michael", ""], ["Mohr", "Stephan", ""], ["Wiebeler", "Hendrik", ""], ["K\u00fchne", "Thomas D.", ""], ["Plessl", "Christian", ""]]}, {"id": "1710.10951", "submitter": "Hiroyuki Kasai", "authors": "Hiroyuki Kasai", "title": "SGDLibrary: A MATLAB library for stochastic gradient descent algorithms", "comments": null, "journal-ref": "Journal of Machine Learning Research, vol.18, no.215, pp.1-5, 2018", "doi": null, "report-no": null, "categories": "cs.MS stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding the minimizer of a function $f:\n\\mathbb{R}^d \\rightarrow \\mathbb{R}$ of the finite-sum form $\\min f(w) =\n1/n\\sum_{i}^n f_i(w)$. This problem has been studied intensively in recent\nyears in the field of machine learning (ML). One promising approach for\nlarge-scale data is to use a stochastic optimization algorithm to solve the\nproblem. SGDLibrary is a readable, flexible and extensible pure-MATLAB library\nof a collection of stochastic optimization algorithms. The purpose of the\nlibrary is to provide researchers and implementers a comprehensive evaluation\nenvironment for the use of these algorithms on various ML problems.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 07:42:06 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 23:32:35 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Kasai", "Hiroyuki", ""]]}]