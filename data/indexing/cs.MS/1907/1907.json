[{"id": "1907.00096", "submitter": "Jan Verschelde", "authors": "Jasmine Otto, Angus Forbes, and Jan Verschelde", "title": "Solving Polynomial Systems with phcpy", "comments": "Accepted for publication in the SciPy 2019 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.SC math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The solutions of a system of polynomials in several variables are often\nneeded, e.g.: in the design of mechanical systems, and in phase-space analyses\nof nonlinear biological dynamics. Reliable, accurate, and comprehensive\nnumerical solutions are available through PHCpack, a FOSS package for solving\npolynomial systems with homotopy continuation. This paper explores new\ndevelopments in phcpy, a scripting interface for PHCpack, over the past five\nyears. For instance, phcpy is now available online through a JupyterHub server\nfeaturing Python2, Python3, and SageMath kernels. As small systems are solved\nin real-time by phcpy, they are suitable for interactive exploration through\nthe notebook interface. Meanwhile, phcpy supports GPU parallelization,\nimproving the speed and quality of solutions to much larger polynomial systems.\nFrom various model design and analysis problems in STEM, certain classes of\npolynomial system frequently arise, to which phcpy is well-suited.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 22:19:04 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Otto", "Jasmine", ""], ["Forbes", "Angus", ""], ["Verschelde", "Jan", ""]]}, {"id": "1907.01005", "submitter": "Denis Davydov", "authors": "Denis Davydov and Martin Kronbichler", "title": "Algorithms and data structures for matrix-free finite element operators\n  with MPI-parallel sparse multi-vectors", "comments": "29 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DS cs.NA math.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional solution approaches for problems in quantum mechanics scale as\n$\\mathcal O(M^3)$, where $M$ is the number of electrons. Various methods have\nbeen proposed to address this issue and obtain linear scaling $\\mathcal O(M)$.\nOne promising formulation is the direct minimization of energy. Such methods\ntake advantage of physical localization of the solution, namely that the\nsolution can be sought in terms of non-orthogonal orbitals with local support.\nIn this work a numerically efficient implementation of sparse parallel vectors\nwithin the open-source finite element library deal.II is proposed. The main\nalgorithmic ingredient is the matrix-free evaluation of the Hamiltonian\noperator by cell-wise quadrature. Based on an a-priori chosen support for each\nvector we develop algorithms and data structures to perform (i) matrix-free\nsparse matrix multivector products (SpMM), (ii) the projection of an operator\nonto a sparse sub-space (inner products), and (iii) post-multiplication of a\nsparse multivector with a square matrix. The node-level performance is analyzed\nusing a roofline model. Our matrix-free implementation of finite element\noperators with sparse multivectors achieves the performance of 157 GFlop/s on\nIntel Cascade Lake architecture. Strong and weak scaling results are reported\nfor a typical benchmark problem using quadratic and quartic finite element\nbases.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 18:24:46 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Davydov", "Denis", ""], ["Kronbichler", "Martin", ""]]}, {"id": "1907.01063", "submitter": "Erik \\v{S}trumbelj", "authors": "Rok \\v{C}e\\v{s}novar, Steve Bronder, Davor Sluga, Jure Dem\\v{s}ar,\n  Tadej Ciglari\\v{c}, Sean Talts, Erik \\v{S}trumbelj", "title": "GPU-based Parallel Computation Support for Stan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper details an extensible OpenCL framework that allows Stan to utilize\nheterogeneous compute devices. It includes GPU-optimized routines for the\nCholesky decomposition, its derivative, other matrix algebra primitives and\nsome commonly used likelihoods, with more additions planned for the near\nfuture. Stan users can now benefit from large speedups offered by GPUs with\nlittle effort and without changes to their existing Stan code. We demonstrate\nthe practical utility of our work with two examples - logistic regression and\nGaussian Process regression.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 20:36:16 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 23:23:03 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["\u010ce\u0161novar", "Rok", ""], ["Bronder", "Steve", ""], ["Sluga", "Davor", ""], ["Dem\u0161ar", "Jure", ""], ["Ciglari\u010d", "Tadej", ""], ["Talts", "Sean", ""], ["\u0160trumbelj", "Erik", ""]]}, {"id": "1907.01952", "submitter": "Erik \\v{S}trumbelj", "authors": "Jure Dem\\v{s}ar, Grega Repov\\v{s}, Erik \\v{S}trumbelj", "title": "bayes4psy -- an Open Source R Package for Bayesian Statistics in\n  Psychology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.MS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in psychology generates interesting data sets and unique statistical\nmodelling tasks. However, these tasks, while important, are often very\nspecific, so appropriate statistical models and methods cannot be found in\naccessible Bayesian tools. As a result, the use of Bayesian methods is limited\nto those that have the technical and statistical fundamentals that are required\nfor probabilistic programming. Such knowledge is not part of the typical\npsychology curriculum and is a difficult obstacle for psychology students and\nresearchers to overcome. The goal of the bayes4psy package is to bridge this\ngap and offer a collection of models and methods to be used for data analysis\nthat arises from psychology experiments and as a teaching tool for Bayesian\nstatistics in psychology. The package contains Bayesian t-test and\nbootstrapping and models for analyzing reaction times, success rates, and\ncolors. It also provides all the diagnostic, analytic and visualization tools\nfor the modern Bayesian data analysis workflow.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 14:01:23 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Dem\u0161ar", "Jure", ""], ["Repov\u0161", "Grega", ""], ["\u0160trumbelj", "Erik", ""]]}, {"id": "1907.02088", "submitter": "Sambit Panda", "authors": "Sambit Panda, Satish Palaniappan, Junhao Xiong, Eric W. Bridgeford,\n  Ronak Mehta, Cencheng Shen, Joshua T. Vogelstein", "title": "hyppo: A Multivariate Hypothesis Testing Python Package", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.MS stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce hyppo, a unified library for performing multivariate hypothesis\ntesting, including independence, two-sample, and k-sample testing. While many\nmultivariate independence tests have R packages available, the interfaces are\ninconsistent and most are not available in Python. hyppo includes many state of\nthe art multivariate testing procedures. The package is easy-to-use and is\nflexible enough to enable future extensions. The documentation and all releases\nare available at https://hyppo.neurodata.io.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 18:05:25 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 19:20:43 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2020 18:29:49 GMT"}, {"version": "v4", "created": "Thu, 9 Apr 2020 15:21:36 GMT"}, {"version": "v5", "created": "Thu, 20 Aug 2020 12:28:44 GMT"}, {"version": "v6", "created": "Thu, 1 Apr 2021 15:13:13 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Panda", "Sambit", ""], ["Palaniappan", "Satish", ""], ["Xiong", "Junhao", ""], ["Bridgeford", "Eric W.", ""], ["Mehta", "Ronak", ""], ["Shen", "Cencheng", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1907.02597", "submitter": "Maarten de Jong", "authors": "Maarten de Jong", "title": "Multi-dimensional interpolations in C++", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A C++ software design is presented that can be used to interpolate data in\nany number of dimensions. The design is based on a combination of templates of\nfunctional collections of elements and so-called type lists. The design allows\nfor different search methodologies and interpolation techniques in each\ndimension. It is also possible to expand and reduce the number of dimensions,\nto interpolate composite data types and to produce on-the-fly additional values\nsuch as derivatives of the interpolating function.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 08:16:49 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["de Jong", "Maarten", ""]]}, {"id": "1907.02778", "submitter": "Henrik Barthels M.Sc.", "authors": "Henrik Barthels, Christos Psarras, Paolo Bientinesi", "title": "Automatic Generation of Efficient Linear Algebra Programs", "comments": null, "journal-ref": "Proceedings of the Platform for Advanced Scientific Computing\n  Conference (2020) 1-11", "doi": "10.1145/3394277.3401836", "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The level of abstraction at which application experts reason about linear\nalgebra computations and the level of abstraction used by developers of\nhigh-performance numerical linear algebra libraries do not match. The former is\nconveniently captured by high-level languages and libraries such as Matlab and\nEigen, while the latter expresses the kernels included in the BLAS and LAPACK\nlibraries. Unfortunately, the translation from a high-level computation to an\nefficient sequence of kernels is a task, far from trivial, that requires\nextensive knowledge of both linear algebra and high-performance computing.\nInternally, almost all high-level languages and libraries use efficient\nkernels; however, the translation algorithms are too simplistic and thus lead\nto a suboptimal use of said kernels, with significant performance losses. In\norder to both achieve the productivity that comes with high-level languages,\nand make use of the efficiency of low level kernels, we are developing Linnea,\na code generator for linear algebra problems. As input, Linnea takes a\nhigh-level description of a linear algebra problem and produces as output an\nefficient sequence of calls to high-performance kernels. In 25 application\nproblems, the code generated by Linnea always outperforms Matlab, Julia, Eigen\nand Armadillo, with speedups up to and exceeding 10x.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 11:41:27 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 09:52:52 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 10:29:09 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Barthels", "Henrik", ""], ["Psarras", "Christos", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1907.03195", "submitter": "Jeremy Kepner", "authors": "Chansup Byun, Jeremy Kepner, William Arcand, David Bestor, William\n  Bergeron, Matthew Hubbell, Vijay Gadepally, Michael Houle, Michael Jones,\n  Anne Klein, Lauren Milechin, Peter Michaleas, Julie Mullen, Andrew Prout,\n  Antonio Rosa, Siddharth Samsi, Charles Yee, Albert Reuther", "title": "Optimizing Xeon Phi for Interactive Data Analysis", "comments": "6 pages, 5 figures, accepted in IEEE High Performance Extreme\n  Computing (HPEC) conference 2019", "journal-ref": null, "doi": "10.1109/HPEC.2019.8916300", "report-no": null, "categories": "cs.PF cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Intel Xeon Phi manycore processor is designed to provide high performance\nmatrix computations of the type often performed in data analysis. Common data\nanalysis environments include Matlab, GNU Octave, Julia, Python, and R.\nAchieving optimal performance of matrix operations within data analysis\nenvironments requires tuning the Xeon Phi OpenMP settings, process pinning, and\nmemory modes. This paper describes matrix multiplication performance results\nfor Matlab and GNU Octave over a variety of combinations of process counts and\nOpenMP threads and Xeon Phi memory modes. These results indicate that using\nKMP_AFFINITY=granlarity=fine, taskset pinning, and all2all cache memory mode\nallows both Matlab and GNU Octave to achieve 66% of the practical peak\nperformance for process counts ranging from 1 to 64 and OpenMP threads ranging\nfrom 1 to 64. These settings have resulted in generally improved performance\nacross a range of applications and has enabled our Xeon Phi system to deliver\nsignificant results in a number of real-world applications.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 22:04:25 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Byun", "Chansup", ""], ["Kepner", "Jeremy", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "William", ""], ["Hubbell", "Matthew", ""], ["Gadepally", "Vijay", ""], ["Houle", "Michael", ""], ["Jones", "Michael", ""], ["Klein", "Anne", ""], ["Milechin", "Lauren", ""], ["Michaleas", "Peter", ""], ["Mullen", "Julie", ""], ["Prout", "Andrew", ""], ["Rosa", "Antonio", ""], ["Samsi", "Siddharth", ""], ["Yee", "Charles", ""], ["Reuther", "Albert", ""]]}, {"id": "1907.03251", "submitter": "Hiroshi Haramoto", "authors": "Hiroshi Haramoto, Makoto Matsumoto, Mutsuo Saito", "title": "Pseudo random number generators: attention for a newly proposed\n  generator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Xorshift128+ is a newly proposed pseudo random number generator (PRNG), which\nis now the standard PRNG in a number of platforms. We point out that\nthree-dimensional plots of the random points generated by the generator have\nvisible structures: they concentrate on particular planes in the cube. We\nprovide mathematical analysis on this phenomenon. A key-observation is that the\nexclusive-or is well-approximated by the arithmetic sum or subtraction with\nrelatively high probability.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 08:40:33 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 01:34:04 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Haramoto", "Hiroshi", ""], ["Matsumoto", "Makoto", ""], ["Saito", "Mutsuo", ""]]}, {"id": "1907.03709", "submitter": "Alberto F. Mart\\'in", "authors": "Santiago Badia, Alberto F. Mart\\'in, Eric Neiva, Francesc Verdugo", "title": "A generic finite element framework on parallel tree-based adaptive\n  meshes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we formally derive and prove the correctness of the algorithms\nand data structures in a parallel, distributed-memory, generic finite element\nframework that supports h-adaptivity on computational domains represented as\nforest-of-trees. The framework is grounded on a rich representation of the\nadaptive mesh suitable for generic finite elements that is built on top of a\nlow-level, light-weight forest-of-trees data structure handled by a\nspecialized, highly parallel adaptive meshing engine, for which we have\nidentified the requirements it must fulfill to be coupled into our framework.\nAtop this two-layered mesh representation, we build the rest of data structures\nrequired for the numerical integration and assembly of the discrete system of\nlinear equations. We consider algorithms that are suitable for both\nsubassembled and fully-assembled distributed data layouts of linear system\nmatrices. The proposed framework has been implemented within the FEMPAR\nscientific software library, using p4est as a practical forest-of-octrees\ndemonstrator. A strong scaling study of this implementation when applied to\nPoisson and Maxwell problems reveals remarkable scalability up to 32.2K CPU\ncores and 482.2M degrees of freedom. Besides, a comparative performance study\nof FEMPAR and the state-of-the-art deal.ii finite element software shows at\nleast comparative performance, and at most factor 2-3 improvements in the\nh-adaptive approximation of a Poisson problem with first- and second-order\nLagrangian finite elements, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 16:30:39 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 15:06:02 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Badia", "Santiago", ""], ["Mart\u00edn", "Alberto F.", ""], ["Neiva", "Eric", ""], ["Verdugo", "Francesc", ""]]}, {"id": "1907.06470", "submitter": "Vadim Demchik", "authors": "Vadim Demchik, Miroslav Ba\\v{c}\\'ak, Stefan Bordag", "title": "Out-of-core singular value decomposition", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Singular value decomposition (SVD) is a standard matrix factorization\ntechnique that produces optimal low-rank approximations of matrices. It has\ndiverse applications, including machine learning, data science and signal\nprocessing. However, many common problems involve very large matrices that\ncannot fit in the main memory of commodity computers, making it impractical to\nuse standard SVD algorithms that assume fast random access or large amounts of\nspace for intermediate calculations. To address this issue, we have implemented\nan out-of-core (external memory) randomized SVD solution that is fully scalable\nand efficiently parallelizable. This solution factors both dense and sparse\nmatrices of arbitrarily large size within arbitrarily small memory limits,\nefficiently using out-of-core storage as needed. It uses an innovative\ntechnique for partitioning matrices that lends itself to out-of-core and\nparallel processing, as well as memory and I/O use planning, automatic load\nbalancing, performance tuning, and makes possible a number of other practical\nenhancements to the current state-of-the-art. Furthermore, by using persistent\nexternal storage (generally HDDs or SSDs), users can resume interrupted\noperations without having to recalculate previously performed steps, solving a\nmajor practical problem in factoring very large matrices.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 12:41:47 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Demchik", "Vadim", ""], ["Ba\u010d\u00e1k", "Miroslav", ""], ["Bordag", "Stefan", ""]]}, {"id": "1907.08316", "submitter": "Lukas Einkemmer", "authors": "Lukas Einkemmer", "title": "Semi-Lagrangian Vlasov simulation on GPUs", "comments": null, "journal-ref": null, "doi": "10.1016/j.cpc.2020.107351", "report-no": null, "categories": "physics.comp-ph cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, our goal is to efficiently solve the Vlasov equation on GPUs.\nA semi-Lagrangian discontinuous Galerkin scheme is used for the discretization.\nSuch kinetic computations are extremely expensive due to the high-dimensional\nphase space. The SLDG code, which is publicly available under the MIT license\nabstracts the number of dimensions and uses a shared codebase for both GPU and\nCPU based simulations. We investigate the performance of the implementation on\na range of both Tesla (V100, Titan V, K80) and consumer (GTX 1080 Ti) GPUs. Our\nimplementation is typically able to achieve a performance of approximately 470\nGB/s on a single GPU and 1600 GB/s on four V100 GPUs connected via NVLink. This\nresults in a speedup of about a factor of ten (comparing a single GPU with a\ndual socket Intel Xeon Gold node) and approximately a factor of 35 (comparing a\nsingle node with and without GPUs). In addition, we investigate the effect of\nsingle precision computation on the performance of the SLDG code and\ndemonstrate that a template based dimension independent implementation can\nachieve good performance regardless of the dimensionality of the problem.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 23:26:49 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 14:11:02 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Einkemmer", "Lukas", ""]]}, {"id": "1907.08492", "submitter": "Martin Kronbichler", "authors": "Martin Kronbichler, Katharina Kormann, Niklas Fehn, Peter Munch and\n  Julius Witte", "title": "A Hermite-like basis for faster matrix-free evaluation of interior\n  penalty discontinuous Galerkin operators", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a basis for improved throughput of matrix-free evaluation\nof discontinuous Galerkin symmetric interior penalty discretizations on\nhexahedral elements. The basis relies on ideas of Hermite polynomials. It is\nused in a fully discontinuous setting not for higher order continuity but to\nminimize the effective stencil width, namely to limit the neighbor access of an\nelement to one data point for the function value and one for the derivative.\nThe basis is extended to higher orders with nodal contributions derived from\nroots of Jacobi polynomials and extended to multiple dimensions with tensor\nproducts, which enable the use of sum factorization. The beneficial effect of\nthe reduced data access on modern processors is shown. Furthermore, the\nviability of the basis in the context of multigrid solvers is analyzed. While a\nplain point-Jacobi approach is less efficient than with the best nodal\npolynomials, a basis change via sum-factorization techniques enables the\ncombination of the fast matrix-vector products with effective multigrid\nconstituents. The basis change is essentially for free on modern hardware\nbecause these computations can be hidden behind the cost of the data access.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 12:52:49 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Kronbichler", "Martin", ""], ["Kormann", "Katharina", ""], ["Fehn", "Niklas", ""], ["Munch", "Peter", ""], ["Witte", "Julius", ""]]}, {"id": "1907.08560", "submitter": "Vedran Novakovi\\'c", "authors": "Sanja Singer, Edoardo Di Napoli, Vedran Novakovi\\'c, Gayatri\n  \\v{C}aklovi\\'c", "title": "The LAPW method with eigendecomposition based on the Hari--Zimmermann\n  generalized hyperbolic SVD", "comments": "The supplementary material is available at\n  https://web.math.pmf.unizg.hr/mfbda/papers/sm-SISC.pdf due to its size. This\n  revised manuscript is currently being considered for publication", "journal-ref": "SIAM J. Sci. Comput. 42 (2020), C265-C293", "doi": "10.1137/19M1277813", "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an accurate, highly parallel algorithm for the\ngeneralized eigendecomposition of a matrix pair $(H, S)$, given in a factored\nform $(F^{\\ast} J F, G^{\\ast} G)$. Matrices $H$ and $S$ are generally complex\nand Hermitian, and $S$ is positive definite. This type of matrices emerges from\nthe representation of the Hamiltonian of a quantum mechanical system in terms\nof an overcomplete set of basis functions. This expansion is part of a class of\nmodels within the broad field of Density Functional Theory, which is considered\nthe golden standard in condensed matter physics. The overall algorithm consists\nof four phases, the second and the fourth being optional, where the two last\nphases are computation of the generalized hyperbolic SVD of a complex matrix\npair $(F,G)$, according to a given matrix $J$ defining the hyperbolic scalar\nproduct. If $J = I$, then these two phases compute the GSVD in parallel very\naccurately and efficiently.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 16:13:30 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 17:43:28 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Singer", "Sanja", ""], ["Di Napoli", "Edoardo", ""], ["Novakovi\u0107", "Vedran", ""], ["\u010caklovi\u0107", "Gayatri", ""]]}, {"id": "1907.08611", "submitter": "Mathieu Besan\\c{c}on", "authors": "Mathieu Besan\\c{c}on, Theodore Papamarkou, David Anthoff, Alex Arslan,\n  Simon Byrne, Dahua Lin, John Pearson", "title": "Distributions.jl: Definition and Modeling of Probability Distributions\n  in the JuliaStats Ecosystem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random variables and their distributions are a central part in many areas of\nstatistical methods. The Distributions.jl package provides Julia users and\ndevelopers tools for working with probability distributions, leveraging Julia\nfeatures for their intuitive and flexible manipulation, while remaining highly\nefficient through zero-cost abstractions.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 17:59:56 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 07:18:54 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 09:30:49 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Besan\u00e7on", "Mathieu", ""], ["Papamarkou", "Theodore", ""], ["Anthoff", "David", ""], ["Arslan", "Alex", ""], ["Byrne", "Simon", ""], ["Lin", "Dahua", ""], ["Pearson", "John", ""]]}, {"id": "1907.10121", "submitter": "Tyler Reddy", "authors": "Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland,\n  Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren\n  Weckesser, Jonathan Bright, St\\'efan J. van der Walt, Matthew Brett, Joshua\n  Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones,\n  Robert Kern, Eric Larson, CJ Carey, \\.Ilhan Polat, Yu Feng, Eric W. Moore,\n  Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian\n  Henriksen, E.A. Quintero, Charles R Harris, Anne M. Archibald, Ant\\^onio H.\n  Ribeiro, Fabian Pedregosa, Paul van Mulbregt, SciPy 1.0 Contributors", "title": "SciPy 1.0--Fundamental Algorithms for Scientific Computing in Python", "comments": "Article source data is available here:\n  https://github.com/scipy/scipy-articles", "journal-ref": "Nature Methods 17, 261 (2020)", "doi": "10.1038/s41592-019-0686-2", "report-no": null, "categories": "cs.MS cs.DS cs.SE physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  SciPy is an open source scientific computing library for the Python\nprogramming language. SciPy 1.0 was released in late 2017, about 16 years after\nthe original version 0.1 release. SciPy has become a de facto standard for\nleveraging scientific algorithms in the Python programming language, with more\nthan 600 unique code contributors, thousands of dependent packages, over\n100,000 dependent repositories, and millions of downloads per year. This\nincludes usage of SciPy in almost half of all machine learning projects on\nGitHub, and usage by high profile projects including LIGO gravitational wave\nanalysis and creation of the first-ever image of a black hole (M87). The\nlibrary includes functionality spanning clustering, Fourier transforms,\nintegration, interpolation, file I/O, linear algebra, image processing,\northogonal distance regression, minimization algorithms, signal processing,\nsparse matrix handling, computational geometry, and statistics. In this work,\nwe provide an overview of the capabilities and development practices of the\nSciPy library and highlight some recent technical developments.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 20:31:36 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Virtanen", "Pauli", ""], ["Gommers", "Ralf", ""], ["Oliphant", "Travis E.", ""], ["Haberland", "Matt", ""], ["Reddy", "Tyler", ""], ["Cournapeau", "David", ""], ["Burovski", "Evgeni", ""], ["Peterson", "Pearu", ""], ["Weckesser", "Warren", ""], ["Bright", "Jonathan", ""], ["van der Walt", "St\u00e9fan J.", ""], ["Brett", "Matthew", ""], ["Wilson", "Joshua", ""], ["Millman", "K. Jarrod", ""], ["Mayorov", "Nikolay", ""], ["Nelson", "Andrew R. J.", ""], ["Jones", "Eric", ""], ["Kern", "Robert", ""], ["Larson", "Eric", ""], ["Carey", "CJ", ""], ["Polat", "\u0130lhan", ""], ["Feng", "Yu", ""], ["Moore", "Eric W.", ""], ["VanderPlas", "Jake", ""], ["Laxalde", "Denis", ""], ["Perktold", "Josef", ""], ["Cimrman", "Robert", ""], ["Henriksen", "Ian", ""], ["Quintero", "E. A.", ""], ["Harris", "Charles R", ""], ["Archibald", "Anne M.", ""], ["Ribeiro", "Ant\u00f4nio H.", ""], ["Pedregosa", "Fabian", ""], ["van Mulbregt", "Paul", ""], ["Contributors", "SciPy 1. 0", ""]]}, {"id": "1907.12349", "submitter": "Matteo Ravasi", "authors": "Matteo Ravasi, Ivan Vasconcelos", "title": "PyLops -- A Linear-Operator Python Library for large scale optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear operators and optimisation are at the core of many algorithms used in\nsignal and image processing, remote sensing, and inverse problems. For small to\nmedium-scale problems, existing software packages (e.g., MATLAB, Python numpy\nand scipy) allow for explicitly building dense (or sparse) matrices and\nperforming algebraic operations (e.g., computation of matrix-vector products\nand manipulation of matrices) with syntax that closely represents their\ncorresponding analytical forms. However, many real application, large-scale\noperators do not lend themselves to explicit matrix representations, usually\nforcing practitioners to forego of the convenient linear-algebra syntax\navailable for their explicit-matrix counterparts. PyLops is an open-source\nPython library providing a flexible and scalable framework for the creation and\ncombination of so-called linear operators, class-based entities that represent\nmatrices and inherit their associated syntax convenience, but do not rely on\nthe creation of explicit matrices. We show that PyLops operators can\ndramatically reduce the memory load and CPU computations compared to\nexplicit-matrix calculations, while still allowing users to seamlessly use\ntheir existing knowledge of compact matrix-based syntax that scales to any\nproblem size because no explicit matrices are required.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 11:53:46 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Ravasi", "Matteo", ""], ["Vasconcelos", "Ivan", ""]]}, {"id": "1907.13419", "submitter": "Anton Ponomarev", "authors": "Anton Ponomarev, Julian Hofmann, Lutz Gr\\\"oll", "title": "Characteristics-based Simulink implementation of first-order quasilinear\n  partial differential equations", "comments": "Abridged and updated conference version. Accepted to SIMULTECH 2020", "journal-ref": null, "doi": "10.5220/0009569001390146", "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with solving first-order quasilinear partial differential\nequations in an online simulation environment, such as Simulink, utilizing the\nwell-known and well-recommended method of characteristics. Compared to the\ncommonly applied space discretization methods on static grids, the\ncharacteristics-based approach provides better numerical stability. Simulink\nsubsystem implementing the method of characteristics is developed. It employs\nSimulink's built-in solver and its zero-crossing detection algorithm to perform\nsimultaneous integration of a pool of characteristics as well as to create new\ncharacteristics dynamically and discard the old ones. Numerical accuracy of the\nsolution thus obtained is established. The subsystem has been tested on a\nfull-state feedback example and produced better results than the space\ndiscretization-based \"method of lines\". The implementation is available for\ndownload and can be used in a wide range of models.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 11:18:54 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 15:46:33 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Ponomarev", "Anton", ""], ["Hofmann", "Julian", ""], ["Gr\u00f6ll", "Lutz", ""]]}, {"id": "1907.13442", "submitter": "Richard Nies", "authors": "Richard Nies and Matthias Hoelzl", "title": "Testing performance with and without Block Low Rank Compression in MUMPS\n  and the new PaStiX 6.0 for JOREK nonlinear MHD simulations", "comments": "21 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interface to the MUMPS solver was updated in the JOREK MHD code to\nsupport Block Low Rank (BLR) compression and an interface to the new PaStiX\nsolver version 6 has been implemented supporting BLR as well. First tests were\ncarried out with JOREK, which solves a large sparse matrix system iteratively\nin each time step. For the preconditioning, a direct solver is applied in the\ncode to sub-matrices, and at this point BLR was applied with the results being\nsummarized in this report. For a simple case with a linearly growing mode,\nresults with both solvers look promising with a considerable reduction of the\nmemory consumption by several ten percent was obtained. A direct increase in\nperformance was seen in particular configurations already.\n  The choice of the BLR accuracy parameter $\\epsilon$ proves to be critical in\nthis simple test and also in more realistic simulations, which were carried out\nonly with MUMPS due to the limited time available. The more realistic test\nshowed an increase in run time when using BLR, which was mitigated when using\nlarger values of $\\epsilon$. However, the GMRes iterative solver does not reach\nconvergence anymore when $\\epsilon$ is too large, since the preconditioner\nbecomes too inaccurate in that case. It is thus critical to use an $\\epsilon$\nas large as possible, while still reaching convergence. More tests regarding\nthis optimum will be necessary in the future. BLR can also lead to an indirect\nspeed-up in particular cases, when the simulation can be run on a smaller\nnumber of compute nodes due to the reduced memory consumption.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 12:16:19 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Nies", "Richard", ""], ["Hoelzl", "Matthias", ""]]}]