[{"id": "1902.01168", "submitter": "Francesc Verdugo Phd", "authors": "Francesc Verdugo, Alberto F. Mart\\'in and Santiago Badia", "title": "Distributed-memory parallelization of the aggregated unfitted finite\n  element method", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2019.112583", "report-no": null, "categories": "cs.NA cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aggregated unfitted finite element method (AgFEM) is a methodology\nrecently introduced in order to address conditioning and stability problems\nassociated with embedded, unfitted, or extended finite element methods. The\nmethod is based on removal of basis functions associated with badly cut cells\nby introducing carefully designed constraints, which results in well-posed\nsystems of linear algebraic equations, while preserving the optimal\napproximation order of the underlying finite element spaces. The specific goal\nof this work is to present the implementation and performance of the method on\ndistributed-memory platforms aiming at the efficient solution of large-scale\nproblems. In particular, we show that, by considering AgFEM, the resulting\nsystems of linear algebraic equations can be effectively solved using standard\nalgebraic multigrid preconditioners. This is in contrast with previous works\nthat consider highly customized preconditioners in order to allow one the usage\nof iterative solvers in combination with unfitted techniques. Another novelty\nwith respect to the methods available in the literature is the problem sizes\nthat can be handled with the proposed approach. While most of previous\nreferences discussing linear solvers for unfitted methods are based on serial\nnon-scalable algorithms, we propose a parallel distributed-memory method able\nto efficiently solve problems at large scales. This is demonstrated by means of\na weak scaling test defined on complex 3D domains up to 300M degrees of freedom\nand one billion cells on 16K CPU cores in the Marenostrum-IV platform. The\nparallel implementation of the AgFEM method is available in the large-scale\nfinite element package FEMPAR.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 13:24:47 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 06:05:31 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Verdugo", "Francesc", ""], ["Mart\u00edn", "Alberto F.", ""], ["Badia", "Santiago", ""]]}, {"id": "1902.01829", "submitter": "Wajih Halim Boukaram", "authors": "Wajih Halim Boukaram, George Turkiyyah, David E. Keyes", "title": "Hierarchical Matrix Operations on GPUs: Matrix-Vector Multiplication and\n  Compression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical matrices are space and time efficient representations of dense\nmatrices that exploit the low rank structure of matrix blocks at different\nlevels of granularity. The hierarchically low rank block partitioning produces\nrepresentations that can be stored and operated on in near-linear complexity\ninstead of the usual polynomial complexity of dense matrices. In this paper, we\npresent high performance implementations of matrix vector multiplication and\ncompression operations for the $\\mathcal{H}^2$ variant of hierarchical matrices\non GPUs. This variant exploits, in addition to the hierarchical block\npartitioning, hierarchical bases for the block representations and results in a\nscheme that requires only $O(n)$ storage and $O(n)$ complexity for the mat-vec\nand compression kernels. These two operations are at the core of algebraic\noperations for hierarchical matrices, the mat-vec being a ubiquitous operation\nin numerical algorithms while compression/recompression represents a key\nbuilding block for other algebraic operations, which require periodic\nrecompression during execution. The difficulties in developing efficient GPU\nalgorithms come primarily from the irregular tree data structures that underlie\nthe hierarchical representations, and the key to performance is to recast the\ncomputations on flattened trees in ways that allow batched linear algebra\noperations to be performed. This requires marshaling the irregularly laid out\ndata in a way that allows them to be used by the batched routines. Marshaling\noperations only involve pointer arithmetic with no data movement and as a\nresult have minimal overhead. Our numerical results on covariance matrices from\n2D and 3D problems from spatial statistics show the high efficiency our\nroutines achieve---over 550GB/s for the bandwidth-limited mat-vec and over\n850GFLOPS/s in sustained performance for the compression on the P100 Pascal\nGPU.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 17:59:51 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Boukaram", "Wajih Halim", ""], ["Turkiyyah", "George", ""], ["Keyes", "David E.", ""]]}, {"id": "1902.01961", "submitter": "Daniel Lemire", "authors": "Daniel Lemire, Owen Kaser, Nathan Kurz", "title": "Faster Remainder by Direct Computation: Applications to Compilers and\n  Software Libraries", "comments": null, "journal-ref": "Software: Practice and Experience 49 (6), 2019", "doi": "10.1002/spe.2689", "report-no": null, "categories": "cs.MS cs.PF", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  On common processors, integer multiplication is many times faster than\ninteger division. Dividing a numerator n by a divisor d is mathematically\nequivalent to multiplication by the inverse of the divisor (n / d = n x 1/d).\nIf the divisor is known in advance---or if repeated integer divisions will be\nperformed with the same divisor---it can be beneficial to substitute a less\ncostly multiplication for an expensive division.\n  Currently, the remainder of the division by a constant is computed from the\nquotient by a multiplication and a subtraction. But if just the remainder is\ndesired and the quotient is unneeded, this may be suboptimal. We present a\ngenerally applicable algorithm to compute the remainder more directly.\nSpecifically, we use the fractional portion of the product of the numerator and\nthe inverse of the divisor. On this basis, we also present a new, simpler\ndivisibility algorithm to detect nonzero remainders.\n  We also derive new tight bounds on the precision required when representing\nthe inverse of the divisor. Furthermore, we present simple C implementations\nthat beat the optimized code produced by state-of-art C compilers on recent x64\nprocessors (e.g., Intel Skylake and AMD Ryzen), sometimes by more than 25%. On\nall tested platforms including 64-bit ARM and POWER8, our divisibility-test\nfunctions are faster than state-of-the-art Granlund-Montgomery\ndivisibility-test functions, sometimes by more than 50%.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 22:33:20 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 18:49:00 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2019 16:52:47 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Lemire", "Daniel", ""], ["Kaser", "Owen", ""], ["Kurz", "Nathan", ""]]}, {"id": "1902.02104", "submitter": "Annalisa Massini", "authors": "Viviana Arrigoni, Annalisa Massini", "title": "Fast Strassen-based $A^t A$ Parallel Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix multiplication $A^t A$ appears as intermediate operation during the\nsolution of a wide set of problems. In this paper, we propose a new\ncache-oblivious algorithm for the $A^t A$ multiplication. Our algorithm,\nA$\\scriptstyle \\mathsf{T}$A, calls classical Strassen's algorithm as\nsub-routine, decreasing the computational cost %(expressed in number of\nperformed products) of the conventional $A^t A$ multiplication to\n$\\frac{2}{7}n^{\\log_2 7}$. It works for generic rectangular matrices and\nexploits the peculiar symmetry of the resulting product matrix for sparing\nmemory. We used the MPI paradigm to implement A$\\scriptstyle \\mathsf{T}$A in\nparallel, and we tested its performances on a small subset of nodes of the\nGalileo cluster. Experiments highlight good scalability and speed-up, also\nthanks to minimal number of exchanged messages in the designed communication\nsystem. Parallel overhead and inherently sequential time fraction are\nnegligible in the tested configurations.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 10:46:03 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Arrigoni", "Viviana", ""], ["Massini", "Annalisa", ""]]}, {"id": "1902.08115", "submitter": "Gianluca Frison", "authors": "Gianluca Frison, Tommaso Sartor, Andrea Zanelli, Moritz Diehl", "title": "The BLAS API of BLASFEO: optimizing performance for small matrices", "comments": null, "journal-ref": "ACM Transactions on Mathematical Software (TOMS): Volume 46 Issue\n  2, May 2020", "doi": "10.1145/3378671", "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BLASFEO is a dense linear algebra library providing high-performance\nimplementations of BLAS- and LAPACK-like routines for use in embedded\noptimization and other applications targeting relatively small matrices.\nBLASFEO defines an API which uses a packed matrix format as its native format.\nThis format is analogous to the internal memory buffers of optimized BLAS, but\nit is exposed to the user and it removes the packing cost from the routine\ncall. For matrices fitting in cache, BLASFEO outperforms optimized BLAS\nimplementations, both open-source and proprietary. This paper investigates the\naddition of a standard BLAS API to the BLASFEO framework, and proposes an\nimplementation switching between two or more algorithms optimized for different\nmatrix sizes. Thanks to the modular assembly framework in BLASFEO, tailored\nlinear algebra kernels with mixed column- and panel-major arguments are easily\ndeveloped. This BLAS API has lower performance than the BLASFEO API, but it\nnonetheless outperforms optimized BLAS and especially LAPACK libraries for\nmatrices fitting in cache. Therefore, it can boost a wide range of\napplications, where standard BLAS and LAPACK libraries are employed and the\nmatrix size is moderate. In particular, this paper investigates the benefits in\nscientific programming languages such as Octave, SciPy and Julia.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 16:05:43 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 09:15:44 GMT"}, {"version": "v3", "created": "Sun, 22 Sep 2019 10:38:09 GMT"}, {"version": "v4", "created": "Tue, 4 Feb 2020 12:26:49 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Frison", "Gianluca", ""], ["Sartor", "Tommaso", ""], ["Zanelli", "Andrea", ""], ["Diehl", "Moritz", ""]]}, {"id": "1902.09046", "submitter": "David Warne", "authors": "David J. Warne (1), Scott A. Sisson (2), Christopher Drovandi (1) ((1)\n  Queensland University of Technology, (2) University of New South Wales)", "title": "Vector operations for accelerating expensive Bayesian computations -- a\n  tutorial guide", "comments": null, "journal-ref": null, "doi": "10.1214/21-BA1265", "report-no": null, "categories": "stat.CO cs.DC cs.MS cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications in Bayesian statistics are extremely computationally\nintensive. However, they are often inherently parallel, making them prime\ntargets for modern massively parallel processors. Multi-core and distributed\ncomputing is widely applied in the Bayesian community, however, very little\nattention has been given to fine-grain parallelisation using single instruction\nmultiple data (SIMD) operations that are available on most modern commodity\nCPUs and is the basis of GPGPU computing. In this work, we practically\ndemonstrate, using standard programming libraries, the utility of the SIMD\napproach for several topical Bayesian applications. We show that SIMD can\nimprove the floating point arithmetic performance resulting in up to $6\\times$\nimprovement in serial algorithm performance. Importantly, these improvements\nare multiplicative to any gains achieved through multi-core processing. We\nillustrate the potential of SIMD for accelerating Bayesian computations and\nprovide the reader with techniques for exploiting modern massively parallel\nprocessing environments using standard tools.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 00:38:23 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 08:31:45 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 06:22:53 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Warne", "David J.", ""], ["Sisson", "Scott A.", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1902.09699", "submitter": "Bas Peters", "authors": "Bas Peters, Felix J. Herrmann", "title": "Algorithms and software for projections onto intersections of convex and\n  non-convex sets with applications to inverse problems", "comments": "37 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose algorithms and software for computing projections onto the\nintersection of multiple convex and non-convex constraint sets. The software\npackage, called SetIntersectionProjection, is intended for the regularization\nof inverse problems in physical parameter estimation and image processing. The\nprimary design criterion is working with multiple sets, which allows us to\nsolve inverse problems with multiple pieces of prior knowledge. Our algorithms\noutperform the well known Dykstra's algorithm when individual sets are not easy\nto project onto because we exploit similarities between constraint sets. Other\ndesign choices that make the software fast and practical to use, include\nrecently developed automatic selection methods for auxiliary algorithm\nparameters, fine and coarse grained parallelism, and a multilevel acceleration\nscheme. We provide implementation details and examples that show how the\nsoftware can be used to regularize inverse problems. Results show that we\nbenefit from working with all available prior information and are not limited\nto one or two regularizers because of algorithmic, computational, or\nhyper-parameter selection issues.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 01:52:07 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 09:11:03 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Peters", "Bas", ""], ["Herrmann", "Felix J.", ""]]}]