[{"id": "2005.02732", "submitter": "Matei Istoan", "authors": "David Defour (LP2A), Pablo de Oliveira Castro (PRISM, LI-PaRAD), Matei\n  Istoan (UVSQ, LI-PaRAD), Eric Petit", "title": "Custom-Precision Mathematical Library Explorations for Code Profiling\n  and Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The typical processors used for scientific computing have fixed-width\ndata-paths. This implies that mathematical libraries were specifically\ndeveloped to target each of these fixed precisions (binary16, binary32,\nbinary64). However, to address the increasing energy consumption and throughput\nrequirements of scientific applications, library and hardware designers are\nmoving beyond this one-size-fits-all approach. In this article we propose to\nstudy the effects and benefits of using user-defined floating-point formats and\ntarget accuracies in calculations involving mathematical functions. Our tool\ncollects input-data profiles and iteratively explores lower precisions for each\ncall-site of a mathematical function in user applications. This profiling data\nwill be a valuable asset for specializing and fine-tuning mathematical function\nimplementations for a given application. We demonstrate the tool's capabilities\non SGP4, a satellite tracking application. The profile data shows the potential\nfor specialization and provides insight into answering where it is useful to\nprovide variable-precision designs for elementary function evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 11:06:11 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Defour", "David", "", "LP2A"], ["Castro", "Pablo de Oliveira", "", "PRISM, LI-PaRAD"], ["Istoan", "Matei", "", "UVSQ, LI-PaRAD"], ["Petit", "Eric", ""]]}, {"id": "2005.02828", "submitter": "Jie Wang", "authors": "Jie Wang, Victor Magron, Jean B. Lasserre, and Ngoc Hoang Anh Mai", "title": "CS-TSSOS: Correlative and term sparsity for large-scale polynomial\n  optimization", "comments": "28 pages, 8 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a new moment-SOS hierarchy, called CS-TSSOS, for solving\nlarge-scale sparse polynomial optimization problems. Its novelty is to exploit\nsimultaneously correlative sparsity and term sparsity by combining advantages\nof two existing frameworks for sparse polynomial optimization. The former is\ndue to Waki et al. while the latter was initially proposed by Wang et al. and\nlater exploited in the TSSOS hierarchy. In doing so we obtain CS-TSSOS -- a\ntwo-level hierarchy of semidefinite programming relaxations with (i), the\ncrucial property to involve blocks of SDP matrices and (ii), the guarantee of\nconvergence to the global optimum under certain conditions. We demonstrate its\nefficiency and scalability on several large-scale instances of the celebrated\nMax-Cut problem and the important industrial optimal power flow problem,\ninvolving up to six thousand variables and tens of thousands of constraints.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 13:55:03 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 21:12:01 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Wang", "Jie", ""], ["Magron", "Victor", ""], ["Lasserre", "Jean B.", ""], ["Mai", "Ngoc Hoang Anh", ""]]}, {"id": "2005.03606", "submitter": "Tobias Weinzierl", "authors": "Charles D. Murray and Tobias Weinzierl", "title": "Delayed approximate matrix assembly in multigrid with dynamic precisions", "comments": null, "journal-ref": null, "doi": "10.1002/cpe.5941", "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate assembly of the system matrix is an important step in any code\nthat solves partial differential equations on a mesh. We either explicitly set\nup a matrix, or we work in a matrix-free environment where we have to be able\nto quickly return matrix entries upon demand. Either way, the construction can\nbecome costly due to non-trivial material parameters entering the equations,\nmultigrid codes requiring cascades of matrices that depend upon each other, or\ndynamic adaptive mesh refinement that necessitates the recomputation of matrix\nentries or the whole equation system throughout the solve. We propose that\nthese constructions can be performed concurrently with the multigrid cycles.\nInitial geometric matrices and low accuracy integrations kickstart the\nmultigrid, while improved assembly data is fed to the solver as and when it\nbecomes available. The time to solution is improved as we eliminate an\nexpensive preparation phase traditionally delaying the actual computation. We\neliminate algorithmic latency. Furthermore, we desynchronise the assembly from\nthe solution process. This anarchic increase of the concurrency level improves\nthe scalability. Assembly routines are notoriously memory- and\nbandwidth-demanding. As we work with iteratively improving operator accuracies,\nwe finally propose the use of a hierarchical, lossy compression scheme such\nthat the memory footprint is brought down aggressively where the system matrix\nentries carry little information or are not yet available with high accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 17:06:01 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Murray", "Charles D.", ""], ["Weinzierl", "Tobias", ""]]}, {"id": "2005.04540", "submitter": "Linjian Ma", "authors": "Linjian Ma, Jiayu Ye, Edgar Solomonik", "title": "AutoHOOT: Automatic High-Order Optimization for Tensors", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-order optimization methods, including Newton's method and its variants\nas well as alternating minimization methods, dominate the optimization\nalgorithms for tensor decompositions and tensor networks. These tensor methods\nare used for data analysis and simulation of quantum systems. In this work, we\nintroduce AutoHOOT, the first automatic differentiation (AD) framework\ntargeting at high-order optimization for tensor computations. AutoHOOT takes\ninput tensor computation expressions and generates optimized derivative\nexpressions. In particular, AutoHOOT contains a new explicit Jacobian / Hessian\nexpression generation kernel whose outputs maintain the input tensors'\ngranularity and are easy to optimize. The expressions are then optimized by\nboth the traditional compiler optimization techniques and specific tensor\nalgebra transformations. Experimental results show that AutoHOOT achieves\ncompetitive CPU and GPU performance for both tensor decomposition and tensor\nnetwork applications compared to existing AD software and other tensor\ncomputation libraries with manually written kernels. The tensor methods\ngenerated by AutoHOOT are also well-parallelizable, and we demonstrate good\nscalability on a distributed memory supercomputer.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 01:15:37 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 20:18:57 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Ma", "Linjian", ""], ["Ye", "Jiayu", ""], ["Solomonik", "Edgar", ""]]}, {"id": "2005.05261", "submitter": "Dmitry Kulyabov", "authors": "Migran N. Gevorkyan and Anna V. Korolkova and Dmitry S. Kulyabov and\n  Leonid A. Sevastianov", "title": "A modular extension for a computer algebra system", "comments": "in English; in Russian", "journal-ref": null, "doi": "10.1134/S036176882002005X", "report-no": null, "categories": "cs.MS cs.SC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computer algebra systems are complex software systems that cover a wide range\nof scientific and practical problems. However, the absolute coverage cannot be\nachieved. Often, it is required to create a user extension for an existing\ncomputer algebra system. In this case, the extensibility of the system should\nbe taken into account. In this paper, we consider a technology for extending\nthe SymPy computer algebra system with a low-level module that implements a\nrandom number generator.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 17:05:30 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Gevorkyan", "Migran N.", ""], ["Korolkova", "Anna V.", ""], ["Kulyabov", "Dmitry S.", ""], ["Sevastianov", "Leonid A.", ""]]}, {"id": "2005.06334", "submitter": "Stefan Lenz", "authors": "Stefan Lenz, Maren Hackenberg, Harald Binder", "title": "The JuliaConnectoR: a functionally oriented interface for integrating\n  Julia in R", "comments": "23 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG cs.PL stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like many groups considering the new programming language Julia, we faced the\nchallenge of accessing the algorithms that we develop in Julia from R.\nTherefore, we developed the R package JuliaConnectoR, available from the CRAN\nrepository and GitHub (https://github.com/stefan-m-lenz/JuliaConnectoR), in\nparticular for making advanced deep learning tools available. For\nmaintainability and stability, we decided to base communication between R and\nJulia on TCP, using an optimized binary format for exchanging data. Our package\nalso specifically contains features that allow for a convenient interactive use\nin R. This makes it easy to develop R extensions with Julia or to simply call\nfunctionality from Julia packages in R. Interacting with Julia objects and\ncalling Julia functions becomes user-friendly, as Julia functions and variables\nare made directly available as objects in the R workspace. We illustrate the\nfurther features of our package with code examples, and also discuss advantages\nover the two alternative packages JuliaCall and XRJulia. Finally, we\ndemonstrate the usage of the package with a more extensive example for\nemploying neural ordinary differential equations, a recent deep learning\ntechnique that has received much attention. This example also provides more\ngeneral guidance for integrating deep learning techniques from Julia into R.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 14:18:34 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 19:14:29 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Lenz", "Stefan", ""], ["Hackenberg", "Maren", ""], ["Binder", "Harald", ""]]}, {"id": "2005.07282", "submitter": "Roman Iakymchuk", "authors": "Roman Iakymchuk, Maria Barreda, Stef Graillat, Jose I. Aliaga, Enrique\n  S. Quintana-Orti", "title": "Reproducibility of Parallel Preconditioned Conjugate Gradient in Hybrid\n  Programming Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Preconditioned Conjugate Gradient method is often employed for the\nsolution of linear systems of equations arising in numerical simulations of\nphysical phenomena. While being widely used, the solver is also known for its\nlack of accuracy while computing the residual. In this article, we propose two\nalgorithmic solutions that originate from the ExBLAS project to enhance the\naccuracy of the solver as well as to ensure its reproducibility in a hybrid MPI\n+ OpenMP tasks programming environment. One is based on ExBLAS and preserves\nevery bit of information until the final rounding, while the other relies upon\nfloating-point expansions and, hence, expands the intermediate precision.\nInstead of converting the entire solver into its ExBLAS-related implementation,\nwe identify those parts that violate reproducibility/non-associativity, secure\nthem, and combine this with the sequential executions. These algorithmic\nstrategies are reinforced with programmability suggestions to assure\ndeterministic executions. Finally, we verify these approaches on two modern HPC\nsystems: both versions deliver reproducible number of iterations, residuals,\ndirect errors, and vector-solutions for the overhead of less than 37.7 % on 768\ncores.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 22:10:15 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Iakymchuk", "Roman", ""], ["Barreda", "Maria", ""], ["Graillat", "Stef", ""], ["Aliaga", "Jose I.", ""], ["Quintana-Orti", "Enrique S.", ""]]}, {"id": "2005.07403", "submitter": "Vedran Novakovi\\'c", "authors": "Vedran Novakovi\\'c", "title": "Batched computation of the singular value decompositions of order two by\n  the AVX-512 vectorization", "comments": "Preprint of an article submitted for consideration in Parallel\n  Processing Letters ( https://www.worldscientific.com/worldscinet/ppl )", "journal-ref": "Parallel Process. Lett. 30 (2020), 4; 2050015", "doi": "10.1142/S0129626420500152", "report-no": null, "categories": "cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a vectorized algorithm for simultaneously computing up to eight\nsingular value decompositions (SVDs, each of the form $A=U\\Sigma V^{\\ast}$) of\nreal or complex matrices of order two is proposed. The algorithm extends to a\nbatch of matrices of an arbitrary length $n$, that arises, for example, in the\nannihilation part of the parallel Kogbetliantz algorithm for the SVD of a\nsquare matrix of order $2n$. The SVD algorithm for a single matrix of order two\nis derived first. It scales, in most instances error-free, the input matrix $A$\nsuch that its singular values $\\Sigma_{ii}$ cannot overflow whenever its\nelements are finite, and then computes the URV factorization of the scaled\nmatrix, followed by the SVD of a non-negative upper-triangular middle factor. A\nvector-friendly data layout for the batch is then introduced, where the\nsame-indexed elements of each of the input and the output matrices form\nvectors, and the algorithm's steps over such vectors are described. The\nvectorized approach is then shown to be about three times faster than\nprocessing each matrix in isolation, while slightly improving accuracy over the\nstraightforward method for the $2\\times 2$ SVD.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 08:16:11 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Novakovi\u0107", "Vedran", ""]]}, {"id": "2005.08803", "submitter": "Ehsan Haghighat", "authors": "Ehsan Haghighat and Ruben Juanes", "title": "SciANN: A Keras/Tensorflow wrapper for scientific computations and\n  physics-informed deep learning using artificial neural networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2020.113552", "report-no": null, "categories": "cs.OH cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce SciANN, a Python package for scientific computing\nand physics-informed deep learning using artificial neural networks. SciANN\nuses the widely used deep-learning packages Tensorflow and Keras to build deep\nneural networks and optimization models, thus inheriting many of Keras's\nfunctionalities, such as batch optimization and model reuse for transfer\nlearning. SciANN is designed to abstract neural network construction for\nscientific computations and solution and discovery of partial differential\nequations (PDE) using the physics-informed neural networks (PINN) architecture,\ntherefore providing the flexibility to set up complex functional forms. We\nillustrate, in a series of examples, how the framework can be used for curve\nfitting on discrete data, and for solution and discovery of PDEs in strong and\nweak forms. We summarize the features currently available in SciANN, and also\noutline ongoing and future developments.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 22:55:15 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 03:18:44 GMT"}], "update_date": "2020-12-30", "authors_parsed": [["Haghighat", "Ehsan", ""], ["Juanes", "Ruben", ""]]}, {"id": "2005.10445", "submitter": "Sivan Toledo", "authors": "Yaniv Rubinpur and Sivan Toledo", "title": "Signal Processing for a Reverse-GPS Wildlife Tracking System: CPU and\n  GPU Implementation Experiences", "comments": "Revised (very slightly, changed a few sentences describing Figure 7)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present robust high-performance implementations of signal-processing tasks\nperformed by a high-throughput wildlife tracking system called ATLAS. The\nsystem tracks radio transmitters attached to wild animals by estimating the\ntime of arrival of radio packets to multiple receivers (base stations).\nTime-of-arrival estimation of wideband radio signals is computationally\nexpensive, especially in acquisition mode (when the time of transmission is not\nknown, not even approximately). These computations are a bottleneck that limits\nthe throughput of the system. We developed a sequential high-performance CPU\nimplementation of the computations a few years back, and more recencely a GPU\nimplementation. Both strive to balance performance with simplicity,\nmaintainability, and development effort, as most real-world codes do. The paper\nreports on the two implementations and carefully evaluates their performance.\nThe evaluations indicates that the GPU implementation dramatically improves\nperformance and power-performance relative to the sequential CPU implementation\nrunning on a desktop CPU typical of the computers in current base stations.\nPerformance improves by more than 50X on a high-end GPU and more than 4X with a\nGPU platform that consumes almost 5 times less power than the CPU platform.\nPerformance-per-Watt ratios also improve (by more than 16X), and so do the\nprice-performance ratios.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 03:28:52 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 08:37:40 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 07:15:52 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Rubinpur", "Yaniv", ""], ["Toledo", "Sivan", ""]]}, {"id": "2005.10635", "submitter": "Randall Balestriero", "authors": "Randall Balestriero", "title": "SymJAX: symbolic CPU/GPU/TPU programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SymJAX is a symbolic programming version of JAX simplifying graph\ninput/output/updates and providing additional functionalities for general\nmachine learning and deep learning applications. From an user perspective\nSymJAX provides a la Theano experience with fast graph optimization/compilation\nand broad hardware support, along with Lasagne-like deep learning\nfunctionalities.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 13:37:25 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Balestriero", "Randall", ""]]}, {"id": "2005.11300", "submitter": "Thomas Foster", "authors": "Thomas Foster, Chon Lok Lei, Martin Robinson, David Gavaghan, Ben\n  Lambert", "title": "Model Evidence with Fast Tree Based Quadrature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional integration is essential to many areas of science, ranging\nfrom particle physics to Bayesian inference. Approximating these integrals is\nhard, due in part to the difficulty of locating and sampling from regions of\nthe integration domain that make significant contributions to the overall\nintegral. Here, we present a new algorithm called Tree Quadrature (TQ) that\nseparates this sampling problem from the problem of using those samples to\nproduce an approximation of the integral. TQ places no qualifications on how\nthe samples provided to it are obtained, allowing it to use state-of-the-art\nsampling algorithms that are largely ignored by existing integration\nalgorithms. Given a set of samples, TQ constructs a surrogate model of the\nintegrand in the form of a regression tree, with a structure optimised to\nmaximise integral precision. The tree divides the integration domain into\nsmaller containers, which are individually integrated and aggregated to\nestimate the overall integral. Any method can be used to integrate each\nindividual container, so existing integration methods, like Bayesian Monte\nCarlo, can be combined with TQ to boost their performance. On a set of\nbenchmark problems, we show that TQ provides accurate approximations to\nintegrals in up to 15 dimensions; and in dimensions 4 and above, it outperforms\nsimple Monte Carlo and the popular Vegas method.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 17:48:06 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Foster", "Thomas", ""], ["Lei", "Chon Lok", ""], ["Robinson", "Martin", ""], ["Gavaghan", "David", ""], ["Lambert", "Ben", ""]]}, {"id": "2005.14025", "submitter": "Jian Ma", "authors": "Jian Ma", "title": "copent: Estimating Copula Entropy and Transfer Entropy in R", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.IT cs.LG cs.MS math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical independence and conditional independence are two fundamental\nconcepts in statistics and machine learning. Copula Entropy is a mathematical\nconcept defined by Ma and Sun for multivariate statistical independence\nmeasuring and testing, and also proved to be closely related to conditional\nindependence (or transfer entropy). As the unified framework for measuring both\nindependence and causality, CE has been applied to solve several related\nstatistical or machine learning problems, including association discovery,\nstructure learning, variable selection, and causal discovery. The nonparametric\nmethods for estimating copula entropy and transfer entropy were also proposed\npreviously. This paper introduces copent, the R package which implements these\nproposed methods for estimating copula entropy and transfer entropy. The\nimplementation detail of the package is introduced. Three examples with\nsimulated data and real-world data on variable selection and causal discovery\nare also presented to demonstrate the usage of this package. The examples on\nvariable selection and causal discovery show the strong ability of copent on\ntesting (conditional) independence compared with the related packages. The\ncopent package is available on the Comprehensive R Archive Network (CRAN) and\nalso on GitHub at https://github.com/majianthu/copent.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 10:01:12 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 00:14:25 GMT"}, {"version": "v3", "created": "Sat, 27 Mar 2021 00:41:58 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ma", "Jian", ""]]}]