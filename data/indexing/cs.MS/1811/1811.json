[{"id": "1811.01277", "submitter": "Pavel Kus", "authors": "P. Kus, A. Marek, S. S. Koecher, H.-H. Kowalski, C. Carbogno, Ch.\n  Scheurer, K. Reuter, M. Scheffler, H. Lederer", "title": "Optimizations of the Eigensolvers in the ELPA Library", "comments": null, "journal-ref": "Parallel Computing 85, pp 167-177 (2019)", "doi": "10.1016/j.parco.2019.04.003", "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The solution of (generalized) eigenvalue problems for symmetric or Hermitian\nmatrices is a common subtask of many numerical calculations in electronic\nstructure theory or materials science. Solving the eigenvalue problem can\neasily amount to a sizeable fraction of the whole numerical calculation. For\nresearchers in the field of computational materials science, an efficient and\nscalable solution of the eigenvalue problem is thus of major importance. The\nELPA-library is a well-established dense direct eigenvalue solver library,\nwhich has proven to be very efficient and scalable up to very large core\ncounts. In this paper, we describe the latest optimizations of the ELPA-library\nfor new HPC architectures of the Intel Skylake processor family with an AVX-512\nSIMD instruction set, or for HPC systems accelerated with recent GPUs. We also\ndescribe a complete redesign of the API in a modern modular way, which, apart\nfrom a much simpler and more flexible usability, leads to a new path to access\nsystem-specific performance optimizations. In order to ensure optimal\nperformance for a particular scientific setting or a specific HPC system, the\nnew API allows the user to influence in straightforward way the internal\ndetails of the algorithms and of performance-critical parameters used in the\nELPA-library. On top of that, we introduced an autotuning functionality, which\nallows for finding the best settings in a self-contained automated way. In\nsituations where many eigenvalue problems with similar settings have to be\nsolved consecutively, the autotuning process of the ELPA-library can be done\n\"on-the-fly\". Practical applications from materials science which rely on\nso-called self-consistency iterations can profit from the autotuning. On some\nexamples of scientific interest, simulated with the FHI-aims application, the\nadvantages of the latest optimizations of the ELPA-library are demonstrated.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 20:07:06 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Kus", "P.", ""], ["Marek", "A.", ""], ["Koecher", "S. S.", ""], ["Kowalski", "H. -H.", ""], ["Carbogno", "C.", ""], ["Scheurer", "Ch.", ""], ["Reuter", "K.", ""], ["Scheffler", "M.", ""], ["Lederer", "H.", ""]]}, {"id": "1811.01719", "submitter": "Dmitry Kulyabov", "authors": "Migran N. Gevorkyan and Anastasia V. Demidova and Anna V. Korolkova\n  and Dmitry S. Kulyabov", "title": "Issues in the software implementation of stochastic numerical\n  Runge-Kutta", "comments": "in English, in Russian", "journal-ref": null, "doi": "10.1007/978-3-319-99447-5_46", "report-no": null, "categories": "cs.NA cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses stochastic numerical methods of Runge-Kutta type with\nweak and strong convergences for systems of stochastic differential equations\nin It\\^o form. At the beginning we give a brief overview of the stochastic\nnumerical methods and information from the theory of stochastic differential\nequations. Then we motivate the approach to the implementation of these methods\nusing source code generation. We discuss the implementation details and the\nused programming languages and libraries\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 08:20:49 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Gevorkyan", "Migran N.", ""], ["Demidova", "Anastasia V.", ""], ["Korolkova", "Anna V.", ""], ["Kulyabov", "Dmitry S.", ""]]}, {"id": "1811.02761", "submitter": "Yohei Miki", "authors": "Yohei Miki", "title": "Gravitational octree code performance evaluation on Volta GPU", "comments": "10 pages, 10 figures, 2 tables, submitted to Computer Physics\n  Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS astro-ph.IM cs.PF physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, the gravitational octree code originally optimized for the\nFermi, Kepler, and Maxwell GPU architectures is adapted to the Volta\narchitecture. The Volta architecture introduces independent thread scheduling\nrequiring either the insertion of the explicit synchronizations at appropriate\nlocations or the enforcement of the same implicit synchronizations as do the\nPascal or earlier architectures by specifying \\texttt{-gencode\narch=compute\\_60,code=sm\\_70}. The performance measurements on Tesla V100, the\ncurrent flagship GPU by NVIDIA, revealed that the $N$-body simulations of the\nAndromeda galaxy model with $2^{23} = 8388608$ particles took $3.8 \\times\n10^{-2}$~s or $3.3 \\times 10^{-2}$~s per step for each case. Tesla V100\nachieves a 1.4 to 2.2-fold acceleration in comparison with Tesla P100, the\nflagship GPU in the previous generation. The observed speed-up of 2.2 is\ngreater than 1.5, which is the ratio of the theoretical peak performance of the\ntwo GPUs. The independence of the units for integer operations from those for\nfloating-point number operations enables the overlapped execution of integer\nand floating-point number operations. It hides the execution time of the\ninteger operations leading to the speed-up rate above the theoretical peak\nperformance ratio. Tesla V100 can execute $N$-body simulation with up to $25\n\\times 2^{20} = 26214400$ particles, and it took $2.0 \\times 10^{-1}$~s per\nstep. It corresponds to $3.5$~TFlop/s, which is 22\\% of the single-precision\ntheoretical peak performance.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 05:00:23 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Miki", "Yohei", ""]]}, {"id": "1811.03559", "submitter": "Eric Polizzi", "authors": "Braegan S. Spring, Eric Polizzi, Ahmed H. Sameh", "title": "A Feature Complete SPIKE Banded Algorithm and Solver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CE cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New features and enhancements for the SPIKE banded solver are presented.\nAmong all the SPIKE algorithm versions, we focus our attention on the recursive\nSPIKE technique which provides the best trade-off between generality and\nparallel efficiency, but was known for its lack of flexibility. Its application\nwas essentially limited to power of two number of cores/processors. This\nlimitation is successfully addressed in this paper. In addition, we present a\nnew transpose solve option, a standard feature of most numerical solver\nlibraries which has never been addressed by the SPIKE algorithm so far. A\npivoting recursive SPIKE strategy is finally presented as an alternative to\nnon-pivoting scheme for systems with large condition numbers. All these new\nenhancements participate to create a feature complete SPIKE algorithm and a new\nblack-box SPIKE-OpenMP package that significantly outperforms the performance\nand scalability obtained with other state-of-the-art banded solvers.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 17:28:23 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Spring", "Braegan S.", ""], ["Polizzi", "Eric", ""], ["Sameh", "Ahmed H.", ""]]}, {"id": "1811.04035", "submitter": "Sukanta Das Dr.", "authors": "Kamalika Bhattacharjee, Krishnendu Maity, Sukanta Das", "title": "A Search for Good Pseudo-random Number Generators : Survey and Empirical\n  Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's world, several applications demand numbers which appear random but\nare generated by a background algorithm; that is, pseudo-random numbers. Since\nlate $19^{th}$ century, researchers have been working on pseudo-random number\ngenerators (PRNGs). Several PRNGs continue to develop, each one demanding to be\nbetter than the previous ones. In this scenario, this paper targets to verify\nthe claim of so-called good generators and rank the existing generators based\non strong empirical tests in same platforms. To do this, the genre of PRNGs\ndeveloped so far has been explored and classified into three groups -- linear\ncongruential generator based, linear feedback shift register based and cellular\nautomata based. From each group, well-known generators have been chosen for\nempirical testing. Two types of empirical testing has been done on each PRNG --\nblind statistical tests with Diehard battery of tests, TestU01 library and NIST\nstatistical test-suite and graphical tests (lattice test and space-time diagram\ntest). Finally, the selected $29$ PRNGs are divided into $24$ groups and are\nranked according to their overall performance in all empirical tests.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 07:32:23 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Bhattacharjee", "Kamalika", ""], ["Maity", "Krishnendu", ""], ["Das", "Sukanta", ""]]}, {"id": "1811.05031", "submitter": "Charles Margossian", "authors": "Charles C. Margossian", "title": "A Review of automatic differentiation and its efficient implementation", "comments": "32 pages, 5 figures, submitted for publication. WIREs Data Mining\n  Knowl Discov, March 2019", "journal-ref": null, "doi": "10.1002/WIDM.1305", "report-no": null, "categories": "cs.MS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Derivatives play a critical role in computational statistics, examples being\nBayesian inference using Hamiltonian Monte Carlo sampling and the training of\nneural networks. Automatic differentiation is a powerful tool to automate the\ncalculation of derivatives and is preferable to more traditional methods,\nespecially when differentiating complex algorithms and mathematical functions.\nThe implementation of automatic differentiation however requires some care to\ninsure efficiency. Modern differentiation packages deploy a broad range of\ncomputational techniques to improve applicability, run time, and memory\nmanagement. Among these techniques are operation overloading, region based\nmemory, and expression templates. There also exist several mathematical\ntechniques which can yield high performance gains when applied to complex\nalgorithms. For example, semi-analytical derivatives can reduce by orders of\nmagnitude the runtime required to numerically solve and differentiate an\nalgebraic equation. Open problems include the extension of current packages to\nprovide more specialized routines, and efficient methods to perform\nhigher-order differentiation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 22:52:46 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 11:37:12 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Margossian", "Charles C.", ""]]}, {"id": "1811.05213", "submitter": "Guoping Long", "authors": "Guoping Long and Jun Yang and Kai Zhu and Wei Lin", "title": "FusionStitching: Deep Fusion and Code Generation for Tensorflow\n  Computations on GPUs", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there is a surge on machine learning applications in\nindustry. Many of them are based on popular AI frameworks like Tensorflow,\nTorch, Caffe, or MxNet, etc, and are enpowered by accelerator platforms such as\nGPUs. One important challenge of running Tensorflow computations on GPUs is the\nfine granularity problem, namely, FLOPS of individual ops are far from enough\nto fully exploit the computing power of underlying accelerators. The XLA\nframework provides a solid foundation to explore this problem further. In this\npaper, we propose FusionStitching, a novel, comprehensive Op fusion and code\ngeneration system to stitch computations into large GPU kernels. Experimental\nresults on four public models and two of our large inhouse applications show\nanother 55% (geometric mean) reduction of GPU kernel launches, compared to the\nXLA fusion baseline. This increases the E2E performance of both of our latency\ncritical inhouse applications up to 20%.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 11:03:16 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Long", "Guoping", ""], ["Yang", "Jun", ""], ["Zhu", "Kai", ""], ["Lin", "Wei", ""]]}, {"id": "1811.05704", "submitter": "Denis Demidov", "authors": "Denis Demidov", "title": "AMGCL: an Efficient, Flexible, and Extensible Algebraic Multigrid\n  Implementation", "comments": null, "journal-ref": null, "doi": "10.1134/S1995080219050056", "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents AMGCL -- an opensource C++ library implementing the\nalgebraic multigrid method (AMG) for solution of large sparse linear systems of\nequations, usually arising from discretization of partial differential\nequations on an unstructured grid. The library supports both shared and\ndistributed memory computation, allows to utilize modern massively parallel\nprocessors via OpenMP, OpenCL, or CUDA technologies, has minimal dependencies,\nand is easily extensible. The design principles behind AMGCL are discussed and\nit is shown that the code performance is on par with alternative\nimplementations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 09:56:30 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Demidov", "Denis", ""]]}, {"id": "1811.07717", "submitter": "Atena Rezaei", "authors": "Qin He, Atena Rezaei, Sampsa Pursiainen", "title": "Zeffiro user interface for electromagnetic brain imaging: a GPU\n  accelerated FEM tool for forward and inverse computations in Matlab", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CE math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces the Zeffiro interface (ZI) version 2.2 for brain\nimaging. ZI aims to provide a simple, accessible and multimodal open source\nplatform for finite element method (FEM) based and graphics processing unit\n(GPU) accelerated forward and inverse computations in the Matlab environment.\nIt allows one to (1) generate a given multi-compartment head model, (2) to\nevaluate a lead field matrix as well as (3) to invert and analyze a given set\nof measurements. GPU acceleration is applied in each of the processing stages\n(1)-(3). In its current configuration, ZI includes forward solvers for\nelectro-/magnetoencephalography (EEG) and linearized electrical impedance\ntomography (EIT) as well as a set of inverse solvers based on the hierarchical\nBayesian model (HBM). We report the results of EEG and EIT inversion tests\nperformed with real and synthetic data, respectively, and demonstrate\nnumerically how the inversion parameters affect the EEG inversion outcome in\nHBM. The GPU acceleration was found to be essential in the generation of the FE\nmesh and the LF matrix in order to achieve a reasonable computing time. The\ncode package can be extended in the future based on the directions given in\nthis article.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 14:26:14 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 14:59:54 GMT"}, {"version": "v3", "created": "Fri, 14 Jun 2019 09:15:05 GMT"}, {"version": "v4", "created": "Tue, 3 Sep 2019 08:56:03 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["He", "Qin", ""], ["Rezaei", "Atena", ""], ["Pursiainen", "Sampsa", ""]]}, {"id": "1811.08282", "submitter": "Kyle Niemeyer", "authors": "Daniel J. Magee, Anthony S. Walker, and Kyle E. Niemeyer", "title": "Applying the swept rule for solving explicit partial differential\n  equations on heterogeneous computing systems", "comments": "24 pages, 9 figures. Accepted for publication by the Journal of\n  Supercomputing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications that exploit the architectural details of high-performance\ncomputing (HPC) systems have become increasingly invaluable in academia and\nindustry over the past two decades. The most important hardware development of\nthe last decade in HPC has been the General Purpose Graphics Processing Unit\n(GPGPU), a class of massively parallel devices that now contributes the\nmajority of computational power in the top 500 supercomputers. As these systems\ngrow, small costs such as latency---due to the fixed cost of memory accesses\nand communication---accumulate in a large simulation and become a significant\nbarrier to performance. The swept time-space decomposition rule is a\ncommunication-avoiding technique for time-stepping stencil update formulas that\nattempts to reduce latency costs. This work extends the swept rule by targeting\nheterogeneous, CPU/GPU architectures representing current and future HPC\nsystems. We compare our approach to a naive decomposition scheme with two test\nequations using an MPI+CUDA pattern on 40 processes over two nodes containing\none GPU. The swept rule produces a factor of 1.9 to 23 speedup for the heat\nequation and a factor of 1.1 to 2.0 speedup for the Euler equations, using the\nsame processors and work distribution, and with the best possible\nconfigurations. These results show the potential effectiveness of the swept\nrule for different equations and numerical schemes on massively parallel\ncomputing systems that incur substantial latency costs.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 20:22:04 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 18:43:27 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Magee", "Daniel J.", ""], ["Walker", "Anthony S.", ""], ["Niemeyer", "Kyle E.", ""]]}, {"id": "1811.08309", "submitter": "Md Aamir Raihan", "authors": "Md Aamir Raihan, Negar Goli, Tor Aamodt", "title": "Modeling Deep Learning Accelerator Enabled GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficacy of deep learning has resulted in its use in a growing number of\napplications. The Volta graphics processor unit (GPU) architecture from NVIDIA\nintroduced a specialized functional unit, the \"tensor core\", that helps meet\nthe growing demand for higher performance for deep learning. In this paper we\nstudy the design of the tensor cores in NVIDIA's Volta and Turing\narchitectures. We further propose an architectural model for the tensor cores\nin Volta. When implemented a GPU simulator, GPGPU-Sim, our tensor core model\nachieves 99.6\\% correlation versus an NVIDIA Titan~V GPU in terms of average\ninstructions per cycle when running tensor core enabled GEMM workloads. We also\ndescribe support added to enable GPGPU-Sim to run CUTLASS, an open-source CUDA\nC++ template library providing customizable GEMM templates that utilize tensor\ncores.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 00:07:34 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 02:11:33 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Raihan", "Md Aamir", ""], ["Goli", "Negar", ""], ["Aamodt", "Tor", ""]]}, {"id": "1811.08768", "submitter": "Conrad Sanderson", "authors": "Conrad Sanderson and Ryan Curtin", "title": "Practical Sparse Matrices in C++ with Hybrid Storage and Template-Based\n  Expression Optimisation", "comments": "extended and revised version of an earlier conference paper\n  arXiv:1805.03380", "journal-ref": null, "doi": "10.3390/mca24030070", "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the importance of sparse matrices in numerous fields of science,\nsoftware implementations remain difficult to use for non-expert users,\ngenerally requiring the understanding of underlying details of the chosen\nsparse matrix storage format. In addition, to achieve good performance, several\nformats may need to be used in one program, requiring explicit selection and\nconversion between the formats. This can be both tedious and error-prone,\nespecially for non-expert users. Motivated by these issues, we present a\nuser-friendly and open-source sparse matrix class for the C++ language, with a\nhigh-level application programming interface deliberately similar to the widely\nused MATLAB language. This facilitates prototyping directly in C++ and aids the\nconversion of research code into production environments. The class internally\nuses two main approaches to achieve efficient execution: (i) a hybrid storage\nframework, which automatically and seamlessly switches between three underlying\nstorage formats (compressed sparse column, Red-Black tree, coordinate list)\ndepending on which format is best suited and/or available for specific\noperations, and (ii) a template-based meta-programming framework to\nautomatically detect and optimise execution of common expression patterns.\nEmpirical evaluations on large sparse matrices with various densities of\nnon-zero elements demonstrate the advantages of the hybrid storage framework\nand the expression optimisation mechanism.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 06:47:37 GMT"}, {"version": "v2", "created": "Sun, 2 Jun 2019 15:07:12 GMT"}, {"version": "v3", "created": "Mon, 22 Jul 2019 06:10:51 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Sanderson", "Conrad", ""], ["Curtin", "Ryan", ""]]}, {"id": "1811.10274", "submitter": "Eva Darulova", "authors": "Eva Darulova, Anastasia Volkova", "title": "Sound Approximation of Programs with Elementary Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.MS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elementary function calls are a common feature in numerical programs. While\ntheir implementions in library functions are highly optimized, their\ncomputation is nonetheless very expensive compared to plain arithmetic. Full\naccuracy is, however, not always needed. Unlike arithmetic, where the\nperformance difference between for example single and double precision\nfloating-point arithmetic is relatively small, elementary function calls\nprovide a much richer tradeoff space between accuracy and efficiency.\nNavigating this space is challenging. First, generating approximations of\nelementary function calls which are guaranteed to satisfy accuracy error bounds\nis highly nontrivial. Second, the performance of such approximations generally\ndepends on several parameters which are unintuitive to choose manually,\nespecially for non-experts.\n  We present a fully automated approach and tool which approximates elementary\nfunction calls inside small programs while guaranteeing overall user provided\nerror bounds. Our tool leverages existing techniques for roundoff error\ncomputation and approximation of individual elementary function calls, and\nprovides automated selection of many parameters. Our experiments show that\nsignificant efficiency improvements are possible in exchange for reduced, but\nguaranteed, accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 10:30:04 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Darulova", "Eva", ""], ["Volkova", "Anastasia", ""]]}]