[{"id": "1602.01376", "submitter": "William March", "authors": "Chenhan D. Yu, William B. March, Bo Xiao, and George Biros", "title": "Inv-ASKIT: A Parallel Fast Diret Solver for Kernel Matrices", "comments": "11 pages, 2 figures, to appear in IPDPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.DS cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parallel algorithm for computing the approximate factorization\nof an $N$-by-$N$ kernel matrix. Once this factorization has been constructed\n(with $N \\log^2 N $ work), we can solve linear systems with this matrix with $N\n\\log N $ work. Kernel matrices represent pairwise interactions of points in\nmetric spaces. They appear in machine learning, approximation theory, and\ncomputational physics. Kernel matrices are typically dense (matrix\nmultiplication scales quadratically with $N$) and ill-conditioned (solves can\nrequire 100s of Krylov iterations). Thus, fast algorithms for matrix\nmultiplication and factorization are critical for scalability.\n  Recently we introduced ASKIT, a new method for approximating a kernel matrix\nthat resembles N-body methods. Here we introduce INV-ASKIT, a factorization\nscheme based on ASKIT. We describe the new method, derive complexity estimates,\nand conduct an empirical study of its accuracy and scalability. We report\nresults on real-world datasets including \"COVTYPE\" ($0.5$M points in 54\ndimensions), \"SUSY\" ($4.5$M points in 8 dimensions) and \"MNIST\" (2M points in\n784 dimensions) using shared and distributed memory parallelism. In our largest\nrun we approximately factorize a dense matrix of size 32M $\\times$ 32M\n(generated from points in 64 dimensions) on 4,096 Sandy-Bridge cores. To our\nknowledge these results improve the state of the art by several orders of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 17:23:24 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Yu", "Chenhan D.", ""], ["March", "William B.", ""], ["Xiao", "Bo", ""], ["Biros", "George", ""]]}, {"id": "1602.01421", "submitter": "Da Zheng", "authors": "Da Zheng, Randal Burns, Joshua Vogelstein, Carey E. Priebe, Alexander\n  S. Szalay", "title": "An SSD-based eigensolver for spectral analysis on billion-node graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many eigensolvers such as ARPACK and Anasazi have been developed to compute\neigenvalues of a large sparse matrix. These eigensolvers are limited by the\ncapacity of RAM. They run in memory of a single machine for smaller eigenvalue\nproblems and require the distributed memory for larger problems.\n  In contrast, we develop an SSD-based eigensolver framework called FlashEigen,\nwhich extends Anasazi eigensolvers to SSDs, to compute eigenvalues of a graph\nwith hundreds of millions or even billions of vertices in a single machine.\nFlashEigen performs sparse matrix multiplication in a semi-external memory\nfashion, i.e., we keep the sparse matrix on SSDs and the dense matrix in\nmemory. We store the entire vector subspace on SSDs and reduce I/O to improve\nperformance through caching the most recent dense matrix. Our result shows that\nFlashEigen is able to achieve 40%-60% performance of its in-memory\nimplementation and has performance comparable to the Anasazi eigensolvers on a\nmachine with 48 CPU cores. Furthermore, it is capable of scaling to a graph\nwith 3.4 billion vertices and 129 billion edges. It takes about four hours to\ncompute eight eigenvalues of the billion-node graph using 120 GB memory.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 19:23:44 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2016 13:22:57 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2016 06:43:03 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Zheng", "Da", ""], ["Burns", "Randal", ""], ["Vogelstein", "Joshua", ""], ["Priebe", "Carey E.", ""], ["Szalay", "Alexander S.", ""]]}, {"id": "1602.03638", "submitter": "Mikael Mortensen", "authors": "Mikael Mortensen and Hans Petter Langtangen", "title": "High performance Python for direct numerical simulations of turbulent\n  flows", "comments": null, "journal-ref": null, "doi": "10.1016/j.cpc.2016.02.005", "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct Numerical Simulations (DNS) of the Navier Stokes equations is an\ninvaluable research tool in fluid dynamics. Still, there are few publicly\navailable research codes and, due to the heavy number crunching implied,\navailable codes are usually written in low-level languages such as C/C++ or\nFortran. In this paper we describe a pure scientific Python pseudo-spectral DNS\ncode that nearly matches the performance of C++ for thousands of processors and\nbillions of unknowns. We also describe a version optimized through Cython, that\nis found to match the speed of C++. The solvers are written from scratch in\nPython, both the mesh, the MPI domain decomposition, and the temporal\nintegrators. The solvers have been verified and benchmarked on the Shaheen\nsupercomputer at the KAUST supercomputing laboratory, and we are able to show\nvery good scaling up to several thousand cores.\n  A very important part of the implementation is the mesh decomposition (we\nimplement both slab and pencil decompositions) and 3D parallel Fast Fourier\nTransforms (FFT). The mesh decomposition and FFT routines have been implemented\nin Python using serial FFT routines (either NumPy, pyFFTW or any other serial\nFFT module), NumPy array manipulations and with MPI communications handled by\nMPI for Python (mpi4py). We show how we are able to execute a 3D parallel FFT\nin Python for a slab mesh decomposition using 4 lines of compact Python code,\nfor which the parallel performance on Shaheen is found to be slightly better\nthan similar routines provided through the FFTW library. For a pencil mesh\ndecomposition 7 lines of code is required to execute a transform.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 08:12:37 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Mortensen", "Mikael", ""], ["Langtangen", "Hans Petter", ""]]}, {"id": "1602.03643", "submitter": "Mikael Mortensen", "authors": "Mikael Mortensen and Kristian Valen-Sendstad", "title": "Oasis: a high-level/high-performance open source Navier-Stokes solver", "comments": null, "journal-ref": "Computer Physics Communications, Volume 188, p 177-188, 2015", "doi": "10.1016/j.cpc.2014.10.026", "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oasis is a high-level/high-performance finite element Navier-Stokes solver\nwritten from scratch in Python using building blocks from the FEniCS project\n(fenicsproject.org). The solver is unstructured and targets large-scale\napplications in complex geometries on massively parallel clusters. Oasis\nutilizes MPI and interfaces, through FEniCS, to the linear algebra backend\nPETSc. Oasis advocates a high-level, programmable user interface through the\ncreation of highly flexible Python modules for new problems. Through the\nhigh-level Python interface the user is placed in complete control of every\naspect of the solver. A version of the solver, that is using piecewise linear\nelements for both velocity and pressure, is shown reproduce very well the\nclassical, spectral, turbulent channel simulations of Moser, Kim and Mansour at\n$Re_{\\tau}=180$ [Phys. Fluids, vol 11(4), p. 964]. The computational speed is\nstrongly dominated by the iterative solvers provided by the linear algebra\nbackend, which is arguably the best performance any similar implicit solver\nusing PETSc may hope for. Higher order accuracy is also demonstrated and new\nsolvers may be easily added within the same framework.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 08:56:43 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Mortensen", "Mikael", ""], ["Valen-Sendstad", "Kristian", ""]]}, {"id": "1602.06763", "submitter": "Elmar Peise", "authors": "Elmar Peise (1), Paolo Bientinesi (1) ((1) AICES, RWTH Aachen)", "title": "Recursive Algorithms for Dense Linear Algebra: The ReLAPACK Collection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To exploit both memory locality and the full performance potential of highly\ntuned kernels, dense linear algebra libraries such as LAPACK commonly implement\noperations as blocked algorithms. However, to achieve next-to-optimal\nperformance with such algorithms, significant tuning is required. On the other\nhand, recursive algorithms are virtually tuning free, and yet attain similar\nperformance. In this paper, we first analyze and compare blocked and recursive\nalgorithms in terms of performance, and then introduce ReLAPACK, an open-source\nlibrary of recursive algorithms to seamlessly replace most of LAPACK's blocked\nalgorithms. In many scenarios, ReLAPACK clearly outperforms reference LAPACK,\nand even improves upon the performance of optimizes libraries.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 13:21:05 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Peise", "Elmar", "", "AICES, RWTH Aachen"], ["Bientinesi", "Paolo", "", "AICES, RWTH Aachen"]]}, {"id": "1602.07527", "submitter": "Iain Murray", "authors": "Iain Murray", "title": "Differentiation of the Cholesky decomposition", "comments": "18 pages, including 7 pages of code listings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review strategies for differentiating matrix-based computations, and\nderive symbolic and algorithmic update rules for differentiating expressions\ncontaining the Cholesky decomposition. We recommend new `blocked' algorithms,\nbased on differentiating the Cholesky algorithm DPOTRF in the LAPACK library,\nwhich uses `Level 3' matrix-matrix operations from BLAS, and so is\ncache-friendly and easy to parallelize. For large matrices, the resulting\nalgorithms are the fastest way to compute Cholesky derivatives, and are an\norder of magnitude faster than the algorithms in common usage. In some\ncomputing environments, symbolically-derived updates are faster for small\nmatrices than those based on differentiating Cholesky algorithms. The symbolic\nand algorithmic approaches can be combined to get the best of both worlds.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 14:35:31 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Murray", "Iain", ""]]}, {"id": "1602.08477", "submitter": "Erik Zenker", "authors": "Erik Zenker, Benjamin Worpitz, Ren\\'e Widera, Axel Huebl, Guido\n  Juckeland, Andreas Kn\\\"upfer, Wolfgang E. Nagel, Michael Bussmann", "title": "Alpaka - An Abstraction Library for Parallel Kernel Acceleration", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": "10.1109/IPDPSW.2016.50", "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Porting applications to new hardware or programming models is a tedious and\nerror prone process. Every help that eases these burdens is saving developer\ntime that can then be invested into the advancement of the application itself\ninstead of preserving the status-quo on a new platform.\n  The Alpaka library defines and implements an abstract hierarchical redundant\nparallelism model. The model exploits parallelism and memory hierarchies on a\nnode at all levels available in current hardware. By doing so, it allows to\nachieve platform and performance portability across various types of\naccelerators by ignoring specific unsupported levels and utilizing only the\nones supported on a specific accelerator. All hardware types (multi- and\nmany-core CPUs, GPUs and other accelerators) are supported for and can be\nprogrammed in the same way. The Alpaka C++ template interface allows for\nstraightforward extension of the library to support other accelerators and\nspecialization of its internals for optimization.\n  Running Alpaka applications on a new (and supported) platform requires the\nchange of only one source code line instead of a lot of \\#ifdefs.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 20:49:37 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Zenker", "Erik", ""], ["Worpitz", "Benjamin", ""], ["Widera", "Ren\u00e9", ""], ["Huebl", "Axel", ""], ["Juckeland", "Guido", ""], ["Kn\u00fcpfer", "Andreas", ""], ["Nagel", "Wolfgang E.", ""], ["Bussmann", "Michael", ""]]}, {"id": "1602.08991", "submitter": "Tobias Leibner", "authors": "Tobias Leibner, Ren\\'e Milk, Felix Schindler", "title": "Extending DUNE: The dune-xt modules", "comments": null, "journal-ref": "Archive of Numerical Software, 5 (2017), pp. 193-216", "doi": "10.11588/ans.2017.1.27720", "report-no": null, "categories": "cs.MS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our effort to extend and complement the core modules of the\nDistributed and Unified Numerics Environment DUNE (http://dune-project.org) by\na well tested and structured collection of utilities and concepts. We describe\nkey elements of our four modules dune-xt-common, dune-xt-grid, dune-xt-la and\ndune-xt-functions, which aim at further enabling the programming of generic\nalgorithms within DUNE as well as adding an extra layer of usability and\nconvenience.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 22:42:50 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Leibner", "Tobias", ""], ["Milk", "Ren\u00e9", ""], ["Schindler", "Felix", ""]]}]