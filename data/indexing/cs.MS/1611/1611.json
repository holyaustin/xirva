[{"id": "1611.00675", "submitter": "Christian Himpe", "authors": "Christian Himpe", "title": "emgr - The Empirical Gramian Framework", "comments": null, "journal-ref": "Algorithms 11(7): 91, 2018", "doi": "10.3390/a11070091", "report-no": null, "categories": "cs.MS cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System Gramian matrices are a well-known encoding for properties of\ninput-output systems such as controllability, observability or minimality.\nThese so-called system Gramians were developed in linear system theory for\napplications such as model order reduction of control systems. Empirical\nGramian are an extension to the system Gramians for parametric and nonlinear\nsystems as well as a data-driven method of computation. The empirical Gramian\nframework - emgr - implements the empirical Gramians in a uniform and\nconfigurable manner, with applications such as Gramian-based (nonlinear) model\nreduction, decentralized control, sensitivity analysis, parameter\nidentification and combined state and parameter reduction.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 16:39:06 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 11:47:47 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Himpe", "Christian", ""]]}, {"id": "1611.01120", "submitter": "Jianyu Huang", "authors": "Jianyu Huang, Leslie Rice, Devin A. Matthews, Robert A. van de Geijn", "title": "Generating Families of Practical Fast Matrix Multiplication Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": "FLAME Working Note #82, The University of Texas at Austin,\n  Department of Computer Science, Technical Report TR-16-18", "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix multiplication (GEMM) is a core operation to numerous scientific\napplications. Traditional implementations of Strassen-like fast matrix\nmultiplication (FMM) algorithms often do not perform well except for very large\nmatrix sizes, due to the increased cost of memory movement, which is\nparticularly noticeable for non-square matrices. Such implementations also\nrequire considerable workspace and modifications to the standard BLAS\ninterface. We propose a code generator framework to automatically implement a\nlarge family of FMM algorithms suitable for multiplications of arbitrary matrix\nsizes and shapes. By representing FMM with a triple of matrices [U,V,W] that\ncapture the linear combinations of submatrices that are formed, we can use the\nKronecker product to define a multi-level representation of Strassen-like\nalgorithms. Incorporating the matrix additions that must be performed for\nStrassen-like algorithms into the inherent packing and micro-kernel operations\ninside GEMM avoids extra workspace and reduces the cost of memory movement.\nAdopting the same loop structures as high-performance GEMM implementations\nallows parallelization of all FMM algorithms with simple but efficient data\nparallelism without the overhead of task parallelism. We present a simple\nperformance model for general FMM algorithms and compare actual performance of\n20+ FMM algorithms to modeled predictions. Our implementations demonstrate a\nperformance benefit over conventional GEMM on single core and multi-core\nsystems. This study shows that Strassen-like fast matrix multiplication can be\nincorporated into libraries for practical use.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 18:27:00 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Huang", "Jianyu", ""], ["Rice", "Leslie", ""], ["Matthews", "Devin A.", ""], ["van de Geijn", "Robert A.", ""]]}, {"id": "1611.01534", "submitter": "Eemeli Lepp\\\"aaho", "authors": "Eemeli Lepp\\\"aaho, Muhammad Ammad-ud-din, Samuel Kaski", "title": "GFA: Exploratory Analysis of Multiple Data Sources with Group Factor\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package GFA provides a full pipeline for factor analysis of multiple\ndata sources that are represented as matrices with co-occurring samples. It\nallows learning dependencies between subsets of the data sources, decomposed\ninto latent factors. The package also implements sparse priors for the\nfactorization, providing interpretable biclusters of the multi-source data\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 10:09:13 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Lepp\u00e4aho", "Eemeli", ""], ["Ammad-ud-din", "Muhammad", ""], ["Kaski", "Samuel", ""]]}, {"id": "1611.02274", "submitter": "Kyle Niemeyer", "authors": "Kyle E Niemeyer and Chih-Jen Sung", "title": "GPU-Based Parallel Integration of Large Numbers of Independent ODE\n  Systems", "comments": "21 pages, 2 figures", "journal-ref": "Numerical Computations with GPUs, Ch. 8 (2014) 159-182. V\n  Kindratenko (Ed.)", "doi": "10.1007/978-3-319-06548-9_8", "report-no": null, "categories": "cs.MS cs.DC physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task of integrating a large number of independent ODE systems arises in\nvarious scientific and engineering areas. For nonstiff systems, common explicit\nintegration algorithms can be used on GPUs, where individual GPU threads\nconcurrently integrate independent ODEs with different initial conditions or\nparameters. One example is the fifth-order adaptive Runge-Kutta-Cash-Karp\n(RKCK) algorithm. In the case of stiff ODEs, standard explicit algorithms\nrequire impractically small time-step sizes for stability reasons, and implicit\nalgorithms are therefore commonly used instead to allow larger time steps and\nreduce the computational expense. However, typical high-order implicit\nalgorithms based on backwards differentiation formulae (e.g., VODE, LSODE)\ninvolve complex logical flow that causes severe thread divergence when\nimplemented on GPUs, limiting the performance. Therefore, alternate algorithms\nare needed. A GPU-based Runge-Kutta-Chebyshev (RKC) algorithm can handle\nmoderate levels of stiffness and performs significantly faster than not only an\nequivalent CPU version but also a CPU-based implicit algorithm (VODE) based on\nresults shown in the literature. In this chapter, we present the mathematical\nbackground, implementation details, and source code for the RKCK and RKC\nalgorithms for use integrating large numbers of independent systems of ODEs on\nGPUs. In addition, brief performance comparisons are shown for each algorithm,\ndemonstrating the potential benefit of moving to GPU-based ODE integrators.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 18:11:16 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Niemeyer", "Kyle E", ""], ["Sung", "Chih-Jen", ""]]}, {"id": "1611.02831", "submitter": "Fredrik Johansson", "authors": "Fredrik Johansson", "title": "Arb: Efficient Arbitrary-Precision Midpoint-Radius Interval Arithmetic", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arb is a C library for arbitrary-precision interval arithmetic using the\nmidpoint-radius representation, also known as ball arithmetic. It supports real\nand complex numbers, polynomials, power series, matrices, and evaluation of\nmany special functions. The core number types are designed for versatility and\nspeed in a range of scenarios, allowing performance that is competitive with\nnon-interval arbitrary-precision types such as MPFR and MPC floating-point\nnumbers. We discuss the low-level number representation, strategies for\nprecision and error bounds, and the implementation of efficient polynomial\narithmetic with interval coefficients.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 06:23:37 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Johansson", "Fredrik", ""]]}, {"id": "1611.03410", "submitter": "Barak Pearlmutter", "authors": "Jeffrey Mark Siskind and Barak A. Pearlmutter", "title": "Binomial Checkpointing for Arbitrary Programs with No User Annotation", "comments": "Extended abstract presented at the AD 2016 Conference, Sep 2016,\n  Oxford UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heretofore, automatic checkpointing at procedure-call boundaries, to reduce\nthe space complexity of reverse mode, has been provided by systems like\nTapenade. However, binomial checkpointing, or treeverse, has only been provided\nin Automatic Differentiation (AD) systems in special cases, e.g., through\nuser-provided pragmas on DO loops in Tapenade, or as the nested taping\nmechanism in adol-c for time integration processes, which requires that user\ncode be refactored. We present a framework for applying binomial checkpointing\nto arbitrary code with no special annotation or refactoring required. This is\naccomplished by applying binomial checkpointing directly to a program trace.\nThis trace is produced by a general-purpose checkpointing mechanism that is\northogonal to AD.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 17:29:24 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Siskind", "Jeffrey Mark", ""], ["Pearlmutter", "Barak A.", ""]]}, {"id": "1611.03416", "submitter": "Barak Pearlmutter", "authors": "Jeffrey Mark Siskind and Barak A. Pearlmutter", "title": "Efficient Implementation of a Higher-Order Language with Built-In AD", "comments": "Extended abstract presented at the AD 2016 Conference, Sep 2016,\n  Oxford UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that Automatic Differentiation (AD) operators can be provided in a\ndynamic language without sacrificing numeric performance. To achieve this,\ngeneral forward and reverse AD functions are added to a simple high-level\ndynamic language, and support for them is included in an aggressive optimizing\ncompiler. Novel technical mechanisms are discussed, which have the ability to\nmigrate the AD transformations from run-time to compile-time. The resulting\nsystem, although only a research prototype, exhibits startlingly good\nperformance. In fact, despite the potential inefficiencies entailed by support\nof a functional-programming language and a first-class AD operator, performance\nis competitive with the fastest available preprocessor-based Fortran AD\nsystems. On benchmarks involving nested use of the AD operators, it can even\ndramatically exceed their performance.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 17:40:53 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Siskind", "Jeffrey Mark", ""], ["Pearlmutter", "Barak A.", ""]]}, {"id": "1611.03423", "submitter": "Barak Pearlmutter", "authors": "At{\\i}l{\\i}m G\\\"une\\c{s} Baydin and Barak A. Pearlmutter and Jeffrey\n  Mark Siskind", "title": "DiffSharp: An AD Library for .NET Languages", "comments": "Extended abstract presented at the AD 2016 Conference, Sep 2016,\n  Oxford UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DiffSharp is an algorithmic differentiation or automatic differentiation (AD)\nlibrary for the .NET ecosystem, which is targeted by the C# and F# languages,\namong others. The library has been designed with machine learning applications\nin mind, allowing very succinct implementations of models and optimization\nroutines. DiffSharp is implemented in F# and exposes forward and reverse AD\noperators as general nestable higher-order functions, usable by any .NET\nlanguage. It provides high-performance linear algebra primitives---scalars,\nvectors, and matrices, with a generalization to tensors underway---that are\nfully supported by all the AD operators, and which use a BLAS/LAPACK backend\nvia the highly optimized OpenBLAS library. DiffSharp currently uses operator\noverloading, but we are developing a transformation-based version of the\nlibrary using F#'s \"code quotation\" metaprogramming facility. Work on a\nCUDA-based GPU backend is also underway.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 17:50:06 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Baydin", "At\u0131l\u0131m G\u00fcne\u015f", ""], ["Pearlmutter", "Barak A.", ""], ["Siskind", "Jeffrey Mark", ""]]}, {"id": "1611.06365", "submitter": "Rafael Rodriguez-Sanchez", "authors": "Sandra Catal\\'an, Jos\\'e R. Herrero, Enrique S. Quintana-Ort\\'i,\n  Rafael Rodr\\'iguez-S\\'anchez, Robert van de Geijn", "title": "A Case for Malleable Thread-Level Linear Algebra Libraries: The LU\n  Factorization with Partial Pivoting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two novel techniques for overcoming load-imbalance encountered\nwhen implementing so-called look-ahead mechanisms in relevant dense matrix\nfactorizations for the solution of linear systems. Both techniques target the\nscenario where two thread teams are created/activated during the factorization,\nwith each team in charge of performing an independent task/branch of execution.\nThe first technique promotes worker sharing (WS) between the two tasks,\nallowing the threads of the task that completes first to be reallocated for use\nby the costlier task. The second technique allows a fast task to alert the\nslower task of completion, enforcing the early termination (ET) of the second\ntask, and a smooth transition of the factorization procedure into the next\niteration.\n  The two mechanisms are instantiated via a new malleable thread-level\nimplementation of the Basic Linear Algebra Subprograms (BLAS), and their\nbenefits are illustrated via an implementation of the LU factorization with\npartial pivoting enhanced with look-ahead. Concretely, our experimental results\non a six core Intel-Xeon processor show the benefits of combining WS+ET,\nreporting competitive performance in comparison with a task-parallel\nruntime-based solution.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 13:55:29 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Catal\u00e1n", "Sandra", ""], ["Herrero", "Jos\u00e9 R.", ""], ["Quintana-Ort\u00ed", "Enrique S.", ""], ["Rodr\u00edguez-S\u00e1nchez", "Rafael", ""], ["van de Geijn", "Robert", ""]]}, {"id": "1611.06892", "submitter": "Julien Langou", "authors": "Mathieu Faverge and Julien Langou and Yves Robert and Jack Dongarra", "title": "Bidiagonalization with Parallel Tiled Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA math.NA math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider algorithms for going from a \"full\" matrix to a condensed \"band\nbidiagonal\" form using orthogonal transformations. We use the framework of\n\"algorithms by tiles\". Within this framework, we study: (i) the tiled\nbidiagonalization algorithm BiDiag, which is a tiled version of the standard\nscalar bidiagonalization algorithm; and (ii) the R-bidiagonalization algorithm\nR-BiDiag, which is a tiled version of the algorithm which consists in first\nperforming the QR factorization of the initial matrix, then performing the\nband-bidiagonalization of the R-factor. For both bidiagonalization algorithms\nBiDiag and R-BiDiag, we use four main types of reduction trees, namely FlatTS,\nFlatTT, Greedy, and a newly introduced auto-adaptive tree, Auto. We provide a\nstudy of critical path lengths for these tiled algorithms, which shows that (i)\nR-BiDiag has a shorter critical path length than BiDiag for tall and skinny\nmatrices, and (ii) Greedy based schemes are much better than earlier proposed\nvariants with unbounded resources. We provide experiments on a single multicore\nnode, and on a few multicore nodes of a parallel distributed shared-memory\nsystem, to show the superiority of the new algorithms on a variety of matrix\nsizes, matrix shapes and core counts.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 15:11:48 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Faverge", "Mathieu", ""], ["Langou", "Julien", ""], ["Robert", "Yves", ""], ["Dongarra", "Jack", ""]]}, {"id": "1611.06945", "submitter": "Matthew Moskewicz", "authors": "Matthew W. Moskewicz and Ali Jannesari and Kurt Keutzer", "title": "A Metaprogramming and Autotuning Framework for Deploying Deep Learning\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep neural networks (DNNs), have yielded strong results on\na wide range of applications. Graphics Processing Units (GPUs) have been one\nkey enabling factor leading to the current popularity of DNNs. However, despite\nincreasing hardware flexibility and software programming toolchain maturity,\nhigh efficiency GPU programming remains difficult: it suffers from high\ncomplexity, low productivity, and low portability. GPU vendors such as NVIDIA\nhave spent enormous effort to write special-purpose DNN libraries. However, on\nother hardware targets, especially mobile GPUs, such vendor libraries are not\ngenerally available. Thus, the development of portable, open, high-performance,\nenergy-efficient GPU code for DNN operations would enable broader deployment of\nDNN-based algorithms. Toward this end, this work presents a framework to enable\nproductive, high-efficiency GPU programming for DNN computations across\nhardware platforms and programming models. In particular, the framework\nprovides specific support for metaprogramming, autotuning, and DNN-tailored\ndata types. Using our framework, we explore implementing DNN operations on\nthree different hardware targets: NVIDIA, AMD, and Qualcomm GPUs. On NVIDIA\nGPUs, we show both portability between OpenCL and CUDA as well competitive\nperformance compared to the vendor library. On Qualcomm GPUs, we show that our\nframework enables productive development of target-specific optimizations, and\nachieves reasonable absolute performance. Finally, On AMD GPUs, we show initial\nresults that indicate our framework can yield reasonable performance on a new\nplatform with minimal effort.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 18:49:23 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Moskewicz", "Matthew W.", ""], ["Jannesari", "Ali", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1611.07819", "submitter": "Steven Eliuk", "authors": "Steven Eliuk, Cameron Upright, Hars Vardhan, Stephen Walsh, Trevor\n  Gale", "title": "dMath: Distributed Linear Algebra for DL", "comments": "5 pages. arXiv admin note: text overlap with arXiv:1604.01416", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a parallel math library, dMath, that demonstrates leading\nscaling when using intranode, internode, and hybrid-parallelism for deep\nlearning (DL). dMath provides easy-to-use distributed primitives and a variety\nof domain-specific algorithms including matrix multiplication, convolutions,\nand others allowing for rapid development of scalable applications like deep\nneural networks (DNNs). Persistent data stored in GPU memory and advanced\nmemory management techniques avoid costly transfers between host and device.\ndMath delivers performance, portability, and productivity to its specific\ndomain of support.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 00:24:12 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Eliuk", "Steven", ""], ["Upright", "Cameron", ""], ["Vardhan", "Hars", ""], ["Walsh", "Stephen", ""], ["Gale", "Trevor", ""]]}, {"id": "1611.08035", "submitter": "Richard Veras", "authors": "Richard Michael Veras, Tze Meng Low, Tyler Michael Smith, Robert van\n  de Geijn, Franz Franchetti", "title": "Automating the Last-Mile for High Performance Dense Linear Algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High performance dense linear algebra (DLA) libraries often rely on a general\nmatrix multiply (Gemm) kernel that is implemented using assembly or with vector\nintrinsics. In particular, the real-valued Gemm kernels provide the\noverwhelming fraction of performance for the complex-valued Gemm kernels, along\nwith the entire level-3 BLAS and many of the real and complex LAPACK routines.\nThus,achieving high performance for the Gemm kernel translates into a high\nperformance linear algebra stack above this kernel. However, it is a monumental\ntask for a domain expert to manually implement the kernel for every\nlibrary-supported architecture. This leads to the belief that the craft of a\nGemm kernel is more dark art than science. It is this premise that drives the\npopularity of autotuning with code generation in the domain of DLA.\n  This paper, instead, focuses on an analytical approach to code generation of\nthe Gemm kernel for different architecture, in order to shed light on the\ndetails or voo-doo required for implementing a high performance Gemm kernel. We\ndistill the implementation of the kernel into an even smaller kernel, an\nouter-product, and analytically determine how available SIMD instructions can\nbe used to compute the outer-product efficiently. We codify this approach into\na system to automatically generate a high performance SIMD implementation of\nthe Gemm kernel. Experimental results demonstrate that our approach yields\ngenerated kernels with performance that is competitive with kernels implemented\nmanually or using empirical search.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 00:01:07 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 15:22:19 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Veras", "Richard Michael", ""], ["Low", "Tze Meng", ""], ["Smith", "Tyler Michael", ""], ["van de Geijn", "Robert", ""], ["Franchetti", "Franz", ""]]}, {"id": "1611.08832", "submitter": "Ambros Gleixner", "authors": "Kevin K. H. Cheung, Ambros Gleixner, Daniel E. Steffy", "title": "Verifying Integer Programming Results", "comments": "Zuse Institute Berlin, Takustr. 7, 14195 Berlin, November 2016,\n  http://nbn-resolving.de/urn:nbn:de:0297-zib-61044", "journal-ref": "In F. Eisenbrand and J. Koenemann, eds., Integer Programming and\n  Combinatorial Optimization: 19th International Conference, IPCO 2017, pages\n  148-160, 2017", "doi": "10.1007/978-3-319-59250-3_13", "report-no": "ZIB-Report 16-58", "categories": "math.OC cs.DM cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software for mixed-integer linear programming can return incorrect results\nfor a number of reasons, one being the use of inexact floating-point\narithmetic. Even solvers that employ exact arithmetic may suffer from\nprogramming or algorithmic errors, motivating the desire for a way to produce\nindependently verifiable certificates of claimed results. Due to the complex\nnature of state-of-the-art MILP solution algorithms, the ideal form of such a\ncertificate is not entirely clear. This paper proposes such a certificate\nformat, illustrating its capabilities and structure through examples. The\ncertificate format is designed with simplicity in mind and is composed of a\nlist of statements that can be sequentially verified using a limited number of\nsimple yet powerful inference rules. We present a supplementary verification\ntool for compressing and checking these certificates independently of how they\nwere created. We report computational results on a selection of mixed-integer\nlinear programming instances from the literature. To this end, we have extended\nthe exact rational version of the MIP solver SCIP to produce such certificates.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 12:27:03 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 20:39:16 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Cheung", "Kevin K. H.", ""], ["Gleixner", "Ambros", ""], ["Steffy", "Daniel E.", ""]]}, {"id": "1611.09567", "submitter": "Walter Mascarenhas", "authors": "Walter F. Mascarenhas", "title": "Moore: Interval Arithmetic in Modern C++", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the library Moore, which implements Interval Arithmetic in modern\nC++. This library is based on a new feature in the C++ language called\nconcepts, which reduces the problems caused by template meta programming, and\nleads to a new approach for implementing interval arithmetic libraries in C++.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 11:12:06 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Mascarenhas", "Walter F.", ""]]}]