[{"id": "1510.01122", "submitter": "Karl Rupp", "authors": "K. Rupp, S. Balay, J. Brown, M. Knepley, L. C. McInnes, B. Smith", "title": "On The Evolution Of User Support Topics in Computational Science and\n  Engineering Software", "comments": "2 pages, 1 figure, whitepaper for the workshop \"Computational Science\n  & Engineering Software Sustainability and Productivity Challenges\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate ten years of user support emails in the large-scale solver\nlibrary PETSc in order to identify changes in user requests. For this purpose\nwe assign each email thread to one or several categories describing the type of\nsupport request. We find that despite several changes in hardware architecture\nas well programming models, the relative share of emails for the individual\ncategories does not show a notable change over time. This is particularly\nremarkable as the total communication volume has increased four-fold in the\nconsidered time frame, indicating a considerable growth of the user base. Our\ndata also demonstrates that user support cannot be substituted with what is\noften referred to as 'better documentation' and that the involvement of core\ndevelopers in user support is essential.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2015 12:19:46 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Rupp", "K.", ""], ["Balay", "S.", ""], ["Brown", "J.", ""], ["Knepley", "M.", ""], ["McInnes", "L. C.", ""], ["Smith", "B.", ""]]}, {"id": "1510.02545", "submitter": "David Avis", "authors": "David Avis and Charles Jordan", "title": "Comparative computational results for some vertex and facet enumeration\n  codes", "comments": "7 pages, 6 tables. Revised version now includes PORTA, normaliz and\n  v6.2 of lrslib. The cluster and test set used were also expanded. Broken\n  table footnote fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report some computational results comparing parallel and sequential codes\nfor vertex/facet enumeration problems for convex polyhedra. The problems chosen\nspan the range from simple to highly degenerate polytopes. We tested one code\n(lrs) based on pivoting and four codes (cddr+, ppl, normaliz, PORTA) based on\nthe double description method. normaliz employs parallelization as do the codes\nplrs and mplrs which are based on lrs. We tested these codes using various\nhardware configurations with up to 1200 cores. Major speedups were obtained by\nparallelization, particularly by the code mplrs which uses MPI and can operate\non clusters of machines.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 02:07:10 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 07:08:04 GMT"}, {"version": "v3", "created": "Tue, 14 Jun 2016 01:45:20 GMT"}], "update_date": "2016-06-15", "authors_parsed": [["Avis", "David", ""], ["Jordan", "Charles", ""]]}, {"id": "1510.02789", "submitter": "Jean-Philippe Chancelier", "authors": "Jean-Philippe Chancelier (CERMICS), Ramine Nikoukhah (METALAU)", "title": "A novel code generation methodology for block diagram modeler and\n  simulators Scicos and VSS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Block operations during simulation in Scicos and VSS environments can\nnaturally be described as Nsp functions. But the direct use of Nsp functions\nfor simulation leads to poor performance since the Nsp language is interpreted,\nnot compiled. The methodology presented in this paper is used to develop a tool\nfor generating efficient compilable code, such as C and ADA, for Scicos and VSS\nmodels from these block Nsp functions. Operator overloading and partial\nevaluation are the key elements of this novel approach. This methodology may be\nused in other simulation environments such as Matlab/Simulink.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 09:47:51 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Chancelier", "Jean-Philippe", "", "CERMICS"], ["Nikoukhah", "Ramine", "", "METALAU"]]}, {"id": "1510.04068", "submitter": "Jeroen B\\'edorf", "authors": "Jeroen B\\'edorf, Evghenii Gaburov and Simon Portegies Zwart", "title": "Sapporo2: A versatile direct $N$-body library", "comments": "15 pages, 7 figures. Accepted for publication in Computational\n  Astrophysics and Cosmology", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astrophysical direct $N$-body methods have been one of the first production\nalgorithms to be implemented using NVIDIA's CUDA architecture. Now, almost\nseven years later, the GPU is the most used accelerator device in astronomy for\nsimulating stellar systems. In this paper we present the implementation of the\nSapporo2 $N$-body library, which allows researchers to use the GPU for $N$-body\nsimulations with little to no effort. The first version, released five years\nago, is actively used, but lacks advanced features and versatility in numerical\nprecision and support for higher order integrators. In this updated version we\nhave rebuilt the code from scratch and added support for OpenCL,\nmulti-precision and higher order integrators. We show how to tune these codes\nfor different GPU architectures and present how to continue utilizing the GPU\noptimal even when only a small number of particles ($N < 100$) is integrated.\nThis careful tuning allows Sapporo2 to be faster than Sapporo1 even with the\nadded options and double precision data loads. The code runs on a range of\nNVIDIA and AMD GPUs in single and double precision accuracy. With the addition\nof OpenCL support the library is also able to run on CPUs and other\naccelerators that support OpenCL.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2015 12:56:13 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["B\u00e9dorf", "Jeroen", ""], ["Gaburov", "Evghenii", ""], ["Zwart", "Simon Portegies", ""]]}, {"id": "1510.04914", "submitter": "Charlie Vanaret", "authors": "Charlie Vanaret and Jean-Baptiste Gotteland and Nicolas Durand and\n  Jean-Marc Alliot", "title": "Hybridization of Interval CP and Evolutionary Algorithms for Optimizing\n  Difficult Problems", "comments": "21st International Conference on Principles and Practice of\n  Constraint Programming (CP 2015), 2015", "journal-ref": null, "doi": "10.1007/978-3-319-23219-5_32", "report-no": null, "categories": "cs.AI cs.DC cs.MS math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The only rigorous approaches for achieving a numerical proof of optimality in\nglobal optimization are interval-based methods that interleave branching of the\nsearch-space and pruning of the subdomains that cannot contain an optimal\nsolution. State-of-the-art solvers generally integrate local optimization\nalgorithms to compute a good upper bound of the global minimum over each\nsubspace. In this document, we propose a cooperative framework in which\ninterval methods cooperate with evolutionary algorithms. The latter are\nstochastic algorithms in which a population of candidate solutions iteratively\nevolves in the search-space to reach satisfactory solutions.\n  Within our cooperative solver Charibde, the evolutionary algorithm and the\ninterval-based algorithm run in parallel and exchange bounds, solutions and\nsearch-space in an advanced manner via message passing. A comparison of\nCharibde with state-of-the-art interval-based solvers (GlobSol, IBBA, Ibex) and\nNLP solvers (Couenne, BARON) on a benchmark of difficult COCONUT problems shows\nthat Charibde is highly competitive against non-rigorous solvers and converges\nfaster than rigorous solvers by an order of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2015 15:18:42 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Vanaret", "Charlie", ""], ["Gotteland", "Jean-Baptiste", ""], ["Durand", "Nicolas", ""], ["Alliot", "Jean-Marc", ""]]}, {"id": "1510.07244", "submitter": "Steffen B\\\"orm", "authors": "Steffen B\\\"orm and Sven Christophersen", "title": "Approximation of boundary element matrices using GPGPUs and nested cross\n  approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficiency of boundary element methods depends crucially on the time\nrequired for setting up the stiffness matrix. The far-field part of the matrix\ncan be approximated by compression schemes like the fast multipole method or\n$\\mathcal{H}$-matrix techniques. The near-field part is typically approximated\nby special quadrature rules like the Sauter-Schwab technique that can handle\nthe singular integrals appearing in the diagonal and near-diagonal matrix\nelements.\n  Since computing one element of the matrix requires only a small amount of\ndata but a fairly large number of operations, we propose to use general-purpose\ngraphics processing units (GPGPUs) to handle vectorizable portions of the\ncomputation: near-field computations are ideally suited for vectorization and\ncan therefore be handled very well by GPGPUs. Modern far-field compression\nschemes can be split into a small adaptive portion that exhibits divergent\ncontrol flows, and should therefore be handled by the CPU, and a vectorizable\nportion that can again be sent to GPGPUs.\n  We propose a hybrid algorithm that splits the computation into tasks for CPUs\nand GPGPUs. Our method presented in this article is able to reduce the setup\ntime of boundary integral operators by a significant factor of 19-30 for both\nthe Laplace and the Helmholtz equation in 3D when using two consumer GPGPUs\ncompared to a quad-core CPU.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2015 13:27:11 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 15:52:08 GMT"}], "update_date": "2017-10-19", "authors_parsed": [["B\u00f6rm", "Steffen", ""], ["Christophersen", "Sven", ""]]}, {"id": "1510.08642", "submitter": "Tomonori Kouya", "authors": "Tomonori Kouya", "title": "Performance evaluation of multiple precision matrix multiplications\n  using parallelized Strassen and Winograd algorithms", "comments": null, "journal-ref": "JSIAM Letters Vol. 8 (2016) p. 21-24", "doi": "10.14495/jsiaml.8.21", "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that Strassen and Winograd algorithms can reduce the\ncomputational costs associated with dense matrix multiplication. We have\nalready shown that they are also very effective for software-based multiple\nprecision floating-point arithmetic environments such as the MPFR/GMP library.\nIn this paper, we show that we can obtain the same effectiveness for\ndouble-double (DD) and quadruple-double (QD) environments supported by the QD\nlibrary, and that parallelization can increase the speed of these multiple\nprecision matrix multiplications. Finally, we demonstrate that our implemented\nparallelized Strassen and Winograd algorithms can increase the speed of\nparallelized LU decomposition.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 10:58:55 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Kouya", "Tomonori", ""]]}]