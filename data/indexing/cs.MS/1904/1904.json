[{"id": "1904.03317", "submitter": "Timo Heister", "authors": "Thomas C. Clevenger and Timo Heister and Guido Kanschat and Martin\n  Kronbichler", "title": "A Flexible, Parallel, Adaptive Geometric Multigrid method for FEM", "comments": null, "journal-ref": null, "doi": "10.1145/3425193", "report-no": null, "categories": "cs.NA cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present data structures and implementation details of a geometric\nmultigrid method on adaptively refined meshes for massively parallel\ncomputations. The method uses local smoothing on the refined part of the mesh.\nPartitioning is achieved by using a space filling curve for the leaf mesh and\ndistributing ancestors in the hierarchy based on the leaves. We present a model\nof the efficiency of mesh hierarchy distribution and compare its predictions to\nruntime measurements. The algorithm is implemented as part of the deal.II\nfinite element library and as such available to the public.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 23:05:33 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Clevenger", "Thomas C.", ""], ["Heister", "Timo", ""], ["Kanschat", "Guido", ""], ["Kronbichler", "Martin", ""]]}, {"id": "1904.05347", "submitter": "Mehdi Goli", "authors": "John Lawson, Mehdi Goli, Duncan McBain, Daniel Soutar, Louis Sugy", "title": "Cross-Platform Performance Portability Using Highly Parametrized SYCL\n  Kernels", "comments": "11 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over recent years heterogeneous systems have become more prevalent across HPC\nsystems, with over 100 supercomputers in the TOP500 incorporating GPUs or other\naccelerators. These hardware platforms have different performance\ncharacteristics and optimization requirements. In order to make the most of\nmultiple accelerators a developer has to provide implementations of their\nalgorithms tuned for each device. Hardware vendors provide libraries targeting\ntheir devices specifically, which provide good performance but frequently have\ndifferent API designs, hampering portability.\n  The SYCL programming model allows users to write heterogeneous programs using\ncompletely standard C++, and so developers have access to the power of C++\ntemplates when developing compute kernels. In this paper we show that by\nwriting highly parameterized kernels for matrix multiplies and convolutions we\nachieve performance competitive with vendor implementations across different\narchitectures. Furthermore, tuning for new devices amounts to choosing the\ncombinations of kernel parameters that perform best on the hardware.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 17:58:23 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Lawson", "John", ""], ["Goli", "Mehdi", ""], ["McBain", "Duncan", ""], ["Soutar", "Daniel", ""], ["Sugy", "Louis", ""]]}, {"id": "1904.05387", "submitter": "Emery Berger", "authors": "Eunice Jun, Maureen Daum, Jared Roesch, Sarah E. Chasins, Emery D.\n  Berger, Rene Just, Katharina Reinecke", "title": "Tea: A High-level Language and Runtime System for Automating Statistical\n  Analysis", "comments": "11 pages", "journal-ref": null, "doi": "10.1145/3332165.3347940", "report-no": null, "categories": "cs.PL cs.HC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though statistical analyses are centered on research questions and\nhypotheses, current statistical analysis tools are not. Users must first\ntranslate their hypotheses into specific statistical tests and then perform API\ncalls with functions and parameters. To do so accurately requires that users\nhave statistical expertise. To lower this barrier to valid, replicable\nstatistical analysis, we introduce Tea, a high-level declarative language and\nruntime system. In Tea, users express their study design, any parametric\nassumptions, and their hypotheses. Tea compiles these high-level specifications\ninto a constraint satisfaction problem that determines the set of valid\nstatistical tests, and then executes them to test the hypothesis. We evaluate\nTea using a suite of statistical analyses drawn from popular tutorials. We show\nthat Tea generally matches the choices of experts while automatically switching\nto non-parametric tests when parametric assumptions are not met. We simulate\nthe effect of mistakes made by non-expert users and show that Tea automatically\navoids both false negatives and false positives that could be produced by the\napplication of incorrect statistical tests.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 18:44:55 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Jun", "Eunice", ""], ["Daum", "Maureen", ""], ["Roesch", "Jared", ""], ["Chasins", "Sarah E.", ""], ["Berger", "Emery D.", ""], ["Just", "Rene", ""], ["Reinecke", "Katharina", ""]]}, {"id": "1904.05717", "submitter": "Tyler Smith PhD", "authors": "Tyler M. Smith and Robert A. van de Geijn", "title": "The MOMMS Family of Matrix Multiplication Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the ratio between the rate of computation and rate with which data can be\nretrieved from various layers of memory continues to deteriorate, a question\narises: Will the current best algorithms for computing matrix-matrix\nmultiplication on future CPUs continue to be (near) optimal? This paper\nprovides compelling analytical and empirical evidence that the answer is \"no\".\nThe analytical results guide us to a new family of algorithms of which the\ncurrent state-of-the-art \"Goto's algorithm\" is but one member. The empirical\nresults, on architectures that were custom built to reduce the amount of\nbandwidth to main memory, show that under different circumstances, different\nand particular members of the family become more superior. Thus, this family\nwill likely start playing a prominent role going forward.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 14:25:27 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Smith", "Tyler M.", ""], ["van de Geijn", "Robert A.", ""]]}, {"id": "1904.05838", "submitter": "Amanda Bienz", "authors": "Amanda Bienz, Luke Olson, William Gropp", "title": "Reducing Communication in Algebraic Multigrid with Multi-step Node Aware\n  Communication", "comments": "11 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.NA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algebraic multigrid (AMG) is often viewed as a scalable $\\mathcal{O}(n)$\nsolver for sparse linear systems. Yet, parallel AMG lacks scalability due to\nincreasingly large costs associated with communication, both in the initial\nconstruction of a multigrid hierarchy as well as the iterative solve phase.\nThis work introduces a parallel implementation of AMG to reduce the cost of\ncommunication, yielding an increase in scalability. Standard inter-process\ncommunication consists of sending data regardless of the send and receive\nprocess locations. Performance tests show notable differences in the cost of\nintra- and inter-node communication, motivating a restructuring of\ncommunication. In this case, the communication schedule takes advantage of the\nless costly intra-node communication, reducing both the number and size of\ninter-node messages. Node-centric communication extends to the range of\ncomponents in both the setup and solve phase of AMG, yielding an increase in\nthe weak and strong scalability of the entire method.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 16:43:11 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 12:27:20 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Bienz", "Amanda", ""], ["Olson", "Luke", ""], ["Gropp", "William", ""]]}, {"id": "1904.06376", "submitter": "Alexander Heinecke", "authors": "Greg Henry, Ping Tak Peter Tang, Alexander Heinecke", "title": "Leveraging the bfloat16 Artificial Intelligence Datatype For\n  Higher-Precision Computations", "comments": "Accepted at ARITH26", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years fused-multiply-add (FMA) units with lower-precision\nmultiplications and higher-precision accumulation have proven useful in machine\nlearning/artificial intelligence applications, most notably in training deep\nneural networks due to their extreme computational intensity. Compared to\nclassical IEEE-754 32 bit (FP32) and 64 bit (FP64) arithmetic, these reduced\nprecision arithmetic can naturally be sped up disproportional to their\nshortened width. The common strategy of all major hardware vendors is to\naggressively further enhance their performance disproportionately. One\nparticular FMA operation that multiplies two BF16 numbers while accumulating in\nFP32 has been found useful in deep learning, where BF16 is the 16-bit floating\npoint datatype with IEEE FP32 numerical range but 8 significant bits of\nprecision. In this paper, we examine the use this FMA unit to implement\nhigher-precision matrix routines in terms of potential performance gain and\nimplications on accuracy. We demonstrate how a decomposition into multiple\nsmaller datatypes can be used to assemble a high-precision result, leveraging\nthe higher precision accumulation of the FMA unit. We first demonstrate that\ncomputations of vector inner products and by natural extension, matrix-matrix\nproducts can be achieved by decomposing FP32 numbers in several BF16 numbers\nfollowed by appropriate computations that can accommodate the dynamic range and\npreserve accuracy compared to standard FP32 computations, while projecting up\nto 5.2x speed-up. Furthermore, we examine solution of linear equations\nformulated in the residual form that allows for iterative refinement. We\ndemonstrate that the solution obtained to be comparable to those offered by\nFP64 under a large range of linear system condition numbers.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 19:04:47 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Henry", "Greg", ""], ["Tang", "Ping Tak Peter", ""], ["Heinecke", "Alexander", ""]]}, {"id": "1904.08684", "submitter": "Sara Faghih-Naini", "authors": "Sara Faghih-Naini, Sebastian Kuckuk, Vadym Aizinger, Daniel Zint,\n  Roberto Grosso, Harald K\\\"ostler", "title": "Towards whole program generation of quadrature-free discontinuous\n  Galerkin methods for the shallow water equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shallow water equations (SWE) are a commonly used model to study\ntsunamis, tides, and coastal ocean circulation. However, there exist various\napproaches to discretize and solve them efficiently. Which of them is best for\na certain scenario is often not known and, in addition, depends heavily on the\nused HPC platform. From a simulation software perspective, this places a\npremium on the ability to adapt easily to different numerical methods and\nhardware architectures. One solution to this problem is to apply code\ngeneration techniques and to express methods and specific hardware-dependent\nimplementations on different levels of abstraction. This allows for a\nseparation of concerns and makes it possible, e.g., to exchange the\ndiscretization scheme without having to rewrite all low-level optimized\nroutines manually. In this paper, we show how code for an advanced\nquadrature-free discontinuous Galerkin (DG) discretized shallow water equation\nsolver can be generated. Here, we follow the multi-layered approach from the\nExaStencils project that starts from the continuous problem formulation, moves\nto the discrete scheme, spells out the numerical algorithms, and, finally, maps\nto a representation that can be transformed to a distributed memory parallel\nimplementation by our in-house Scala-based source-to-source compiler. Our\ncontributions include: A new quadrature-free discontinuous Galerkin\nformulation, an extension of the class of supported computational grids, and an\nextension of our toolchain allowing to evaluate discrete integrals stemming\nfrom the DG discretization implemented in Python. As first results we present\nthe whole toolchain and also demonstrate the convergence of our method for\nhigher order DG discretizations.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 11:02:13 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Faghih-Naini", "Sara", ""], ["Kuckuk", "Sebastian", ""], ["Aizinger", "Vadym", ""], ["Zint", "Daniel", ""], ["Grosso", "Roberto", ""], ["K\u00f6stler", "Harald", ""]]}, {"id": "1904.10119", "submitter": "Doru Thom Popovici", "authors": "Doru Thom Popovici, Martin D. Schatz, Franz Franchetti, Tze Meng Low", "title": "A Flexible Framework for Parallel Multi-Dimensional DFTs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-dimensional discrete Fourier transforms (DFT) are typically decomposed\ninto multiple 1D transforms. Hence, parallel implementations of any\nmulti-dimensional DFT focus on parallelizing within or across the 1D DFT.\nExisting DFT packages exploit the inherent parallelism across the 1D DFTs and\noffer rigid frameworks, that cannot be extended to incorporate both forms of\nparallelism and various data layouts to enable some of the parallelism.\nHowever, in the era of exascale, where systems have thousand of nodes and\nintricate network topologies, flexibility and parallel efficiency are key\naspects all multi-dimensional DFT frameworks need to have in order to map and\nscale the computation appropriately. In this work, we present a flexible\nframework, built on the Redistribution Operations and Tensor Expressions (ROTE)\nframework, that facilitates the development of a family of parallel\nmulti-dimensional DFT algorithms by 1) unifying the two parallelization schemes\nwithin a single framework, 2) exploiting the two different parallelization\nschemes to different degrees and 3) using different data layouts to distribute\nthe data across the compute nodes. We demonstrate the need of a versatile\nframework and thus a need for a family of parallel multi-dimensional DFT\nalgorithms on the K-Computer, where we show almost linear strong scaling\nresults for problem sizes of 1024^3 on 32k compute nodes.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 02:16:49 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 21:04:22 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Popovici", "Doru Thom", ""], ["Schatz", "Martin D.", ""], ["Franchetti", "Franz", ""], ["Low", "Tze Meng", ""]]}, {"id": "1904.10405", "submitter": "Michael Kohlhase", "authors": "Jacques Carette and William M. Farmer and Michael Kohlhase and Florian\n  Rabe", "title": "Big Math and the One-Brain Barrier A Position Paper and Architecture\n  Proposal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.AI math.HO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decades, a class of important mathematical results have\nrequired an ever increasing amount of human effort to carry out. For some, the\nhelp of computers is now indispensable. We analyze the implications of this\ntrend towards \"big mathematics\", its relation to human cognition, and how\nmachine support for big math can be organized. The central contribution of this\nposition paper is an information model for \"doing mathematics\", which posits\nthat humans very efficiently integrate four aspects: inference, computation,\ntabulation, and narration around a well-organized core of mathematical\nknowledge. The challenge for mathematical software systems is that these four\naspects need to be integrated as well. We briefly survey the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 16:15:52 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 05:02:06 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Carette", "Jacques", ""], ["Farmer", "William M.", ""], ["Kohlhase", "Michael", ""], ["Rabe", "Florian", ""]]}, {"id": "1904.10464", "submitter": "Francesco Torsello", "authors": "Francesco Torsello", "title": "$\\mathtt{bimEX}$: A Mathematica package for exact computations in $3+1$\n  bimetric relativity", "comments": "9 pages. It matches with the published version. GitHub repository at\n  https://github.com/nubirel/bimEX. Program files doi:\n  http://dx.doi.org/10.17632/2s5d7csc9w.1", "journal-ref": "Computer Physics Communications 247 (2020) 106948", "doi": "10.1016/j.cpc.2019.106948", "report-no": null, "categories": "cs.SC astro-ph.CO cs.MS gr-qc", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present $\\mathtt{bimEX}$, a Mathematica package for exact computations in\n3$+$1 bimetric relativity. It is based on the $\\mathtt{xAct}$ bundle, which can\nhandle computations involving both abstract tensors and their components. In\nthis communication, we refer to the latter case as concrete computations. The\npackage consists of two main parts. The first part involves the abstract\ntensors, and focuses on how to deal with multiple metrics in $\\mathtt{xAct}$.\nThe second part takes an ansatz for the primary variables in a chart as the\ninput, and returns the covariant BSSN bimetric equations in components in that\nchart. Several functions are implemented to make this process as fast and\nuser-friendly as possible. The package has been used and tested extensively in\nspherical symmetry and was the workhorse in obtaining the bimetric covariant\nBSSN equations and reproducing the bimetric 3$+$1 equations in the spherical\npolar chart.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 18:00:01 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 12:29:37 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Torsello", "Francesco", ""]]}, {"id": "1904.11263", "submitter": "Mantas Mikaitis", "authors": "Michael Hopkins and Mantas Mikaitis and Dave R. Lester and Steve\n  Furber", "title": "Stochastic rounding and reduced-precision fixed-point arithmetic for\n  solving neural ordinary differential equations", "comments": "Submitted to Philosophical Transactions of the Royal Society A", "journal-ref": null, "doi": "10.1098/rsta.2019.0052", "report-no": null, "categories": "cs.DS cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although double-precision floating-point arithmetic currently dominates\nhigh-performance computing, there is increasing interest in smaller and simpler\narithmetic types. The main reasons are potential improvements in energy\nefficiency and memory footprint and bandwidth. However, simply switching to\nlower-precision types typically results in increased numerical errors. We\ninvestigate approaches to improving the accuracy of reduced-precision\nfixed-point arithmetic types, using examples in an important domain for\nnumerical computation in neuroscience: the solution of Ordinary Differential\nEquations (ODEs). The Izhikevich neuron model is used to demonstrate that\nrounding has an important role in producing accurate spike timings from\nexplicit ODE solution algorithms. In particular, fixed-point arithmetic with\nstochastic rounding consistently results in smaller errors compared to single\nprecision floating-point and fixed-point arithmetic with round-to-nearest\nacross a range of neuron behaviours and ODE solvers. A computationally much\ncheaper alternative is also investigated, inspired by the concept of dither\nthat is a widely understood mechanism for providing resolution below the least\nsignificant bit (LSB) in digital signal processing. These results will have\nimplications for the solution of ODEs in other subject areas, and should also\nbe directly relevant to the huge range of practical problems that are\nrepresented by Partial Differential Equations (PDEs).\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 11:26:40 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 16:20:46 GMT"}, {"version": "v3", "created": "Mon, 12 Aug 2019 11:38:58 GMT"}, {"version": "v4", "created": "Mon, 30 Sep 2019 08:42:45 GMT"}, {"version": "v5", "created": "Wed, 22 Jan 2020 10:07:04 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Hopkins", "Michael", ""], ["Mikaitis", "Mantas", ""], ["Lester", "Dave R.", ""], ["Furber", "Steve", ""]]}, {"id": "1904.12380", "submitter": "Tomasz Patejko", "authors": "Jacek Czaja (1), Michal Gallus (1), Tomasz Patejko (1), Jian Tang (2)\n  ((1) Intel Corporation, (2) Baidu)", "title": "Softmax Optimizations for Intel Xeon Processor-based Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Softmax is popular normalization method used in machine learning. Deep\nlearning solutions like Transformer or BERT use the softmax function\nintensively, so it is worthwhile to optimize its performance. This article\npresents our methodology of optimization and its results applied to softmax. By\npresenting this methodology, we hope to increase an interest in deep learning\noptimizations for CPUs. We believe that the optimization process presented here\ncould be transferred to other deep learning frameworks such as TensorFlow or\nPyTorch.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 20:19:22 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 08:33:00 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Czaja", "Jacek", "", "Intel Corporation"], ["Gallus", "Michal", "", "Intel Corporation"], ["Patejko", "Tomasz", "", "Intel Corporation"], ["Tang", "Jian", "", "Baidu"]]}]