[{"id": "1801.00826", "submitter": "Charles Truong", "authors": "Charles Truong, Laurent Oudre, Nicolas Vayatis", "title": "ruptures: change point detection in Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ruptures is a Python library for offline change point detection. This package\nprovides methods for the analysis and segmentation of non-stationary signals.\nImplemented algorithms include exact and approximate detection for various\nparametric and non-parametric models. ruptures focuses on ease of use by\nproviding a well-documented and consistent interface. In addition, thanks to\nits modular structure, different algorithms and models can be connected and\nextended within this package.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 20:35:23 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Truong", "Charles", ""], ["Oudre", "Laurent", ""], ["Vayatis", "Nicolas", ""]]}, {"id": "1801.01134", "submitter": "B\\'erenger Bramas", "authors": "Berenger Bramas, Pavel Kus", "title": "Computing the sparse matrix vector product using block-based kernels\n  without zero padding on processors with AVX-512 instructions", "comments": "Published in Peer J CS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The sparse matrix-vector product (SpMV) is a fundamental operation in many\nscientific applications from various fields. The High Performance Computing\n(HPC) community has therefore continuously invested a lot of effort to provide\nan efficient SpMV kernel on modern CPU architectures. Although it has been\nshown that block-based kernels help to achieve high performance, they are\ndifficult to use in practice because of the zero padding they require. In the\ncurrent paper, we propose new kernels using the AVX-512 instruction set, which\nmakes it possible to use a blocking scheme without any zero padding in the\nmatrix memory storage. We describe mask-based sparse matrix formats and their\ncorresponding SpMV kernels highly optimized in assembly language. Considering\nthat the optimal blocking size depends on the matrix, we also provide a method\nto predict the best kernel to be used utilizing a simple interpolation of\nresults from previous executions. We compare the performance of our approach to\nthat of the Intel MKL CSR kernel and the CSR5 open-source package on a set of\nstandard benchmark matrices. We show that we can achieve significant\nimprovements in many cases, both for sequential and for parallel executions.\nFinally, we provide the corresponding code in an open source library, called\nSPC5.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 19:00:06 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 10:05:38 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Bramas", "Berenger", ""], ["Kus", "Pavel", ""]]}, {"id": "1801.01928", "submitter": "Alexander Novikov", "authors": "Alexander Novikov, Pavel Izmailov, Valentin Khrulkov, Michael\n  Figurnov, Ivan Oseledets", "title": "Tensor Train decomposition on TensorFlow (T3F)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor Train decomposition is used across many branches of machine learning.\nWe present T3F -- a library for Tensor Train decomposition based on TensorFlow.\nT3F supports GPU execution, batch processing, automatic differentiation, and\nversatile functionality for the Riemannian optimization framework, which takes\ninto account the underlying manifold structure to construct efficient\noptimization methods. The library makes it easier to implement machine learning\npapers that rely on the Tensor Train decomposition. T3F includes documentation,\nexamples and 94% test coverage.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 21:58:01 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 21:51:20 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Novikov", "Alexander", ""], ["Izmailov", "Pavel", ""], ["Khrulkov", "Valentin", ""], ["Figurnov", "Michael", ""], ["Oseledets", "Ivan", ""]]}, {"id": "1801.03614", "submitter": "Martin Peter Neuenhofen", "authors": "Martin Neuenhofen", "title": "Review of theory and implementation of hyper-dual numbers for first and\n  second order automatic differentiation", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this review we present hyper-dual numbers as a tool for the automatic\ndifferentiation of computer programs via operator overloading.\n  We start with a motivational introduction into the ideas of algorithmic\ndifferentiation. Then we illuminate the concepts behind operator overloading\nand dual numbers.\n  Afterwards, we present hyper-dual numbers (and vectors) as an extension of\ndual numbers for the computation of the Jacobian and the Hessian matrices of a\ncomputer program. We review a mathematical theorem that proves the correctness\nof the derivative information that is obtained from hyper-dual numbers.\n  Finally, we refer to a freely available implementation of a hyper-dual number\nclass in Matlab. We explain an interface that can be called with a function as\nargument such that the Jacobian and Hessian of this function are returned.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 02:16:35 GMT"}, {"version": "v2", "created": "Sun, 14 Jan 2018 19:12:21 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Neuenhofen", "Martin", ""]]}, {"id": "1801.04582", "submitter": "Afshin Zafari", "authors": "Afshin Zafari, Elisabeth Larsson", "title": "Distributed dynamic load balancing for task parallel programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we derive and investigate approaches to dynamically load\nbalance a distributed task parallel application software. The load balancing\nstrategy is based on task migration. Busy processes export parts of their ready\ntask queue to idle processes. Idle--busy pairs of processes find each other\nthrough a random search process that succeeds within a few steps with high\nprobability. We evaluate the load balancing approach for a block Cholesky\nfactorization implementation and observe a reduction in execution time on the\norder of 5\\% in the selected test cases.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jan 2018 16:47:52 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Zafari", "Afshin", ""], ["Larsson", "Elisabeth", ""]]}, {"id": "1801.05554", "submitter": "Jeremy Yee", "authors": "Jeremy Yee", "title": "rlsm: R package for least squares Monte Carlo", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short paper briefly describes the implementation of the least squares\nMonte Carlo method in the rlsm package. This package provides users with an\neasy manner to experiment with the large amount of R regression tools on any\nregression basis and reward functions. This package also computes lower and\nupper bounds for the true value function via duality methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 05:19:49 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Yee", "Jeremy", ""]]}, {"id": "1801.06029", "submitter": "Jeremy Yee", "authors": "Juri Hinz and Jeremy Yee", "title": "rcss: Subgradient and duality approach for dynamic programming", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short paper gives an introduction to the \\emph{rcss} package. The R\npackage \\emph{rcss} provides users with a tool to approximate the value\nfunctions in the Bellman recursion using convex piecewise linear functions\nformed using operations on tangents. A pathwise method is then used to gauge\nthe quality of the numerical results.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 14:19:36 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Hinz", "Juri", ""], ["Yee", "Jeremy", ""]]}, {"id": "1801.06601", "submitter": "Vikas Chandra", "authors": "Liangzhen Lai, Naveen Suda, Vikas Chandra", "title": "CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks are becoming increasingly popular in always-on IoT edge\ndevices performing data analytics right at the source, reducing latency as well\nas energy consumption for data communication. This paper presents CMSIS-NN,\nefficient kernels developed to maximize the performance and minimize the memory\nfootprint of neural network (NN) applications on Arm Cortex-M processors\ntargeted for intelligent IoT edge devices. Neural network inference based on\nCMSIS-NN kernels achieves 4.6X improvement in runtime/throughput and 4.9X\nimprovement in energy efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 23:39:15 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Lai", "Liangzhen", ""], ["Suda", "Naveen", ""], ["Chandra", "Vikas", ""]]}, {"id": "1801.08682", "submitter": "Tobias Weinzierl", "authors": "Dominic E. Charrier and Tobias Weinzierl", "title": "Stop talking to me -- a communication-avoiding ADER-DG realisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a communication- and data-sensitive formulation of ADER-DG for\nhyperbolic differential equation systems. Sensitive here has multiple flavours:\nFirst, the formulation reduces the persistent memory footprint. This reduces\npressure on the memory subsystem. Second, the formulation realises the\nunderlying predictor-corrector scheme with single-touch semantics, i.e., each\ndegree of freedom is read on average only once per time step from the main\nmemory. This reduces communication through the memory controllers. Third, the\nformulation breaks up the tight coupling of the explicit time stepping's\nalgorithmic steps to mesh traversals. This averages out data access peaks.\nDifferent operations and algorithmic steps are ran on different grid entities.\nFinally, the formulation hides distributed memory data transfer behind the\ncomputation aligned with the mesh traversal. This reduces pressure on the\nmachine interconnects. All techniques applied by our formulation are elaborated\nby means of a rigorous task formalism. They break up ADER-DG's tight causal\ncoupling of compute steps and can be generalised to other predictor-corrector\nschemes.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 05:56:11 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Charrier", "Dominic E.", ""], ["Weinzierl", "Tobias", ""]]}, {"id": "1801.09229", "submitter": "David Chalupa", "authors": "David Chalupa, Ken A Hawick", "title": "GraphCombEx: A Software Tool for Exploration of Combinatorial\n  Optimisation Properties of Large Graphs", "comments": null, "journal-ref": null, "doi": "10.1007/s00500-018-3230-x", "report-no": null, "categories": "cs.SI cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a prototype of a software tool for exploration of multiple\ncombinatorial optimisation problems in large real-world and synthetic complex\nnetworks. Our tool, called GraphCombEx (an acronym of Graph Combinatorial\nExplorer), provides a unified framework for scalable computation and\npresentation of high-quality suboptimal solutions and bounds for a number of\nwidely studied combinatorial optimisation problems. Efficient representation\nand applicability to large-scale graphs and complex networks are particularly\nconsidered in its design. The problems currently supported include maximum\nclique, graph colouring, maximum independent set, minimum vertex clique\ncovering, minimum dominating set, as well as the longest simple cycle problem.\nSuboptimal solutions and intervals for optimal objective values are estimated\nusing scalable heuristics. The tool is designed with extensibility in mind,\nwith the view of further problems and both new fast and high-performance\nheuristics to be added in the future. GraphCombEx has already been successfully\nused as a support tool in a number of recent research studies using\ncombinatorial optimisation to analyse complex networks, indicating its promise\nas a research software tool.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 13:43:50 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Chalupa", "David", ""], ["Hawick", "Ken A", ""]]}]