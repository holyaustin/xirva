[{"id": "1802.00303", "submitter": "Thomas Gibson", "authors": "Thomas H. Gibson, Lawrence Mitchell, David A. Ham, Colin J. Cotter", "title": "Slate: extending Firedrake's domain-specific abstraction to hybridized\n  solvers for geoscience and beyond", "comments": "Revisions for submission to GMD", "journal-ref": "Geoscientific Model Development 13:735-761 (2020)", "doi": "10.5194/gmd-13-735-2020", "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the finite element community, discontinuous Galerkin (DG) and mixed\nfinite element methods have become increasingly popular in simulating\ngeophysical flows. However, robust and efficient solvers for the resulting\nsaddle-point and elliptic systems arising from these discretizations continue\nto be an on-going challenge. One possible approach for addressing this issue is\nto employ a method known as hybridization, where the discrete equations are\ntransformed such that classic static condensation and local post-processing\nmethods can be employed. However, it is challenging to implement hybridization\nas performant parallel code within complex models, whilst maintaining\nseparation of concerns between applications scientists and software experts. In\nthis paper, we introduce a domain-specific abstraction within the Firedrake\nfinite element library that permits the rapid execution of these hybridization\ntechniques within a code-generating framework. The resulting framework composes\nnaturally with Firedrake's solver environment, allowing for the implementation\nof hybridization and static condensation as runtime-configurable\npreconditioners via the Python interface to PETSc, petsc4py. We provide\nexamples derived from second order elliptic problems and geophysical fluid\ndynamics. In addition, we demonstrate that hybridization shows great promise\nfor improving the performance of solvers for mixed finite element\ndiscretizations of equations related to large-scale geophysical flows.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 14:37:13 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 19:24:55 GMT"}, {"version": "v3", "created": "Fri, 6 Jul 2018 14:14:24 GMT"}, {"version": "v4", "created": "Mon, 1 Apr 2019 19:34:09 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Gibson", "Thomas H.", ""], ["Mitchell", "Lawrence", ""], ["Ham", "David A.", ""], ["Cotter", "Colin J.", ""]]}, {"id": "1802.00405", "submitter": "William Farmer", "authors": "Jacques Carette and William M. Farmer and Patrick Laskowski", "title": "HOL Light QE", "comments": "20 pages. arXiv admin note: text overlap with arXiv:1612.02785", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in algorithms that manipulate mathematical expressions in\nmathematically meaningful ways. Expressions are syntactic, but most logics do\nnot allow one to discuss syntax. ${\\rm CTT}_{\\rm qe}$ is a version of Church's\ntype theory that includes quotation and evaluation operators, akin to quote and\neval in the Lisp programming language. Since the HOL logic is also a version of\nChurch's type theory, we decided to add quotation and evaluation to HOL Light\nto demonstrate the implementability of ${\\rm CTT}_{\\rm qe}$ and the benefits of\nhaving quotation and evaluation in a proof assistant. The resulting system is\ncalled HOL Light QE. Here we document the design of HOL Light QE and the\nchallenges that needed to be overcome. The resulting implementation is freely\navailable.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 17:31:06 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 18:05:54 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Carette", "Jacques", ""], ["Farmer", "William M.", ""], ["Laskowski", "Patrick", ""]]}, {"id": "1802.02247", "submitter": "Johannes Willkomm", "authors": "Johannes Willkomm", "title": "Automatic differentiation of ODE integration", "comments": "13 pages, 7 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the calculation of the derivatives of ODE systems with the\nautomatic differentiation tool ADiMat. Using the well-known Lotka-Volterra\nequations and the ode23 ODE solver as examples we show the analytic derivatives\nand detail how to differentiate a top-level function that calls ode23 somewhere\nwith ADiMat. This involves the manual construction of substitution function to\npropagate the derivatives in forward and reverse mode. We also show how to use\nthe reverse mode code to evaluate the Hessian in forward-over-reverse mode.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 22:13:26 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Willkomm", "Johannes", ""]]}, {"id": "1802.02474", "submitter": "Navjot Kukreja", "authors": "Navjot Kukreja, Jan H\\\"uckelheim, Michael Lange, Mathias Louboutin,\n  Andrea Walther, Simon W. Funke, Gerard Gorman", "title": "High-level python abstractions for optimal checkpointing in inversion\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Inversion and PDE-constrained optimization problems often rely on solving the\nadjoint problem to calculate the gradient of the objec- tive function. This\nrequires storing large amounts of intermediate data, setting a limit to the\nlargest problem that might be solved with a given amount of memory available.\nCheckpointing is an approach that can reduce the amount of memory required by\nredoing parts of the computation instead of storing intermediate results. The\nRevolve checkpointing algorithm o ers an optimal schedule that trades\ncomputational cost for smaller memory footprints. Integrat- ing Revolve into a\nmodern python HPC code and combining it with code generation is not\nstraightforward. We present an API that makes checkpointing accessible from a\nDSL-based code generation environment along with some initial performance gures\nwith a focus on seismic applications.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 16:02:09 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Kukreja", "Navjot", ""], ["H\u00fcckelheim", "Jan", ""], ["Lange", "Michael", ""], ["Louboutin", "Mathias", ""], ["Walther", "Andrea", ""], ["Funke", "Simon W.", ""], ["Gorman", "Gerard", ""]]}, {"id": "1802.02619", "submitter": "Adam P. Harrison", "authors": "Adam P. Harrison, Dileepan Joseph", "title": "High Performance Rearrangement and Multiplication Routines for Sparse\n  Tensor Arithmetic", "comments": "To appear in SIAM Journal on Scientific Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers are increasingly incorporating numeric high-order data, i.e.,\nnumeric tensors, within their practice. Just like the matrix/vector (MV)\nparadigm, the development of multi-purpose, but high-performance, sparse data\nstructures and algorithms for arithmetic calculations, e.g., those found in\nEinstein-like notation, is crucial for the continued adoption of tensors. We\nuse the example of high-order differential operators to illustrate this need.\nAs sparse tensor arithmetic is an emerging research topic, with challenges\ndistinct from the MV paradigm, many aspects require further articulation. We\nfocus on three core facets. First, aligning with prominent voices in the field,\nwe emphasise the importance of data structures able to accommodate the\noperational complexity of tensor arithmetic. However, we describe a linearised\ncoordinate (LCO) data structure that provides faster and more memory-efficient\nsorting performance. Second, flexible data structures, like the LCO, rely\nheavily on sorts and permutations. We introduce an innovative permutation\nalgorithm, based on radix sort, that is tailored to rearrange already-sorted\nsparse data, producing significant performance gains. Third, we introduce a\nnovel poly-algorithm for sparse tensor products, where hyper-sparsity is a\npossibility. Different manifestations of hyper-sparsity demand their own\napproach, which our poly-algorithm is the first to provide. These developments\nare incorporated within our LibNT and NTToolbox software libraries. Benchmarks,\nfrequently drawn from the high-order differential operators example,\ndemonstrate the practical impact of our routines, with speed-ups of 40% or\nhigher compared to alternative high-performance implementations. Comparisons\nagainst the MATLAB Tensor Toolbox show over 10 times speed improvements. Thus,\nthese advancements produce significant practical improvements for sparse tensor\narithmetic.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 20:01:45 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Harrison", "Adam P.", ""], ["Joseph", "Dileepan", ""]]}, {"id": "1802.03433", "submitter": "Yueming Liu", "authors": "Tao Cui, Xiaohu Guo and Hui Liu", "title": "GPU Accelerated Finite Element Assembly with Runtime Compilation", "comments": "6 pages, 8 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, high performance scientific computing on graphics processing\nunits (GPUs) have gained widespread acceptance. These devices are designed to\noffer massively parallel threads for running code with general purpose. There\nare many researches focus on finite element method with GPUs. However, most of\nthe works are specific to certain problems and applications. Some works propose\nmethods for finite element assembly that is general for a wide range of finite\nelement models. But the development of finite element code is dependent on the\nhardware architectures. It is usually complicated and error prone using the\nlibraries provided by the hardware vendors. In this paper, we present\narchitecture and implementation of finite element assembly for partial\ndifferential equations (PDEs) based on symbolic computation and runtime\ncompilation technique on GPU. User friendly programming interface with symbolic\ncomputation is provided. At the same time, high computational efficiency is\nachieved by using runtime compilation technique. As far as we know, it is the\nfirst work using this technique to accelerate finite element assembly for\nsolving PDEs. Experiments show that a one to two orders of speedup is achieved\nfor the problems studied in the paper.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 19:56:16 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Cui", "Tao", ""], ["Guo", "Xiaohu", ""], ["Liu", "Hui", ""]]}, {"id": "1802.03650", "submitter": "Farhad Merchant", "authors": "Farhad Merchant, Tarun Vatwani, Anupam Chattopadhyay, Soumyendu Raha,\n  S K Nandy, Ranjani Narayan", "title": "Achieving Efficient Realization of Kalman Filter on CGRA through\n  Algorithm-Architecture Co-design", "comments": "Accepted in ARC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present efficient realization of Kalman Filter (KF) that\ncan achieve up to 65% of the theoretical peak performance of underlying\narchitecture platform. KF is realized using Modified Faddeeva Algorithm (MFA)\nas a basic building block due to its versatility and REDEFINE Coarse Grained\nReconfigurable Architecture (CGRA) is used as a platform for experiments since\nREDEFINE is capable of supporting realization of a set algorithmic compute\nstructures at run-time on a Reconfigurable Data-path (RDP). We perform several\nhardware and software based optimizations in the realization of KF to achieve\n116% improvement in terms of Gflops over the first realization of KF. Overall,\nwith the presented approach for KF, 4-105x performance improvement in terms of\nGflops/watt over several academically and commercially available realizations\nof KF is attained. In REDEFINE, we show that our implementation is scalable and\nthe performance attained is commensurate with the underlying hardware resources\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 20:51:30 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Merchant", "Farhad", ""], ["Vatwani", "Tarun", ""], ["Chattopadhyay", "Anupam", ""], ["Raha", "Soumyendu", ""], ["Nandy", "S K", ""], ["Narayan", "Ranjani", ""]]}, {"id": "1802.03749", "submitter": "Andr\\'as Attila Sulyok", "authors": "Andr\\'as Attila Sulyok, G\\'abor D\\'aniel Balogh, Istv\\'an Zolt\\'an\n  Reguly and Gihan R. Mudalige", "title": "Locality Optimized Unstructured Mesh Algorithms on GPUs", "comments": "Number of pages: 36 Number of figures: 21 Submitted to JPDC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unstructured-mesh based numerical algorithms such as finite volume and finite\nelement algorithms form an important class of applications for many scientific\nand engineering domains. The key difficulty in achieving higher performance\nfrom these applications is the indirect accesses that lead to data-races when\nparallelized. Current methods for handling such data-races lead to reduced\nparallelism and suboptimal performance. Particularly on modern many-core\narchitectures, such as GPUs, that has increasing core/thread counts, reducing\ndata movement and exploiting memory locality is vital for gaining good\nperformance.\n  In this work we present novel locality-exploiting optimizations for the\nefficient execution of unstructured-mesh algorithms on GPUs. Building on a\ntwo-layered coloring strategy for handling data races, we introduce novel\nreordering and partitioning techniques to further improve efficient execution.\nThe new optimizations are then applied to several well established\nunstructured-mesh applications, investigating their performance on NVIDIA's\nlatest P100 and V100 GPUs. We demonstrate significant speedups\n($1.1\\text{--}1.75\\times$) compared to the state-of-the-art. A range of\nperformance metrics are benchmarked including runtime, memory transactions,\nachieved bandwidth performance, GPU occupancy and data reuse factors and are\nused to understand and explain the key factors impacting performance. The\noptimized algorithms are implemented as an open-source software library and we\nillustrate its use for improving performance of existing or new\nunstructured-mesh applications.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 14:59:49 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 19:54:41 GMT"}, {"version": "v3", "created": "Fri, 1 Mar 2019 20:37:58 GMT"}, {"version": "v4", "created": "Sat, 27 Jul 2019 12:40:28 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Sulyok", "Andr\u00e1s Attila", ""], ["Balogh", "G\u00e1bor D\u00e1niel", ""], ["Reguly", "Istv\u00e1n Zolt\u00e1n", ""], ["Mudalige", "Gihan R.", ""]]}, {"id": "1802.03773", "submitter": "Jan Svoboda", "authors": "Jan Svoboda, Thomas Cashman, Andrew Fitzgibbon", "title": "QRkit: Sparse, Composable QR Decompositions for Efficient and Stable\n  Solutions to Problems in Computer Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedded computer vision applications increasingly require the speed and\npower benefits of single-precision (32 bit) floating point. However,\napplications which make use of Levenberg-like optimization can lose significant\naccuracy when reducing to single precision, sometimes unrecoverably so. This\naccuracy can be regained using solvers based on QR rather than Cholesky\ndecomposition, but the absence of sparse QR solvers for common sparsity\npatterns found in computer vision means that many applications cannot benefit.\nWe introduce an open-source suite of solvers for Eigen, which efficiently\ncompute the QR decomposition for matrices with some common sparsity patterns\n(block diagonal, horizontal and vertical concatenation, and banded). For\nproblems with very particular sparsity structures, these elements can be\ncomposed together in 'kit' form, hence the name QRkit. We apply our methods to\nseveral computer vision problems, showing competitive performance and\nsuitability especially in single precision arithmetic.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 17:18:18 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Svoboda", "Jan", ""], ["Cashman", "Thomas", ""], ["Fitzgibbon", "Andrew", ""]]}, {"id": "1802.03948", "submitter": "Fredrik Johansson", "authors": "Fredrik Johansson (LFANT), Marc Mezzarobba (ARIC)", "title": "Fast and rigorous arbitrary-precision computation of Gauss-Legendre\n  quadrature nodes and weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a strategy for rigorous arbitrary-precision evaluation of\nLegendre polynomials on the unit interval and its application in the generation\nof Gauss-Legendre quadrature rules. Our focus is on making the evaluation\npractical for a wide range of realistic parameters, corresponding to the\nrequirements of numerical integration to an accuracy of about 100 to 100 000\nbits. Our algorithm combines the summation by rectangular splitting of several\ntypes of expansions in terms of hypergeometric series with a fixed-point\nimplementation of Bonnet's three-term recurrence relation. We then compute\nrigorous enclosures of the Gauss-Legendre nodes and weights using the interval\nNewton method. We provide rigorous error bounds for all steps of the algorithm.\nThe approach is validated by an implementation in the Arb library, which\nachieves order-of-magnitude speedups over previous code for computing\nGauss-Legendre rules with simultaneous high degree and precision.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 09:47:05 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 07:33:50 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Johansson", "Fredrik", "", "LFANT"], ["Mezzarobba", "Marc", "", "ARIC"]]}, {"id": "1802.04243", "submitter": "Kiril Shterev", "authors": "Kiril S. Shterev", "title": "GPU implementation of algorithm SIMPLE-TS for calculation of unsteady,\n  viscous, compressible and heat-conductive gas flows", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent trend of using Graphics Processing Units (GPU's) for high\nperformance computations is driven by the high ratio of price performance for\nthese units, complemented by their cost effectiveness. At first glance,\ncomputational fluid dynamics (CFD) solvers match perfectly to GPU resources\nbecause these solvers make intensive calculations and use relatively little\nmemory. Nevertheless, there are scarce results about the practical use of this\nserious advantage of GPU over CPU, especially for calculations of viscous,\ncompressible, heat-conductive gas flows with double precision accuracy. In this\npaper, two GPU algorithms according to time approximation of convective terms\nwere presented: explicit and implicit scheme. To decrease data transfers\nbetween device memories and increase the arithmetic intensity of a GPU code we\nminimize the number of kernels. The GPU algorithm was implemented in one kernel\nfor the implicit scheme and two kernels for the explicit scheme. The numerical\nequations were put together using macros and optimization, data copy from\nglobal to private memory, and data reuse were left to the compiler. Thus keeps\nthe code simpler with excellent maintenance. As a test case, we model the flow\npast squares in a microchannel at supersonic speed. The tests show that overall\nspeedup of AMD Radeon R9 280X is up to 102x compared to Intel Core i5-4690 core\nand up to 184x compared to Intel Core i7-920 core, while speedup of NVIDIA\nTesla M2090 is up to 11x compared to Intel Core i5-4690 core and up to 20x\ncompared to Intel Core i7-920 core. Memory requirements of GPU code are\nimproved compared to CPU one. It requires 1[GB] global memory for 5.9 million\nfinite volumes that are two times less compared to C++ CPU code. After all the\ncode is simple, portable (written in OpenCL), memory efficient and easily\nmodifiable moreover demonstrates excellent performance.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 18:45:13 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Shterev", "Kiril S.", ""]]}, {"id": "1802.04385", "submitter": "Victor Magron", "authors": "Victor Magron and Alexandre Rocca and Thao Dang", "title": "Certified Roundoff Error Bounds using Bernstein Expansions and Sparse\n  Krivine-Stengle Representations", "comments": "14 pages, 2 figures, 2 tables. Extension of the work in\n  arXiv:1610.07038", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Floating point error is a drawback of embedded systems implementation that is\ndifficult to avoid. Computing rigorous upper bounds of roundoff errors is\nabsolutely necessary for the validation of critical software. This problem of\ncomputing rigorous upper bounds is even more challenging when addressing\nnon-linear programs. In this paper, we propose and compare two new algorithms\nbased on Bernstein expansions and sparse Krivine-Stengle representations,\nadapted from the field of the global optimization, to compute upper bounds of\nroundoff errors for programs implementing polynomial and rational functions. We\nalso provide the convergence rate of these two algorithms. We release two\nrelated software package FPBern and FPKriSten, and compare them with the\nstate-of-the-art tools. We show that these two methods achieve competitive\nperformance, while providing accurate upper bounds by comparison with the other\ntools.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 22:56:20 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Magron", "Victor", ""], ["Rocca", "Alexandre", ""], ["Dang", "Thao", ""]]}, {"id": "1802.04450", "submitter": "Yu Jin", "authors": "Yu Jin, Joseph F. JaJa", "title": "A High Performance Implementation of Spectral Clustering on CPU-GPU\n  Platforms", "comments": "2016 IEEE International Parallel and Distributed Processing Symposium\n  Workshops (Parallel Computing and Optimization (PCO) workshop). Codes are\n  available on https://github.com/yuj-umd/fastsc", "journal-ref": null, "doi": "10.1109/IPDPSW.2016.79", "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is one of the most popular graph clustering algorithms,\nwhich achieves the best performance for many scientific and engineering\napplications. However, existing implementations in commonly used software\nplatforms such as Matlab and Python do not scale well for many of the emerging\nBig Data applications. In this paper, we present a fast implementation of the\nspectral clustering algorithm on a CPU-GPU heterogeneous platform. Our\nimplementation takes advantage of the computational power of the multi-core CPU\nand the massive multithreading and SIMD capabilities of GPUs. Given the input\nas data points in high dimensional space, we propose a parallel scheme to build\na sparse similarity graph represented in a standard sparse representation\nformat. Then we compute the smallest $k$ eigenvectors of the Laplacian matrix\nby utilizing the reverse communication interfaces of ARPACK software and\ncuSPARSE library, where $k$ is typically very large. Moreover, we implement a\nvery fast parallelized $k$-means algorithm on GPUs. Our implementation is shown\nto be significantly faster compared to the best known Matlab and Python\nimplementations for each step. In addition, our algorithm scales to problems\nwith a very large number of clusters.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 03:08:22 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Jin", "Yu", ""], ["JaJa", "Joseph F.", ""]]}, {"id": "1802.06504", "submitter": "Charisee Chiw", "authors": "Charisee Chiw and Gordon L. Kindlmann and John Reppy", "title": "Compiling Diderot: From Tensor Calculus to C", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diderot is a parallel domain-specific language for analysis and visualization\nof multidimensional scientific images, such as those produced by CT and MRI\nscanners. In particular, it supports algorithms where tensor fields (i.e.,\nfunctions from 3D points to tensor values) are used to represent the underlying\nphysical objects that were scanned by the imaging device. Diderot supports\nhigher-order programming where tensor fields are first-class values and where\ndifferential operators and lifted linear-algebra operators can be used to\nexpress mathematical reasoning directly in the language. While such lifted\nfield operations are central to the definition and computation of many\nscientific visualization algorithms, to date they have required extensive\nmanual derivations and laborious implementation.\n  The challenge for the Diderot compiler is to effectively translate the\nhigh-level mathematical concepts that are expressible in the surface language\nto a low-level and efficient implementation in C. This paper describes our\napproach to this challenge, which is based around the careful design of an\nintermediate representation (IR), called EIN, and a number of compiler\ntransformations that lower the program from tensor calculus to C while avoiding\ncombinatorial explosion in the size of the IR. We describe the challenges in\ncompiling a language like Diderot, the design of EIN, and the transformation\nused by the compiler. We also present an evaluation of EIN with respect to both\ncompiler efficiency and quality of generated code.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 02:38:51 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Chiw", "Charisee", ""], ["Kindlmann", "Gordon L.", ""], ["Reppy", "John", ""]]}, {"id": "1802.07832", "submitter": "Justin Chang", "authors": "Justin Chang, Maurice S. Fabien, Matthew G. Knepley, Richard T. Mills", "title": "Comparative study of finite element methods using the Time-Accuracy-Size\n  (TAS) spectrum analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a performance analysis appropriate for comparing algorithms using\ndifferent numerical discretizations. By taking into account the total\ntime-to-solution, numerical accuracy with respect to an error norm, and the\ncomputation rate, a cost-benefit analysis can be performed to determine which\nalgorithm and discretization are particularly suited for an application. This\nwork extends the performance spectrum model in Chang et. al. 2017 for\ninterpretation of hardware and algorithmic tradeoffs in numerical PDE\nsimulation. As a proof-of-concept, popular finite element software packages are\nused to illustrate this analysis for Poisson's equation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 22:11:58 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Chang", "Justin", ""], ["Fabien", "Maurice S.", ""], ["Knepley", "Matthew G.", ""], ["Mills", "Richard T.", ""]]}, {"id": "1802.07942", "submitter": "Fredrik Johansson", "authors": "Fredrik Johansson", "title": "Numerical integration in arbitrary-precision ball arithmetic", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an implementation of arbitrary-precision numerical integration\nwith rigorous error bounds in the Arb library. Rapid convergence is ensured for\npiecewise complex analytic integrals by use of the Petras algorithm, which\ncombines adaptive bisection with adaptive Gaussian quadrature where error\nbounds are determined via complex magnitudes without evaluating derivatives.\nThe code is general, easy to use, and efficient, often outperforming existing\nnon-rigorous software.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 08:46:14 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Johansson", "Fredrik", ""]]}, {"id": "1802.08252", "submitter": "Jeremy Reizenstein", "authors": "Jeremy Reizenstein, Benjamin Graham", "title": "The iisignature library: efficient calculation of iterated-integral\n  signatures and log signatures", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.MS math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterated-integral signatures and log signatures are vectors calculated from a\npath that characterise its shape. They come from the theory of differential\nequations driven by rough paths, and also have applications in statistics and\nmachine learning. We present algorithms for efficiently calculating these\nsignatures, and benchmark their performance. We release the methods as a Python\npackage.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 14:29:30 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Reizenstein", "Jeremy", ""], ["Graham", "Benjamin", ""]]}, {"id": "1802.08558", "submitter": "Walter Mascarenhas", "authors": "Walter F. Mascarenhas", "title": "Moore: Interval Arithmetic in C++20", "comments": "arXiv admin note: text overlap with arXiv:1611.09567\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the Moore library for interval arithmetic in C++20. It\ngives examples of how the library can be used, and explains the basic\nprinciples underlying its design.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 19:02:45 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Mascarenhas", "Walter F.", ""]]}, {"id": "1802.10574", "submitter": "Fredrik Kjolstad", "authors": "Fredrik Kjolstad and Peter Ahrens and Shoaib Kamil and Saman\n  Amarasinghe", "title": "Sparse Tensor Algebra Optimizations with Workspaces", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows how to optimize sparse tensor algebraic expressions by\nintroducing temporary tensors, called workspaces, into the resulting loop\nnests. We develop a new intermediate language for tensor operations called\nconcrete index notation that extends tensor index notation. Concrete index\nnotation expresses when and where sub-computations occur and what tensor they\nare stored into. We then describe the workspace optimization in this language,\nand how to compile it to sparse code by building on prior work in the\nliterature.\n  We demonstrate the importance of the optimization on several important sparse\ntensor kernels, including sparse matrix-matrix multiplication (SpMM), sparse\ntensor addition (SpAdd), and the matricized tensor times Khatri-Rao product\n(MTTKRP) used to factorize tensors. Our results show improvements over prior\nwork on tensor algebra compilation and brings the performance of these kernels\non par with state-of-the-art hand-optimized implementations. For example, SpMM\nwas not supported by prior tensor algebra compilers, the performance of MTTKRP\non the nell-2 data set improves by 35%, and MTTKRP can for the first time have\nsparse results.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 18:28:10 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 17:55:16 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Kjolstad", "Fredrik", ""], ["Ahrens", "Peter", ""], ["Kamil", "Shoaib", ""], ["Amarasinghe", "Saman", ""]]}]