[{"id": "1502.01367", "submitter": "Klaus Rohe", "authors": "Klaus Rohe", "title": "Visualizing Marden's theorem with Scilab", "comments": "Scilab, Marden's theorem, 2D implicit plots, geometry of complex\n  numbers, Steiner ellipses", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A theorem which is named after the American Mathematician Moris Marden states\na very surprising and interesting fact concerning the relationship between the\npoints of a triangle in the complex plane and the zeros of two complex\npolynomials related to this triangle: \"Suppose the zeroes z1, z2, and z3 of a\nthird-degree polynomial p(z) are non-collinear. There is a unique ellipse\ninscribed in the triangle with vertices z1, z2, z3 and tangent to the sides at\ntheir midpoints: the Steiner in-ellipse. The foci of that ellipse are the\nzeroes of the derivative p'(z).\" (Wikipedia contributors, \"Marden's theorem\",\nhttp://en.wikipedia.org/wiki/Marden%27s_theorem). This document describes how\nScilab, a popular and powerful open source alternative to MATLAB, can be used\nto visualize the above stated theorem for arbitrary complex numbers z1, z2, and\nz3 which are not collinear. It is further demonstrated how the equations of the\nSteiner ellipses of a triangle in the complex plane can be calculated and\nplotted by applying this theorem.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 21:48:25 GMT"}, {"version": "v2", "created": "Wed, 11 Feb 2015 23:47:10 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Rohe", "Klaus", ""]]}, {"id": "1502.05216", "submitter": "Evgeny Latkin", "authors": "Evgeny Latkin", "title": "Twofold exp and log", "comments": "Experimental code and tests at \"twofolds\" project Web site:\n  https://sites.google.com/site/yevgenylatkin/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is about twofold arithmetic. Here I introduce algorithms and\nexperimental code for twofold variant of C/C++ standard functions exp() and\nlog(), and expm1() and log1p(). Twofold function $y_0+y_1 \\approx f(x_0+x_1)$\nis nearly 2x-precise so can assess accuracy of standard one. Performance allows\nassessing on-fly: twofold texp() over double is ~10x times faster than expq()\nby GNU quadmath.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 02:55:35 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Latkin", "Evgeny", ""]]}, {"id": "1502.05366", "submitter": "Sergey Voronin", "authors": "Sergey Voronin and Per-Gunnar Martinsson", "title": "RSVDPACK: An implementation of randomized algorithms for computing the\n  singular value, interpolative, and CUR decompositions of matrices on\n  multi-core and GPU architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RSVDPACK is a library of functions for computing low rank approximations of\nmatrices. The library includes functions for computing standard (partial)\nfactorizations such as the Singular Value Decomposition (SVD), and also so\ncalled \"structure preserving\" factorizations such as the Interpolative\nDecomposition (ID) and the CUR decomposition. The ID and CUR factorizations\npick subsets of the rows/columns of a matrix to use as bases for its row/column\nspace. Such factorizations preserve properties of the matrix such as sparsity\nor non-negativity, are helpful in data interpretation, and require in certain\ncontexts less memory than a partial SVD. The package implements highly\nefficient computational algorithms based on randomized sampling, as described\nand analyzed in [N. Halko, P.G. Martinsson, J. Tropp, \"Finding structure with\nrandomness: Probabilistic algorithms for constructing approximate matrix\ndecompositions,\" SIAM Review, 53(2), 2011], and subsequent papers. This\nmanuscript presents some modifications to the basic algorithms that improve\nperformance and ease of use. The library is written in C and supports both\nmulti-core CPU and GPU architectures.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 20:13:26 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 15:35:22 GMT"}, {"version": "v3", "created": "Mon, 29 Aug 2016 19:19:35 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Voronin", "Sergey", ""], ["Martinsson", "Per-Gunnar", ""]]}, {"id": "1502.06064", "submitter": "Ken Miura", "authors": "Ken Miura, Tetsuaki Mano, Atsushi Kanehira, Yuichiro Tsuchiya and\n  Tatsuya Harada", "title": "MILJS : Brand New JavaScript Libraries for Matrix Calculation and\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MILJS is a collection of state-of-the-art, platform-independent, scalable,\nfast JavaScript libraries for matrix calculation and machine learning. Our core\nlibrary offering a matrix calculation is called Sushi, which exhibits far\nbetter performance than any other leading machine learning libraries written in\nJavaScript. Especially, our matrix multiplication is 177 times faster than the\nfastest JavaScript benchmark. Based on Sushi, a machine learning library called\nTempura is provided, which supports various algorithms widely used in machine\nlearning research. We also provide Soba as a visualization library. The\nimplementations of our libraries are clearly written, properly documented and\nthus can are easy to get started with, as long as there is a web browser. These\nlibraries are available from http://mil-tokyo.github.io/ under the MIT license.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 04:29:41 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Miura", "Ken", ""], ["Mano", "Tetsuaki", ""], ["Kanehira", "Atsushi", ""], ["Tsuchiya", "Yuichiro", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1502.07191", "submitter": "Peter Opsomer", "authors": "Alfredo Dea\\~no, Daan Huybrechs, Peter Opsomer", "title": "Construction and implementation of asymptotic expansions for\n  Jacobi--type orthogonal polynomials", "comments": "39 pages, 5 figures, 35 references. The article mentioned is\n  arXiv:math/0111252 and the implementation is available on\n  http://nines.cs.kuleuven.be/software/JACOBI/. The final publication is\n  available at Springer via http://dx.doi.org/10.1007/s10444-015-9442-z", "journal-ref": null, "doi": "10.1007/s10444-015-9442-z", "report-no": "TW658", "categories": "cs.MS math.CA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in the asymptotic behavior of orthogonal polynomials of the\ngeneralized Jacobi type as their degree $n$ goes to $\\infty$. These are defined\non the interval $[-1,1]$ with weight function\n$w(x)=(1-x)^{\\alpha}(1+x)^{\\beta}h(x)$, $\\alpha,\\beta>-1$ and $h(x)$ a real,\nanalytic and strictly positive function on $[-1,1]$. This information is\navailable in the work of Kuijlaars, McLaughlin, Van Assche and Vanlessen, where\nthe authors use the Riemann--Hilbert formulation and the Deift--Zhou non-linear\nsteepest descent method. We show that computing higher-order terms can be\nsimplified, leading to their efficient construction. The resulting asymptotic\nexpansions in every region of the complex plane are implemented both\nsymbolically and numerically, and the code is made publicly available. The main\nadvantage of these expansions is that they lead to increasing accuracy for\nincreasing degree of the polynomials, at a computational cost that is actually\nindependent of the degree. In contrast, the typical use of the recurrence\nrelation for orthogonal polynomials in computations leads to a cost that is at\nleast linear in the degree. Furthermore, the expansions may be used to compute\nGaussian quadrature rules in $\\mathcal{O}(n)$ operations, rather than\n$\\mathcal{O}(n^2)$ based on the recurrence relation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 15:07:30 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 13:29:05 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2015 15:49:29 GMT"}, {"version": "v4", "created": "Thu, 22 Oct 2015 12:28:37 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["Dea\u00f1o", "Alfredo", ""], ["Huybrechs", "Daan", ""], ["Opsomer", "Peter", ""]]}, {"id": "1502.07405", "submitter": "Pieter Ghysels", "authors": "Pieter Ghysels, Xiaoye S. Li, Francois-Henry Rouet, Samuel Williams,\n  Artem Napov", "title": "An efficient multi-core implementation of a novel HSS-structured\n  multifrontal solver using randomized sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sparse linear system solver that is based on a multifrontal\nvariant of Gaussian elimination, and exploits low-rank approximation of the\nresulting dense frontal matrices. We use hierarchically semiseparable (HSS)\nmatrices, which have low-rank off-diagonal blocks, to approximate the frontal\nmatrices. For HSS matrix construction, a randomized sampling algorithm is used\ntogether with interpolative decompositions. The combination of the randomized\ncompression with a fast ULV HSS factorization leads to a solver with lower\ncomputational complexity than the standard multifrontal method for many\napplications, resulting in speedups up to 7 fold for problems in our test\nsuite. The implementation targets many-core systems by using task parallelism\nwith dynamic runtime scheduling. Numerical experiments show performance\nimprovements over state-of-the-art sparse direct solvers. The implementation\nachieves high performance and good scalability on a range of modern shared\nmemory parallel systems, including the Intel Xeon Phi (MIC). The code is part\nof a software package called STRUMPACK -- STRUctured Matrices PACKage, which\nalso has a distributed memory component for dense rank-structured matrices.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 23:54:16 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Ghysels", "Pieter", ""], ["Li", "Xiaoye S.", ""], ["Rouet", "Francois-Henry", ""], ["Williams", "Samuel", ""], ["Napov", "Artem", ""]]}]