[{"id": "2009.00761", "submitter": "Drew Schmidt", "authors": "Drew Schmidt", "title": "A Survey of Singular Value Decomposition Methods for Distributed\n  Tall/Skinny Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Singular Value Decomposition (SVD) is one of the most important matrix\nfactorizations, enjoying a wide variety of applications across numerous\napplication domains. In statistics and data analysis, the common applications\nof SVD such as Principal Components Analysis (PCA) and linear regression.\nUsually these applications arise on data that has far more rows than columns,\nso-called \"tall/skinny\" matrices. In the big data analytics context, this may\ntake the form of hundreds of millions to billions of rows with only a few\nhundred columns. There is a need, therefore, for fast, accurate, and scalable\ntall/skinny SVD implementations which can fully utilize modern computing\nresources. To that end, we present a survey of three different algorithms for\ncomputing the SVD for these kinds of tall/skinny data layouts using MPI for\ncommunication. We contextualize these with common big data analytics\ntechniques, principally PCA. Finally, we present both CPU and GPU timing\nresults from the Summit supercomputer, and discuss possible alternative\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 00:34:54 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Schmidt", "Drew", ""]]}, {"id": "2009.01924", "submitter": "Nina Montana Brown Miss", "authors": "N. Montana Brown, Y. Fu, S. U. Saeed, A. Casamitjana, Z. M. C. Baum,\n  R. Delaunay, Q. Yang, A. Grimwood, Z. Min, E. Bonmati, T. Vercauteren, M. J.\n  Clarkson, and Y. Hu", "title": "Introduction to Medical Image Registration with DeepReg, Between Old and\n  New", "comments": "Submitted to MICCAI Educational Challenge 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This document outlines a tutorial to get started with medical image\nregistration using the open-source package DeepReg. The basic concepts of\nmedical image registration are discussed, linking classical methods to newer\nmethods using deep learning. Two iterative, classical algorithms using\noptimisation and one learning-based algorithm using deep learning are coded\nstep-by-step using DeepReg utilities, all with real, open-accessible, medical\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 19:44:23 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 06:45:40 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Brown", "N. Montana", ""], ["Fu", "Y.", ""], ["Saeed", "S. U.", ""], ["Casamitjana", "A.", ""], ["Baum", "Z. M. C.", ""], ["Delaunay", "R.", ""], ["Yang", "Q.", ""], ["Grimwood", "A.", ""], ["Min", "Z.", ""], ["Bonmati", "E.", ""], ["Vercauteren", "T.", ""], ["Clarkson", "M. J.", ""], ["Hu", "Y.", ""]]}, {"id": "2009.02993", "submitter": "Raphael Sonabend", "authors": "Raphael Sonabend and Franz Kiraly", "title": "distr6: R6 Object-Oriented Probability Distributions Interface in R", "comments": "Accepted in The R Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.MS stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  distr6 is an object-oriented (OO) probability distributions interface\nleveraging the extensibility and scalability of R6, and the speed and\nefficiency of Rcpp. Over 50 probability distributions are currently implemented\nin the package with `core' methods including density, distribution, and\ngenerating functions, and more `exotic' ones including hazards and distribution\nfunction anti-derivatives. In addition to simple distributions, distr6 supports\ncompositions such as truncation, mixtures, and product distributions. This\npaper presents the core functionality of the package and demonstrates examples\nfor key use-cases. In addition this paper provides a critical review of the\nobject-oriented programming paradigms in R and describes some novel\nimplementations for design patterns and core object-oriented features\nintroduced by the package for supporting distr6 components.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2020 10:20:00 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 12:19:34 GMT"}, {"version": "v3", "created": "Sat, 20 Mar 2021 11:04:18 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sonabend", "Raphael", ""], ["Kiraly", "Franz", ""]]}, {"id": "2009.04399", "submitter": "Olena Rubanenko", "authors": "Gergely M\\'at\\'e Kiss, Jan Kaska, Roberto Andr\\'e Henrique de\n  Oliveira, Olena Rubanenko, Bal\\'azs T\\'oth", "title": "Performance Analysis of FEM Solvers on Practical Electromagnetic\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper presents a comparative analysis of different commercial and\nacademic software. The comparison aims to examine how the integrated adaptive\ngrid refinement methodologies can deal with challenging, electromagnetic-field\nrelated problems. For this comparison, two benchmark problems were examined in\nthe paper. The first example is a solution of an L-shape domain like test\nproblem, which has a singularity at a certain point in the geometry. The second\nproblem is an induction heated aluminum rod, which accurate solution needs to\nsolve a non-linear, coupled physical fields. The accurate solution of this\nproblem requires applying adaptive mesh generation strategies or applying a\nvery fine mesh in the electromagnetic domain, which can significantly increase\nthe computational complexity. The results show that the fully-hp adaptive\nmeshing strategies, which are integrated into Agros-suite, can significantly\nreduce the task's computational complexity compared to the automatic\nh-adaptivity, which is part of the examined, popular commercial solvers.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2020 21:24:36 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Kiss", "Gergely M\u00e1t\u00e9", ""], ["Kaska", "Jan", ""], ["de Oliveira", "Roberto Andr\u00e9 Henrique", ""], ["Rubanenko", "Olena", ""], ["T\u00f3th", "Bal\u00e1zs", ""]]}, {"id": "2009.04938", "submitter": "Simon Praetorius", "authors": "Simon Praetorius and Florian Stenger", "title": "Dune-CurvedGrid -- A Dune module for surface parametrization", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce and describe an implementation of curved (surface)\ngeometries within the Dune framework for grid-based discretizations. Therefore,\nwe employ the abstraction of geometries as local-functions bound to a grid\nelement, and the abstraction of a grid as connectivity of elements together\nwith a grid-function that can be localized to the elements to provide element\nlocal parametrizations of the curved surface.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 15:32:42 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 15:59:27 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Praetorius", "Simon", ""], ["Stenger", "Florian", ""]]}, {"id": "2009.06371", "submitter": "Noslen Hernandez", "authors": "Noslen Hern\\'andez and Aline Duarte", "title": "SeqROCTM: A Matlab toolbox for the analysis of Sequence of Random\n  Objects driven by Context Tree Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several research problems we deal with probabilistic sequences of inputs\n(e.g., sequence of stimuli) from which an agent generates a corresponding\nsequence of responses and it is of interest to model the relation between them.\nA new class of stochastic processes, namely \\textit{sequences of random objects\ndriven by context tree models}, has been introduced to model such relation in\nthe context of auditory statistical learning. This paper introduces a freely\navailable Matlab toolbox (SeqROCTM) that implements this new class of\nstochastic processes and three model selection procedures to make inference on\nit. Besides, due to the close relation of the new mathematical framework with\ncontext tree models, the toolbox also implements several existing model\nselection algorithms for context tree models.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 15:28:32 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 00:33:42 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 16:38:05 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Hern\u00e1ndez", "Noslen", ""], ["Duarte", "Aline", ""]]}, {"id": "2009.07495", "submitter": "Takeshi Iwashita", "authors": "Takeshi Iwashita, Kengo Suzuki, and Takeshi Fukaya", "title": "An Integer Arithmetic-Based Sparse Linear Solver Using a GMRES Method\n  and Iterative Refinement", "comments": null, "journal-ref": null, "doi": "10.1109/ScalA51936.2020.00006", "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a (preconditioned) GMRES solver based on integer\narithmetic, and introduce an iterative refinement framework for the solver. We\ndescribe the data format for the coefficient matrix and vectors for the solver\nthat is based on integer or fixed-point numbers. To avoid overflow in\ncalculations, we introduce initial scaling and logical shifts (adjustments) of\noperands in arithmetic operations. We present the approach for operand shifts,\nconsidering the characteristics of the GMRES algorithm. Numerical tests\ndemonstrate that the integer arithmetic-based solver with iterative refinement\nhas comparable solver performance in terms of convergence to the standard\nsolver based on floating-point arithmetic. Moreover, we show that\npreconditioning is important, not only for improving convergence but also\nreducing the risk of overflow.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 06:38:18 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 07:19:25 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Iwashita", "Takeshi", ""], ["Suzuki", "Kengo", ""], ["Fukaya", "Takeshi", ""]]}, {"id": "2009.07530", "submitter": "Luca Parisi", "authors": "Luca Parisi", "title": "m-arcsinh: An Efficient and Reliable Function for SVM and MLP in\n  scikit-learn", "comments": "20 pages, 4 listings/Python code snippets, 2 figures, 15 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.MS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the 'm-arcsinh', a modified ('m-') version of the\ninverse hyperbolic sine function ('arcsinh'). Kernel and activation functions\nenable Machine Learning (ML)-based algorithms, such as Support Vector Machine\n(SVM) and Multi-Layer Perceptron (MLP), to learn from data in a supervised\nmanner. m-arcsinh, implemented in the open source Python library\n'scikit-learn', is hereby presented as an efficient and reliable kernel and\nactivation function for SVM and MLP respectively. Improvements in reliability\nand speed to convergence in classification tasks on fifteen (N = 15) datasets\navailable from scikit-learn and the University California Irvine (UCI) Machine\nLearning repository are discussed. Experimental results demonstrate the overall\ncompetitive classification performance of both SVM and MLP, achieved via the\nproposed function. This function is compared to gold standard kernel and\nactivation functions, demonstrating its overall competitive reliability\nregardless of the complexity of the classification tasks involved.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 07:59:15 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Parisi", "Luca", ""]]}, {"id": "2009.07785", "submitter": "Boro Sofranac", "authors": "Boro Sofranac, Ambros Gleixner, Sebastian Pokutta", "title": "Accelerating Domain Propagation: an Efficient GPU-Parallel Algorithm\n  over Sparse Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.DS cs.MS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast domain propagation of linear constraints has become a crucial component\nof today's best algorithms and solvers for mixed integer programming and\npseudo-boolean optimization to achieve peak solving performance. Irregularities\nin the form of dynamic algorithmic behaviour, dependency structures, and\nsparsity patterns in the input data make efficient implementations of domain\npropagation on GPUs and, more generally, on parallel architectures challenging.\nThis is one of the main reasons why domain propagation in state-of-the-art\nsolvers is single thread only. In this paper, we present a new algorithm for\ndomain propagation which (a) avoids these problems and allows for an efficient\nimplementation on GPUs, and is (b) capable of running propagation rounds\nentirely on the GPU, without any need for synchronization or communication with\nthe CPU. We present extensive computational results which demonstrate the\neffectiveness of our approach and show that ample speedups are possible on\npractically relevant problems: on state-of-the-art GPUs, our geometric mean\nspeed-up for reasonably-large instances is around 10x to 20x and can be as high\nas 180x on favorably-large instances.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 16:25:29 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 13:17:25 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 16:27:00 GMT"}, {"version": "v4", "created": "Fri, 9 Jul 2021 14:28:53 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Sofranac", "Boro", ""], ["Gleixner", "Ambros", ""], ["Pokutta", "Sebastian", ""]]}, {"id": "2009.08805", "submitter": "Matteo Giacomini", "authors": "Matteo Giacomini, Ruben Sevilla and Antonio Huerta", "title": "HDGlab: An open-source implementation of the hybridisable discontinuous\n  Galerkin method in MATLAB", "comments": "90 pages, 51 figures", "journal-ref": "Archives of Computational Methods in Engineering, Vol. 28, Issue\n  3, pp. 1941-1986, 2021", "doi": "10.1007/s11831-020-09502-5", "report-no": null, "categories": "cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents HDGlab, an open source MATLAB implementation of the\nhybridisable discontinuous Galerkin (HDG) method. The main goal is to provide a\ndetailed description of both the HDG method for elliptic problems and its\nimplementation available in HDGlab. Ultimately, this is expected to make this\nrelatively new advanced discretisation method more accessible to the\ncomputational engineering community. HDGlab presents some features not\navailable in other implementations of the HDG method that can be found in the\nfree domain. First, it implements high-order polynomial shape functions up to\ndegree nine, with both equally-spaced and Fekete nodal distributions. Second,\nit supports curved isoparametric simplicial elements in two and three\ndimensions. Third, it supports non-uniform degree polynomial approximations and\nit provides a flexible structure to devise degree adaptivity strategies.\nFinally, an interface with the open-source high-order mesh generator Gmsh is\nprovided to facilitate its application to practical engineering problems.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 21:28:09 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Giacomini", "Matteo", ""], ["Sevilla", "Ruben", ""], ["Huerta", "Antonio", ""]]}, {"id": "2009.10071", "submitter": "Denisa Roberts", "authors": "Denisa A.O. Roberts and Lucas R. Roberts", "title": "QR and LQ Decomposition Matrix Backpropagation Algorithms for Square,\n  Wide, and Deep -- Real or Complex -- Matrices and Their Software\n  Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.MS cs.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents matrix backpropagation algorithms for the QR\ndecomposition of matrices $A_{m, n}$, that are either square (m = n), wide (m <\nn), or deep (m > n), with rank $k = min(m, n)$. Furthermore, we derive novel\nmatrix backpropagation results for the pivoted (full-rank) QR decomposition and\nfor the LQ decomposition of deep input matrices. Differentiable QR\ndecomposition offers a numerically stable, computationally efficient method to\nsolve least squares problems frequently encountered in machine learning and\ncomputer vision. Other use cases such as graph learning and network compression\nare listed in the article. Software implementation across popular deep learning\nframeworks (PyTorch, TensorFlow, MXNet) incorporate the methods for general use\nwithin the deep learning community. Furthermore, this article aids the\npractitioner in understanding the matrix backpropagation methodology as part of\nlarger computational graphs.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 21:03:37 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 13:43:04 GMT"}, {"version": "v3", "created": "Sat, 14 Nov 2020 21:18:53 GMT"}, {"version": "v4", "created": "Fri, 11 Dec 2020 12:54:23 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Roberts", "Denisa A. O.", ""], ["Roberts", "Lucas R.", ""]]}, {"id": "2009.10917", "submitter": "Noel Chalmers", "authors": "Noel Chalmers and Tim Warburton", "title": "Portable high-order finite element kernels I: Streaming Operations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the development of highly efficient kernels\nperforming vector operations relevant in linear system solvers. In particular,\nwe focus on the low arithmetic intensity operations (i.e., streaming\noperations) performed within the conjugate gradient iterative method, using the\nparameters specified in the CEED benchmark problems for high-order hexahedral\nfinite elements. We propose a suite of new Benchmark Streaming tests to focus\non the distinct streaming operations which must be performed. We implemented\nthese new tests using the OCCA abstraction framework to demonstrate portability\nof these streaming operations on different GPU architectures, and propose a\nsimple performance model for such kernels which can accurately capture data\nmovement rates as well as kernel launch costs.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 03:11:10 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Chalmers", "Noel", ""], ["Warburton", "Tim", ""]]}, {"id": "2009.11742", "submitter": "Francesco Rizzi", "authors": "Francesco Rizzi, Eric J. Parish, Patrick J. Blonigan, John Tencer", "title": "A compute-bound formulation of Galerkin model reduction for linear\n  time-invariant dynamical systems", "comments": "Revised version, 28 pages, 9 figures", "journal-ref": null, "doi": "10.1016/j.cma.2021.113973", "report-no": "SAND2020-10291", "categories": "physics.comp-ph cs.CE cs.DC cs.MS math.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work aims to advance computational methods for projection-based reduced\norder models (ROMs) of linear time-invariant (LTI) dynamical systems. For such\nsystems, current practice relies on ROM formulations expressing the state as a\nrank-1 tensor (i.e., a vector), leading to computational kernels that are\nmemory bandwidth bound and, therefore, ill-suited for scalable performance on\nmodern many-core and hybrid computing nodes. This weakness can be particularly\nlimiting when tackling many-query studies, where one needs to run a large\nnumber of simulations. This work introduces a reformulation, called rank-2\nGalerkin, of the Galerkin ROM for LTI dynamical systems which converts the\nnature of the ROM problem from memory bandwidth to compute bound. We present\nthe details of the formulation and its implementation, and demonstrate its\nutility through numerical experiments using, as a test case, the simulation of\nelastic seismic shear waves in an axisymmetric domain. We quantify and analyze\nperformance and scaling results for varying numbers of threads and problem\nsizes. Finally, we present an end-to-end demonstration of using the rank-2\nGalerkin ROM for a Monte Carlo sampling study. We show that the rank-2 Galerkin\nROM is one order of magnitude more efficient than the rank-1 Galerkin ROM (the\ncurrent practice) and about 970X more efficient than the full order model,\nwhile maintaining accuracy in both the mean and statistics of the field.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 15:06:00 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 16:02:32 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 11:55:18 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Rizzi", "Francesco", ""], ["Parish", "Eric J.", ""], ["Blonigan", "Patrick J.", ""], ["Tencer", "John", ""]]}, {"id": "2009.12009", "submitter": "Andrew  Myers", "authors": "Weiqun Zhang, Andrew Myers, Kevin Gott, Ann Almgren, John Bell", "title": "AMReX: Block-Structured Adaptive Mesh Refinement for Multiphysics\n  Applications", "comments": "16 pages, 9 figures, submitted to IJHPCA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Block-structured adaptive mesh refinement (AMR) provides the basis for the\ntemporal and spatial discretization strategy for a number of ECP applications\nin the areas of accelerator design, additive manufacturing, astrophysics,\ncombustion, cosmology, multiphase flow, and wind plant modelling. AMReX is a\nsoftware framework that provides a unified infrastructure with the\nfunctionality needed for these and other AMR applications to be able to\neffectively and efficiently utilize machines from laptops to exascale\narchitectures. AMR reduces the computational cost and memory footprint compared\nto a uniform mesh while preserving accurate descriptions of different physical\nprocesses in complex multi-physics algorithms. AMReX supports algorithms that\nsolve systems of partial differential equations (PDEs) in simple or complex\ngeometries, and those that use particles and/or particle-mesh operations to\nrepresent component physical processes. In this paper, we will discuss the core\nelements of the AMReX framework such as data containers and iterators as well\nas several specialized operations to meet the needs of the application\nprojects. In addition we will highlight the strategy that the AMReX team is\npursuing to achieve highly performant code across a range of accelerator-based\narchitectures for a variety of different applications.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 02:59:30 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Zhang", "Weiqun", ""], ["Myers", "Andrew", ""], ["Gott", "Kevin", ""], ["Almgren", "Ann", ""], ["Bell", "John", ""]]}, {"id": "2009.12101", "submitter": "Thomas Gr\\\"utzmacher", "authors": "Jos\\'e I. Aliaga and Hartwig Anzt and Thomas Gr\\\"utzmacher and Enrique\n  S. Quintana-Ort\\'i and Andr\\'es E. Tom\\'as", "title": "Compressed Basis GMRES on High Performance GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Krylov methods provide a fast and highly parallel numerical tool for the\niterative solution of many large-scale sparse linear systems. To a large\nextent, the performance of practical realizations of these methods is\nconstrained by the communication bandwidth in all current computer\narchitectures, motivating the recent investigation of sophisticated techniques\nto avoid, reduce, and/or hide the message-passing costs (in distributed\nplatforms) and the memory accesses (in all architectures).\n  This paper introduces a new communication-reduction strategy for the (Krylov)\nGMRES solver that advocates for decoupling the storage format (i.e., the data\nrepresentation in memory) of the orthogonal basis from the arithmetic precision\nthat is employed during the operations with that basis. Given that the\nexecution time of the GMRES solver is largely determined by the memory access,\nthe datatype transforms can be mostly hidden, resulting in the acceleration of\nthe iterative step via a lower volume of bits being retrieved from memory.\nTogether with the special properties of the orthonormal basis (whose elements\nare all bounded by 1), this paves the road toward the aggressive customization\nof the storage format, which includes some floating point as well as fixed\npoint formats with little impact on the convergence of the iterative process.\n  We develop a high performance implementation of the \"compressed basis GMRES\"\nsolver in the Ginkgo sparse linear algebra library and using a large set of\ntest problems from the SuiteSparse matrix collection we demonstrate robustness\nand performance advantages on a modern NVIDIA V100 GPU of up to 50% over the\nstandard GMRES solver that stores all data in IEEE double precision.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 09:37:38 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Aliaga", "Jos\u00e9 I.", ""], ["Anzt", "Hartwig", ""], ["Gr\u00fctzmacher", "Thomas", ""], ["Quintana-Ort\u00ed", "Enrique S.", ""], ["Tom\u00e1s", "Andr\u00e9s E.", ""]]}, {"id": "2009.12263", "submitter": "Tim Besard", "authors": "Thomas Faingnaert, Tim Besard, Bjorn De Sutter", "title": "Flexible Performant GEMM Kernels on GPUs", "comments": "This paper was submitted to IEEE TPDS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General Matrix Multiplication or GEMM kernels take center place in high\nperformance computing and machine learning. Recent NVIDIA GPUs include GEMM\naccelerators, such as NVIDIA's Tensor Cores. Their exploitation is hampered by\nthe two-language problem: it requires either low-level programming which\nimplies low programmer productivity or using libraries that only offer a\nlimited set of components. Because rephrasing algorithms in terms of\nestablished components often introduces overhead, the libraries' lack of\nflexibility limits the freedom to explore new algorithms. Researchers using\nGEMMs can hence not enjoy programming productivity, high performance, and\nresearch flexibility at once.\n  In this paper we solve this problem. We present three sets of abstractions\nand interfaces to program GEMMs within the scientific Julia programming\nlanguage. The interfaces and abstractions are co-designed for researchers'\nneeds and Julia's features to achieve sufficient separation of concerns and\nflexibility to easily extend basic GEMMs in many different ways without paying\na performance price. Comparing our GEMMs to state-of-the-art libraries cuBLAS\nand CUTLASS, we demonstrate that our performance is mostly on par with, and in\nsome cases even exceeds, the libraries, without having to write a single line\nof code in CUDA C++ or assembly, and without facing flexibility limitations.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 14:29:08 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 12:41:57 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 06:43:53 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Faingnaert", "Thomas", ""], ["Besard", "Tim", ""], ["De Sutter", "Bjorn", ""]]}, {"id": "2009.12386", "submitter": "Eduardo M. Vasconcelos", "authors": "Eduardo M. Vasconcelos and Adriano Gouveia de Souza", "title": "Regressor: A C program for Combinatorial Regressions", "comments": "6 pages, 3 figures, 1 algorithm and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistics, researchers use Regression models for data analysis and\nprediction in many productive sectors (industry, business, academy, etc.).\nRegression models are mathematical functions representing an approximation of\ndependent variable $Y$ from n independent variables $X_i \\in X$. The literature\npresents many regression methods divided into single and multiple regressions.\nThere are several procedures to generate regression models and sets of\ncommercial and academic tools that implement these procedures. This work\npresents one open-source program called Regressor that makes models from a\nspecific variation of polynomial regression. These models relate the\nindependent variables to generate an approximation of the original output\ndependent data. In many tests, Regressor was able to build models five times\nmore accurate than commercial tools.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 18:10:14 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Vasconcelos", "Eduardo M.", ""], ["de Souza", "Adriano Gouveia", ""]]}, {"id": "2009.12638", "submitter": "Nick Brown", "authors": "Nick Brown, J. Mark Bull, Iain Bethune", "title": "A highly scalable approach to solving linear systems using two-stage\n  multisplitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative methods for solving large sparse systems of linear equations are\nwidely used in many HPC applications. Extreme scaling of these methods can be\ndifficult, however, since global communication to form dot products is\ntypically required at every iteration.\n  To try to overcome this limitation we propose a hybrid approach, where the\nmatrix is partitioned into blocks. Within each block, we use a highly optimised\n(parallel) conventional solver, but we then couple the blocks together using\nblock Jacobi or some other multisplitting technique that can be implemented in\neither a synchronous or an asynchronous fashion. This allows us to limit the\nblock size to the point where the conventional iterative methods no longer\nscale, and to avoid global communication (and possibly synchronisation) across\nall processes.\n  Our block framework has been built to use PETSc, a popular scientific suite\nfor solving sparse linear systems, as the synchronous intra-block solver, and\nwe demonstrate results on up to 32768 cores of a Cray XE6 system. At this\nscale, the conventional solvers are still more efficient, though trends suggest\nthat the hybrid approach may be beneficial at higher core counts.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 16:44:34 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Brown", "Nick", ""], ["Bull", "J. Mark", ""], ["Bethune", "Iain", ""]]}, {"id": "2009.13357", "submitter": "Risheng Liu", "authors": "Yaohua Liu, Risheng Liu", "title": "BOML: A Modularized Bilevel Optimization Library in Python for Meta\n  Learning", "comments": "six pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning (a.k.a. learning to learn) has recently emerged as a promising\nparadigm for a variety of applications. There are now many meta-learning\nmethods, each focusing on different modeling aspects of base and meta learners,\nbut all can be (re)formulated as specific bilevel optimization problems. This\nwork presents BOML, a modularized optimization library that unifies several\nmeta-learning algorithms into a common bilevel optimization framework. It\nprovides a hierarchical optimization pipeline together with a variety of\niteration modules, which can be used to solve the mainstream categories of\nmeta-learning methods, such as meta-feature-based and meta-initialization-based\nformulations. The library is written in Python and is available at\nhttps://github.com/dut-media-lab/BOML.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 14:21:55 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Liu", "Yaohua", ""], ["Liu", "Risheng", ""]]}, {"id": "2009.13416", "submitter": "Robert Kl\\\"ofkorn", "authors": "Andreas Dedner, Robert Kl\\\"ofkorn", "title": "Extendible and Efficient Python Framework for Solving Evolution\n  Equations with Stabilized Discontinuous Galerkin Method", "comments": "36 pages, 15 figures, various Python code examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA math.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses a Python interface for the recently published\nDUNE-FEM-DG module which provides highly efficient implementations of the\nDiscontinuous Galerkin (DG) method for solving a wide range of non linear\npartial differential equations (PDE). Although the C++ interfaces of\nDUNE-FEM-DG are highly flexible and customizable, a solid knowledge of C++ is\nnecessary to make use of this powerful tool. With this work easier user\ninterfaces based on Python and the Unified Form Language are provided to open\nDUNE-FEM-DG for a broader audience. The Python interfaces are demonstrated for\nboth parabolic and first order hyperbolic PDEs.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2020 16:23:57 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 09:28:39 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Dedner", "Andreas", ""], ["Kl\u00f6fkorn", "Robert", ""]]}, {"id": "2009.14229", "submitter": "Amir Shahmoradi", "authors": "Amir Shahmoradi, Fatemeh Bagheri", "title": "ParaMonte: A high-performance serial/parallel Monte Carlo simulation\n  library for C, C++, Fortran", "comments": "submitted to JOSS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS astro-ph.IM physics.data-an q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ParaMonte (standing for Parallel Monte Carlo) is a serial and\nMPI/Coarray-parallelized library of Monte Carlo routines for sampling\nmathematical objective functions of arbitrary-dimensions, in particular, the\nposterior distributions of Bayesian models in data science, Machine Learning,\nand scientific inference. The ParaMonte library has been developed with the\ndesign goal of unifying the **automation**, **accessibility**,\n**high-performance**, **scalability**, and **reproducibility** of Monte Carlo\nsimulations. The current implementation of the library includes **ParaDRAM**, a\n**Para**llel **D**elyaed-**R**ejection **A**daptive **M**etropolis Markov Chain\nMonte Carlo sampler, accessible from a wide range of programming languages\nincluding C, C++, Fortran, with a unified Application Programming Interface and\nsimulation environment across all supported programming languages. The\nParaMonte library is MIT-licensed and is permanently located and maintained at\n[https://github.com/cdslaborg/paramonte](https://github.com/cdslaborg/paramonte).\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 18:04:02 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Shahmoradi", "Amir", ""], ["Bagheri", "Fatemeh", ""]]}, {"id": "2009.14276", "submitter": "Rasmus Christiansen", "authors": "Rasmus E. Christiansen, Ole Sigmund", "title": "Compact 200 line MATLAB code for inverse design in photonics by topology\n  optimization: tutorial", "comments": "5 Figures, 17 pages", "journal-ref": null, "doi": "10.1364/JOSAB.405955", "report-no": null, "categories": "cs.MS cs.CE physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a compact 200 line MATLAB code demonstrating how topology\noptimization (TopOpt) as an inverse design tool may be used in photonics,\ntargeting the design of two-dimensional dielectric metalenses and a metallic\nreflector as examples. The physics model is solved using the finite element\nmethod, and the code utilizes MATLAB's fmincon algorithm to solve the\noptimization problem. In addition to presenting the code itself, we briefly\ndiscuss a number of extensions and provide the code required to implement some\nof these. Finally, we demonstrate the superiority of using a gradient-based\nmethod compared to a genetic-algorithm-based method (using MATLAB's ga\nalgorithm) for solving inverse design problems in photonics. The MATLAB\nsoftware is freely available in the paper and may be downloaded from\nhttps://www.topopt.mek.dtu.dk.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 14:07:07 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 09:07:26 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 14:56:52 GMT"}, {"version": "v4", "created": "Sat, 28 Nov 2020 13:28:53 GMT"}, {"version": "v5", "created": "Tue, 23 Feb 2021 12:19:51 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Christiansen", "Rasmus E.", ""], ["Sigmund", "Ole", ""]]}, {"id": "2009.14600", "submitter": "Orestis Zachariadis", "authors": "Orestis Zachariadis, Nitin Satpute, Juan G\\'omez-Luna, Joaqu\\'in\n  Olivares", "title": "Accelerating Sparse Matrix-Matrix Multiplication with GPU Tensor Cores", "comments": "Accepted in CAEE", "journal-ref": "Comput. Electr. Eng. 88 (2020) 106848", "doi": "10.1016/j.compeleceng.2020.106848", "report-no": null, "categories": "cs.MS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse general matrix-matrix multiplication (spGEMM) is an essential\ncomponent in many scientific and data analytics applications. However, the\nsparsity pattern of the input matrices and the interaction of their patterns\nmake spGEMM challenging. Modern GPUs include Tensor Core Units (TCUs), which\nspecialize in dense matrix multiplication. Our aim is to re-purpose TCUs for\nsparse matrices. The key idea of our spGEMM algorithm, tSparse, is to multiply\nsparse rectangular blocks using the mixed precision mode of TCUs. tSparse\npartitions the input matrices into tiles and operates only on tiles which\ncontain one or more elements. It creates a task list of the tiles, and performs\nmatrix multiplication of these tiles using TCUs. To the best of our knowledge,\nthis is the first time that TCUs are used in the context of spGEMM. We show\nthat spGEMM, with our tiling approach, benefits from TCUs. Our approach\nsignificantly improves the performance of spGEMM in comparison to cuSPARSE,\nCUSP, RMerge2, Nsparse, AC-SpGEMM and spECK.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 14:10:15 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Zachariadis", "Orestis", ""], ["Satpute", "Nitin", ""], ["G\u00f3mez-Luna", "Juan", ""], ["Olivares", "Joaqu\u00edn", ""]]}]