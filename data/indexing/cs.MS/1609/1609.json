[{"id": "1609.00076", "submitter": "Robert van De Geijn", "authors": "Jianyu Huang and Robert A. van de Geijn", "title": "BLISlab: A Sandbox for Optimizing GEMM", "comments": "FLAME Working Note #80", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix-matrix multiplication is a fundamental operation of great importance\nto scientific computing and, increasingly, machine learning. It is a simple\nenough concept to be introduced in a typical high school algebra course yet in\npractice important enough that its implementation on computers continues to be\nan active research topic. This note describes a set of exercises that use this\noperation to illustrate how high performance can be attained on modern CPUs\nwith hierarchical memories (multiple caches). It does so by building on the\ninsights that underly the BLAS-like Library Instantiation Software (BLIS)\nframework by exposing a simplified \"sandbox\" that mimics the implementation in\nBLIS. As such, it also becomes a vehicle for the \"crowd sourcing\" of the\noptimization of BLIS. We call this set of exercises BLISlab.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 01:11:48 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Huang", "Jianyu", ""], ["van de Geijn", "Robert A.", ""]]}, {"id": "1609.00829", "submitter": "Javier Segura", "authors": "A. Gil, J. Segura, N. M. Temme", "title": "Efficient computation of Laguerre polynomials", "comments": "To appear in Computer Physics Communications", "journal-ref": null, "doi": "10.1016/j.cpc.2016.09.002", "report-no": null, "categories": "cs.NA cs.MS math.CA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient algorithm and a Fortran 90 module (LaguerrePol) for computing\nLaguerre polynomials $L^{(\\alpha)}_n(z)$ are presented. The standard three-term\nrecurrence relation satisfied by the polynomials and different types of\nasymptotic expansions valid for $n$ large and $\\alpha$ small, are used\ndepending on the parameter region.\n  Based on tests of contiguous relations in the parameter $\\alpha$ and the\ndegree $n$ satisfied by the polynomials, we claim that a relative accuracy\nclose or better than $10^{-12}$ can be obtained using the module LaguerrePol\nfor computing the functions $L^{(\\alpha)}_n(z)$ in the parameter range $z \\ge\n0$, $-1 < \\alpha \\le 5$, $n \\ge 0$.\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2016 13:59:43 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Gil", "A.", ""], ["Segura", "J.", ""], ["Temme", "N. M.", ""]]}, {"id": "1609.00999", "submitter": "Lingchuan Meng", "authors": "Lingchuan Meng", "title": "Automatic Generation of Vectorized Montgomery Algorithm", "comments": "14 pages, 5 figures, based on the thesis work by Lingchuan Meng", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modular arithmetic is widely used in crytography and symbolic computation.\nThis paper presents a vectorized Montgomery algorithm for modular\nmultiplication, the key to fast modular arithmetic, that fully utilizes the\nSIMD instructions. We further show how the vectorized algorithm can be\nautomatically generated by the {\\SPIRAL} system, as part of the effort for\nautomatic generation of a modular polynomial multiplication library.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2016 23:44:20 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Meng", "Lingchuan", ""]]}, {"id": "1609.01088", "submitter": "Evgeny Burnaev", "authors": "Mikhail Belyaev, Evgeny Burnaev, Ermek Kapushev, Maxim Panov, Pavel\n  Prikhodko, Dmitry Vetrov, Dmitry Yarotsky", "title": "GTApprox: surrogate modeling for industrial design", "comments": "31 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe GTApprox - a new tool for medium-scale surrogate modeling in\nindustrial design. Compared to existing software, GTApprox brings several\ninnovations: a few novel approximation algorithms, several advanced methods of\nautomated model selection, novel options in the form of hints. We demonstrate\nthe efficiency of GTApprox on a large collection of test problems. In addition,\nwe describe several applications of GTApprox to real engineering problems.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 10:41:14 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Belyaev", "Mikhail", ""], ["Burnaev", "Evgeny", ""], ["Kapushev", "Ermek", ""], ["Panov", "Maxim", ""], ["Prikhodko", "Pavel", ""], ["Vetrov", "Dmitry", ""], ["Yarotsky", "Dmitry", ""]]}, {"id": "1609.01277", "submitter": "Christian Jacobs", "authors": "Christian T. Jacobs, Satya P. Jammy, Neil D. Sandham", "title": "OpenSBLI: A framework for the automated derivation and parallel\n  execution of finite difference solvers on a range of computer architectures", "comments": "Author accepted version, with a small amendment: the link in the\n  \"Code Availability\" section has been updated, and now refers to the OpenSBLI\n  source code repository on GitHub. Accepted for publication in the Journal of\n  Computational Science on 8 November 2016", "journal-ref": "Journal of Computational Science 18 (2017) 12-23", "doi": "10.1016/j.jocs.2016.11.001", "report-no": null, "categories": "cs.MS cs.SC cs.SE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exascale computing will feature novel and potentially disruptive hardware\narchitectures. Exploiting these to their full potential is non-trivial.\nNumerical modelling frameworks involving finite difference methods are\ncurrently limited by the 'static' nature of the hand-coded discretisation\nschemes and repeatedly may have to be re-written to run efficiently on new\nhardware. In contrast, OpenSBLI uses code generation to derive the model's code\nfrom a high-level specification. Users focus on the equations to solve, whilst\nnot concerning themselves with the detailed implementation. Source-to-source\ntranslation is used to tailor the code and enable its execution on a variety of\nhardware.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 10:11:31 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 17:17:48 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Jacobs", "Christian T.", ""], ["Jammy", "Satya P.", ""], ["Sandham", "Neil D.", ""]]}, {"id": "1609.02831", "submitter": "Stefano Marchesini", "authors": "Benedikt J. Daurer, Hari Krishnan, Talita Perciano, Filipe R.N.C.\n  Maia, David A. Shapiro, James A. Sethian and Stefano Marchesini", "title": "Nanosurveyor: a framework for real-time data processing", "comments": "8 pages, 3 figures", "journal-ref": "Advanced Structural and Chemical Imaging 2017 3:7", "doi": "10.1186/s40679-017-0039-0", "report-no": null, "categories": "physics.ins-det cs.MS physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists are drawn to synchrotrons and accelerator based light sources\nbecause of their brightness, coherence and flux. The rate of improvement in\nbrightness and detector technology has outpaced Moore's law growth seen for\ncomputers, networks, and storage, and is enabling novel observations and\ndiscoveries with faster frame rates, larger fields of view, higher resolution,\nand higher dimensionality. Here we present an integrated software/algorithmic\nframework designed to capitalize on high throughput experiments, and describe\nthe streamlined processing pipeline of ptychography data analysis. The pipeline\nprovides throughput, compression, and resolution as well as rapid feedback to\nthe microscope operators.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 15:21:19 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Daurer", "Benedikt J.", ""], ["Krishnan", "Hari", ""], ["Perciano", "Talita", ""], ["Maia", "Filipe R. N. C.", ""], ["Shapiro", "David A.", ""], ["Sethian", "James A.", ""], ["Marchesini", "Stefano", ""]]}, {"id": "1609.03361", "submitter": "Michael Lange", "authors": "Michael Lange, Navjot Kukreja, Mathias Louboutin, Fabio Luporini,\n  Felippe Vieira, Vincenzo Pandolfo, Paulius Velesko, Paulius Kazakas, Gerard\n  Gorman", "title": "Devito: Towards a generic Finite Difference DSL using Symbolic Python", "comments": "pyHPC 2016 conference submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain specific languages (DSL) have been used in a variety of fields to\nexpress complex scientific problems in a concise manner and provide automated\nperformance optimization for a range of computational architectures. As such\nDSLs provide a powerful mechanism to speed up scientific Python computation\nthat goes beyond traditional vectorization and pre-compilation approaches,\nwhile allowing domain scientists to build applications within the comforts of\nthe Python software ecosystem. In this paper we present Devito, a new finite\ndifference DSL that provides optimized stencil computation from high-level\nproblem specifications based on symbolic Python expressions. We demonstrate\nDevito's symbolic API and performance advantages over traditional Python\nacceleration methods before highlighting its use in the scientific context of\nseismic inversion problems.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 12:15:36 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Lange", "Michael", ""], ["Kukreja", "Navjot", ""], ["Louboutin", "Mathias", ""], ["Luporini", "Fabio", ""], ["Vieira", "Felippe", ""], ["Pandolfo", "Vincenzo", ""], ["Velesko", "Paulius", ""], ["Kazakas", "Paulius", ""], ["Gorman", "Gerard", ""]]}, {"id": "1609.03457", "submitter": "Deanna Pineau", "authors": "Deanna C. Pineau (University of Waterloo)", "title": "Math-Aware Search Engines: Physics Applications and Overview", "comments": "Full abstract in PDF; 66 pages, 4 tables, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.IR cs.MS math.HO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search engines for equations now exist, which return results matching the\nquery's mathematical meaning or structural presentation. Operating over\nscientific papers, online encyclopedias, and math discussion forums, their\ncontent includes physics, math, and other sciences. They enable physicists to\navoid jargon and more easily target mathematical content within and across\ndisciplines. As a natural extension of keyword-based search, they open up a new\nworld for discovering both exact and approximate mathematical solutions;\nphysical systems' analogues and alternative models; and physics' patterns.\n  This review presents the existing math-aware search engines, discusses\nmethods for maximizing their search success, and overviews their math-matching\ncapabilities. Proposed applications to physics are also given, to contribute\ntowards developers' and physicists' exploration of the newly available search\nhorizons.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 06:07:23 GMT"}], "update_date": "2016-09-13", "authors_parsed": [["Pineau", "Deanna C.", "", "University of Waterloo"]]}, {"id": "1609.04504", "submitter": "Brett Naul", "authors": "Brett Naul, St\\'efan van der Walt, Arien Crellin-Quick, Joshua S.\n  Bloom, Fernando P\\'erez", "title": "cesium: Open-Source Platform for Time-Series Inference", "comments": "Proceedings of the 15th Python in Science Conference (SciPy 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference on time series data is a common requirement in many scientific\ndisciplines and internet of things (IoT) applications, yet there are few\nresources available to domain scientists to easily, robustly, and repeatably\nbuild such complex inference workflows: traditional statistical models of time\nseries are often too rigid to explain complex time domain behavior, while\npopular machine learning packages require already-featurized dataset inputs.\nMoreover, the software engineering tasks required to instantiate the\ncomputational platform are daunting. cesium is an end-to-end time series\nanalysis framework, consisting of a Python library as well as a web front-end\ninterface, that allows researchers to featurize raw data and apply modern\nmachine learning techniques in a simple, reproducible, and extensible way.\nUsers can apply out-of-the-box feature engineering workflows as well as save\nand replay their own analyses. Any steps taken in the front end can also be\nexported to a Jupyter notebook, so users can iterate between possible models\nwithin the front end and then fine-tune their analysis using the additional\ncapabilities of the back-end library. The open-source packages make us of many\nuse modern Python toolkits, including xarray, dask, Celery, Flask, and\nscikit-learn.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 04:09:48 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Naul", "Brett", ""], ["van der Walt", "St\u00e9fan", ""], ["Crellin-Quick", "Arien", ""], ["Bloom", "Joshua S.", ""], ["P\u00e9rez", "Fernando", ""]]}, {"id": "1609.04809", "submitter": "Sashikumaar Ganesan", "authors": "Sashikumaar Ganesan, Volker John, Gunar Matthies, Raviteja Meesala,\n  Shamim Abdus, Ulrich Wilbrandt", "title": "An object oriented parallel finite element scheme for computations of\n  PDEs: Design and implementation", "comments": "10 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel finite element algorithms based on object-oriented concepts are\npresented. Moreover, the design and implementation of a data structure proposed\nare utilized in realizing a parallel geometric multigrid method. The\nParFEMapper and the ParFECommunicator are the key components of the data\nstructure in the proposed parallel scheme. These classes are constructed based\non the type of finite elements (continuous or nonconforming or discontinuous)\nused. The proposed solver is compared with the open source direct solvers,\nMUMPS and PasTiX. Further, the performance of the parallel multigrid solver is\nanalyzed up to 1080 processors. The solver shows a very good speedup up to 960\nprocessors and the problem size has to be increased in order to maintain the\ngood speedup when the number of processors are increased further. As a result,\nthe parallel solver is able to handle large scale problems on massively\nparallel supercomputers. The proposed parallel finite element algorithms and\nmultigrid solver are implemented in our in-house package ParMooN.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 05:05:29 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Ganesan", "Sashikumaar", ""], ["John", "Volker", ""], ["Matthies", "Gunar", ""], ["Meesala", "Raviteja", ""], ["Abdus", "Shamim", ""], ["Wilbrandt", "Ulrich", ""]]}, {"id": "1609.07008", "submitter": "Edgar Solomonik", "authors": "Edgar Solomonik, Maciej Besta, Flavio Vella, and Torsten Hoefler", "title": "Scaling betweenness centrality using communication-efficient sparse\n  matrix multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DM cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Betweenness centrality (BC) is a crucial graph problem that measures the\nsignificance of a vertex by the number of shortest paths leading through it. We\npropose Maximal Frontier Betweenness Centrality (MFBC): a succinct BC algorithm\nbased on novel sparse matrix multiplication routines that performs a factor of\n$p^{1/3}$ less communication on $p$ processors than the best known\nalternatives, for graphs with $n$ vertices and average degree $k=n/p^{2/3}$. We\nformulate, implement, and prove the correctness of MFBC for weighted graphs by\nleveraging monoids instead of semirings, which enables a surprisingly succinct\nformulation. MFBC scales well for both extremely sparse and relatively dense\ngraphs. It automatically searches a space of distributed data decompositions\nand sparse matrix multiplication algorithms for the most advantageous\nconfiguration. The MFBC implementation outperforms the well-known CombBLAS\nlibrary by up to 8x and shows more robust performance. Our design methodology\nis readily extensible to other graph problems.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 15:01:30 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 15:30:00 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Solomonik", "Edgar", ""], ["Besta", "Maciej", ""], ["Vella", "Flavio", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1609.07301", "submitter": "Maxie Schmidt", "authors": "Maxie D. Schmidt", "title": "A Computer Algebra Package for Polynomial Sequence Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The software package developed in the MS thesis research implements functions\nfor the intelligent guessing of polynomial sequence formulas based on\nuser-defined expected sequence factors of the input coefficients. We present a\nspecialized hybrid approach to finding exact representations for polynomial\nsequences that is motivated by the need for an automated procedures to discover\nthe precise forms of these sums based on user guidance, or intuition, as to\nspecial sequence factors present in the formulas. In particular, the package\ncombines the user input on the expected special sequence factors in the\npolynomial coefficient formulas with calls to the existing functions as\nsubroutines that then process formulas for the remaining sequence terms already\nrecognized by these packages.\n  The factorization-based approach to polynomial sequence recognition is unique\nto this package and allows the search functions to find expressions for\npolynomial sums involving Stirling numbers and other special triangular\nsequences that are not readily handled by other software packages. In contrast\nto many other sequence recognition and summation software, the package not\nprovide an explicit proof, or certificate, for the correctness of these\nsequence formulas -- only computationally guided educated guesses at a complete\nidentity generating the sequence over all $n$. The thesis contains a number of\nconcrete, working examples of the package that are intended to both demonstrate\nits usage and to document its current sequence recognition capabilities.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 10:31:39 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Schmidt", "Maxie D.", ""]]}, {"id": "1609.08642", "submitter": "Jeremy Kepner", "authors": "Timothy Weale, Vijay Gadepally, Dylan Hutchison, Jeremy Kepner", "title": "Benchmarking the Graphulo Processing Framework", "comments": "5 pages, 4 figures, IEEE High Performance Extreme Computing (HPEC)\n  conference 2016", "journal-ref": null, "doi": "10.1109/HPEC.2016.7761640", "report-no": null, "categories": "cs.DB cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph algorithms have wide applicablity to a variety of domains and are often\nused on massive datasets. Recent standardization efforts such as the GraphBLAS\nspecify a set of key computational kernels that hardware and software\ndevelopers can adhere to. Graphulo is a processing framework that enables\nGraphBLAS kernels in the Apache Accumulo database. In our previous work, we\nhave demonstrated a core Graphulo operation called \\textit{TableMult} that\nperforms large-scale multiplication operations of database tables. In this\narticle, we present the results of scaling the Graphulo engine to larger\nproblems and scalablity when a greater number of resources is used.\nSpecifically, we present two experiments that demonstrate Graphulo scaling\nperformance is linear with the number of available resources. The first\nexperiment demonstrates cluster processing rates through Graphulo's TableMult\noperator on two large graphs, scaled between $2^{17}$ and $2^{19}$ vertices.\nThe second experiment uses TableMult to extract a random set of rows from a\nlarge graph ($2^{19}$ nodes) to simulate a cued graph analytic. These\nbenchmarking results are of relevance to Graphulo users who wish to apply\nGraphulo to their graph problems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 20:09:03 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Weale", "Timothy", ""], ["Gadepally", "Vijay", ""], ["Hutchison", "Dylan", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1609.08722", "submitter": "Anton Leykin", "authors": "Timothy Duff, Cvetelina Hill, Anders Jensen, Kisun Lee, Anton Leykin,\n  Jeff Sommars", "title": "Solving polynomial systems via homotopy continuation and monodromy", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study methods for finding the solution set of a generic system in a family\nof polynomial systems with parametric coefficients. We present a framework for\ndescribing monodromy based solvers in terms of decorated graphs. Under the\ntheoretical assumption that monodromy actions are generated uniformly, we show\nthat the expected number of homotopy paths tracked by an algorithm following\nthis framework is linear in the number of solutions. We demonstrate that our\nsoftware implementation is competitive with the existing state-of-the-art\nmethods implemented in other software packages.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2016 01:48:48 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 14:55:40 GMT"}, {"version": "v3", "created": "Mon, 5 Jun 2017 19:16:25 GMT"}, {"version": "v4", "created": "Mon, 16 Apr 2018 20:01:02 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Duff", "Timothy", ""], ["Hill", "Cvetelina", ""], ["Jensen", "Anders", ""], ["Lee", "Kisun", ""], ["Leykin", "Anton", ""], ["Sommars", "Jeff", ""]]}, {"id": "1609.09841", "submitter": "Arturo Vargas", "authors": "Arturo Vargas, Jesse Chan, Thomas Hagstrom, and Timothy Warburton", "title": "GPU Acceleration of Hermite Methods for the Simulation of Wave\n  Propagation", "comments": "12 pages. Submitted to ICOSAHOM 2016 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hermite methods of Goodrich, Hagstrom, and Lorenz (2006) use Hermite\ninterpolation to construct high order numerical methods for hyperbolic initial\nvalue problems. The structure of the method has several favorable features for\nparallel computing. In this work, we propose algorithms that take advantage of\nthe many-core architecture of Graphics Processing Units. The algorithm exploits\nthe compact stencil of Hermite methods and uses data structures that allow for\nefficient data load and stores. Additionally the highly localized evolution\noperator of Hermite methods allows us to combine multi-stage time-stepping\nmethods within the new algorithms incurring minimal accesses of global memory.\nUsing a scalar linear wave equation, we study the algorithm by considering\nHermite interpolation and evolution as individual kernels and alternatively\ncombined them into a monolithic kernel. For both approaches we demonstrate\nstrategies to increase performance. Our numerical experiments show that\nalthough a two kernel approach allows for better performance on the hardware, a\nmonolithic kernel can offer a comparable time to solution with less global\nmemory usage.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 18:02:54 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Vargas", "Arturo", ""], ["Chan", "Jesse", ""], ["Hagstrom", "Thomas", ""], ["Warburton", "Timothy", ""]]}]