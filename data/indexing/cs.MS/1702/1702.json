[{"id": "1702.00629", "submitter": "Matthias Werner", "authors": "Peter Steinbach and Matthias Werner", "title": "gearshifft - The FFT Benchmark Suite for Heterogeneous Platforms", "comments": null, "journal-ref": "High Performance Computing, Theoretical Computer Science and\n  General Issues, Vol. 10266, Springer International Publishing. (ISC High\n  Performance 2017)", "doi": "10.1007/978-3-319-58667-0", "report-no": null, "categories": "cs.PF cs.MS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fast Fourier Transforms (FFTs) are exploited in a wide variety of fields\nranging from computer science to natural sciences and engineering. With the\nrising data production bandwidths of modern FFT applications, judging best\nwhich algorithmic tool to apply, can be vital to any scientific endeavor. As\ntailored FFT implementations exist for an ever increasing variety of high\nperformance computer hardware, choosing the best performing FFT implementation\nhas strong implications for future hardware purchase decisions, for resources\nFFTs consume and for possibly decisive financial and time savings ahead of the\ncompetition. This paper therefor presents gearshifft, which is an open-source\nand vendor agnostic benchmark suite to process a wide variety of problem sizes\nand types with state-of-the-art FFT implementations (fftw, clfft and cufft).\ngearshifft provides a reproducible, unbiased and fair comparison on a wide\nvariety of hardware to explore which FFT variant is best for a given problem\nsize.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 11:41:32 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 14:37:00 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Steinbach", "Peter", ""], ["Werner", "Matthias", ""]]}, {"id": "1702.01332", "submitter": "Michael Huth", "authors": "Andrea Callia D'Iddio and Michael Huth", "title": "Manyopt: An Extensible Tool for Mixed, Non-Linear Optimization Through\n  SMT Solving", "comments": "17 pages, 3 figures, link to open research data and code available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization of Mixed-Integer Non-Linear Programming (MINLP) supports\nimportant decisions in applications such as Chemical Process Engineering. But\ncurrent solvers have limited ability for deductive reasoning or the use of\ndomain-specific theories, and the management of integrality constraints does\nnot yet exploit automated reasoning tools such as SMT solvers. This seems to\nlimit both scalability and reach of such tools in practice. We therefore\npresent a tool, ManyOpt, for MINLP optimization that enables experimentation\nwith reduction techniques which transform a MINLP problem to feasibility\nchecking realized by an SMT solver. ManyOpt is similar to the SAT solver\nManySAT in that it runs a specified number of such reduction techniques in\nparallel to get the strongest result on a given MINLP problem. The tool is\nimplemented in layers, which we may see as features and where reduction\ntechniques are feature vectors. Some of these features are inspired by known\nMINLP techniques whereas others are novel and specific to SMT. Our experimental\nresults on standard benchmarks demonstrate the benefits of this approach. The\ntool supports a variety of SMT solvers and is easily extensible with new\nfeatures, courtesy of its layered structure. For example, logical formulas for\ndeductive reasoning are easily added to constrain further the optimization of a\nMINLP problem of interest.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 19:49:06 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["D'Iddio", "Andrea Callia", ""], ["Huth", "Michael", ""]]}, {"id": "1702.01460", "submitter": "Piotr Szyma\\'nski", "authors": "Piotr Szyma\\'nski, Tomasz Kajdanowicz", "title": "A scikit-based Python environment for performing multi-label\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  scikit-multilearn is a Python library for performing multi-label\nclassification. The library is compatible with the scikit/scipy ecosystem and\nuses sparse matrices for all internal operations. It provides native Python\nimplementations of popular multi-label classification methods alongside a novel\nframework for label space partitioning and division. It includes modern\nalgorithm adaptation methods, network-based label space division approaches,\nwhich extracts label dependency information and multi-label embedding\nclassifiers. It provides python wrapped access to the extensive multi-label\nmethod stack from Java libraries and makes it possible to extend deep learning\nsingle-label methods for multi-label tasks. The library allows multi-label\nstratification and data set management. The implementation is more efficient in\nproblem transformation than other established libraries, has good test coverage\nand follows PEP8. Source code and documentation can be downloaded from\nhttp://scikit.ml and also via pip. The library follows BSD licensing scheme.\n", "versions": [{"version": "v1", "created": "Sun, 5 Feb 2017 22:28:20 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 17:28:44 GMT"}, {"version": "v3", "created": "Thu, 9 Feb 2017 15:03:23 GMT"}, {"version": "v4", "created": "Fri, 7 Dec 2018 10:39:48 GMT"}, {"version": "v5", "created": "Mon, 10 Dec 2018 15:01:50 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Szyma\u0144ski", "Piotr", ""], ["Kajdanowicz", "Tomasz", ""]]}, {"id": "1702.04715", "submitter": "Carlos Palenzuela", "authors": "A. Arbona, B. Mi\\~nano, A. Rigo, C. Bona, C. Palenzuela, A. Artigues,\n  C. Bona-Casas and J. Mass\\'o", "title": "Simflowny 2: An upgraded platform for scientific modeling and simulation", "comments": "26 pages, 21 figures", "journal-ref": null, "doi": "10.1016/j.cpc.2018.03.015", "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simflowny is an open platform which automatically generates parallel code of\nscientific dynamical models for different simulation frameworks. Here we\npresent major upgrades on this software to support an extended set of families\nof models, in particular: i) a new generic family for partial differential\nequations, which can include spatial derivatives of any order, ii) a new family\nfor agent based models to study complex phenomena --either on a spatial domain\nor on a graph--. Additionally we introduce a flexible graphical user interface\n(GUI) to accommodate these and future families of equations. This paper\ndescribes the new GUI architecture and summarizes the formal representation and\nimplementation of these new families, providing several validation results.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 20:49:22 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Arbona", "A.", ""], ["Mi\u00f1ano", "B.", ""], ["Rigo", "A.", ""], ["Bona", "C.", ""], ["Palenzuela", "C.", ""], ["Artigues", "A.", ""], ["Bona-Casas", "C.", ""], ["Mass\u00f3", "J.", ""]]}, {"id": "1702.06343", "submitter": "Satoshi Egi", "authors": "Satoshi Egi", "title": "Scalar and Tensor Parameters for Importing Tensor Index Notation\n  including Einstein Summation Notation", "comments": "Scheme and Functional Programming Workshop 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a method for importing tensor index notation,\nincluding Einstein summation notation, into functional programming. This method\ninvolves introducing two types of parameters, i.e, scalar and tensor\nparameters, and simplified tensor index rules that do not handle expressions\nthat are valid only for the Cartesian coordinate system, in which the index can\nmove up and down freely. An example of such an expression is \"c = A_i B_i\". As\nan ordinary function, when a tensor parameter obtains a tensor as an argument,\nthe function treats the tensor argument as a whole. In contrast, when a scalar\nparameter obtains a tensor as an argument, the function is applied to each\ncomponent of the tensor. In this paper, we show that introducing these two\ntypes of parameters and our simplified index rules enables us to apply\narbitrary user-defined functions to tensor arguments using index notation\nincluding Einstein summation notation without requiring an additional\ndescription to enable each function to handle tensors.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 12:05:59 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 12:03:34 GMT"}, {"version": "v3", "created": "Thu, 30 Mar 2017 09:26:02 GMT"}, {"version": "v4", "created": "Tue, 9 May 2017 01:30:28 GMT"}, {"version": "v5", "created": "Mon, 24 Jul 2017 07:34:39 GMT"}, {"version": "v6", "created": "Tue, 8 Aug 2017 00:35:19 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Egi", "Satoshi", ""]]}, {"id": "1702.06407", "submitter": "John V Monaco", "authors": "John V. Monaco, Malka Gorfine, Li Hsu", "title": "General Semiparametric Shared Frailty Model Estimation and Simulation\n  with frailtySurv", "comments": null, "journal-ref": "Monaco, J., Gorfine, M., & Hsu, L. (2018). General Semiparametric\n  Shared Frailty Model: Estimation and Simulation with frailtySurv. Journal of\n  Statistical Software, 86(4), 1 - 42.\n  doi:http://dx.doi.org/10.18637/jss.v086.i04", "doi": null, "report-no": null, "categories": "stat.CO cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The R package frailtySurv for simulating and fitting semi-parametric shared\nfrailty models is introduced. Package frailtySurv implements semi-parametric\nconsistent estimators for a variety of frailty distributions, including gamma,\nlog-normal, inverse Gaussian and power variance function, and provides\nconsistent estimators of the standard errors of the parameters' estimators. The\nparameters' estimators are asymptotically normally distributed, and therefore\nstatistical inference based on the results of this package, such as hypothesis\ntesting and confidence intervals, can be performed using the normal\ndistribution. Extensive simulations demonstrate the flexibility and correct\nimplementation of the estimator. Two case studies performed with publicly\navailable datasets demonstrate applicability of the package. In the Diabetic\nRetinopathy Study, the onset of blindness is clustered by patient, and in a\nlarge hard drive failure dataset, failure times are thought to be clustered by\nthe hard drive manufacturer and model.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 14:40:29 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 14:07:34 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 22:01:05 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Monaco", "John V.", ""], ["Gorfine", "Malka", ""], ["Hsu", "Li", ""]]}, {"id": "1702.06898", "submitter": "Jose A. Fonseca", "authors": "Carsten Burstedde, Jose A. Fonseca, Stefan Kollet", "title": "Enhancing speed and scalability of the ParFlow simulation code", "comments": "The final publication is available at link.springer.com", "journal-ref": "Computational Geosciences 2017", "doi": "10.1007/s10596-017-9696-2", "report-no": null, "categories": "cs.MS cs.DC physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regional hydrology studies are often supported by high resolution simulations\nof subsurface flow that require expensive and extensive computations. Efficient\nusage of the latest high performance parallel computing systems becomes a\nnecessity. The simulation software ParFlow has been demonstrated to meet this\nrequirement and shown to have excellent solver scalability for up to 16,384\nprocesses. In the present work we show that the code requires further\nenhancements in order to fully take advantage of current petascale machines. We\nidentify ParFlow's way of parallelization of the computational mesh as a\ncentral bottleneck. We propose to reorganize this subsystem using fast mesh\npartition algorithms provided by the parallel adaptive mesh refinement library\np4est. We realize this in a minimally invasive manner by modifying selected\nparts of the code to reinterpret the existing mesh data structures. We evaluate\nthe scaling performance of the modified version of ParFlow, demonstrating good\nweak and strong scaling up to 458k cores of the Juqueen supercomputer, and test\nan example application at large scale.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 17:12:49 GMT"}, {"version": "v2", "created": "Mon, 2 Oct 2017 18:51:52 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Burstedde", "Carsten", ""], ["Fonseca", "Jose A.", ""], ["Kollet", "Stefan", ""]]}, {"id": "1702.08425", "submitter": "Lois Curfman McInnes", "authors": "Roscoe Bartlett, Irina Demeshko, Todd Gamblin, Glenn Hammond, Michael\n  Heroux, Jeffrey Johnson, Alicia Klinvex, Xiaoye Li, Lois Curfman McInnes, J.\n  David Moulton, Daniel Osei-Kuffuor, Jason Sarich, Barry Smith, Jim\n  Willenbring, Ulrike Meier Yang", "title": "xSDK Foundations: Toward an Extreme-scale Scientific Software\n  Development Kit", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme-scale computational science increasingly demands multiscale and\nmultiphysics formulations. Combining software developed by independent groups\nis imperative: no single team has resources for all predictive science and\ndecision support capabilities. Scientific libraries provide high-quality,\nreusable software components for constructing applications with improved\nrobustness and portability. However, without coordination, many libraries\ncannot be easily composed. Namespace collisions, inconsistent arguments, lack\nof third-party software versioning, and additional difficulties make\ncomposition costly.\n  The Extreme-scale Scientific Software Development Kit (xSDK) defines\ncommunity policies to improve code quality and compatibility across\nindependently developed packages (hypre, PETSc, SuperLU, Trilinos, and\nAlquimia) and provides a foundation for addressing broader issues in software\ninteroperability, performance portability, and sustainability. The xSDK\nprovides turnkey installation of member software and seamless combination of\naggregate capabilities, and it marks first steps toward extreme-scale\nscientific software ecosystems from which future applications can be composed\nrapidly with assured quality and scalability.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 18:37:36 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Bartlett", "Roscoe", ""], ["Demeshko", "Irina", ""], ["Gamblin", "Todd", ""], ["Hammond", "Glenn", ""], ["Heroux", "Michael", ""], ["Johnson", "Jeffrey", ""], ["Klinvex", "Alicia", ""], ["Li", "Xiaoye", ""], ["McInnes", "Lois Curfman", ""], ["Moulton", "J. David", ""], ["Osei-Kuffuor", "Daniel", ""], ["Sarich", "Jason", ""], ["Smith", "Barry", ""], ["Willenbring", "Jim", ""], ["Yang", "Ulrike Meier", ""]]}, {"id": "1702.08880", "submitter": "Mark Adams", "authors": "M. F. Adams, E. Hirvijoki, M. G. Knepley, J. Brown, T. Isaac, and R.\n  Mills", "title": "Landau Collision Integral Solver with Adaptive Mesh Refinement on\n  Emerging Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Landau collision integral is an accurate model for the small-angle\ndominated Coulomb collisions in fusion plasmas. We investigate a high order\naccurate, fully conservative, finite element discretization of the nonlinear\nmulti-species Landau integral with adaptive mesh refinement using the PETSc\nlibrary (www.mcs.anl.gov/petsc). We develop algorithms and techniques to\nefficiently utilize emerging architectures with an approach that minimizes\nmemory usage and movement and is suitable for vector processing. The Landau\ncollision integral is vectorized with Intel AVX-512 intrinsics and the solver\nsustains as much as 22% of the theoretical peak flop rate of the Second\nGeneration Intel Xeon Phi, Knights Landing, processor.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 15:06:53 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 04:15:28 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Adams", "M. F.", ""], ["Hirvijoki", "E.", ""], ["Knepley", "M. G.", ""], ["Brown", "J.", ""], ["Isaac", "T.", ""], ["Mills", "R.", ""]]}]