[{"id": "2101.00086", "submitter": "Emanuele Guidotti", "authors": "Emanuele Guidotti", "title": "calculus: High Dimensional Numerical and Symbolic Calculus in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The R package calculus implements C++ optimized functions for numerical and\nsymbolic calculus, such as the Einstein summing convention, fast computation of\nthe Levi-Civita symbol and generalized Kronecker delta, Taylor series\nexpansion, multivariate Hermite polynomials, high-order derivatives, ordinary\ndifferential equations, differential operators and numerical integration in\narbitrary orthogonal coordinate systems. The library applies numerical methods\nwhen working with R functions or symbolic programming when working with\ncharacters or expressions. The package handles multivariate numerical calculus\nin arbitrary dimensions and coordinates and implements the symbolic counterpart\nof the numerical methods whenever possible, without depending on external\ncomputer algebra systems. Except for Rcpp, the package has no strict\ndependencies in order to provide a stable self-contained toolbox that invites\nre-use.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 21:52:19 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Guidotti", "Emanuele", ""]]}, {"id": "2101.01867", "submitter": "Neha R. Gupta", "authors": "Neha R. Gupta (1), Vittorio Orlandi (1), Chia-Rui Chang (2), Tianyu\n  Wang (1), Marco Morucci (1), Pritam Dey (1), Thomas J. Howell (1), Xian Sun\n  (1), Angikar Ghosal (1), Sudeepa Roy (1), Cynthia Rudin (1), Alexander\n  Volfovsky (1) ((1) Duke University, (2) Harvard University)", "title": "dame-flame: A Python Library Providing Fast Interpretable Matching for\n  Causal Inference", "comments": "5 pages, 1 figure; Reference and discussion of CEM corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  dame-flame is a Python package for performing matching for observational\ncausal inference on datasets containing discrete covariates. This package\nimplements the Dynamic Almost Matching Exactly (DAME) and Fast Large-Scale\nAlmost Matching Exactly (FLAME) algorithms, which match treatment and control\nunits on subsets of the covariates. The resulting matched groups are\ninterpretable, because the matches are made on covariates (rather than, for\ninstance, propensity scores), and high-quality, because machine learning is\nused to determine which covariates are important to match on. DAME solves an\noptimization problem that matches units on as many covariates as possible,\nprioritizing matches on important covariates. FLAME approximates the solution\nfound by DAME via a much faster backward feature selection procedure. The\npackage provides several adjustable parameters to adapt the algorithms to\nspecific applications, and can calculate treatment effects after matching.\nDescriptions of these parameters, details on estimating treatment effects, and\nfurther examples, can be found in the documentation at\nhttps://almost-matching-exactly.github.io/DAME-FLAME-Python-Package/\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 04:38:57 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 18:21:44 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Gupta", "Neha R.", "", "Duke University"], ["Orlandi", "Vittorio", "", "Duke University"], ["Chang", "Chia-Rui", "", "Harvard University"], ["Wang", "Tianyu", "", "Duke University"], ["Morucci", "Marco", "", "Duke University"], ["Dey", "Pritam", "", "Duke University"], ["Howell", "Thomas J.", "", "Duke University"], ["Sun", "Xian", "", "Duke University"], ["Ghosal", "Angikar", "", "Duke University"], ["Roy", "Sudeepa", "", "Duke University"], ["Rudin", "Cynthia", "", "Duke University"], ["Volfovsky", "Alexander", "", "Duke University"]]}, {"id": "2101.02164", "submitter": "Dominique Orban", "authors": "Ding Ma, Dominique Orban, Michael A. Saunders", "title": "A Julia implementation of Algorithm NCL for constrained optimization", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.29888.35841", "report-no": "Cahier du GERAD G-2021-02", "categories": "math.OC cs.MS cs.NA math.NA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Algorithm NCL is designed for general smooth optimization problems where\nfirst and second derivatives are available, including problems whose\nconstraints may not be linearly independent at a solution (i.e., do not satisfy\nthe LICQ). It is equivalent to the LANCELOT augmented Lagrangian method,\nreformulated as a short sequence of nonlinearly constrained subproblems that\ncan be solved efficiently by IPOPT and KNITRO, with warm starts on each\nsubproblem. We give numerical results from a Julia implementation of Algorithm\nNCL on tax policy models that do not satisfy the LICQ, and on nonlinear\nleast-squares problems and general problems from the CUTEst test set.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 17:57:44 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Ma", "Ding", ""], ["Orban", "Dominique", ""], ["Saunders", "Michael A.", ""]]}, {"id": "2101.05063", "submitter": "Angelika Schwarz", "authors": "Angelika Schwarz", "title": "Robust level-3 BLAS Inverse Iteration from the Hessenberg Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse iteration is known to be an effective method for computing\neigenvectors corresponding to simple and well-separated eigenvalues. In the\nnon-symmetric case, the solution of shifted Hessenberg systems is a central\nstep. Existing inverse iteration solvers approach the solution of the shifted\nHessenberg systems with either RQ or LU factorizations and, once factored,\nsolve the corresponding systems. This approach has limited level-3 BLAS\npotential since distinct shifts have distinct factorizations. This paper\nrearranges the RQ approach such that data shared between distinct shifts is\nexposed. Thereby the backward substitution with the triangular R factor can be\nexpressed mostly with matrix-matrix multiplications (level-3 BLAS). The\nresulting algorithm computes eigenvectors in a tiled, overflow-free, and\ntask-parallel fashion. The numerical experiments show that the new algorithm\noutperforms existing inverse iteration solvers for the computation of both real\nand complex eigenvectors.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 13:49:38 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Schwarz", "Angelika", ""]]}, {"id": "2101.05158", "submitter": "David Ham", "authors": "David A. Ham", "title": "UFL Dual Spaces, a proposal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This white paper highlights current limitations in the algebraic closure\nUnified Form Language (UFL). UFL currently represents forms over finite element\nspaces, however finite element problems naturally result in objects in the dual\nto a finite element space, and operators mapping between primal and dual finite\nelement spaces. This document sketches the relevant mathematical areas and\nproposes changes to the UFL language to support dual spaces as first class\ntypes in UFL.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 15:55:51 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Ham", "David A.", ""]]}, {"id": "2101.06550", "submitter": "Andrew Gloster", "authors": "Andrew Gloster", "title": "GPU Methodologies for Numerical Partial Differential Equations", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis we develop techniques to efficiently solve numerical Partial\nDifferential Equations (PDEs) using Graphical Processing Units (GPUs). Focus is\nput on both performance and re--usability of the methods developed, to this end\na library, cuSten, for applying finite--difference stencils to numerical grids\nis presented herein. On top of this various batched tridiagonal and\npentadiagonal matrix solvers are discussed. These have been benchmarked against\nthe current state of the art and shown to improve performance in the solution\nof numerical PDEs. A variety of other benchmarks and use cases for the GPU\nmethodologies are presented using the Cahn--Hilliard equation as a core\nexample, but it is emphasised the methods are completely general. Finally\nthrough the application of the GPU methodologies to the Cahn--Hilliard equation\nnew results are presented on the growth rates of the coarsened domains. In\nparticular a statistical model is built up using batches of simulations run on\nGPUs from which the growth rates are extracted, it is shown that in a finite\ndomain that the traditionally presented results of 1/3 scaling is in fact a\ndistribution around this value. This result is discussed in conjunction with\nmodelling via a stochastic PDE and sheds new light on the behaviour of the\nCahn--Hilliard equation in finite domains.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 23:24:40 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Gloster", "Andrew", ""]]}, {"id": "2101.06584", "submitter": "Tomonori Kouya", "authors": "Tomonori Kouya", "title": "Acceleration of multiple precision matrix multiplication based on\n  multi-component floating-point arithmetic using AVX2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we report the results obtained from the acceleration of\nmulti-binary64-type multiple precision matrix multiplication with AVX2. We\ntarget double-double (DD), triple-double (TD), and quad-double (QD) precision\narithmetic designed by certain types of error-free transformation (EFT)\narithmetic. Furthermore, we implement SIMDized EFT functions, which\nsimultaneously compute with four binary64 numbers on x86_64 computing\nenvironment, and by using help of them, we also develop SIMDized DD, TD, and QD\nadditions and multiplications. In addition, AVX2 load/store functions were\nadopted to efficiently speed up reading and storing matrix elements from/to\nmemory. Owing to these combined techniques, our implemented multiple precision\nmatrix multiplications have been accelerated more than three times compared\nwith non-accelerated ones. Our accelerated matrix multiplication modifies the\nperformance of parallelization with OpenMP.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 04:05:13 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Kouya", "Tomonori", ""]]}, {"id": "2101.06665", "submitter": "Farhad Merchant", "authors": "Vinay Saxena, Ankitha Reddy, Jonathan Neudorfer, John Gustafson,\n  Sangeeth Nambiar, Rainer Leupers, Farhad Merchant", "title": "Brightening the Optical Flow through Posit Arithmetic", "comments": "To appear in ISQED 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.MS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  As new technologies are invented, their commercial viability needs to be\ncarefully examined along with their technical merits and demerits. The posit\ndata format, proposed as a drop-in replacement for IEEE 754 float format, is\none such invention that requires extensive theoretical and experimental study\nto identify products that can benefit from the advantages of posits for\nspecific market segments. In this paper, we present an extensive empirical\nstudy of posit-based arithmetic vis-\\`a-vis IEEE 754 compliant arithmetic for\nthe optical flow estimation method called Lucas-Kanade (LuKa). First, we use\nSoftPosit and SoftFloat format emulators to perform an empirical error analysis\nof the LuKa method. Our study shows that the average error in LuKa with\nSoftPosit is an order of magnitude lower than LuKa with SoftFloat. We then\npresent the integration of the hardware implementation of a posit adder and\nmultiplier in a RISC-V open-source platform. We make several recommendations,\nalong with the analysis of LuKa in the RISC-V context, for future generation\nplatforms incorporating posit arithmetic units.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 13:19:10 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Saxena", "Vinay", ""], ["Reddy", "Ankitha", ""], ["Neudorfer", "Jonathan", ""], ["Gustafson", "John", ""], ["Nambiar", "Sangeeth", ""], ["Leupers", "Rainer", ""], ["Merchant", "Farhad", ""]]}, {"id": "2101.06682", "submitter": "Radoslava Hristova", "authors": "I. Hristov, R. Hristova, S. Dimova, P. Armyanov, N. Shegunov, I.\n  Puzynin, T. Puzynina, Z. Sharipov, Z. Tukhliev", "title": "On the efficient parallel computing of long term reliable trajectories\n  for the Lorenz system", "comments": "10 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:2010.14993", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose an efficient parallelization of multiple-precision\nTaylor series method with variable stepsize and fixed order. For given level of\naccuracy the optimal variable stepsize determines higher order of the method\nthan in the case of optimal fixed stepsize. Although the used order of the\nmethod is greater then that in the case of fixed stepsize, and hence the\ncomputational work per step is greater, the reduced number of steps gives less\noverall work. Also the greater order of the method is beneficial in the sense\nthat it increases the parallel efficiency. As a model problem we use the\nparadigmatic Lorenz system. With 256 CPU cores in Nestum cluster, Sofia,\nBulgaria, we succeed to obtain a correct reference solution in the rather long\ntime interval - [0,11000]. To get this solution we performed two large\ncomputations: one computation with 4566 decimal digits of precision and 5240-th\norder method, and second computation for verification - with 4778 decimal\ndigits of precision and 5490-th order method.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 14:42:42 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Hristov", "I.", ""], ["Hristova", "R.", ""], ["Dimova", "S.", ""], ["Armyanov", "P.", ""], ["Shegunov", "N.", ""], ["Puzynin", "I.", ""], ["Puzynina", "T.", ""], ["Sharipov", "Z.", ""], ["Tukhliev", "Z.", ""]]}, {"id": "2101.07758", "submitter": "Robert Y. Lewis", "authors": "Robert Y. Lewis and Minchao Wu", "title": "A bi-directional extensible interface between Lean and Mathematica", "comments": "arXiv admin note: substantial text overlap with arXiv:1712.09288", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We implement a user-extensible ad hoc connection between the Lean proof\nassistant and the computer algebra system Mathematica. By reflecting the syntax\nof each system in the other and providing a flexible interface for extending\ntranslation, our connection allows for the exchange of arbitrary information\nbetween the two systems.\n  We show how to make use of the Lean metaprogramming framework to verify\ncertain Mathematica computations, so that the rigor of the proof assistant is\nnot compromised. We also use Mathematica as an untrusted oracle to guide proof\nsearch in the proof assistant and interact with a Mathematica notebook from\nwithin a Lean session. In the other direction, we import and process Lean\ndeclarations from within Mathematica. The proof assistant library serves as a\ndatabase of mathematical knowledge that the CAS can display and explore.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 11:33:25 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Lewis", "Robert Y.", ""], ["Wu", "Minchao", ""]]}, {"id": "2101.10881", "submitter": "Jan Verschelde", "authors": "Jan Verschelde", "title": "Accelerated Polynomial Evaluation and Differentiation at Power Series in\n  Multiple Double Precision", "comments": "Improved the introduction, adding two citations to related work;\n  fixed error, added table on the fluctuations of wall clock times. To appear\n  in the Proceedings of the 2021 IEEE International Parallel and Distributed\n  Processing Symposium Workshops (IPDPSW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA cs.SC math.AG math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem is to evaluate a polynomial in several variables and its gradient\nat a power series truncated to some finite degree with multiple double\nprecision arithmetic. To compensate for the cost overhead of multiple double\nprecision and power series arithmetic, data parallel algorithms for general\npurpose graphics processing units are presented. The reverse mode of\nalgorithmic differentiation is organized into a massively parallel computation\nof many convolutions and additions of truncated power series. Experimental\nresults demonstrate that teraflop performance is obtained in deca double\nprecision with power series truncated at degree 152. The algorithms scale well\nfor increasing precision and increasing degrees.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 19:42:43 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 16:14:34 GMT"}, {"version": "v3", "created": "Sat, 13 Mar 2021 00:22:10 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Verschelde", "Jan", ""]]}, {"id": "2101.11003", "submitter": "Steven Golovkine", "authors": "Steven Golovkine", "title": "FDApy: a Python package for functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the Python package, FDApy, as an implementation of functional\ndata. This package provide modules for the analysis of such data. It includes\nclasses for different dimensional data as well as irregularly sampled\nfunctional data. A simulation toolbox is also provided. It might be used to\nsimulate different clusters of functional data. Some methodologies to handle\nthese data are implemented, such as dimension reduction and clustering. New\nmethods can be easily added. The package is publicly available on the Python\nPackage Index and Github.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 10:07:33 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Golovkine", "Steven", ""]]}, {"id": "2101.11227", "submitter": "David Issa Mattos", "authors": "David Issa Mattos, \\'Erika Martins Silva Ramos", "title": "Bayesian Paired-Comparison with the bpcs Package", "comments": "In Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces the bpcs R package (Bayesian Paired Comparison in\nStan) and the statistical models implemented in the package. This package aims\nto facilitate the use of Bayesian models for paired comparison data in\nbehavioral research. Bayesian analysis of paired comparison data allows\nparameter estimation even in conditions where the maximum likelihood does not\nexist, allows easy extension of paired comparison models, provide\nstraightforward interpretation of the results with credible intervals, have\nbetter control of type I error, have more robust evidence towards the null\nhypothesis, allows propagation of uncertainties, includes prior information,\nand perform well when handling models with many parameters and latent\nvariables. The bpcs package provides a consistent interface for R users and\nseveral functions to evaluate the posterior distribution of all parameters, to\nestimate the posterior distribution of any contest between items, and to obtain\nthe posterior distribution of the ranks. Three reanalyses of recent studies\nthat used the frequentist Bradley-Terry model are presented. These reanalyses\nare conducted with the Bayesian models of the bpcs package, and all the code\nused to fit the models, generate the figures, and the tables are available in\nthe online appendix.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 07:13:46 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 13:10:11 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 10:42:15 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Mattos", "David Issa", ""], ["Ramos", "\u00c9rika Martins Silva", ""]]}, {"id": "2101.11408", "submitter": "Daniel Lemire", "authors": "Daniel Lemire", "title": "Number Parsing at a Gigabyte per Second", "comments": "Software at https://github.com/fastfloat/fast_float and\n  https://github.com/lemire/simple_fastfloat_benchmark/", "journal-ref": "Software: Practice and Experience 51 (8), 2021", "doi": "10.1002/spe.2984", "report-no": null, "categories": "cs.DS cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With disks and networks providing gigabytes per second, parsing decimal\nnumbers from strings becomes a bottleneck. We consider the problem of parsing\ndecimal numbers to the nearest binary floating-point value. The general problem\nrequires variable-precision arithmetic. However, we need at most 17 digits to\nrepresent 64-bit standard floating-point numbers (IEEE 754). Thus we can\nrepresent the decimal significand with a single 64-bit word. By combining the\nsignificand and precomputed tables, we can compute the nearest floating-point\nnumber using as few as one or two 64-bit multiplications. Our implementation\ncan be several times faster than conventional functions present in standard C\nlibraries on modern 64-bit systems (Intel, AMD, ARM and POWER9). Our work is\navailable as open source software used by major systems such as Apache Arrow\nand Yandex ClickHouse. The Go standard library has adopted a version of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 20:31:27 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 23:57:29 GMT"}, {"version": "v3", "created": "Wed, 24 Feb 2021 17:17:14 GMT"}, {"version": "v4", "created": "Tue, 23 Mar 2021 00:52:54 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Lemire", "Daniel", ""]]}, {"id": "2101.12127", "submitter": "Jiri Simsa", "authors": "Derek G. Murray, Jiri Simsa, Ana Klimovic, Ihor Indyk", "title": "tf.data: A Machine Learning Data Processing Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training machine learning models requires feeding input data for models to\ningest. Input pipelines for machine learning jobs are often challenging to\nimplement efficiently as they require reading large volumes of data, applying\ncomplex transformations, and transferring data to hardware accelerators while\noverlapping computation and communication to achieve optimal performance. We\npresent tf.data, a framework for building and executing efficient input\npipelines for machine learning jobs. The tf.data API provides operators which\ncan be parameterized with user-defined computation, composed, and reused across\ndifferent machine learning domains. These abstractions allow users to focus on\nthe application logic of data processing, while tf.data's runtime ensures that\npipelines run efficiently.\n  We demonstrate that input pipeline performance is critical to the end-to-end\ntraining time of state-of-the-art machine learning models. tf.data delivers the\nhigh performance required, while avoiding the need for manual tuning of\nperformance knobs. We show that tf.data features, such as parallelism, caching,\nstatic optimizations, and non-deterministic execution are essential for high\nperformance. Finally, we characterize machine learning input pipelines for\nmillions of jobs that ran in Google's fleet, showing that input data processing\nis highly diverse and consumes a significant fraction of job resources. Our\nanalysis motivates future research directions, such as sharing computation\nacross jobs and pushing data projection to the storage layer.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 17:16:46 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 22:56:12 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Murray", "Derek G.", ""], ["Simsa", "Jiri", ""], ["Klimovic", "Ana", ""], ["Indyk", "Ihor", ""]]}, {"id": "2101.12425", "submitter": "David Avis", "authors": "David Avis and Charles Jordan", "title": "lrsarith: a small fixed/hybrid arithmetic C library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe lrsarith which is a small fixed precision and hybrid arithmetic C\nlibrary for integers and rationals that we developed for use in the lrslib\nlibrary for polyhedral computation. Using a generic set of operations, a\nprogram can be compiled with either 64-bit or 128-bit (if available) fixed\nprecision, with an extended precision library such as GMP or the built-in MP\nroutines. A simple scheme checks for overflow and either terminates the program\nor, in hybrid mode, changes to a higher precision arithmetic. Implementing\nthese arithmetics in lrslib resulted in only minimal changes to the original\ncode. We give computational results using lrs and mplrs, vertex/facet\nenumeration codes in lrslib, using 64 and 128 bit fixed integer arithmetic with\nand without overflow checking, GMP arithmetic, lrsarith hybrid arithmetic with\nboth GMP and MP, and FLINT hybrid arithmetic. We give a small self-contained\nexample C program using the lrsarith package in both fixed precision and hybrid\nmode.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 06:40:53 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Avis", "David", ""], ["Jordan", "Charles", ""]]}]