[{"id": "2011.00715", "submitter": "Richard Mills", "authors": "Richard Tran Mills, Mark F. Adams, Satish Balay, Jed Brown, Alp Dener,\n  Matthew Knepley, Scott E. Kruger, Hannah Morgan, Todd Munson, Karl Rupp,\n  Barry F. Smith, Stefano Zampini, Hong Zhang, Junchao Zhang", "title": "Toward Performance-Portable PETSc for GPU-based Exascale Systems", "comments": "14 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": "ANL/MCS-P9401-1020", "categories": "cs.MS cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Portable Extensible Toolkit for Scientific computation (PETSc) library\ndelivers scalable solvers for nonlinear time-dependent differential and\nalgebraic equations and for numerical optimization.The PETSc design for\nperformance portability addresses fundamental GPU accelerator challenges and\nstresses flexibility and extensibility by separating the programming model used\nby the application from that used by the library, and it enables application\ndevelopers to use their preferred programming model, such as Kokkos, RAJA,\nSYCL, HIP, CUDA, or OpenCL, on upcoming exascale systems. A blueprint for using\nGPUs from PETSc-based codes is provided, and case studies emphasize the\nflexibility and high performance achieved on current GPU-based systems.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 04:05:29 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Mills", "Richard Tran", ""], ["Adams", "Mark F.", ""], ["Balay", "Satish", ""], ["Brown", "Jed", ""], ["Dener", "Alp", ""], ["Knepley", "Matthew", ""], ["Kruger", "Scott E.", ""], ["Morgan", "Hannah", ""], ["Munson", "Todd", ""], ["Rupp", "Karl", ""], ["Smith", "Barry F.", ""], ["Zampini", "Stefano", ""], ["Zhang", "Hong", ""], ["Zhang", "Junchao", ""]]}, {"id": "2011.00898", "submitter": "Christian M\\\"uller", "authors": "L\\'eo Simpson, Patrick L. Combettes, Christian L. M\\\"uller", "title": "c-lasso -- a Python package for constrained sparse and robust regression\n  and classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.MS math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce c-lasso, a Python package that enables sparse and robust linear\nregression and classification with linear equality constraints. The underlying\nstatistical forward model is assumed to be of the following form: \\[ y = X\n\\beta + \\sigma \\epsilon \\qquad \\textrm{subject to} \\qquad C\\beta=0 \\] Here, $X\n\\in \\mathbb{R}^{n\\times d}$is a given design matrix and the vector $y \\in\n\\mathbb{R}^{n}$ is a continuous or binary response vector. The matrix $C$ is a\ngeneral constraint matrix. The vector $\\beta \\in \\mathbb{R}^{d}$ contains the\nunknown coefficients and $\\sigma$ an unknown scale. Prominent use cases are\n(sparse) log-contrast regression with compositional data $X$, requiring the\nconstraint $1_d^T \\beta = 0$ (Aitchion and Bacon-Shone 1984) and the\nGeneralized Lasso which is a special case of the described problem (see, e.g,\n(James, Paulson, and Rusmevichientong 2020), Example 3). The c-lasso package\nprovides estimators for inferring unknown coefficients and scale (i.e.,\nperspective M-estimators (Combettes and M\\\"uller 2020a)) of the form \\[\n\\min_{\\beta \\in \\mathbb{R}^d, \\sigma \\in \\mathbb{R}_{0}} f\\left(X\\beta -\ny,{\\sigma} \\right) + \\lambda \\left\\lVert \\beta\\right\\rVert_1 \\qquad\n\\textrm{subject to} \\qquad C\\beta = 0 \\] for several convex loss functions\n$f(\\cdot,\\cdot)$. This includes the constrained Lasso, the constrained scaled\nLasso, and sparse Huber M-estimators with linear equality constraints.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 11:16:27 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Simpson", "L\u00e9o", ""], ["Combettes", "Patrick L.", ""], ["M\u00fcller", "Christian L.", ""]]}, {"id": "2011.01207", "submitter": "Jean-Philip Piquemal", "authors": "Olivier Adjoua, Louis Lagard\\`ere, Luc-Henri Jolly, Arnaud Durocher,\n  Thibaut Very, Isabelle Dupays, Zhi Wang, Th\\'eo Jaffrelot Inizan,\n  Fr\\'ed\\'eric C\\'elerse, Pengyu Ren, Jay W. Ponder, Jean-Philip Piquemal", "title": "Tinker-HP : Accelerating Molecular Dynamics Simulations of Large Complex\n  Systems with Advanced Point Dipole Polarizable Force Fields using GPUs and\n  Multi-GPUs systems", "comments": null, "journal-ref": "Journal of Chemical Theory and Computation, 2021, 17, 4, 2034-2053", "doi": "10.1021/acs.jctc.0c01164", "report-no": null, "categories": "physics.comp-ph cs.DC cs.MS physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the extension of the Tinker-HP package (Lagard\\`ere et al., Chem.\nSci., 2018,9, 956-972) to the use of Graphics Processing Unit (GPU) cards to\naccelerate molecular dynamics simulations using polarizable many-body force\nfields. The new high-performance module allows for an efficient use of single-\nand multi-GPU architectures ranging from research laboratories to modern\nsupercomputer centers. After detailing an analysis of our general scalable\nstrategy that relies on OpenACC and CUDA, we discuss the various capabilities\nof the package. Among them, the multi-precision possibilities of the code are\ndiscussed. If an efficient double precision implementation is provided to\npreserve the possibility of fast reference computations, we show that a lower\nprecision arithmetic is preferred providing a similar accuracy for molecular\ndynamics while exhibiting superior performances. As Tinker-HP is mainly\ndedicated to accelerate simulations using new generation point dipole\npolarizable force field, we focus our study on the implementation of the AMOEBA\nmodel. Testing various NVIDIA platforms including 2080Ti, 3090, V100 and A100\ncards, we provide illustrative benchmarks of the code for single- and\nmulti-cards simulations on large biosystems encompassing up to millions of\natoms. The new code strongly reduces time to solution and offers the best\nperformances to date obtained using the AMOEBA polarizable force field.\nPerspectives toward the strong-scaling performance of our multi-node massive\nparallelization strategy, unsupervised adaptive sampling and large scale\napplicability of the Tinker-HP code in biophysics are discussed. The present\nsoftware has been released in phase advance on GitHub in link with the High\nPerformance Computing community COVID-19 research efforts and is free for\nAcademics (see https://github.com/TinkerTools/tinker-hp).\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 18:50:39 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 17:08:19 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2021 16:51:06 GMT"}, {"version": "v4", "created": "Wed, 3 Mar 2021 20:01:20 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Adjoua", "Olivier", ""], ["Lagard\u00e8re", "Louis", ""], ["Jolly", "Luc-Henri", ""], ["Durocher", "Arnaud", ""], ["Very", "Thibaut", ""], ["Dupays", "Isabelle", ""], ["Wang", "Zhi", ""], ["Inizan", "Th\u00e9o Jaffrelot", ""], ["C\u00e9lerse", "Fr\u00e9d\u00e9ric", ""], ["Ren", "Pengyu", ""], ["Ponder", "Jay W.", ""], ["Piquemal", "Jean-Philip", ""]]}, {"id": "2011.01728", "submitter": "Fredrik Johansson", "authors": "Fredrik Johansson (LFANT)", "title": "Calcium: computing in exact real and complex fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calcium is a C library for real and complex numbers in a form suitable for\nexact algebraic and symbolic computation. Numbers are represented as elements\nof fields $\\mathbb{Q}(a_1,\\ldots,a_n)$ where the extensions numbers $a_k$ may\nbe algebraic or transcendental. The system combines efficient field operations\nwith automatic discovery and certification of algebraic relations, resulting in\na practical computational model of $\\mathbb{R}$ and $\\mathbb{C}$ in which\nequality is rigorously decidable for a large class of numbers.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 14:22:18 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Johansson", "Fredrik", "", "LFANT"]]}, {"id": "2011.01740", "submitter": "Ferenc Heged\\H{u}s Dr.", "authors": "D\\'aniel Nagy and Lambert Plavecz and Ferenc Heged\\H{u}s", "title": "Solving large number of non-stiff, low-dimensional ordinary differential\n  equation systems on GPUs and CPUs: performance comparisons of MPGOS, ODEINT\n  and DifferentialEquations.jl", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.NA cs.PF math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the performance characteristics of different solution\ntechniques and program packages to solve a large number of independent ordinary\ndifferential equation systems is examined. The employed hardware are an Intel\nCore i7-4820K CPU with 30.4 GFLOPS peak double-precision performance per cores\nand an Nvidia GeForce Titan Black GPU that has a total of 1707 GFLOPS peak\ndouble-precision performance. The tested systems (Lorenz equation,\nKeller--Miksis equation and a pressure relief valve model) are non-stiff and\nhave low dimension. Thus, the performance of the codes are not limited by\nmemory bandwidth, and Runge--Kutta type solvers are efficient and suitable\nchoices. The tested program packages are MPGOS written in C++ and specialised\nonly for GPUs; ODEINT implemented in C++, which supports execution on both CPUs\nand GPUs; finally, DifferentialEquations.jl written in Julia that also supports\nexecution on both CPUs and GPUs. Using GPUs, the program package MPGOS is\nsuperior. For CPU computations, the ODEINT program package has the best\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 09:40:24 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Nagy", "D\u00e1niel", ""], ["Plavecz", "Lambert", ""], ["Heged\u0171s", "Ferenc", ""]]}, {"id": "2011.01850", "submitter": "Neil Lindquist", "authors": "Neil Lindquist, Piotr Luszczek, Jack Dongarra", "title": "Improving the Performance of the GMRES Method using Mixed-Precision\n  Techniques", "comments": "16 pages. In the 17th Smoky Mountains Computational Sciences and\n  Engineering Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The GMRES method is used to solve sparse, non-symmetric systems of linear\nequations arising from many scientific applications. The solver performance\nwithin a single node is memory bound, due to the low arithmetic intensity of\nits computational kernels. To reduce the amount of data movement, and thus, to\nimprove performance, we investigated the effect of using a mix of single and\ndouble precision while retaining double-precision accuracy. Previous efforts\nhave explored reduced precision in the preconditioner, but the use of reduced\nprecision in the solver itself has received limited attention. We found that\nGMRES only needs double precision in computing the residual and updating the\napproximate solution to achieve double-precision accuracy, although it must\nrestart after each improvement of single-precision accuracy. This finding holds\nfor the tested orthogonalization schemes: Modified Gram-Schmidt (MGS) and\nClassical Gram-Schmidt with Re-orthogonalization (CGSR). Furthermore, our\nmixed-precision GMRES, when restarted at least once, performed 19% and 24%\nfaster on average than double-precision GMRES for MGS and CGSR, respectively.\nOur implementation uses generic programming techniques to ease the burden of\ncoding implementations for different data types. Our use of the Kokkos library\nallowed us to exploit parallelism and optimize data management. Additionally,\nKokkosKernels was used when producing performance results. In conclusion, using\na mix of single and double precision in GMRES can improve performance while\nretaining double-precision accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 17:12:35 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Lindquist", "Neil", ""], ["Luszczek", "Piotr", ""], ["Dongarra", "Jack", ""]]}, {"id": "2011.03977", "submitter": "Vasileios Gkolemis", "authors": "Vasileios Gkolemis, Michael Gutmann", "title": "Extending the statistical software package Engine for Likelihood-Free\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference is a principled framework for dealing with uncertainty.\nThe practitioner can perform an initial assumption for the physical phenomenon\nthey want to model (prior belief), collect some data and then adjust the\ninitial assumption in the light of the new evidence (posterior belief).\nApproximate Bayesian Computation (ABC) methods, also known as likelihood-free\ninference techniques, are a class of models used for performing inference when\nthe likelihood is intractable. The unique requirement of these models is a\nblack-box sampling machine. Due to the modelling-freedom they provide these\napproaches are particularly captivating. Robust Optimisation Monte Carlo (ROMC)\nis one of the most recent techniques of the specific domain. It approximates\nthe posterior distribution by solving independent optimisation problems. This\ndissertation focuses on the implementation of the ROMC method in the software\npackage Engine for Likelihood-Free Inference (ELFI). In the first chapters, we\nprovide the mathematical formulation and the algorithmic description of the\nROMC approach. In the following chapters, we describe our implementation; (a)\nwe present all the functionalities provided to the user and (b) we demonstrate\nhow to perform inference on some real examples. Our implementation provides a\nrobust and efficient solution to a practitioner who wants to perform inference\non a simulator-based model. Furthermore, it exploits parallel processing for\naccelerating the inference wherever it is possible. Finally, it has been\ndesigned to serve extensibility; the user can easily replace specific subparts\nof the method without significant overhead on the development side. Therefore,\nit can be used by a researcher for further experimentation.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 13:22:37 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Gkolemis", "Vasileios", ""], ["Gutmann", "Michael", ""]]}, {"id": "2011.04216", "submitter": "Amit Sharma", "authors": "Amit Sharma, Emre Kiciman", "title": "DoWhy: An End-to-End Library for Causal Inference", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.MS econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In addition to efficient statistical estimators of a treatment's effect,\nsuccessful application of causal inference requires specifying assumptions\nabout the mechanisms underlying observed data and testing whether they are\nvalid, and to what extent. However, most libraries for causal inference focus\nonly on the task of providing powerful statistical estimators. We describe\nDoWhy, an open-source Python library that is built with causal assumptions as\nits first-class citizens, based on the formal framework of causal graphs to\nspecify and test causal assumptions. DoWhy presents an API for the four steps\ncommon to any causal analysis---1) modeling the data using a causal graph and\nstructural assumptions, 2) identifying whether the desired effect is estimable\nunder the causal model, 3) estimating the effect using statistical estimators,\nand finally 4) refuting the obtained estimate through robustness checks and\nsensitivity analyses. In particular, DoWhy implements a number of robustness\nchecks including placebo tests, bootstrap tests, and tests for unoberved\nconfounding. DoWhy is an extensible library that supports interoperability with\nother implementations, such as EconML and CausalML for the the estimation step.\nThe library is available at https://github.com/microsoft/dowhy\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 06:22:11 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Sharma", "Amit", ""], ["Kiciman", "Emre", ""]]}, {"id": "2011.07119", "submitter": "Nicola Bastianello", "authors": "Nicola Bastianello", "title": "tvopt: A Python Framework for Time-Varying Optimization", "comments": "Code available here: https://github.com/nicola-bastianello/tvopt", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces tvopt, a Python framework for prototyping and\nbenchmarking time-varying (or online) optimization algorithms. The paper first\ndescribes the theoretical approach that informed the development of tvopt. Then\nit discusses the different components of the framework and their use for\nmodeling and solving time-varying optimization problems. In particular, tvopt\nprovides functionalities for defining both centralized and distributed online\nproblems, and a collection of built-in algorithms to solve them, for example\ngradient-based methods, ADMM and other splitting methods. Moreover, the\nframework implements prediction strategies to improve the accuracy of the\nonline solvers. The paper then proposes some numerical results on a benchmark\nproblem and discusses their implementation using tvopt. The code for tvopt is\navailable at https://github.com/nicola-bastianello/tvopt.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 16:14:09 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Bastianello", "Nicola", ""]]}, {"id": "2011.07769", "submitter": "Chao Chen", "authors": "Chao Chen, Tianyu Liang, George Biros", "title": "RCHOL: Randomized Cholesky Factorization for Solving SDD Linear Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a randomized algorithm, namely RCHOL, to construct an\napproximate Cholesky factorization for a given Laplacian matrix (a.k.a., graph\nLaplacian). From a graph perspective, the exact Cholesky factorization\nintroduces a clique in the underlying graph after eliminating a row/column. By\nrandomization, RCHOL only retains a sparse subset of the edges in the clique\nusing a random sampling developed by Spielman and Kyng. We prove RCHOL is\nbreakdown-free and apply it to solving large sparse linear systems with\nsymmetric diagonally dominant matrices. In addition, we parallelize RCHOL based\non the nested dissection ordering for shared-memory machines. We report\nnumerical experiments that demonstrate the robustness and the scalability of\nRCHOL. For example, our parallel code scaled up to 64 threads on a single node\nfor solving the 3D Poisson equation, discretized with the 7-point stencil on a\n$1024\\times 1024 \\times 1024$ grid, a problem that has one billion unknowns.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 08:08:05 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 09:18:33 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 17:09:35 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Chen", "Chao", ""], ["Liang", "Tianyu", ""], ["Biros", "George", ""]]}, {"id": "2011.07919", "submitter": "Tom Gustafsson", "authors": "Tom Gustafsson", "title": "A simple technique for unstructured mesh generation via adaptive finite\n  elements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes a concise algorithm for the generation of triangular\nmeshes with the help of standard adaptive finite element methods. We\ndemonstrate that a generic adaptive finite element solver can be repurposed\ninto a triangular mesh generator if a robust mesh smoothing algorithm is\napplied between the mesh refinement steps. We present an implementation of the\nmesh generator and demonstrate the resulting meshes via examples.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 13:12:33 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 13:59:46 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Gustafsson", "Tom", ""]]}, {"id": "2011.07963", "submitter": "Marcel Van De Vel", "authors": "Marcel Van de Vel", "title": "Combining the Mersenne Twister and the Xorgens Designs", "comments": "8 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.MS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine the design of two \\emph{random number generators}, \\emph{Mersenne\nTwister} and \\emph{Xorgens}, to obtain a new class of generators with\nheavy-weight characteristic polynomials (exceeded only by the {\\sc well}\ngenerators) and high speed (comparable with the originals). Tables with\nparameter combinations are included for state sizes ranging from 521 to 44497\nbits and each of the word lengths 32, 64, 128. These generators passed all\ntests of the \\emph{TestU01}-package for each 32-bit integer part and each\n64-bit derived real part of the output. We determine \\emph{dimension gaps} for\n32-bit words, neglecting the non-linear tempering, and compare with an\nalternative experimental linear tempering.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 17:50:47 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Van de Vel", "Marcel", ""]]}, {"id": "2011.08126", "submitter": "Sonja Petrovic", "authors": "Sonja Petrovi\\'c and Shahrzad Jamshidi Zelenberg", "title": "Threaded Gr\\\"{o}bner Bases: a Macaulay2 package", "comments": "5 pages, package in revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AC cs.MS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The complexity of Gr\\\"{o}bner computations has inspired many improvements to\nBuchberger's algorithm over the years. Looking for further insights into the\nalgorithm's performance, we offer a threaded implementation of classical\nBuchberger's algorithm in {\\it Macaulay2}. The output of the main function of\nthe package includes information about {\\it lineages} of non-zero remainders\nthat are added to the basis during the computation. This information can be\nused for further algorithm improvements and optimization.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 17:46:50 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 20:54:38 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Petrovi\u0107", "Sonja", ""], ["Zelenberg", "Shahrzad Jamshidi", ""]]}, {"id": "2011.08461", "submitter": "Andrei Nicolae Ph.D", "authors": "Andrei Nicolae", "title": "Deep Learning Framework From Scratch Using Numpy", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is a rigorous development of a complete and general-purpose deep\nlearning framework from the ground up. The fundamental components of deep\nlearning - automatic differentiation and gradient methods of optimizing\nmultivariable scalar functions - are developed from elementary calculus and\nimplemented in a sensible object-oriented approach using only Python and the\nNumpy library. Demonstrations of solved problems using the framework, named\nArrayFlow, include a computer vision classification task, solving for the shape\nof a catenary, and a 2nd order differential equation.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 06:28:05 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Nicolae", "Andrei", ""]]}, {"id": "2011.08879", "submitter": "Terry Cojean", "authors": "Terry Cojean, Yu-Hsiang \"Mike\" Tsai, Hartwig Anzt", "title": "Ginkgo -- A Math Library designed for Platform Portability", "comments": "Submitted to Parallel Computing Journal (PARCO)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first associations to software sustainability might be the existence of a\ncontinuous integration (CI) framework; the existence of a testing framework\ncomposed of unit tests, integration tests, and end-to-end tests; and also the\nexistence of software documentation. However, when asking what is a common\ndeathblow for a scientific software product, it is often the lack of platform\nand performance portability. Against this background, we designed the Ginkgo\nlibrary with the primary focus on platform portability and the ability to not\nonly port to new hardware architectures, but also achieve good performance. In\nthis paper we present the Ginkgo library design, radically separating\nalgorithms from hardware-specific kernels forming the distinct hardware\nexecutors, and report our experience when adding execution backends for NVIDIA,\nAMD, and Intel GPUs. We also comment on the different levels of performance\nportability, and the performance we achieved on the distinct hardware backends.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 19:10:12 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Cojean", "Terry", ""], ["Tsai", "Yu-Hsiang \"Mike\"", ""], ["Anzt", "Hartwig", ""]]}, {"id": "2011.10073", "submitter": "David Gardner", "authors": "David J. Gardner, Daniel R. Reynolds, Carol S. Woodward, Cody J. Balos", "title": "Enabling New Flexibility in the SUNDIALS Suite of Nonlinear and\n  Differential/Algebraic Equation Solvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the SUite of Nonlinear and DIfferential/ALgebraic equation\nSolvers (SUNDIALS) has been redesigned to better enable the use of\napplication-specific and third-party algebraic solvers and data structures.\nThroughout this work, we have adhered to specific guiding principles that\nminimized the impact to current users while providing maximum flexibility for\nlater evolution of solvers and data structures. The redesign was done through\ncreation of new classes for linear and nonlinear solvers, enhancements to the\nvector class, and the creation of modern Fortran interfaces that leverage\ninteroperability features of the Fortran 2003 standard. The vast majority of\nthis work has been performed \"behind-the-scenes,\" with minimal changes to the\nuser interface and no reduction in solver capabilities or performance. However,\nthese changes now allow advanced users to create highly customized solvers that\nexploit their problem structure, enabling SUNDIALS use on extreme-scale,\nheterogeneous computational architectures.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 19:13:38 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Gardner", "David J.", ""], ["Reynolds", "Daniel R.", ""], ["Woodward", "Carol S.", ""], ["Balos", "Cody J.", ""]]}, {"id": "2011.10214", "submitter": "Xu Zhang", "authors": "Daoru Han, Xiaoming He, David Lund, Xu Zhang", "title": "PIFE-PIC: Parallel Immersed-Finite-Element Particle-In-Cell For 3-D\n  Kinetic Simulations of Plasma-Material Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a recently developed particle simulation code package\nPIFE-PIC, which is a novel three-dimensional (3-D) Parallel\nImmersed-Finite-Element (IFE) Particle-in-Cell (PIC) simulation model for\nparticle simulations of plasma-material interactions. This framework is based\non the recently developed non-homogeneous electrostatic IFE-PIC algorithm,\nwhich is designed to handle complex plasma-material interface conditions\nassociated with irregular geometries using a Cartesian-mesh-based PIC.\nThree-dimensional domain decomposition is utilized for both the electrostatic\nfield solver with IFE and the particle operations in PIC to distribute the\ncomputation among multiple processors. A simulation of the\norbital-motion-limited (OML) sheath of a dielectric sphere immersed in a\nstationary plasma is carried out to validate PIFE-PIC and profile the parallel\nperformance of the code package. Furthermore, a large-scale simulation of\nplasma charging at a lunar crater containing 2 million PIC cells (10 million\nFE/IFE cells) and about 520 million particles, running for 20,000 PIC steps in\nabout 109 wall-clock hours, is presented to demonstrate the high-performance\ncomputing capability of PIFE-PIC.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 04:53:16 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Han", "Daoru", ""], ["He", "Xiaoming", ""], ["Lund", "David", ""], ["Zhang", "Xu", ""]]}, {"id": "2011.10570", "submitter": "Milinda Fernando", "authors": "Milinda Fernando and Hari Sundar", "title": "Scalable Local Timestepping on Octree Grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Numerical solutions of hyperbolic partial differential equations(PDEs) are\nubiquitous in science and engineering. Method of lines is a popular approach to\ndiscretize PDEs defined in spacetime, where space and time are discretized\nindependently. When using explicit timesteppers on adaptive grids, the use of a\nglobal timestep-size dictated by the finest grid-spacing leads to\ninefficiencies in the coarser regions. Even though adaptive space\ndiscretizations are widely used in computational sciences, temporal adaptivity\nis less common due to its sophisticated nature. In this paper, we present\nhighly scalable algorithms to enable local timestepping (LTS) for explicit\ntimestepping schemes on fully adaptive octrees. We demonstrate the accuracy of\nour methods as well as the scalability of our framework across 16K cores in\nTACC's Frontera. We also present a speed up estimation model for LTS, which\npredicts the speedup compared to global timestepping (GTS) with an average of\n0.1 relative error.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 21:44:23 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Fernando", "Milinda", ""], ["Sundar", "Hari", ""]]}, {"id": "2011.10648", "submitter": "Youngsoo Choi", "authors": "Youngkyu Kim, Karen May Wang, Youngsoo Choi", "title": "Efficient space-time reduced order model for linear dynamical systems in\n  Python using less than 120 lines of code", "comments": "24 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical reduced order model (ROM) for dynamical problems typically\ninvolves only the spatial reduction of a given problem. Recently, a novel\nspace-time ROM for linear dynamical problems has been developed, which further\nreduces the problem size by introducing a temporal reduction in addition to a\nspatial reduction without much loss in accuracy. The authors show an order of a\nthousand speed-up with a relative error of less than 0.00001 for a large-scale\nBoltzmann transport problem. In this work, we present for the first time the\nderivation of the space-time Petrov-Galerkin projection for linear dynamical\nsystems and its corresponding block structures. Utilizing these block\nstructures, we demonstrate the ease of construction of the space-time ROM\nmethod with two model problems: 2D diffusion and 2D convection diffusion, with\nand without a linear source term. For each problem, we demonstrate the entire\nprocess of generating the full order model (FOM) data, constructing the\nspace-time ROM, and predicting the reduced-order solutions, all in less than\n120 lines of Python code. We compare our Petrov-Galerkin method with the\ntraditional Galerkin method and show that the space-time ROMs can achieve\nO(100) speed-ups with O(0.001) to O(0.0001) relative errors for these problems.\nFinally, we present an error analysis for the space-time Petrov-Galerkin\nprojection and derive an error bound, which shows an improvement compared to\ntraditional spatial Galerkin ROM methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 21:31:58 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Kim", "Youngkyu", ""], ["Wang", "Karen May", ""], ["Choi", "Youngsoo", ""]]}, {"id": "2011.11430", "submitter": "Ta-Chu Kao", "authors": "Ta-Chu Kao and Guillaume Hennequin", "title": "Automatic differentiation of Sylvester, Lyapunov, and algebraic Riccati\n  equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sylvester, Lyapunov, and algebraic Riccati equations are the bread and butter\nof control theorists. They are used to compute infinite-horizon Gramians, solve\noptimal control problems in continuous or discrete time, and design observers.\nWhile popular numerical computing frameworks (e.g., scipy) provide efficient\nsolvers for these equations, these solvers are still largely missing from most\nautomatic differentiation libraries. Here, we derive the forward and\nreverse-mode derivatives of the solutions to all three types of equations, and\nshowcase their application on an inverse control problem.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 14:33:31 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 10:53:43 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Kao", "Ta-Chu", ""], ["Hennequin", "Guillaume", ""]]}, {"id": "2011.11762", "submitter": "Emanuel Rubensson", "authors": "Emanuel H. Rubensson, Elias Rudberg, Anastasia Kruchinina, Anton G.\n  Artemov", "title": "The Chunks and Tasks Matrix Library 2.0", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a C++ header-only parallel sparse matrix library, based on sparse\nquadtree representation of matrices using the Chunks and Tasks programming\nmodel. The library implements a number of sparse matrix algorithms for\ndistributed memory parallelization that are able to dynamically exploit data\nlocality to avoid movement of data. This is demonstrated for the example of\nblock-sparse matrix-matrix multiplication applied to three sequences of\nmatrices with different nonzero structure, using the CHT-MPI 2.0 runtime\nlibrary implementation of the Chunks and Tasks model. The runtime library\nsucceeds to dynamically load balance the calculation regardless of the sparsity\nstructure.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 22:04:50 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Rubensson", "Emanuel H.", ""], ["Rudberg", "Elias", ""], ["Kruchinina", "Anastasia", ""], ["Artemov", "Anton G.", ""]]}, {"id": "2011.12984", "submitter": "Cody Balos", "authors": "Cody J. Balos and David J. Gardner and Carol S. Woodward and Daniel R.\n  Reynolds", "title": "Enabling GPU Accelerated Computing in the SUNDIALS Time Integration\n  Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As part of the Exascale Computing Project (ECP), a recent focus of\ndevelopment efforts for the SUite of Nonlinear and DIfferential/ALgebraic\nequation Solvers (SUNDIALS) has been to enable GPU-accelerated time integration\nin scientific applications at extreme scales. This effort has resulted in\nseveral new GPU-enabled implementations of core SUNDIALS data structures,\nsupport for programming paradigms which are aware of the heterogeneous\narchitectures, and the introduction of utilities to provide new points of\nflexibility. In this paper, we discuss our considerations, both internal and\nexternal, when designing these new features and present the features\nthemselves. We also present performance results for several of the features on\nthe Summit supercomputer and early access hardware for the Frontier\nsupercomputer, which demonstrate negligible performance overhead resulting from\nthe additional infrastructure and significant speedups when using both NVIDIA\nand AMD GPUs.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 19:09:12 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Balos", "Cody J.", ""], ["Gardner", "David J.", ""], ["Woodward", "Carol S.", ""], ["Reynolds", "Daniel R.", ""]]}, {"id": "2011.14616", "submitter": "Andre Greiner-Petter", "authors": "Andr\\'e Greiner-Petter", "title": "Automatic Mathematical Information Retrieval to Perform Translations up\n  to Computer Algebra Systems", "comments": "Doctoral Consortium Paper at the Joint Conference on Digital\n  Libraries (JCDL), Fort Worth, TX, USA, June 03-07, 2018", "journal-ref": "Bulletin of IEEE Technical Committee on Digital Libraries 15.1\n  (Jan. 2019)", "doi": null, "report-no": null, "categories": "cs.IR cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In mathematics, LaTeX is the de facto standard to prepare documents, e.g.,\nscientific publications. While some formulae are still developed using pen and\npaper, more complicated mathematical expressions used more and more often with\ncomputer algebra systems. Mathematical expressions are often manually\ntranscribed to computer algebra systems. The goal of my doctoral thesis is to\nimprove the efficiency of this workflow. My envisioned method will\nautomatically semantically enrich mathematical expressions so that they can be\nimported to computer algebra systems and other systems that can take advantage\nof the semantics, such as search engines or automatic plagiarism detection\nsystems. These imports should preserve the essential semantic features of the\nexpression.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 08:36:58 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Greiner-Petter", "Andr\u00e9", ""]]}]