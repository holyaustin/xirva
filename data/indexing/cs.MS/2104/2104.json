[{"id": "2104.00237", "submitter": "Zixuan Jiang", "authors": "Zixuan Jiang, Jiaqi Gu, Mingjie Liu, Keren Zhu, David Z. Pan", "title": "Optimizer Fusion: Efficient Training with Better Locality and\n  Parallelism", "comments": "It is published as a paper at the Hardware Aware Efficient Training\n  (HAET) workshop of ICLR 2021. There are 4 pages excluding references and\n  appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning frameworks adopt iterative optimizers to train neural\nnetworks. Conventional eager execution separates the updating of trainable\nparameters from forward and backward computations. However, this approach\nintroduces nontrivial training time overhead due to the lack of data locality\nand computation parallelism. In this work, we propose to fuse the optimizer\nwith forward or backward computation to better leverage locality and\nparallelism during training. By reordering the forward computation, gradient\ncalculation, and parameter updating, our proposed method improves the\nefficiency of iterative optimizers. Experimental results demonstrate that we\ncan achieve an up to 20% training time reduction on various configurations.\nSince our methods do not alter the optimizer algorithm, they can be used as a\ngeneral \"plug-in\" technique to the training process.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 03:44:13 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Jiang", "Zixuan", ""], ["Gu", "Jiaqi", ""], ["Liu", "Mingjie", ""], ["Zhu", "Keren", ""], ["Pan", "David Z.", ""]]}, {"id": "2104.00507", "submitter": "Przemyslaw Biecek", "authors": "Jakub Wi\\'sniewski, Przemys{\\l}aw Biecek", "title": "fairmodels: A Flexible Tool For Bias Detection, Visualization, And\n  Mitigation", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning decision systems are getting omnipresent in our lives. From\ndating apps to rating loan seekers, algorithms affect both our well-being and\nfuture. Typically, however, these systems are not infallible. Moreover, complex\npredictive models are really eager to learn social biases present in historical\ndata that can lead to increasing discrimination. If we want to create models\nresponsibly then we need tools for in-depth validation of models also from the\nperspective of potential discrimination. This article introduces an R package\nfairmodels that helps to validate fairness and eliminate bias in classification\nmodels in an easy and flexible fashion. The fairmodels package offers a\nmodel-agnostic approach to bias detection, visualization and mitigation. The\nimplemented set of functions and fairness metrics enables model fairness\nvalidation from different perspectives. The package includes a series of\nmethods for bias mitigation that aim to diminish the discrimination in the\nmodel. The package is designed not only to examine a single model, but also to\nfacilitate comparisons between multiple models.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 15:06:13 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wi\u015bniewski", "Jakub", ""], ["Biecek", "Przemys\u0142aw", ""]]}, {"id": "2104.01196", "submitter": "Stephen Thomas", "authors": "Luc Berger-Vergiat, Brian Kelley, Sivasankaran Rajamanickam, Jonathan\n  Hu, Katarzyna Swirydowicz, Paul Mullowney, Stephen Thomas, Ichitaro Yamazaki", "title": "Two-Stage Gauss--Seidel Preconditioners and Smoothers for Krylov Solvers\n  on a GPU cluster", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gauss-Seidel (GS) relaxation is often employed as a preconditioner for a\nKrylov solver or as a smoother for Algebraic Multigrid (AMG). However, the\nrequisite sparse triangular solve is difficult to parallelize on many-core\narchitectures such as graphics processing units (GPUs). In the present study,\nthe performance of the traditional GS relaxation based on a triangular solve is\ncompared with two-stage variants, replacing the direct triangular solve with a\nfixed number of inner Jacobi-Richardson (JR) iterations. When a small number of\ninner iterations is sufficient to maintain the Krylov convergence rate, the\ntwo-stage GS (GS2) often outperforms the traditional algorithm on many-core\narchitectures. We also compare GS2 with JR. When they perform the same number\nof flops for SpMV (e.g. three JR sweeps compared to two GS sweeps with one\ninner JR sweep), the GS2 iterations, and the Krylov solver preconditioned with\nGS2, may converge faster than the JR iterations. Moreover, for some problems\n(e.g. elasticity), it was found that JR may diverge with a damping factor of\none, whereas two-stage GS may improve the convergence with more inner\niterations. Finally, to study the performance of the two-stage smoother and\npreconditioner for a practical problem, %(e.g. using tuned damping factors),\nthese were applied to incompressible fluid flow simulations on GPUs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 18:49:32 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 14:09:09 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Berger-Vergiat", "Luc", ""], ["Kelley", "Brian", ""], ["Rajamanickam", "Sivasankaran", ""], ["Hu", "Jonathan", ""], ["Swirydowicz", "Katarzyna", ""], ["Mullowney", "Paul", ""], ["Thomas", "Stephen", ""], ["Yamazaki", "Ichitaro", ""]]}, {"id": "2104.01532", "submitter": "Thomas Athey", "authors": "Thomas L. Athey, Jacopo Teneggi, Joshua T. Vogelstein, Daniel Tward,\n  Ulrich Mueller, Michael I. Miller", "title": "Fitting Splines to Axonal Arbors Quantifies Relationship between Branch\n  Order and Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.MS math.DG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neuromorphology is crucial to identifying neuronal subtypes and understanding\nlearning. It is also implicated in neurological disease. However, standard\nmorphological analysis focuses on macroscopic features such as branching\nfrequency and connectivity between regions, and often neglects the internal\ngeometry of neurons. In this work, we treat neuron trace points as a sampling\nof differentiable curves and fit them with a set of branching B-splines. We\ndesigned our representation with the Frenet-Serret formulas from differential\ngeometry in mind. The Frenet-Serret formulas completely characterize smooth\ncurves, and involve two parameters, curvature and torsion. Our representation\nmakes it possible to compute these parameters from neuron traces in closed\nform. These parameters are defined continuously along the curve, in contrast to\nother parameters like tortuosity which depend on start and end points. We\napplied our method to a dataset of cortical projection neurons traced in two\nmouse brains, and found that the parameters are distributed differently between\nprimary, collateral, and terminal axon branches, thus quantifying geometric\ndifferences between different components of an axonal arbor. The results agreed\nin both brains, further validating our representation. The code used in this\nwork can be readily applied to neuron traces in SWC format and is available in\nour open-source Python package brainlit: http://brainlit.neurodata.io/.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 03:38:42 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 15:04:09 GMT"}, {"version": "v3", "created": "Sat, 5 Jun 2021 15:19:43 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Athey", "Thomas L.", ""], ["Teneggi", "Jacopo", ""], ["Vogelstein", "Joshua T.", ""], ["Tward", "Daniel", ""], ["Mueller", "Ulrich", ""], ["Miller", "Michael I.", ""]]}, {"id": "2104.01661", "submitter": "G\\'abor Sz\\'arnyas", "authors": "G\\'abor Sz\\'arnyas, David A. Bader, Timothy A. Davis, James Kitchen,\n  Timothy G. Mattson, Scott McMillan, Erik Welch", "title": "LAGraph: Linear Algebra, Network Analysis Libraries, and the Study of\n  Graph Algorithms", "comments": "Accepted to GrAPL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph algorithms can be expressed in terms of linear algebra. GraphBLAS is a\nlibrary of low-level building blocks for such algorithms that targets algorithm\ndevelopers. LAGraph builds on top of the GraphBLAS to target users of graph\nalgorithms with high-level algorithms common in network analysis. In this\npaper, we describe the first release of the LAGraph library, the design\ndecisions behind the library, and performance using the GAP benchmark suite.\nLAGraph, however, is much more than a library. It is also a project to document\nand analyze the full range of algorithms enabled by the GraphBLAS. To that end,\nwe have developed a compact and intuitive notation for describing these\nalgorithms. In this paper, we present that notation with examples from the GAP\nbenchmark suite.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 18:49:58 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Sz\u00e1rnyas", "G\u00e1bor", ""], ["Bader", "David A.", ""], ["Davis", "Timothy A.", ""], ["Kitchen", "James", ""], ["Mattson", "Timothy G.", ""], ["McMillan", "Scott", ""], ["Welch", "Erik", ""]]}, {"id": "2104.01965", "submitter": "Aaditya Chandrasekhar", "authors": "Aaditya Chandrasekhar, Saketh Sridhara, Krishnan Suresh", "title": "AuTO: A Framework for Automatic differentiation in Topology Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CE cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A critical step in topology optimization (TO) is finding sensitivities.\nManual derivation and implementation of the sensitivities can be quite\nlaborious and error-prone, especially for non-trivial objectives, constraints\nand material models. An alternate approach is to utilize automatic\ndifferentiation (AD). While AD has been around for decades, and has also been\napplied in TO, wider adoption has largely been absent.\n  In this educational paper, we aim to reintroduce AD for TO, and make it\neasily accessible through illustrative codes. In particular, we employ JAX, a\nhigh-performance Python library for automatically computing sensitivities from\na user defined TO problem. The resulting framework, referred to here as AuTO,\nis illustrated through several examples in compliance minimization, compliant\nmechanism design and microstructural design.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 15:36:17 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Chandrasekhar", "Aaditya", ""], ["Sridhara", "Saketh", ""], ["Suresh", "Krishnan", ""]]}, {"id": "2104.02494", "submitter": "Nils-Arne Dreier", "authors": "Nils-Arne Dreier", "title": "Hardware-Oriented Krylov Methods for High-Performance Computing", "comments": "PhD thesis (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.MS cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Krylov subspace methods are an essential building block in numerical\nsimulation software. The efficient utilization of modern hardware is a\nchallenging problem in the development of these methods. In this work, we\ndevelop Krylov subspace methods to solve linear systems with multiple\nright-hand sides, tailored to modern hardware in high-performance computing. To\nthis end, we analyze an innovative block Krylov subspace framework that allows\nto balance the computational and data-transfer costs to the hardware. Based on\nthe framework, we formulate commonly used Krylov methods. For the CG and\nBiCGStab methods, we introduce a novel stabilization approach as an alternative\nto a deflation strategy. This helps us to retain the block size, thus leading\nto a simpler and more efficient implementation. In addition, we optimize the\nmethods further for distributed memory systems and the communication overhead.\nFor the CG method, we analyze approaches to overlap the communication and\ncomputation and present multiple variants of the CG method, which differ in\ntheir communication properties. Furthermore, we present optimizations of the\northogonalization procedure in the GMRes method. Beside introducing a pipelined\nGram-Schmidt variant that overlaps the global communication with the\ncomputation of inner products, we present a novel orthonormalization method\nbased on the TSQR algorithm, which is communication-optimal and stable. For all\noptimized method, we present tests that show their superiority in a distributed\nsetting.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 13:25:22 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Dreier", "Nils-Arne", ""]]}, {"id": "2104.04043", "submitter": "Santosh Nagarakatte", "authors": "Jay P. Lim and Santosh Nagarakatte", "title": "RLIBM-32: High Performance Correctly Rounded Math Libraries for 32-bit\n  Floating Point Representations", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": "Rutgers Department of Computer Science Technical Report DCS-TR-754", "categories": "cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a set of techniques to develop correctly rounded math\nlibraries for 32-bit float and posit types. It enhances our RLibm approach that\nframes the problem of generating correctly rounded libraries as a linear\nprogramming problem in the context of 16-bit types to scale to 32-bit types.\nSpecifically, this paper proposes new algorithms to (1) generate polynomials\nthat produce correctly rounded outputs for all inputs using counterexample\nguided polynomial generation, (2) generate efficient piecewise polynomials with\nbit-pattern based domain splitting, and (3) deduce the amount of freedom\navailable to produce correct results when range reduction involves multiple\nelementary functions. The resultant math library for the 32-bit float type is\nfaster than state-of-the-art math libraries while producing the correct output\nfor all inputs. We have also developed a set of correctly rounded elementary\nfunctions for 32-bit posits.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 20:37:17 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Lim", "Jay P.", ""], ["Nagarakatte", "Santosh", ""]]}, {"id": "2104.04771", "submitter": "Alberto Gomez", "authors": "Alberto Gomez", "title": "MIPROT: A Medical Image Processing Toolbox for MATLAB", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Matlab toolbox to perform basic image processing and\nvisualization tasks, particularly designed for medical image processing. The\nfunctionalities available are similar to basic functions found in other\nnon-Matlab widely used libraries such as the Insight Toolkit (ITK). The toolbox\nis entirely written in native Matlab code, but is fast and flexible.\n  Main use cases for the toolbox are illustrated here, including image\ninput/output, pre-processing, filtering, image registration and visualisation.\nBoth the code and sample data are made publicly available and open source.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 13:56:39 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Gomez", "Alberto", ""]]}, {"id": "2104.05782", "submitter": "Gregorio Quintana-Ort\\'i", "authors": "N. Heavner, F. D. Igual, G. Quintana-Ort\\'i, P.G. Martinsson", "title": "Efficient algorithms for computing a rank-revealing UTV factorization on\n  parallel computing architectures", "comments": "31 pages and 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The randomized singular value decomposition (RSVD) is by now a well\nestablished technique for efficiently computing an approximate singular value\ndecomposition of a matrix. Building on the ideas that underpin the RSVD, the\nrecently proposed algorithm \"randUTV\" computes a FULL factorization of a given\nmatrix that provides low-rank approximations with near-optimal error. Because\nthe bulk of randUTV is cast in terms of communication-efficient operations like\nmatrix-matrix multiplication and unpivoted QR factorizations, it is faster than\ncompeting rank-revealing factorization methods like column pivoted QR in most\nhigh performance computational settings. In this article, optimized randUTV\nimplementations are presented for both shared memory and distributed memory\ncomputing environments. For shared memory, randUTV is redesigned in terms of an\n\"algorithm-by-blocks\" that, together with a runtime task scheduler, eliminates\nbottlenecks from data synchronization points to achieve acceleration over the\nstandard \"blocked algorithm\", based on a purely fork-join approach. The\ndistributed memory implementation is based on the ScaLAPACK library. The\nperformances of our new codes compare favorably with competing factorizations\navailable on both shared memory and distributed memory architectures.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 19:15:36 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Heavner", "N.", ""], ["Igual", "F. D.", ""], ["Quintana-Ort\u00ed", "G.", ""], ["Martinsson", "P. G.", ""]]}, {"id": "2104.05999", "submitter": "Pieter Boom", "authors": "Pieter D. Boom, Ashley Seepujak, Odysseas Kosmas, Lee Margetts and\n  Andrey Jivkov", "title": "Parallelized Discrete Exterior Calculus for Three-Dimensional Elliptic\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A formulation of elliptic boundary value problems is used to develop the\nfirst discrete exterior calculus (DEC) library for massively parallel\ncomputations with 3D domains. This can be used for steady-state analysis of any\nphysical process driven by the gradient of a scalar quantity, e.g. temperature,\nconcentration, pressure or electric potential, and is easily extendable to\ntransient analysis. In addition to offering this library to the community, we\ndemonstrate one important benefit from the DEC formulation: effortless\nintroduction of strong heterogeneities and discontinuities. These are typical\nfor real materials, but challenging for widely used domain discretization\nschemes, such as finite elements. Specifically, we demonstrate the efficiency\nof the method for calculating the evolution of thermal conductivity of a solid\nwith a growing crack population. Future development of the library will deal\nwith transient problems, and more importantly with processes driven by\ngradients of vector quantities.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 08:06:48 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Boom", "Pieter D.", ""], ["Seepujak", "Ashley", ""], ["Kosmas", "Odysseas", ""], ["Margetts", "Lee", ""], ["Jivkov", "Andrey", ""]]}, {"id": "2104.07097", "submitter": "Mario Vazquez Corte MSc", "authors": "Mario Vazquez Corte, Luis V. Montiel", "title": "Novel Matrix Hit and Run for Sampling Polytopes and Its GPU\n  Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.MS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a new Markov Chain Monte Carlo algorithm that\ngenerates a uniform sample over full and non-full dimensional polytopes. This\nalgorithm, termed \"Matrix Hit and Run\" (MHAR), is a modification of the Hit and\nRun framework. For the regime $n^{1+\\frac{1}{3}} \\ll m$, MHAR has a lower\nasymptotic cost per sample in terms of soft-O notation ($\\SO$) than do existing\nsampling algorithms after a \\textit{warm start}. MHAR is designed to take\nadvantage of matrix multiplication routines that require less computational and\nmemory resources. Our tests show this implementation to be substantially faster\nthan the \\textit{hitandrun} R package, especially for higher dimensions.\nFinally, we provide a python library based on Pytorch and a Colab notebook with\nthe implementation ready for deployment in architectures with GPU or just CPU.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 19:55:04 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Corte", "Mario Vazquez", ""], ["Montiel", "Luis V.", ""]]}, {"id": "2104.07406", "submitter": "Julien Siebert", "authors": "Julien Siebert, Janek Gro{\\ss}, Christof Schroth", "title": "A systematic review of Python packages for time series analysis", "comments": "12 pages, 3 figures, 4 tables, accepted to ITISE2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper presents a systematic review of Python packages with a focus on\ntime series analysis. The objective is to provide (1) an overview of the\ndifferent time series analysis tasks and preprocessing methods implemented, and\n(2) an overview of the development characteristics of the packages (e.g.,\ndocumentation, dependencies, and community size). This review is based on a\nsearch of literature databases as well as GitHub repositories. Following the\nfiltering process, 40 packages were analyzed. We classified the packages\naccording to the analysis tasks implemented, the methods related to data\npreparation, and the means for evaluating the results produced (methods and\naccess to evaluation data). We also reviewed documentation aspects, the\nlicenses, the size of the packages' community, and the dependencies used. Among\nother things, our results show that forecasting is by far the most frequently\nimplemented task, that half of the packages provide access to real datasets or\nallow generating synthetic data, and that many packages depend on a few\nlibraries (the most used ones being numpy, scipy and pandas). We hope that this\nreview can help practitioners and researchers navigate the space of Python\npackages dedicated to time series analysis. We will provide an updated list of\nthe reviewed packages online at\nhttps://siebert-julien.github.io/time-series-analysis-python/.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 12:09:54 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 08:24:49 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Siebert", "Julien", ""], ["Gro\u00df", "Janek", ""], ["Schroth", "Christof", ""]]}, {"id": "2104.07651", "submitter": "Lukas Heumos", "authors": "Lukas Heumos, Philipp Ehmele, Kevin Menden, Luis Kuhn Cuellar, Edmund\n  Miller, Steffen Lemke, Gisela Gabernet and Sven Nahnsen", "title": "mlf-core: a framework for deterministic machine learning", "comments": "https://mlf-core.com and https://github.com/mlf-core/mlf-core", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG q-bio.QM stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Machine learning has shown extensive growth in recent years. However,\npreviously existing studies highlighted a reproducibility crisis in machine\nlearning. The reasons for irreproducibility are manifold. Major machine\nlearning libraries default to the usage of non-deterministic algorithms based\non atomic operations. Solely fixing all random seeds is not sufficient for\ndeterministic machine learning. To overcome this shortcoming, various machine\nlearning libraries released deterministic counterparts to the non-deterministic\nalgorithms. We evaluated the effect of these algorithms on determinism and\nruntime. Based on these results, we formulated a set of requirements for\nreproducible machine learning and developed a new software solution, the\nmlf-core ecosystem, which aids machine learning projects to meet and keep these\nrequirements. We applied mlf-core to develop fully reproducible models in\nvarious biomedical fields including a single cell autoencoder with TensorFlow,\na PyTorch-based U-Net model for liver-tumor segmentation in CT scans, and a\nliver cancer classifier based on gene expression profiles with XGBoost.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:58:03 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Heumos", "Lukas", ""], ["Ehmele", "Philipp", ""], ["Menden", "Kevin", ""], ["Cuellar", "Luis Kuhn", ""], ["Miller", "Edmund", ""], ["Lemke", "Steffen", ""], ["Gabernet", "Gisela", ""], ["Nahnsen", "Sven", ""]]}, {"id": "2104.08012", "submitter": "Jack Betteridge", "authors": "Jack D. Betteridge, Patrick E. Farrell and David A. Ham", "title": "Code generation for productive portable scalable finite element\n  simulation in Firedrake", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Creating scalable, high performance PDE-based simulations requires a suitable\ncombination of discretizations, differential operators, preconditioners and\nsolvers. The required combination changes with the application and with the\navailable hardware, yet software development time is a severely limited\nresource for most scientists and engineers. Here we demonstrate that generating\nsimulation code from a high-level Python interface provides an effective\nmechanism for creating high performance simulations from very few lines of user\ncode. We demonstrate that moving from one supercomputer to another can require\nsignificant algorithmic changes to achieve scalable performance, but that the\ncode generation approach enables these algorithmic changes to be achieved with\nminimal development effort.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 10:14:54 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Betteridge", "Jack D.", ""], ["Farrell", "Patrick E.", ""], ["Ham", "David A.", ""]]}, {"id": "2104.08416", "submitter": "Roger Ara\\'ujo", "authors": "Roger R. F. Ara\\'ujo, Lutz Gross, Samuel Xavier-de-Souza", "title": "Boosting Memory Access Locality of the Spectral Element Method with\n  Hilbert Space-Filling Curves", "comments": "23 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm based on Hilbert space-filling curves to reorder mesh\nelements in memory for use with the Spectral Element Method, aiming to attain\nfewer cache misses, better locality of data reference and faster execution. We\npresent a technique to numerically simulate acoustic wave propagation in 2D\ndomains using the Spectral Element Method, and discuss computational\nperformance aspects of this procedure. We reorder mesh-related data via Hilbert\ncurves to achieve sizable reductions in execution time under several mesh\nconfigurations in shared-memory systems. Our experiments show that the Hilbert\ncurve approach works well with meshes of several granularities and also with\nsmall and large variations in element sizes, achieving reductions between 9%\nand 25% in execution time when compared to three other ordering schemes.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 01:27:38 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ara\u00fajo", "Roger R. F.", ""], ["Gross", "Lutz", ""], ["Xavier-de-Souza", "Samuel", ""]]}, {"id": "2104.11120", "submitter": "Conrad Sanderson", "authors": "Jason Rumengan, Terry Yue Zhuo, Conrad Sanderson", "title": "PyArmadillo: a streamlined linear algebra library for Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the Python language has extension frameworks such as NumPy and SciPy\nfor providing functionality involving numerical arrays and scientific\ncomputing, these frameworks can be cumbersome to use in contrast to the\nwell-established Matlab language. The issues include: a focus on operations\ninvolving multi-dimensional arrays rather than matrices, nested organisation of\nfunctions which increases code verbosity (leading to reduced user\nproductivity), and syntax that significantly differs from Matlab. To address\nthese shortcomings, we propose PyArmadillo, a streamlined linear algebra\nlibrary for Python, with an emphasis on ease of use. PyArmadillo aims to\nprovide a high-level syntax and functionality deliberately similar to\nMatlab/Octave, allowing mathematical operations to be expressed in a familiar\nand natural manner. Objects for matrices and cubes are provided, as well as\nover 200 associated functions for manipulating data stored in the objects.\nInteger, floating point and complex numbers are supported. Various matrix\nfactorisations are provided through integration with LAPACK, or one of its high\nperformance drop-in replacements such as Intel MKL or OpenBLAS. PyArmadillo is\nopen-source software, distributed under the Apache 2.0 license; it can be\nobtained at https://pyarma.sourceforge.io or via the Python Package Index in\nprecompiled form.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 15:13:33 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 02:13:16 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Rumengan", "Jason", ""], ["Zhuo", "Terry Yue", ""], ["Sanderson", "Conrad", ""]]}, {"id": "2104.11627", "submitter": "S\\'ebastien Le Digabel", "authors": "Charles Audet, S\\'ebastien Le Digabel, Viviane Rochon Montplaisir,\n  Christophe Tribes", "title": "NOMAD version 4: Nonlinear optimization with the MADS algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": "Technical Report, Les Cahiers du GERAD G-2021-23", "categories": "math.OC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NOMAD is software for optimizing blackbox problems. In continuous development\nsince 2001, it constantly evolved with the integration of new algorithmic\nfeatures published in scientific publications. These features are motivated by\nreal applications encountered by industrial partners. The latest major release\nof NOMAD, version 3, dates from 2008. Minor releases are produced as new\nfeatures are incorporated. The present work describes NOMAD 4, a complete\nredesign of the previous version, with a new architecture providing more\nflexible code, added functionalities and reusable code. We introduce\nalgorithmic components, which are building blocks for more complex algorithms,\nand can initiate other components, launch nested algorithms, or perform\nspecialized tasks. They facilitate the implementation of new ideas, including\nthe MegaSearchPoll component, warm and hot restarts, and a revised version of\nthe PSD-MADS algorithm. Another main improvement of NOMAD 4 is the usage of\nparallelism, to simultaneously compute multiple blackbox evaluations, and to\nmaximize usage of available cores. Running different algorithms, tuning their\nparameters, and comparing their performance for optimization is simpler than\nbefore, while overall optimization performance is maintained between versions 3\nand 4. NOMAD is freely available at www.gerad.ca/nomad and the whole project is\nvisible at github.com/bbopt/nomad.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 14:28:57 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 01:13:51 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Audet", "Charles", ""], ["Digabel", "S\u00e9bastien Le", ""], ["Montplaisir", "Viviane Rochon", ""], ["Tribes", "Christophe", ""]]}, {"id": "2104.12657", "submitter": "Micha{\\l} Narajewski", "authors": "Micha{\\l} Narajewski, Jens Kley-Holsteg, Florian Ziel", "title": "tsrobprep -- an R package for robust preprocessing of time series data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MS stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Data cleaning is a crucial part of every data analysis exercise. Yet, the\ncurrently available R packages do not provide fast and robust methods for\ncleaning and preparation of time series data. The open source package tsrobprep\nintroduces efficient methods for handling missing values and outliers using\nmodel based approaches. For data imputation a probabilistic replacement model\nis proposed, which may consist of autoregressive components and external\ninputs. For outlier detection a clustering algorithm based on finite mixture\nmodelling is introduced, which considers typical time series related properties\nas features. By assigning to each observation a probability of being an\noutlying data point, the degree of outlyingness can be determined. The methods\nwork robust and are fully tunable. Moreover, by providing the\nauto_data_cleaning function the data preprocessing can be carried out in one\ncast, without manual tuning and providing suitable results. The primary\nmotivation of the package is the preprocessing of energy system data, however,\nthe package is also suited for other moderate and large sized time series data\nset. We present application for electricity load, wind and solar power data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 15:35:11 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Narajewski", "Micha\u0142", ""], ["Kley-Holsteg", "Jens", ""], ["Ziel", "Florian", ""]]}, {"id": "2104.12986", "submitter": "Justin Crum", "authors": "Justin Crum, Cyrus Cheng, David A. Ham, Lawrence Mitchell, Robert C.\n  Kirby, Joshua A. Levine, Andrew Gillette", "title": "Bringing Trimmed Serendipity Methods to Computational Practice in\n  Firedrake", "comments": "19 pages, 7 figures, 3 tables, 2 listings", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA math.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an implementation of the trimmed serendipity finite element\nfamily, using the open source finite element package Firedrake. The new\nelements can be used seamlessly within the software suite for problems\nrequiring $H^1$, \\hcurl, or \\hdiv-conforming elements on meshes of squares or\ncubes. To test how well trimmed serendipity elements perform in comparison to\ntraditional tensor product elements, we perform a sequence of numerical\nexperiments including the primal Poisson, mixed Poisson, and Maxwell cavity\neigenvalue problems. Overall, we find that the trimmed serendipity elements\nconverge, as expected, at the same rate as the respective tensor product\nelements while being able to offer significant savings in the time or memory\nrequired to solve certain problems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 05:25:02 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Crum", "Justin", ""], ["Cheng", "Cyrus", ""], ["Ham", "David A.", ""], ["Mitchell", "Lawrence", ""], ["Kirby", "Robert C.", ""], ["Levine", "Joshua A.", ""], ["Gillette", "Andrew", ""]]}, {"id": "2104.14186", "submitter": "Hatem Ltaief", "authors": "D. Keyes, H. Ltaief, Y. Nakatsukasa, and D. Sukkari", "title": "High-Performance Partial Spectrum Computation for Symmetric eigenvalue\n  problems and the SVD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current dense symmetric eigenvalue (EIG) and singular value decomposition\n(SVD) implementations may suffer from the lack of concurrency during the\ntridiagonal and bidiagonal reductions, respectively. This performance\nbottleneck is typical for the two-sided transformations due to the Level-2 BLAS\nmemory-bound calls. Therefore, the current state-of-the-art EIG and SVD\nimplementations may achieve only a small fraction of the system's sustained\npeak performance. The QR-based Dynamically Weighted Halley (QDWH) algorithm may\nbe used as a pre-processing step toward the EIG and SVD solvers, while\nmitigating the aforementioned bottleneck. QDWH-EIG and QDWH-SVD expose more\nparallelism, while relying on compute-bound matrix operations. Both run closer\nto the sustained peak performance of the system, but at the expense of\nperforming more FLOPS than the standard EIG and SVD algorithms. In this paper,\nwe introduce a new QDWH-based solver for computing the partial spectrum for EIG\n(QDWHpartial-EIG) and SVD (QDWHpartial-SVD) problems. By optimizing the\nrational function underlying the algorithms only in the desired part of the\nspectrum, QDWHpartial-EIG and QDWHpartial-SVD algorithms efficiently compute a\nfraction (say 1-20%) of the corresponding spectrum. We develop high-performance\nimplementations of QDWHpartial-EIG and QDWHpartial-SVD on distributed-memory\nanymore systems and demonstrate their numerical robustness. Experimental\nresults using up to 36K MPI processes show performance speedups for\nQDWHpartial-SVD up to 6X and 2X against PDGESVD from ScaLAPACK and KSVD,\nrespectively. QDWHpartial-EIG outperforms PDSYEVD from ScaLAPACK up to 3.5X but\nremains slower compared to ELPA. QDWHpartial-EIG achieves, however, a better\noccupancy of the underlying hardware by extracting higher sustained peak\nperformance than ELPA, which is critical moving forward with accelerator-based\nsupercomputers.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 08:04:23 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Keyes", "D.", ""], ["Ltaief", "H.", ""], ["Nakatsukasa", "Y.", ""], ["Sukkari", "D.", ""]]}, {"id": "2104.14447", "submitter": "Quang-Thinh Ha", "authors": "Quang-Thinh Ha, Paul A. Kuberry, Nathaniel A. Trask, Emily M. Ryan", "title": "Parallel implementation of a compatible high-order meshless method for\n  the Stokes' equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DC cs.MS cs.NA cs.PF math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A parallel implementation of a compatible discretization scheme for\nsteady-state Stokes problems is presented in this work. The scheme uses\ngeneralized moving least squares to generate differential operators and apply\nboundary conditions. This meshless scheme allows a high-order convergence for\nboth the velocity and pressure, while also incorporates finite-difference-like\nsparse discretization. Additionally, the method is inherently scalable: the\nstencil generation process requires local inversion of matrices amenable to GPU\nacceleration, and the divergence-free treatment of velocity replaces the\ntraditional saddle point structure of the global system with elliptic diagonal\nblocks amenable to algebraic multigrid. The implementation in this work uses a\nvariety of Trilinos packages to exploit this local and global parallelism, and\nbenchmarks demonstrating high-order convergence and weak scalability are\nprovided.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 16:06:13 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Ha", "Quang-Thinh", ""], ["Kuberry", "Paul A.", ""], ["Trask", "Nathaniel A.", ""], ["Ryan", "Emily M.", ""]]}]