[{"id": "2102.00104", "submitter": "Melven R\\\"ohrig-Z\\\"ollner", "authors": "Melven R\\\"ohrig-Z\\\"ollner and Jonas Thies and Achim Basermann", "title": "Performance of low-rank approximations in tensor train format (TT-SVD)\n  for large dense tensors", "comments": "27 pages, 11 figures, submitted to SISC", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are several factorizations of multi-dimensional tensors into\nlower-dimensional components, known as `tensor networks'. We consider the\npopular `tensor-train' (TT) format and ask, how efficiently can we compute a\nlow-rank approximation from a full tensor on current multi-core CPUs.\n  Compared to sparse and dense linear algebra, there are much fewer and less\nextensive well-optimized kernel libraries for multi-linear algebra. Linear\nalgebra libraries like BLAS and LAPACK may provide the required operations in\nprinciple, but often at the cost of additional data movements for rearranging\nmemory layouts. Furthermore, these libraries are typically optimized for the\ncompute-bound case (e.g.\\ square matrix operations) whereas low-rank tensor\ndecompositions lead to memory bandwidth limited operations.\n  We propose a `tensor-train singular value decomposition' (TT-SVD) algorithm\nbased on two building blocks: a `Q-less tall-skinny QR' factorization, and a\nfused tall-skinny matrix-matrix multiplication and reshape operation. We\nanalyze the performance of the resulting TT-SVD algorithm using the Roofline\nperformance model. In addition, we present performance results for different\nalgorithmic variants for shared-memory as well as distributed-memory\narchitectures. Our experiments show that commonly used TT-SVD implementations\nsuffer severe performance penalties. We conclude that a dedicated library for\ntensor factorization kernels would benefit the community: Computing a low-rank\napproximation can be as cheap as reading the data twice from main memory. As a\nconsequence, an implementation that achieves realistic performance will move\nthe limit at which one has to resort to randomized methods that only process\npart of the data.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 22:56:55 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["R\u00f6hrig-Z\u00f6llner", "Melven", ""], ["Thies", "Jonas", ""], ["Basermann", "Achim", ""]]}, {"id": "2102.03681", "submitter": "James Yang", "authors": "James Yang", "title": "FastAD: Expression Template-Based C++ Library for Fast and\n  Memory-Efficient Automatic Differentiation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic differentiation is a set of techniques to efficiently and\naccurately compute the derivative of a function represented by a computer\nprogram. Existing C++ libraries for automatic differentiation (e.g. Adept, Stan\nMath Library), however, exhibit large memory consumptions and runtime\nperformance issues. This paper introduces FastAD, a new C++ template library\nfor automatic differentiation, that overcomes all of these challenges in\nexisting libraries by using vectorization, simpler memory management using a\nfully expression-template-based design, and other compile-time optimizations to\nremove some run-time overhead. Benchmarks show that FastAD performs 2-10 times\nfaster than Adept and 2-19 times faster than Stan across various test cases\nincluding a few real-world examples.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 23:17:10 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Yang", "James", ""]]}, {"id": "2102.04560", "submitter": "Jakob Sauer J{\\o}rgensen", "authors": "Jakob S. J{\\o}rgensen, Evelina Ametova, Genoveva Burca, Gemma Fardell,\n  Evangelos Papoutsellis, Edoardo Pasca, Kris Thielemans, Martin Turner, Ryan\n  Warr, William R. B. Lionheart and Philip J. Withers", "title": "Core Imaging Library -- Part I: a versatile Python framework for\n  tomographic imaging", "comments": "22 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the Core Imaging Library (CIL), an open-source Python framework\nfor tomographic imaging with particular emphasis on reconstruction of\nchallenging datasets. Conventional filtered back-projection reconstruction\ntends to be insufficient for highly noisy, incomplete, non-standard or\nmulti-channel data arising for example in dynamic, spectral and in situ\ntomography. CIL provides an extensive modular optimisation framework for\nprototyping reconstruction methods including sparsity and total variation\nregularisation, as well as tools for loading, preprocessing and visualising\ntomographic data. The capabilities of CIL are demonstrated on a synchrotron\nexample dataset and three challenging cases spanning golden-ratio neutron\ntomography, cone-beam X-ray laminography and positron emission tomography.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 22:26:37 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 21:00:21 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["J\u00f8rgensen", "Jakob S.", ""], ["Ametova", "Evelina", ""], ["Burca", "Genoveva", ""], ["Fardell", "Gemma", ""], ["Papoutsellis", "Evangelos", ""], ["Pasca", "Edoardo", ""], ["Thielemans", "Kris", ""], ["Turner", "Martin", ""], ["Warr", "Ryan", ""], ["Lionheart", "William R. B.", ""], ["Withers", "Philip J.", ""]]}, {"id": "2102.05340", "submitter": "Minyoung Kim", "authors": "Minyoung Kim", "title": "On PyTorch Implementation of Density Estimators for von Mises-Fisher and\n  Its Mixture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The von Mises-Fisher (vMF) is a well-known density model for directional\nrandom variables. The recent surge of the deep embedding methodologies for\nhigh-dimensional structured data such as images or texts, aimed at extracting\nsalient directional information, can make the vMF model even more popular. In\nthis article, we will review the vMF model and its mixture, provide detailed\nrecipes of how to train the models, focusing on the maximum likelihood\nestimators, in Python/PyTorch. In particular, implementation of vMF typically\nsuffers from the notorious numerical issue of the Bessel function evaluation in\nthe density normalizer, especially when the dimensionality is high, and we\naddress the issue using the MPMath library that supports arbitrary precision.\nFor the mixture learning, we provide both minibatch-based large-scale SGD\nlearning, as well as the EM algorithm which is a full batch estimator. For each\nestimator/methodology, we test our implementation on some synthetic data, while\nwe also demonstrate the use case in a more realistic scenario of image\nclustering. Our code is publicly available in\nhttps://github.com/minyoungkim21/vmf-lib.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 09:26:56 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Kim", "Minyoung", ""]]}, {"id": "2102.06126", "submitter": "Evangelos Papoutsellis", "authors": "Evangelos Papoutsellis, Evelina Ametova, Claire Delplancke, Gemma\n  Fardell, Jakob S. J{\\o}rgensen, Edoardo Pasca, Martin Turner, Ryan Warr,\n  William R. B. Lionheart, Philip J. Withers", "title": "Core Imaging Library -- Part II: Multichannel reconstruction for dynamic\n  and spectral tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.MS math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The newly developed Core Imaging Library (CIL) is a flexible plug and play\nlibrary for tomographic imaging with a specific focus on iterative\nreconstruction. CIL provides building blocks for tailored regularised\nreconstruction algorithms and explicitly supports multichannel tomographic\ndata. In the first part of this two-part publication, we introduced the\nfundamentals of CIL. This paper focuses on applications of CIL for multichannel\ndata, e.g., dynamic and spectral. We formalise different optimisation problems\nfor colour processing, dynamic and hyperspectral tomography and demonstrate\nCIL's capabilities for designing state of the art reconstruction methods\nthrough case studies and code snapshots.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2021 12:21:34 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 16:30:38 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Papoutsellis", "Evangelos", ""], ["Ametova", "Evelina", ""], ["Delplancke", "Claire", ""], ["Fardell", "Gemma", ""], ["J\u00f8rgensen", "Jakob S.", ""], ["Pasca", "Edoardo", ""], ["Turner", "Martin", ""], ["Warr", "Ryan", ""], ["Lionheart", "William R. B.", ""], ["Withers", "Philip J.", ""]]}, {"id": "2102.06570", "submitter": "Harald Hofst\\\"atter", "authors": "Harald Hofst\\\"atter", "title": "User manual for bch, a program for the fast computation of the\n  Baker-Campbell-Hausdorff and similar series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS math.RA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manual describes bch, an efficient program written in the C programming\nlanguage for the fast computation of the Baker-Campbell-Hausdorff (BCH) and\nsimilar Lie series. The Lie series can be represented in the Lyndon basis, in\nthe classical Hall basis, or in the right-normed basis of E.S. Chibrikov. In\nthe Lyndon basis, which proves to be particularly efficient for this purpose,\nthe computation of 111013 coefficients for the BCH series up to terms of degree\n20 takes less than half a second on an ordinary personal computer and requires\nnegligible 11MB of memory. Up to terms of degree 30, which is the maximum\ndegree the program can handle, the computation of 74248451 coefficients takes\n55 hours but still requires only a modest 5.5GB of memory.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 15:10:39 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Hofst\u00e4tter", "Harald", ""]]}, {"id": "2102.06621", "submitter": "Alex Kogan", "authors": "Dave Dice and Alex Kogan", "title": "Optimizing Inference Performance of Transformers on CPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DC cs.LG cs.MS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The Transformer architecture revolutionized the field of natural language\nprocessing (NLP). Transformers-based models (e.g., BERT) power many important\nWeb services, such as search, translation, question-answering, etc. While\nenormous research attention is paid to the training of those models, relatively\nlittle efforts are made to improve their inference performance. This paper\ncomes to address this gap by presenting an empirical analysis of scalability\nand performance of inferencing a Transformer-based model on CPUs. Focusing on\nthe highly popular BERT model, we identify key components of the Transformer\narchitecture where the bulk of the computation happens, and propose three\noptimizations to speed them up. The optimizations are evaluated using the\ninference benchmark from HuggingFace, and are shown to achieve the speedup of\nup to x2.37. The considered optimizations do not require any changes to the\nimplementation of the models nor affect their accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 17:01:35 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 22:30:35 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2021 16:54:34 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Dice", "Dave", ""], ["Kogan", "Alex", ""]]}, {"id": "2102.06827", "submitter": "Gokcen Kestor", "authors": "Erdal Mutlu, Ruiqin Tian, Bin Ren, Sriram Krishnamoorthy, Roberto\n  Gioiosa, Jacques Pienaar, Gokcen Kestor", "title": "COMET: A Domain-Specific Compilation of High-Performance Computational\n  Chemistry", "comments": "Proceeding of the 33rd the Workshop on Languages and Compilers for\n  Parallel Computing (LCPC), October 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The computational power increases over the past decades havegreatly enhanced\nthe ability to simulate chemical reactions andunderstand ever more complex\ntransformations. Tensor contractions are the fundamental computational building\nblock of these simulations. These simulations have often been tied to one\nplatform and restricted in generality by the interface provided to the user.\nThe expanding prevalence of accelerators and researcher demands necessitate a\nmore general approach which is not tied to specific hardware or requires\ncontortion of algorithms to specific hardware platforms. In this paper we\npresent COMET, a domain-specific programming language and compiler\ninfrastructure for tensor contractions targeting heterogeneous accelerators. We\npresent a system of progressive lowering through multiple layers of abstraction\nand optimization that achieves up to 1.98X speedup for 30 tensor contractions\ncommonly used in computational chemistry and beyond.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2021 00:25:13 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Mutlu", "Erdal", ""], ["Tian", "Ruiqin", ""], ["Ren", "Bin", ""], ["Krishnamoorthy", "Sriram", ""], ["Gioiosa", "Roberto", ""], ["Pienaar", "Jacques", ""], ["Kestor", "Gokcen", ""]]}, {"id": "2102.07670", "submitter": "Anibal M. Medina-Mardones", "authors": "Anibal M. Medina-Mardones", "title": "A computer algebra system for the study of commutativity up-to-coherent\n  homotopies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Python package ComCH is a lightweight specialized computer algebra system\nthat provides models for well known objects, the surjection and Barratt-Eccles\noperads, parameterizing the product structure of algebras that are commutative\nin a derived sense. The primary examples of such algebras treated by ComCH are\nthe cochain complexes of spaces, for which it provides effective constructions\nof Steenrod cohomology operations at all prime.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 16:58:19 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Medina-Mardones", "Anibal M.", ""]]}, {"id": "2102.07833", "submitter": "Aleksei Sorokin", "authors": "Sou-Cheng T. Choi, Fred J. Hickernell, R. Jagadeeswaran, Michael J.\n  McCourt, and Aleksei G. Sorokin", "title": "Quasi-Monte Carlo Software", "comments": "24 pages, 7 figures, to be published in the MCQMC2020 Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practitioners wishing to experience the efficiency gains from using low\ndiscrepancy sequences need correct, well-written software. This article, based\non our MCQMC 2020 tutorial, describes some of the better quasi-Monte Carlo\n(QMC) software available. We highlight the key software components required to\napproximate multivariate integrals or expectations of functions of vector\nrandom variables by QMC. We have combined these components in QMCPy, a Python\nopen source library, which we hope will draw the support of the QMC community.\nHere we introduce QMCPy.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 20:21:05 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Choi", "Sou-Cheng T.", ""], ["Hickernell", "Fred J.", ""], ["Jagadeeswaran", "R.", ""], ["McCourt", "Michael J.", ""], ["Sorokin", "Aleksei G.", ""]]}, {"id": "2102.08463", "submitter": "Yu-Hsuan Shih", "authors": "Yu-hsuan Shih, Garrett Wright, Joakim And\\'en, Johannes Blaschke, Alex\n  H. Barnett", "title": "cuFINUFFT: a load-balanced GPU library for general-purpose nonuniform\n  FFTs", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS cs.NA eess.SP math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonuniform fast Fourier transforms dominate the computational cost in many\napplications including image reconstruction and signal processing. We thus\npresent a general-purpose GPU-based CUDA library for type 1 (nonuniform to\nuniform) and type 2 (uniform to nonuniform) transforms in dimensions 2 and 3,\nin single or double precision. It achieves high performance for a given\nuser-requested accuracy, regardless of the distribution of nonuniform points,\nvia cache-aware point reordering, and load-balanced blocked spreading in shared\nmemory. At low accuracies, this gives on-GPU throughputs around $10^9$\nnonuniform points per second, and (even including host-device transfer) is\ntypically 4-10$\\times$ faster than the latest parallel CPU code FINUFFT (at 28\nthreads). It is competitive with two established GPU codes, being up to\n90$\\times$ faster at high accuracy and/or type 1 clustered point distributions.\nFinally we demonstrate a 5-12$\\times$ speedup versus CPU in an X-ray\ndiffraction 3D iterative reconstruction task at $10^{-12}$ accuracy, observing\nexcellent multi-GPU weak scaling up to one rank per GPU.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 21:57:23 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 23:18:05 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Shih", "Yu-hsuan", ""], ["Wright", "Garrett", ""], ["And\u00e9n", "Joakim", ""], ["Blaschke", "Johannes", ""], ["Barnett", "Alex H.", ""]]}, {"id": "2102.08514", "submitter": "Joshua Horacsek", "authors": "Joshua Horacsek, Usman Alim", "title": "Automatic Generation of Interpolants for Lattice Samplings: Part I --\n  Theory and Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpolation is a fundamental technique in scientific computing and is at\nthe heart of many scientific visualization techniques. There is usually a\ntrade-off between the approximation capabilities of an interpolation scheme and\nits evaluation efficiency. For many applications, it is important for a user to\nbe able to navigate their data in real time. In practice, the evaluation\nefficiency (or speed) outweighs any incremental improvements in reconstruction\nfidelity. In this two-part work, we first analyze from a general standpoint the\nuse of compact piece-wise polynomial basis functions to efficiently interpolate\ndata that is sampled on a lattice. In the sequel, we detail how we generate\nefficient implementations via automatic code generation on both CPU and GPU\narchitectures. Specifically, in this paper, we propose a general framework that\ncan produce a fast evaluation scheme by analyzing the algebro-geometric\nstructure of the convolution sum for a given lattice and basis function\ncombination. We demonstrate the utility and generality of our framework by\nproviding fast implementations of various box splines on the Body Centered and\nFace Centered Cubic lattices, as well as some non-separable box splines on the\nCartesian lattice. We also provide fast implementations for certain Voronoi\nsplines that have not yet appeared in the literature. Finally, we demonstrate\nthat this framework may also be used for non-Cartesian lattices in 4D.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 00:45:23 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Horacsek", "Joshua", ""], ["Alim", "Usman", ""]]}, {"id": "2102.08518", "submitter": "Joshua Horacsek", "authors": "Joshua Horacsek, Usman Alim", "title": "Automatic Generation of Interpolants for Lattice Samplings: Part II --\n  Implementation and Code Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the prequel to this paper, we presented a systematic framework for\nprocessing spline spaces. In this paper, we take the results of that framework\nand provide a code generation pipeline that automatically generates efficient\nimplementations of spline spaces. We decompose the final algorithm from Part I\nand translate the resulting components into LLVM-IR (a low level language that\ncan be compiled to various targets/architectures). Our design provides a\nhandful of parameters for a practitioner to tune - this is one of the avenues\nthat provides us with the flexibility to target many different computational\narchitectures and tune performance on those architectures. We also provide an\nevaluation of the effect of the different parameters on performance.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 00:55:19 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Horacsek", "Joshua", ""], ["Alim", "Usman", ""]]}, {"id": "2102.08530", "submitter": "Sami Abu-El-Haija", "authors": "Sami Abu-El-Haija, Valentino Crespi, Greg Ver Steeg, Aram Galstyan", "title": "Fast Graph Learning with Unique Optimal Solutions", "comments": null, "journal-ref": "ICLR 2021 Workshop on Geometrical and Topological Representation\n  Learning", "doi": null, "report-no": null, "categories": "cs.LG cs.MS cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider two popular Graph Representation Learning (GRL) methods: message\npassing for node classification and network embedding for link prediction. For\neach, we pick a popular model that we: (i) linearize and (ii) and switch its\ntraining objective to Frobenius norm error minimization. These simplifications\ncan cast the training into finding the optimal parameters in closed-form. We\nprogram in TensorFlow a functional form of Truncated Singular Value\nDecomposition (SVD), such that, we could decompose a dense matrix $\\mathbf{M}$,\nwithout explicitly computing $\\mathbf{M}$. We achieve competitive performance\non popular GRL tasks while providing orders of magnitude speedup. We\nopen-source our code at http://github.com/samihaija/tf-fsvd\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 02:00:07 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2021 14:39:03 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 08:43:54 GMT"}, {"version": "v4", "created": "Thu, 22 Apr 2021 09:32:50 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Abu-El-Haija", "Sami", ""], ["Crespi", "Valentino", ""], ["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "2102.09562", "submitter": "Marijan Beg", "authors": "Marijan Beg, Juliette Taka, Thomas Kluyver, Alexander Konovalov, Min\n  Ragan-Kelley, Nicolas M. Thi\\'ery, and Hans Fangohr", "title": "Using Jupyter for reproducible scientific workflows", "comments": "11 pages, 3 figures", "journal-ref": "Computing in Science & Engineering 23, 36-46 (2021)", "doi": "10.1109/MCSE.2021.3052101", "report-no": null, "categories": "cs.MS cs.NA math.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literate computing has emerged as an important tool for computational studies\nand open science, with growing folklore of best practices. In this work, we\nreport two case studies - one in computational magnetism and another in\ncomputational mathematics - where domain-specific software was exposed to the\nJupyter environment. This enables high-level control of simulations and\ncomputation, interactive exploration of computational results, batch processing\non HPC resources, and reproducible workflow documentation in Jupyter notebooks.\nIn the first study, Ubermag drives existing computational micromagnetics\nsoftware through a domain-specific language embedded in Python. In the second\nstudy, a dedicated Jupyter kernel interfaces with the GAP system for\ncomputational discrete algebra and its dedicated programming language. In light\nof these case studies, we discuss the benefits of this approach, including\nprogress toward more reproducible and reusable research results and outputs,\nnotably through the use of infrastructure such as JupyterHub and Binder.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 14:20:15 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Beg", "Marijan", ""], ["Taka", "Juliette", ""], ["Kluyver", "Thomas", ""], ["Konovalov", "Alexander", ""], ["Ragan-Kelley", "Min", ""], ["Thi\u00e9ry", "Nicolas M.", ""], ["Fangohr", "Hans", ""]]}, {"id": "2102.09811", "submitter": "Jan Zapletal", "authors": "Jan Zapletal and Raphael Watschinger and G\\\"unther Of and Michal Merta", "title": "Semi-analytic integration for a parallel space-time boundary element\n  method modeling the heat equation", "comments": "public access to https://github.com/zap150/besthea will be provided\n  soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presented paper concentrates on the boundary element method (BEM) for the\nheat equation in three spatial dimensions. In particular, we deal with tensor\nproduct space-time meshes allowing for quadrature schemes analytic in time and\nnumerical in space. The spatial integrals can be treated by standard BEM\ntechniques known from three dimensional stationary problems. The contribution\nof the paper is twofold. First, we provide temporal antiderivatives of the heat\nkernel necessary for the assembly of BEM matrices and the evaluation of the\nrepresentation formula. Secondly, the presented approach has been implemented\nin a publicly available library besthea allowing researchers to reuse the\nformulae and BEM routines straightaway. The results are validated by numerical\nexperiments in an HPC environment.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 08:54:43 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Zapletal", "Jan", ""], ["Watschinger", "Raphael", ""], ["Of", "G\u00fcnther", ""], ["Merta", "Michal", ""]]}, {"id": "2102.09904", "submitter": "Michael Bommarito Ii", "authors": "Ethan Bommarito, Michael J Bommarito II", "title": "An Empirical Analysis of the R Package Ecosystem", "comments": "20 pages, 3 figures, 23 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CY cs.SE physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this research, we present a comprehensive, longitudinal empirical summary\nof the R package ecosystem, including not just CRAN, but also Bioconductor and\nGitHub. We analyze more than 25,000 packages, 150,000 releases, and 15 million\nfiles across two decades, providing comprehensive counts and trends for common\nmetrics across packages, releases, authors, licenses, and other important\nmetadata. We find that the historical growth of the ecosystem has been robust\nunder all measures, with a compound annual growth rate of 29% for active\npackages, 28% for new releases, and 26% for active maintainers. As with many\nsimilar social systems, we find a number of highly right-skewed distributions\nwith practical implications, including the distribution of releases per\npackage, packages and releases per author or maintainer, package and maintainer\ndependency in-degree, and size per package and release. For example, the top\nfive packages are imported by nearly 25% of all packages, and the top ten\nmaintainers support packages that are imported by over half of all packages. We\nalso highlight the dynamic nature of the ecosystem, recording both dramatic\nacceleration and notable deceleration in the growth of R. From a licensing\nperspective, we find a notable majority of packages are distributed under\ncopyleft licensing or omit licensing information entirely. The data, methods,\nand calculations herein provide an anchor for public discourse and industry\ndecisions related to R and CRAN, serving as a foundation for future research on\nthe R software ecosystem and \"data science\" more broadly.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 12:55:18 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Bommarito", "Ethan", ""], ["Bommarito", "Michael J", "II"]]}, {"id": "2102.11572", "submitter": "Johannes Bl\\\"uhdorn", "authors": "Johannes Bl\\\"uhdorn, Max Sagebaum, Nicolas R. Gauger", "title": "Event-Based Automatic Differentiation of OpenMP with OpDiLib", "comments": "34 pages, 10 figures, 2 tables, 12 listings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the new software OpDiLib, a universal add-on for classical\noperator overloading AD tools that enables the automatic differentiation (AD)\nof OpenMP parallelized code. With it, we establish support for OpenMP features\nin a reverse mode operator overloading AD tool to an extent that was previously\nonly reported on in source transformation tools. We achieve this with an\nevent-based implementation ansatz that is unprecedented in AD. Combined with\nmodern OpenMP features around OMPT, we demonstrate how it can be used to\nachieve differentiation without any additional modifications of the source\ncode; neither do we impose a priori restrictions on the data access patterns,\nwhich makes OpDiLib highly applicable. For further performance optimizations,\nrestrictions like atomic updates on the adjoint variables can be lifted in a\nfine-grained manner for any parts of the code. OpDiLib can also be applied in a\nsemi-automatic fashion via a macro interface, which supports compilers that do\nnot implement OMPT. In a detailed performance study, we demonstrate the\napplicability of OpDiLib for a pure operator overloading approach in a hybrid\nparallel environment. We quantify the cost of atomic updates on the adjoint\nvector and showcase the speedup and scaling that can be achieved with the\ndifferent configurations of OpDiLib in both the forward and the reverse pass.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 09:22:04 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Bl\u00fchdorn", "Johannes", ""], ["Sagebaum", "Max", ""], ["Gauger", "Nicolas R.", ""]]}]