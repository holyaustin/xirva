[{"id": "1610.00345", "submitter": "Daniel W. Meyer", "authors": "Daniel W. Meyer", "title": "Density Estimation with Distribution Element Trees", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-017-9751-9", "report-no": null, "categories": "stat.ME cs.MS stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of probability densities based on available data is a central\ntask in many statistical applications. Especially in the case of large\nensembles with many samples or high-dimensional sample spaces, computationally\nefficient methods are needed. We propose a new method that is based on a\ndecomposition of the unknown distribution in terms of so-called distribution\nelements (DEs). These elements enable an adaptive and hierarchical\ndiscretization of the sample space with small or large elements in regions with\nsmoothly or highly variable densities, respectively. The novel refinement\nstrategy that we propose is based on statistical goodness-of-fit and pair-wise\n(as an approximation to mutual) independence tests that evaluate the local\napproximation of the distribution in terms of DEs. The capabilities of our new\nmethod are inspected based on several examples of different dimensionality and\nsuccessfully compared with other state-of-the-art density estimators.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 20:07:28 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 07:08:16 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Meyer", "Daniel W.", ""]]}, {"id": "1610.03034", "submitter": "Justin Chen", "authors": "Justin Chen, Joe Kileel", "title": "Numerical Implicitization", "comments": "5 pages, various improvements, to appear in Journal of Software for\n  Algebra and Geometry", "journal-ref": "J. Softw. Alg. Geom. 9 (2019) 55-63", "doi": "10.2140/jsag.2019.9.55", "report-no": null, "categories": "math.AG cs.MS math.AC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the $\\textit{NumericalImplicitization}$ package for\n$\\textit{Macaulay2}$, which allows for user-friendly computation of the\ninvariants of the image of a polynomial map, such as dimension, degree, and\nHilbert function values. This package relies on methods of numerical algebraic\ngeometry, including homotopy continuation and monodromy.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 19:14:59 GMT"}, {"version": "v2", "created": "Sat, 13 Apr 2019 01:50:06 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Chen", "Justin", ""], ["Kileel", "Joe", ""]]}, {"id": "1610.05141", "submitter": "Lorenz H\\\"ubschle-Schneider", "authors": "Peter Sanders, Sebastian Lamm, Lorenz H\\\"ubschle-Schneider, Emanuel\n  Schrade and Carsten Dachsbacher", "title": "Efficient Random Sampling -- Parallel, Vectorized, Cache-Efficient, and\n  Online", "comments": null, "journal-ref": "ACM Transactions on Mathematical Software (TOMS), Volume 44, Issue\n  3 (April 2018), pages 29:1-29:14", "doi": "10.1145/3157734", "report-no": null, "categories": "cs.DS cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sampling $n$ numbers from the range\n$\\{1,\\ldots,N\\}$ without replacement on modern architectures. The main result\nis a simple divide-and-conquer scheme that makes sequential algorithms more\ncache efficient and leads to a parallel algorithm running in expected time\n$\\mathcal{O}(n/p+\\log p)$ on $p$ processors, i.e., scales to massively parallel\nmachines even for moderate values of $n$. The amount of communication between\nthe processors is very small (at most $\\mathcal{O}(\\log p)$) and independent of\nthe sample size. We also discuss modifications needed for load balancing,\nonline sampling, sampling with replacement, Bernoulli sampling, and\nvectorization on SIMD units or GPUs.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 14:38:02 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 15:27:42 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Sanders", "Peter", ""], ["Lamm", "Sebastian", ""], ["H\u00fcbschle-Schneider", "Lorenz", ""], ["Schrade", "Emanuel", ""], ["Dachsbacher", "Carsten", ""]]}, {"id": "1610.05329", "submitter": "Antun Balaz", "authors": "Vladimir Loncar, Luis E. Young-S., Srdjan Skrbic, Paulsamy\n  Muruganandam, Sadhan K. Adhikari, Antun Balaz", "title": "OpenMP, OpenMP/MPI, and CUDA/MPI C programs for solving the\n  time-dependent dipolar Gross-Pitaevskii equation", "comments": "8 pages, 6 figures; to download the programs, click other formats and\n  download the source", "journal-ref": "Comput. Phys. Commun. 209 (2016) 190", "doi": "10.1016/j.cpc.2016.07.029", "report-no": null, "categories": "cond-mat.quant-gas cs.MS nlin.PS physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new versions of the previously published C and CUDA programs for\nsolving the dipolar Gross-Pitaevskii equation in one, two, and three spatial\ndimensions, which calculate stationary and non-stationary solutions by\npropagation in imaginary or real time. Presented programs are improved and\nparallelized versions of previous programs, divided into three packages\naccording to the type of parallelization. First package contains improved and\nthreaded version of sequential C programs using OpenMP. Second package\nadditionally parallelizes three-dimensional variants of the OpenMP programs\nusing MPI, allowing them to be run on distributed-memory systems. Finally,\nprevious three-dimensional CUDA-parallelized programs are further parallelized\nusing MPI, similarly as the OpenMP programs. We also present speedup test\nresults obtained using new versions of programs in comparison with the previous\nsequential C and parallel CUDA programs. The improvements to the sequential\nversion yield a speedup of 1.1 to 1.9, depending on the program. OpenMP\nparallelization yields further speedup of 2 to 12 on a 16-core workstation,\nwhile OpenMP/MPI version demonstrates a speedup of 11.5 to 16.5 on a computer\ncluster with 32 nodes used. CUDA/MPI version shows a speedup of 9 to 10 on a\ncomputer cluster with 32 nodes.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 20:07:09 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Loncar", "Vladimir", ""], ["Young-S.", "Luis E.", ""], ["Skrbic", "Srdjan", ""], ["Muruganandam", "Paulsamy", ""], ["Adhikari", "Sadhan K.", ""], ["Balaz", "Antun", ""]]}, {"id": "1610.06385", "submitter": "Farhad Merchant", "authors": "Farhad Merchant, Tarun Vatwani, Anupam Chattopadhyay, Soumyendu Raha,\n  S K Nandy, and Ranjani Narayan", "title": "Accelerating BLAS on Custom Architecture through Algorithm-Architecture\n  Co-design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basic Linear Algebra Subprograms (BLAS) play key role in high performance and\nscientific computing applications. Experimentally, yesteryear multicore and\nGeneral Purpose Graphics Processing Units (GPGPUs) are capable of achieving up\nto 15 to 57% of the theoretical peak performance at 65W to 240W respectively\nfor compute bound operations like Double/Single Precision General Matrix\nMultiplication (XGEMM). For bandwidth bound operations like Single/Double\nprecision Matrix-vector Multiplication (XGEMV) the performance is merely 5 to\n7% of the theoretical peak performance in multicores and GPGPUs respectively.\nAchieving performance in BLAS requires moving away from conventional wisdom and\nevolving towards customized accelerator tailored for BLAS through\nalgorithm-architecture co-design. In this paper, we present acceleration of\nLevel-1 (vector operations), Level-2 (matrix-vector operations), and Level-3\n(matrix-matrix operations) BLAS through algorithm architecture co-design on a\nCoarse-grained Reconfigurable Architecture (CGRA). We choose REDEFINE CGRA as a\nplatform for our experiments since REDEFINE can be adapted to support domain of\ninterest through tailor-made Custom Function Units (CFUs). For efficient\nsequential realization of BLAS, we present design of a Processing Element (PE)\nand perform micro-architectural enhancements in the PE to achieve up-to 74% of\nthe theoretical peak performance of PE in DGEMM, 40% in DGEMV and 20% in double\nprecision inner product (DDOT). We attach this PE to REDEFINE CGRA as a CFU and\nshow the scalability of our solution. Finally, we show performance improvement\nof 3-140x in PE over commercially available Intel micro-architectures,\nClearSpeed CSX700, FPGA, and Nvidia GPGPUs.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2016 12:46:07 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 15:37:28 GMT"}, {"version": "v3", "created": "Mon, 7 Nov 2016 04:26:29 GMT"}, {"version": "v4", "created": "Wed, 23 Nov 2016 14:23:17 GMT"}, {"version": "v5", "created": "Sun, 27 Nov 2016 14:11:36 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Merchant", "Farhad", ""], ["Vatwani", "Tarun", ""], ["Chattopadhyay", "Anupam", ""], ["Raha", "Soumyendu", ""], ["Nandy", "S K", ""], ["Narayan", "Ranjani", ""]]}, {"id": "1610.07310", "submitter": "Rodrigo Canales", "authors": "Rodrigo Canales, Elmar Peise, Paolo Bientinesi", "title": "Large Scale Parallel Computations in R through Elemental", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though in recent years the scale of statistical analysis problems has\nincreased tremendously, many statistical software tools are still limited to\nsingle-node computations. However, statistical analyses are largely based on\ndense linear algebra operations, which have been deeply studied, optimized and\nparallelized in the high-performance-computing community. To make\nhigh-performance distributed computations available for statistical analysis,\nand thus enable large scale statistical computations, we introduce RElem, an\nopen source package that integrates the distributed dense linear algebra\nlibrary Elemental into R. While on the one hand, RElem provides direct wrappers\nof Elemental's routines, on the other hand, it overloads various operators and\nfunctions to provide an entirely native R experience for distributed\ncomputations. We showcase how simple it is to port existing R programs to Relem\nand demonstrate that Relem indeed allows to scale beyond the single-node\nlimitation of R with the full performance of Elemental without any overhead.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 07:30:27 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Canales", "Rodrigo", ""], ["Peise", "Elmar", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1610.07944", "submitter": "Nicholas Battista", "authors": "Nicholas A. Battista and W. Christopher Strickland and Laura A. Miller", "title": "IB2d: a Python and MATLAB implementation of the immersed boundary method", "comments": "34 pages, 24 figures", "journal-ref": null, "doi": "10.1088/1748-3190/aa5e08", "report-no": null, "categories": "physics.flu-dyn cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of fluid-structure interaction (FSI) software involves\ntrade-offs between ease of use, generality, performance, and cost. Typically\nthere are large learning curves when using low-level software to model the\ninteraction of an elastic structure immersed in a uniform density fluid. Many\nexisting codes are not publicly available, and the commercial software that\nexists usually requires expensive licenses and may not be as robust or allow\nthe necessary flexibility that in house codes can provide. We present an open\nsource immersed boundary software package, IB2d, with full implementations in\nboth MATLAB and Python, that is capable of running a vast range of biomechanics\nmodels and is accessible to scientists who have experience in high-level\nprogramming environments. IB2d contains multiple options for constructing\nmaterial properties of the fiber structure, as well as the advection-diffusion\nof a chemical gradient, muscle mechanics models, and artificial forcing to\ndrive boundaries with a preferred motion.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 16:15:40 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Battista", "Nicholas A.", ""], ["Strickland", "W. Christopher", ""], ["Miller", "Laura A.", ""]]}, {"id": "1610.08128", "submitter": "Aydin Buluc", "authors": "Ariful Azad, Mathias Jacquelin, Aydin Buluc, Esmond G. Ng", "title": "The Reverse Cuthill-McKee Algorithm in Distributed-Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.MS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordering vertices of a graph is key to minimize fill-in and data structure\nsize in sparse direct solvers, maximize locality in iterative solvers, and\nimprove performance in graph algorithms. Except for naturally parallelizable\nordering methods such as nested dissection, many important ordering methods\nhave not been efficiently mapped to distributed-memory architectures. In this\npaper, we present the first-ever distributed-memory implementation of the\nreverse Cuthill-McKee (RCM) algorithm for reducing the profile of a sparse\nmatrix. Our parallelization uses a two-dimensional sparse matrix decomposition.\nWe achieve high performance by decomposing the problem into a small number of\nprimitives and utilizing optimized implementations of these primitives. Our\nimplementation shows strong scaling up to 1024 cores for smaller matrices and\nup to 4096 cores for larger matrices.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 00:26:44 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Azad", "Ariful", ""], ["Jacquelin", "Mathias", ""], ["Buluc", "Aydin", ""], ["Ng", "Esmond G.", ""]]}, {"id": "1610.08713", "submitter": "Sebastian Junges", "authors": "Christian Dehnert, Sebastian Junges, Joost-Pieter Katoen, Matthias\n  Volk", "title": "The Probabilistic Model Checker Storm (Extended Abstract)", "comments": "Extended abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LO cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new probabilistic model checker Storm. Using state-of-the-art\nlibraries, we aim for both high performance and versatility. This extended\nabstract gives a brief overview of the features of Storm.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 11:14:53 GMT"}], "update_date": "2016-10-30", "authors_parsed": [["Dehnert", "Christian", ""], ["Junges", "Sebastian", ""], ["Katoen", "Joost-Pieter", ""], ["Volk", "Matthias", ""]]}, {"id": "1610.09146", "submitter": "Satya Pramod Jammy", "authors": "Satya P. Jammy, Christian T. Jacobs, Neil D. Sandham", "title": "Performance evaluation of explicit finite difference algorithms with\n  varying amounts of computational and memory intensity", "comments": "Author accepted version. Accepted for publication in Journal of\n  Computational Science on 27 October 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC cs.MS physics.comp-ph physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future architectures designed to deliver exascale performance motivate the\nneed for novel algorithmic changes in order to fully exploit their\ncapabilities. In this paper, the performance of several numerical algorithms,\ncharacterised by varying degrees of memory and computational intensity, are\nevaluated in the context of finite difference methods for fluid dynamics\nproblems. It is shown that, by storing some of the evaluated derivatives as\nsingle thread- or process-local variables in memory, or recomputing the\nderivatives on-the-fly, a speed-up of ~2 can be obtained compared to\ntraditional algorithms that store all derivatives in global arrays.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 09:45:31 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Jammy", "Satya P.", ""], ["Jacobs", "Christian T.", ""], ["Sandham", "Neil D.", ""]]}, {"id": "1610.09874", "submitter": "Matthew Knepley", "authors": "Nicolas Barral and Matthew G. Knepley and Michael Lange and Matthew D.\n  Piggott and Gerard J. Gorman", "title": "Anisotropic mesh adaptation in Firedrake with PETSc DMPlex", "comments": "5 page, 2 figures, Proceedings of the 25th International Meshing\n  Roundtable, ed. Steve Owen and Hang Si, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite decades of research in this area, mesh adaptation capabilities are\nstill rarely found in numerical simulation software. We postulate that the\nprimary reason for this is lack of usability. Integrating mesh adaptation into\nexisting software is difficult as non-trivial operators, such as error metrics\nand interpolation operators, are required, and integrating available adaptive\nremeshers is not straightforward. Our approach presented here is to first\nintegrate Pragmatic, an anisotropic mesh adaptation library, into DMPlex, a\nPETSc object that manages unstructured meshes and their interactions with\nPETSc's solvers and I/O routines. As PETSc is already widely used, this will\nmake anisotropic mesh adaptation available to a much larger community. As a\ndemonstration of this we describe the integration of anisotropic mesh\nadaptation into Firedrake, an automated Finite Element based system for the\nportable solution of partial differential equations which already uses PETSc\nsolvers and I/O via DMPlex. We present a proof of concept of this integration\nwith a three-dimensional advection test case.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 11:34:29 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Barral", "Nicolas", ""], ["Knepley", "Matthew G.", ""], ["Lange", "Michael", ""], ["Piggott", "Matthew D.", ""], ["Gorman", "Gerard J.", ""]]}]