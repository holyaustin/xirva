[{"id": "1808.00532", "submitter": "Christian Mendl", "authors": "Lisa Sahlmann and Christian B. Mendl", "title": "GuiTeNet: A graphical user interface for tensor networks", "comments": "7 pages, 4 figures", "journal-ref": "J. Open Res. Softw. 8(1), 29 (2020)", "doi": "10.5334/jors.304", "report-no": null, "categories": "cs.MS cond-mat.str-el physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a graphical user interface for constructing arbitrary tensor\nnetworks and specifying common operations like contractions or splitting,\ndenoted GuiTeNet. Tensors are represented as nodes with attached legs,\ncorresponding to the ordered dimensions of the tensor. GuiTeNet visualizes the\ncurrent network, and instantly generates Python/NumPy source code for the\nhitherto sequence of user actions. Support for additional programming languages\nis planned for the future. We discuss the elementary operations on tensor\nnetworks used by GuiTeNet, together with high-level optimization strategies.\nThe software runs directly in web browsers and is available online at\nhttp://guitenet.org.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 18:09:13 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Sahlmann", "Lisa", ""], ["Mendl", "Christian B.", ""]]}, {"id": "1808.02638", "submitter": "Xinsheng Qin", "authors": "Xinsheng Qin, Randall J. LeVeque, Michael R. Motley", "title": "Accelerating wave-propagation algorithms with adaptive mesh refinement\n  using the Graphics Processing Unit (GPU)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clawpack is a library for solving nonlinear hyperbolic partial differential\nequations using high-resolution finite volume methods based on Riemann solvers\nand limiters. It supports Adaptive Mesh Refinement (AMR), which is essential in\nsolving multi-scale problems. Recently, we added capabilities to accelerate the\ncode by using the Graphics Process Unit (GPU). Routines that manage CPU and GPU\nAMR data and facilitate the execution of GPU kernels are added. Customized and\nCPU thread-safe memory managers are designed to manage GPU and CPU memory\npools, which is essential in eliminating the overhead of memory allocation and\nde-allocation. A global reduction is conducted every time step for dynamically\nadjusting the time step based on Courant number restrictions. Some small GPU\nkernels are merged into bigger kernels, which greatly reduces kernel launching\noverhead. A speed-up between $2$ and $3$ for the total running time is observed\nin an acoustics benchmark problem.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 06:21:56 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Qin", "Xinsheng", ""], ["LeVeque", "Randall J.", ""], ["Motley", "Michael R.", ""]]}, {"id": "1808.02731", "submitter": "Robert Speck", "authors": "Robert Speck", "title": "pySDC - Prototyping spectral deferred corrections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the Python framework pySDC for solving collocation\nproblems with spectral deferred correction methods (SDC) and their\ntime-parallel variant PFASST, the parallel full approximation scheme in space\nand time. pySDC features many implementations of SDC and PFASST, from simple\nimplicit time-stepping to high-order implicit-explicit or multi-implicit\nsplitting and multi-level spectral deferred corrections. It comes with many\ndifferent, pre-implemented examples and has seven tutorials to help new users\nwith their first steps. Time-parallelism is implemented either in an emulated\nway for debugging and prototyping as well as using MPI for benchmarking. The\ncode is fully documented and tested using continuous integration, including\nmost results of previous publications. Here, we describe the structure of the\ncode by taking two different perspectives: the user's and the developer's\nperspective. While the first sheds light on the front-end, the examples and the\ntutorials, the second is used to describe the underlying implementation and the\ndata structures. We show three different examples to highlight various aspects\nof the implementation, the capabilities and the usage of pySDC. Also, couplings\nto the FEniCS framework and PETSc, the latter including spatial parallelism\nwith MPI, are described.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 11:44:27 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Speck", "Robert", ""]]}, {"id": "1808.03370", "submitter": "Jiahao Chen", "authors": "Jeff Bezanson and Jake Bolewski and Jiahao Chen", "title": "Fast Flexible Function Dispatch in Julia", "comments": "15 pages, repository at https://github.com/jiahao/julia-type-system", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technical computing is a challenging application area for programming\nlanguages to address. This is evinced by the unusually large number of\nspecialized languages in the area (e.g. MATLAB, R), and the complexity of\ncommon software stacks, often involving multiple languages and custom code\ngenerators. We believe this is ultimately due to key characteristics of the\ndomain: highly complex operators, a need for extensive code specialization for\nperformance, and a desire for permissive high-level programming styles allowing\nproductive experimentation. The Julia language attempts to provide a more\neffective structure for this kind of programming by allowing programmers to\nexpress complex polymorphic behaviors using dynamic multiple dispatch over\nparametric types. The forms of extension and reuse permitted by this paradigm\nhave proven valuable for technical computing. We report on how this approach\nhas allowed domain experts to express useful abstractions while simultaneously\nproviding a natural path to better performance for high-level technical code.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 23:09:16 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Bezanson", "Jeff", ""], ["Bolewski", "Jake", ""], ["Chen", "Jiahao", ""]]}, {"id": "1808.03916", "submitter": "Jiahao Chen", "authors": "Jiahao Chen", "title": "Linguistic Relativity and Programming Languages", "comments": "10 pages, repo at\n  https://github.com/jiahao/statistical-computing-linguistics, Published in\n  Proceedings of the 2016 Joint Statistical Meetings, Chicago, IL, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.MS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of programming languages can wax and wane across the decades. We\nexamine the split-apply- combine pattern that is common in statistical\ncomputing, and consider how its invocation or implementation in languages like\nMATLAB and APL differ from R/dplyr. The differences in spelling illustrate how\nthe concept of linguistic relativity applies to programming languages in ways\nthat are analogous to human languages. Finally, we discuss how Julia, by being\na high performance yet general purpose dynamic language, allows its users to\nexpress different abstractions to suit individual preferences.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 09:38:34 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Chen", "Jiahao", ""]]}, {"id": "1808.04579", "submitter": "Aaron Montag", "authors": "Aaron Montag and J\\\"urgen Richter-Gebert", "title": "Bringing Together Dynamic Geometry Software and the Graphics Processing\n  Unit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.DC cs.SC math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We equip dynamic geometry software (DGS) with a user-friendly method that\nenables massively parallel calculations on the graphics processing unit (GPU).\nThis interplay of DGS and GPU opens up various applications in education and\nmathematical research. The GPU-aided discovery of mathematical properties,\ninteractive visualizations of algebraic surfaces (raycasting), the mathematical\ndeformation of images and footage in real-time, and computationally demanding\nnumerical simulations of PDEs are examples from the long and versatile list of\nnew domains that our approach makes accessible within a DGS. We ease the\ndevelopment of complex (mathematical) visualizations and provide a\nrapid-prototyping scheme for general-purpose computations (GPGPU).\n  The possibility to program both CPU and GPU with the use of only one\nhigh-level (scripting) programming language is a crucial aspect of our concept.\nWe embed shader programming seamlessly within a high-level (scripting)\nprogramming environment. The aforementioned requires the symbolic process of\nthe transcompilation of a high-level programming language into shader\nprogramming language for GPU and, in this article, we address the challenge of\nthe automatic translation of a high-level programming language to a shader\nlanguage of the GPU. To maintain platform independence and the possibility to\nuse our technology on modern devices, we focus on a realization through WebGL.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 08:20:36 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Montag", "Aaron", ""], ["Richter-Gebert", "J\u00fcrgen", ""]]}, {"id": "1808.05513", "submitter": "Lawrence Mitchell", "authors": "Robert C. Kirby and Lawrence Mitchell", "title": "Code generation for generally mapped finite elements", "comments": "23 pages", "journal-ref": "ACM Transactions on Mathematical Software 45(41):1-23 (2019)", "doi": "10.1145/3361745", "report-no": null, "categories": "cs.MS cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many classical finite elements such as the Argyris and Bell elements have\nlong been absent from high-level PDE software. Building on recent theoretical\nwork, we describe how to implement very general finite element transformations\nin FInAT and hence into the Firedrake finite element system. Numerical results\nevaluate the new elements, comparing them to existing methods for classical\nproblems. For a second order model problem, we find that new elements give\nsmooth solutions at a mild increase in cost over standard Lagrange elements.\nFor fourth-order problems, however, the newly-enabled methods significantly\noutperform interior penalty formulations. We also give some advanced use cases,\nsolving the nonlinear Cahn-Hilliard equation and some biharmonic eigenvalue\nproblems (including Chladni plates) using $C^1$ discretizations.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 14:30:14 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 10:30:57 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Kirby", "Robert C.", ""], ["Mitchell", "Lawrence", ""]]}, {"id": "1808.06736", "submitter": "Alex Barnett", "authors": "Alex H. Barnett, Jeremy F. Magland, Ludvig af Klinteberg", "title": "A parallel non-uniform fast Fourier transform library based on an\n  \"exponential of semicircle\" kernel", "comments": "25 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nonuniform fast Fourier transform (NUFFT) generalizes the FFT to off-grid\ndata. Its many applications include image reconstruction, data analysis, and\nthe numerical solution of differential equations. We present FINUFFT, an\nefficient parallel library for type 1 (nonuiform to uniform), type 2 (uniform\nto nonuniform), or type 3 (nonuniform to nonuniform) transforms, in dimensions\n1, 2, or 3. It uses minimal RAM, requires no precomputation or plan steps, and\nhas a simple interface to several languages. We perform the expensive\nspreading/interpolation between nonuniform points and the fine grid via a\nsimple new kernel---the `exponential of semicircle' $e^{\\beta \\sqrt{1-x^2}}$ in\n$x\\in[-1,1]$---in a cache-aware load-balanced multithreaded implementation. The\ndeconvolution step requires the Fourier transform of the kernel, for which we\npropose efficient numerical quadrature. For types 1 and 2, rigorous error\nbounds asymptotic in the kernel width approach the fastest known exponential\nrate, namely that of the Kaiser--Bessel kernel. We benchmark against several\npopular CPU-based libraries, showing favorable speed and memory footprint,\nespecially in three dimensions when high accuracy and/or clustered point\ndistributions are desired.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 01:55:35 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 20:26:13 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Barnett", "Alex H.", ""], ["Magland", "Jeremy F.", ""], ["Klinteberg", "Ludvig af", ""]]}, {"id": "1808.07832", "submitter": "Devangi Parikh", "authors": "Devangi N. Parikh, Margaret E. Myers, Richard Vuduc, Robert A. van de\n  Geijn", "title": "A Simple Methodology for Computing Families of Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": "FLAME Working Note #87, The University of Texas at Austin,\n  Department of Computer Science, Technical Report TR-18-06", "categories": "cs.PL cs.LO cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering \"good\" algorithms for an operation is often considered an art\nbest left to experts. What if there is a simple methodology, an algorithm, for\nsystematically deriving a family of algorithms as well as their cost analyses,\nso that the best algorithm can be chosen? We discuss such an approach for\nderiving loop-based algorithms. The example used to illustrate this\nmethodology, evaluation of a polynomial, is itself simple yet the best\nalgorithm that results is surprising to a non-expert: Horner's rule. We finish\nby discussing recent advances that make this approach highly practical for the\ndomain of high-performance linear algebra software libraries.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 23:17:06 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Parikh", "Devangi N.", ""], ["Myers", "Margaret E.", ""], ["Vuduc", "Richard", ""], ["van de Geijn", "Robert A.", ""]]}, {"id": "1808.07984", "submitter": "Jianyu Huang", "authors": "Jianyu Huang, Chenhan D. Yu, Robert A. van de Geijn", "title": "Implementing Strassen's Algorithm with CUTLASS on NVIDIA Volta GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": "FLAME Working Note #88, The University of Texas at Austin,\n  Department of Computer Science, Technical Report TR-18-08", "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional GPU implementations of Strassen's algorithm (Strassen) typically\nrely on the existing high-performance matrix multiplication (GEMM), trading\nspace for time. As a result, such approaches can only achieve practical speedup\nfor relatively large, \"squarish\" matrices due to the extra memory overhead, and\ntheir usages are limited due to the considerable workspace. We present novel\nStrassen primitives for GPUs that can be composed to generate a family of\nStrassen algorithms. Our algorithms utilize both the memory and thread\nhierarchies on GPUs, reusing shared memory and register files inherited from\nGEMM, fusing additional operations, and avoiding extra workspace. We further\nexploit intra- and inter-kernel parallelism by batching, streaming, and\nemploying atomic operations. We also develop a performance model for NVIDIA\nVolta GPUs to select the appropriate blocking parameters and predict the\nperformance for GEMM and Strassen. Overall, our 1-level Strassen can achieve up\nto 1.11x speedup with a crossover point as small as 1,536 compared to\ncublasSgemm on a NVIDIA Tesla V100 GPU. With additional workspace, our 2-level\nStrassen can achieve 1.19x speedup with a crossover point at 7,680.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 02:28:51 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Huang", "Jianyu", ""], ["Yu", "Chenhan D.", ""], ["van de Geijn", "Robert A.", ""]]}, {"id": "1808.08431", "submitter": "Timo de Wolff", "authors": "Henning Seidler and Timo de Wolff", "title": "An Experimental Comparison of SONC and SOS Certificates for\n  Unconstrained Optimization", "comments": "25 pages, 3 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.MS math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the minimum of a multivariate real polynomial is a well-known hard\nproblem with various applications. We present a polynomial time algorithm to\napproximate such lower bounds via sums of nonnegative circuit polynomials\n(SONC). As a main result, we carry out the first large-scale comparison of\nSONC, using this algorithm and different geometric programming (GP) solvers,\nwith the classical sums of squares (SOS) approach, using several of the most\ncommon semidefinite programming (SDP) solvers. SONC yields bounds competitive\nto SOS in several cases, but using significantly less time and memory. In\nparticular, SONC/GP can handle much larger problem instances than SOS/SDP.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 14:17:59 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Seidler", "Henning", ""], ["de Wolff", "Timo", ""]]}]