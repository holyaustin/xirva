[{"id": "1810.00674", "submitter": "Vladim\\'ir Luke\\v{s}", "authors": "Robert Cimrman, Vladim\\'ir Luke\\v{s}, Eduard Rohan", "title": "Multiscale finite element calculations in Python using SfePy", "comments": "This manuscript version is made available under the CC-BY-NC-ND 4.0\n  license", "journal-ref": "Advances in Computational Mathematics, 45(4): 1897-1921 (2019)", "doi": "10.1007/s10444-019-09666-0", "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SfePy (Simple finite elements in Python) is a software for solving various\nkinds of problems described by partial differential equations in one, two or\nthree spatial dimensions by the finite element method. Its source code is\nmostly (85\\%) Python and relies on fast vectorized operations provided by the\nNumPy package. For a particular problem two interfaces can be used: a\ndeclarative application programming interface (API), where problem\ndescription/definition files (Python modules) are used to define a calculation,\nand an imperative API, that can be used for interactive commands, or in scripts\nand libraries. After outlining the SfePy package development, the paper\nintroduces its implementation, structure and general features. The components\nfor defining a partial differential equation are described using an example of\na simple heat conduction problem. Specifically, the declarative API of SfePy is\npresented in the example. To illustrate one of SfePy's main assets, the\nframework for implementing complex multiscale models based on the theory of\nhomogenization, an example of a two-scale piezoelastic model is presented,\nshowing both the mathematical description of the problem and the corresponding\ncode.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 12:41:29 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Cimrman", "Robert", ""], ["Luke\u0161", "Vladim\u00edr", ""], ["Rohan", "Eduard", ""]]}, {"id": "1810.01361", "submitter": "Luisa Carracciuolo", "authors": "Luisa Carracciuolo and Emil M. Constantinescu and Luisa D'Amore", "title": "Validation of a PETSc based software implementing a 4DVAR Data\n  Assimilation algorithm: a case study related with an Oceanic Model based on\n  Shallow Water equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work are presented and discussed some results related to the\nvalidation process of a software module based on PETSc which implements a Data\nAssimilation algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 16:38:07 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 09:40:53 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Carracciuolo", "Luisa", ""], ["Constantinescu", "Emil M.", ""], ["D'Amore", "Luisa", ""]]}, {"id": "1810.02561", "submitter": "Simon Olofsson", "authors": "Simon Olofsson and Lukas Hebing and Sebastian Niedenf\\\"uhr and Marc\n  Peter Deisenroth and Ruth Misener", "title": "GPdoemd: a Python package for design of experiments for model\n  discrimination", "comments": null, "journal-ref": "Computers & Chemical Engineering, Volume 125, 2019, Pages 54-70", "doi": "10.1016/j.compchemeng.2019.03.010", "report-no": null, "categories": "cs.MS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model discrimination identifies a mathematical model that usefully explains\nand predicts a given system's behaviour. Researchers will often have several\nmodels, i.e. hypotheses, about an underlying system mechanism, but insufficient\nexperimental data to discriminate between the models, i.e. discard inaccurate\nmodels. Given rival mathematical models and an initial experimental data set,\noptimal design of experiments suggests maximally informative experimental\nobservations that maximise a design criterion weighted by prediction\nuncertainty. The model uncertainty requires gradients, which may not be readily\navailable for black-box models. This paper (i) proposes a new design criterion\nusing the Jensen-R\\'enyi divergence, and (ii) develops a novel method replacing\nblack-box models with Gaussian process surrogates. Using the surrogates, we\nmarginalise out the model parameters with approximate inference. Results show\nthese contributions working well for both classical and new test instances. We\nalso (iii) introduce and discuss GPdoemd, the open-source implementation of the\nGaussian process surrogate method.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 08:02:28 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 17:34:03 GMT"}, {"version": "v3", "created": "Fri, 8 Mar 2019 15:24:29 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Olofsson", "Simon", ""], ["Hebing", "Lukas", ""], ["Niedenf\u00fchr", "Sebastian", ""], ["Deisenroth", "Marc Peter", ""], ["Misener", "Ruth", ""]]}, {"id": "1810.03940", "submitter": "Tobias Weinzierl", "authors": "Dominic E. Charrier, Benjamin Hazelwood, Ekaterina Tutlyaeva, Michael\n  Bader, Michael Dumbser, Andrey Kudryavtsev, Alexander Moskovsky, Tobias\n  Weinzierl", "title": "Studies on the energy and deep memory behaviour of a cache-oblivious,\n  task-based hyperbolic PDE solver", "comments": null, "journal-ref": null, "doi": "10.1177/1094342019842645", "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance behaviour of a seismic simulation using the ExaHyPE\nengine with a specific focus on memory characteristics and energy needs.\nExaHyPE combines dynamically adaptive mesh refinement (AMR) with ADER-DG. It is\nparallelized using tasks, and it is cache efficient. AMR plus ADER-DG yields a\ntask graph which is highly dynamic in nature and comprises both arithmetically\nexpensive tasks and tasks which challenge the memory's latency. The expensive\ntasks and thus the whole code benefit from AVX vectorization, though we suffer\nfrom memory access bursts. A frequency reduction of the chip improves the\ncode's energy-to-solution. Yet, it does not mitigate burst effects. The bursts'\nlatency penalty becomes worse once we add Intel Optane technology, increase the\ncore count significantly, or make individual, computationally heavy tasks fall\nout of close caches. Thread overbooking to hide away these latency penalties\ncontra-productive with non-inclusive caches as it destroys the cache and\nvectorization character. In cases where memory-intense and computationally\nexpensive tasks overlap, ExaHyPE's cache-oblivious implementation can exploit\ndeep, non-inclusive, heterogeneous memory effectively, as main memory misses\narise infrequently and slow down only few cores. We thus propose that upcoming\nsupercomputing simulation codes with dynamic, inhomogeneous task graphs are\nactively supported by thread runtimes in intermixing tasks of different compute\ncharacter, and we propose that future hardware actively allows codes to\ndownclock the cores running particular task types.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 12:37:46 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 07:38:07 GMT"}, {"version": "v3", "created": "Fri, 15 Feb 2019 14:17:17 GMT"}, {"version": "v4", "created": "Mon, 25 Mar 2019 14:22:14 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Charrier", "Dominic E.", ""], ["Hazelwood", "Benjamin", ""], ["Tutlyaeva", "Ekaterina", ""], ["Bader", "Michael", ""], ["Dumbser", "Michael", ""], ["Kudryavtsev", "Andrey", ""], ["Moskovsky", "Alexander", ""], ["Weinzierl", "Tobias", ""]]}, {"id": "1810.04033", "submitter": "Benjamin Hazelwood", "authors": "Benjamin Hazelwood, Tobias Weinzierl", "title": "Coloured and task-based stencil codes", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple stencil codes are and remain an important building block in scientific\ncomputing. On shared memory nodes, they are traditionally parallelised through\ncolouring or (recursive) tiling. New OpenMP versions alternatively allow users\nto specify data dependencies explicitly and to outsource the decision how to\ndistribute the work to the runtime system. We evaluate traditional\nmultithreading strategies on both Broadwell and KNL, study the arising\nassignment of tasks to threads and, from there, derive two efficient ways to\nparallelise stencil codes on regular Cartesian grids that fuse colouring and\ntask-based approaches.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 14:37:52 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Hazelwood", "Benjamin", ""], ["Weinzierl", "Tobias", ""]]}, {"id": "1810.04125", "submitter": "Gustavo Ch\\'avez", "authors": "Christopher Gorman, Gustavo Ch\\'avez, Pieter Ghysels, Th\\'eo Mary,\n  Fran\\c{c}ois-Henry Rouet, Xiaoye Sherry Li", "title": "Matrix-free construction of HSS representation using adaptive randomized\n  sampling", "comments": "24 pages, 4 figures, 20 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new algorithms for the randomized construction of hierarchically\nsemi-separable matrices, addressing several practical issues. The HSS\nconstruction algorithms use a partially matrix-free, adaptive randomized\nprojection scheme to determine the maximum off-diagonal block rank. We develop\nboth relative and absolute stopping criteria to determine the minimum dimension\nof the random projection matrix that is sufficient for the desired accuracy.\nTwo strategies are discussed to adaptively enlarge the random sample matrix:\nrepeated doubling of the number of random vectors, and iteratively incrementing\nthe number of random vectors by a fixed number. The relative and absolute\nstopping criteria are based on probabilistic bounds for the Frobenius norm of\nthe random projection of the Hankel blocks of the input matrix. We discuss\nparallel implementation and computation and communication cost of both\nvariants. Parallel numerical results for a range of applications, including\nboundary element method matrices and quantum chemistry Toeplitz matrices, show\nthe effectiveness, scalability and numerical robustness of the proposed\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 9 Oct 2018 16:50:10 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 18:55:38 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Gorman", "Christopher", ""], ["Ch\u00e1vez", "Gustavo", ""], ["Ghysels", "Pieter", ""], ["Mary", "Th\u00e9o", ""], ["Rouet", "Fran\u00e7ois-Henry", ""], ["Li", "Xiaoye Sherry", ""]]}, {"id": "1810.07026", "submitter": "Markus H\\\"ohnerbach", "authors": "Markus H\\\"ohnerbach, Paolo Bientinesi", "title": "Optimizing AIREBO: Navigating the Journey from Complex Legacy Code to\n  High Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite initiatives to improve the quality of scientific codes, there still\nis a large presence of legacy code. Such code often needs to implement a lot of\nfunctionality under time constrains, sacrificing quality. Additionally, quality\nis rarely improved by optimizations for new architectures. This development\nmodel leads to code that is increasingly difficult to work with. Our suggested\nsolution includes complexity-reducing refactoring and hardware abstraction. We\nfocus on the AIREBO potential from LAMMPS, where the challenge is that any\npotential kernel is rather large and complex, hindering systematic\noptimization. This issue is common to codes that model multiple physical\nphenomena. We present our journey from the C++ port of a previous Fortran code\nto performance-portable, KNC-hybrid, vectorized, scalable, optimized code\nsupporting full and reduced precision. The journey includes extensive testing\nthat fixed bugs in the original code. Large-scale, full-precision runs sustain\nspeedups of more than 4x (KNL) and 3x (Skylake).\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 14:21:18 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["H\u00f6hnerbach", "Markus", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1810.07517", "submitter": "Hongbo Rong", "authors": "Hongbo Rong", "title": "Expressing Sparse Matrix Computations for Productive Performance on\n  Spatial Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses spatial programming of sparse matrix computations for\nproductive performance. The challenge is how to express an irregular\ncomputation and its optimizations in a regular way.\n  A sparse matrix has (non-zero) values and a structure. In this paper, we\npropose to classify the implementations of a computation on a sparse matrix\ninto two categories: (1) structure-driven, or top-down, approach, which\ntraverses the structure with given row and column indices and locates the\ncorresponding values, and (2) values-driven, or bottom-up, approach, which\nloads and processes the values in parallel streams, and decodes the structure\nfor the values' corresponding row and column indices.\n  On a spatial architecture like FPGAs, the values-driven approach is the norm.\nWe show how to express a sparse matrix computation and its optimizations for a\nvalues-driven implementation. A compiler automatically synthesizes a code to\ndecode the structure. In this way, programmers focus on optimizing the\nprocessing of the values, using familiar optimizations for dense matrices,\nwhile leaving the complex, irregular structure traversal to an automatic\ncompiler. We also attempt to regularize the optimizations of the reduction for\na dynamic number of values, which is common in a sparse matrix computation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 18:37:06 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Rong", "Hongbo", ""]]}, {"id": "1810.08297", "submitter": "Jarrett Revels", "authors": "Jarrett Revels, Tim Besard, Valentin Churavy, Bjorn De Sutter and Juan\n  Pablo Vielma", "title": "Dynamic Automatic Differentiation of GPU Broadcast Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how forward-mode automatic differentiation (AD) can be employed\nwithin larger reverse-mode computations to dynamically differentiate broadcast\noperations in a GPU-friendly manner. Our technique fully exploits the broadcast\nJacobian's inherent sparsity structure, and unlike a pure reverse-mode\napproach, this \"mixed-mode\" approach does not require a backwards pass over the\nbroadcasted operation's subgraph, obviating the need for several\nreverse-mode-specific programmability restrictions on user-authored broadcast\noperations. Most notably, this approach allows broadcast fusion in primal code\ndespite the presence of data-dependent control flow. We discuss an experiment\nin which a Julia implementation of our technique outperformed pure reverse-mode\nTensorFlow and Julia implementations for differentiating through broadcast\noperations within an HM-LSTM cell update calculation.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 22:52:52 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 11:56:49 GMT"}, {"version": "v3", "created": "Wed, 24 Oct 2018 21:05:28 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Revels", "Jarrett", ""], ["Besard", "Tim", ""], ["Churavy", "Valentin", ""], ["De Sutter", "Bjorn", ""], ["Vielma", "Juan Pablo", ""]]}, {"id": "1810.08723", "submitter": "Ewout van den Berg", "authors": "Ewout van den Berg", "title": "The Ocean Tensor Package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix and tensor operations form the basis of a wide range of fields and\napplications, and in many cases constitute a substantial part of the overall\ncomputational complexity. The ability of general-purpose GPUs to speed up many\nof these operations and enable others has resulted in a widespread adaptation\nof these devices. In order for tensor operations to take full advantage of the\ncomputational power, specialized software is required, and currently there\nexist several packages (predominantly in the area of deep learning) that\nincorporate tensor operations on both CPU and GPU. Nevertheless, a stand-alone\nframework that supports general tensor operations is still missing. In this\npaper we fill this gap and propose the Ocean Tensor Library: a modular\ntensor-support package that is designed to serve as a foundational layer for\napplications that require dense tensor operations on a variety of device types.\nThe API is carefully designed to be powerful, extensible, and at the same time\neasy to use. The package is available as open source.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 00:56:12 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Berg", "Ewout van den", ""]]}, {"id": "1810.09361", "submitter": "Conrad Sanderson", "authors": "Shikhar Bhardwaj and Ryan R. Curtin and Marcus Edel and Yannis\n  Mentekidis and Conrad Sanderson", "title": "ensmallen: a flexible C++ library for efficient function optimization", "comments": "Workshop on Systems for ML and Open Source Software at NIPS /\n  NeurIPS, 2018", "journal-ref": null, "doi": "10.5281/zenodo.2008650", "report-no": null, "categories": "cs.MS cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ensmallen, a fast and flexible C++ library for mathematical\noptimization of arbitrary user-supplied functions, which can be applied to many\nmachine learning problems. Several types of optimizations are supported,\nincluding differentiable, separable, constrained, and categorical objective\nfunctions. The library provides many pre-built optimizers (including numerous\nvariants of SGD and Quasi-Newton optimizers) as well as a flexible framework\nfor implementing new optimizers and objective functions. Implementation of a\nnew optimizer requires only one method and a new objective function requires\ntypically one or two C++ functions. This can aid in the quick implementation\nand prototyping of new machine learning algorithms. Due to the use of C++\ntemplate metaprogramming, ensmallen is able to support compiler optimizations\nthat provide fast runtimes. Empirical comparisons show that ensmallen is able\nto outperform other optimization frameworks (like Julia and SciPy), sometimes\nby large margins. The library is distributed under the BSD license and is ready\nfor use in production environments.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 15:26:35 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 04:37:09 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Bhardwaj", "Shikhar", ""], ["Curtin", "Ryan R.", ""], ["Edel", "Marcus", ""], ["Mentekidis", "Yannis", ""], ["Sanderson", "Conrad", ""]]}, {"id": "1810.09891", "submitter": "Michael Schmischke", "authors": "Michael Schmischke", "title": "Nonequispaced Fast Fourier Transform (NFFT) Interface for Julia", "comments": "19 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes the newly added Julia interface to the NFFT3 library.\nWe explain the multidimensional NFFT algorithm and basics of the interface.\nFurthermore, we go into detail about the different parameters and how to adjust\nthem properly.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 14:46:09 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Schmischke", "Michael", ""]]}, {"id": "1810.11363", "submitter": "Anna Veronika Dorogush", "authors": "Anna Veronika Dorogush, Vasily Ershov, Andrey Gulin", "title": "CatBoost: gradient boosting with categorical features support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.MS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present CatBoost, a new open-sourced gradient boosting\nlibrary that successfully handles categorical features and outperforms existing\npublicly available implementations of gradient boosting in terms of quality on\na set of popular publicly available datasets. The library has a GPU\nimplementation of learning algorithm and a CPU implementation of scoring\nalgorithm, which are significantly faster than other gradient boosting\nlibraries on ensembles of similar sizes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 13:08:24 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Dorogush", "Anna Veronika", ""], ["Ershov", "Vasily", ""], ["Gulin", "Andrey", ""]]}]