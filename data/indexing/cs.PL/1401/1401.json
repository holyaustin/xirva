[{"id": "1401.0514", "submitter": "Chris J. Maddison", "authors": "Chris J. Maddison and Daniel Tarlow", "title": "Structured Generative Models of Natural Source Code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of building generative models of natural source code\n(NSC); that is, source code written and understood by humans. Our primary\ncontribution is to describe a family of generative models for NSC that have\nthree key properties: First, they incorporate both sequential and hierarchical\nstructure. Second, we learn a distributed representation of source code\nelements. Finally, they integrate closely with a compiler, which allows\nleveraging compiler logic and abstractions when building structure into the\nmodel. We also develop an extension that includes more complex structure,\nrefining how the model generates identifier tokens based on what variables are\ncurrently in scope. Our models can be learned efficiently, and we show\nempirically that including appropriate structure greatly improves the models,\nmeasured by the probability of generating test programs.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2014 19:35:31 GMT"}, {"version": "v2", "created": "Fri, 20 Jun 2014 08:12:20 GMT"}], "update_date": "2014-06-23", "authors_parsed": [["Maddison", "Chris J.", ""], ["Tarlow", "Daniel", ""]]}, {"id": "1401.0968", "submitter": "EPTCS", "authors": "Rodrigo Casta\\~no (Departamento de Computaci\\'on. FCEyN. UBA), Juan\n  Pablo Galeotti (Saarland University), Diego Garbervetsky (Departamento de\n  Computaci\\'on. FCEyN. UBA), Jonathan Tapicer (Departamento de Computaci\\'on.\n  FCEyN. UBA), Edgardo Zoppi (Departamento de Computaci\\'on. FCEyN. UBA)", "title": "On Verifying Resource Contracts using Code Contracts", "comments": "In Proceedings LAFM 2013, arXiv:1401.0564", "journal-ref": "EPTCS 139, 2014, pp. 1-15", "doi": "10.4204/EPTCS.139.1", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an approach to check resource consumption contracts\nusing an off-the-shelf static analyzer.\n  We propose a set of annotations to support resource usage specifications, in\nparticular, dynamic memory consumption constraints. Since dynamic memory may be\nrecycled by a memory manager, the consumption of this resource is not monotone.\nThe specification language can express both memory consumption and lifetime\nproperties in a modular fashion.\n  We develop a proof-of-concept implementation by extending Code Contracts'\nspecification language. To verify the correctness of these annotations we rely\non the Code Contracts static verifier and a points-to analysis. We also briefly\ndiscuss possible extensions of our approach to deal with non-linear\nexpressions.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2014 02:31:56 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Casta\u00f1o", "Rodrigo", "", "Departamento de Computaci\u00f3n. FCEyN. UBA"], ["Galeotti", "Juan Pablo", "", "Saarland University"], ["Garbervetsky", "Diego", "", "Departamento de\n  Computaci\u00f3n. FCEyN. UBA"], ["Tapicer", "Jonathan", "", "Departamento de Computaci\u00f3n.\n  FCEyN. UBA"], ["Zoppi", "Edgardo", "", "Departamento de Computaci\u00f3n. FCEyN. UBA"]]}, {"id": "1401.1460", "submitter": "Jan Rochel", "authors": "Clemens Grabmayer and Jan Rochel", "title": "Maximal Sharing in the Lambda Calculus with letrec", "comments": "18 pages, plus 19 pages appendix", "journal-ref": null, "doi": "10.1145/2628136.2628148", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Increasing sharing in programs is desirable to compactify the code, and to\navoid duplication of reduction work at run-time, thereby speeding up execution.\nWe show how a maximal degree of sharing can be obtained for programs expressed\nas terms in the lambda calculus with letrec. We introduce a notion of `maximal\ncompactness' for lambda-letrec-terms among all terms with the same infinite\nunfolding. Instead of defined purely syntactically, this notion is based on a\ngraph semantics. lambda-letrec-terms are interpreted as first-order term graphs\nso that unfolding equivalence between terms is preserved and reflected through\nbisimilarity of the term graph interpretations. Compactness of the term graphs\ncan then be compared via functional bisimulation.\n  We describe practical and efficient methods for the following two problems:\ntransforming a lambda-letrec-term into a maximally compact form; and deciding\nwhether two lambda-letrec-terms are unfolding-equivalent. The transformation of\na lambda-letrec-term $L$ into maximally compact form $L_0$ proceeds in three\nsteps:\n  (i) translate L into its term graph $G = [[ L ]]$; (ii) compute the maximally\nshared form of $G$ as its bisimulation collapse $G_0$; (iii) read back a\nlambda-letrec-term $L_0$ from the term graph $G_0$ with the property $[[ L_0 ]]\n= G_0$. This guarantees that $L_0$ and $L$ have the same unfolding, and that\n$L_0$ exhibits maximal sharing.\n  The procedure for deciding whether two given lambda-letrec-terms $L_1$ and\n$L_2$ are unfolding-equivalent computes their term graph interpretations $[[\nL_1 ]]$ and $[[ L_2 ]]$, and checks whether these term graphs are bisimilar.\n  For illustration, we also provide a readily usable implementation.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2014 18:01:41 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2014 14:42:02 GMT"}, {"version": "v3", "created": "Thu, 10 Apr 2014 12:46:41 GMT"}, {"version": "v4", "created": "Thu, 17 Apr 2014 13:05:03 GMT"}, {"version": "v5", "created": "Mon, 23 Jun 2014 21:41:15 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Grabmayer", "Clemens", ""], ["Rochel", "Jan", ""]]}, {"id": "1401.2039", "submitter": "Konstantin Petrov", "authors": "Denis Barthou, Olivier Brand-Foissac, Romain Dolbeau, Gilbert\n  Grosdidier, Christina Eisenbeis, Michael Kruse, Olivier Pene, Konstantin\n  Petrov, Claude Tadonki", "title": "Automated Code Generation for Lattice Quantum Chromodynamics and beyond", "comments": null, "journal-ref": null, "doi": "10.1088/1742-6596/510/1/012005", "report-no": "LPT-Orsay-13-142, hal-00926513", "categories": "hep-lat cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present here our ongoing work on a Domain Specific Language which aims to\nsimplify Monte-Carlo simulations and measurements in the domain of Lattice\nQuantum Chromodynamics. The tool-chain, called Qiral, is used to produce\nhigh-performance OpenMP C code from LaTeX sources. We discuss conceptual issues\nand details of implementation and optimization. The comparison of the\nperformance of the generated code to the well-established simulation software\nis also made.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2014 15:56:31 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Barthou", "Denis", ""], ["Brand-Foissac", "Olivier", ""], ["Dolbeau", "Romain", ""], ["Grosdidier", "Gilbert", ""], ["Eisenbeis", "Christina", ""], ["Kruse", "Michael", ""], ["Pene", "Olivier", ""], ["Petrov", "Konstantin", ""], ["Tadonki", "Claude", ""]]}, {"id": "1401.2567", "submitter": "Danko Ilik", "authors": "Danko Ilik", "title": "Axioms and Decidability for Type Isomorphism in the Presence of Sums", "comments": null, "journal-ref": null, "doi": "10.1145/2603088.2603115", "report-no": null, "categories": "cs.LO cs.PL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of characterizing isomorphisms of types, or,\nequivalently, constructive cardinality of sets, in the simultaneous presence of\ndisjoint unions, Cartesian products, and exponentials. Mostly relying on\nresults about polynomials with exponentiation that have not been used in our\ncontext, we derive: that the usual finite axiomatization known as High-School\nIdentities (HSI) is complete for a significant subclass of types; that it is\ndecidable for that subclass when two types are isomorphic; that, for the whole\nof the set of types, a recursive extension of the axioms of HSI exists that is\ncomplete; and that, for the whole of the set of types, the question as to\nwhether two types are isomorphic is decidable when base types are to be\ninterpreted as finite sets. We also point out certain related open problems.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2014 20:56:42 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2014 07:36:43 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Ilik", "Danko", ""]]}, {"id": "1401.3041", "submitter": "Maxime Chevalier-Boisvert", "authors": "Maxime Chevalier-Boisvert, Marc Feeley", "title": "Removing Dynamic Type Tests with Context-Driven Basic Block Versioning", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic typing is an important feature of dynamic programming languages.\nPrimitive operators such as those for performing arithmetic and comparisons\ntypically operate on a wide variety of in put value types, and as such, must\ninternally implement some form of dynamic type dispatch and type checking.\nRemoving such type tests is important for an efficient implementation.\n  In this paper, we examine the effectiveness of a novel approach to reducing\nthe number of dynamically executed type tests called context-driven basic block\nversioning. This simple technique clones and specializes basic blocks in such a\nway as to allow the compiler to accumulate type information while machine code\nis generated, without a separate type analysis pass. The accumulated\ninformation allows the removal of some redundant type tests, particularly in\nperformance-critical paths.\n  We have implemented intraprocedural context-driven basic block versioning in\na JavaScript JIT compiler. For comparison, we have also implemented a classical\nflow-based type analysis operating on the same concrete types. Our results show\nthat basic block versioning performs better on most benchmarks and removes a\nlarge fraction of type tests at the expense of a moderate code size increase.\nWe believe that this technique offers a good tradeoff between implementation\ncomplexity and performance, and is suitable for integration in production JIT\ncompilers.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2014 00:25:38 GMT"}], "update_date": "2014-01-15", "authors_parsed": [["Chevalier-Boisvert", "Maxime", ""], ["Feeley", "Marc", ""]]}, {"id": "1401.4339", "submitter": "Abhishek Bichhawat", "authors": "Abhishek Bichhawat, Vineet Rajani, Deepak Garg and Christian Hammer", "title": "Information Flow Control in WebKit's JavaScript Bytecode", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Websites today routinely combine JavaScript from multiple sources, both\ntrusted and untrusted. Hence, JavaScript security is of paramount importance. A\nspecific interesting problem is information flow control (IFC) for JavaScript.\nIn this paper, we develop, formalize and implement a dynamic IFC mechanism for\nthe JavaScript engine of a production Web browser (specifically, Safari's\nWebKit engine). Our IFC mechanism works at the level of JavaScript bytecode and\nhence leverages years of industrial effort on optimizing both the source to\nbytecode compiler and the bytecode interpreter. We track both explicit and\nimplicit flows and observe only moderate overhead. Working with bytecode\nresults in new challenges including the extensive use of unstructured control\nflow in bytecode (which complicates lowering of program context taints),\nunstructured exceptions (which complicate the matter further) and the need to\nmake IFC analysis permissive. We explain how we address these challenges,\nformally model the JavaScript bytecode semantics and our instrumentation, prove\nthe standard property of termination-insensitive non-interference, and present\nexperimental results on an optimized prototype.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2014 13:38:08 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2014 09:01:09 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Bichhawat", "Abhishek", ""], ["Rajani", "Vineet", ""], ["Garg", "Deepak", ""], ["Hammer", "Christian", ""]]}, {"id": "1401.5097", "submitter": "Olle Fredriksson", "authors": "Olle Fredriksson", "title": "Distributed call-by-value machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new abstract machine, called DCESH, which describes the\nexecution of higher-order programs running in distributed architectures. DCESH\nimplements a generalised form of Remote Procedure Call that supports calling\nhigher-order functions across node boundaries, without sending actual code. Our\nstarting point is a variant of the SECD machine that we call the CES machine,\nwhich implements reduction for untyped call-by-value PCF. We successively add\nthe features that we need for distributed execution and show the correctness of\neach addition. First we add heaps, forming the CESH machine, which provides\nfeatures necessary for more efficient execution, and show that there is a\nbisimulation between the CES and the CESH machine. Then we construct a\ntwo-level operational semantics, where the high level is a network of\ncommunicating machines, and the low level is given by local machine\ntransitions. Using these networks, we arrive at our final system, the\ndistributed CESH machine (DCESH). We show that there is a bisimulation relation\nalso between the CESH machine and the DCESH machine. All the technical results\nhave been formalised and proved correct in Agda, and a prototype compiler has\nbeen developed.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 21:32:38 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Fredriksson", "Olle", ""]]}, {"id": "1401.5107", "submitter": "Martin Hofmann", "authors": "Martin Hofmann and Wei Chen", "title": "B\\\"uchi Types for Infinite Traces and Liveness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new type and effect system based on B\\\"uchi automata to capture\nfinite and infinite traces produced by programs in a small language which\nallows non-deterministic choices and infinite recursions. There are two key\ntechnical contributions: (a) an abstraction based on equivalence relations\ndefined by the policy B\\\"uchi automaton, the B\\\"uchi abstraction; (b) a novel\ntype and effect system to correctly capture infinite traces. We show how the\nB\\\"uchi abstraction fits into the abstract interpretation framework and show\nsoundness and completeness.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2014 22:06:15 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Hofmann", "Martin", ""], ["Chen", "Wei", ""]]}, {"id": "1401.5292", "submitter": "Etienne Payet", "authors": "\\'Etienne Payet, Fred Mesnard, Fausto Spoto", "title": "Non-Termination Analysis of Java Bytecode", "comments": "17 pages, technical report written in May 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a fully automated static analysis that takes a sequential Java\nbytecode program P as input and attempts to prove that there exists an infinite\nexecution of P. The technique consists in compiling P into a constraint logic\nprogram P_CLP and in proving non-termination of P_CLP; when P consists of\ninstructions that are exactly compiled into constraints, the non-termination of\nP_CLP entails that of P. Our approach can handle method calls; to the best of\nour knowledge, it is the first static approach for Java bytecode able to prove\nthe existence of infinite recursions. We have implemented our technique inside\nthe Julia analyser. We have compared the results of Julia on a set of 113\nprograms with those provided by AProVE and Invel, the only freely usable\nnon-termination analysers comparable to ours that we are aware of. Only Julia\ncould detect non-termination due to infinite recursion.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 12:35:09 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Payet", "\u00c9tienne", ""], ["Mesnard", "Fred", ""], ["Spoto", "Fausto", ""]]}, {"id": "1401.5300", "submitter": "Yanqing Wang", "authors": "Yanqing Wang, Chong Wang, Xiaojie Li, Sijing Yun, Minjing Song", "title": "How are identifiers named in open source software? About popularity and\n  consistency", "comments": "10 pages, 3 figures, 5 tables", "journal-ref": "International Journal of Computer and Information Technology, vol\n  3, no 3, 2014", "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid increasing of software project size and maintenance cost,\nadherence to coding standards especially by managing identifier naming, is\nattracting a pressing concern from both computer science educators and software\nmanagers. Software developers mainly use identifier names to represent the\nknowledge recorded in source code. However, the popularity and adoption\nconsistency of identifier naming conventions have not been revealed yet in this\nfield. Taking forty-eight popular open source projects written in three\ntop-ranking programming languages Java, C and C++ as examples, an identifier\nextraction tool based on regular expression matching is developed. In the\nsubsequent investigation, some interesting findings are obtained. For the\nidentifier naming popularity, it is found that Camel and Pascal naming\nconventions are leading the road while Hungarian notation is vanishing. For the\nidentifier naming consistency, we have found that the projects written in Java\nhave a much better performance than those written in C and C++. Finally,\nacademia and software industry are urged to adopt the most popular naming\nconventions consistently in their practices so as to lead the identifier naming\nto a standard, unified and high-quality road.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 13:02:36 GMT"}, {"version": "v2", "created": "Sat, 31 May 2014 05:49:41 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Wang", "Yanqing", ""], ["Wang", "Chong", ""], ["Li", "Xiaojie", ""], ["Yun", "Sijing", ""], ["Song", "Minjing", ""]]}, {"id": "1401.5334", "submitter": "Laurent  Michel D", "authors": "Laurent Michel and Pascal Van Hentenryck", "title": "A Microkernel Architecture for Constraint Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a microkernel architecture for constraint programming\norganized around a number of small number of core functionalities and minimal\ninterfaces. The architecture contrasts with the monolithic nature of many\nimplementations. Experimental results indicate that the software engineering\nbenefits are not incompatible with runtime efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 14:56:14 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Michel", "Laurent", ""], ["Van Hentenryck", "Pascal", ""]]}, {"id": "1401.5341", "submitter": "Laurent  Michel D", "authors": "Pascal Van Hentenryck and Laurent Michel", "title": "Domain Views for Constraint Programming", "comments": "Workshop: TRICS13: Techniques foR Implementing Constraint\n  programming, September 2013, CP, Uppsala", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Views are a standard abstraction in constraint programming: They make it\npossible to implement a single version of each constraint, while avoiding to\ncreate new variables and constraints that would slow down propagation.\nTraditional constraint-programming systems provide the concept of {\\em variable\nviews} which implement a view of the type $y = f(x)$ by delegating all (domain\nand constraint) operations on variable $y$ to variable $x$. This paper proposes\nthe alternative concept of {\\em domain views} which only delegate domain\noperations. Domain views preserve the benefits of variable views but simplify\nthe implementation of value-based propagation. Domain views also support\nnon-injective views compositionally, expanding the scope of views\nsignificantly. Experimental results demonstrate the practical benefits of\ndomain views.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 15:22:29 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Van Hentenryck", "Pascal", ""], ["Michel", "Laurent", ""]]}, {"id": "1401.5391", "submitter": "Dominic Orchard", "authors": "Dominic Orchard, Tomas Petricek, Alan Mycroft", "title": "The semantic marriage of monads and effects", "comments": "extended abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wadler and Thiemann unified type-and-effect systems with monadic semantics\nvia a syntactic correspondence and soundness results with respect to an\noperational semantics. They conjecture that a general, \"coherent\" denotational\nsemantics can be given to unify effect systems with a monadic-style semantics.\nWe provide such a semantics based on the novel structure of an indexed monad,\nwhich we introduce. We redefine the semantics of Moggi's computational\nlambda-calculus in terms of (strong) indexed monads which gives a one-to-one\ncorrespondence between indices of the denotations and the effect annotations of\ntraditional effect systems. Dually, this approach yields indexed comonads which\ngives a unified semantics and effect system to contextual notions of effect\n(called coeffects), which we have previously described.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2014 17:23:45 GMT"}], "update_date": "2014-01-22", "authors_parsed": [["Orchard", "Dominic", ""], ["Petricek", "Tomas", ""], ["Mycroft", "Alan", ""]]}, {"id": "1401.5842", "submitter": "Florian Zuleger", "authors": "Moritz Sinn, Florian Zuleger, Helmut Veith", "title": "A Simple and Scalable Static Analysis for Bound Analysis and Amortized\n  Complexity Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first scalable bound analysis that achieves amortized\ncomplexity analysis. In contrast to earlier work, our bound analysis is not\nbased on general purpose reasoners such as abstract interpreters, software\nmodel checkers or computer algebra tools. Rather, we derive bounds directly\nfrom abstract program models, which we obtain from programs by comparatively\nsimple invariant generation and symbolic execution techniques. As a result, we\nobtain an analysis that is more predictable and more scalable than earlier\napproaches. Our experiments demonstrate that our analysis is fast and at the\nsame time able to compute bounds for challenging loops in a large real-world\nbenchmark. Technically, our approach is based on lossy vector addition systems\n(VASS). Our bound analysis first computes a lexicographic ranking function that\nproves the termination of a VASS, and then derives a bound from this ranking\nfunction. Our methodology achieves amortized analysis based on a new insight\nhow lexicographic ranking functions can be used for bound analysis.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2014 01:40:43 GMT"}, {"version": "v2", "created": "Mon, 2 Jun 2014 20:44:29 GMT"}], "update_date": "2014-06-04", "authors_parsed": [["Sinn", "Moritz", ""], ["Zuleger", "Florian", ""], ["Veith", "Helmut", ""]]}, {"id": "1401.6227", "submitter": "Niki Vazou", "authors": "Niki Vazou, Eric L. Seidel, Ranjit Jhala", "title": "From Safety To Termination And Back: SMT-Based Verification For Lazy\n  Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SMT-based verifiers have long been an effective means of ensuring safety\nproperties of programs. While these techniques are well understood, we show\nthat they implicitly require eager semantics; directly applying them to a lazy\nlanguage is unsound due to the presence of divergent sub-computations. We\nrecover soundness by composing the safety analysis with a termination analysis.\nOf course, termination is itself a challenging problem, but we show how the\nsafety analysis can be used to ensure termination, thereby bootstrapping\nsoundness for the entire system. Thus, while safety invariants have long been\nrequired to prove termination, we show how termination proofs can be to soundly\nestablish safety. We have implemented our approach in liquidHaskell, a\nRefinement Type-based verifier for Haskell. We demonstrate its effectiveness\nvia an experimental evaluation using liquidHaskell to verify safety, functional\ncorrectness and termination properties of real-world Haskell libraries,\ntotaling over 10,000 lines of code.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2014 00:45:45 GMT"}], "update_date": "2014-01-27", "authors_parsed": [["Vazou", "Niki", ""], ["Seidel", "Eric L.", ""], ["Jhala", "Ranjit", ""]]}, {"id": "1401.6325", "submitter": "Jonathan Kochems", "authors": "Jonathan Kochems, C-H Luke Ong", "title": "Safety verification of asynchronous pushdown systems with shaped stacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the program-point reachability problem of concurrent\npushdown systems that communicate via unbounded and unordered message buffers.\nOur goal is to relax the common restriction that messages can only be retrieved\nby a pushdown process when its stack is empty. We use the notion of partially\ncommutative context-free grammars to describe a new class of asynchronously\ncommunicating pushdown systems with a mild shape constraint on the stacks for\nwhich the program-point coverability problem remains decidable. Stacks that fit\nthe shape constraint may reach arbitrary heights; further a process may execute\nany communication action (be it process creation, message send or retrieval)\nwhether or not its stack is empty. This class extends previous computational\nmodels studied in the context of asynchronous programs, and enables the safety\nverification of a large class of message passing programs.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2014 12:26:27 GMT"}], "update_date": "2014-01-27", "authors_parsed": [["Kochems", "Jonathan", ""], ["Ong", "C-H Luke", ""]]}]