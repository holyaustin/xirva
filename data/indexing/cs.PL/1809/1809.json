[{"id": "1809.00821", "submitter": "Roberto Bagnara", "authors": "Roberto Bagnara and Abramo Bagnara and Patricia M. Hill", "title": "The MISRA C Coding Standard and its Role in the Development and Analysis\n  of Safety- and Security-Critical Embedded Software", "comments": "19 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MISRA project started in 1990 with the mission of providing world-leading\nbest practice guidelines for the safe and secure application of both embedded\ncontrol systems and standalone software. MISRA C is a coding standard defining\na subset of the C language, initially targeted at the automotive sector, but\nnow adopted across all industry sectors that develop C software in safety-\nand/or security-critical contexts. In this paper, we introduce MISRA C, its\nrole in the development of critical software, especially in embedded systems,\nits relevance to industry safety standards, as well as the challenges of\nworking with a general-purpose programming language standard that is written in\nnatural language with a slow evolution over the last 40+ years. We also outline\nthe role of static analysis in the automatic checking of compliance with\nrespect to MISRA C, and the role of the MISRA C language subset in enabling a\nwider application of formal methods to industrial software written in C.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 07:50:30 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Bagnara", "Roberto", ""], ["Bagnara", "Abramo", ""], ["Hill", "Patricia M.", ""]]}, {"id": "1809.00959", "submitter": "Meng Wang", "authors": "Meng Wang, Cong Tian, Nan Zhang, Zhenhua Duan, Chenguang Yao", "title": "Translating Xd-C programs to MSVL programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  C language is one of the most popular languages for software systems. In\norder to verify safety, reliability and security properties of such systems\nwritten in C, a tool UMC4M for runtime verification at code level based on\nModeling, Simulation and Verification Language (MSVL) and its compiler MC is\nemployed. To do so, a C program $P$ has to be translated to an MSVL program M\nand the negation of a desired property $Q$ is also translated to an MSVL\nprogram M', then \"M and M\" is compiled and executed armed with MC. Whether $P$\nviolates $Q$ is checked by evaluating whether there exists an acceptable\nexecution of new MSVL program M and M\". Therefore, how to translate a C program\nto an MSVL program is a critical issue. However, in general, C is of\ncomplicated structures with goto statement. In this paper, we confine the\nsyntax of C in a suitable subset called Xd-C without loss of expressiveness.\nFurther, we present a translation algorithm from an Xd-C program to an MSVL\nprogram based on translation algorithms for expressions and statements.\nMoreover, the equivalences between expressions and statements involved in Xd-C\nand MSVL programs are inductively proved. Subsequently, the equivalence between\nthe original Xd-C program and the translated MSVL program is also proved. In\naddition, the proposed approach has been implemented by a tool called $C2M$. A\nbenchmark of experiments including 13 real-world Xd-C programs is conducted.\nThe results show that $C2M$ works effectively.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 07:07:48 GMT"}, {"version": "v2", "created": "Sat, 19 Jan 2019 10:15:57 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Wang", "Meng", ""], ["Tian", "Cong", ""], ["Zhang", "Nan", ""], ["Duan", "Zhenhua", ""], ["Yao", "Chenguang", ""]]}, {"id": "1809.00976", "submitter": "Kostadin Kratchanov", "authors": "Kostadin Kratchanov, Efe Erg\\\"un", "title": "Language Interoperability in Control Network Programming", "comments": "12 pages", "journal-ref": "Int. J. of Science and Engineering Investigation, vol. 7, issue\n  78, Jul. 2018, 79 - 90", "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Control Network Programming (CNP) is a programming paradigm which is being\ndescribed with the maxim \"Primitives + Control Network = Control Network\nprogram\". It is a type of graphic programming. The Control Network is a\nrecursive system of graphs; it can be a purely descriptive specification of the\nproblem being solved. Clearly, \"drawing\" the control network does not include\nany programming. The Primitives are elementary, easily understandable and\nclearly specified actions. Ultimately, they have to be programmed.\nHistorically, they are usually coded in Free Pascal. The actual code of the\nprimitives has never been considered important. The essence of an \"algorithm\"\nis represented by its control network. CNP was always meant to be an easy and\nfast approach for software application development that actually involves very\nlittle real programming. Language interoperability (using different languages\nin the same software project) is a distinguished current trend in software\ndevelopment. It is even more important and natural in the case of CNP than for\nother programming paradigms. Here, interoperability practically means the\npossibility to use primitives written in various programming languages. The\ncurrent report describes our first steps in creating applications using a\nmulti-language set of primitives. Most popular and interesting programming\nlanguages have been addressed: Python, Java, and C. We show how to create\napplications with primitives written in those \"non-native\" languages. We\nconsider examples where the primitives in all those four programming languages\nare simultaneously used (multiple-language CNP). We also discuss CNP\nprogramming without programming (language-free CNP).\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 17:35:25 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Kratchanov", "Kostadin", ""], ["Erg\u00fcn", "Efe", ""]]}, {"id": "1809.01427", "submitter": "Giuseppe Castagna", "authors": "Giuseppe Castagna", "title": "Covariance and Controvariance: a fresh look at an old issue (a primer in\n  advanced type systems for learning functional programmers)", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 1 (February\n  13, 2020) lmcs:6098", "doi": "10.23638/LMCS-16(1:15)2020", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Twenty years ago, in an article titled \"Covariance and contravariance:\nconflict without a cause\", I argued that covariant and contravariant\nspecialization of method parameters in object-oriented programming had\ndifferent purposes and deduced that, not only they could, but actually they\nshould both coexist in the same language.\n  In this work I reexamine the result of that article in the light of recent\nadvances in (sub-)typing theory and programming languages, taking a fresh look\nat this old issue.\n  Actually, the revamping of this problem is just an excuse for writing an\nessay that aims at explaining sophisticated type-theoretic concepts, in simple\nterms and by examples, to undergraduate computer science students and/or\nwilling functional programmers.\n  Finally, I took advantage of this opportunity to describe some undocumented\nadvanced techniques of type-systems implementation that are known only to few\ninsiders that dug in the code of some compilers: therefore, even expert\nlanguage designers and implementers may find this work worth of reading.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 10:40:07 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 07:14:36 GMT"}, {"version": "v3", "created": "Mon, 10 Sep 2018 16:38:34 GMT"}, {"version": "v4", "created": "Wed, 21 Nov 2018 09:54:33 GMT"}, {"version": "v5", "created": "Fri, 3 May 2019 11:45:33 GMT"}, {"version": "v6", "created": "Thu, 1 Aug 2019 14:11:51 GMT"}, {"version": "v7", "created": "Wed, 12 Feb 2020 13:55:29 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Castagna", "Giuseppe", ""]]}, {"id": "1809.01955", "submitter": "Patrick Metzler", "authors": "Patrick Metzler, Habib Saissi, P\\'eter Bokor, Neeraj Suri", "title": "Safe Execution of Concurrent Programs by Enforcement of Scheduling\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated software verification of concurrent programs is challenging because\nof exponentially large state spaces with respect to the number of threads and\nnumber of events per thread. Verification techniques such as model checking\nneed to explore a large number of possible executions that are possible under a\nnon-deterministic scheduler. State space reduction techniques such as partial\norder reduction simplify the verification problem, however, the reduced state\nspace may still be exponentially large and intractable.\n  This paper discusses \\emph{Iteratively Relaxed Scheduling}, a framework that\nuses scheduling constraints in order to simplify the verification problem and\nenable automated verification of programs which could not be handled with fully\nnon-deterministic scheduling. Program executions are safe as long as the same\nscheduling constraints are enforced under which the program has been verified,\ne.g., by instrumenting a program with additional synchronization. As strict\nenforcement of scheduling constraints may induce a high execution time\noverhead, we present optimizations over a naive solution that reduce this\noverhead. Our evaluation of a prototype implementation on well-known benchmark\nprograms shows the effect of scheduling constraints on the execution time\noverhead and how this overhead can be reduced by relaxing and choosing\nconstraints.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 12:49:01 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 14:44:47 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Metzler", "Patrick", ""], ["Saissi", "Habib", ""], ["Bokor", "P\u00e9ter", ""], ["Suri", "Neeraj", ""]]}, {"id": "1809.02161", "submitter": "John Regehr", "authors": "Nuno P. Lopes and John Regehr", "title": "Future Directions for Optimizing Compilers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As software becomes larger, programming languages become higher-level, and\nprocessors continue to fail to be clocked faster, we'll increasingly require\ncompilers to reduce code bloat, eliminate abstraction penalties, and exploit\ninteresting instruction sets. At the same time, compiler execution time must\nnot increase too much and also compilers should never produce the wrong output.\nThis paper examines the problem of making optimizing compilers faster, less\nbuggy, and more capable of generating high-quality output.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 18:29:54 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Lopes", "Nuno P.", ""], ["Regehr", "John", ""]]}, {"id": "1809.02283", "submitter": "Yuepeng Wang", "authors": "Yuepeng Wang, Xinyu Wang, Isil Dillig", "title": "Relational Program Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes relational program synthesis, a new problem that concerns\nsynthesizing one or more programs that collectively satisfy a relational\nspecification. As a dual of relational program verification, relational program\nsynthesis is an important problem that has many practical applications, such as\nautomated program inversion and automatic generation of comparators. However,\nthis relational synthesis problem introduces new challenges over its\nnon-relational counterpart due to the combinatorially larger search space. As a\nfirst step towards solving this problem, this paper presents a synthesis\ntechnique that combines the counterexample-guided inductive synthesis framework\nwith a novel inductive synthesis algorithm that is based on relational version\nspace learning. We have implemented the proposed technique in a framework\ncalled Relish, which can be instantiated to different application domains by\nproviding a suitable domain-specific language and the relevant relational\nspecification. We have used the Relish framework to build relational\nsynthesizers to automatically generate string encoders/decoders as well as\ncomparators, and we evaluate our tool on several benchmarks taken from prior\nwork and online forums. Our experimental results show that the proposed\ntechnique can solve almost all of these benchmarks and that it significantly\noutperforms EUSolver, a generic synthesis framework that won the general track\nof the most recent SyGuS competition.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 02:43:01 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 00:48:40 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Wang", "Yuepeng", ""], ["Wang", "Xinyu", ""], ["Dillig", "Isil", ""]]}, {"id": "1809.02613", "submitter": "Yusuke Kawamoto", "authors": "Fabrizio Biondi, Yusuke Kawamoto, Axel Legay, Louis-Marie Traonouez", "title": "Hybrid Statistical Estimation of Mutual Information and its Application\n  to Information Flow", "comments": "Accepted by Formal Aspects of Computing", "journal-ref": null, "doi": "10.1007/s00165-018-0469-z", "report-no": null, "categories": "cs.IT cs.CR cs.PL math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of a probabilistic system often requires to learn the joint\nprobability distribution of its random variables. The computation of the exact\ndistribution is usually an exhaustive precise analysis on all executions of the\nsystem. To avoid the high computational cost of such an exhaustive search,\nstatistical analysis has been studied to efficiently obtain approximate\nestimates by analyzing only a small but representative subset of the system's\nbehavior. In this paper we propose a hybrid statistical estimation method that\ncombines precise and statistical analyses to estimate mutual information,\nShannon entropy, and conditional entropy, together with their confidence\nintervals. We show how to combine the analyses on different components of a\ndiscrete system with different accuracy to obtain an estimate for the whole\nsystem. The new method performs weighted statistical analysis with different\nsample sizes over different components and dynamically finds their optimal\nsample sizes. Moreover, it can reduce sample sizes by using prior knowledge\nabout systems and a new abstraction-then-sampling technique based on\nqualitative analysis. To apply the method to the source code of a system, we\nshow how to decompose the code into components and to determine the analysis\nmethod for each component by overviewing the implementation of those techniques\nin the HyLeak tool. We demonstrate with case studies that the new method\noutperforms the state of the art in quantifying information leakage.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 07:16:33 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Biondi", "Fabrizio", ""], ["Kawamoto", "Yusuke", ""], ["Legay", "Axel", ""], ["Traonouez", "Louis-Marie", ""]]}, {"id": "1809.02781", "submitter": "Ale\\v{s} Bizjak", "authors": "Dimitris Mostrous, Vasco T. Vasconcelos", "title": "Affine Sessions", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 14, Issue 4 (November\n  15, 2018) lmcs:4973", "doi": "10.23638/LMCS-14(4:14)2018", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Session types describe the structure of communications implemented by\nchannels. In particular, they prescribe the sequence of communications, whether\nthey are input or output actions, and the type of value exchanged. Crucial to\nany language with session types is the notion of linearity, which is essential\nto ensure that channels exhibit the behaviour prescribed by their type without\ninterference in the presence of concurrency. In this work we relax the\ncondition of linearity to that of affinity, by which channels exhibit at most\nthe behaviour prescribed by their types. This more liberal setting allows us to\nincorporate an elegant error handling mechanism which simplifies and improves\nrelated works on exceptions. Moreover, our treatment does not affect the\nprogress properties of the language: sessions never get stuck.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2018 09:39:30 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 10:46:33 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Mostrous", "Dimitris", ""], ["Vasconcelos", "Vasco T.", ""]]}, {"id": "1809.03252", "submitter": "Satoshi Egi", "authors": "Satoshi Egi", "title": "Loop Patterns: Extension of Kleene Star Operator for More Expressive\n  Pattern Matching against Arbitrary Data Structures", "comments": "14 pages, Scheme and Functional Programming Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kleene star operator is an important pattern construct for representing a\npattern that repeats multiple times. Due to its simplicity and usefulness, it\nis imported into various pattern-matching systems other than regular\nexpressions. For example, Mathematica has a similar pattern construct called\nthe repeated pattern. However, they have the following limitations: (i) We\ncannot change the pattern repeated depending on the current repeat count, and\n(ii) we cannot apply them to arbitrary data structures such as trees and graphs\nother than lists. This paper proposes the loop patterns that overcome these\nlimitations. This paper presents numerous working examples and formal semantics\nof the loop patterns. The examples in this paper are coded in the Egison\nprogramming language, which features the customizable non-linear\npattern-matching facility for non-free data types.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 11:59:47 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Egi", "Satoshi", ""]]}, {"id": "1809.03981", "submitter": "Bernhard Scholz", "authors": "Lexi Brent, Anton Jurisevic, Michael Kong, Eric Liu, Francois\n  Gauthier, Vincent Gramoli, Ralph Holz, Bernhard Scholz", "title": "Vandal: A Scalable Security Analysis Framework for Smart Contracts", "comments": "28 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of modern blockchains has facilitated the emergence of smart\ncontracts: autonomous programs that live and run on the blockchain. Smart\ncontracts have seen a rapid climb to prominence, with applications predicted in\nlaw, business, commerce, and governance.\n  Smart contracts are commonly written in a high-level language such as\nEthereum's Solidity, and translated to compact low-level bytecode for\ndeployment on the blockchain. Once deployed, the bytecode is autonomously\nexecuted, usually by a %Turing-complete virtual machine. As with all programs,\nsmart contracts can be highly vulnerable to malicious attacks due to deficient\nprogramming methodologies, languages, and toolchains, including buggy\ncompilers. At the same time, smart contracts are also high-value targets, often\ncommanding large amounts of cryptocurrency. Hence, developers and auditors need\nsecurity frameworks capable of analysing low-level bytecode to detect potential\nsecurity vulnerabilities.\n  In this paper, we present Vandal: a security analysis framework for Ethereum\nsmart contracts. Vandal consists of an analysis pipeline that converts\nlow-level Ethereum Virtual Machine (EVM) bytecode to semantic logic relations.\nUsers of the framework can express security analyses in a declarative fashion:\na security analysis is expressed in a logic specification written in the\n\\souffle language. We conduct a large-scale empirical study for a set of common\nsmart contract security vulnerabilities, and show the effectiveness and\nefficiency of Vandal. Vandal is both fast and robust, successfully analysing\nover 95\\% of all 141k unique contracts with an average runtime of 4.15 seconds;\noutperforming the current state of the art tools---Oyente, EthIR, Mythril, and\nRattle---under equivalent conditions.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 15:39:35 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Brent", "Lexi", ""], ["Jurisevic", "Anton", ""], ["Kong", "Michael", ""], ["Liu", "Eric", ""], ["Gauthier", "Francois", ""], ["Gramoli", "Vincent", ""], ["Holz", "Ralph", ""], ["Scholz", "Bernhard", ""]]}, {"id": "1809.04059", "submitter": "Vaibhav Rastogi", "authors": "Jinman Zhao, Aws Albarghouthi, Vaibhav Rastogi, Somesh Jha, Damien\n  Octeau", "title": "Neural-Augmented Static Analysis of Android Communication", "comments": "Appears in Proceedings of the 2018 ACM Joint European Software\n  Engineering Conference and Symposium on the Foundations of Software\n  Engineering (ESEC/FSE)", "journal-ref": null, "doi": "10.1145/3236024.3236066", "report-no": null, "categories": "cs.PL cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of discovering communication links between\napplications in the popular Android mobile operating system, an important\nproblem for security and privacy in Android. Any scalable static analysis in\nthis complex setting is bound to produce an excessive amount of\nfalse-positives, rendering it impractical. To improve precision, we propose to\naugment static analysis with a trained neural-network model that estimates the\nprobability that a communication link truly exists. We describe a\nneural-network architecture that encodes abstractions of communicating objects\nin two applications and estimates the probability with which a link indeed\nexists. At the heart of our architecture are type-directed encoders (TDE), a\ngeneral framework for elegantly constructing encoders of a compound data type\nby recursively composing encoders for its constituent types. We evaluate our\napproach on a large corpus of Android applications, and demonstrate that it\nachieves very high accuracy. Further, we conduct thorough interpretability\nstudies to understand the internals of the learned neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 17:50:47 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Zhao", "Jinman", ""], ["Albarghouthi", "Aws", ""], ["Rastogi", "Vaibhav", ""], ["Jha", "Somesh", ""], ["Octeau", "Damien", ""]]}, {"id": "1809.04151", "submitter": "Leif Andersen", "authors": "Leif Andersen, Vincent St-Amour, Jan Vitek, Matthias Felleisen", "title": "Feature-Specific Profiling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While high-level languages come with significant readability and\nmaintainability benefits, their performance remains difficult to predict. For\nexample, programmers may unknowingly use language features inappropriately,\nwhich cause their programs to run slower than expected. To address this issue,\nwe introduce feature-specific profiling, a technique that reports performance\ncosts in terms of linguistic constructs. Feature-specific profilers help\nprogrammers find expensive uses of specific features of their language. We\ndescribe the architecture of a profiler that implements our approach, explain\nprototypes of the profiler for two languages with different characteristics and\nimplementation strategies, and provide empirical evidence for the approach's\ngeneral usefulness as a performance debugging tool.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 20:47:06 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Andersen", "Leif", ""], ["St-Amour", "Vincent", ""], ["Vitek", "Jan", ""], ["Felleisen", "Matthias", ""]]}, {"id": "1809.04193", "submitter": "Chu-Pan Wong", "authors": "Chu-Pan Wong, Jens Meinicke, Lukas Lazarek, Christian K\\\"astner", "title": "Faster Variational Execution with Transparent Bytecode Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational execution is a novel dynamic analysis technique for exploring\nhighly configurable systems and accurately tracking information flow. It is\nable to efficiently analyze many configurations by aggressively sharing\nredundancies of program executions. The idea of variational execution has been\ndemonstrated to be effective in exploring variations in the program, especially\nwhen the configuration space grows out of control. Existing implementations of\nvariational execution often require heavy lifting of the runtime interpreter,\nwhich is painstaking and error-prone. Furthermore, the performance of this\napproach is suboptimal. For example, the state-of-the-art variational execution\ninterpreter for Java, VarexJ, slows down executions by 100 to 800 times over a\nsingle execution for small to medium size Java programs. Instead of modifying\nexisting JVMs, we propose to transform existing bytecode to make it\nvariational, so it can be executed on an unmodified commodity JVM. Our\nevaluation shows a dramatic improvement on performance over the\nstate-of-the-art, with a speedup of 2 to 46 times, and high efficiency in\nsharing computations.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 22:54:37 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Wong", "Chu-Pan", ""], ["Meinicke", "Jens", ""], ["Lazarek", "Lukas", ""], ["K\u00e4stner", "Christian", ""]]}, {"id": "1809.04209", "submitter": "Ravi Chugh", "authors": "Mika\\\"el Mayer and Viktor Kun\\v{c}ak and Ravi Chugh", "title": "Bidirectional Evaluation with Direct Manipulation", "comments": "OOPSLA 2018 Paper + Supplementary Appendix", "journal-ref": null, "doi": "10.1145/3276497", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an evaluation update (or simply, update) algorithm for a\nfull-featured functional programming language, which synthesizes program\nchanges based on output changes. Intuitively, the update algorithm retraces the\nsteps of the original evaluation, rewriting the program as needed to reconcile\ndifferences between the original and updated output values. Our approach,\nfurthermore, allows expert users to define custom lenses that augment the\nupdate algorithm with more advanced or domain-specific program updates.\n  To demonstrate the utility of evaluation update, we implement the algorithm\nin Sketch-n-Sketch, a novel direct manipulation programming system for\ngenerating HTML documents. In Sketch-n-Sketch, the user writes an ML-style\nfunctional program to generate HTML output. When the user directly manipulates\nthe output using a graphical user interface, the update algorithm reconciles\nthe changes. We evaluate bidirectional evaluation in Sketch-n-Sketch by\nauthoring ten examples comprising approximately 1400 lines of code in total.\nThese examples demonstrate how a variety of HTML documents and applications can\nbe developed and edited interactively in Sketch-n-Sketch, mitigating the\ntedious edit-run-view cycle in traditional programming environments.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 00:58:55 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2018 21:03:21 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Mayer", "Mika\u00ebl", ""], ["Kun\u010dak", "Viktor", ""], ["Chugh", "Ravi", ""]]}, {"id": "1809.04554", "submitter": "EPTCS", "authors": "Temesghen Kahsai (Amazon), German Vidal (Universitat Politecnica de\n  Valencia)", "title": "Proceedings 5th Workshop on Horn Clauses for Verification and Synthesis", "comments": null, "journal-ref": "EPTCS 278, 2018", "doi": "10.4204/EPTCS.278", "report-no": null, "categories": "cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Program Verification and Synthesis problems of interest can be modeled\ndirectly using Horn clauses and many recent advances in the CLP and CAV\ncommunities have centered around efficiently solving problems presented as Horn\nclauses.\n  The HCVS series of workshops aims to bring together researchers working in\nthe two communities of Constraint/Logic Programming (e.g., ICLP and CP),\nProgram Verification (e.g., CAV, TACAS, and VMCAI), and Automated Deduction\n(e.g., CADE, IJCAR), on the topic of Horn clause based analysis, verification,\nand synthesis.\n  Horn clauses for verification and synthesis have been advocated by these\ncommunities in different times and from different perspectives and HCVS is\norganized to stimulate interaction and a fruitful exchange and integration of\nexperiences.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 16:44:51 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Kahsai", "Temesghen", "", "Amazon"], ["Vidal", "German", "", "Universitat Politecnica de\n  Valencia"]]}, {"id": "1809.04676", "submitter": "Sergey Pupyrev", "authors": "Andy Newell and Sergey Pupyrev", "title": "Improved Basic Block Reordering", "comments": "Published in IEEE Transactions on Computers", "journal-ref": null, "doi": "10.1109/TC.2020.2982888", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basic block reordering is an important step for profile-guided binary\noptimization. The state-of-the-art goal for basic block reordering is to\nmaximize the number of fall-through branches. However, we demonstrate that such\norderings may impose suboptimal performance on instruction and I-TLB caches. We\npropose a new algorithm that relies on a model combining the effects of\nfall-through and caching behavior. As details of modern processor caching is\nquite complex and often unknown, we show how to use machine learning in\nselecting parameters that best trade off different caching effects to maximize\nbinary performance.\n  An extensive evaluation on a variety of applications, including Facebook\nproduction workloads, the open-source compilers Clang and GCC, and SPEC CPU\nbenchmarks, indicate that the new method outperforms existing block reordering\ntechniques, improving the resulting performance of applications with large code\nsize. We have open sourced the code of the new algorithm as a part of a\npost-link binary optimization tool, BOLT.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 21:09:19 GMT"}, {"version": "v2", "created": "Sat, 11 Apr 2020 18:22:51 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Newell", "Andy", ""], ["Pupyrev", "Sergey", ""]]}, {"id": "1809.04682", "submitter": "Amit Zohar", "authors": "Amit Zohar, Lior Wolf", "title": "Automatic Program Synthesis of Long Programs with a Learned Garbage\n  Collector", "comments": "Published at NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of generating automatic code given sample\ninput-output pairs. We train a neural network to map from the current state and\nthe outputs to the program's next statement. The neural network optimizes\nmultiple tasks concurrently: the next operation out of a set of high level\ncommands, the operands of the next statement, and which variables can be\ndropped from memory. Using our method we are able to create programs that are\nmore than twice as long as existing state-of-the-art solutions, while improving\nthe success rate for comparable lengths, and cutting the run-time by two orders\nof magnitude. Our code, including an implementation of various literature\nbaselines, is publicly available at https://github.com/amitz25/PCCoder\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 21:25:28 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 15:58:09 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Zohar", "Amit", ""], ["Wolf", "Lior", ""]]}, {"id": "1809.04770", "submitter": "EPTCS", "authors": "Emanuele De Angelis (DEC, University \"G. d'Annunzio\" of\n  Chieti-Pescara, Pescara, Italy), Fabio Fioravanti (DEC, University \"G.\n  d'Annunzio\" of Chieti-Pescara, Pescara, Italy), Adri\\'an Palacios (MiST,\n  DSIC, Universitat Polit\\`ecnica de Val\\`encia, Val\\`encia, Spain), Alberto\n  Pettorossi (University of Roma Tor Vergata, Roma, Italy), Maurizio Proietti\n  (CNR-IASI, Roma, Italy)", "title": "Bounded Symbolic Execution for Runtime Error Detection of Erlang\n  Programs", "comments": "In Proceedings HCVS 2018, arXiv:1809.04554", "journal-ref": "EPTCS 278, 2018, pp. 19-26", "doi": "10.4204/EPTCS.278.4", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamically typed languages, like Erlang, allow developers to quickly write\nprograms without explicitly providing any type information on expressions or\nfunction definitions. However, this feature makes those languages less reliable\nthan statically typed languages, where many runtime errors can be detected at\ncompile time. In this paper, we present a preliminary work on a tool that, by\nusing the well-known techniques of metaprogramming and symbolic execution, can\nbe used to perform bounded verification of Erlang programs. In particular, by\nusing Constraint Logic Programming, we develop an interpreter that, given an\nErlang program and a symbolic input for that program, returns answer\nconstraints that represent sets of concrete data for which the Erlang program\ngenerates a runtime error.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 04:48:36 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["De Angelis", "Emanuele", "", "DEC, University \"G. d'Annunzio\" of\n  Chieti-Pescara, Pescara, Italy"], ["Fioravanti", "Fabio", "", "DEC, University \"G.\n  d'Annunzio\" of Chieti-Pescara, Pescara, Italy"], ["Palacios", "Adri\u00e1n", "", "MiST,\n  DSIC, Universitat Polit\u00e8cnica de Val\u00e8ncia, Val\u00e8ncia, Spain"], ["Pettorossi", "Alberto", "", "University of Roma Tor Vergata, Roma, Italy"], ["Proietti", "Maurizio", "", "CNR-IASI, Roma, Italy"]]}, {"id": "1809.05193", "submitter": "Rohan Bavishi", "authors": "Rohan Bavishi, Michael Pradel, Koushik Sen", "title": "Context2Name: A Deep Learning-Based Approach to Infer Natural Variable\n  Names from Usage Contexts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the JavaScript code deployed in the wild has been minified, a process\nin which identifier names are replaced with short, arbitrary and meaningless\nnames. Minified code occupies less space, but also makes the code extremely\ndifficult to manually inspect and understand. This paper presents Context2Name,\na deep learningbased technique that partially reverses the effect of\nminification by predicting natural identifier names for minified names. The\ncore idea is to predict from the usage context of a variable a name that\ncaptures the meaning of the variable. The approach combines a lightweight,\ntoken-based static analysis with an auto-encoder neural network that summarizes\nusage contexts and a recurrent neural network that predict natural names for a\ngiven usage context. We evaluate Context2Name with a large corpus of real-world\nJavaScript code and show that it successfully predicts 47.5% of all minified\nidentifiers while taking only 2.9 milliseconds on average to predict a name. A\ncomparison with the state-of-the-art tools JSNice and JSNaughty shows that our\napproach performs comparably in terms of accuracy while improving in terms of\nefficiency. Moreover, Context2Name complements the state-of-the-art by\npredicting 5.3% additional identifiers that are missed by both existing tools.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 20:52:10 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Bavishi", "Rohan", ""], ["Pradel", "Michael", ""], ["Sen", "Koushik", ""]]}, {"id": "1809.05649", "submitter": "Atsushi Igarashi", "authors": "Atsushi Igarashi (1), Peter Thiemann (2), Yuya Tsuda (1), Vasco T.\n  Vasconcelos (3), Philip Wadler (4) ((1) Kyoto University, Japan, (2)\n  University of Freiburg, Germany, (3) University of Lisbon, Portugal, (4)\n  University of Edinburgh, Scotland)", "title": "Gradual Session Types", "comments": "Preprint of an article to appear in Journal of Functional Programming", "journal-ref": "J. Funct. Prog. 29 (2019) e17", "doi": "10.1017/S0956796819000169", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Session types are a rich type discipline, based on linear types, that lifts\nthe sort of safety claims that come with type systems to communications.\nHowever, web-based applications and microservices are often written in a mix of\nlanguages, with type disciplines in a spectrum between static and dynamic\ntyping. Gradual session types address this mixed setting by providing a\nframework which grants seamless transition between statically typed handling of\nsessions and any required degree of dynamic typing.\n  We propose Gradual GV as a gradually typed extension of the functional\nsession type system GV. Following a standard framework of gradual typing,\nGradual GV consists of an external language, which relaxes the type system of\nGV using dynamic types, and an internal language with casts, for which\noperational semantics is given, and a cast-insertion translation from the\nformer to the latter. We demonstrate type and communication safety as well as\nblame safety, thus extending previous results to functional languages with\nsession-based communication. The interplay of linearity and dynamic types\nrequires a novel approach to specifying the dynamics of the language.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 05:03:56 GMT"}, {"version": "v2", "created": "Sat, 4 May 2019 07:30:19 GMT"}, {"version": "v3", "created": "Mon, 16 Sep 2019 13:08:11 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Igarashi", "Atsushi", ""], ["Thiemann", "Peter", ""], ["Tsuda", "Yuya", ""], ["Vasconcelos", "Vasco T.", ""], ["Wadler", "Philip", ""]]}, {"id": "1809.05771", "submitter": "Joaqu\\'in Arias M.Sc.", "authors": "Joaqu\\'in Arias and Manuel Carro", "title": "Description, Implementation, and Evaluation of a Generic Design for\n  Tabled CLP", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logic programming with tabling and constraints (TCLP, tabled constraint logic\nprogramming) has been shown to be more expressive and in some cases more\nefficient than LP, CLP or LP + tabling. Previous designs of TCLP systems did\nnot fully use entailment to determine call / answer subsumption and did not\nprovide a simple and well-documented interface to facilitate the integration of\nconstraint solvers in existing tabling systems. We study the role of projection\nand entailment in the termination, soundness and completeness of TCLP systems,\nand present the design and an experimental evaluation of Mod TCLP, a framework\nthat eases the integration of additional constraint solvers. Mod TCLP views\nconstraint solvers as clients of the tabling system, which is generic w.r.t.\nthe solver and only requires a clear interface from the latter. We validate our\ndesign by integrating four constraint solvers: a previously existing constraint\nsolver for difference constraints, written in C; the standard versions of\nHolzbaur's CLP(Q) and CLP(R), written in Prolog; and a new constraint solver\nfor equations over finite lattices. We evaluate the performance of our\nframework in several benchmarks using the aforementioned constraint solvers.\nMod TCLP is developed in Ciao Prolog, a robust, mature, next-generation Prolog\nsystem. Under consideration in Theory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 21:18:26 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Arias", "Joaqu\u00edn", ""], ["Carro", "Manuel", ""]]}, {"id": "1809.05859", "submitter": "Phillip Stanley-Marbell", "authors": "Phillip Stanley-Marbell and Armin Alaghi and Michael Carbin and Eva\n  Darulova and Lara Dolecek and Andreas Gerstlauer and Ghayoor Gillani and\n  Djordje Jevdjic and Thierry Moreau and Mattia Cacciotti and Alexandros Daglis\n  and Natalie Enright Jerger and Babak Falsafi and Sasa Misailovic and Adrian\n  Sampson and Damien Zufferey", "title": "Exploiting Errors for Efficiency: A Survey from Circuits to Algorithms", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.ET cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a computational task tolerates a relaxation of its specification or when\nan algorithm tolerates the effects of noise in its execution, hardware,\nprogramming languages, and system software can trade deviations from correct\nbehavior for lower resource usage. We present, for the first time, a synthesis\nof research results on computing systems that only make as many errors as their\nusers can tolerate, from across the disciplines of computer aided design of\ncircuits, digital system design, computer architecture, programming languages,\noperating systems, and information theory.\n  Rather than over-provisioning resources at each layer to avoid errors, it can\nbe more efficient to exploit the masking of errors occurring at one layer which\ncan prevent them from propagating to a higher layer. We survey tradeoffs for\nindividual layers of computing systems from the circuit level to the operating\nsystem level and illustrate the potential benefits of end-to-end approaches\nusing two illustrative examples. To tie together the survey, we present a\nconsistent formalization of terminology, across the layers, which does not\nsignificantly deviate from the terminology traditionally used by research\ncommunities in their layer of focus.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 11:55:33 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Stanley-Marbell", "Phillip", ""], ["Alaghi", "Armin", ""], ["Carbin", "Michael", ""], ["Darulova", "Eva", ""], ["Dolecek", "Lara", ""], ["Gerstlauer", "Andreas", ""], ["Gillani", "Ghayoor", ""], ["Jevdjic", "Djordje", ""], ["Moreau", "Thierry", ""], ["Cacciotti", "Mattia", ""], ["Daglis", "Alexandros", ""], ["Jerger", "Natalie Enright", ""], ["Falsafi", "Babak", ""], ["Misailovic", "Sasa", ""], ["Sampson", "Adrian", ""], ["Zufferey", "Damien", ""]]}, {"id": "1809.06274", "submitter": "Aaron Bembenek", "authors": "Aaron Bembenek and Stephen Chong", "title": "FormuLog: Datalog for static analysis involving logical formulae", "comments": "Proceedings of the 11th Workshop on Answer Set Programming and Other\n  Computing Paradigms 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datalog has become a popular language for writing static analyses. Because\nDatalog is very limited, some implementations of Datalog for static analysis\nhave extended it with new language features. However, even with these features\nit is hard or impossible to express a large class of analyses because they use\nlogical formulae to represent program state. FormuLog fills this gap by\nextending Datalog to represent, manipulate, and reason about logical formulae.\nWe have used FormuLog to implement declarative versions of symbolic execution\nand abstract model checking, analyses previously out of the scope of\nDatalog-based languages. While this paper focuses on the design of FormuLog and\none of the analyses we have implemented in it, it also touches on a prototype\nimplementation of the language and identifies performance optimizations that we\nbelieve will be necessary to scale FormuLog to real-world static analysis\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 15:22:29 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Bembenek", "Aaron", ""], ["Chong", "Stephen", ""]]}, {"id": "1809.06336", "submitter": "Ahmad Salim Al-Sibahi", "authors": "Ahmad Salim Al-Sibahi and Thomas P. Jensen and Aleksandar S. Dimovski\n  and Andrzej Wasowski", "title": "Verification of High-Level Transformations with Inductive Refinement\n  Types", "comments": null, "journal-ref": null, "doi": "10.1145/3278122.3278125", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-level transformation languages like Rascal include expressive features\nfor manipulating large abstract syntax trees: first-class traversals,\nexpressive pattern matching, backtracking and generalized iterators. We present\nthe design and implementation of an abstract interpretation tool, Rabit, for\nverifying inductive type and shape properties for transformations written in\nsuch languages. We describe how to perform abstract interpretation based on\noperational semantics, specifically focusing on the challenges arising when\nanalyzing the expressive traversals and pattern matching. Finally, we evaluate\nRabit on a series of transformations (normalization, desugaring, refactoring,\ncode generators, type inference, etc.) showing that we can effectively verify\nstated properties.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 17:14:06 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Al-Sibahi", "Ahmad Salim", ""], ["Jensen", "Thomas P.", ""], ["Dimovski", "Aleksandar S.", ""], ["Wasowski", "Andrzej", ""]]}, {"id": "1809.06962", "submitter": "Berkay Celik", "authors": "Z. Berkay Celik and Earlence Fernandes and Eric Pauley and Gang Tan\n  and Patrick McDaniel", "title": "Program Analysis of Commodity IoT Applications for Security and Privacy:\n  Challenges and Opportunities", "comments": "syntax and grammar error are fixed, and IoT platforms are updated to\n  match with the submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Internet of Things (IoT) have enabled myriad domains such\nas smart homes, personal monitoring devices, and enhanced manufacturing. IoT is\nnow pervasive---new applications are being used in nearly every conceivable\nenvironment, which leads to the adoption of device-based interaction and\nautomation. However, IoT has also raised issues about the security and privacy\nof these digitally augmented spaces. Program analysis is crucial in identifying\nthose issues, yet the application and scope of program analysis in IoT remains\nlargely unexplored by the technical community. In this paper, we study privacy\nand security issues in IoT that require program-analysis techniques with an\nemphasis on identified attacks against these systems and defenses implemented\nso far. Based on a study of five IoT programming platforms, we identify the key\ninsights that result from research efforts in both the program analysis and\nsecurity communities and relate the efficacy of program-analysis techniques to\nsecurity and privacy issues. We conclude by studying recent IoT analysis\nsystems and exploring their implementations. Through these explorations, we\nhighlight key challenges and opportunities in calibrating for the environments\nin which IoT systems will be used.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 23:31:29 GMT"}, {"version": "v2", "created": "Sat, 13 Oct 2018 03:43:11 GMT"}, {"version": "v3", "created": "Mon, 24 Dec 2018 08:02:33 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Celik", "Z. Berkay", ""], ["Fernandes", "Earlence", ""], ["Pauley", "Eric", ""], ["Tan", "Gang", ""], ["McDaniel", "Patrick", ""]]}, {"id": "1809.07080", "submitter": "Benjamin Kowarsch", "authors": "Benjamin Kowarsch", "title": "On the Maintenance of Classic Modula-2 Compilers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classic Modula-2 language was specified in [Wir78] by N.Wirth at ETH\nZ\\\"urich in 1978. The last revision [Wir88] was published in 1988. Many\ncomputer science books of that era used Modula-2 in programming examples. Many\nof these are still valuable resources in computer science education today. To\ncompile and run the examples therein, it is essential to have compilers\navailable that follow the classic Modula-2 language definition and run on\nmodern computer hardware and operating systems. Although most Modula-2\ncompilers of that era have disappeared, a few have since been re-released under\nopen source licenses. Whilst the original authors have long ceased work on\nthese compilers, new maintainers have stepped in. This paper gives\nrecommendations for maintenance on classic Modula-2 compilers while balancing\nthe aim to modernise with the need to maintain the capability to compile\nprogramming examples in the literature with minimal effort. Nevertheless, the\nprinciples, methods and conclusions presented are adaptable to maintenance on\nother languages.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 09:07:47 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Kowarsch", "Benjamin", ""]]}, {"id": "1809.07444", "submitter": "Matthias Springer", "authors": "Matthias Springer", "title": "DynaSOAr: Accelerating Single-Method Multiple-Objects Applications on\n  GPUs", "comments": "ACM Student Research Competition, Grand Finals Submission, Graduate\n  Category", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object-oriented programming (OOP) has long been regarded as too inefficient\nfor SIMD high-performance computing, despite the fact that many important HPC\napplications have an inherent object structure. We discovered a broad subset of\nOOP that can be implemented efficiently on massively parallel SIMD\naccelerators. We call it Single-Method Multiple-Objects (SMMO), because\nparallelism is expressed by running a method on all objects of a type.\n  To make fast GPU programming available to domain experts who are less\nexperienced in GPU programming, we developed DynaSOAr, a CUDA framework for\nSMMO applications. DynaSOAr improves the usage of allocated memory with an SOA\ndata layout and achieves low memory fragmentation through efficient management\nof free and allocated memory blocks with lock-free, hierarchical bitmaps.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 01:39:05 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 03:59:43 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Springer", "Matthias", ""]]}, {"id": "1809.07897", "submitter": "G. A. Kavvos", "authors": "G. A. Kavvos", "title": "Modalities, Cohesion, and Information Flow", "comments": null, "journal-ref": "G. A. Kavvos. 2019. Modalities, Cohesion, and Information Flow.\n  Proc. ACM Program. Lang. 3, POPL, Article 20 (January 2019), 29 pages", "doi": "10.1145/3290333", "report-no": null, "categories": "cs.PL cs.CR cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is informally understood that the purpose of modal type constructors in\nprogramming calculi is to control the flow of information between types. In\norder to lend rigorous support to this idea, we study the category of\nclassified sets, a variant of a denotational semantics for information flow\nproposed by Abadi et al. We use classified sets to prove multiple\nnoninterference theorems for modalities of a monadic and comonadic flavour. The\ncommon machinery behind our theorems stems from the the fact that classified\nsets are a (weak) model of Lawvere's theory of axiomatic cohesion. In the\nprocess, we show how cohesion can be used for reasoning about multi-modal\nsettings. This leads to the conclusion that cohesion is a particularly useful\nsetting for the study of both information flow, but also modalities in type\ntheory and programming languages at large.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 00:25:28 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 20:32:48 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Kavvos", "G. A.", ""]]}, {"id": "1809.09310", "submitter": "Daniel Fremont", "authors": "Daniel J. Fremont, Tommaso Dreossi, Shromona Ghosh, Xiangyu Yue,\n  Alberto L. Sangiovanni-Vincentelli, Sanjit A. Seshia", "title": "Scenic: A Language for Scenario Specification and Scene Generation", "comments": "41 pages, 36 figures. Full version of a PLDI 2019 paper (extending UC\n  Berkeley EECS Department Tech Report No. UCB/EECS-2018-8)", "journal-ref": null, "doi": "10.1145/3314221.3314633", "report-no": null, "categories": "cs.PL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new probabilistic programming language for the design and\nanalysis of perception systems, especially those based on machine learning.\nSpecifically, we consider the problems of training a perception system to\nhandle rare events, testing its performance under different conditions, and\ndebugging failures. We show how a probabilistic programming language can help\naddress these problems by specifying distributions encoding interesting types\nof inputs and sampling these to generate specialized training and test sets.\nMore generally, such languages can be used for cyber-physical systems and\nrobotics to write environment models, an essential prerequisite to any formal\nanalysis. In this paper, we focus on systems like autonomous cars and robots,\nwhose environment is a \"scene\", a configuration of physical objects and agents.\nWe design a domain-specific language, Scenic, for describing \"scenarios\" that\nare distributions over scenes. As a probabilistic programming language, Scenic\nallows assigning distributions to features of the scene, as well as\ndeclaratively imposing hard and soft constraints over the scene. We develop\nspecialized techniques for sampling from the resulting distribution, taking\nadvantage of the structure provided by Scenic's domain-specific syntax.\nFinally, we apply Scenic in a case study on a convolutional neural network\ndesigned to detect cars in road images, improving its performance beyond that\nachieved by state-of-the-art synthetic data generation methods.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 03:57:00 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 01:12:24 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Fremont", "Daniel J.", ""], ["Dreossi", "Tommaso", ""], ["Ghosh", "Shromona", ""], ["Yue", "Xiangyu", ""], ["Sangiovanni-Vincentelli", "Alberto L.", ""], ["Seshia", "Sanjit A.", ""]]}, {"id": "1809.09387", "submitter": "Suejb Memeti", "authors": "Suejb Memeti and Sabri Pllana", "title": "HSTREAM: A directive-based language extension for heterogeneous stream\n  computing", "comments": "Preprint, 21st IEEE International Conference on Computational Science\n  and Engineering (CSE 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data streaming applications require utilization of heterogeneous parallel\ncomputing systems, which may comprise multiple multi-core CPUs and many-core\naccelerating devices such as NVIDIA GPUs and Intel Xeon Phis. Programming such\nsystems require advanced knowledge of several hardware architectures and\ndevice-specific programming models, including OpenMP and CUDA. In this paper,\nwe present HSTREAM, a compiler directive-based language extension to support\nprogramming stream computing applications for heterogeneous parallel computing\nsystems. HSTREAM source-to-source compiler aims to increase the programming\nproductivity by enabling programmers to annotate the parallel regions for\nheterogeneous execution and generate target specific code. The HSTREAM runtime\nautomatically distributes the workload across CPUs and accelerating devices. We\ndemonstrate the usefulness of HSTREAM language extension with various\napplications from the STREAM benchmark. Experimental evaluation results show\nthat HSTREAM can keep the same programming simplicity as OpenMP, and the\ngenerated code can deliver performance beyond what CPUs-only and GPUs-only\nexecutions can deliver.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 09:56:36 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Memeti", "Suejb", ""], ["Pllana", "Sabri", ""]]}, {"id": "1809.09749", "submitter": "Alan Schmitt", "authors": "Martin Bodin and Philippa Gardner and Thomas Jensen and Alan Schmitt", "title": "Skeletal Semantics and their Interpretations", "comments": "31 pages, POPL, article 44", "journal-ref": "Proceedings of the ACM on Programming Languages 3, POPL (January\n  2019)", "doi": "10.1145/3290357", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of mechanised language specification based on structured\noperational semantics, with applications to verified compilers and sound\nprogram analysis, requires huge effort. General theory and frameworks have been\nproposed to help with this effort. However, none of this work provides a\nsystematic way of developing concrete and abstract semantics, connected\ntogether by a general consistency result. We introduce a skeletal semantics of\na language, where each skeleton describes the complete semantic behaviour of a\nlanguage construct. We define a general notion of interpretation, which\nprovides a systematic and language-independent way of deriving semantic\njudgements from the skeletal semantics. We explore four generic\ninterpretations: a simple well-formedness interpretation; a concrete\ninterpretation; an abstract interpretation; and a constraint generator for\nflow-sensitive analysis. We prove general consistency results between\ninterpretations, depending only on simple language-dependent lemmas. We\nillustrate our ideas using a simple While language.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 22:45:07 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 15:57:41 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Bodin", "Martin", ""], ["Gardner", "Philippa", ""], ["Jensen", "Thomas", ""], ["Schmitt", "Alan", ""]]}, {"id": "1809.10756", "submitter": "Jan-Willem Van De Meent", "authors": "Jan-Willem van de Meent and Brooks Paige and Hongseok Yang and Frank\n  Wood", "title": "An Introduction to Probabilistic Programming", "comments": "Under review at Foundations and Trends in Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is designed to be a first-year graduate-level introduction to\nprobabilistic programming. It not only provides a thorough background for\nanyone wishing to use a probabilistic programming system, but also introduces\nthe techniques needed to design and build these systems. It is aimed at people\nwho have an undergraduate-level understanding of either or, ideally, both\nprobabilistic machine learning and programming languages.\n  We start with a discussion of model-based reasoning and explain why\nconditioning as a foundational computation is central to the fields of\nprobabilistic machine learning and artificial intelligence. We then introduce a\nsimple first-order probabilistic programming language (PPL) whose programs\ndefine static-computation-graph, finite-variable-cardinality models. In the\ncontext of this restricted PPL we introduce fundamental inference algorithms\nand describe how they can be implemented in the context of models denoted by\nprobabilistic programs.\n  In the second part of this document, we introduce a higher-order\nprobabilistic programming language, with a functionality analogous to that of\nestablished programming languages. This affords the opportunity to define\nmodels with dynamic computation graphs, at the cost of requiring inference\nmethods that generate samples by repeatedly executing the program. Foundational\ninference algorithms for this kind of probabilistic programming language are\nexplained in the context of an interface between program executions and an\ninference controller.\n  This document closes with a chapter on advanced topics which we believe to\nbe, at the time of writing, interesting directions for probabilistic\nprogramming research; directions that point towards a tight integration with\ndeep neural network research and the development of systems for next-generation\nartificial intelligence applications.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 20:44:23 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["van de Meent", "Jan-Willem", ""], ["Paige", "Brooks", ""], ["Yang", "Hongseok", ""], ["Wood", "Frank", ""]]}]