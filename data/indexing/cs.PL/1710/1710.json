[{"id": "1710.00077", "submitter": "Manuel Krebber B.Sc.", "authors": "Manuel Krebber, Henrik Barthels, Paolo Bientinesi", "title": "Efficient Pattern Matching in Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern matching is a powerful tool for symbolic computations. Applications\ninclude term rewriting systems, as well as the manipulation of symbolic\nexpressions, abstract syntax trees, and XML and JSON data. It also allows for\nan intuitive description of algorithms in the form of rewrite rules. We present\nthe open source Python module MatchPy, which offers functionality and\nexpressiveness similar to the pattern matching in Mathematica. In particular,\nit includes syntactic pattern matching, as well as matching for commutative\nand/or associative functions, sequence variables, and matching with\nconstraints. MatchPy uses new and improved algorithms to efficiently find\nmatches for large pattern sets by exploiting similarities between patterns. The\nperformance of MatchPy is investigated on several real-world problems.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 20:14:47 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Krebber", "Manuel", ""], ["Barthels", "Henrik", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1710.01291", "submitter": "Hila Peleg", "authors": "Hila Peleg, Sharon Shoham and Eran Yahav", "title": "Programming Not Only by Example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been tremendous progress in automated synthesis\ntechniques that are able to automatically generate code based on some intent\nexpressed by the programmer. A major challenge for the adoption of synthesis\nremains in having the programmer communicate their intent. When the expressed\nintent is coarse-grained (for example, restriction on the expected type of an\nexpression), the synthesizer often produces a long list of results for the\nprogrammer to choose from, shifting the heavy-lifting to the user. An\nalternative approach, successfully used in end-user synthesis is programming by\nexample (PBE), where the user leverages examples to interactively and\niteratively refine the intent. However, using only examples is not expressive\nenough for programmers, who can observe the generated program and refine the\nintent by directly relating to parts of the generated program.\n  We present a novel approach to interacting with a synthesizer using a\ngranular interaction model. Our approach employs a rich interaction model where\n(i) the synthesizer decorates a candidate program with debug information that\nassists in understanding the program and identifying good or bad parts, and\n(ii) the user is allowed to provide feedback not only on the expected output of\na program, but also on the underlying program itself. That is, when the user\nidentifies a program as (partially) correct or incorrect, they can also\nexplicitly indicate the good or bad parts, to allow the synthesizer to accept\nor discard parts of the program instead of discarding the program as a whole.\n  We show the value of our approach in a controlled user study. Our study shows\nthat participants have strong preference to using granular feedback instead of\nexamples, and are able to provide granular feedback much faster.\n", "versions": [{"version": "v1", "created": "Tue, 3 Oct 2017 17:45:07 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Peleg", "Hila", ""], ["Shoham", "Sharon", ""], ["Yahav", "Eran", ""]]}, {"id": "1710.01547", "submitter": "Mark Noone", "authors": "Mark Noone, Aidan Mooney", "title": "Visual and Textual Programming Languages: A Systematic Review of the\n  Literature", "comments": "18 pages (including 2 bibliography pages), 3 figures", "journal-ref": "Journal of Computers in Education, 2018", "doi": "10.1007/s40692-018-0101-5", "report-no": null, "categories": "cs.CY cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well documented, and has been the topic of much research, that Computer\nScience courses tend to have higher than average drop out rates at third level.\nThis is a problem that needs to be addressed with urgency but also caution. The\nrequired number of Computer Science graduates is growing every year but the\nnumber of graduates is not meeting this demand and one way that this problem\ncan be alleviated is to encourage students at an early age towards studying\nComputer Science courses.\n  This paper presents a systematic literature review on the role of visual and\ntextual programming languages when learning to program, particularly as a first\nprogramming language. The approach is systematic, in that a structured search\nof electronic resources has been conducted, and the results are presented and\nquantitatively analysed. This study will give insight into whether or not the\ncurrent approaches to teaching young learners programming are viable, and\nexamines what we can do to increase the interest and retention of these\nstudents as they progress through their education.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 11:03:40 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 16:35:33 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Noone", "Mark", ""], ["Mooney", "Aidan", ""]]}, {"id": "1710.02332", "submitter": "L\\'eo Stefanesco", "authors": "Paul-Andr\\'e Melli\\`es and L\\'eo Stefanesco", "title": "A Game Semantics of Concurrent Separation Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a game-theoretic account of concurrent separation\nlogic. To every execution trace of the Code confronted to the Environment, we\nassociate a specification game where Eve plays for the Code, and Adam for the\nEnvironment. The purpose of Eve and Adam is to decompose every intermediate\nmachine state of the execution trace into three pieces: one piece for the Code,\none piece for the Environment, and one piece for the available shared\nresources. We establish the soundness of concurrent separation logic by\ninterpreting every derivation tree of the logic as a winning strategy of this\nspecification game.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 09:57:40 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Melli\u00e8s", "Paul-Andr\u00e9", ""], ["Stefanesco", "L\u00e9o", ""]]}, {"id": "1710.02804", "submitter": "Germ\\'an Vidal", "authors": "Naoki Nishida, Adri\\'an Palacios and Germ\\'an Vidal", "title": "Reversible Computation in Term Rewriting", "comments": "To appear in the Journal of Logical and Algebraic Methods in\n  Programming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Essentially, in a reversible programming language, for each forward\ncomputation from state $S$ to state $S'$, there exists a constructive method to\ngo backwards from state $S'$ to state $S$. Besides its theoretical interest,\nreversible computation is a fundamental concept which is relevant in many\ndifferent areas like cellular automata, bidirectional program transformation,\nor quantum computing, to name a few.\n  In this work, we focus on term rewriting, a computation model that underlies\nmost rule-based programming languages. In general, term rewriting is not\nreversible, even for injective functions; namely, given a rewrite step $t_1\n\\rightarrow t_2$, we do not always have a decidable method to get $t_1$ from\n$t_2$. Here, we introduce a conservative extension of term rewriting that\nbecomes reversible. Furthermore, we also define two transformations,\ninjectivization and inversion, to make a rewrite system reversible using\nstandard term rewriting. We illustrate the usefulness of our transformations in\nthe context of bidirectional program transformation.\n", "versions": [{"version": "v1", "created": "Sun, 8 Oct 2017 08:31:49 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Nishida", "Naoki", ""], ["Palacios", "Adri\u00e1n", ""], ["Vidal", "Germ\u00e1n", ""]]}, {"id": "1710.03248", "submitter": "Anders Miltner", "authors": "Anders Miltner and Kathleen Fisher and Benjamin C. Pierce and David\n  Walker and Steve Zdancewic", "title": "Synthesizing Bijective Lenses", "comments": "127 Pages, Extended Version with Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bidirectional transformations between different data representations occur\nfrequently in modern software systems. They appear as serializers and\ndeserializers, as database views and view updaters, and more. Manually building\nbidirectional transformations---by writing two separate functions that are\nintended to be inverses---is tedious and error prone. A better approach is to\nuse a domain-specific language in which both directions can be written as a\nsingle expression. However, these domain-specific languages can be difficult to\nprogram in, requiring programmers to manage fiddly details while working in a\ncomplex type system.\n  To solve this, we present Optician, a tool for type-directed synthesis of\nbijective string transformers. The inputs to Optician are two ordinary regular\nexpressions representing two data formats and a few concrete examples for\ndisambiguation. The output is a well-typed program in Boomerang (a\nbidirectional language based on the theory of lenses). The main technical\nchallenge involves navigating the vast program search space efficiently enough.\nUnlike most prior work on type-directed synthesis, our system operates in the\ncontext of a language with a rich equivalence relation on types (the theory of\nregular expressions). We synthesize terms of a equivalent language and convert\nthose generated terms into our lens language. We prove the correctness of our\nsynthesis algorithm. We also demonstrate empirically that our new language\nchanges the synthesis problem from one that admits intractable solutions to one\nthat admits highly efficient solutions. We evaluate Optician on a benchmark\nsuite of 39 examples including both microbenchmarks and realistic examples\nderived from other data management systems including Flash Fill, a tool for\nsynthesizing string transformations in spreadsheets, and Augeas, a tool for\nbidirectional processing of Linux system configuration files.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 18:05:02 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Miltner", "Anders", ""], ["Fisher", "Kathleen", ""], ["Pierce", "Benjamin C.", ""], ["Walker", "David", ""], ["Zdancewic", "Steve", ""]]}, {"id": "1710.03357", "submitter": "Caleb Voss", "authors": "Caleb Voss, David Heath, William Harris", "title": "Proofs as Relational Invariants of Synthesized Execution Grammars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic verification of programs that maintain unbounded low-level data\nstructures is a critical and open problem. Analyzers and verifiers developed in\nprevious work can synthesize invariants that only describe data structures of\nheavily restricted forms, or require an analyst to provide predicates over\nprogram data and structure that are used in a synthesized proof of correctness.\n  In this work, we introduce a novel automatic safety verifier of programs that\nmaintain low-level data structures, named LTTP. LTTP synthesizes proofs of\nprogram safety represented as a grammar of a given program's control paths,\nannotated with invariants that relate program state at distinct points within\nits path of execution. LTTP synthesizes such proofs completely automatically,\nusing a novel inductive-synthesis algorithm.\n  We have implemented LTTP as a verifier for JVM bytecode and applied it to\nverify the safety of a collection of verification benchmarks. Our results\ndemonstrate that LTTP can be applied to automatically verify the safety of\nprograms that are beyond the scope of previously-developed verifiers.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 00:05:21 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Voss", "Caleb", ""], ["Heath", "David", ""], ["Harris", "William", ""]]}, {"id": "1710.03391", "submitter": "EPTCS", "authors": "Bernd Finkbeiner (Universit\\\"at des Saarlandes), Andrey Kupriyanov\n  (Institute of Science and Technology Austria)", "title": "Causality-based Model Checking", "comments": "In Proceedings CREST 2017, arXiv:1710.02770", "journal-ref": "EPTCS 259, 2017, pp. 31-38", "doi": "10.4204/EPTCS.259.3", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model checking is usually based on a comprehensive traversal of the state\nspace. Causality-based model checking is a radically different approach that\ninstead analyzes the cause-effect relationships in a program. We give an\noverview on a new class of model checking algorithms that capture the causal\nrelationships in a special data structure called concurrent traces. Concurrent\ntraces identify key events in an execution history and link them through their\ncause-effect relationships. The model checker builds a tableau of concurrent\ntraces, where the case splits represent different causal explanations of a\nhypothetical error. Causality-based model checking has been implemented in the\nARCTOR tool, and applied to previously intractable multi-threaded benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 03:51:18 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Finkbeiner", "Bernd", "", "Universit\u00e4t des Saarlandes"], ["Kupriyanov", "Andrey", "", "Institute of Science and Technology Austria"]]}, {"id": "1710.03666", "submitter": "Ale\\v{s} Bizjak", "authors": "Robert Gl\\\"uck and Robin Kaarsgaard", "title": "A categorical foundation for structured reversible flowchart languages:\n  Soundness and adequacy", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 14, Issue 3 (September\n  5, 2018) lmcs:4802", "doi": "10.23638/LMCS-14(3:16)2018", "report-no": null, "categories": "cs.PL math.CT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Structured reversible flowchart languages is a class of imperative reversible\nprogramming languages allowing for a simple diagrammatic representation of\ncontrol flow built from a limited set of control flow structures. This class\nincludes the reversible programming language Janus (without recursion), as well\nas more recently developed reversible programming languages such as R-CORE and\nR-WHILE.\n  In the present paper, we develop a categorical foundation for this class of\nlanguages based on inverse categories with joins. We generalize the notion of\nextensivity of restriction categories to one that may be accommodated by\ninverse categories, and use the resulting decisions to give a reversible\nrepresentation of predicates and assertions. This leads to a categorical\nsemantics for structured reversible flowcharts, which we show to be\ncomputationally sound and adequate, as well as equationally fully abstract with\nrespect to the operational semantics under certain conditions.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 15:45:12 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 14:56:15 GMT"}, {"version": "v3", "created": "Mon, 3 Sep 2018 06:46:50 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Gl\u00fcck", "Robert", ""], ["Kaarsgaard", "Robin", ""]]}, {"id": "1710.03832", "submitter": "Artjoms Sinkarovs PhD", "authors": "Artjoms Sinkarovs and Sven-Bodo Scholz", "title": "A Lambda Calculus for Transfinite Arrays: Unifying Arrays and Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Array programming languages allow for concise and generic formulations of\nnumerical algorithms, thereby providing a huge potential for program\noptimisation such as fusion, parallelisation, etc. One of the restrictions that\nthese languages typically have is that the number of elements in every array\nhas to be finite. This means that implementing streaming algorithms in such\nlanguages requires new types of data structures, with operations that are not\nimmediately compatible with existing array operations or compiler\noptimisations.\n  In this paper, we propose a design for a functional language that natively\nsupports infinite arrays. We use ordinal numbers to introduce the notion of\ninfinity in shapes and indices. By doing so, we obtain a calculus that\nnaturally extends existing array calculi and, at the same time, allows for\nrecursive specifications as they are found in stream- and list-based settings.\nFurthermore, the main language construct that can be thought of as an $n$-fold\ncons operator gives rise to expressing transfinite recursion in data, something\nthat lists or streams usually do not support. This makes it possible to treat\nthe proposed calculus as a unifying theory of arrays, lists and streams. We\ngive an operational semantics of the proposed language, discuss design choices\nthat we have made, and demonstrate its expressibility with several examples. We\nalso demonstrate that the proposed formalism preserves a number of well-known\nuniversal equalities from array/list/stream theories, and discuss\nimplementation-related challenges.\n", "versions": [{"version": "v1", "created": "Tue, 10 Oct 2017 21:52:11 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Sinkarovs", "Artjoms", ""], ["Scholz", "Sven-Bodo", ""]]}, {"id": "1710.03912", "submitter": "Amin Timany", "authors": "Amin Timany, Matthieu Sozeau", "title": "Consistency of the Predicative Calculus of Cumulative Inductive\n  Constructions (pCuIC)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In order to avoid well-know paradoxes associated with self-referential\ndefinitions, higher-order dependent type theories stratify the theory using a\ncountably infinite hierarchy of universes (also known as sorts), Type$_0$ :\nType$_1$ : $\\cdots$ . Such type systems are called cumulative if for any type\n$A$ we have that $A$ : Type$_{i}$ implies $A$ : Type$_{i+1}$. The predicative\ncalculus of inductive constructions (pCIC) which forms the basis of the Coq\nproof assistant, is one such system.\n  In this paper we present and establish the soundness of the predicative\ncalculus of cumulative inductive constructions (pCuIC) which extends the\ncumulativity relation to inductive types.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 05:31:26 GMT"}, {"version": "v2", "created": "Mon, 22 Jan 2018 11:53:50 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 12:24:14 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Timany", "Amin", ""], ["Sozeau", "Matthieu", ""]]}, {"id": "1710.03928", "submitter": "Christopher M. Poskitt", "authors": "Claudio Corrodi, Alexander Heu{\\ss}ner, Christopher M. Poskitt", "title": "A Semantics Comparison Workbench for a Concurrent, Asynchronous,\n  Distributed Programming Language", "comments": "Accepted by Formal Aspects of Computing", "journal-ref": "Formal Asp. Comput. 30(1): 163-192 (2018)", "doi": "10.1007/s00165-017-0443-1", "report-no": null, "categories": "cs.SE cs.DC cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of high-level languages and libraries have been proposed that offer\nnovel and simple to use abstractions for concurrent, asynchronous, and\ndistributed programming. The execution models that realise them, however, often\nchange over time---whether to improve performance, or to extend them to new\nlanguage features---potentially affecting behavioural and safety properties of\nexisting programs. This is exemplified by SCOOP, a message-passing approach to\nconcurrent object-oriented programming that has seen multiple changes proposed\nand implemented, with demonstrable consequences for an idiomatic usage of its\ncore abstraction. We propose a semantics comparison workbench for SCOOP with\nfully and semi-automatic tools for analysing and comparing the state spaces of\nprograms with respect to different execution models or semantics. We\ndemonstrate its use in checking the consistency of properties across semantics\nby applying it to a set of representative programs, and highlighting a\ndeadlock-related discrepancy between the principal execution models of SCOOP.\nFurthermore, we demonstrate the extensibility of the workbench by generalising\nthe formalisation of an execution model to support recently proposed extensions\nfor distributed programming. Our workbench is based on a modular and\nparameterisable graph transformation semantics implemented in the GROOVE tool.\nWe discuss how graph transformations are leveraged to atomically model\nintricate language abstractions, how the visual yet algebraic nature of the\nmodel can be used to ascertain soundness, and highlight how the approach could\nbe applied to similar languages.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 06:36:58 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Corrodi", "Claudio", ""], ["Heu\u00dfner", "Alexander", ""], ["Poskitt", "Christopher M.", ""]]}, {"id": "1710.03979", "submitter": "Stavros Tripakis", "authors": "Viorel Preoteasa, Iulia Dragomir, Stavros Tripakis", "title": "The Refinement Calculus of Reactive Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Refinement Calculus of Reactive Systems (RCRS) is a compositional formal\nframework for modeling and reasoning about reactive systems. RCRS provides a\nlanguage which allows to describe atomic components as symbolic transition\nsystems or QLTL formulas, and composite components formed using three primitive\ncomposition operators: serial, parallel, and feedback. The semantics of the\nlanguage is given in terms of monotonic property transformers, an extension to\nreactive systems of monotonic predicate transformers, which have been used to\ngive compositional semantics to sequential programs. RCRS allows to specify\nboth safety and liveness properties. It also allows to model input-output\nsystems which are both non-deterministic and non-input-receptive (i.e., which\nmay reject some inputs at some points in time), and can thus be seen as a\nbehavioral type system. RCRS provides a set of techniques for symbolic\ncomputer-aided reasoning, including compositional static analysis and\nverification. RCRS comes with a publicly available implementation which\nincludes a complete formalization of the RCRS theory in the Isabelle proof\nassistant.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 09:41:59 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 11:19:27 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Preoteasa", "Viorel", ""], ["Dragomir", "Iulia", ""], ["Tripakis", "Stavros", ""]]}, {"id": "1710.03984", "submitter": "Koko Muroya", "authors": "Koko Muroya, Steven Cheung, Dan R. Ghica", "title": "Abductive functional programming, a semantic approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a call-by-value lambda calculus extended with a new construct\ninspired by abductive inference and motivated by the programming idioms of\nmachine learning. Although syntactically simple the abductive construct has a\ncomplex and subtle operational semantics which we express using a style based\non the Geometry of Interaction. We show that the calculus is sound, in the\nsense that well typed programs terminate normally. We also give a visual\nimplementation of the semantics which relies on additional garbage collection\nrules, which we also prove sound.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 09:57:51 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Muroya", "Koko", ""], ["Cheung", "Steven", ""], ["Ghica", "Dan R.", ""]]}, {"id": "1710.04132", "submitter": "Roberto Almeida Bittencourt", "authors": "Cleison Simoes Santos, Allen Hichard Marques Santos, Suenny\n  Mascarenhas Souza, Roberto Almeida Bittencourt", "title": "Aprendendo Programacao Orientada a Objetos com uma Abordagem Ludica\n  Baseada em Greenfoot e Robocode", "comments": "10 pages, 3 figures, 2 tables, COBENGE 2015 - XLIII Congresso\n  Brasileiro de Educa\\c{c}\\~ao em Engenharia, in Portuguese", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One the major challenges in undergraduate computing programs is the learning\nof object-oriented programming (OOP). This paradigm has a variety of concepts\nwith an abstraction level usually high for most beginners, even the ones who\nalready code in an imperative language. Furthermore, transitioning from\nimperative programming to OOP is a complex issue, with various inappropriate\nside effects. A significant effort has been pursued in the search of motivating\nand attractive solutions for such issues. One of those is the use of playful\nenvironments that merge games with learning. In this work, we report our\nexperience with OOP learning workshops by means of games, challenges and\ncompetitions, supported by Greenfoot and Robocode learning environments. A\nworkshop with sophomore students in a Computer Engineering program is presented\nhere. Lessons learning to motive students include: design of motivating\nexamples, use of competitive challenges, and an appropriate ratio between\ntutors and students. Results suggest that the workshop was a practical and\neffective way to introduce OOP and motivate students to learn it.\n", "versions": [{"version": "v1", "created": "Sat, 7 Oct 2017 18:51:48 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 00:46:24 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Santos", "Cleison Simoes", ""], ["Santos", "Allen Hichard Marques", ""], ["Souza", "Suenny Mascarenhas", ""], ["Bittencourt", "Roberto Almeida", ""]]}, {"id": "1710.04259", "submitter": "Sizhuo Zhang", "authors": "Sizhuo Zhang, Muralidaran Vijayaraghavan, Dan Lustig, Arvind", "title": "Weak Memory Models with Matching Axiomatic and Operational Definitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory consistency models are notorious for being difficult to define\nprecisely, to reason about, and to verify. More than a decade of effort has\ngone into nailing down the definitions of the ARM and IBM Power memory models,\nand yet there still remain aspects of those models which (perhaps surprisingly)\nremain unresolved to this day. In response to these complexities, there has\nbeen somewhat of a recent trend in the (general-purpose) architecture community\nto limit new memory models to being (multicopy) atomic: where store values can\nbe read by the issuing processor before being advertised to other processors.\nTSO is the most notable example, used in the past by IBM 370 and SPARC-TSO, and\ncurrently used in x86. Recently (in March 2017) ARM has also switched to a\nmulticopy atomic memory model, and the new RISC-V ISA and recent academic\nproposals such as WMM are pushing to do the same.\n  In this paper, we show that when memory models are atomic, it becomes much\neasier to produce axiomatic definitions, operational definitions, and proofs of\nequivalence than doing the same under non-atomic models. The increased ease\nwith which these definitions can be produced in turn allows architects to build\nprocessors much more confidently, and yet the relaxed nature of the models we\npropose still allows most or all of the performance of non-atomic models to be\nretained. In fact, in this paper, we show that atomic memory models can be\ndefined in a way that is parametrized by basic instruction and fence orderings.\nOur operational vs. axiomatic equivalence proofs, which are likewise\nparameterized, show that the operational model is sound with respect to the\naxioms and that the operational model is complete: that it can show any\nbehavior permitted by axiomatic model.\n", "versions": [{"version": "v1", "created": "Wed, 11 Oct 2017 19:11:40 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 19:58:11 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Zhang", "Sizhuo", ""], ["Vijayaraghavan", "Muralidaran", ""], ["Lustig", "Dan", ""], ["Arvind", "", ""]]}, {"id": "1710.04839", "submitter": "John Wickerson", "authors": "Nathan Chong, Tyler Sorensen, John Wickerson", "title": "The Semantics of Transactions and Weak Memory in x86, Power, ARM, and\n  C++", "comments": null, "journal-ref": "Proceedings of 39th ACM SIGPLAN Conference on Programming Language\n  Design and Implementation (PLDI'18), ACM, New York, NY, USA. 2018", "doi": "10.1145/3192366.3192373", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weak memory models provide a complex, system-centric semantics for concurrent\nprograms, while transactional memory (TM) provides a simpler,\nprogrammer-centric semantics. Both have been studied in detail, but their\ncombined semantics is not well understood. This is problematic because such\nwidely-used architectures and languages as x86, Power, and C++ all support TM,\nand all have weak memory models.\n  Our work aims to clarify the interplay between weak memory and TM by\nextending existing axiomatic weak memory models (x86, Power, ARMv8, and C++)\nwith new rules for TM. Our formal models are backed by automated tooling that\nenables (1) the synthesis of tests for validating our models against existing\nimplementations and (2) the model-checking of TM-related transformations, such\nas lock elision and compiling C++ transactions to hardware. A key finding is\nthat a proposed TM extension to ARMv8 currently being considered within ARM\nResearch is incompatible with lock elision without sacrificing portability or\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 08:36:16 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 21:19:34 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Chong", "Nathan", ""], ["Sorensen", "Tyler", ""], ["Wickerson", "John", ""]]}, {"id": "1710.06125", "submitter": "Gregory Duck", "authors": "Gregory J. Duck and Roland H. C. Yap", "title": "EffectiveSan: Type and Memory Error Detection using Dynamically Typed\n  C/C++", "comments": "To appear in the Proceedings of 39th ACM SIGPLAN Conference on\n  Programming Language Design and Implementation (PLDI2018)", "journal-ref": null, "doi": "10.1145/3192366.3192388", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-level programming languages with weak/static type systems, such as C and\nC++, are vulnerable to errors relating to the misuse of memory at runtime, such\nas (sub-)object bounds overflows, (re)use-after-free, and type confusion. Such\nerrors account for many security and other undefined behavior bugs for programs\nwritten in these languages. In this paper, we introduce the notion of\ndynamically typed C/C++, which aims to detect such errors by dynamically\nchecking the \"effective type\" of each object before use at runtime. We also\npresent an implementation of dynamically typed C/C++ in the form of the\nEffective Type Sanitizer (EffectiveSan). EffectiveSan enforces type and memory\nsafety using a combination of low-fat pointers, type meta data and type/bounds\ncheck instrumentation. We evaluate EffectiveSan against the SPEC2006 benchmark\nsuite and the Firefox web browser, and detect several new type and memory\nerrors. We also show that EffectiveSan achieves high compatibility and\nreasonable overheads for the given error coverage. Finally, we highlight that\nEffectiveSan is one of only a few tools that can detect sub-object bounds\nerrors, and uses a novel approach (dynamic type checking) to do so.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 07:03:13 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 07:15:22 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Duck", "Gregory J.", ""], ["Yap", "Roland H. C.", ""]]}, {"id": "1710.06146", "submitter": "Kostadin Kratchanov", "authors": "Kostadin Kratchanov", "title": "Cinnamons: A Computation Model Underlying Control Network Programming", "comments": "7th Intl Conf. on Computer Science, Engineering & Applications\n  (ICCSEA 2017) September 23~24, 2017, Copenhagen, Denmark", "journal-ref": null, "doi": "10.5121/csit.2017.71101", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the easily recognizable name \"cinnamon\" and \"cinnamon programming\" to\na new computation model intended to form a theoretical foundation for Control\nNetwork Programming (CNP). CNP has established itself as a programming paradigm\ncombining declarative and imperative features, built-in search engine, powerful\ntools for search control that allow easy, intuitive, visual development of\nheuristic, nondeterministic, and randomized solutions. We define rigorously the\nsyntax and semantics of the new model of computation, at the same time trying\nto keep clear the intuition behind and to include enough examples. The\npurposely simplified theoretical model is then compared to both WHILE-programs\n(thus demonstrating its Turing-completeness), and the \"real\" CNP. Finally,\nfuture research possibilities are mentioned that would eventually extend the\ncinnamon programming into the directions of nondeterminism, randomness, and\nfuzziness.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 08:13:10 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Kratchanov", "Kostadin", ""]]}, {"id": "1710.06515", "submitter": "Quang Loc Le", "authors": "Quang Loc Le", "title": "Enhancing Inductive Entailment Proofs in Separation Logic with Lemma\n  Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach to lemma synthesis to support advanced\ninductive entailment procedures based on separation logic. We first propose a\nmechanism where lemmas are automatically proven and systematically applied. The\nlemmas may include universal guard and/or unknown predicate. While the former\nis critical for expressivity, the latter is essential for supporting\nrelationships between multiple predicates. We further introduce lemma synthesis\nto support (i) automated inductive reasoning together with frame inference and\n(ii) theorem exploration. For (i) we automatically discover and prove auxiliary\nlemmas during an inductive proof; and for (ii) we automatically generate a\nuseful set of lemmas to relate user-defined or system-generated predicates. We\nhave implemented our proposed approach into an existing verification system and\ntested its capability in inductive reasoning and theorem exploration. The\nexperimental results show that the enhanced system can automatically synthesize\nuseful lemmas to facilitate reasoning on a broad range of non-trivial inductive\nproofs.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 22:13:25 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 10:10:29 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Le", "Quang Loc", ""]]}, {"id": "1710.06744", "submitter": "Christoph Rauch", "authors": "Clovis Eberhart (LAMA), Tom Hirschowitz (LAMA), Thomas Seiller (IHES)", "title": "An intensionally fully-abstract sheaf model for $\\pi$ (expanded version)", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 13, Issue 4 (November\n  15, 2017) lmcs:4069", "doi": "10.23638/LMCS-13(4:9)2017", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following previous work on CCS, we propose a compositional model for the\n$\\pi$-calculus in which processes are interpreted as sheaves on certain simple\nsites. Such sheaves are a concurrent form of innocent strategies, in the sense\nof Hyland-Ong/Nickau game semantics. We define an analogue of fair testing\nequivalence in the model and show that our interpretation is intensionally\nfully abstract for it. That is, the interpretation preserves and reflects fair\ntesting equivalence; and furthermore, any innocent strategy is fair testing\nequivalent to the interpretation of some process. The central part of our work\nis the construction of our sites, relying on a combinatorial presentation of\n$\\pi$-calculus traces in the spirit of string diagrams.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 14:21:10 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 09:48:28 GMT"}, {"version": "v3", "created": "Tue, 14 Nov 2017 10:50:00 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Eberhart", "Clovis", "", "LAMA"], ["Hirschowitz", "Tom", "", "LAMA"], ["Seiller", "Thomas", "", "IHES"]]}, {"id": "1710.06892", "submitter": "Tongfei Chen", "authors": "Tongfei Chen", "title": "Typesafe Abstractions for Tensor Operations", "comments": null, "journal-ref": "Tongfei Chen (2017): Typesafe Abstractions for Tensor Operations\n  (Short Paper). In Proceedings of the 8th ACM SIGPLAN International Symposium\n  on Scala. pp. 45-50", "doi": "10.1145/3136000.3136001", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a typesafe abstraction to tensors (i.e. multidimensional arrays)\nexploiting the type-level programming capabilities of Scala through\nheterogeneous lists (HList), and showcase typesafe abstractions of common\ntensor operations and various neural layers such as convolution or recurrent\nneural networks. This abstraction could lay the foundation of future typesafe\ndeep learning frameworks that runs on Scala/JVM.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 18:43:38 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Chen", "Tongfei", ""]]}, {"id": "1710.06915", "submitter": "Manuel Krebber B.Sc.", "authors": "Manuel Krebber, Henrik Barthels, Paolo Bientinesi", "title": "MatchPy: A Pattern Matching Library", "comments": "arXiv admin note: substantial text overlap with arXiv:1710.00077", "journal-ref": null, "doi": "10.25080/shinma-7f4c6e7-00b", "report-no": null, "categories": "cs.PL cs.SC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pattern matching is a powerful tool for symbolic computations, based on the\nwell-defined theory of term rewriting systems. Application domains include\nalgebraic expressions, abstract syntax trees, and XML and JSON data.\nUnfortunately, no lightweight implementation of pattern matching as general and\nflexible as Mathematica exists for Python Mathics,MacroPy,patterns,PyPatt.\nTherefore, we created the open source module MatchPy which offers similar\npattern matching functionality in Python using a novel algorithm which finds\nmatches for large pattern sets more efficiently by exploiting similarities\nbetween patterns.\n", "versions": [{"version": "v1", "created": "Mon, 16 Oct 2017 22:29:25 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Krebber", "Manuel", ""], ["Barthels", "Henrik", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1710.07021", "submitter": "M. Ammar Ben Khadra", "authors": "M. Ammar Ben Khadra", "title": "E3Solver: decision tree unification by enumeration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce E3Solver, a unification-based solver for programming-by-example\n(PBE) participating in the 2017 edition of the SyGuS Competition. Our tool\nproceeds in two phases. First, for each individual example, we enumerate a\nterminal expression consistent with it. Then, we unify these expressions using\nconditional expressions in a decision tree. To this end, a suitable condition\nis enumerated for each pair of conflicting examples. This incremental method\nterminates after fitting all examples in the decision tree. E3Solver solves all\n750 instances in the bitvector sub-track in an average time of few seconds\neach. We make our contributions publicly available\n(https://github.com/sygus-tools)\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 07:41:12 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Khadra", "M. Ammar Ben", ""]]}, {"id": "1710.07047", "submitter": "Georges-Axel Jaloyan", "authors": "Georges-Axel Jaloyan", "title": "Safe Pointers in SPARK 2014", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the context of deductive software verification, programs with pointers\npresent a major challenge due to pointer aliasing. In this paper, we introduce\npointers to SPARK, a well-defined subset of the Ada language, intended for\nformal verification of mission-critical software. Our solution is based on\nstatic alias analysis inspired by Rust's borrow-checker and affine types, and\nenforces the Concurrent Read, Exclusive Write principle. This analysis has been\nimplemented in the GNAT Ada compiler and tested against a number of challenging\nexamples including parts of real-life applications. Our tests show that only\nminor changes in the source code are required to fit the idiomatic Ada code\ninto SPARK extended with pointers, which is a significant improvement upon the\nprevious state of the art. The proposed extension has been approved by the\nLanguage Design Committee for SPARK for inclusion in a future version of SPARK,\nand is being discussed by the Ada Rapporteur Group for inclusion in the next\nversion of Ada. In the report, we give a formal presentation of the analysis\nrules for a miniature version of SPARK and prove their soundness. We discuss\nthe implementation and the case studies, and compare our solution with Rust.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 09:19:18 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Jaloyan", "Georges-Axel", ""]]}, {"id": "1710.07191", "submitter": "Oded Padon", "authors": "Oded Padon, Giuliano Losa, Mooly Sagiv, Sharon Shoham", "title": "Paxos Made EPR: Decidable Reasoning about Distributed Protocols", "comments": "61 pages. Full version of paper by the same title presented in OOPSLA\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed protocols such as Paxos play an important role in many computer\nsystems. Therefore, a bug in a distributed protocol may have tremendous\neffects. Accordingly, a lot of effort has been invested in verifying such\nprotocols. However, checking invariants of such protocols is undecidable and\nhard in practice, as it requires reasoning about an unbounded number of nodes\nand messages. Moreover, protocol actions and invariants involve both quantifier\nalternations and higher-order concepts such as set cardinalities and\narithmetic.\n  This paper makes a step towards automatic verification of such protocols. We\naim at a technique that can verify correct protocols and identify bugs in\nincorrect protocols. To this end, we develop a methodology for deductive\nverification based on effectively propositional logic (EPR)---a decidable\nfragment of first-order logic (also known as the Bernays-Sch\\\"onfinkel-Ramsey\nclass). In addition to decidability, EPR also enjoys the finite model property,\nallowing to display violations as finite structures which are intuitive for\nusers. Our methodology involves modeling protocols using general\n(uninterpreted) first-order logic, and then systematically transforming the\nmodel to obtain a model and an inductive invariant that are decidable to check.\nThe steps of the transformations are also mechanically checked, ensuring the\nsoundness of the method. We have used our methodology to verify the safety of\nPaxos, and several of its variants, including Multi-Paxos, Vertical Paxos, Fast\nPaxos, Flexible Paxos and Stoppable Paxos. To the best of our knowledge, this\nwork is the first to verify these protocols using a decidable logic, and the\nfirst formal verification of Vertical Paxos, Fast Paxos and Stoppable Paxos.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 15:37:42 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Padon", "Oded", ""], ["Losa", "Giuliano", ""], ["Sagiv", "Mooly", ""], ["Shoham", "Sharon", ""]]}, {"id": "1710.07308", "submitter": "Catalin Hritcu", "authors": "Guglielmo Fachini, Catalin Hritcu, Marco Stronati, Ana Nora Evans,\n  Th\\'eo Laurent, Arthur Azevedo de Amorim, Benjamin C. Pierce, Andrew Tolmach", "title": "Formally Secure Compilation of Unsafe Low-Level Components (Extended\n  Abstract)", "comments": "PriSC'18 submission, updated to fix a few things", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new formal criterion for secure compilation, providing strong\nsecurity guarantees for components written in unsafe, low-level languages with\nC-style undefined behavior. Our criterion goes beyond recent proposals, which\nprotect the trace properties of a single component against an adversarial\ncontext, to model dynamic compromise in a system of mutually distrustful\ncomponents. Each component is protected from all the others until it receives\nan input that triggers an undefined behavior, causing it to become compromised\nand attack the remaining uncompromised components. To illustrate this model, we\ndemonstrate a secure compilation chain for an unsafe language with buffers,\nprocedures, and components, compiled to a simple RISC abstract machine with\nbuilt-in compartmentalization. The protection guarantees offered by this\nabstract machine can be achieved at the machine-code level using either\nsoftware fault isolation or tag-based reference monitoring. We are working on\nmachine-checked proofs showing that this compiler satisfies our secure\ncompilation criterion.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 18:29:13 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 15:46:40 GMT"}, {"version": "v3", "created": "Tue, 31 Oct 2017 17:45:52 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Fachini", "Guglielmo", ""], ["Hritcu", "Catalin", ""], ["Stronati", "Marco", ""], ["Evans", "Ana Nora", ""], ["Laurent", "Th\u00e9o", ""], ["de Amorim", "Arthur Azevedo", ""], ["Pierce", "Benjamin C.", ""], ["Tolmach", "Andrew", ""]]}, {"id": "1710.07309", "submitter": "Catalin Hritcu", "authors": "Deepak Garg, Catalin Hritcu, Marco Patrignani, Marco Stronati, David\n  Swasey", "title": "Robust Hyperproperty Preservation for Secure Compilation (Extended\n  Abstract)", "comments": "PriSC'18 final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We map the space of soundness criteria for secure compilation based on the\npreservation of hyperproperties in arbitrary adversarial contexts, which we\ncall robust hyperproperty preservation. For this, we study the preservation of\nseveral classes of hyperproperties and for each class we propose an equivalent\n\"property-free\" characterization of secure compilation that is generally better\ntailored for proofs. Even the strongest of our soundness criteria, the robust\npreservation of all hyperproperties, seems achievable for simple\ntransformations and provable using context back-translation techniques\npreviously developed for showing fully abstract compilation. While proving the\nrobust preservation of hyperproperties that are not safety requires such\npowerful context back-translation techniques, for preserving safety\nhyperproperties robustly, translating each finite trace prefix back to a source\ncontext seems to suffice.\n", "versions": [{"version": "v1", "created": "Thu, 19 Oct 2017 18:35:54 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 15:49:53 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 09:48:45 GMT"}, {"version": "v4", "created": "Wed, 20 Dec 2017 18:41:56 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Garg", "Deepak", ""], ["Hritcu", "Catalin", ""], ["Patrignani", "Marco", ""], ["Stronati", "Marco", ""], ["Swasey", "David", ""]]}, {"id": "1710.07623", "submitter": "Gustavo Maciel Dias Vieira", "authors": "Fellipe A. Ugliara, Gustavo M. D. Vieira, Jos\\'e de O. Guimar\\~aes", "title": "Transparent Replication Using Metaprogramming in Cyan", "comments": "24 pages, revised and expanded journal version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replication can be used to increase the availability of a service by creating\nmany operational copies of its data called replicas. Active replication is a\nform of replication that has strong consistency semantics, easier to reason\nabout and program. However, creating replicated services using active\nreplication still demands from the programmer the knowledge of subtleties of\nthe replication mechanism. In this paper we show how to use the metaprogramming\ninfrastructure of the Cyan language to shield the application programmer from\nthese details, allowing easier creation of fault-tolerant replicated\napplications through simple annotations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Oct 2017 17:45:37 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 19:31:19 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Ugliara", "Fellipe A.", ""], ["Vieira", "Gustavo M. D.", ""], ["Guimar\u00e3es", "Jos\u00e9 de O.", ""]]}, {"id": "1710.07740", "submitter": "Xinyu Wang", "authors": "Xinyu Wang and Isil Dillig and Rishabh Singh", "title": "Program Synthesis using Abstraction Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to example-guided program synthesis based on\ncounterexample-guided abstraction refinement. Our method uses the abstract\nsemantics of the underlying DSL to find a program $P$ whose abstract behavior\nsatisfies the examples. However, since program $P$ may be spurious with respect\nto the concrete semantics, our approach iteratively refines the abstraction\nuntil we either find a program that satisfies the examples or prove that no\nsuch DSL program exists. Because many programs have the same input-output\nbehavior in terms of their abstract semantics, this synthesis methodology\nsignificantly reduces the search space compared to existing techniques that use\npurely concrete semantics. While synthesis using abstraction refinement\n(SYNGAR) could be implemented in different settings, we propose a\nrefinement-based synthesis algorithm that uses abstract finite tree automata\n(AFTA). Our technique uses a coarse initial program abstraction to construct an\ninitial AFTA, which is iteratively refined by constructing a proof of\nincorrectness of any spurious program. In addition to ruling out the spurious\nprogram accepted by the previous AFTA, proofs of incorrectness are also useful\nfor ruling out many other spurious programs. We implement these ideas in a\nframework called \\tool. We have used the BLAZE framework to build synthesizers\nfor string and matrix transformations, and we compare BLAZE with existing\ntechniques. Our results for the string domain show that BLAZE compares\nfavorably with FlashFill, a domain-specific synthesizer that is now deployed in\nMicrosoft PowerShell. In the context of matrix manipulations, we compare BLAZE\nagainst Prose, a state-of-the-art general-purpose VSA-based synthesizer, and\nshow that BLAZE results in a 90x speed-up over Prose.\n", "versions": [{"version": "v1", "created": "Sat, 21 Oct 2017 02:23:46 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Wang", "Xinyu", ""], ["Dillig", "Isil", ""], ["Singh", "Rishabh", ""]]}, {"id": "1710.08016", "submitter": "Luca Laurenti", "authors": "Alessandro Abate, Luca Cardelli, Marta Kwiatkowska, Luca Laurenti,\n  Boyan Yordanov", "title": "Experimental Biological Protocols with Formal Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both experimental and computational biology is becoming increasingly\nautomated. Laboratory experiments are now performed automatically on\nhigh-throughput machinery, while computational models are synthesized or\ninferred automatically from data. However, integration between automated tasks\nin the process of biological discovery is still lacking, largely due to\nincompatible or missing formal representations. While theories are expressed\nformally as computational models, existing languages for encoding and\nautomating experimental protocols often lack formal semantics. This makes it\nchallenging to extract novel understanding by identifying when theory and\nexperimental evidence disagree due to errors in the models or the protocols\nused to validate them. To address this, we formalize the syntax of a core\nprotocol language, which provides a unified description for the models of\nbiochemical systems being experimented on, together with the discrete events\nrepresenting the liquid-handling steps of biological protocols. We present both\na deterministic and a stochastic semantics to this language, both defined in\nterms of hybrid processes. In particular, the stochastic semantics captures\nuncertainties in equipment tolerances, making it a suitable tool for both\nexperimental and computational biologists. We illustrate how the proposed\nprotocol language can be used for automated verification and synthesis of\nlaboratory experiments on case studies from the fields of chemistry and\nmolecular programming.\n", "versions": [{"version": "v1", "created": "Sun, 22 Oct 2017 21:03:34 GMT"}, {"version": "v2", "created": "Sun, 6 May 2018 11:01:56 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Abate", "Alessandro", ""], ["Cardelli", "Luca", ""], ["Kwiatkowska", "Marta", ""], ["Laurenti", "Luca", ""], ["Yordanov", "Boyan", ""]]}, {"id": "1710.08332", "submitter": "Robert Atkey", "authors": "Robert Atkey, Michel Steuwer, Sam Lindley, Christophe Dubach", "title": "Strategy Preserving Compilation for Parallel Functional Code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graphics Processing Units (GPUs) and other parallel devices are widely\navailable and have the potential for accelerating a wide class of algorithms.\nHowever, expert programming skills are required to achieving maximum\nperformance. hese devices expose low-level hardware details through imperative\nprogramming interfaces where programmers explicity encode device-specific\noptimisation strategies. This inevitably results in non-performance-portable\nprograms delivering suboptimal performance on other devices.\n  Functional programming models have recently seen a renaissance in the systems\ncommunity as they offer possible solutions for tackling the performance\nportability challenge. Recent work has shown how to automatically choose\nhigh-performance parallelisation strategies for a wide range of hardware\narchitectures encoded in a functional representation. However, the translation\nof such functional representations to the imperative program expected by the\nhardware interface is typically performed ad hoc with no correctness guarantees\nand no guarantees to preserve the intended parallelisation strategy.\n  In this paper, we present a formalised strategy-preserving translation from\nhigh-level functional code to low-level data race free parallel imperative\ncode. This translation is formulated and proved correct within a language we\ncall Data Parallel Idealised Algol (DPIA), a dialect of Reynolds' Idealised\nAlgol. Performance results on GPUs and a multicore CPU show that the formalised\ntranslation process generates low-level code with performance on a par with\ncode generated from ad hoc approaches.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 15:24:30 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Atkey", "Robert", ""], ["Steuwer", "Michel", ""], ["Lindley", "Sam", ""], ["Dubach", "Christophe", ""]]}, {"id": "1710.08444", "submitter": "Vasileios Koutavas", "authors": "Edsko de Vries, Vasileios Koutavas", "title": "Locally Nameless Permutation Types", "comments": "Coq code in ancillary files", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define \"Locally Nameless Permutation Types\", which fuse permutation types\nas used in Nominal Isabelle with the locally nameless representation. We show\nthat this combination is particularly useful when formalizing programming\nlanguages where bound names may become free during execution (\"extrusion\"),\ncommon in process calculi. It inherits the generic definition of permutations\nand support, and associated lemmas, from the Nominal approach, and the ability\nto stay close to pencil-and-paper proofs from the locally nameless approach. We\nexplain how to use cofinite quantification in this setting, show why reasoning\nabout renaming is more important here than in languages without extrusion, and\nprovide results about infinite support, necessary when reasoning about\ncountable choice.\n", "versions": [{"version": "v1", "created": "Mon, 23 Oct 2017 18:30:04 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["de Vries", "Edsko", ""], ["Koutavas", "Vasileios", ""]]}, {"id": "1710.08614", "submitter": "Naoki Kobayashi", "authors": "Naoki Kobayashi, Takeshi Tsukada, Keiichi Watanabe", "title": "Higher-Order Program Verification via HFL Model Checking", "comments": "A shorter version is published in Proceedings of ESOP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two kinds of higher-order extensions of model checking: HORS model\nchecking and HFL model checking. Whilst the former has been applied to\nautomated verification of higher-order functional programs, applications of the\nlatter have not been well studied. In the present paper, we show that various\nverification problems for functional programs, including may/must-reachability,\ntrace properties, and linear-time temporal properties (and their negations),\ncan be naturally reduced to (extended) HFL model checking. The reductions yield\na sound and complete logical characterization of those program properties.\nCompared with the previous approaches based on HORS model checking, our\napproach provides a more uniform, streamlined method for higher-order program\nverification.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 06:25:05 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 02:05:26 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Kobayashi", "Naoki", ""], ["Tsukada", "Takeshi", ""], ["Watanabe", "Keiichi", ""]]}, {"id": "1710.08668", "submitter": "Yotam Feldman", "authors": "Yotam M. Y. Feldman, Oded Padon, Neil Immerman, Mooly Sagiv and Sharon\n  Shoham", "title": "Bounded Quantifier Instantiation for Checking Inductive Invariants", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 3 (August\n  21, 2019) lmcs:5700", "doi": "10.23638/LMCS-15(3:18)2019", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of checking whether a proposed invariant $\\varphi$\nexpressed in first-order logic with quantifier alternation is inductive, i.e.\npreserved by a piece of code. While the problem is undecidable, modern SMT\nsolvers can sometimes solve it automatically. However, they employ powerful\nquantifier instantiation methods that may diverge, especially when $\\varphi$ is\nnot preserved. A notable difficulty arises due to counterexamples of infinite\nsize.\n  This paper studies Bounded-Horizon instantiation, a natural method for\nguaranteeing the termination of SMT solvers. The method bounds the depth of\nterms used in the quantifier instantiation process. We show that this method is\nsurprisingly powerful for checking quantified invariants in uninterpreted\ndomains. Furthermore, by producing partial models it can help the user diagnose\nthe case when $\\varphi$ is not inductive, especially when the underlying reason\nis the existence of infinite counterexamples.\n  Our main technical result is that Bounded-Horizon is at least as powerful as\ninstrumentation, which is a manual method to guarantee convergence of the\nsolver by modifying the program so that it admits a purely universal invariant.\nWe show that with a bound of 1 we can simulate a natural class of\ninstrumentations, without the need to modify the code and in a fully automatic\nway. We also report on a prototype implementation on top of Z3, which we used\nto verify several examples by Bounded-Horizon of bound 1.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 09:18:55 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 20:36:00 GMT"}, {"version": "v3", "created": "Tue, 20 Aug 2019 08:41:52 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Feldman", "Yotam M. Y.", ""], ["Padon", "Oded", ""], ["Immerman", "Neil", ""], ["Sagiv", "Mooly", ""], ["Shoham", "Sharon", ""]]}, {"id": "1710.09010", "submitter": "Justin Hsu", "authors": "Tetsuya Sato, Gilles Barthe, Marco Gaboardi, Justin Hsu, Shin-ya\n  Katsumata", "title": "Approximate Span Liftings", "comments": null, "journal-ref": null, "doi": "10.1109/LICS.2019.8785668", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop new abstractions for reasoning about relaxations of differential\nprivacy: R\\'enyi differential privacy, zero-concentrated differential privacy,\nand truncated concentrated differential privacy, which express different bounds\non statistical divergences between two output probability distributions. In\norder to reason about such properties compositionally, we introduce approximate\nspan-lifting, a novel construction extending the approximate relational lifting\napproaches previously developed for standard differential privacy to a more\ngeneral class of divergences, and also to continuous distributions. As an\napplication, we develop a program logic based on approximate span-liftings\ncapable of proving relaxations of differential privacy and other statistical\ndivergence properties.\n", "versions": [{"version": "v1", "created": "Tue, 24 Oct 2017 22:30:15 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 16:04:04 GMT"}, {"version": "v3", "created": "Mon, 16 Jul 2018 09:58:48 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Sato", "Tetsuya", ""], ["Barthe", "Gilles", ""], ["Gaboardi", "Marco", ""], ["Hsu", "Justin", ""], ["Katsumata", "Shin-ya", ""]]}, {"id": "1710.09469", "submitter": "Ale\\v{s} Bizjak", "authors": "Dariusz Biernacki and Piotr Polesiuk", "title": "Logical relations for coherence of effect subtyping", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 14, Issue 1 (January\n  30, 2018) lmcs:4243", "doi": "10.23638/LMCS-14(1:11)2018", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A coercion semantics of a programming language with subtyping is typically\ndefined on typing derivations rather than on typing judgments. To avoid\nsemantic ambiguity, such a semantics is expected to be coherent, i.e.,\nindependent of the typing derivation for a given typing judgment. In this\narticle we present heterogeneous, biorthogonal, step-indexed logical relations\nfor establishing the coherence of coercion semantics of programming languages\nwith subtyping. To illustrate the effectiveness of the proof method, we develop\na proof of coherence of a type-directed, selective CPS translation from a typed\ncall-by-value lambda calculus with delimited continuations and control-effect\nsubtyping. The article is accompanied by a Coq formalization that relies on a\nnovel shallow embedding of a logic for reasoning about step-indexing.\n", "versions": [{"version": "v1", "created": "Wed, 25 Oct 2017 21:32:11 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 09:34:46 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Biernacki", "Dariusz", ""], ["Polesiuk", "Piotr", ""]]}, {"id": "1710.09500", "submitter": "Shusen Liu", "authors": "Shusen Liu, Xin Wang, Li Zhou, Ji Guan, Yinan Li, Yang He, Runyao\n  Duan, Mingsheng Ying", "title": "$Q|SI\\rangle$: A Quantum Programming Environment", "comments": "30 pages, software available at http://www.qcompiler.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a quantum programming environment, named $Q|SI\\rangle$.\nIt is a platform embedded in the .Net language that supports quantum\nprogramming using a quantum extension of the $\\mathbf{while}$-language. The\nframework of the platform includes a compiler of the quantum\n$\\mathbf{while}$-language and a suite of tools for simulating quantum\ncomputation, optimizing quantum circuits, and analyzing and verifying quantum\nprograms. Throughout the paper, using $Q|SI\\rangle$ to simulate quantum\nbehaviors on classical platforms with a combination of components is\ndemonstrated. The scalable framework allows the user to program customized\nfunctions on the platform. The compiler works as the core of $Q|SI\\rangle$\nbridging the gap from quantum hardware to quantum software. The built-in\ndecomposition algorithms enable the universal quantum computation on the\npresent quantum hardware.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 00:32:18 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Liu", "Shusen", ""], ["Wang", "Xin", ""], ["Zhou", "Li", ""], ["Guan", "Ji", ""], ["Li", "Yinan", ""], ["He", "Yang", ""], ["Duan", "Runyao", ""], ["Ying", "Mingsheng", ""]]}, {"id": "1710.09635", "submitter": "Quang-Trung Ta", "authors": "Quang-Trung Ta, Ton Chanh Le, Siau-Cheng Khoo, Wei-Ngan Chin", "title": "Automated Lemma Synthesis in Symbolic-Heap Separation Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The symbolic-heap fragment of separation logic has been actively developed\nand advocated for verifying the memory-safety property of computer programs. At\npresent, one of its biggest challenges is to effectively prove entailments\ncontaining inductive heap predicates. These entailments are usually proof\nobligations generated when verifying programs that manipulate complex data\nstructures like linked lists, trees, or graphs.\n  To assist in proving such entailments, this paper introduces a lemma\nsynthesis framework, which automatically discovers lemmas to serve as eureka\nsteps in the proofs. Mathematical induction and template-based constraint\nsolving are two pillars of our framework. To derive the supporting lemmas for a\ngiven entailment, the framework firstly identifies possible lemma templates\nfrom the entailment's heap structure. It then sets up unknown relations among\neach template's variables and conducts structural induction proof to generate\nconstraints about these relations. Finally, it solves the constraints to find\nout actual definitions of the unknown relations, thus discovers the lemmas. We\nhave integrated this framework into a prototype prover and have experimented it\non various entailment benchmarks. The experimental results show that our\nlemma-synthesis-assisted prover can prove many entailments that could not be\nhandled by existing techniques. This new proposal opens up more opportunities\nto automatically reason with complex inductive heap predicates.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 10:49:29 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 15:41:02 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 07:01:50 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Ta", "Quang-Trung", ""], ["Le", "Ton Chanh", ""], ["Khoo", "Siau-Cheng", ""], ["Chin", "Wei-Ngan", ""]]}, {"id": "1710.09756", "submitter": "Arnaud Spiwack", "authors": "Jean-Philippe Bernardy, Mathieu Boespflug, Ryan R. Newton, Simon\n  Peyton Jones, Arnaud Spiwack", "title": "Linear Haskell: practical linearity in a higher-order polymorphic\n  language", "comments": null, "journal-ref": null, "doi": "10.1145/3158093", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Linear type systems have a long and storied history, but not a clear path\nforward to integrate with existing languages such as OCaml or Haskell. In this\npaper, we study a linear type system designed with two crucial properties in\nmind: backwards-compatibility and code reuse across linear and non-linear users\nof a library. Only then can the benefits of linear types permeate conventional\nfunctional programming. Rather than bifurcate types into linear and non-linear\ncounterparts, we instead attach linearity to function arrows. Linear functions\ncan receive inputs from linearly-bound values, but can also operate over\nunrestricted, regular values.\n  To demonstrate the efficacy of our linear type system - both how easy it can\nbe integrated in an existing language implementation and how streamlined it\nmakes it to write programs with linear types - we implemented our type system\nin GHC, the leading Haskell compiler, and demonstrate two kinds of applications\nof linear types: mutable data with pure interfaces; and enforcing protocols in\nI/O-performing functions.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 15:28:32 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 17:07:41 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Bernardy", "Jean-Philippe", ""], ["Boespflug", "Mathieu", ""], ["Newton", "Ryan R.", ""], ["Jones", "Simon Peyton", ""], ["Spiwack", "Arnaud", ""]]}, {"id": "1710.09844", "submitter": "Kartik Nagar", "authors": "Gowtham Kaki, Kartik Nagar, Mahsa Nazafzadeh and Suresh Jagannathan", "title": "Alone Together: Compositional Reasoning and Inference for Weak Isolation", "comments": "46 pages, 12 figures", "journal-ref": null, "doi": "10.1145/3158115", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serializability is a well-understood correctness criterion that simplifies\nreasoning about the behavior of concurrent transactions by ensuring they are\nisolated from each other while they execute. However, enforcing serializable\nisolation comes at a steep cost in performance and hence database systems in\npractice support, and often encourage, developers to implement transactions\nusing weaker alternatives. Unfortunately, the semantics of weak isolation is\npoorly understood, and usually explained only informally in terms of low-level\nimplementation artifacts. Consequently, verifying high-level correctness\nproperties in such environments remains a challenging problem.\n  To address this issue, we present a novel program logic that enables\ncompositional reasoning about the behavior of concurrently executing\nweakly-isolated transactions. Recognizing that the proof burden necessary to\nuse this logic may dissuade application developers, we also describe an\ninference procedure based on this foundation that ascertains the weakest\nisolation level that still guarantees the safety of high-level consistency\ninvariants associated with such transactions. The key to effective inference is\nthe observation that weakly-isolated transactions can be viewed as functional\n(monadic) computations over an abstract database state, allowing us to treat\ntheir operations as state transformers over the database. This interpretation\nenables automated verification using off-the-shelf SMT solvers. Case studies\nand experiments of real-world applications (written in an embedded DSL in\nOCaml) demonstrate the utility of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 18:00:07 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 21:43:32 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Kaki", "Gowtham", ""], ["Nagar", "Kartik", ""], ["Nazafzadeh", "Mahsa", ""], ["Jagannathan", "Suresh", ""]]}, {"id": "1710.09951", "submitter": "Justin Hsu", "authors": "Justin Hsu", "title": "Probabilistic Couplings for Probabilistic Reasoning", "comments": "PhD thesis, University of Pennsylvania, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis explores proofs by coupling from the perspective of formal\nverification. Long employed in probability theory and theoretical computer\nscience, these proofs construct couplings between the output distributions of\ntwo probabilistic processes. Couplings can imply various guarantees comparing\ntwo runs of a probabilistic computation. We first show that proofs in the\nprogram logic pRHL describe couplings. We formalize couplings that establish\nvarious probabilistic properties, including distribution equivalence,\nconvergence, and stochastic domination. Then we give a proofs-as-programs\ninterpretation: a coupling proof encodes a probabilistic product program, whose\nproperties imply relational properties of the original programs. We design the\nlogic xpRHL to construct the product, with extensions to model shift coupling\nand path coupling. We then propose an approximate version of probabilistic\ncoupling and a corresponding proof technique---proof by approximate\ncoupling---inspired by the logic apRHL, a version of pRHL for building\napproximate liftings. Drawing on ideas from existing privacy proofs, we extend\napRHL with novel proof rules for constructing new approximate couplings. We\ngive an approximate coupling proof of privacy for the Sparse Vector mechanism,\na well-known algorithm from the privacy literature whose privacy proof is\nnotoriously subtle, and produce the first formalized proof of privacy for\nSparse Vector in apRHL. Finally, we propose several more sophisticated\nconstructions for approximate couplings: a principle for showing\naccuracy-dependent privacy, a generalization of the advanced composition\ntheorem, and an optimal approximate coupling relating two subsets. We also show\nequivalences between our approximate couplings and other existing definitions.\nThese ingredients support the first formalized proof of privacy for the Between\nThresholds mechanism.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 00:13:45 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 00:44:30 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Hsu", "Justin", ""]]}, {"id": "1710.09968", "submitter": "Mingyu Wu", "authors": "Mingyu Wu, Ziming Zhao, Haoyu Li, Heting Li, Haibo Chen, Binyu Zang,\n  Haibing Guan", "title": "Espresso: Brewing Java For More Non-Volatility with Non-volatile Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast, byte-addressable non-volatile memory (NVM) embraces both near-DRAM\nlatency and disk-like persistence, which has generated considerable interests\nto revolutionize system software stack and programming models. However, it is\nless understood how NVM can be combined with managed runtime like Java virtual\nmachine (JVM) to ease persistence management. This paper proposes Espresso, a\nholistic extension to Java and its runtime, to enable Java programmers to\nexploit NVM for persistence management with high performance. Espresso first\nprovides a general persistent heap design called Persistent Java Heap (PJH) to\nmanage persistent data as normal Java objects. The heap is then strengthened\nwith a recoverable mechanism to provide crash consistency for heap metadata. It\nthen provides a new abstraction called Persistent Java Object (PJO) to provide\nan easy-to-use but safe persistent programming model for programmers to persist\napplication data. The evaluation confirms that Espresso significantly\noutperforms state-of-art NVM support for Java (i.e., JPA and PCJ) while being\ncompatible to existing data structures in Java programs.\n", "versions": [{"version": "v1", "created": "Fri, 27 Oct 2017 02:17:28 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Wu", "Mingyu", ""], ["Zhao", "Ziming", ""], ["Li", "Haoyu", ""], ["Li", "Heting", ""], ["Chen", "Haibo", ""], ["Zang", "Binyu", ""], ["Guan", "Haibing", ""]]}, {"id": "1710.10385", "submitter": "James Koppel", "authors": "James Koppel, Gabriel Scherer, Armando Solar-Lezama", "title": "Capturing the Future by Replaying the Past", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delimited continuations are the mother of all monads! So goes the slogan\ninspired by Filinski's 1994 paper, which showed that delimited continuations\ncan implement any monadic effect, letting the programmer use an effect as\neasily as if it was built into the language. It's a shame that not many\nlanguages have delimited continuations.\n  Luckily, exceptions and state are also the mother of all monads! In this\nPearl, we show how to implement delimited continuations in terms of exceptions\nand state, a construction we call $\\textit{thermometer continuations}$. While\ntraditional implementations of delimited continuations require some way of\n\"capturing\" an intermediate state of the computation, the insight of\nthermometer continuations is to reach this intermediate state by replaying the\nentire computation from the start, guiding it using a recording it so that the\nsame thing happens until the captured point.\n  Along the way, we explain delimited continuations and monadic reflection,\nshow how the Filinski construction lets thermometer continuations express any\nmonadic effect, share an elegant special-case for nondeterminism, and discuss\nwhy our construction is not prevented by theoretical results that exceptions\nand state cannot macro-express continuations.\n", "versions": [{"version": "v1", "created": "Sat, 28 Oct 2017 04:11:48 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 06:52:16 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Koppel", "James", ""], ["Scherer", "Gabriel", ""], ["Solar-Lezama", "Armando", ""]]}, {"id": "1710.10805", "submitter": "Ranald Clouston", "authors": "Zh\\'e H\\'ou, Ranald Clouston, Rajeev Gor\\'e, Alwen Tiu", "title": "Modular Labelled Sequent Calculi for Abstract Separation Logics", "comments": "Accepted for publication in ACM Transactions on Computational Logic\n  (TOCL). arXiv admin note: text overlap with arXiv:1307.5592", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract separation logics are a family of extensions of Hoare logic for\nreasoning about programs that manipulate resources such as memory locations.\nThese logics are \"abstract\" because they are independent of any particular\nconcrete resource model. Their assertion languages, called propositional\nabstract separation logics (PASLs), extend the logic of (Boolean) Bunched\nImplications (BBI) in various ways. In particular, these logics contain the\nconnectives $*$ and $-\\!*$, denoting the composition and extension of resources\nrespectively.\n  This added expressive power comes at a price since the resulting logics are\nall undecidable. Given their wide applicability, even a semi-decision procedure\nfor these logics is desirable. Although several PASLs and their relationships\nwith BBI are discussed in the literature, the proof theory and automated\nreasoning for these logics were open problems solved by the conference version\nof this paper, which developed a modular proof theory for various PASLs using\ncut-free labelled sequent calculi. This paper non-trivially improves upon this\nprevious work by giving a general framework of calculi on which any new axiom\nin the logic satisfying a certain form corresponds to an inference rule in our\nframework, and the completeness proof is generalised to consider such axioms.\n  Our base calculus handles Calcagno et al.'s original logic of separation\nalgebras by adding sound rules for partial-determinism and cancellativity,\nwhile preserving cut-elimination. We then show that many important properties\nin separation logic, such as indivisible unit, disjointness, splittability, and\ncross-split, can be expressed in our general axiom form. Thus our framework\noffers inference rules and completeness for these properties for free. Finally,\nwe show how our calculi reduce to calculi with global label substitutions,\nenabling more efficient implementation.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 08:38:02 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 14:46:57 GMT"}, {"version": "v3", "created": "Mon, 26 Mar 2018 15:00:23 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["H\u00f3u", "Zh\u00e9", ""], ["Clouston", "Ranald", ""], ["Gor\u00e9", "Rajeev", ""], ["Tiu", "Alwen", ""]]}, {"id": "1710.11054", "submitter": "Rishabh Singh", "authors": "Jacob Devlin, Jonathan Uesato, Rishabh Singh, Pushmeet Kohli", "title": "Semantic Code Repair using Neuro-Symbolic Transformation Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of semantic code repair, which can be broadly defined as\nautomatically fixing non-syntactic bugs in source code. The majority of past\nwork in semantic code repair assumed access to unit tests against which\ncandidate repairs could be validated. In contrast, the goal here is to develop\na strong statistical model to accurately predict both bug locations and exact\nfixes without access to information about the intended correct behavior of the\nprogram. Achieving such a goal requires a robust contextual repair model, which\nwe train on a large corpus of real-world source code that has been augmented\nwith synthetically injected bugs. Our framework adopts a two-stage approach\nwhere first a large set of repair candidates are generated by rule-based\nprocessors, and then these candidates are scored by a statistical model using a\nnovel neural network architecture which we refer to as Share, Specialize, and\nCompete. Specifically, the architecture (1) generates a shared encoding of the\nsource code using an RNN over the abstract syntax tree, (2) scores each\ncandidate repair using specialized network modules, and (3) then normalizes\nthese scores together so they can compete against one another in comparable\nprobability space. We evaluate our model on a real-world test set gathered from\nGitHub containing four common categories of bugs. Our model is able to predict\nthe exact correct repair 41\\% of the time with a single guess, compared to 13\\%\naccuracy for an attentional sequence-to-sequence model.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 16:32:45 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Devlin", "Jacob", ""], ["Uesato", "Jonathan", ""], ["Singh", "Rishabh", ""], ["Kohli", "Pushmeet", ""]]}]