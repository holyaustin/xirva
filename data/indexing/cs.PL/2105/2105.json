[{"id": "2105.00399", "submitter": "Ryu Hasegawa", "authors": "Ryu Hasegawa", "title": "Semantic Proof of Confluence of the Categorical Reduction System for\n  Linear Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We verify a confluence result for the rewriting calculus of the linear\ncategory introduced in our previous paper. Together with the termination result\nproved therein, the generalized coherence theorem for linear category is\nestablished. Namely, we obtain a method to determine if two morphisms are equal\nup to a certain equivalence.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 06:14:25 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Hasegawa", "Ryu", ""]]}, {"id": "2105.00417", "submitter": "Sean Anderson", "authors": "Sean Noble Anderson, Leonidas Lampropoulos, Roberto Blanco, Benjamin\n  C. Pierce, and Andrew Tolmach", "title": "Security Properties for Stack Safety", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  What exactly does \"stack safety\" mean? The phrase is associated with a\nvariety of compiler, run-time, and hardware mechanisms for protecting stack\nmemory. But these mechanisms typically lack precise specifications, relying\ninstead on informal descriptions and examples of bad behaviors that they\nprevent.\n  We propose a formal characterization of stack safety, formulated with\nconcepts from language-based security: a combination of an integrity property\n(\"the private state in each caller's stack frame is held invariant by the\ncallee\"), a confidentiality property (\"the callee's behavior is insensitive to\nthe caller's private state\"), and a well-bracketedness property (\"each callee\nreturns control to its immediate caller\"). We use these properties to validate\nthe stack-safety \"micro-policies\" proposed by Roessler and DeHon [2018].\nSpecifically, we check (with property-based random testing) that Roessler and\nDehon's \"eager\" micro-policy, which catches violations as early as possible,\nenforces a simple \"stepwise\" variant of our properties and correctly detects\nseveral broken variants, and that (a repaired version of) their more performant\n\"lazy\" micro-policy corresponds to a slightly weaker and more extensional\n\"observational\" variant of our properties.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 08:18:34 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Anderson", "Sean Noble", ""], ["Lampropoulos", "Leonidas", ""], ["Blanco", "Roberto", ""], ["Pierce", "Benjamin C.", ""], ["Tolmach", "Andrew", ""]]}, {"id": "2105.00493", "submitter": "Pankaj Kumar Kalita", "authors": "Pankaj Kumar Kalita, Sujit Kumar Muduli, Loris D'Antoni, Thomas Reps\n  and Subhajit Roy", "title": "Synthesizing Abstract Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of creating abstract transformers\nautomatically. The method we present provides the basis for creating a tool to\nautomate the construction of program analyzers in a fashion similar to the way\nyacc automates the construction of parsers. Our method treats the problem as a\nprogram-synthesis problem. The user provides specifications of (i) the concrete\nsemantics of a given operation O, (ii) the abstract domain A to be used by the\nanalyzer, and (iii) the semantics of a domain-specific language L in which the\nabstract transformer is to be expressed. As output, our method creates an\nabstract transformer for O for abstract domain A, expressed in DSL L. We\nimplemented our method, and used it to create a set of replacement abstract\ntransformers for those used in an existing analyzer, and obtained essentially\nidentical performance. However, when we compared the existing transformers with\nthe generated transformers, we discovered that two of the existing transformers\nwere unsound, which demonstrates the risk of using manually created\ntransformers.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 15:09:41 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Kalita", "Pankaj Kumar", ""], ["Muduli", "Sujit Kumar", ""], ["D'Antoni", "Loris", ""], ["Reps", "Thomas", ""], ["Roy", "Subhajit", ""]]}, {"id": "2105.00564", "submitter": "Andr\\'es Ezequiel Viso", "authors": "Delia Kesner and Andr\\'es Viso", "title": "The Power of Tightness for Call-By-Push-Value", "comments": "arXiv admin note: text overlap with arXiv:2002.04011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose tight type systems for Call-by-Name (CBN) and Call-by-Value (CBV)\nthat can be both encoded in a tight type system for Call-by-Push-Value (CBPV).\nAll such systems are quantitative, in the sense that they provide exact\ninformation about the length of normalization sequences to normal form\n(discriminated between multiplicative and exponential steps) as well as the\nsize of these normal forms.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 22:27:57 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 13:53:22 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Kesner", "Delia", ""], ["Viso", "Andr\u00e9s", ""]]}, {"id": "2105.00969", "submitter": "Nathanael Arkor", "authors": "Nathanael Arkor, Dylan McDermott", "title": "Abstract clones for abstract syntax", "comments": "To appear in the proceedings of FSCD 2021; 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give a formal treatment of simple type theories, such as the simply-typed\n$\\lambda$-calculus, using the framework of abstract clones. Abstract clones\ntraditionally describe first-order structures, but by equipping them with\nadditional algebraic structure, one can further axiomatize second-order,\nvariable-binding operators. This provides a syntax-independent representation\nof simple type theories. We describe multisorted second-order presentations,\nsuch as the presentation of the simply-typed $\\lambda$-calculus, and their\nclone-theoretic algebras; free algebras on clones abstractly describe the\nsyntax of simple type theories quotiented by equations such as $\\beta$- and\n$\\eta$-equality. We give a construction of free algebras and derive a\ncorresponding induction principle, which facilitates syntax-independent proofs\nof properties such as adequacy and normalization for simple type theories.\nWorking only with clones avoids some of the complexities inherent in\npresheaf-based frameworks for abstract syntax.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 16:09:55 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Arkor", "Nathanael", ""], ["McDermott", "Dylan", ""]]}, {"id": "2105.01344", "submitter": "David Monniaux", "authors": "David Monniaux (VERIMAG - IMAG), Cyril Six", "title": "Simple, Light, Yet Formally Verified, Global Common Subexpression\n  Elimination and Loop-Invariant Code Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for implementing a formally certified loop-invariant\ncode motion optimization by composing an unrolling pass and a formally\ncertified yet efficient global subexpression elimination.This approach is\nlightweight: each pass comes with a simple and independent proof of\ncorrectness.Experiments show the approach significantly narrows the performance\ngap between the CompCert certified compiler and state-of-the-art optimizing\ncompilers.Our static analysis employs an efficient yet verified hashed set\nstructure, resulting in fast compilation.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 07:44:16 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Monniaux", "David", "", "VERIMAG - IMAG"], ["Six", "Cyril", ""]]}, {"id": "2105.01632", "submitter": "Chike Abuah", "authors": "Chike Abuah, David Darais, Joseph P. Near", "title": "Solo: Enforcing Differential Privacy Without Fancy Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  All current approaches for statically enforcing differential privacy in\nhigher order languages make use of either linear or relational refinement\ntypes. A barrier to adoption for these approaches is the lack of support for\nexpressing these \"fancy types\" in mainstream programming languages. For\nexample, no mainstream language supports relational refinement types, and\nalthough Rust and modern versions of Haskell both employ some linear typing\ntechniques, they are inadequate for embedding enforcement of differential\nprivacy, which requires \"full\" linear types a la Girard/Reynolds. We propose a\nnew type system that enforces differential privacy, avoids the use of linear\nand relational refinement types, and can be easily embedded in mainstream\nrichly typed programming languages such as Scala, OCaml and Haskell. We\ndemonstrate such an embedding in Haskell, demonstrate its expressiveness on\ncase studies, and prove that our type-based enforcement of differential privacy\nis sound.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 17:18:16 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Abuah", "Chike", ""], ["Darais", "David", ""], ["Near", "Joseph P.", ""]]}, {"id": "2105.01954", "submitter": "Anish Tondwalkar", "authors": "Anish Tondwalkar, Matthew Kolosick, Ranjit Jhala", "title": "Refinements of Futures Past: Higher-Order Specification with Implicit\n  Refinement Types (Extended Version)", "comments": "To appear at the 35th European Conference on Object-Oriented\n  Programming (ECOOP 2021) Artifact available at:\n  https://github.com/ucsd-progsys/mist/tree/ecoop21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Refinement types decorate types with assertions that enable automatic\nverification. Like assertions, refinements are limited to binders that are in\nscope, and hence, cannot express higher-order specifications. Ghost variables\ncircumvent this limitation but are prohibitively tedious to use as the\nprogrammer must divine and explicate their values at all call-sites. We\nintroduce Implicit Refinement Types which turn ghost variables into implicit\npair and function types, in a way that lets the refinement typechecker\nautomatically synthesize their values at compile time. Implicit Refinement\nTypes further take advantage of refinement type information, allowing them to\nbe used as a lightweight verification tool, rather than merely as a technique\nto automate programming tasks. We evaluate the utility of Implicit Refinement\nTypes by showing how they enable the modular specification and automatic\nverification of various higher-order examples including stateful protocols,\naccess control, and resource usage.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 09:46:45 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Tondwalkar", "Anish", ""], ["Kolosick", "Matthew", ""], ["Jhala", "Ranjit", ""]]}, {"id": "2105.02156", "submitter": "Sean Moss", "authors": "Cristina Matache, Sean Moss, Sam Staton", "title": "Recursion and Sequentiality in Categories of Sheaves", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.FSCD.2021.25", "report-no": null, "categories": "cs.PL cs.LO math.CT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a fully abstract model of a call-by-value language with\nhigher-order functions, recursion and natural numbers, as an exponential ideal\nin a topos. Our model is inspired by the fully abstract models of O'Hearn,\nRiecke and Sandholm, and Marz and Streicher. In contrast with semantics based\non cpo's, we treat recursion as just one feature in a model built by combining\na choice of modular components.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 16:10:06 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Matache", "Cristina", ""], ["Moss", "Sean", ""], ["Staton", "Sam", ""]]}, {"id": "2105.02485", "submitter": "Pierre Clairambault", "authors": "Lison Blondeau-Patissier (ENS Lyon), Pierre Clairambault (LIP, PLUME)", "title": "Positional Injectivity for Innocent Strategies", "comments": null, "journal-ref": "6TH INTERNATIONAL CONFERENCE ON FORMAL STRUCTURES FOR COMPUTATION\n  AND DEDUCTION, Jul 2021, Buenos Aires, Argentina", "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In asynchronous games, Melli{\\`e}s proved that innocent strategies are\npositional: their behaviour only depends on the position, not the temporal\norder used to reach it. This insightful result shaped our understanding of the\nlink between dynamic (i.e. game) and static (i.e. relational) semantics. In\nthis paper, we investigate the positionality of innocent strategies in the\ntraditional setting of Hyland-Ong-Nickau-Coquand pointer games. We show that\nthough innocent strategies are not positional, total finite innocent strategies\nstill enjoy a key consequence of positionality, namely positional injectivity:\nthey are entirely determined by their positions. Unfortunately, this does not\nhold in general: we show a counterexample if finiteness and totality are\nlifted. For finite partial strategies we leave the problem open; we show\nhowever the partial result that two strategies with the same positions must\nhave the same P-views of maximal length.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 07:38:07 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Blondeau-Patissier", "Lison", "", "ENS Lyon"], ["Clairambault", "Pierre", "", "LIP, PLUME"]]}, {"id": "2105.02541", "submitter": "Vasileios Koutavas", "authors": "Vasileios Koutavas and Yu-Yang Lin and Nikos Tzevelekos", "title": "There and Back Again: From Bounded Checking to Verification of Program\n  Equivalence via Symbolic Up-to Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a bounded equivalence verification technique for higher-order\nprograms with local state that combines fully abstract symbolic environmental\nbisimulations similar to symbolic game models, novel up-to techniques which are\neffective in practice even when terms diverge, and lightweight invariant\nannotations. The combination yields an equivalence checking technique with no\nfalse positives or negatives where all inequivalences can be automatically\ndetected, and many equivalences can be automatically or semi-automatically\nproved, including all classical Meyer and Sieber equivalences. We realise the\ntechnique in a tool prototype called Hobbit and benchmark it with an extensive\nset of new and existing examples.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 09:30:31 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Koutavas", "Vasileios", ""], ["Lin", "Yu-Yang", ""], ["Tzevelekos", "Nikos", ""]]}, {"id": "2105.02578", "submitter": "Blair Archibald", "authors": "Blair Archibald, Muffy Calder, Michele Sevegnani, Mengwei Xu", "title": "Modelling and Verifying BDI Agents with Bigraphs", "comments": "50 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Belief-Desire-Intention (BDI) architecture is a popular framework for\nrational agents; most verification approaches are based on reasoning about\nimplementations of BDI programming languages. We investigate an alternative\napproach based on reasoning about BDI agent semantics, through a model of the\nexecution of an agent program. We employ Milner's bigraphs as the modelling\nframework and present an encoding for the Conceptual Agent Notation (CAN)\nlanguage - a superset of AgentSpeak featuring declarative goals, concurrency,\nand failure recovery.\n  We provide an encoding of the syntax and semantics of CAN agents, and give a\nrigorous proof that the encoding is faithful. Verification is based on the use\nof mainstream software tools including BigraphER, and a small case study\nverifying several properties of Unmanned Aerial Vehicles (UAVs) illustrates the\nframework in action. The executable framework is a foundational step that will\nenable more advanced reasoning such as plan preference, intention priorities\nand trade-offs, and interactions with an environment under uncertainty.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 10:49:45 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Archibald", "Blair", ""], ["Calder", "Muffy", ""], ["Sevegnani", "Michele", ""], ["Xu", "Mengwei", ""]]}, {"id": "2105.02632", "submitter": "Han Xu", "authors": "Han Xu and Zhenjiang Hu", "title": "Analytical Differential Calculus with Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Differential lambda-calculus was first introduced by Thomas Ehrhard and\nLaurent Regnier in 2003. Despite more than 15 years of history, little work has\nbeen done on a differential calculus with integration. In this paper, we shall\npropose a differential calculus with integration from programming point of\nview. We show its good correspondence with mathematics, which is manifested by\nhow we construct these reduction rules and how we preserve important\nmathematical theorems in our calculus. Moreover, we highlight applications of\nthe calculus in incremental computation, automatic differentiation, and\ncomputation approximation.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 13:06:55 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 01:43:31 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Xu", "Han", ""], ["Hu", "Zhenjiang", ""]]}, {"id": "2105.02856", "submitter": "Krzysztof Maziarz", "authors": "Krzysztof Maziarz, Tom Ellis, Alan Lawrence, Andrew Fitzgibbon, Simon\n  Peyton Jones", "title": "Hashing Modulo Alpha-Equivalence", "comments": "Accepted for publication at the 42nd ACM SIGPLAN International\n  Conference on Programming Language Design and Implementation (PLDI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications one wants to identify identical subtrees of a program\nsyntax tree. This identification should ideally be robust to alpha-renaming of\nthe program, but no existing technique has been shown to achieve this with good\nefficiency (better than $\\mathcal{O}(n^2)$ in expression size). We present a\nnew, asymptotically efficient way to hash modulo alpha-equivalence. A key\ninsight of our method is to use a weak (commutative) hash combiner at exactly\none point in the construction, which admits an algorithm with $\\mathcal{O}(n\n(\\log n)^2)$ time complexity. We prove that the use of the commutative combiner\nnevertheless yields a strong hash with low collision probability. Numerical\nbenchmarks attest to the asymptotic behaviour of the method.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:44:30 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Maziarz", "Krzysztof", ""], ["Ellis", "Tom", ""], ["Lawrence", "Alan", ""], ["Fitzgibbon", "Andrew", ""], ["Jones", "Simon Peyton", ""]]}, {"id": "2105.03099", "submitter": "Shmuel Tyszberowicz", "authors": "Aharon Abadi and Bar Makovitzki and Ron Shemer and Shmuel Tyszberowicz", "title": "NoCFG: A Lightweight Approach for Sound Call Graph Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interprocedural analysis refers to gathering information about the entire\nprogram rather than for a single procedure only, as in intraprocedural\nanalysis. Interprocedural analysis enables a more precise analysis; however, it\nis complicated due to the difficulty of constructing an accurate program call\ngraph. Current algorithms for constructing sound and precise call graphs\nanalyze complex program dependencies, therefore they might be difficult to\nscale. Their complexity stems from the kind of type-inference analysis they\nuse, in particular the use of some variations of points-to analysis. To address\nthis problem, we propose NoCFG, a new sound and scalable method for\napproximating a call graph that supports a wide variety of programming\nlanguages. A key property of NoCFG is that it works on a coarse abstraction of\nthe program, discarding many of the programming language constructs. Due to the\ncoarse program abstraction, extending it to support also other languages is\neasy. We provide a formal proof for the soundness of NoCFG and evaluations for\nreal-world projects written in both Python and C#. The experimental results\ndemonstrate a high precision rate of 90% (lower bound) and scalability through\na security use-case over projects with up to 2 million lines of code.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 07:58:41 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Abadi", "Aharon", ""], ["Makovitzki", "Bar", ""], ["Shemer", "Ron", ""], ["Tyszberowicz", "Shmuel", ""]]}, {"id": "2105.03131", "submitter": "Zeki Bilgin", "authors": "Zeki Bilgin", "title": "Code2Image: Intelligent Code Analysis by Computer Vision Techniques and\n  Application to Vulnerability Prediction", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CR cs.LG cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Intelligent code analysis has received increasing attention in parallel with\nthe remarkable advances in the field of machine learning (ML) in recent years.\nA major challenge in leveraging ML for this purpose is to represent source code\nin a useful form that ML algorithms can accept as input. In this study, we\npresent a novel method to represent source code as image while preserving\nsemantic and syntactic properties, which paves the way for leveraging computer\nvision techniques to use for code analysis. Indeed the method makes it possible\nto directly enter the resulting image representation of source codes into deep\nlearning (DL) algorithms as input without requiring any further data\npre-processing or feature extraction step. We demonstrate feasibility and\neffectiveness of our method by realizing a vulnerability prediction use case\nover a public dataset containing a large number of real-world source code\nsamples with performance evaluation in comparison to the state-of-art\nsolutions. Our implementation is publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 09:10:20 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Bilgin", "Zeki", ""]]}, {"id": "2105.03215", "submitter": "Yida Wang", "authors": "Zhi Chen, Cody Hao Yu, Trevor Morris, Jorn Tuyls, Yi-Hsiang Lai, Jared\n  Roesch, Elliott Delaye, Vin Sharma, Yida Wang", "title": "Bring Your Own Codegen to Deep Learning Compiler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have been ubiquitously applied in many\napplications, and accelerators are emerged as an enabler to support the fast\nand efficient inference tasks of these applications. However, to achieve high\nmodel coverage with high performance, each accelerator vendor has to develop a\nfull compiler stack to ingest, optimize, and execute the DNNs. This poses\nsignificant challenges in the development and maintenance of the software\nstack. In addition, the vendors have to contiguously update their hardware\nand/or software to cope with the rapid evolution of the DNN model architectures\nand operators. To address these issues, this paper proposes an open source\nframework that enables users to only concentrate on the development of their\nproprietary code generation tools by reusing as many as possible components in\nthe existing deep learning compilers. Our framework provides users flexible and\neasy-to-use interfaces to partition their models into segments that can be\nexecuted on \"the best\" processors to take advantage of the powerful computation\ncapability of accelerators. Our case study shows that our framework has been\ndeployed in multiple commercial vendors' compiler stacks with only a few\nthousand lines of code.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:22:25 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Chen", "Zhi", ""], ["Yu", "Cody Hao", ""], ["Morris", "Trevor", ""], ["Tuyls", "Jorn", ""], ["Lai", "Yi-Hsiang", ""], ["Roesch", "Jared", ""], ["Delaye", "Elliott", ""], ["Sharma", "Vin", ""], ["Wang", "Yida", ""]]}, {"id": "2105.03317", "submitter": "Celine Lee", "authors": "Celine Lee (1 and 2), Justin Gottschlich (1 and 2), Dan Roth (2) ((1)\n  Intel Labs, (2) University of Pennsylvania)", "title": "Toward Code Generation: A Survey and Lessons from Semantic Parsing", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of natural language processing techniques and demand for\nimproved software engineering efficiency, there is an emerging interest in\ntranslating intention from human languages to programming languages. In this\nsurvey paper, we attempt to provide an overview of the growing body of research\nin this space. We begin by reviewing natural language semantic parsing\ntechniques and draw parallels with program synthesis efforts. We then consider\nsemantic parsing works from an evolutionary perspective, with specific analyses\non neuro-symbolic methods, architecture, and supervision. We then analyze\nadvancements in frameworks for semantic parsing for code generation. In\nclosing, we present what we believe are some of the emerging open challenges in\nthis domain.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 22:05:22 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Lee", "Celine", "", "1 and 2"], ["Gottschlich", "Justin", "", "1 and 2"], ["Roth", "Dan", ""]]}, {"id": "2105.03389", "submitter": "Daniel Jeffries", "authors": "Patricia Johann, Enrico Ghiorzi, and Daniel Jeffries", "title": "GADTs, Functoriality, Parametricity: Pick Two", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GADTs can be represented either as their Church encodings \\`a la Atkey, or as\nfixpoints \\`a la Johann and Polonsky. While a GADT represented as its Church\nencoding need not support a map function satisfying the functor laws, the\nfixpoint representation of a GADT must support such a map function even to be\nwell-defined. The two representations of a GADT thus need not be the same in\ngeneral. This observation forces a choice of representation of data types in\nlanguages supporting GADTs. In this paper we show that choosing whether to\nrepresent data types as their Church encodings or as fixpoints determines\nwhether or not a language supporting GADTs can have parametric models. This\nchoice thus has important consequences for how we can program with, and reason\nabout, these advanced data types.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 16:50:42 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Johann", "Patricia", ""], ["Ghiorzi", "Enrico", ""], ["Jeffries", "Daniel", ""]]}, {"id": "2105.03522", "submitter": "Andrea Colledan", "authors": "Andrea Colledan", "title": "On Abstract Machine Semantics for Proto-Quipper-M", "comments": "72 pages (34 without appendix), 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO quant-ph", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Quipper is a domain-specific programming language for the description of\nquantum circuits. Because it is implemented as an embedded language in Haskell,\nQuipper is a very practical functional language. However, for the same reason,\nit lacks a formal semantics and it is limited by Haskell's type system. In\nparticular, because Haskell lacks linear types, it is easy to write Quipper\nprograms that violate the non-cloning property of quantum states. In order to\nformalize relevant fragments of Quipper in a type-safe way, the Proto-Quipper\nfamily of research languages has been introduced over the last years. In this\npaper we first review Proto-Quipper-M, an instance of the Proto-Quipper family\nbased on a categorical model for quantum circuits, which features a linear type\nsystem that guarantees that the non-cloning property holds at compile time. We\nthen derive a tentative small-step operational semantics from the big-step\nsemantics of Proto-Quipper-M and we prove that the two are equivalent. After\nproving subject reduction and progress results for the tentative semantics, we\nbuild upon it to obtain a truly small-step semantics in the style of an\nabstract machine, which we eventually prove to be equivalent to the original\nsemantics.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 22:16:11 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Colledan", "Andrea", ""]]}, {"id": "2105.03595", "submitter": "Yun Peng", "authors": "Yun Peng, Zongjie Li, Cuiyun Gao, Bowei Gao, David Lo, Michael Lyu", "title": "HiTyper: A Hybrid Static Type Inference Framework with Neural Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Type inference for dynamic programming languages is an important yet\nchallenging task. By leveraging the natural language information of existing\nhuman annotations, deep neural networks outperform other traditional techniques\nand become the state-of-the-art (SOTA) in this task. However, they are facing\nsome new challenges, such as fixed type set, type drift, type correctness, and\ncomposite type prediction. To mitigate the challenges, in this paper, we\npropose a hybrid type inference framework named HiTyper, which integrates\nstatic inference into deep learning (DL) models for more accurate type\nprediction. Specifically, HiTyper creates a new syntax graph for each program,\ncalled type graph, illustrating the type flow among all variables in the\nprogram. Based on the type graph, HiTyper statically infers the types of the\nvariables with appropriate static constraints. HiTyper then adopts a SOTA DL\nmodel to predict the types of other variables that cannot be inferred\nstatically, during which process a type correction algorithm is employed to\nvalidate and correct the types recommended by the DL model. Extensive\nexperiments show that HiTyper outperforms the SOTA DL approach by 12.7% in\nterms of top-1 F1-score. Moreover, HiTyper filters out 50.6% of incorrect\ncandidate types recommended by the SOTA DL model, indicating that HiTyper could\nimprove the correctness of predicted types. Case studies also demonstrate the\ncapability of HiTyper in alleviating the fixed type set issue, and in handling\ntype drift and complicated types such as composite data types.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 05:00:52 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Peng", "Yun", ""], ["Li", "Zongjie", ""], ["Gao", "Cuiyun", ""], ["Gao", "Bowei", ""], ["Lo", "David", ""], ["Lyu", "Michael", ""]]}, {"id": "2105.03949", "submitter": "Shashi Gowda", "authors": "Shashi Gowda, Yingbo Ma, Alessandro Cheli, Maja Gwozdz, Viral B. Shah,\n  Alan Edelman, Christopher Rackauckas", "title": "High-performance symbolic-numerics via multiple dispatch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.MS cs.PL cs.SC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As mathematical computing becomes more democratized in high-level languages,\nhigh-performance symbolic-numeric systems are necessary for domain scientists\nand engineers to get the best performance out of their machine without deep\nknowledge of code optimization. Naturally, users need different term types\neither to have different algebraic properties for them, or to use efficient\ndata structures. To this end, we developed Symbolics.jl, an extendable symbolic\nsystem which uses dynamic multiple dispatch to change behavior depending on the\ndomain needs. In this work we detail an underlying abstract term interface\nwhich allows for speed without sacrificing generality. We show that by\nformalizing a generic API on actions independent of implementation, we can\nretroactively add optimized data structures to our system without changing the\npre-existing term rewriters. We showcase how this can be used to optimize term\nconstruction and give a 113x acceleration on general symbolic transformations.\nFurther, we show that such a generic API allows for complementary\nterm-rewriting implementations. We demonstrate the ability to swap between\nclassical term-rewriting simplifiers and e-graph-based term-rewriting\nsimplifiers. We showcase an e-graph ruleset which minimizes the number of CPU\ncycles during expression evaluation, and demonstrate how it simplifies a\nreal-world reaction-network simulation to halve the runtime. Additionally, we\nshow a reaction-diffusion partial differential equation solver which is able to\nbe automatically converted into symbolic expressions via multiple dispatch\ntracing, which is subsequently accelerated and parallelized to give a 157x\nsimulation speedup. Together, this presents Symbolics.jl as a next-generation\nsymbolic-numeric computing environment geared towards modeling and simulation.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 14:22:43 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 17:02:31 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Gowda", "Shashi", ""], ["Ma", "Yingbo", ""], ["Cheli", "Alessandro", ""], ["Gwozdz", "Maja", ""], ["Shah", "Viral B.", ""], ["Edelman", "Alan", ""], ["Rackauckas", "Christopher", ""]]}, {"id": "2105.04297", "submitter": "Shuxin Zheng", "authors": "Dinglan Peng, Shuxin Zheng, Yatao Li, Guolin Ke, Di He, Tie-Yan Liu", "title": "How could Neural Networks understand Programs?", "comments": null, "journal-ref": "ICML 2021", "doi": null, "report-no": null, "categories": "cs.PL cs.LG cs.SE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Semantic understanding of programs is a fundamental problem for programming\nlanguage processing (PLP). Recent works that learn representations of code\nbased on pre-training techniques in NLP have pushed the frontiers in this\ndirection. However, the semantics of PL and NL have essential differences.\nThese being ignored, we believe it is difficult to build a model to better\nunderstand programs, by either directly applying off-the-shelf NLP pre-training\ntechniques to the source code, or adding features to the model by the\nheuristic. In fact, the semantics of a program can be rigorously defined by\nformal semantics in PL theory. For example, the operational semantics,\ndescribes the meaning of a valid program as updating the environment (i.e., the\nmemory address-value function) through fundamental operations, such as memory\nI/O and conditional branching. Inspired by this, we propose a novel program\nsemantics learning paradigm, that the model should learn from information\ncomposed of (1) the representations which align well with the fundamental\noperations in operational semantics, and (2) the information of environment\ntransition, which is indispensable for program understanding. To validate our\nproposal, we present a hierarchical Transformer-based pre-training model called\nOSCAR to better facilitate the understanding of programs. OSCAR learns from\nintermediate representation (IR) and an encoded representation derived from\nstatic analysis, which are used for representing the fundamental operations and\napproximating the environment transitions respectively. OSCAR empirically shows\nthe outstanding capability of program semantics understanding on many practical\nsoftware engineering tasks.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 12:21:42 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 08:44:44 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Peng", "Dinglan", ""], ["Zheng", "Shuxin", ""], ["Li", "Yatao", ""], ["Ke", "Guolin", ""], ["He", "Di", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2105.04385", "submitter": "Alexandra Bugariu", "authors": "Alexandra Bugariu, Arshavir Ter-Gabrielyan, and Peter M\\\"uller", "title": "Identifying Overly Restrictive Matching Patterns in SMT-based Program\n  Verifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal quantifiers occur frequently in proof obligations produced by\nprogram verifiers, for instance, to axiomatize uninterpreted functions and to\nexpress properties of arrays. SMT-based verifiers typically reason about them\nvia E-matching, an SMT algorithm that requires syntactic matching patterns to\nguide the quantifier instantiations. Devising good matching patterns is\nchallenging. In particular, overly restrictive patterns may lead to spurious\nverification errors if the quantifiers needed for a proof are not instantiated;\nthey may also conceal unsoundness caused by inconsistent axiomatizations. In\nthis paper, we present the first technique that identifies and helps the users\nremedy the effects of overly restrictive matching patterns. We designed a novel\nalgorithm to synthesize missing triggering terms required to complete a proof.\nTool developers can use this information to refine their matching patterns and\nprevent similar verification errors, or to fix a detected unsoundness.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 14:03:39 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Bugariu", "Alexandra", ""], ["Ter-Gabrielyan", "Arshavir", ""], ["M\u00fcller", "Peter", ""]]}, {"id": "2105.04397", "submitter": "James Davis", "authors": "James C. Davis, Louis G. Michael IV, Christy A. Coghlan, Francisco\n  Servant, and Dongyoon Lee", "title": "Why Aren't Regular Expressions a Lingua Franca? An Empirical Study on\n  the Re-use and Portability of Regular Expressions", "comments": "ESEC/FSE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper explores the extent to which regular expressions (regexes) are\nportable across programming languages. Many languages offer similar regex\nsyntaxes, and it would be natural to assume that regexes can be ported across\nlanguage boundaries. But can regexes be copy/pasted across language boundaries\nwhile retaining their semantic and performance characteristics?\n  In our survey of 158 professional software developers, most indicated that\nthey re-use regexes across language boundaries and about half reported that\nthey believe regexes are a universal language. We experimentally evaluated the\nriskiness of this practice using a novel regex corpus -- 537,806 regexes from\n193,524 projects written in JavaScript, Java, PHP, Python, Ruby, Go, Perl, and\nRust. Using our polyglot regex corpus, we explored the hitherto-unstudied regex\nportability problems: logic errors due to semantic differences, and security\nvulnerabilities due to performance differences.\n  We report that developers' belief in a regex lingua franca is understandable\nbut unfounded. Though most regexes compile across language boundaries, 15%\nexhibit semantic differences across languages and 10% exhibit performance\ndifferences across languages. We explained these differences using regex\ndocumentation, and further illuminate our findings by investigating regex\nengine implementations. Along the way we found bugs in the regex engines of\nJavaScript-V8, Python, Ruby, and Rust, and potential semantic and performance\nregex bugs in thousands of modules.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 14:22:09 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Davis", "James C.", ""], ["Michael", "Louis G.", "IV"], ["Coghlan", "Christy A.", ""], ["Servant", "Francisco", ""], ["Lee", "Dongyoon", ""]]}, {"id": "2105.04555", "submitter": "Jaehoon Koo", "authors": "Jaehoon Koo, Prasanna Balaprakash, Michael Kruse, Xingfu Wu, Paul\n  Hovland, Mary Hall", "title": "Customized Monte Carlo Tree Search for LLVM/Polly's Composable Loop\n  Optimization Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.DC cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Polly is the LLVM project's polyhedral loop nest optimizer. Recently,\nuser-directed loop transformation pragmas were proposed based on LLVM/Clang and\nPolly. The search space exposed by the transformation pragmas is a tree,\nwherein each node represents a specific combination of loop transformations\nthat can be applied to the code resulting from the parent node's loop\ntransformations. We have developed a search algorithm based on Monte Carlo tree\nsearch (MCTS) to find the best combination of loop transformations. Our\nalgorithm consists of two phases: exploring loop transformations at different\ndepths of the tree to identify promising regions in the tree search space and\nexploiting those regions by performing a local search. Moreover, a restart\nmechanism is used to avoid the MCTS getting trapped in a local solution. The\nbest and worst solutions are transferred from the previous phases of the\nrestarts to leverage the search history. We compare our approach with random,\ngreedy, and breadth-first search methods on PolyBench kernels and ECP proxy\napplications. Experimental results show that our MCTS algorithm finds pragma\ncombinations with a speedup of 2.3x over Polly's heuristic optimizations on\naverage.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 21:57:39 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Koo", "Jaehoon", ""], ["Balaprakash", "Prasanna", ""], ["Kruse", "Michael", ""], ["Wu", "Xingfu", ""], ["Hovland", "Paul", ""], ["Hall", "Mary", ""]]}, {"id": "2105.04910", "submitter": "Luca Roversi", "authors": "Armando B. Matos, Luca Paolini, Luca Roversi", "title": "Splitting recursion schemes into reversible and classical interacting\n  threads", "comments": "The first 10 pages will appear in the proceedings of 13th Conference\n  on Reversible Computation. Appendix is for this authors version. arXiv admin\n  note: substantial text overlap with arXiv:2102.09436", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Given a simple recursive function, we show how to extract from it a\nreversible and an classical iterative part. Those parts can synchronously\ncooperate under a Producer/Consumer pattern in order to implement the original\nrecursive function. The reversible producer is meant to run on reversible\nhardware. We also discuss how to extend the extraction to a more general\ncompilation scheme.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:55:30 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Matos", "Armando B.", ""], ["Paolini", "Luca", ""], ["Roversi", "Luca", ""]]}, {"id": "2105.04963", "submitter": "Laila El-Hamamsy", "authors": "Laila El-Hamamsy, Vaios Papaspyros, Taavet Kangur, Laura Mathex,\n  Christian Giang, Melissa Skweres, Barbara Bruno, Francesco Mondada", "title": "Exploring a Handwriting Programming Language for Educational Robots", "comments": "To appear in the proceedings of the 12th International Conference on\n  Robotics in Education (RiE, 2021)", "journal-ref": null, "doi": "10.1007/978-3-030-82544-7_25", "report-no": null, "categories": "cs.PL cs.CY cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, introducing computer science and educational robots in compulsory\neducation has received increasing attention. However, the use of screens in\nclassrooms is often met with resistance, especially in primary school. To\naddress this issue, this study presents the development of a handwriting-based\nprogramming language for educational robots. Aiming to align better with\nexisting classroom practices, it allows students to program a robot by drawing\nsymbols with ordinary pens and paper. Regular smartphones are leveraged to\nprocess the hand-drawn instructions using computer vision and machine learning\nalgorithms, and send the commands to the robot for execution. To align with the\nlocal computer science curriculum, an appropriate playground and scaffolded\nlearning tasks were designed. The system was evaluated in a preliminary test\nwith eight teachers, developers and educational researchers. While the\nparticipants pointed out that some technical aspects could be improved, they\nalso acknowledged the potential of the approach to make computer science\neducation in primary school more accessible.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 12:00:34 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 15:50:01 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["El-Hamamsy", "Laila", ""], ["Papaspyros", "Vaios", ""], ["Kangur", "Taavet", ""], ["Mathex", "Laura", ""], ["Giang", "Christian", ""], ["Skweres", "Melissa", ""], ["Bruno", "Barbara", ""], ["Mondada", "Francesco", ""]]}, {"id": "2105.05159", "submitter": "Yuandong Cyrus Liu", "authors": "Yuandong Cyrus Liu (1), Chengbin Pang (1), Daniel Dietsch (2) and Eric\n  Koskinen (1), Ton-Chanh Le (1), Georgios Portokalidis (1), Jun Xu (1) ((1)\n  Stevens Institute of Technology, (2) University of Freiburg)", "title": "Source-Level Bitwise Branching for Temporal Verification of Lifted\n  Binaries", "comments": "38 pages(including Appendix), 10 tables, 2 Postscript figures,\n  submitted to ATVA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.FL cs.SE cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is increasing interest in applying verification tools to programs that\nhave bitvector operations (eg., binaries). SMT solvers, which serve as a\nfoundation for these tools, have thus increased support for bitvector reasoning\nthrough bit-blasting and linear arithmetic approximations. In this paper we\nshow that similar linear arithmetic approximation of bitvector operations can\nbe done at the source level through transformations. Specifically, we introduce\nnew paths that over-approximate bitvector operations with linear\nconditions/constraints, increasing branching but allowing us to better exploit\nthe well-developed integer reasoning and interpolation of verification tools.\nWe show that, for reachability of bitvector programs, increased branching\nincurs negligible overhead yet, when combined with integer interpolation\noptimizations, enables more programs to be verified. We further show this\nexploitation of integer interpolation in the common case also enables\ncompetitive termination verification of bitvector programs and leads to the\nfirst effective technique for LTL verification of bitvector programs. Finally,\nwe provide an in-depth case study of decompiled (\"lifted\") binary programs,\nwhich emulate X86 execution through frequent use of bitvector operations. We\npresent a new tool DarkSea, the first tool capable of verifying reachability,\ntermination, and LTL of such lifted binaries.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 16:12:02 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Liu", "Yuandong Cyrus", ""], ["Pang", "Chengbin", ""], ["Dietsch", "Daniel", ""], ["Koskinen", "Eric", ""], ["Le", "Ton-Chanh", ""], ["Portokalidis", "Georgios", ""], ["Xu", "Jun", ""]]}, {"id": "2105.05398", "submitter": "Santosh Nagarakatte", "authors": "Harishankar Vishwanathan, Matan Shachnai, Srinivas Narayana, and\n  Santosh Nagarakatte", "title": "Semantics, Verification, and Efficient Implementations for Tristate\n  Numbers", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": "Rutgers Department of Computer Science Technical Report DCS-TR-755", "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extended Berkeley Packet Filter(BPF)is an in-kernel, register-based virtual\nmachine in the Linux operating system that allows non-superusers to execute\ncode at specific points within the Linux kernel. To ensure that such user code\nis safe within the kernel, BPF relies on an in-kernel static analyzer that\nproves properties such as bounded memory access and the absence of illegal\noperations. This static analyzer uses an abstract domain, which it calls tnums\n(tristate numbers), to over-approximate the set of values that a variable may\nstore. This abstract domain is implemented efficiently with bitwise and\narithmetic operations. This paper formalizes the semantics and various\nproperties of tnums and provides the first proofs of soundness and precision of\narithmetic and logical operations with tnums. We describe a novel sound\nalgorithm for multiplying two tnums that is more precise and efficient (runs\n55% faster on average) than the Linux kernel's tnum multiplication.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 01:58:27 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Vishwanathan", "Harishankar", ""], ["Shachnai", "Matan", ""], ["Narayana", "Srinivas", ""], ["Nagarakatte", "Santosh", ""]]}, {"id": "2105.05720", "submitter": "Abhinav Jangda", "authors": "Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet,\n  Saeed Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, Olli Sarikivi", "title": "CoCoNet: Co-Optimizing Computation and Communication for Distributed\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep learning workloads run on distributed hardware and are difficult\nto optimize -- data, model, and pipeline parallelism require a developer to\nthoughtfully restructure their workload around optimized computation and\ncommunication kernels in libraries such as cuBLAS and NCCL. The logical\nseparation between computation and communication leaves performance on the\ntable with missed optimization opportunities across abstraction boundaries. To\nexplore these opportunities, this paper presents CoCoNet, which consists of a\ncompute language to express programs with both computation and communication, a\nscheduling language to apply transformations on such programs, and a compiler\nto generate high performance kernels. Providing both computation and\ncommunication as first class constructs enables new optimizations, such as\noverlapping or fusion of communication with computation. CoCoNet allowed us to\noptimize several data, model and pipeline parallel workloads in existing deep\nlearning systems with very few lines of code. We show significant improvements\nafter integrating novel CoCoNet generated kernels.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:13:43 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 01:04:11 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Jangda", "Abhinav", ""], ["Huang", "Jun", ""], ["Liu", "Guodong", ""], ["Sabet", "Amir Hossein Nodehi", ""], ["Maleki", "Saeed", ""], ["Miao", "Youshan", ""], ["Musuvathi", "Madanlal", ""], ["Mytkowicz", "Todd", ""], ["Sarikivi", "Olli", ""]]}, {"id": "2105.05801", "submitter": "Sunjay Cauligi", "authors": "Sunjay Cauligi, Craig Disselkoen, Daniel Moghimi, Gilles Barthe, Deian\n  Stefan", "title": "SoK: Practical Foundations for Spectre Defenses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spectre vulnerabilities violate our fundamental assumptions about\narchitectural abstractions, allowing attackers to steal sensitive data despite\npreviously state-of-the-art countermeasures. To defend against Spectre,\ndevelopers of verification tools and compiler-based mitigations are forced to\nreason about microarchitectural details such as speculative execution. In order\nto aid developers with these attacks in a principled way, the research\ncommunity has sought formal foundations for speculative execution upon which to\nrebuild provable security guarantees.\n  This paper systematizes the community's current knowledge about software\nverification and mitigation for Spectre. We study state-of-the-art software\ndefenses, both with and without associated formal models, and use a cohesive\nframework to compare the security properties each defense provides. We explore\na wide variety of tradeoffs in the complexity of formal frameworks, the\nperformance of defense tools, and the resulting security guarantees. As a\nresult of our analysis, we suggest practical choices for developers of analysis\nand mitigation tools, and we identify several open problems in this area to\nguide future work on grounded software defenses.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:09:43 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Cauligi", "Sunjay", ""], ["Disselkoen", "Craig", ""], ["Moghimi", "Daniel", ""], ["Barthe", "Gilles", ""], ["Stefan", "Deian", ""]]}, {"id": "2105.06024", "submitter": "Siva Somayyajula", "authors": "Siva Somayyajula, Frank Pfenning", "title": "Circular Proofs as Processes: Type-Based Termination via Arithmetic\n  Refinements", "comments": "14 pages. Submitted to CONCUR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Type systems for concurrent programs guarantee such desirable properties as\ncommunication safety and type refinements facilitate the verification of\nprogram invariants. Yet, type-based termination of recursive concurrent\nprograms has been largely unexplored. On the other hand, sized types enable\ntermination checking of functional programs with complex patterns of recursion\nin the presence of mixed inductive and coinductive types. In this paper, we\nadapt sized types to the concurrent setting. In particular, we extend a core\nlanguage for persistent shared memory concurrency based on the semi-axiomatic\nsequent calculus with recursive types and arithmetic refinements to express\nsize indexing. To prove termination of program reduction, we first define a\nnovel semantic model that reflects persistence in the type system and admits a\nstraightforward generalization to substructural typing. We then develop a\ncompositional validity condition for recursive concurrent programs viewed as\ncircular proofs that guarantees termination.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 01:21:30 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Somayyajula", "Siva", ""], ["Pfenning", "Frank", ""]]}, {"id": "2105.06081", "submitter": "Sam Estep", "authors": "Sam Estep (Carnegie Mellon University), Jenna Wise (Carnegie Mellon\n  University), Jonathan Aldrich (Carnegie Mellon University), \\'Eric Tanter\n  (University of Chile), Johannes Bader (Jane Street), Joshua Sunshine\n  (Carnegie Mellon University)", "title": "Gradual Program Analysis for Null Pointers", "comments": "31 pages, 12 figures, published in ECOOP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Static analysis tools typically address the problem of excessive false\npositives by requiring programmers to explicitly annotate their code. However,\nwhen faced with incomplete annotations, many analysis tools are either too\nconservative, yielding false positives, or too optimistic, resulting in unsound\nanalysis results. In order to flexibly and soundly deal with\npartially-annotated programs, we propose to build upon and adapt the gradual\ntyping approach to abstract-interpretation-based program analyses.\nSpecifically, we focus on null-pointer analysis and demonstrate that a gradual\nnull-pointer analysis hits a sweet spot, by gracefully applying static analysis\nwhere possible and relying on dynamic checks where necessary for soundness. In\naddition to formalizing a gradual null-pointer analysis for a core imperative\nlanguage, we build a prototype using the Infer static analysis framework, and\npresent preliminary evidence that the gradual null-pointer analysis reduces\nfalse positives compared to two existing null-pointer checkers for Infer.\nFurther, we discuss ways in which the gradualization approach used to derive\nthe gradual analysis from its static counterpart can be extended to support\nmore domains. This work thus provides a basis for future analysis tools that\ncan smoothly navigate the tradeoff between human effort and run-time overhead\nto reduce the number of reported false positives.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 05:08:10 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 01:39:23 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Estep", "Sam", "", "Carnegie Mellon University"], ["Wise", "Jenna", "", "Carnegie Mellon\n  University"], ["Aldrich", "Jonathan", "", "Carnegie Mellon University"], ["Tanter", "\u00c9ric", "", "University of Chile"], ["Bader", "Johannes", "", "Jane Street"], ["Sunshine", "Joshua", "", "Carnegie Mellon University"]]}, {"id": "2105.06291", "submitter": "Christian Bartolo Burl\u00f2", "authors": "Christian Batrolo Burl\\`o, Adrian Francalanza, Alceste Scalas", "title": "On the Monitorability of Session Types, in Theory and Practice (Extended\n  Version)", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.ECOOP.2021.22", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In concurrent and distributed systems, software components are expected to\ncommunicate according to predetermined protocols and APIs - and if a component\ndoes not observe them, the system's reliability is compromised. Furthermore,\nisolating and fixing protocol/API errors can be very difficult. Many methods\nhave been proposed to check the correctness of communicating systems, ranging\nfrom compile-time to run-time verification; among such methods, session types\nhave been applied for both static type-checking, and run-time monitoring. This\nwork takes a fresh look at the run-time verification of communicating systems\nusing session types, in theory and in practice. On the theoretical side, we\ndevelop a novel formal model of session-monitored processes; with it, we\nformulate and prove new results on the monitorability of session types,\nconnecting their run-time and static verification - in terms of soundness\n(i.e., whether monitors only flag ill-typed processes) and completeness (i.e.,\nwhether all ill-typed processes can be flagged by a monitor). On the practical\nside, we show that our monitoring theory is indeed realisable: building upon\nour formal model, we develop a Scala toolkit for the automatic generation of\nsession monitors. Our executable monitors can be used to instrument black-box\nprocesses written in any programming language; we assess the viability of our\napproach with a series of benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 13:36:42 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 09:10:16 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Burl\u00f2", "Christian Batrolo", ""], ["Francalanza", "Adrian", ""], ["Scalas", "Alceste", ""]]}, {"id": "2105.06424", "submitter": "Viktor Toman", "authors": "Pratyush Agarwal, Krishnendu Chatterjee, Shreya Pathak, Andreas\n  Pavlogiannis, Viktor Toman", "title": "Stateless Model Checking under a Reads-Value-From Equivalence", "comments": "Full technical report of the CAV2021 work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stateless model checking (SMC) is one of the standard approaches to the\nverification of concurrent programs. As scheduling non-determinism creates\nexponentially large spaces of thread interleavings, SMC attempts to partition\nthis space into equivalence classes and explore only a few representatives from\neach class. The efficiency of this approach depends on two factors: (a) the\ncoarseness of the partitioning, and (b) the time to generate representatives in\neach class. For this reason, the search for coarse partitionings that are\nefficiently explorable is an active research challenge.\n  In this work we present RVF-SMC, a new SMC algorithm that uses a novel\n\\emph{reads-value-from (RVF)} partitioning. Intuitively, two interleavings are\ndeemed equivalent if they agree on the value obtained in each read event, and\nread events induce consistent causal orderings between them. The RVF\npartitioning is provably coarser than recent approaches based on Mazurkiewicz\nand \"reads-from\" partitionings. Our experimental evaluation reveals that RVF is\nquite often a very effective equivalence, as the underlying partitioning is\nexponentially coarser than other approaches. Moreover, RVF-SMC generates\nrepresentatives very efficiently, as the reduction in the partitioning is often\nmet with significant speed-ups in the model checking task.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:00:07 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Agarwal", "Pratyush", ""], ["Chatterjee", "Krishnendu", ""], ["Pathak", "Shreya", ""], ["Pavlogiannis", "Andreas", ""], ["Toman", "Viktor", ""]]}, {"id": "2105.06973", "submitter": "Simon Fowler", "authors": "Paul Harvey and Simon Fowler and Ornela Dardha and Simon J. Gay", "title": "Multiparty Session Types for Safe Runtime Adaptation in an Actor\n  Language (Extended version)", "comments": "Extended version of paper to appear at ECOOP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human fallibility, unpredictable operating environments, and the\nheterogeneity of hardware devices are driving the need for software to be able\nto adapt as seen in the Internet of Things or telecommunication networks.\nUnfortunately, mainstream programming languages do not readily allow a software\ncomponent to sense and respond to its operating environment, by discovering,\nreplacing, and communicating with components that are not part of the original\nsystem design, while maintaining static correctness guarantees. In particular,\nif a new component is discovered at runtime, there is no guarantee that its\ncommunication behaviour is compatible with existing components.\n  We address this problem by using multiparty session types with explicit\nconnection actions, a type formalism used to model distributed communication\nprotocols. By associating session types with software components, the discovery\nprocess can check protocol compatibility and, when required, correctly replace\ncomponents without jeapordising safety.\n  We present the design and implementation of EnsembleS, the first actor-based\nlanguage with adaptive features and a static session type system, and apply it\nto a case study based on an adaptive DNS server. We formalise the type system\nof EnsembleS and prove the safety of well-typed programs, making essential use\nof recent advances in non-classical multiparty session types.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 17:22:21 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Harvey", "Paul", ""], ["Fowler", "Simon", ""], ["Dardha", "Ornela", ""], ["Gay", "Simon J.", ""]]}, {"id": "2105.07277", "submitter": "Thomas Wahl", "authors": "Andrew Johnson and Thomas Wahl", "title": "Delay-Bounded Scheduling Without Delay! (Extended Technical Report)", "comments": "This is an extended technical report of a paper published in CAV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the broad problem of analyzing safety properties of asynchronous\nconcurrent programs under arbitrary thread interleavings. Delay-bounded\ndeterministic scheduling, introduced in prior work, is an efficient bug-finding\ntechnique to curb the large cost associated with full scheduling\nnondeterminism. In this paper we first present a technique to lift the delay\nbound for the case of finite-domain variable programs, thus adding to the\nefficiency of bug detection the ability to prove safety of programs under\narbitrary thread interleavings. Second, we demonstrate how, combined with\npredicate abstraction, our technique can both refute and verify safety\nproperties of programs with unbounded variable domains, even for unbounded\nthread counts. Previous work has established that, for non-trivial concurrency\nroutines, predicate abstraction induces a highly complex abstract program\nsemantics. Our technique, however, never statically constructs an abstract\nparametric program; it only requires some abstract-states set to be closed\nunder certain actions, thus eliminating the dependence on the existence of\nverification algorithms for abstract programs. We demonstrate the efficiency of\nour technique on many examples used in prior work, and showcase its simplicity\ncompared to earlier approaches on the unbounded-thread Ticket Lock protocol.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 19:04:06 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Johnson", "Andrew", ""], ["Wahl", "Thomas", ""]]}, {"id": "2105.08645", "submitter": "Long Phan", "authors": "Long Phan, Hieu Tran, Daniel Le, Hieu Nguyen, James Anibal, Alec\n  Peltekian, and Yanfang Ye", "title": "CoTexT: Multi-task Learning with Code-Text Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present CoTexT, a pre-trained, transformer-based encoder-decoder model\nthat learns the representative context between natural language (NL) and\nprogramming language (PL). Using self-supervision, CoTexT is pre-trained on\nlarge programming language corpora to learn a general understanding of language\nand code. CoTexT supports downstream NL-PL tasks such as code\nsummarizing/documentation, code generation, defect detection, and code\ndebugging. We train CoTexT on different combinations of available PL corpus\nincluding both \"bimodal\" and \"unimodal\" data. Here, bimodal data is the\ncombination of text and corresponding code snippets, whereas unimodal data is\nmerely code snippets. We first evaluate CoTexT with multi-task learning: we\nperform Code Summarization on 6 different programming languages and Code\nRefinement on both small and medium size featured in the CodeXGLUE dataset. We\nfurther conduct extensive experiments to investigate CoTexT on other tasks\nwithin the CodeXGlue dataset, including Code Generation and Defect Detection.\nWe consistently achieve SOTA results in these tasks, demonstrating the\nversatility of our models.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:22:05 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 05:42:26 GMT"}, {"version": "v3", "created": "Sat, 12 Jun 2021 08:41:01 GMT"}, {"version": "v4", "created": "Mon, 21 Jun 2021 11:34:45 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Phan", "Long", ""], ["Tran", "Hieu", ""], ["Le", "Daniel", ""], ["Nguyen", "Hieu", ""], ["Anibal", "James", ""], ["Peltekian", "Alec", ""], ["Ye", "Yanfang", ""]]}, {"id": "2105.08996", "submitter": "Simon Fowler", "authors": "Simon Fowler, Wen Kokke, Ornela Dardha, Sam Lindley, and J. Garrett\n  Morris", "title": "Separating Sessions Smoothly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces Hypersequent GV (HGV), a modular and extensible core\ncalculus for functional programming with session types that enjoys deadlock\nfreedom, confluence, and strong normalisation. HGV exploits hyper-environments,\nwhich are collections of type environments, to ensure that structural\ncongruence is type preserving. As a consequence we obtain a tight operational\ncorrespondence between HGV and HCP, a hypersequent-based process-calculus\ninterpretation of classical linear logic. Our translations from HGV to HCP and\nvice-versa both preserve and reflect reduction. HGV scales smoothly to support\nGirard's Mix rule, a crucial ingredient for channel forwarding and exceptions.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 09:02:51 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Fowler", "Simon", ""], ["Kokke", "Wen", ""], ["Dardha", "Ornela", ""], ["Lindley", "Sam", ""], ["Morris", "J. Garrett", ""]]}, {"id": "2105.09257", "submitter": "Paul Wilson", "authors": "Paul Wilson, Fabio Zanasi", "title": "The Cost of Compositionality: A High-Performance Implementation of\n  String Diagram Composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  String diagrams are an increasingly popular algebraic language for the\nanalysis of graphical models of computations across different research fields.\nWhereas string diagrams have been thoroughly studied as semantic structures,\nmuch fewer attention has been given to their algorithmic properties, and\nefficient implementations of diagrammatic reasoning are almost an unexplored\nsubject. This work intends to be a contribution in such direction. We introduce\na data structure representing string diagrams in terms of adjacency matrices.\nThis encoding has the key advantage of providing simple and efficient\nalgorithms for composition and tensor product of diagrams. We demonstrate its\neffectiveness by showing that the complexity of the two operations is linear in\nthe size of string diagrams. Also, as our approach is based on basic linear\nalgebraic operations, we can take advantage of heavily optimised\nimplementations, which we use to measure performances of string diagrammatic\noperations via several benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 17:01:06 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Wilson", "Paul", ""], ["Zanasi", "Fabio", ""]]}, {"id": "2105.09377", "submitter": "Gus Henry Smith", "authors": "Gus Henry Smith, Andrew Liu, Steven Lyubomirsky, Scott Davidson,\n  Joseph McMahan, Michael Taylor, Luis Ceze, Zachary Tatlock", "title": "Pure Tensor Program Rewriting via Access Patterns (Representation Pearl)", "comments": "To be published at MAPS 2021", "journal-ref": null, "doi": "10.1145/3460945.3464953", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Tensor kernels in machine learning (ML) often correspond to pure mathematical\nexpressions, making term rewriting an attractive strategy for optimization and\nmapping to specialized hardware accelerators. However, existing ML intermediate\nrepresentations (IRs) tend to either be \\textit{pure but high-level}, making\nlow-level rewrites to hardware targets inexpressible, or \\textit{low-level but\nimpure}, hampering the use of term rewriting altogether. This paper introduces\nGlenside, a pure IR whose core abstraction -- the \\textit{access pattern} --\nenables low-level, layout-aware, hardware-centric program rewrites.\n  We demonstrate how term rewriting in Glenside can be used to map program\nfragments to hardware accelerator invocations and automatically discover\nclassic data layout transformations like \\texttt{im2col}. Glenside establishes\na new foundation for exploring further term rewriting techniques in optimizing\nlow-level tensor programs.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 19:56:44 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Smith", "Gus Henry", ""], ["Liu", "Andrew", ""], ["Lyubomirsky", "Steven", ""], ["Davidson", "Scott", ""], ["McMahan", "Joseph", ""], ["Taylor", "Michael", ""], ["Ceze", "Luis", ""], ["Tatlock", "Zachary", ""]]}, {"id": "2105.09469", "submitter": "Roy Frostig", "authors": "Roy Frostig, Matthew J. Johnson, Dougal Maclaurin, Adam Paszke, Alexey\n  Radul", "title": "Decomposing reverse-mode automatic differentiation", "comments": "Presented at the LAFI 2021 workshop at POPL, 17 January 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We decompose reverse-mode automatic differentiation into (forward-mode)\nlinearization followed by transposition. Doing so isolates the essential\ndifference between forward- and reverse-mode AD, and simplifies their joint\nimplementation. In particular, once forward-mode AD rules are defined for every\nprimitive operation in a source language, only linear primitives require an\nadditional transposition rule in order to arrive at a complete reverse-mode AD\nimplementation. This is how reverse-mode AD is written in JAX and Dex.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 02:33:56 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Frostig", "Roy", ""], ["Johnson", "Matthew J.", ""], ["Maclaurin", "Dougal", ""], ["Paszke", "Adam", ""], ["Radul", "Alexey", ""]]}, {"id": "2105.09929", "submitter": "Mathys Rennela", "authors": "Robin Kaarsgaard and Mathys Rennela", "title": "Join inverse rig categories for reversible functional programming, and\n  beyond", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL math.CT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reversible computing is a computational paradigm in which computations are\ndeterministic in both the forward and backward direction, so that programs have\nwell-defined forward and backward semantics. We investigate the formal\nsemantics of the reversible functional programming language Rfun. For this\npurpose, we introduce join inverse rig categories, the natural marriage of join\ninverse categories and rig categories, which we show can be used to model the\nlanguage Rfun, under reasonable assumptions. These categories turn out to be a\nparticularly natural fit for reversible computing as a whole, as they encompass\nmodels for other reversible programming languages, notably Theseus and\nreversible flowcharts. This suggests that join inverse rig categories really\nare the categorical models of reversible computing.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 17:44:23 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Kaarsgaard", "Robin", ""], ["Rennela", "Mathys", ""]]}, {"id": "2105.10687", "submitter": "Rathnakar Madhukar Yerraguntla", "authors": "Sanjiva Prasad, R. Madhukar Yerraguntla", "title": "Normalising Lustre Preserves Security", "comments": "Submitted version. 27 Pages including appendix and references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The synchronous reactive data flow language LUSTRE is an expressive language,\nequipped with a suite of tools for modelling, simulating and model-checking a\nwide variety of safety-critical systems. A critical intermediate step in the\nformally certified compilation of LUSTRE involves translation to a well-behaved\nsub-language called \"Normalised LUSTRE\" (NLUSTRE). Recently, we proposed a\nsimple Denning-style lattice-based secure information flow type system for\nNLUSTRE, and proved its soundness by establishing that security-typed programs\nare non-interfering with respect to the co-inductive stream semantics.\n  In this paper, we propose a similar security type system for unrestricted\nLUSTRE, and show that Bourke et al.'s semantics-preserving normalisation\ntransformations from LUSTRE to NLUSTRE are security-preserving as well. A\nnovelty is the use of refinement security types for node calls. The main result\nis the preservation of security types by the normalisation transformations. The\nsoundness of our security typing rules is shown by establishing that\nwell-security-typed programs are non-interfering, via a reduction to\ntype-preservation (here), semantics-preservation (Bourke et al.) and our\nprevious result of non-interference for NLUSTRE.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 10:57:01 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Prasad", "Sanjiva", ""], ["Yerraguntla", "R. Madhukar", ""]]}, {"id": "2105.10726", "submitter": "B\\'erenger Bramas", "authors": "Garip Kusoglu, Berenger Bramas, Stephane Genaud", "title": "Automatic task-based parallelization of C++ applications by\n  source-to-source transformations", "comments": "Published at Compas 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Currently, multi/many-core CPUs are considered standard in most types of\ncomputers including, mobile phones, PCs or supercomputers. However, the\nparallelization of applications as well as refactoring/design of applications\nfor efficient hardware usage remains restricted to experts who have advanced\ntechnical knowledge and who can invest time tuning their software. In this\ncontext, the compilation community has proposed different methods for automatic\nparallelization, but their focus is traditionally on loops and nested loops\nwith the support of polyhedral techniques. In this study, we propose a new\napproach to transform sequential C++ source code into a task-based parallel one\nby inserting annotations. We explain the different mechanisms we used to create\ntasks at each function/method call, and how we can limit the number of tasks.\nOur method can be implemented on top of the OpenMP 4.0 standard. It is\ncompiler-independent and can rely on external well-optimized OpenMP libraries.\nFinally, we provide preliminary performance results that illustrate the\npotential of our method.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 13:27:09 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Kusoglu", "Garip", ""], ["Bramas", "Berenger", ""], ["Genaud", "Stephane", ""]]}, {"id": "2105.10819", "submitter": "Artjoms Sinkarovs PhD", "authors": "Artjoms \\v{S}inkarovs and Jesper Cockx", "title": "Choosing is Losing: How to combine the benefits of shallow and deep\n  embeddings through reflection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Dependently-typed host languages empower users to verify a wide range of\nproperties of embedded languages and programs written in them. Designers of\nsuch embedded languages are faced with a difficult choice between using a\nshallow or a deep embedding. The former is easier to use because the entire\ninfrastructure of the host langauge is immediately available. Meanwhile, the\nlatter gives full access to the structure of embedded programs, but is\ndifficult to use in practice, especially when the embedded language is itself\ndependently typed.\n  The main insight presented in this paper is that the choice between shallow\nand deep embedding can be eliminated by working in a host language with\nreflection capabilities: we start from a shallow embedding that can use all\nlibraries and tools of the host language, and later use reflection to expose\nthe deep structure of the embedded programs.\n  Concretely, we apply this technique to embed three programming languages --\nKaleidoscope, SaC, and (a subset of) APL -- into the dependently typed theorem\nprover Agda, using dependent types to statically enforce several properties of\ninterest. We then use Agda's reflection capabilities to extract the embedded\nprograms back into the original language, so that the existing toolchain can be\nleveraged.\n  In this process, statically verified properties of the host language are\nmapped onto runtime checks in the target language, allowing extracted programs\nto interact safely with existing code.\n  Finally, we demonstrate the feasibility of our approach with the\nimplementation and extraction of a convolutional neural network in our\nembedding of APL.\\@\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 22:11:18 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["\u0160inkarovs", "Artjoms", ""], ["Cockx", "Jesper", ""]]}, {"id": "2105.11148", "submitter": "Felicien Ihirwe", "authors": "Felicien Ihirwe", "title": "CHESSIoT support of event-based modeling for the Internet of Things\n  applications", "comments": "12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Internet of Things systems design and development suffers from heterogeneity\nin different aspects. The component behaviors also change due to events being\ninternal or external and the system needs to take action subsequently. In this\npaper, we demo the event-based modeling capabilities of the CHESSIoT tool on a\nsmart parking application. Different components was been decomposed with\nfunctional and behavioral specifications following the component-based\napproach. In the end, we have given a brief description of the future work.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 08:28:09 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Ihirwe", "Felicien", ""]]}, {"id": "2105.11267", "submitter": "Alasdair Hill", "authors": "Alasdair Hill, Ekaterina Komendantskaya, Matthew L. Daggitt and Ronald\n  P. A. Petrick", "title": "Actions You Can Handle: Dependent Types for AI Plans", "comments": "14 pages, 5 figures, Accepted to TyDe 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification of AI is a challenge that has engineering, algorithmic and\nprogramming language components. For example, AI planners are deployed to model\nactions of autonomous agents. They comprise a number of searching algorithms\nthat, given a set of specified properties, find a sequence of actions that\nsatisfy these properties. Although AI planners are mature tools from the\nalgorithmic and engineering points of view, they have limitations as\nprogramming languages. Decidable and efficient automated search entails\nrestrictions on the syntax of the language, prohibiting use of higher-order\nproperties or recursion. This paper proposes a methodology for embedding plans\nproduced by AI planners into dependently-typed language Agda, which enables\nusers to reason about and verify more general and abstract properties of plans,\nand also provides a more holistic programming language infrastructure for\nmodelling plan execution.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 13:33:56 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 20:12:06 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Hill", "Alasdair", ""], ["Komendantskaya", "Ekaterina", ""], ["Daggitt", "Matthew L.", ""], ["Petrick", "Ronald P. A.", ""]]}, {"id": "2105.11620", "submitter": "Yanjun Wang", "authors": "Yanjun Wang, Zixuan Li, Xiaokang Qiu, Sanjay G. Rao", "title": "Comparative Synthesis: Learning Optimal Programs with Indeterminate\n  Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.HC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative program synthesis aims to generate a program that satisfies not\nonly boolean specifications but also quantitative objectives. Nonetheless,\nobtaining precise quantitative objectives per se can be a challenging task. In\nthis paper, we propose comparative synthesis, a bootstrapping quantitative\nsynthesis framework in which an indeterminate objective and a satisfying\nprogram are synthesized in tandem. The key idea is to make comparative queries\nto learn the user's preference over candidate programs, with which objectives\ncan be conjectured. These objectives, which are indeterminate as they can be\nrefined along the course of user interaction, guide the search of satisfying\nprograms.\n  Within the comparative synthesis framework, we developed two novel\ncomparative synthesis procedures (CLPs) with the aim of minimizing the number\nof queries to the user. We prove that both CLPs converge and use them in two\ncase studies: generating bandwidth allocations for network design and solving\nSyGuS benchmarks with syntactic objectives. Experiments show that our framework\ncan successfully synthesize satisfying/optimal solutions by making queries\nonly, without a priori knowledge about the quantitative objective.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 02:36:13 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Wang", "Yanjun", ""], ["Li", "Zixuan", ""], ["Qiu", "Xiaokang", ""], ["Rao", "Sanjay G.", ""]]}, {"id": "2105.11896", "submitter": "Aleksander Boruch-Gruszecki", "authors": "Aleksander Boruch-Gruszecki, Jonathan Immanuel Brachth\\\"auser, Edward\n  Lee, Ond\\v{r}ej Lhot\\'ak, Martin Odersky", "title": "Tracking Captured Variables in Types", "comments": "23 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Type systems usually characterize the shape of values but not their free\nvariables. However, there are many desirable safety properties one could\nguarantee if one could track how references can escape. For example, one may\nimplement algebraic effect handlers using capabilities -- a value which permits\none to perform the effect -- safely if one can guarantee that the capability\nitself does not escape the scope bound by the effect handler. To this end, we\nstudy the $\\textrm{CF}_{<:}$ calculus, a conservative and lightweight extension\nof $\\textrm{System F}_{<:}$, to track how values and their references can be\ncaptured and escape. We show that existing terms in $\\textrm{System F}_{<:}$\nembed naturally in our calculus, and that many natural problems can be\nexpressed in a system that tracks variable references like we do in\n$\\textrm{CF}_{<:}$. We also give mechanized proofs of the soundness properties\nof $\\textrm{CF}_{<:}$ in Coq. The type system presented in $\\textrm{CF}_{<:}$\nis powerful enough to reason about safety in the context of many natural\nextensions of $\\textrm{CF}_{<:}$ such as region-based memory-management,\nnon-local returns, and effect handlers.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 13:08:09 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Boruch-Gruszecki", "Aleksander", ""], ["Brachth\u00e4user", "Jonathan Immanuel", ""], ["Lee", "Edward", ""], ["Lhot\u00e1k", "Ond\u0159ej", ""], ["Odersky", "Martin", ""]]}, {"id": "2105.12077", "submitter": "Elizabeth Dietrich", "authors": "Elizabeth Dietrich", "title": "A beginner guide to Iris, Coq and separation logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Creating safe concurrent algorithms is challenging and error-prone. For this\nreason, a formal verification framework is necessary especially when those\nconcurrent algorithms are used in safety-critical systems. The goal of this\nguide is to provide resources for beginners to get started in their journey of\nformal verification using the powerful tool Iris. The difference between this\nguide and many others is that it provides (i) an in-depth explanation of\nexamples and tactics, (ii) an explicit discussion of separation logic, and\n(iii) a thorough coverage of Iris and Coq. References to other guides and to\npapers are included throughout to provide readers with resources through which\nto continue their learning.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:57:25 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Dietrich", "Elizabeth", ""]]}, {"id": "2105.12485", "submitter": "Chen Lyu", "authors": "Xue Jiang, Zhuoran Zheng, Chen Lyu, Liang Li, Lei Lyu", "title": "TreeBERT: A Tree-Based Pre-Trained Model for Programming Language", "comments": "Accepted by UAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source code can be parsed into the abstract syntax tree (AST) based on\ndefined syntax rules. However, in pre-training, little work has considered the\nincorporation of tree structure into the learning process. In this paper, we\npresent TreeBERT, a tree-based pre-trained model for improving programming\nlanguage-oriented generation tasks. To utilize tree structure, TreeBERT\nrepresents the AST corresponding to the code as a set of composition paths and\nintroduces node position embedding. The model is trained by tree masked\nlanguage modeling (TMLM) and node order prediction (NOP) with a hybrid\nobjective. TMLM uses a novel masking strategy designed according to the tree's\ncharacteristics to help the model understand the AST and infer the missing\nsemantics of the AST. With NOP, TreeBERT extracts the syntactical structure by\nlearning the order constraints of nodes in AST. We pre-trained TreeBERT on\ndatasets covering multiple programming languages. On code summarization and\ncode documentation tasks, TreeBERT outperforms other pre-trained models and\nstate-of-the-art models designed for these tasks. Furthermore, TreeBERT\nperforms well when transferred to the pre-trained unseen programming language.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 11:36:43 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 02:34:01 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Jiang", "Xue", ""], ["Zheng", "Zhuoran", ""], ["Lyu", "Chen", ""], ["Li", "Liang", ""], ["Lyu", "Lei", ""]]}, {"id": "2105.12819", "submitter": "Anthony Savidis", "authors": "Anthony Savidis, Vangelis Tsiatsianas", "title": "Implementation of Live Reverse Debugging in LLDB", "comments": "28 pages, 32 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Debugging is an essential process with a large share of the development\neffort, being a relentless quest for offensive code through tracing, inspection\nand iterative running sessions. Probably every developer has been in a\nsituation with a clear wish to rewind time just for a while, only to retry some\nactions alternatively, instead of restarting the entire session. Well, the\ngenie to fulfill such a wish is known as a reverse debugger. Their inherent\ntechnical complexity makes them very hard to implement, while the imposed\nexecution overhead turns them to less preferable for adoption. There are only a\nfew available, most being off-line tools, working on recorded, previously run,\nsessions. We consider live reverse debuggers both challenging and promising,\nsince they can fit into existing forward debuggers, and we developed the first\nlive reverse debugger on top of LLDB, discussing in detail our implementation\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 20:15:53 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Savidis", "Anthony", ""], ["Tsiatsianas", "Vangelis", ""]]}, {"id": "2105.13114", "submitter": "Walt Woods", "authors": "Walt Woods", "title": "RL-GRIT: Reinforcement Learning for Grammar Inference", "comments": "13 pages, published at IEEE LangSec 2021\n  (https://langsec.org/spw21/papers.html). ArXiv version: lacking correct\n  'minted' package behavior, so some atoms may look a little off", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When working to understand usage of a data format, examples of the data\nformat are often more representative than the format's specification. For\nexample, two different applications might use very different JSON\nrepresentations, or two PDF-writing applications might make use of very\ndifferent areas of the PDF specification to realize the same rendered content.\nThe complexity arising from these distinct origins can lead to large,\ndifficult-to-understand attack surfaces, presenting a security concern when\nconsidering both exfiltration and data schizophrenia. Grammar inference can aid\nin describing the practical language generator behind examples of a data\nformat. However, most grammar inference research focuses on natural language,\nnot data formats, and fails to support crucial features such as type recursion.\nWe propose a novel set of mechanisms for grammar inference, RL-GRIT, and apply\nthem to understanding de facto data formats. After reviewing existing grammar\ninference solutions, it was determined that a new, more flexible scaffold could\nbe found in Reinforcement Learning (RL). Within this work, we lay out the many\nalgorithmic changes required to adapt RL from its traditional, sequential-time\nenvironment to the highly interdependent environment of parsing. The result is\nan algorithm which can demonstrably learn recursive control structures in\nsimple data formats, and can extract meaningful structure from fragments of the\nPDF format. Whereas prior work in grammar inference focused on either regular\nlanguages or constituency parsing, we show that RL can be used to surpass the\nexpressiveness of both classes, and offers a clear path to learning\ncontext-sensitive languages. The proposed algorithm can serve as a building\nblock for understanding the ecosystems of de facto data formats.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 23:48:39 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Woods", "Walt", ""]]}, {"id": "2105.13411", "submitter": "Joost-Pieter Katoen", "authors": "Milan Ceska, Christian Dehnert, Nils Jansen, Sebastian Junges,\n  Joost-Pieter Katoen", "title": "Model Repair Revamped: On the Automated Synthesis of Markov Chains", "comments": "18 pages", "journal-ref": null, "doi": "10.1007/978-3-030-31514-6\\_7", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper outlines two approaches|based on counterexample-guided abstraction\nrefinement (CEGAR) and counterexample-guided inductive synthesis (CEGIS),\nrespectively to the automated synthesis of finite-state probabilistic models\nand programs. Our CEGAR approach iteratively partitions the design space\nstarting from an abstraction of this space and refines this by a light-weight\nanalysis of verification results. The CEGIS technique exploits critical\nsubsystems as counterexamples to prune all programs behaving incorrectly on\nthat input. We show the applicability of these synthesis techniques to\nsketching of probabilistic programs, controller synthesis of POMDPs, and\nsoftware product lines.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 19:30:32 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ceska", "Milan", ""], ["Dehnert", "Christian", ""], ["Jansen", "Nils", ""], ["Junges", "Sebastian", ""], ["Katoen", "Joost-Pieter", ""]]}, {"id": "2105.13468", "submitter": "Abhiroop Sarkar", "authors": "Abhiroop Sarkar, Mary Sheeran", "title": "Hailstorm : A Statically-Typed, Purely Functional Language for IoT\n  Applications", "comments": null, "journal-ref": null, "doi": "10.1145/3414080.3414092", "report-no": null, "categories": "cs.PL cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the growing ubiquity of Internet of Things(IoT), more complex logic is\nbeing programmed on resource-constrained IoT devices, almost exclusively using\nthe C programming language. While C provides low-level control over memory, it\nlacks a number of high-level programming abstractions such as higher-order\nfunctions, polymorphism, strong static typing, memory safety, and automatic\nmemory management.\n  We present Hailstorm, a statically-typed, purely functional programming\nlanguage that attempts to address the above problem. It is a high-level\nprogramming language with a strict typing discipline. It supports features like\nhigher-order functions, tail-recursion, and automatic memory management, to\nprogram IoT devices in a declarative manner. Applications running on these\ndevices tend to be heavily dominated by I/O. Hailstorm tracks side effects\nlikeI/O in its type system using resource types. This choice allowed us to\nexplore the design of a purely functional standalone language, in an area where\nit is more common to embed a functional core in an imperative shell. The\nlanguage borrows the combinators of arrowized FRP, but has discrete-time\nsemantics. The design of the full set of combinators is work in progress,\ndriven by examples. So far, we have evaluated Hailstorm by writing standard\nexamples from the literature (earthquake detection, a railway crossing system\nand various other clocked systems), and also running examples on the GRiSP\nembedded systems board, through generation of Erlang.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 22:09:15 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Sarkar", "Abhiroop", ""], ["Sheeran", "Mary", ""]]}, {"id": "2105.13840", "submitter": "Felix A. Wolf", "authors": "Felix A. Wolf, Linard Arquint, Martin Clochard, Wytse Oortwijn, Jo\\~ao\n  C. Pereira, Peter M\\\"uller", "title": "Gobra: Modular Specification and Verification of Go Programs (extended\n  version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Go is an increasingly-popular systems programming language targeting,\nespecially, concurrent and distributed systems. Go differentiates itself from\nother imperative languages by offering structural subtyping and lightweight\nconcurrency through goroutines with message-passing communication. This\ncombination of features poses interesting challenges for static verification,\nmost prominently the combination of a mutable heap and advanced concurrency\nprimitives.\n  We present Gobra, a modular, deductive program verifier for Go that proves\nmemory safety, crash safety, data-race freedom, and user-provided\nspecifications. Gobra is based on separation logic and supports a large subset\nof Go. Its implementation translates an annotated Go program into the Viper\nintermediate verification language and uses an existing SMT-based verification\nbackend to compute and discharge proof obligations.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 13:56:55 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Wolf", "Felix A.", ""], ["Arquint", "Linard", ""], ["Clochard", "Martin", ""], ["Oortwijn", "Wytse", ""], ["Pereira", "Jo\u00e3o C.", ""], ["M\u00fcller", "Peter", ""]]}, {"id": "2105.13941", "submitter": "Shaowei Zhu", "authors": "Shaowei Zhu and Zachary Kincaid", "title": "Reflections on Termination of Linear Loops", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper shows how techniques for linear dynamical systems can be used to\nreason about the behavior of general loops. We present two main results. First,\nwe show that every loop that can be expressed as a transition formula in linear\ninteger arithmetic has a best model as a deterministic affine transition\nsystem. Second, we show that for any linear dynamical system $f$ with integer\neigenvalues and any integer arithmetic formula $G$, there is a linear integer\narithmetic formula that holds exactly for the states of $f$ for which $G$ is\neventually invariant. Combining the two, we develop a monotone conditional\ntermination analysis for general loops.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 16:03:45 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Zhu", "Shaowei", ""], ["Kincaid", "Zachary", ""]]}, {"id": "2105.14136", "submitter": "Felicien Ihirwe", "authors": "Felicien Ihirwe, Davide Di Ruscio, Silvia Mazzini, Alfonso Pierantonio", "title": "Towards a modeling and analysis environment for industrial IoT systems", "comments": "7 figures, 10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The development of Industrial Internet of Things systems (IIoT) requires\ntools robust enough to cope with the complexity and heterogeneity of such\nsystems, which are supposed to work in safety-critical conditions. The\navailability of methodologies to support early analysis, verification, and\nvalidation is still an open issue in the research community. The early\nreal-time schedulability analysis can help quantify to what extent the desired\nsystem's timing performance can eventually be achieved. In this paper, we\npresent CHESSIoT, a model-driven environment to support the design and analysis\nof industrial IoT systems. CHESSIoT follows a multi-view, component-based\nmodelling approach with a comprehensive way to perform event-based modelling on\nsystem components for code generation purposes employing an intermediate\nThingML model. To showcase the capability of the extension, we have designed\nand analysed an Industrial real-time safety use case.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 23:03:21 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 09:50:41 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Ihirwe", "Felicien", ""], ["Di Ruscio", "Davide", ""], ["Mazzini", "Silvia", ""], ["Pierantonio", "Alfonso", ""]]}, {"id": "2105.14381", "submitter": "Gaurav Parthasarathy", "authors": "Gaurav Parthasarathy and Peter M\\\"uller and Alexander J. Summers", "title": "Formally Validating a Practical Verification Condition Generator\n  (extended version)", "comments": "Extended version of CAV 2021 publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A program verifier produces reliable results only if both the logic used to\njustify the program's correctness is sound, and the implementation of the\nprogram verifier is itself correct. Whereas it is common to formally prove\nsoundness of the logic, the implementation of a verifier typically remains\nunverified. Bugs in verifier implementations may compromise the trustworthiness\nof successful verification results. Since program verifiers used in practice\nare complex, evolving software systems, it is generally not feasible to\nformally verify their implementation.\n  In this paper, we present an alternative approach: we validate successful\nruns of the widely-used Boogie verifier by producing a certificate which proves\ncorrectness of the obtained verification result. Boogie performs a complex\nseries of program translations before ultimately generating a verification\ncondition whose validity should imply the correctness of the input program. We\nshow how to certify three of Boogie's core transformation phases: the\nelimination of cyclic control flow paths, the (SSA-like) replacement of\nassignments by assumptions using fresh variables (passification), and the final\ngeneration of verification conditions. Similar translations are employed by\nother verifiers. Our implementation produces certificates in Isabelle, based on\na novel formalisation of the Boogie language.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 22:11:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Parthasarathy", "Gaurav", ""], ["M\u00fcller", "Peter", ""], ["Summers", "Alexander J.", ""]]}, {"id": "2105.14467", "submitter": "Ruyi Ji", "authors": "Ruyi Ji, Jingtao Xia, Yingfei Xiong, Zhenjiang Hu", "title": "Occam Learning Meets Synthesis Through Unification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalizability of PBE solvers is the key to the empirical synthesis\nperformance. Despite the importance of generalizability, related studies on PBE\nsolvers are still limited. In theory, few existing solvers provide theoretical\nguarantees on generalizability, and in practice, there is a lack of PBE solvers\nwith satisfactory generalizability on important domains such as conditional\nlinear integer arithmetic (CLIA). In this paper, we adopt a concept from the\ncomputational learning theory, Occam learning, and perform a comprehensive\nstudy on the framework of synthesis through unification (STUN), a\nstate-of-the-art framework for synthesizing programs with nested if-then-else\noperators. We prove that Eusolver, a state-of-the-art STUN solver, does not\nsatisfy the condition of Occam learning, and then we design a novel STUN\nsolver, PolyGen, of which the generalizability is theoretically guaranteed by\nOccam learning. We evaluate PolyGen on the domains of CLIA and demonstrate that\nPolyGen significantly outperforms two state-of-the-art PBE solvers on CLIA,\nEusolver and Euphony, on both generalizability and efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 08:53:11 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ji", "Ruyi", ""], ["Xia", "Jingtao", ""], ["Xiong", "Yingfei", ""], ["Hu", "Zhenjiang", ""]]}, {"id": "2105.14748", "submitter": "Divyesh Unadkat", "authors": "Supratik Chakraborty, Ashutosh Gupta, Divyesh Unadkat", "title": "Diffy: Inductive Reasoning of Array Programs using Difference Invariants", "comments": "Revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel verification technique to prove interesting properties of\na class of array programs with a symbolic parameter N denoting the size of\narrays. The technique relies on constructing two slightly different versions of\nthe same program. It infers difference relations between the corresponding\nvariables at key control points of the joint control-flow graph of the two\nprogram versions. The desired post-condition is then proved by inducting on the\nprogram parameter $N$, wherein the difference invariants are crucially used in\nthe inductive step. This contrasts with classical techniques that rely on\nfinding potentially complex loop invaraints for each loop in the program. Our\nsynergistic combination of inductive reasoning and finding simple difference\ninvariants helps prove properties of programs that cannot be proved even by the\nwinner of Arrays sub-category from SV-COMP 2021. We have implemented a\nprototype tool called diffy to demonstrate these ideas. We present results\ncomparing the performance of diffy with that of state-of-the-art tools.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 07:28:30 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 14:18:11 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Chakraborty", "Supratik", ""], ["Gupta", "Ashutosh", ""], ["Unadkat", "Divyesh", ""]]}, {"id": "2105.14769", "submitter": "Petar Maksimovi\\'c", "authors": "Petar Maksimovi\\'c, Jos\\'e Fragoso Santos, Sacha-\\'Elie Ayoun,\n  Philippa Gardner", "title": "Gillian: A Multi-Language Platform for Unified Symbolic Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This is an evolving document describing the meta-theory, the implementation,\nand the instantiations of Gillian, a multi-language symbolic analysis platform.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 08:01:45 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 15:09:11 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Maksimovi\u0107", "Petar", ""], ["Santos", "Jos\u00e9 Fragoso", ""], ["Ayoun", "Sacha-\u00c9lie", ""], ["Gardner", "Philippa", ""]]}, {"id": "2105.14840", "submitter": "Tesla Zhang", "authors": "Tesla Zhang", "title": "Elegant elaboration with function invocation", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present an elegant design of the core language in a dependently-typed\nlambda calculus with $\\delta$-reduction and an elaboration algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:53:33 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 10:35:16 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 18:39:51 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Zhang", "Tesla", ""]]}]