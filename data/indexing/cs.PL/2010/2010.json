[{"id": "2010.00282", "submitter": "David Tolpin", "authors": "David Tolpin, Yuan Zhou, Tom Rainforth, Hongseok Yang", "title": "Probabilistic Programs with Stochastic Conditioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of conditioning probabilistic programs on distributions\nof observable variables. Probabilistic programs are usually conditioned on\nsamples from the joint data distribution, which we refer to as deterministic\nconditioning. However, in many real-life scenarios, the observations are given\nas marginal distributions, summary statistics, or samplers. Conventional\nprobabilistic programming systems lack adequate means for modeling and\ninference in such scenarios. We propose a generalization of deterministic\nconditioning to stochastic conditioning, that is, conditioning on the marginal\ndistribution of a variable taking a particular form. To this end, we first\ndefine the formal notion of stochastic conditioning and discuss its key\nproperties. We then show how to perform inference in the presence of stochastic\nconditioning. We demonstrate potential usage of stochastic conditioning on\nseveral case studies which involve various kinds of stochastic conditioning and\nare difficult to solve otherwise. Although we present stochastic conditioning\nin the context of probabilistic programming, our formalization is general and\napplicable to other settings.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 10:17:52 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 22:01:19 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 12:41:46 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Tolpin", "David", ""], ["Zhou", "Yuan", ""], ["Rainforth", "Tom", ""], ["Yang", "Hongseok", ""]]}, {"id": "2010.00354", "submitter": "Artur Sterz", "authors": "Artur Sterz, Matthias Eichholz, Ragnar Mogk, Lars Baumg\\\"artner, Pablo\n  Graubner, Matthias Hollick, Mira Mezini, Bernd Freisleben", "title": "ReactiFi: Reactive Programming of Wi-Fi Firmware on Mobile Devices", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2021, Vol. 5,\n  Issue 2, Article 4", "doi": "10.22152/programming-journal.org/2021/5/4", "report-no": null, "categories": "cs.PL cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network programmability will be required to handle future increased network\ntraffic and constantly changing application needs. However, there is currently\nno way of using a high-level, easy to use programming language to program Wi-Fi\nfirmware. This impedes rapid prototyping and deployment of novel network\nservices/applications and hinders continuous performance optimization in Wi-Fi\nnetworks, since expert knowledge is required for both the used hardware\nplatforms and the Wi-Fi domain. In this paper, we present ReactiFi, a\nhigh-level reactive programming language to program Wi-Fi chips on mobile\nconsumer devices. ReactiFi enables programmers to implement extensions of PHY,\nMAC, and IP layer mechanisms without requiring expert knowledge of Wi-Fi chips,\nallowing for novel applications and network protocols. ReactiFi programs are\nexecuted directly on the Wi-Fi chip, improving performance and power\nconsumption compared to execution on the main CPU. ReactiFi is conceptually\nsimilar to functional reactive languages, but is dedicated to the\ndomain-specific needs of Wi-Fi firmware. First, it handles low-level\nplatform-specific details without interfering with the core functionality of\nWi-Fi chips. Second, it supports static reasoning about memory usage of\napplications, which is important for typically memory-constrained Wi-Fi chips.\nThird, it limits dynamic changes of dependencies between computations to\ndynamic branching, in order to enable static reasoning about the order of\ncomputations. We evaluate ReactiFi empirically in two real-world case studies.\nOur results show that throughput, latency, and power consumption are\nsignificantly improved when executing applications on the Wi-Fi chip rather\nthan in the operating system kernel or in user space. Moreover, we show that\nthe high-level programming abstractions of ReactiFi have no performance\noverhead compared to manually written C code.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 12:43:15 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 10:46:53 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 14:54:30 GMT"}, {"version": "v4", "created": "Thu, 4 Mar 2021 10:24:25 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Sterz", "Artur", ""], ["Eichholz", "Matthias", ""], ["Mogk", "Ragnar", ""], ["Baumg\u00e4rtner", "Lars", ""], ["Graubner", "Pablo", ""], ["Hollick", "Matthias", ""], ["Mezini", "Mira", ""], ["Freisleben", "Bernd", ""]]}, {"id": "2010.00697", "submitter": "Ramy Shahin", "authors": "Ramy Shahin, Marsha Chechik", "title": "Automatic and Efficient Variability-Aware Lifting of Functional Programs", "comments": "OOPSLA'20 pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A software analysis is a computer program that takes some representation of a\nsoftware product as input and produces some useful information about that\nproduct as output. A software product line encompasses \\emph{many} software\nproduct variants, and thus existing analyses can be applied to each of the\nproduct variations individually, but not to the entire product line as a whole.\nEnumerating all product variants and analyzing them one by one is usually\nintractable due to the combinatorial explosion of the number of product\nvariants with respect to product line features. Several software analyses\n(e.g., type checkers, model checkers, data flow analyses) have been\nredesigned/re-implemented to support variability. This usually requires a lot\nof time and effort, and the variability-aware version of the analysis might\nhave new errors/bugs that do not exist in the original one.\n  Given an analysis program written in a functional language based on PCF, in\nthis paper we present two approaches to transforming (lifting) it into a\nsemantically equivalent variability-aware analysis. A light-weight approach\n(referred to as \\emph{shallow lifting}) wraps the analysis program into a\nvariability-aware version, exploring all combinations of its input arguments.\nDeep lifting, on the other hand, is a program rewriting mechanism where the\nsyntactic constructs of the input program are rewritten into their\nvariability-aware counterparts. Compositionally this results in an efficient\nprogram semantically equivalent to the input program, modulo variability. We\npresent the correctness criteria for functional program lifting, together with\ncorrectness proof sketches of our program transformations. We evaluate our\napproach on a set of program analyses applied to the BusyBox C-language product\nline.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 21:47:58 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Shahin", "Ramy", ""], ["Chechik", "Marsha", ""]]}, {"id": "2010.00774", "submitter": "Talia Ringer", "authors": "Talia Ringer, RanDair Porter, Nathaniel Yazdani, John Leo, Dan\n  Grossman", "title": "Proof Repair across Type Equivalences", "comments": "Tool repository with code guide:\n  https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/GUIDE.md", "journal-ref": null, "doi": "10.1145/3453483.3454033", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new approach to automatically repairing broken proofs in the\nCoq proof assistant in response to changes in types. Our approach combines a\nconfigurable proof term transformation with a decompiler from proof terms to\ntactic scripts. The proof term transformation implements transport across\nequivalences in a way that removes references to the old version of the changed\ntype and does not rely on axioms beyond those Coq assumes.\n  We have implemented this approach in PUMPKIN Pi, an extension to the PUMPKIN\nPATCH Coq plugin suite for proof repair. We demonstrate PUMPKIN Pi's\nflexibility on eight case studies, including supporting a benchmark from a user\nstudy, easing development with dependent types, porting functions and proofs\nbetween unary and binary numbers, and supporting an industrial proof engineer\nto interoperate between Coq and other verification tools more easily.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 04:49:32 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 03:19:47 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 17:23:58 GMT"}, {"version": "v4", "created": "Wed, 12 May 2021 03:31:08 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Ringer", "Talia", ""], ["Porter", "RanDair", ""], ["Yazdani", "Nathaniel", ""], ["Leo", "John", ""], ["Grossman", "Dan", ""]]}, {"id": "2010.00843", "submitter": "Frederic Prost", "authors": "Dominique Duval (LJK), Rachid Echahed (LIG), Frederic Prost (LIG)", "title": "All You Need Is CONSTRUCT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In SPARQL, the query forms SELECT and CONSTRUCT have been the subject of\nseveral studies, both theoretical and practical. However, the composition of\nsuch queries and their interweaving when forming involved nested queries has\nnot yet received much interest in the literature. We mainly tackle the problem\nof composing such queries. For this purpose, we introduce a language close to\nSPARQL where queries can be nested at will, involving either CONSTRUCT or\nSELECT query forms and provide a formal semantics for it. This semantics is\nbased on a uniform interpretation of queries. This uniformity is due to an\nextension of the notion of RDF graphs to include isolated items such as\nvariables. As a key feature of this work, we show how classical SELECT queries\ncan be easily encoded as a particular case of CONSTRUCT queries.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 08:10:32 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Duval", "Dominique", "", "LJK"], ["Echahed", "Rachid", "", "LIG"], ["Prost", "Frederic", "", "LIG"]]}, {"id": "2010.01240", "submitter": "Kesha Hietala", "authors": "Kesha Hietala, Robert Rand, Shih-Han Hung, Liyi Li and Michael Hicks", "title": "Proving Quantum Programs Correct", "comments": "version 4 updated DOI (paper content is the same); version 3 (final\n  version) has updated formatting and improved writing; version 2 includes\n  updated acknowledgments and a new appendix with simple SQIR example programs", "journal-ref": null, "doi": "10.4230/LIPIcs.ITP.2021.21", "report-no": null, "categories": "cs.PL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As quantum computing progresses steadily from theory into practice,\nprogrammers will face a common problem: How can they be sure that their code\ndoes what they intend it to do? This paper presents encouraging results in the\napplication of mechanized proof to the domain of quantum programming in the\ncontext of the SQIR development. It verifies the correctness of a range of a\nquantum algorithms including Grover's algorithm and quantum phase estimation, a\nkey component of Shor's algorithm. In doing so, it aims to highlight both the\nsuccesses and challenges of formal verification in the quantum context and\nmotivate the theorem proving community to target quantum computing as an\napplication domain.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 00:55:41 GMT"}, {"version": "v2", "created": "Sun, 27 Dec 2020 22:00:20 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 21:00:09 GMT"}, {"version": "v4", "created": "Tue, 13 Jul 2021 18:50:44 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Hietala", "Kesha", ""], ["Rand", "Robert", ""], ["Hung", "Shih-Han", ""], ["Li", "Liyi", ""], ["Hicks", "Michael", ""]]}, {"id": "2010.01548", "submitter": "Nick Brown", "authors": "Maurice Jamieson, Nick Brown", "title": "High level programming abstractions for leveraging hierarchical memories\n  with micro-core architectures", "comments": "Accepted manuscript of paper in Journal of Parallel and Distributed\n  Computing 138", "journal-ref": "In Journal of Parallel and Distributed Computing. 2020 Apr\n  1;138:128-38", "doi": "10.1016/j.jpdc.2019.11.011", "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro-core architectures combine many low memory, low power computing cores\ntogether in a single package. These are attractive for use as accelerators but\ndue to limited on-chip memory and multiple levels of memory hierarchy, the way\nin which programmers offload kernels needs to be carefully considered. In this\npaper we use Python as a vehicle for exploring the semantics and abstractions\nof higher level programming languages to support the offloading of\ncomputational kernels to these devices. By moving to a pass by reference model,\nalong with leveraging memory kinds, we demonstrate the ability to easily and\nefficiently take advantage of multiple levels in the memory hierarchy, even\nones that are not directly accessible to the micro-cores. Using a machine\nlearning benchmark, we perform experiments on both Epiphany-III and MicroBlaze\nbased micro-cores, demonstrating the ability to compute with data sets of\narbitrarily large size. To provide context of our results, we explore the\nperformance and power efficiency of these technologies, demonstrating that\nwhilst these two micro-core technologies are competitive within their own\nembedded class of hardware, there is still a way to go to reach HPC class GPUs.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 11:31:12 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Jamieson", "Maurice", ""], ["Brown", "Nick", ""]]}, {"id": "2010.01678", "submitter": "Xi Ye", "authors": "Xi Ye, Qiaochu Chen, Isil Dillig, Greg Durrett", "title": "Optimal Neural Program Synthesis from Multimodal Specifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal program synthesis, which leverages different types of user input\nto synthesize a desired program, is an attractive way to scale program\nsynthesis to challenging settings; however, it requires integrating noisy\nsignals from the user (like natural language) with hard constraints on the\nprogram's behavior. This paper proposes an optimal neural synthesis approach\nwhere the goal is to find a program that satisfies user-provided constraints\nwhile also maximizing the program's score with respect to a neural model.\nSpecifically, we focus on multimodal synthesis tasks in which the user intent\nis expressed using combination of natural language (NL) and input-output\nexamples. At the core of our method is a top-down recurrent neural model that\nplaces distributions over abstract syntax trees conditioned on the NL input.\nThis model not only allows for efficient search over the space of syntactically\nvalid programs, but it allows us to leverage automated program analysis\ntechniques for pruning the search space based on infeasibility of partial\nprograms with respect to the user's constraints. The experimental results on a\nmultimodal synthesis dataset (StructuredRegex) show that our method\nsubstantially outperforms prior state-of-the-art techniques in terms of\naccuracy %, finds model-optimal programs more frequently, and explores fewer\nstates during search.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 20:51:21 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ye", "Xi", ""], ["Chen", "Qiaochu", ""], ["Dillig", "Isil", ""], ["Durrett", "Greg", ""]]}, {"id": "2010.01700", "submitter": "Emery Berger", "authors": "Breanna Devore-McDonald and Emery D. Berger", "title": "Mossad: Defeating Software Plagiarism Detection", "comments": "30 pages. To appear, OOPSLA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.NE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic software plagiarism detection tools are widely used in educational\nsettings to ensure that submitted work was not copied. These tools have grown\nin use together with the rise in enrollments in computer science programs and\nthe widespread availability of code on-line. Educators rely on the robustness\nof plagiarism detection tools; the working assumption is that the effort\nrequired to evade detection is as high as that required to actually do the\nassigned work.\n  This paper shows this is not the case. It presents an entirely automatic\nprogram transformation approach, Mossad, that defeats popular software\nplagiarism detection tools. Mossad comprises a framework that couples\ntechniques inspired by genetic programming with domain-specific knowledge to\neffectively undermine plagiarism detectors. Mossad is effective at defeating\nfour plagiarism detectors, including Moss and JPlag. Mossad is both fast and\neffective: it can, in minutes, generate modified versions of programs that are\nlikely to escape detection. More insidiously, because of its non-deterministic\napproach, Mossad can, from a single program, generate dozens of variants, which\nare classified as no more suspicious than legitimate assignments. A detailed\nstudy of Mossad across a corpus of real student assignments demonstrates its\nefficacy at evading detection. A user study shows that graduate student\nassistants consistently rate Mossad-generated code as just as readable as\nauthentic student code. This work motivates the need for both research on more\nrobust plagiarism detection tools and greater integration of naturally\nplagiarism-resistant methodologies like code review into computer science\neducation.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 22:02:38 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Devore-McDonald", "Breanna", ""], ["Berger", "Emery D.", ""]]}, {"id": "2010.01709", "submitter": "William S. Moses", "authors": "William S. Moses and Valentin Churavy", "title": "Instead of Rewriting Foreign Code for Machine Learning, Automatically\n  Synthesize Fast Gradients", "comments": "To be published in NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.AI cs.LG cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying differentiable programming techniques and machine learning\nalgorithms to foreign programs requires developers to either rewrite their code\nin a machine learning framework, or otherwise provide derivatives of the\nforeign code. This paper presents Enzyme, a high-performance automatic\ndifferentiation (AD) compiler plugin for the LLVM compiler framework capable of\nsynthesizing gradients of statically analyzable programs expressed in the LLVM\nintermediate representation (IR). Enzyme synthesizes gradients for programs\nwritten in any language whose compiler targets LLVM IR including C, C++,\nFortran, Julia, Rust, Swift, MLIR, etc., thereby providing native AD\ncapabilities in these languages. Unlike traditional source-to-source and\noperator-overloading tools, Enzyme performs AD on optimized IR. On a\nmachine-learning focused benchmark suite including Microsoft's ADBench, AD on\noptimized IR achieves a geometric mean speedup of 4.5x over AD on IR before\noptimization allowing Enzyme to achieve state-of-the-art performance. Packaging\nEnzyme for PyTorch and TensorFlow provides convenient access to gradients of\nforeign code with state-of-the art performance, enabling foreign code to be\ndirectly incorporated into existing machine learning workflows.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 22:32:51 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Moses", "William S.", ""], ["Churavy", "Valentin", ""]]}, {"id": "2010.01723", "submitter": "Donald Pinckney", "authors": "Donald Pinckney, Arjun Guha, Yuriy Brun", "title": "Wasm/k: Delimited Continuations for WebAssembly", "comments": null, "journal-ref": "Proceedings of the ACM SIGPLAN International Symposium on Dynamic\n  Languages (DLS 2020)", "doi": "10.1145/3426422.3426978", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  WebAssembly is designed to be an alternative to JavaScript that is a safe,\nportable, and efficient compilation target for a variety of languages. The\nperformance of high-level languages depends not only on the underlying\nperformance of WebAssembly, but also on the quality of the generated\nWebAssembly code. In this paper, we identify several features of high-level\nlanguages that current approaches can only compile to WebAssembly by generating\ncomplex and inefficient code. We argue that these problems could be addressed\nif WebAssembly natively supported first-class continuations. We then present\nWasm/k, which extends WebAssembly with delimited continuations. Wasm/k\nintroduces no new value types, and thus does not require significant changes to\nthe WebAssembly type system (validation). Wasm/k is safe, even in the presence\nof foreign function calls (e.g., to and from JavaScript). Finally, Wasm/k is\namenable to efficient implementation: we implement Wasm/k as a local change to\nWasmtime, an existing WebAssembly JIT. We evaluate Wasm/k by implementing C/k,\nwhich adds delimited continuations to C/C++. C/k uses Emscripten and its\nimplementation serves as a case study on how to use Wasm/k in a compiler that\ntargets WebAssembly. We present several case studies using C/k, and show that\non implementing green threads, it can outperform the state-of-the-art approach\nAsyncify with an 18% improvement in performance and a 30% improvement in code\nsize.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 00:09:01 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Pinckney", "Donald", ""], ["Guha", "Arjun", ""], ["Brun", "Yuriy", ""]]}, {"id": "2010.02080", "submitter": "Olivier Fl\\\"uckiger", "authors": "Olivier Fl\\\"uckiger and Andreas W\\\"alchli and Sebasti\\'an Krynski and\n  Jan Vitek", "title": "Sampling Optimized Code for Type Feedback", "comments": null, "journal-ref": "Proceedings of the 16th ACM SIGPLAN International Symposium on\n  Dynamic Languages (DLS 2020)", "doi": "10.1145/3426422.3426984", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To efficiently execute dynamically typed languages, many language\nimplementations have adopted a two-tier architecture. The first tier aims for\nlow-latency startup times and collects dynamic profiles, such as the dynamic\ntypes of variables. The second tier provides high-throughput using an\noptimizing compiler that specializes code to the recorded type information. If\nthe program behavior changes to the point that not previously seen types occur\nin specialized code, that specialized code becomes invalid, it is deoptimized,\nand control is transferred back to the first tier execution engine which will\nstart specializing anew. However, if the program behavior becomes more\nspecific, for instance, if a polymorphic variable becomes monomorphic, nothing\nchanges. Once the program is running optimized code, there are no means to\nnotice that an opportunity for optimization has been missed.\n  We propose to employ a sampling-based profiler to monitor native code without\nany instrumentation. The absence of instrumentation means that when the\nprofiler is not active, no overhead is incurred. We present an implementation\nis in the context of the \\v{R} just-in-time, optimizing compiler for the R\nlanguage. Based on the sampled profiles, we are able to detect when the native\ncode produced by \\v{R} is specialized for stale type feedback and recompile it\nto more type-specific code. We show that sampling adds an overhead of less than\n3% in most cases and up to 9% in few cases and that it reliably detects stale\ntype feedback within milliseconds.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 15:16:16 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Fl\u00fcckiger", "Olivier", ""], ["W\u00e4lchli", "Andreas", ""], ["Krynski", "Sebasti\u00e1n", ""], ["Vitek", "Jan", ""]]}, {"id": "2010.02340", "submitter": "Aquinas Hobor", "authors": "Xuan-Bach Le and Aquinas Hobor and Anthony W. Lin", "title": "Complexity Analysis of Tree Share Structure", "comments": "20 pages including appendix. Published at the 16th Asian Symposium on\n  Programming Languages and Systems (APLAS 2018) in December 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tree share structure proposed by Dockins et al. is an elegant model for\ntracking disjoint ownership in concurrent separation logic, but decision\nprocedures for tree shares are hard to implement due to a lack of a systematic\ntheoretical study. We show that the first-order theory of the full Boolean\nalgebra of tree shares (that is, with all tree-share constants) is decidable\nand has the same complexity as of the first-order theory of Countable Atomless\nBoolean Algebras. We prove that combining this additive structure with a\nconstant-restricted unary multiplicative \"relativization\" operator has a\nnon-elementary lower bound. We examine the consequences of this lower bound and\nprove that it comes from the combination of both theories by proving an upper\nbound on a generalization of the restricted multiplicative theory in isolation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 21:17:18 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Le", "Xuan-Bach", ""], ["Hobor", "Aquinas", ""], ["Lin", "Anthony W.", ""]]}, {"id": "2010.02642", "submitter": "Rekha Pai", "authors": "Rishi Tulsyan, Rekha Pai, Deepak D'Souza", "title": "Static Race Detection for RTOS Applications", "comments": "18 pages Accepted in FSTTCS 2020 This version contains detailed\n  semantics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a static analysis technique for detecting data races in Real-Time\nOperating System (RTOS) applications. These applications are often employed in\nsafety-critical tasks and the presence of races may lead to erroneous behaviour\nwith serious consequences. Analyzing these applications is challenging due to\nthe variety of non-standard synchronization mechanisms they use. We propose a\ntechnique based on the notion of an \"occurs-in-between\" relation between\nstatements. This notion enables us to capture the interplay of various\nsynchronization mechanisms. We use a pre-analysis and a small set of\nnot-occurs-in-between patterns to detect whether two statements may race with\neach other. Our experimental evaluation shows that the technique is efficient\nand effective in identifying races with high precision.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 11:44:11 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Tulsyan", "Rishi", ""], ["Pai", "Rekha", ""], ["D'Souza", "Deepak", ""]]}, {"id": "2010.02727", "submitter": "Belinda Fang", "authors": "Belinda Fang, Elaine Yang, and Fei Xie", "title": "Symbolic Techniques for Deep Learning: Challenges and Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the number of deep learning frameworks increase and certain ones gain\npopularity, it spurs the discussion of what methodologies are employed by these\nframeworks and the reasoning behind them. The goal of this survey is to study\nhow symbolic techniques are utilized in deep learning. To do this, we look at\nsome of the most popular deep learning frameworks being used today, including\nTensorFlow, Keras, PyTorch, and MXNet. While these frameworks greatly differ\nfrom one another, many of them use symbolic techniques, whether it be symbolic\nexecution, graphs, or programming. We focus this paper on symbolic techniques\nbecause they influence not only how neural networks are built but also the way\nin which they are executed.\n  Limitations of symbolic techniques have led to efforts in integrating\nsymbolic and nonsymbolic aspects in deep learning, opening up new possibilities\nfor symbolic techniques. For example, the Gluon API by Apache MXNet bridges the\ngap between imperative programming and symbolic execution through\nhybridization. Frameworks such as JANUS attempt to translate imperative\nprograms into symbolic graphs, while approaches like DeepCheck attempt to use\nsymbolic execution to analyze and validate imperative neural network programs.\nSymbolic analysis has also been paired with concrete execution in a technique\ncalled concolic testing in order to better test deep neural networks. Our study\nof these developments exemplifies just a few of the many ways the symbolic\ntechniques employed by popular frameworks have the opportunity to be altered\nand utilized to achieve better performance.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 23:02:45 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Fang", "Belinda", ""], ["Yang", "Elaine", ""], ["Xie", "Fei", ""]]}, {"id": "2010.03074", "submitter": "Sanjay Rajopadhye", "authors": "Sanjay Rajopadhye", "title": "On Simplifying Dependent Polyhedral Reductions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\emph{Reductions} combine collections of input values with an associative\n(and usually also commutative) operator to produce either a single, or a\ncollection of outputs. They are ubiquitous in computing, especially with big\ndata and deep learning. When the \\emph{same} input value contributes to\nmultiple output values, there is a tremendous opportunity for reducing (pun\nintended) the computational effort. This is called \\emph{simplification}.\n\\emph{Polyhedral reductions} are reductions where the input and output data\ncollections are (dense) multidimensional arrays (i.e., \\emph{tensors}),\naccessed with linear/affine functions of the indices. % \\emph{generalized\ntensor contractions} Gautam and Rajopadhye \\cite{sanjay-popl06} showed how\npolyhedral reductions could be simplified automatically (through compile time\nanalysis) and optimally (the resulting program had minimum asymptotic\ncomplexity). Yang, Atkinson and Carbin \\cite{yang2020simplifying} extended this\nto the case when (some) input values depend on (some) outputs. Specifically,\nthey showed how the optimal simplification problem could be formulated as a\nbilinear programming problem, and for the case when the reduction operator\nadmits an inverse, they gave a heuristic solution that retained optimality. In\nthis note, we show that simplification of dependent reductions can be\nformulated as a simple extension of the Gautam-Rajopadhye backtracking search\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 22:57:20 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Rajopadhye", "Sanjay", ""]]}, {"id": "2010.03444", "submitter": "Marcel Moosbrugger", "authors": "Marcel Moosbrugger, Ezio Bartocci, Joost-Pieter Katoen, Laura Kov\\'acs", "title": "Automated Termination Analysis of Polynomial Probabilistic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The termination behavior of probabilistic programs depends on the outcomes of\nrandom assignments. Almost sure termination (AST) is concerned with the\nquestion whether a program terminates with probability one on all possible\ninputs. Positive almost sure termination (PAST) focuses on termination in a\nfinite expected number of steps. This paper presents a fully automated approach\nto the termination analysis of probabilistic while-programs whose guards and\nexpressions are polynomial expressions. As proving (positive) AST is\nundecidable in general, existing proof rules typically provide sufficient\nconditions. These conditions mostly involve constraints on supermartingales. We\nconsider four proof rules from the literature and extend these with\ngeneralizations of existing proof rules for (P)AST. We automate the resulting\nset of proof rules by effectively computing asymptotic bounds on polynomials\nover the program variables. These bounds are used to decide the sufficient\nconditions - including the constraints on supermartingales - of a proof rule.\nOur software tool Amber can thus check AST, PAST, as well as their negations\nfor a large class of polynomial probabilistic programs, while carrying out the\ntermination reasoning fully with polynomial witnesses. Experimental results\nshow the merits of our generalized proof rules and demonstrate that Amber can\nhandle probabilistic programs that are out of reach for other state-of-the-art\ntools.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 14:37:23 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 18:06:31 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 16:10:38 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Moosbrugger", "Marcel", ""], ["Bartocci", "Ezio", ""], ["Katoen", "Joost-Pieter", ""], ["Kov\u00e1cs", "Laura", ""]]}, {"id": "2010.03485", "submitter": "Feras Saad", "authors": "Feras A. Saad, Martin C. Rinard, Vikash K. Mansinghka", "title": "SPPL: Probabilistic Programming with Fast Exact Symbolic Inference", "comments": null, "journal-ref": "Proceedings of the 42nd ACM SIGPLAN International Conference on\n  Programming Language Design and Implementation (PLDI '21), June 20-25, 2021,\n  Virtual, Canada. ACM, New York, NY, USA", "doi": "10.1145/3453483.3454078", "report-no": null, "categories": "cs.PL cs.LG cs.SC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Sum-Product Probabilistic Language (SPPL), a new probabilistic\nprogramming language that automatically delivers exact solutions to a broad\nrange of probabilistic inference queries. SPPL translates probabilistic\nprograms into sum-product expressions, a new symbolic representation and\nassociated semantic domain that extends standard sum-product networks to\nsupport mixed-type distributions, numeric transformations, logical formulas,\nand pointwise and set-valued constraints. We formalize SPPL via a novel\ntranslation strategy from probabilistic programs to sum-product expressions and\ngive sound exact algorithms for conditioning on and computing probabilities of\nevents. SPPL imposes a collection of restrictions on probabilistic programs to\nensure they can be translated into sum-product expressions, which allow the\nsystem to leverage new techniques for improving the scalability of translation\nand inference by automatically exploiting probabilistic structure. We implement\na prototype of SPPL with a modular architecture and evaluate it on benchmarks\nthe system targets, showing that it obtains up to 3500x speedups over\nstate-of-the-art symbolic systems on tasks such as verifying the fairness of\ndecision tree classifiers, smoothing hidden Markov models, conditioning\ntransformed random variables, and computing rare event probabilities.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 15:42:37 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 07:29:46 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 12:21:13 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Saad", "Feras A.", ""], ["Rinard", "Martin C.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "2010.03608", "submitter": "Sam Tobin-Hochstadt", "authors": "Yuquan Fu and Sam Tobin-Hochstadt", "title": "Type checking extracted methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many object-oriented dynamic languages allow programmers to extract methods\nfrom objects and treat them as functions. This allows for flexible programming\npatterns, but presents challenges for type systems. In particular, a simple\ntreatment of method extraction would require methods to be contravariant in the\nreciever type, making overriding all-but-impossible.\n  We present a detailed investigation of this problem, as well as an\nimplemented and evaluated solution. We show how existing gradual type systems\nsuch as TypeScript and Flow are unsound in the presence of method extraction.\nWorking in the context of Racket, whose \\emph{structure} system includes a\nlow-level form of methods, we show how to combine two existing type system\nfeatures---existential types and occurrence typing--to produce a sound approach\nto typing method extraction. Our design is proved sound, has been implemented\nand shipped in Typed Racket, and works for existing Racket programs using these\nfeatures.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 19:05:51 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Fu", "Yuquan", ""], ["Tobin-Hochstadt", "Sam", ""]]}, {"id": "2010.03993", "submitter": "Graham Campbell", "authors": "Graham Campbell, Jack Romo, Detlef Plump", "title": "The Improved GP 2 Compiler", "comments": "11 pages, 2020. arXiv admin note: substantial text overlap with\n  arXiv:2002.02914", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  GP 2 is a rule-based programming language based on graph transformation rules\nwhich aims to facilitate program analysis and verification. Writing efficient\nprograms in such a language is challenging because graph matching is expensive.\nGP 2 addresses this problem by providing rooted rules which, under mild\nconditions, can be matched in constant time. Recently, we implemented a number\nof changes to Bak's GP 2-to-C compiler in order to speed up graph programs. One\nkey improvement is a new data structure for dynamic arrays called BigArray.\nThis is an array of pointers to arrays of entries, successively doubling in\nsize. To demonstrate the speed-up achievable with the new implementation, we\npresent a reduction program for recognising binary DAGs which previously ran in\nquadratic time but now runs in linear time when compiled with the new compiler.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 19:47:33 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 01:32:59 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Campbell", "Graham", ""], ["Romo", "Jack", ""], ["Plump", "Detlef", ""]]}, {"id": "2010.04017", "submitter": "Alex Renda", "authors": "Alex Renda, Yishen Chen, Charith Mendis, Michael Carbin", "title": "DiffTune: Optimizing CPU Simulator Parameters with Learned\n  Differentiable Surrogates", "comments": null, "journal-ref": "MICRO 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.AR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CPU simulators are useful tools for modeling CPU execution behavior. However,\nthey suffer from inaccuracies due to the cost and complexity of setting their\nfine-grained parameters, such as the latencies of individual instructions. This\ncomplexity arises from the expertise required to design benchmarks and\nmeasurement frameworks that can precisely measure the values of parameters at\nsuch fine granularity. In some cases, these parameters do not necessarily have\na physical realization and are therefore fundamentally approximate, or even\nunmeasurable.\n  In this paper we present DiffTune, a system for learning the parameters of\nx86 basic block CPU simulators from coarse-grained end-to-end measurements.\nGiven a simulator, DiffTune learns its parameters by first replacing the\noriginal simulator with a differentiable surrogate, another function that\napproximates the original function; by making the surrogate differentiable,\nDiffTune is then able to apply gradient-based optimization techniques even when\nthe original function is non-differentiable, such as is the case with CPU\nsimulators. With this differentiable surrogate, DiffTune then applies\ngradient-based optimization to produce values of the simulator's parameters\nthat minimize the simulator's error on a dataset of ground truth end-to-end\nperformance measurements. Finally, the learned parameters are plugged back into\nthe original simulator.\n  DiffTune is able to automatically learn the entire set of\nmicroarchitecture-specific parameters within the Intel x86 simulation model of\nllvm-mca, a basic block CPU simulator based on LLVM's instruction scheduling\nmodel. DiffTune's learned parameters lead llvm-mca to an average error that not\nonly matches but lowers that of its original, expert-provided parameter values.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 14:28:03 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 00:40:02 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Renda", "Alex", ""], ["Chen", "Yishen", ""], ["Mendis", "Charith", ""], ["Carbin", "Michael", ""]]}, {"id": "2010.04126", "submitter": "Hengchu Zhang", "authors": "Hengchu Zhang, Edo Roth, Andreas Haeberlen, Benjamin C. Pierce, Aaron\n  Roth", "title": "Testing Differential Privacy with Dual Interpreters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying differential privacy at scale requires convenient ways to check that\nprograms computing with sensitive data appropriately preserve privacy. We\npropose here a fully automated framework for {\\em testing} differential\nprivacy, adapting a well-known \"pointwise\" technique from informal proofs of\ndifferential privacy. Our framework, called DPCheck, requires no programmer\nannotations, handles all previously verified or tested algorithms, and is the\nfirst fully automated framework to distinguish correct and buggy\nimplementations of PrivTree, a probabilistically terminating algorithm that has\nnot previously been mechanically checked.\n  We analyze the probability of DPCheck mistakenly accepting a non-private\nprogram and prove that, theoretically, the probability of false acceptance can\nbe made exponentially small by suitable choice of test size.\n  We demonstrate DPCheck's utility empirically by implementing all benchmark\nalgorithms from prior work on mechanical verification of differential privacy,\nplus several others and their incorrect variants, and show DPCheck accepts the\ncorrect implementations and rejects the incorrect variants.\n  We also demonstrate how DPCheck can be deployed in a practical workflow to\ntest differentially privacy for the 2020 US Census Disclosure Avoidance System\n(DAS).\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 17:09:03 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Zhang", "Hengchu", ""], ["Roth", "Edo", ""], ["Haeberlen", "Andreas", ""], ["Pierce", "Benjamin C.", ""], ["Roth", "Aaron", ""]]}, {"id": "2010.04449", "submitter": "David Castro-Perez", "authors": "David Castro-Perez and Nobuko Yoshida", "title": "CAMP: Cost-Aware Multiparty Session Protocols", "comments": "Accepted at OOPSLA'20", "journal-ref": null, "doi": "10.1145/3428223", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents CAMP, a new static performance analysis framework for\nmessage-passing concurrent and distributed systems, based on the theory of\nmultiparty session types (MPST). Understanding the run-time performance of\nconcurrent and distributed systems is of great importance for the\nidentification of bottlenecks and optimisation opportunities. In the\nmessage-passing setting, these bottlenecks are generally communication\noverheads and synchronisation times. Despite its importance, reasoning about\nthese intensional properties of software, such as performance, has received\nlittle attention, compared to verifying extensional properties, such as\ncorrectness. Behavioural protocol specifications based on sessions types\ncapture not only extensional, but also intensional properties of concurrent and\ndistributed systems. CAMP augments MPST with annotations of communication\nlatency and local computation cost, defined as estimated execution times, that\nwe use to extract cost equations from protocol descriptions. CAMP is also\nextendable to analyse asynchronous communication optimisation built on a recent\nadvance of session type theories. We apply our tool to different existing\nbenchmarks and use cases in the literature with a wide range of communication\nprotocols, implemented in C, MPI-C, Scala, Go, and OCaml. Our benchmarks show\nthat, in most of the cases, we predict an upper-bound on the real execution\ncosts with < 15% error.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 09:23:16 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Castro-Perez", "David", ""], ["Yoshida", "Nobuko", ""]]}, {"id": "2010.04633", "submitter": "Philipp Krause", "authors": "Philipp Klaus Krause, Nicolas Lesser", "title": "C for a tiny system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We have implemented support for Padauk microcontrollers, tiny 8-Bit devices\nwith 60 B to 256 B of RAM, in the Small Device C Compiler (SDCC), showing that\nthe use of (mostly) standard C to program such minimal devices is feasible. We\nreport on our experience and on the difficulties in supporting the hardware\nmultithreading present on some of these devices. To make the devices a better\ntarget for C, we propose various enhancements of the architecture, and\nempirically evaluated their impact on code size.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 15:25:33 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Krause", "Philipp Klaus", ""], ["Lesser", "Nicolas", ""]]}, {"id": "2010.04800", "submitter": "Reed Oei", "authors": "Reed Oei, Michael Coblenz, Jonathan Aldrich", "title": "Psamathe: A DSL with Flows for Safe Blockchain Assets", "comments": "Working draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchains host smart contracts for crowdfunding, tokens, and many other\npurposes. Vulnerabilities in contracts are often discovered, leading to the\nloss of large quantities of money. Psamathe is a new language we are designing\naround a new flow abstraction, reducing asset bugs and making contracts more\nconcise than in existing languages. We present an overview of Psamathe,\nincluding a partial formalization. We also discuss several example contracts in\nPsamathe, and compare the Psamathe examples to the same contracts written in\nSolidity.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 20:47:29 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 16:10:08 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Oei", "Reed", ""], ["Coblenz", "Michael", ""], ["Aldrich", "Jonathan", ""]]}, {"id": "2010.04811", "submitter": "Bruce Collie", "authors": "Bruce Collie, Jackson Woodruff, Michael F.P. O'Boyle", "title": "Modeling Black-Box Components with Probabilistic Synthesis", "comments": "Accepted to GPCE 2020", "journal-ref": null, "doi": "10.1145/3425898.3426952", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with synthesizing programs based on black-box\noracles: we are interested in the case where there exists an executable\nimplementation of a component or library, but its internal structure is\nunknown. We are provided with just an API or function signature, and aim to\nsynthesize a program with equivalent behavior.\n  To attack this problem, we detail Presyn: a program synthesizer designed for\nflexible interoperation with existing programs and compiler toolchains. Presyn\nuses high-level imperative control-flow structures and a pair of cooperating\npredictive models to efficiently narrow the space of potential programs. These\nmodels can be trained effectively on small corpora of synthesized examples.\n  We evaluate Presyn against five leading program synthesizers on a collection\nof 112 synthesis benchmarks collated from previous studies and real-world\nsoftware libraries. We show that Presyn is able to synthesize a wider range of\nprograms than each of them with less human input. We demonstrate the\napplication of our approach to real-world code and software engineering\nproblems with two case studies: accelerator library porting and detection of\nduplicated library reimplementations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 21:17:03 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Collie", "Bruce", ""], ["Woodruff", "Jackson", ""], ["O'Boyle", "Michael F. P.", ""]]}, {"id": "2010.04918", "submitter": "James Koppel", "authors": "James Koppel, Jackson Kearl, Armando Solar-Lezama", "title": "Automatically Deriving Control-Flow Graph Generators from Operational\n  Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the first theory of control-flow graphs from first principles, and\nuse it to create an algorithm for automatically synthesizing many variants of\ncontrol-flow graph generators from a language's operational semantics. Our\napproach first introduces a new algorithm for converting a large class of\nsmall-step operational semantics to an abstract machine. It next uses a\ntechnique called \"abstract rewriting\" to automatically abstract the semantics\nof a language, which is used both to directly generate a CFG from a program\n(\"interpreted mode\") and to generate standalone code, similar to a\nhuman-written CFG generator, for any program in a language. We show how the\nchoice of two abstraction and projection parameters allow our approach to\nsynthesize several families of CFG-generators useful for different kinds of\ntools. We prove the correspondence between the generated graphs and the\noriginal semantics. We provide and prove an algorithm for automatically proving\nthe termination of interpreted-mode generators. In addition to our theoretical\nresults, we have implemented this algorithm in a tool called Mandate, and show\nthat it produces human-readable code on two medium-size languages with 60-80\nrules, featuring nearly all intraprocedural control constructs common in modern\nlanguages. We then showed these CFG-generators were sufficient to build two\nstatic analyzers atop them. Our work is a promising step towards the grand\nvision of being able to synthesize all desired tools from the semantics of a\nprogramming language.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 06:28:11 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Koppel", "James", ""], ["Kearl", "Jackson", ""], ["Solar-Lezama", "Armando", ""]]}, {"id": "2010.05050", "submitter": "Yicheng Luo", "authors": "Yicheng Luo, Antonio Filieri, Yuan Zhou", "title": "Symbolic Parallel Adaptive Importance Sampling for Probabilistic Program\n  Analysis", "comments": "Extended pre-print version of ESEC/FSE '21 paper", "journal-ref": null, "doi": "10.1145/3468264.3468593", "report-no": null, "categories": "cs.LG cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic software analysis aims at quantifying the probability of a\ntarget event occurring during the execution of a program processing uncertain\nincoming data or written itself using probabilistic programming constructs.\nRecent techniques combine symbolic execution with model counting or solution\nspace quantification methods to obtain accurate estimates of the occurrence\nprobability of rare target events, such as failures in a mission-critical\nsystem. However, they face several scalability and applicability limitations\nwhen analyzing software processing with high-dimensional and correlated\nmultivariate input distributions. In this paper, we present SYMbolic Parallel\nAdaptive Importance Sampling (SYMPAIS), a new inference method tailored to\nanalyze path conditions generated from the symbolic execution of programs with\nhigh-dimensional, correlated input distributions. SYMPAIS combines results from\nimportance sampling and constraint solving to produce accurate estimates of the\nsatisfaction probability for a broad class of constraints that cannot be\nanalyzed by current solution space quantification methods. We demonstrate\nSYMPAIS's generality and performance compared with state-of-the-art\nalternatives on a set of problems from different application domains.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 17:39:12 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 19:32:14 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Luo", "Yicheng", ""], ["Filieri", "Antonio", ""], ["Zhou", "Yuan", ""]]}, {"id": "2010.05167", "submitter": "Tatsuya Hagino", "authors": "Tatsuya Hagino", "title": "A Categorical Programming Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A theory of data types based on category theory is presented. We organize\ndata types under a new categorical notion of F,G-dialgebras which is an\nextension of the notion of adjunctions as well as that of T-algebras.\nT-algebras are also used in domain theory, but while domain theory needs some\nprimitive data types, like products, to start with, we do not need any.\nProducts, coproducts and exponentiations (i.e. function spaces) are defined\nexactly like in category theory using adjunctions. F,G-dialgebras also enable\nus to define the natural number object, the object for finite lists and other\nfamiliar data types in programming. Furthermore, their symmetry allows us to\nhave the dual of the natural number object and the object for infinite lists\n(or lazy lists). We also introduce a programming language in a categorical\nstyle using F,G-dialgebras as its data type declaration mechanism. We define\nthe meaning of the language operationally and prove that any program terminates\nusing Tait's computability method.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 04:44:19 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Hagino", "Tatsuya", ""]]}, {"id": "2010.05484", "submitter": "Rupak Majumdar", "authors": "Rupak Majumdar and Nobuko Yoshida and Damien Zufferey", "title": "Multiparty Motion Coordination: From Choreographies to Robotics Programs", "comments": "Full version of OOPSLA 2020 Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a programming model and typing discipline for complex multi-robot\ncoordination programming. Our model encompasses both synchronisation through\nmessage passing and continuous-time dynamic motion primitives in physical\nspace. We specify \\emph{continuous-time motion primitives} in an\nassume-guarantee logic that ensures compatibility of motion primitives as well\nas collision freedom. We specify global behaviour of programs in a\n\\emph{choreographic} type system that extends multiparty session types with\njointly executed motion primitives, predicated refinements, as well as a\n\\emph{separating conjunction} that allows reasoning about subsets of\ninteracting robots. We describe a notion of \\emph{well-formedness} for global\ntypes that ensures motion and communication can be correctly synchronised and\nprovide algorithms for checking well-formedness, projecting a type, and local\ntype checking. A well-typed program is \\emph{communication safe}, \\emph{motion\ncompatible}, and \\emph{collision free}. Our type system provides a\ncompositional approach to ensuring these properties.\n  We have implemented our model on top of the ROS framework. This allows us to\nprogram multi-robot coordination scenarios on top of commercial and custom\nrobotics hardware platforms. We show through case studies that we can model and\nstatically verify quite complex manoeuvres involving multiple manipulators and\nmobile robots---such examples are beyond the scope of previous approaches.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 07:08:44 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Majumdar", "Rupak", ""], ["Yoshida", "Nobuko", ""], ["Zufferey", "Damien", ""]]}, {"id": "2010.05747", "submitter": "Ton Chanh Le", "authors": "Ton Chanh Le, Timos Antonopoulos, Parisa Fathololumi, Eric Koskinen,\n  ThanhVu Nguyen", "title": "DynamiTe: Dynamic Termination and Non-termination Proofs", "comments": "To appear at OOPSLA 2020", "journal-ref": "Proc. ACM Program. Lang. 4, OOPSLA, November 2020", "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing interest in termination reasoning for non-linear programs\nand, meanwhile, recent dynamic strategies have shown they are able to infer\ninvariants for such challenging programs. These advances led us to hypothesize\nthat perhaps such dynamic strategies for non-linear invariants could be adapted\nto learn recurrent sets (for non-termination) and/or ranking functions (for\ntermination). In this paper, we exploit dynamic analysis and draw termination\nand non-termination as well as static and dynamic strategies closer together in\norder to tackle non-linear programs. For termination, our algorithm infers\nranking functions from concrete transitive closures, and, for non-termination,\nthe algorithm iteratively collects executions and dynamically learns conditions\nto refine recurrent sets. Finally, we describe an integrated algorithm that\nallows these algorithms to mutually inform each other, taking counterexamples\nfrom a failed validation in one endeavor and crossing both the static/dynamic\nand termination/non-termination lines, to create new execution samples for the\nother one.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 14:39:47 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Le", "Ton Chanh", ""], ["Antonopoulos", "Timos", ""], ["Fathololumi", "Parisa", ""], ["Koskinen", "Eric", ""], ["Nguyen", "ThanhVu", ""]]}, {"id": "2010.05807", "submitter": "Keita Takenouchi", "authors": "Keita Takenouchi, Takashi Ishio, Joji Okada, Yuji Sakata", "title": "PATSQL: Efficient Synthesis of SQL Queries from Example Tables with\n  Quick Inference of Projected Columns", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SQL is one of the most popular tools for data analysis and used by an\nincreasing number of users without having expertise in databases. In order to\nhelp such non-experts to write correct SQL queries, several studies have\nproposed programming-by-example approaches. In these approaches, the user can\nobtain a desired query just by giving input and output (I/O) tables as an\nexample. While existing methods support a variety of SQL features such as\naggregation and nested query, they suffer a significant increase in\ncomputational cost as the scale of I/O tables increases. In this paper, we\npropose an efficient algorithm that synthesizes SQL queries from I/O tables.\nSpecifically, it has strengths in both the execution time and the scale of\nsupported tables. We adopt a sketch-based synthesis algorithm and focus on the\nquick inference of the columns used in the projection operator. In particular,\nwe restrict the structures of sketches based on transformation rules in\nrelational algebra and propagate a novel form of constraint using the output\ntable in a top-down manner. We implemented this algorithm in our tool PATSQL\nand evaluated it on 118 queries from prior benchmarks and Kaggle's tutorials.\nAs a result, PATSQL solved 72% of the benchmarks and found 92% of the solutions\nwithin a second. Our tool is available at https://naist-se.github.io/patsql/ .\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 16:02:27 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 07:01:56 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Takenouchi", "Keita", ""], ["Ishio", "Takashi", ""], ["Okada", "Joji", ""], ["Sakata", "Yuji", ""]]}, {"id": "2010.05812", "submitter": "Gidon Ernst", "authors": "Gidon Ernst", "title": "A Complete Approach to Loop Verification with Invariants and Summaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Loop invariants characterize the partial result computed by a loop so far up\nto an intermediate state. It has been noted, however, that complementing\ninvariants by summaries, which characterize the remaining iterations of a loop,\ncan often lead to simpler correctness proofs. In this paper, we derive sound\nverification conditions for this approach, and moreover characterize\ncompleteness relative to a class of \"safe\" invariants, alongside with\nfundamental and novel insights in the relation between invariants and\nsummaries. All theoretical results have immediate practical consequences for\ntool use and construction. Summaries should therefore be regarded as a\nprincipal alternative to invariants. To substantiate this claim experimentally,\nwe evaluate the automation potential using state-of-the-art Horn solvers, which\nshows that the the proposed approach is competitive, even without specialized\nsolving strategies.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 16:07:17 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 08:37:49 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Ernst", "Gidon", ""]]}, {"id": "2010.06135", "submitter": "Lei Shi", "authors": "Lei Shi, Yahui Li, Rajeev Alur, Boon Thau Loo", "title": "Session-layer Attack Traffic Classification by Program Synthesis", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writing classification rules to identify malicious network traffic is a\ntime-consuming and error-prone task. Learning-based classification systems\nautomatically extract such rules from positive and negative traffic examples.\nHowever, due to limitations in the representation of network traffic and the\nlearning strategy, these systems lack both expressiveness to cover a range of\nattacks and interpretability in fully describing the attack traffic's structure\nat the session layer. This paper presents Sharingan system, which uses program\nsynthesis techniques to generate network classification programs at the session\nlayer. Sharingan accepts raw network traces as inputs, and reports potential\npatterns of the attack traffic in NetQRE, a domain specific language designed\nfor specifying session-layer quantitative properties. Using Sharingan, network\noperators can better analyze the attack pattern due to the following advantages\nof Sharingan's learning process: (1) it requires minimal feature engineering,\n(2) it is amenable to efficient implementation of the learnt classifier, and\n(3) the synthesized program is easy to decipher and edit. We develop a range of\nnovel optimizations that reduce the synthesis time for large and complex tasks\nto a matter of minutes. Our experiments show that Sharingan is able to\ncorrectly identify attacks from a diverse set of network attack traces and\ngenerates explainable outputs, while achieving accuracy comparable to\nstate-of-the-art learning-based intrusion detection systems.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 03:07:08 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Shi", "Lei", ""], ["Li", "Yahui", ""], ["Alur", "Rajeev", ""], ["Loo", "Boon Thau", ""]]}, {"id": "2010.06216", "submitter": "Koar Marntirosian", "authors": "Koar Marntirosian, Tom Schrijvers, Bruno C. d. S. Oliveira, Georgios\n  Karachalias", "title": "Resolution as Intersection Subtyping via Modus Ponens", "comments": "43 pages, 20 figures; typos corrected, link to artifact added", "journal-ref": null, "doi": "10.1145/3428274", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Resolution and subtyping are two common mechanisms in programming languages.\nResolution is used by features such as type classes or Scala-style implicits to\nsynthesize values automatically from contextual type information. Subtyping is\ncommonly used to automatically convert the type of a value into another\ncompatible type. So far the two mechanisms have been considered independently\nof each other. This paper shows that, with a small extension, subtyping with\nintersection types can subsume resolution. This has three main consequences.\nFirstly, resolution does not need to be implemented as a separate mechanism.\nSecondly, the interaction between resolution and subtyping becomes apparent.\nFinally, the integration of resolution into subtyping enables first-class\n(implicit) environments. The extension that recovers the power of resolution\nvia subtyping is the modus ponens rule of propositional logic. While it is\neasily added to declarative subtyping, significant care needs to be taken to\nretain desirable properties, such as transitivity and decidability of\nalgorithmic subtyping, and coherence. To materialize these ideas we develop\n$\\lambda_i^{\\mathsf{MP}}$, a calculus that extends a iprevious calculus with\ndisjoint intersection types, and develop its metatheory in the Coq theorem\nprover.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 07:58:17 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 09:32:19 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Marntirosian", "Koar", ""], ["Schrijvers", "Tom", ""], ["Oliveira", "Bruno C. d. S.", ""], ["Karachalias", "Georgios", ""]]}, {"id": "2010.06256", "submitter": "Razan Ghzouli", "authors": "Razan Ghzouli, Thorsten Berger, Einar Broch Johnsen, Swaib Dragule,\n  Andrzej W\\k{a}sowski", "title": "Behavior Trees in Action: A Study of Robotics Applications", "comments": "14 pages, 5 figures, 13rd ACM SIGPLAN International Conference on\n  Software Language Engineering (SLE) (SLE 2020)", "journal-ref": null, "doi": "10.1145/3426425.3426942", "report-no": null, "categories": "cs.RO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous robots combine a variety of skills to form increasingly complex\nbehaviors called missions. While the skills are often programmed at a\nrelatively low level of abstraction, their coordination is architecturally\nseparated and often expressed in higher-level languages or frameworks.\nRecently, the language of Behavior Trees gained attention among roboticists for\nthis reason. Originally designed for computer games to model autonomous actors,\nBehavior Trees offer an extensible tree-based representation of missions.\nHowever, even though, several implementations of the language are in use,\nlittle is known about its usage and scope in the real world. How do behavior\ntrees relate to traditional languages for describing behavior? How are behavior\ntree concepts used in applications? What are the benefits of using them?\n  We present a study of the key language concepts in Behavior Trees and their\nuse in real-world robotic applications. We identify behavior tree languages and\ncompare their semantics to the most well-known behavior modeling languages:\nstate and activity diagrams. We mine open source repositories for robotics\napplications that use the language and analyze this usage. We find that\nBehavior Trees are a pragmatic language, not fully specified, allowing projects\nto extend it even for just one model. Behavior trees clearly resemble the\nmodels-at-runtime paradigm. We contribute a dataset of real-world behavior\nmodels, hoping to inspire the community to use and further develop this\nlanguage, associated tools, and analysis techniques.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 09:45:58 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 09:23:20 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Ghzouli", "Razan", ""], ["Berger", "Thorsten", ""], ["Johnsen", "Einar Broch", ""], ["Dragule", "Swaib", ""], ["W\u0105sowski", "Andrzej", ""]]}, {"id": "2010.06474", "submitter": "D. S. Hollman", "authors": "D. S. Hollman and Bryce Adelstein Lelbach and H. Carter Edwards and\n  Mark Hoemmen and Daniel Sunderland and Christian R. Trott", "title": "mdspan in C++: A Case Study in the Integration of Performance Portable\n  Features into International Language Standards", "comments": null, "journal-ref": "2019 IEEE/ACM International Workshop on Performance, Portability\n  and Productivity in HPC (P3HPC), Denver, CO, USA, 2019, pp. 60-70", "doi": "10.1109/P3HPC49587.2019.00011", "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-dimensional arrays are ubiquitous in high-performance computing (HPC),\nbut their absence from the C++ language standard is a long-standing and\nwell-known limitation of their use for HPC. This paper describes the design and\nimplementation of mdspan, a proposed C++ standard multidimensional array view\n(planned for inclusion in C++23). The proposal is largely inspired by work done\nin the Kokkos project---a C++ performance-portable programming model deployed\nby numerous HPC institutions to prepare their code base for exascale-class\nsupercomputing systems. This paper describes the final design of mdspan after a\nfive-year process to achieve consensus in the C++ community. In particular, we\nwill lay out how the design addresses some of the core challenges of\nperformance-portable programming, and how its customization points allow a\nseamless extension into areas not currently addressed by the C++ Standard but\nwhich are of critical importance in the heterogeneous computing world of\ntoday's systems. Finally, we have provided a production-quality implementation\nof the proposal in its current form. This work includes several benchmarks of\nthis implementation aimed at demonstrating the zero-overhead nature of the\nmodern design.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 15:29:20 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Hollman", "D. S.", ""], ["Lelbach", "Bryce Adelstein", ""], ["Edwards", "H. Carter", ""], ["Hoemmen", "Mark", ""], ["Sunderland", "Daniel", ""], ["Trott", "Christian R.", ""]]}, {"id": "2010.06482", "submitter": "Ankush Das", "authors": "Ankush Das, Henry DeYoung, Andreia Mordido, Frank Pfenning", "title": "Nested Session Types", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session types statically describe communication protocols between concurrent\nmessage-passing processes. Unfortunately, parametric polymorphism even in its\nrestricted prenex form is not fully understood in the context of session types.\nIn this paper, we present the metatheory of session types extended with prenex\npolymorphism and, as a result, nested recursive datatypes. Remarkably, we prove\nthat type equality is decidable by exhibiting a reduction to trace equivalence\nof deterministic first-order grammars. Recognizing the high theoretical\ncomplexity of the latter, we also propose a novel type equality algorithm and\nprove its soundness. We observe that the algorithm is surprisingly efficient\nand, despite its incompleteness, sufficient for all our examples. We have\nimplemented our ideas by extending the Rast programming language with nested\nsession types. We conclude with several examples illustrating the expressivity\nof our enhanced type system.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 15:40:39 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 02:30:52 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 01:10:04 GMT"}, {"version": "v4", "created": "Wed, 9 Dec 2020 21:10:46 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Das", "Ankush", ""], ["DeYoung", "Henry", ""], ["Mordido", "Andreia", ""], ["Pfenning", "Frank", ""]]}, {"id": "2010.06580", "submitter": "Daniel Fremont", "authors": "Daniel J. Fremont and Edward Kim and Tommaso Dreossi and Shromona\n  Ghosh and Xiangyu Yue and Alberto L. Sangiovanni-Vincentelli and Sanjit A.\n  Seshia", "title": "Scenic: A Language for Scenario Specification and Data Generation", "comments": "Supercedes arXiv:1809.09310", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new probabilistic programming language for the design and\nanalysis of cyber-physical systems, especially those based on machine learning.\nSpecifically, we consider the problems of training a system to be robust to\nrare events, testing its performance under different conditions, and debugging\nfailures. We show how a probabilistic programming language can help address\nthese problems by specifying distributions encoding interesting types of\ninputs, then sampling these to generate specialized training and test data.\nMore generally, such languages can be used to write environment models, an\nessential prerequisite to any formal analysis. In this paper, we focus on\nsystems like autonomous cars and robots, whose environment at any point in time\nis a 'scene', a configuration of physical objects and agents. We design a\ndomain-specific language, Scenic, for describing scenarios that are\ndistributions over scenes and the behaviors of their agents over time. As a\nprobabilistic programming language, Scenic allows assigning distributions to\nfeatures of the scene, as well as declaratively imposing hard and soft\nconstraints over the scene. We develop specialized techniques for sampling from\nthe resulting distribution, taking advantage of the structure provided by\nScenic's domain-specific syntax. Finally, we apply Scenic in a case study on a\nconvolutional neural network designed to detect cars in road images, improving\nits performance beyond that achieved by state-of-the-art synthetic data\ngeneration methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 17:58:31 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Fremont", "Daniel J.", ""], ["Kim", "Edward", ""], ["Dreossi", "Tommaso", ""], ["Ghosh", "Shromona", ""], ["Yue", "Xiangyu", ""], ["Sangiovanni-Vincentelli", "Alberto L.", ""], ["Seshia", "Sanjit A.", ""]]}, {"id": "2010.06622", "submitter": "Carla Ferreira", "authors": "Filipe Meirim and M\\'ario Pereira and Carla Ferreira", "title": "CISE3: Verifying Weakly Consistent Applications with Why3", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a tool for the formal analysis of applications built\non top of replicated databases, where data integrity can be at stake. To\naddress this issue, one can introduce synchronization in the system.\nIntroducing synchronization in too many places can hurt the system's\navailability but if introduced in too few places, then data integrity can be\ncompromised. The goal of our tool is to aid the programmer reason about the\ncorrect balance of synchronization in the system. Our tool analyses a\nsequential specification and deduces which operations require synchronization\nin order for the program to safely execute in a distributed environment. Our\nprototype is built on top of the deductive verification platform Why3, which\nprovides a friendly and integrated user experience. Several case studies have\nbeen successfully verified using our tool.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 18:24:10 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Meirim", "Filipe", ""], ["Pereira", "M\u00e1rio", ""], ["Ferreira", "Carla", ""]]}, {"id": "2010.07080", "submitter": "Felix A. Wolf", "authors": "Felix A. Wolf and Malte Schwerhoff and Peter M\\\"uller", "title": "Concise Outlines for a Complex Logic: A Proof Outline Checker for TaDA\n  (Full Paper)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern separation logics allow one to prove rich properties of intricate\ncode, e.g. functional correctness and linearizability of non-blocking\nconcurrent code. However, this expressiveness leads to a complexity that makes\nthese logics difficult to apply. Manual proofs or proofs in interactive theorem\nprovers consist of a large number of steps, often with subtle side conditions.\nOn the other hand, automation with dedicated verifiers typically requires\nsophisticated proof search algorithms that are specific to the given program\nlogic, resulting in limited tool support that makes it difficult to experiment\nwith program logics, e.g. when learning, improving, or comparing them. Proof\noutline checkers fill this gap. Their input is a program annotated with the\nmost essential proof steps, just like the proof outlines typically presented in\npapers. The tool then checks automatically that this outline represents a valid\nproof in the program logic. In this paper, we systematically develop a proof\noutline checker for the TaDA logic, which reduces the checking to a simpler\nverification problem, for which automated tools exist. Our approach leads to\nproof outline checkers that provide substantially more automation than\ninteractive provers, but are much simpler to develop than custom automatic\nverifiers.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:35:53 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 17:20:41 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Wolf", "Felix A.", ""], ["Schwerhoff", "Malte", ""], ["M\u00fcller", "Peter", ""]]}, {"id": "2010.07516", "submitter": "Julia Belyakova", "authors": "Julia Belyakova, Benjamin Chung, Jack Gelinas, Jameson Nash, Ross\n  Tate, Jan Vitek", "title": "World Age in Julia: Optimizing Method Dispatch in the Presence of Eval\n  (Extended Version)", "comments": "OOPSLA 2020 (Extended version with proofs)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic programming languages face semantic and performance challenges in the\npresence of features, such as eval, that can inject new code into a running\nprogram. The Julia programming language introduces the novel concept of world\nage to insulate optimized code from one of the most disruptive side-effects of\neval: changes to the definition of an existing function. This paper provides\nthe first formal semantics of world age in a core calculus named Juliette, and\nshows how world age enables compiler optimizations, such as inlining, in the\npresence of eval. While Julia also provides programmers with the means to\nbypass world age, we found that this mechanism is not used extensively: a\nstatic analysis of over 4,000 registered Julia packages shows that only 4-9% of\npackages bypass world age. This suggests that Julia's semantics aligns with\nprogrammer expectations.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 04:43:45 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 03:25:23 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Belyakova", "Julia", ""], ["Chung", "Benjamin", ""], ["Gelinas", "Jack", ""], ["Nash", "Jameson", ""], ["Tate", "Ross", ""], ["Vitek", "Jan", ""]]}, {"id": "2010.07763", "submitter": "Ranjit Jhala", "authors": "Ranjit Jhala, Niki Vazou", "title": "Refinement Types: A Tutorial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Refinement types enrich a language's type system with logical predicates that\ncircumscribe the set of values described by the type, thereby providing\nsoftware developers a tunable knob with which to inform the type system about\nwhat invariants and correctness properties should be checked on their code. In\nthis article, we distill the ideas developed in the substantial literature on\nrefinement types into a unified tutorial that explains the key ingredients of\nmodern refinement type systems. In particular, we show how to implement a\nrefinement type checker via a progression of languages that incrementally add\nfeatures to the language or type system.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 14:05:27 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Jhala", "Ranjit", ""], ["Vazou", "Niki", ""]]}, {"id": "2010.07800", "submitter": "Tobias Reinhard", "authors": "Tobias Reinhard, Amin Timany, Bart Jacobs", "title": "A Separation Logic to Verify Termination of Busy-Waiting for Abrupt\n  Program Exit", "comments": "7 pages, 11 figures, accepted at FTfJP 2020, corresponding technical\n  report: arXiv:2007.10215", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programs for multiprocessor machines commonly perform busy-waiting for\nsynchronisation. In this paper, we make a first step towards proving\ntermination of such programs. We approximate (i) arbitrary waitable events by\nabrupt program termination and (ii) busy-waiting for events by busy-waiting to\nbe abruptly terminated.\n  We propose a separation logic for modularly verifying termination (under fair\nscheduling) of programs where some threads eventually abruptly terminate the\nprogram, and other threads busy-wait for this to happen.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 08:37:41 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 06:57:40 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Reinhard", "Tobias", ""], ["Timany", "Amin", ""], ["Jacobs", "Bart", ""]]}, {"id": "2010.07874", "submitter": "Peter Belc\\'ak", "authors": "Peter Belcak", "title": "The LL(finite) strategy for optimal LL(k) parsing", "comments": "An error was found in one of the algorithms for weak LL(k) grammars", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The LL(finite) parsing strategy for parsing of LL(k) grammars where k needs\nnot to be known is presented. The strategy parses input in linear time, uses\narbitrary but always minimal lookahead necessary to disambiguate between\nalternatives of nonterminals, and it is optimal in the number of lookahead\nterminal scans performed. Modifications to the algorithm are shown that allow\nfor resolution of grammar ambiguities by precedence -- effectively interpreting\nthe input as a parsing expression grammar -- as well as for the use of\npredicates, and a proof of concept, the open-source parser generator Astir,\nemploys the LL(finite) strategy in the output it generates.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 16:52:29 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 10:22:50 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Belcak", "Peter", ""]]}, {"id": "2010.08040", "submitter": "Xingfu Wu", "authors": "Xingfu Wu, Michael Kruse, Prasanna Balaprakash, Hal Finkel, Paul\n  Hovland, Valerie Taylor, Mary Hall", "title": "Autotuning PolyBench Benchmarks with LLVM Clang/Polly Loop Optimization\n  Pragmas Using Bayesian Optimization", "comments": "to be published in the 11th International Workshop on Performance\n  Modeling, Benchmarking and Simulation of High Performance Computer Systems\n  (PMBS20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An autotuning is an approach that explores a search space of possible\nimplementations/configurations of a kernel or an application by selecting and\nevaluating a subset of implementations/configurations on a target platform\nand/or use models to identify a high performance implementation/configuration.\nIn this paper, we develop an autotuning framework that leverages Bayesian\noptimization to explore the parameter space search. We select six of the most\ncomplex benchmarks from the application domains of the PolyBench benchmarks\n(syr2k, 3mm, heat-3d, lu, covariance, and Floyd-Warshall) and apply the newly\ndeveloped LLVM Clang/Polly loop optimization pragmas to the benchmarks to\noptimize them. We then use the autotuning framework to optimize the pragma\nparameters to improve their performance. The experimental results show that our\nautotuning approach outperforms the other compiling methods to provide the\nsmallest execution time for the benchmarks syr2k, 3mm, heat-3d, lu, and\ncovariance with two large datasets in 200 code evaluations for effectively\nsearching the parameter spaces with up to 170,368 different configurations. We\ncompare four different supervised learning methods within Bayesian optimization\nand evaluate their effectiveness. We find that the Floyd-Warshall benchmark did\nnot benefit from autotuning because Polly uses heuristics to optimize the\nbenchmark to make it run much slower. To cope with this issue, we provide some\ncompiler option solutions to improve the performance.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 22:09:42 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Wu", "Xingfu", ""], ["Kruse", "Michael", ""], ["Balaprakash", "Prasanna", ""], ["Finkel", "Hal", ""], ["Hovland", "Paul", ""], ["Taylor", "Valerie", ""], ["Hall", "Mary", ""]]}, {"id": "2010.08051", "submitter": "Ruben Martins", "authors": "Joshua Clune, Vijay Ramamurthy, Ruben Martins, Umut A. Acar", "title": "Program Equivalence for Assisted Grading of Functional Programs\n  (Extended Version)", "comments": null, "journal-ref": null, "doi": "10.1145/3428239", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In courses that involve programming assignments, giving meaningful feedback\nto students is an important challenge. Human beings can give useful feedback by\nmanually grading the programs but this is a time-consuming, labor intensive,\nand usually boring process. Automatic graders can be fast and scale well but\nthey usually provide poor feedback. Although there has been research on\nimproving automatic graders, research on scaling and improving human grading is\nlimited.\n  We propose to scale human grading by augmenting the manual grading process\nwith an equivalence algorithm that can identify the equivalences between\nstudent submissions. This enables human graders to give targeted feedback for\nmultiple student submissions at once. Our technique is conservative in two\naspects. First, it identifies equivalence between submissions that are\nalgorithmically similar, e.g., it cannot identify the equivalence between\nquicksort and mergesort. Second, it uses formal methods instead of clustering\nalgorithms from the machine learning literature. This allows us to prove a\nsoundness result that guarantees that submissions will never be clustered\ntogether in error. Despite only reporting equivalence when there is algorithmic\nsimilarity and the ability to formally prove equivalence, we show that our\ntechnique can significantly reduce grading time for thousands of programming\nsubmissions from an introductory functional programming course.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 22:31:12 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Clune", "Joshua", ""], ["Ramamurthy", "Vijay", ""], ["Martins", "Ruben", ""], ["Acar", "Umut A.", ""]]}, {"id": "2010.08261", "submitter": "Peter Thiemann", "authors": "Hannes Saffrich and Peter Thiemann", "title": "Relating Functional and Imperative Session Types", "comments": "33 pages, insubmission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imperative session types provide an imperative interface to session-typed\ncommunication in a functional language. Compared to functional session type\nAPIs, the program structure is simpler at the surface, but typestate is\nrequired to model the current state of communication throughout.\n  Most work on session types has neglected the imperative approach. We\ndemonstrate that the functional approach subsumes previous work on imperative\nsession types by exhibiting a typing and semantics preserving translation into\na system of linear functional session types.\n  We further show that the untyped backwards translation from the functional to\nthe imperative calculus is semantics preserving. We restrict the type system of\nthe functional calculus such that the backwards translation becomes type\npreserving. Thus, we precisely capture the difference in expressiveness of the\ntwo calculi.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 09:28:57 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Saffrich", "Hannes", ""], ["Thiemann", "Peter", ""]]}, {"id": "2010.08439", "submitter": "Alexander McCaskey", "authors": "Hal Finkel, Alexander McCaskey, Tobi Popoola, Dmitry Lyakh, Johannes\n  Doerfert", "title": "Really Embedding Domain-Specific Languages into C++", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain-specific languages (DSLs) are both pervasive and powerful, but remain\ndifficult to integrate into large projects. As a result, while DSLs can bring\ndistinct advantages in performance, reliability, and maintainability, their use\noften involves trading off other good software-engineering practices. In this\npaper, we describe an extension to the Clang C++ compiler to support syntax\nplugins, and we demonstrate how this mechanism allows making use of DSLs inside\nof a C++ code base without needing to separate the DSL source code from the\nsurrounding C++ code.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 15:12:13 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Finkel", "Hal", ""], ["McCaskey", "Alexander", ""], ["Popoola", "Tobi", ""], ["Lyakh", "Dmitry", ""], ["Doerfert", "Johannes", ""]]}, {"id": "2010.08454", "submitter": "Vinod Grover", "authors": "Alexander Collins, Vinod Grover", "title": "Probabilistic Programming with CuPPL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic Programming Languages (PPLs) are a powerful tool in machine\nlearning, allowing highly expressive generative models to be expressed\nsuccinctly. They couple complex inference algorithms, implemented by the\nlanguage, with an expressive modelling language that allows a user to implement\nany computable function as the generative model. Such languages are usually\nimplemented on top of existing high level programming languages and do not make\nuse of hardware accelerators. PPLs that do make use of accelerators exist, but\nrestrict the expressivity of the language in order to do so. In this paper, we\npresent a language and toolchain that generates highly efficient code for both\nCPUs and GPUs. The language is functional in style, and the tool chain is built\non top of LLVM. Our implementation uses de-limited continuations on CPU to\nperform inference, and custom CUDA codes on GPU. We obtain significant speed\nups across a suite of PPL workloads, compared to other state of the art\napproaches on CPU. Furthermore, our compiler can also generate efficient code\nthat runs on CUDA GPUs.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 15:32:42 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Collins", "Alexander", ""], ["Grover", "Vinod", ""]]}, {"id": "2010.08599", "submitter": "Jonathan Sterling", "authors": "Jonathan Sterling and Robert Harper", "title": "Logical Relations as Types: Proof-Relevant Parametricity for Program\n  Modules", "comments": "To appear, J.ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of program modules is of interest to language designers not only\nfor its practical importance to programming, but also because it lies at the\nnexus of three fundamental concerns in language design: the phase distinction,\ncomputational effects, and type abstraction. We contribute a fresh \"synthetic\"\ntake on program modules that treats modules as the fundamental constructs, in\nwhich the usual suspects of prior module calculi (kinds, constructors, dynamic\nprograms) are rendered as derived notions in terms of a modal type-theoretic\naccount of the phase distinction. We simplify the account of type abstraction\n(embodied in the generativity of module functors) through a lax modality that\nencapsulates computational effects. Our main result is a (significant)\nproof-relevant and phase-sensitive generalization of the Reynolds abstraction\ntheorem for a calculus of program modules, based on a new kind of logical\nrelation called a parametricity structure. Parametricity structures generalize\nthe proof-irrelevant relations of classical parametricity to proof-relevant\nfamilies, where there may be non-trivial evidence witnessing the relatedness of\ntwo programs -- simplifying the metatheory of strong sums over the collection\nof types, for although there can be no \"relation classifying relations\", one\neasily accommodates a \"family classifying small families\". Using the insight\nthat logical relations/parametricity is itself a form of phase distinction\nbetween the syntactic and the semantic, we contribute a new synthetic approach\nto phase separated parametricity based on the slogan \"logical relations as\ntypes\", iterating our modal account of the phase distinction. Then, to\nconstruct a simulation between two implementations of an abstract type, one\nsimply programs a third implementation whose type component carries the\nrepresentation invariant.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 19:37:17 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 16:05:15 GMT"}, {"version": "v3", "created": "Fri, 14 May 2021 14:33:04 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Sterling", "Jonathan", ""], ["Harper", "Robert", ""]]}, {"id": "2010.08663", "submitter": "Shraddha Barke", "authors": "Shraddha Barke, Hila Peleg, Nadia Polikarpova", "title": "Just-in-Time Learning for Bottom-Up Enumerative Synthesis", "comments": "Accepted at OOPSLA 2020", "journal-ref": null, "doi": "10.1145/3428295", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A key challenge in program synthesis is the astronomical size of the search\nspace the synthesizer has to explore. In response to this challenge, recent\nwork proposed to guide synthesis using learned probabilistic models. Obtaining\nsuch a model, however, might be infeasible for a problem domain where no\nhigh-quality training data is available. In this work we introduce an\nalternative approach to guided program synthesis: instead of training a model\nahead of time we show how to bootstrap one just in time, during synthesis, by\nlearning from partial solutions encountered along the way. To make the best use\nof the model, we also propose a new program enumeration algorithm we dub guided\nbottom-up search, which extends the efficient bottom-up search with guidance\nfrom probabilistic models.\n  We implement this approach in a tool called Probe, which targets problems in\nthe popular syntax-guided synthesis (SyGuS) format. We evaluate Probe on\nbenchmarks from the literature and show that it achieves significant\nperformance gains both over unguided bottom-up search and over a\nstate-of-the-art probability-guided synthesizer, which had been trained on a\ncorpus of existing solutions. Moreover, we show that these performance gains do\nnot come at the cost of solution quality: programs generated by Probe are only\nslightly more verbose than the shortest solutions and perform no unnecessary\ncase-splitting.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 23:28:02 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Barke", "Shraddha", ""], ["Peleg", "Hila", ""], ["Polikarpova", "Nadia", ""]]}, {"id": "2010.08802", "submitter": "Jos\\'e-Miguel P\\'erez-\\'Alvarez", "authors": "Jos\\'e Miguel P\\'erez-\\'Alvarez, Adrian Mos", "title": "Modeling Support for Domain-Specific Application Definition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the modeling support infrastructure for\ndomain-specific application definition. This consists of a set of meta-models\nand the associated generators to allow the definition of reusable and\ndomain-specific behavior blocks, which can later be used to compose complex\nbehaviors. In addition we also present the related visual languages that\nfacilitate the creation of these models.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 14:41:24 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["P\u00e9rez-\u00c1lvarez", "Jos\u00e9 Miguel", ""], ["Mos", "Adrian", ""]]}, {"id": "2010.08886", "submitter": "Kinjal Shah", "authors": "Sourabh Kulkarni, Kinjal Divesh Shah, Nimar Arora, Xiaoyan Wang, Yucen\n  Lily Li, Nazanin Khosravani Tehrani, Michael Tingley, David Noursi, Narjes\n  Torabi, Sepehr Akhavan Masouleh, Eric Lippert, and Erik Meijer", "title": "PPL Bench: Evaluation Framework For Probabilistic Programming Languages", "comments": "6 pages, PROBPROG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce PPL Bench, a new benchmark for evaluating Probabilistic\nProgramming Languages (PPLs) on a variety of statistical models. The benchmark\nincludes data generation and evaluation code for a number of models as well as\nimplementations in some common PPLs. All of the benchmark code and PPL\nimplementations are available on Github. We welcome contributions of new models\nand PPLs and as well as improvements in existing PPL implementations. The\npurpose of the benchmark is two-fold. First, we want researchers as well as\nconference reviewers to be able to evaluate improvements in PPLs in a\nstandardized setting. Second, we want end users to be able to pick the PPL that\nis most suited for their modeling application. In particular, we are interested\nin evaluating the accuracy and speed of convergence of the inferred posterior.\nEach PPL only needs to provide posterior samples given a model and observation\ndata. The framework automatically computes and plots growth in predictive\nlog-likelihood on held out data in addition to reporting other common metrics\nsuch as effective sample size and $\\hat{r}$.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 23:12:23 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Kulkarni", "Sourabh", ""], ["Shah", "Kinjal Divesh", ""], ["Arora", "Nimar", ""], ["Wang", "Xiaoyan", ""], ["Li", "Yucen Lily", ""], ["Tehrani", "Nazanin Khosravani", ""], ["Tingley", "Michael", ""], ["Noursi", "David", ""], ["Torabi", "Narjes", ""], ["Masouleh", "Sepehr Akhavan", ""], ["Lippert", "Eric", ""], ["Meijer", "Erik", ""]]}, {"id": "2010.09073", "submitter": "Yizhou Zhang", "authors": "Yizhou Zhang and Guido Salvaneschi and Andrew C. Myers", "title": "Handling Bidirectional Control Flow: Technical Report", "comments": "Technical report of paper \"Handling Bidirectional Control Flow\"\n  (OOPSLA 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Pressed by the difficulty of writing asynchronous, event-driven code,\nmainstream languages have recently been building in support for a variety of\nadvanced control-flow features. Meanwhile, experimental language designs have\nsuggested effect handlers as a unifying solution to programmer-defined control\neffects, subsuming exceptions, generators, and async--await. Despite these\ntrends, complex control flow---in particular, control flow exhibiting a\nbidirectional pattern---remains challenging to manage.\n  We introduce bidirectional algebraic effects, a new programming abstraction\nthat supports bidirectional control transfer in a more natural way. Handlers of\nbidirectional effects can raise further effects to transfer control back to the\nsite where the initiating effect was raised, and can use themselves to handle\ntheir own effects. We present applications of this expressive power, which\nfalls out naturally as we push toward the unification of effectful programming\nwith object-oriented programming. We pin down the mechanism and the unification\nformally using a core language that generalizes to effect operations and effect\nhandlers.\n  The usual propagation semantics of control effects such as exceptions\nconflicts with modular reasoning in the presence of effect polymorphism---it\nbreaks parametricity. Bidirectionality exacerbates the problem. Hence, we set\nout to show the core language, which builds on the existing tunneling semantics\nfor algebraic effects, is not only type-safe (no effects go unhandled), but\nalso abstraction-safe (no effects are accidentally handled). We devise a\nstep-indexed logical-relations model, and construct its parametricity and\nsoundness proofs. These core results are fully mechanized in Coq. Preliminary\nexperiments show that as a first-class language feature, bidirectional handlers\ncan be implemented efficiently.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 19:28:51 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zhang", "Yizhou", ""], ["Salvaneschi", "Guido", ""], ["Myers", "Andrew C.", ""]]}, {"id": "2010.09647", "submitter": "Alexey Radul", "authors": "Alexey Radul and Boris Alexeev", "title": "The Base Measure Problem and its Solution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic programming systems generally compute with probability density\nfunctions, leaving the base measure of each such function implicit. This mostly\nworks, but creates problems when densities with respect to different base\nmeasures are accidentally combined or compared. Mistakes also happen when\ncomputing volume corrections for continuous changes of variables, which in\ngeneral depend on the support measure. We motivate and clarify the problem in\nthe context of a composable library of probability distributions and bijective\ntransformations. We solve the problem by standardizing on Hausdorff measure as\na base, and deriving formulas for comparing and combining mixed-dimension\ndensities, as well as updating densities with respect to Hausdorff measure\nunder diffeomorphic transformations. We also propose a software architecture\nthat implements these formulas efficiently in the common case. We hope that by\nadopting our solution, probabilistic programming systems can become more robust\nand general, and make a broader class of models accessible to practitioners.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 19:58:50 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 19:14:05 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Radul", "Alexey", ""], ["Alexeev", "Boris", ""]]}, {"id": "2010.09652", "submitter": "Kostas Ferles", "authors": "Kostas Ferles, Jon Stephens, Isil Dillig", "title": "Verifying Correct Usage of Context-Free API Protocols (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several real-world libraries (e.g., reentrant locks, GUI frameworks,\nserialization libraries) require their clients to use the provided API in a\nmanner that conforms to a context-free specification. Motivated by this\nobservation, this paper describes a new technique for verifying the correct\nusage of context-free API protocols. The key idea underlying our technique is\nto over-approximate the program's feasible API call sequences using a\ncontext-free grammar (CFG) and then check language inclusion between this\ngrammar and the specification. However, since this inclusion check may fail due\nto imprecision in the program's CFG abstraction, we propose a novel refinement\ntechnique to progressively improve the CFG. In particular, our method obtains\ncounterexamples from CFG inclusion queries and uses them to introduce new\nnon-terminals and productions to the grammar while still over-approximating the\nprogram's relevant behavior.\n  We have implemented the proposed algorithm in a tool called CFPChecker and\nevaluate it on 10 popular Java applications that use at least one API with a\ncontext-free specification. Our evaluation shows that CFPChecker is able to\nverify correct usage of the API in clients that use it correctly and produces\ncounterexamples for those that do not. We also compare our method against three\nrelevant baselines and demonstrate that CFPChecker enables verification of\nsafety properties that are beyond the reach of existing tools.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 16:48:54 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Ferles", "Kostas", ""], ["Stephens", "Jon", ""], ["Dillig", "Isil", ""]]}, {"id": "2010.09803", "submitter": "Jie Zhao", "authors": "Jie Zhao, Huan Sun", "title": "Adversarial Training for Code Retrieval with Question-Description\n  Relevance Regularization", "comments": "Accepted to Findings of EMNLP 2020. 11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Code retrieval is a key task aiming to match natural and programming\nlanguages. In this work, we propose adversarial learning for code retrieval,\nthat is regularized by question-description relevance. First, we adapt a simple\nadversarial learning technique to generate difficult code snippets given the\ninput question, which can help the learning of code retrieval that faces\nbi-modal and data-scarce challenges. Second, we propose to leverage\nquestion-description relevance to regularize adversarial learning, such that a\ngenerated code snippet should contribute more to the code retrieval training\nloss, only if its paired natural language description is predicted to be less\nrelevant to the user given question. Experiments on large-scale code retrieval\ndatasets of two programming languages show that our adversarial learning method\nis able to improve the performance of state-of-the-art models. Moreover, using\nan additional duplicate question prediction model to regularize adversarial\nlearning further improves the performance, and this is more effective than\nusing the duplicated questions in strong multi-task learning baselines\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 19:32:03 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 05:49:02 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Zhao", "Jie", ""], ["Sun", "Huan", ""]]}, {"id": "2010.10296", "submitter": "Yutaka Nagashima", "authors": "Yutaka Nagashima", "title": "SeLFiE: Modular Semantic Reasoning for Induction in Isabelle/HOL", "comments": "under review at the 23rd International Symposium on Practical Aspects\n  of Declarative Languages (PADL) 2021. arXiv admin note: substantial text\n  overlap with arXiv:2009.09215", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proof assistants offer tactics to apply proof by induction, but these tactics\nrely on inputs given by human engineers. We address this problem withSeLFiE, a\ndomain-specific language to encode experienced users' expertise on how to apply\nthe induct tactic in Isabelle/HOL: when we apply an induction heuristic written\nin SeLFiE to an inductive problem and arguments to the induct tactic, the\nSeLFiE interpreter examines both the syntactic structure of the problem and\nsemantics of the relevant constants to judge whether the arguments to the\ninduct tactic are plausible for that problem according to the heuristic. SeLFiE\nfacilitates the intricate interaction between syntactic and semantic analyses\nusing semantic constructs while maintaining the modularity of each analysis.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 09:05:09 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Nagashima", "Yutaka", ""]]}, {"id": "2010.10503", "submitter": "Matthew Francis-Landau", "authors": "Matthew Francis-Landau and Tim Vieira and Jason Eisner", "title": "Evaluation of Logic Programs with Built-Ins and Aggregation: A Calculus\n  for Bag Relations", "comments": "An earlier version of this paper appeared at WRLA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a scheme for translating logic programs, which may use aggregation\nand arithmetic, into algebraic expressions that denote bag relations over\nground terms of the Herbrand universe. To evaluate queries against these\nrelations, we develop an operational semantics based on term rewriting of the\nalgebraic expressions. This approach can exploit arithmetic identities and\nrecovers a range of useful strategies, including lazy strategies that defer\nwork until it becomes possible or necessary.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 17:55:36 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Francis-Landau", "Matthew", ""], ["Vieira", "Tim", ""], ["Eisner", "Jason", ""]]}, {"id": "2010.10859", "submitter": "Marco Patrignani", "authors": "Marco Patrignani, Eric Mark Martin, Dominique Devriese", "title": "On the Semantic Expressiveness of Recursive Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive types extend the simply-typed lambda calculus (STLC) with the\nadditional expressive power to enable diverging computation and to encode\nrecursive data-types (e.g., lists). Two formulations of recursive types exist:\niso-recursive and equi-recursive. The relative advantages of iso- and\nequi-recursion are well-studied when it comes to their impact on\ntype-inference. However, the relative semantic expressiveness of the two\nformulations remains unclear so far. This paper studies the semantic\nexpressiveness of STLC with iso- and equi-recursive types, proving that these\nformulations are equally expressive. In fact, we prove that they are both as\nexpressive as STLC with only term-level recursion. We phrase these\nequi-expressiveness results in terms of full abstraction of three canonical\ncompilers between these three languages (STLC with iso-, with equi-recursive\ntypes and with term-level recursion). Our choice of languages allows us to\nstudy expressiveness when interacting over both a simply-typed and a\nrecursively-typed interface. The three proofs all rely on a typed version of a\nproof technique called approximate backtranslation. Together, our results show\nthat there is no difference in semantic expressiveness between STLCs with iso-\nand equi-recursive types. In this paper, we focus on a simply-typed setting but\nwe believe our results scale to more powerful type systems like System F.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 09:35:50 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Patrignani", "Marco", ""], ["Martin", "Eric Mark", ""], ["Devriese", "Dominique", ""]]}, {"id": "2010.11342", "submitter": "David Darais", "authors": "Mat\\'ias Toro, David Darais, Chike Abuah, Joe Near, Federico Olmedo,\n  \\'Eric Tanter", "title": "Contextual Linear Types for Differential Privacy", "comments": "Journal submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language support for differentially-private programming is both crucial and\ndelicate. While elaborate program logics can be very expressive, type-system\nbased approaches using linear types tend to be more lightweight and amenable to\nautomatic checking and inference, and in particular in the presence of\nhigher-order programming. Since the seminal design of Fuzz, which is restricted\nto ${\\epsilon}$-differential privacy, a lot of effort has been made to support\nmore advanced variants of differential privacy, like\n$({\\epsilon},{\\delta})$-differential privacy. However, no existing type system\nsupports these advanced privacy variants while also supporting higher-order\nprogramming in full generality. We present Jazz, a language and type system\nwhich uses linear types and latent contextual effects to support both advanced\nvariants of differential privacy and higher order programming . Even when\navoiding advanced variants and higher order programming, our system achieves\nhigher precision than prior work for a large class of programming patterns. We\nformalize the core of the Jazz language, prove it sound for privacy via a\nlogical relation for metric preservation, and illustrate its expressive power\nthrough a number of case studies drawn from the recent differential privacy\nliterature.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 23:01:50 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Toro", "Mat\u00edas", ""], ["Darais", "David", ""], ["Abuah", "Chike", ""], ["Near", "Joe", ""], ["Olmedo", "Federico", ""], ["Tanter", "\u00c9ric", ""]]}, {"id": "2010.11605", "submitter": "Jens Oliver Gutsfeld", "authors": "Jens Oliver Gutsfeld, Markus M\\\"uller-Olm and Christoph Ohrem", "title": "Automata and Fixpoints for Asynchronous Hyperproperties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperproperties have received increasing attention in the last decade due to\ntheir importance e.g. for security analyses. Past approaches have focussed on\nsynchronous analyses, i.e. techniques in which different paths are compared\nlockstepwise. In this paper, we systematically study asynchronous analyses for\nhyperproperties by introducing both a novel automata model (Alternating\nAsynchronous Parity Automata) and the temporal fixpoint calculus $\\Hmu$, the\nfirst fixpoint calculus that can systematically express hyperproperties in an\nasynchronous manner and at the same time subsumes the existing logic HyperLTL.\nWe show that the expressive power of both models coincides over fixed path\nassignments. The high expressive power of both models is evidenced by the fact\nthat decision problems of interest are highly undecidable, i.e. not even\narithmetical. As a remedy, we propose approximative analyses for both models\nthat also induce natural decidable fragments.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 11:06:16 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Gutsfeld", "Jens Oliver", ""], ["M\u00fcller-Olm", "Markus", ""], ["Ohrem", "Christoph", ""]]}, {"id": "2010.11762", "submitter": "Tobias Reinhard", "authors": "Tobias Reinhard, Bart Jacobs", "title": "Ghost Signals: Verifying Termination of Busy-Waiting (Extended Version)", "comments": "68 pages; 42 figures; Simplified logic by removing permissions and\n  updated soundness proof in appendix. Simplified the logic's presentation.\n  Added case studies to appendix.; This is the extended version of a paper\n  which is to be published at CAV21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programs for multiprocessor machines commonly perform busy waiting for\nsynchronization. We propose the first separation logic for modularly verifying\ntermination of such programs under fair scheduling. Our logic requires the\nproof author to associate a ghost signal with each busy-waiting loop and allows\nsuch loops to iterate while their corresponding signal $s$ is not set. The\nproof author further has to define a well-founded order on signals and to prove\nthat if the looping thread holds an obligation to set a signal $s^\\prime$, then\n$s^\\prime$ is ordered above $s$. By using conventional shared state invariants\nto associate the state of ghost signals with the state of data structures,\nprograms busy-waiting for arbitrary conditions over arbitrary data structures\ncan be verified.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 14:33:35 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 10:28:18 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Reinhard", "Tobias", ""], ["Jacobs", "Bart", ""]]}, {"id": "2010.11887", "submitter": "Maria I. Gorinova", "authors": "Maria I. Gorinova, Andrew D. Gordon, Charles Sutton, Matthijs Vakar", "title": "Conditional independence by typing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A central goal of probabilistic programming languages (PPLs) is to separate\nmodelling from inference. However, this goal is hard to achieve in practice.\nUsers are often forced to re-write their models in order to improve efficiency\nof inference or meet restrictions imposed by the PPL. Conditional independence\n(CI) relationships among parameters are a crucial aspect of probabilistic\nmodels that captures a qualitative summary of the specified model and can\nfacilitate more efficient inference.\n  We present an information flow type system for probabilistic programming that\ncaptures conditional independence (CI) relationships, and show that, for a\nwell-typed program in our system, the distribution it implements is guaranteed\nto have certain CI-relationships. Further, by using type inference, we can\nstatically \\emph{deduce} which CI-properties are present in a specified model.\n  As a practical application, we consider the problem of how to perform\ninference on models with mixed discrete and continuous parameters. Inference on\nsuch models is challenging in many existing PPLs, but can be improved through a\nworkaround, where the discrete parameters are used \\textit{implicitly}, at the\nexpense of manual model re-writing. We present a source-to-source\nsemantics-preserving transformation, which uses our CI-type system to automate\nthis workaround by eliminating the discrete parameters from a probabilistic\nprogram. The resulting program can be seen as a hybrid inference algorithm on\nthe original program, where continuous parameters can be drawn using efficient\ngradient-based inference methods, while the discrete parameters are drawn using\nvariable elimination.\n  We implement our CI-type system and its example application in SlicStan: a\ncompositional variant of Stan.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:27:22 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Gorinova", "Maria I.", ""], ["Gordon", "Andrew D.", ""], ["Sutton", "Charles", ""], ["Vakar", "Matthijs", ""]]}, {"id": "2010.11999", "submitter": "Martin Kong", "authors": "Martin Kong", "title": "Exploring the Impact of Affine Loop Transformations in Qubit Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.ET cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most quantum compiler transformations and qubit allocation techniques to date\nare either peep-hole focused or rely on sliding windows that depend on a number\nof external parameters. Thus, global optimization criteria are still lacking.\nIn this paper we explore the synergies and impact of affine loop\ntransformations in the context of qubit allocation and mapping. With this goal\nin mind, we have implemented a domain specific language and source-to-source\ncompiler for quantum circuits that can be directly described with affine\nrelations. We conduct an extensive evaluation spanning 8 quantum circuits taken\nfrom the literature, 3 distinct coupling graphs, 4 affine transformations\n(including the Pluto dependence distance minimization and Feautrier's minimum\nlatency algorithms), and 4 qubit allocators. Our results demonstrate that\naffine transformations using global optimization criteria can cooperate\neffectively in several scenarios with quantum qubit mapping algorithms to\nreduce the circuit depth, size and allocation time.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 19:23:02 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Kong", "Martin", ""]]}, {"id": "2010.12071", "submitter": "David Chiang", "authors": "David Chiang and Chung-chieh Shan", "title": "Translating Recursive Probabilistic Programs to Factor Graph Grammars", "comments": "Extended abstract of presentation at PROBPROG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is natural for probabilistic programs to use conditionals to express\nalternative substructures in models, and loops (recursion) to express repeated\nsubstructures in models. Thus, probabilistic programs with conditionals and\nrecursion motivate ongoing interest in efficient and general inference. A\nfactor graph grammar (FGG) generates a set of factor graphs that do not all\nneed to be enumerated in order to perform inference. We provide a\nsemantics-preserving translation from first-order probabilistic programs with\nconditionals and recursion to FGGs.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 21:17:04 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Chiang", "David", ""], ["Shan", "Chung-chieh", ""]]}, {"id": "2010.12338", "submitter": "Christian Graulund", "authors": "Christian Uldal Graulund, Dmitrij Szamozvancev, Neel Krishnaswami", "title": "Adjoint Reactive GUI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most interaction with a computer is done via a graphical user interface.\nTraditionally, these are implemented in an imperative fashion using shared\nmutable state and callbacks. This is efficient, but is also difficult to reason\nabout and error prone. Functional Reactive Programming (FRP) provides an\nelegant alternative which allows GUIs to be designed in a declarative fashion.\nHowever, most FRP languages are synchronous and continually check for new data.\nThis means that an FRP-style GUI will \"wake up\" on each program cycle. This is\nproblematic for applications like text editors and browsers, where often\nnothing happens for extended periods of time, and we want the implementation to\nsleep until new data arrives. In this paper, we present an asynchronous FRP\nlanguage for designing GUIs called $\\lambda_{\\mathsf{Widget}}$. Our language\nprovides a novel semantics for widgets, the building block of GUIs, which\noffers both a natural Curry--Howard logical interpretation and an efficient\nimplementation strategy.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 12:20:22 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 09:22:04 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Graulund", "Christian Uldal", ""], ["Szamozvancev", "Dmitrij", ""], ["Krishnaswami", "Neel", ""]]}, {"id": "2010.12450", "submitter": "Nariyoshi Chida", "authors": "Nariyoshi Chida and Tachio Terauchi", "title": "Automatic Repair of Vulnerable Regular Expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A regular expression is called vulnerable if there exist input strings on\nwhich the usual backtracking-based matching algorithm runs super linear time.\nSoftware containing vulnerable regular expressions are prone to\nalgorithmic-complexity denial of service attack in which the malicious user\nprovides input strings exhibiting the bad behavior. Due to the prevalence of\nregular expressions in modern software, vulnerable regular expressions are\nserious threat to software security. While there has been prior work on\ndetecting vulnerable regular expressions, in this paper, we present a first\nstep toward repairing a possibly vulnerable regular expression. Importantly,\nour method handles real world regular expressions containing extended features\nsuch as lookarounds, capturing groups, and backreferencing. (The problem is\nactually trivial without such extensions since any pure regular expression can\nbe made invulnerable via a DFA conversion.) We build our approach on the recent\nwork on example-based repair of regular expressions by Pan et al. [Pan et al.\n2019] which synthesizes a regular expression that is syntactically close to the\noriginal one and correctly classifies the given set of positive and negative\nexamples. The key new idea is the use of linear-time constraints, which\ndisambiguate a regular expression and ensure linear time matching. We generate\nthe constraints using an extended nondeterministic finite automaton that\nsupports the extended features in real-world regular expressions. While our\nmethod is not guaranteed to produce a semantically equivalent regular\nexpressions, we empirically show that the repaired regular expressions tend to\nbe nearly indistinguishable from the original ones.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 14:45:18 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Chida", "Nariyoshi", ""], ["Terauchi", "Tachio", ""]]}, {"id": "2010.12686", "submitter": "Frantisek Farka", "authors": "Franti\\v{s}ek Farka, Aleksandar Nanevski, Anindya Banerjee, Germ\\'an\n  Andr\\'es Delbianco, Ignacio F\\'abregas", "title": "On Algebraic Abstractions for Concurrent Separation Logics", "comments": "35 pages", "journal-ref": "Proc. ACM Program. Lang. 5, POPL, Article 5 (January 2021)", "doi": "10.1145/3434286", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concurrent separation logic is distinguished by transfer of state ownership\nupon parallel composition and framing. The algebraic structure that underpins\nownership transfer is that of partial commutative monoids (PCMs). Extant\nresearch considers ownership transfer primarily from the logical perspective\nwhile comparatively less attention is drawn to the algebraic considerations.\nThis paper provides an algebraic formalization of ownership transfer in\nconcurrent separation logic by means of structure-preserving partial functions\n(i.e., morphisms) between PCMs, and an associated notion of separating\nrelations. Morphisms of structures are a standard concept in algebra and\ncategory theory, but haven't seen ubiquitous use in separation logic before.\nSeparating relations are binary relations that generalize disjointness and\ncharacterize the inputs on which morphisms preserve structure. The two\nabstractions facilitate verification by enabling concise ways of writing specs,\nby providing abstract views of threads' states that are preserved under\nownership transfer, and by enabling user-level construction of new PCMs out of\nexisting ones.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 22:06:12 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 11:06:02 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 19:37:31 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Farka", "Franti\u0161ek", ""], ["Nanevski", "Aleksandar", ""], ["Banerjee", "Anindya", ""], ["Delbianco", "Germ\u00e1n Andr\u00e9s", ""], ["F\u00e1bregas", "Ignacio", ""]]}, {"id": "2010.12695", "submitter": "Leif Andersen", "authors": "Leif Andersen, Michael Ballantyne, Matthias Felleisen", "title": "Adding Interactive Visual Syntax to Textual Code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many programming problems call for turning geometrical thoughts into code:\ntables, hierarchical structures, nests of objects, trees, forests, graphs, and\nso on. Linear text does not do justice to such thoughts. But, it has been the\ndominant programming medium for the past and will remain so for the foreseeable\nfuture.\n  This paper proposes a novel mechanism for conveniently extending textual\nprogramming languages with problem-specific visual syntax. It argues the\nnecessity of this language feature, demonstrates the feasibility with a robust\nprototype, and sketches a design plan for adapting the idea to other languages.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 22:42:54 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Andersen", "Leif", ""], ["Ballantyne", "Michael", ""], ["Felleisen", "Matthias", ""]]}, {"id": "2010.12988", "submitter": "Gabriele Vanoni", "authors": "Beniamino Accattoli and Ugo Dal Lago and Gabriele Vanoni", "title": "The (In)Efficiency of Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating higher-order functional programs through abstract machines\ninspired by the geometry of the interaction is known to induce $\\textit{space}$\nefficiencies, the price being $\\textit{time}$ performances often poorer than\nthose obtainable with traditional, environment-based, abstract machines.\nAlthough families of lambda-terms for which the former is exponentially less\nefficient than the latter do exist, it is currently unknown how \\emph{general}\nthis phenomenon is, and how far the inefficiencies can go, in the worst case.\nWe answer these questions formulating four different well-known abstract\nmachines inside a common definitional framework, this way being able to give\nsharp results about the relative time efficiencies. We also prove that\nnon-idempotent intersection type theories are able to precisely reflect the\ntime performances of the interactive abstract machine, this way showing that\nits time-inefficiency ultimately descends from the presence of higher-order\ntypes.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 21:11:21 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Accattoli", "Beniamino", ""], ["Lago", "Ugo Dal", ""], ["Vanoni", "Gabriele", ""]]}, {"id": "2010.13191", "submitter": "Ethan Cecchetti", "authors": "Andrew K. Hirsch and Ethan Cecchetti", "title": "Giving Semantics to Program-Counter Labels via Secure Effects", "comments": null, "journal-ref": "Proceedings of the ACM on Programming Languages 5, POPL, Article\n  35 (January 2021)", "doi": "10.1145/3434316", "report-no": null, "categories": "cs.PL cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Type systems designed for information-flow control commonly use a\nprogram-counter label to track the sensitivity of the context and rule out data\nleakage arising from effectful computation in a sensitive context. Currently,\ntype-system designers reason about this label informally except in security\nproofs, where they use ad-hoc techniques. We develop a framework based on\nmonadic semantics for effects to give semantics to program-counter labels. This\nframework leads to three results about program-counter labels. First, we\ndevelop a new proof technique for noninterference, the core security theorem\nfor information-flow control in effectful languages. Second, we unify notions\nof security for different types of effects, including state, exceptions, and\nnontermination. Finally, we formalize the folklore that program-counter labels\nare a lower bound on effects. We show that, while not universally true, this\nfolklore has a good semantic foundation.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 19:02:35 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 18:07:30 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Hirsch", "Andrew K.", ""], ["Cecchetti", "Ethan", ""]]}, {"id": "2010.13593", "submitter": "Artem Khyzha", "authors": "Artem Khyzha and Ori Lahav", "title": "Taming x86-TSO Persistency (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the formal semantics of non-volatile memory in the x86-TSO\narchitecture. We show that while the explicit persist operations in the recent\nmodel of Raad et al. from POPL'20 only enforce order between writes to the\nnon-volatile memory, it is equivalent, in terms of reachable states, to a model\nwhose explicit persist operations mandate that prior writes are actually\nwritten to the non-volatile memory. The latter provides a novel model that is\nmuch closer to common developers' understanding of persistency semantics. We\nfurther introduce a simpler and stronger sequentially consistent persistency\nmodel, develop a sound mapping from this model to x86, and establish a\ndata-race-freedom guarantee providing programmers with a safe programming\ndiscipline. Our operational models are accompanied with equivalent declarative\nformulations, which facilitate our formal arguments, and may prove useful for\nprogram verification under x86 persistency.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 14:00:52 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 06:53:48 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Khyzha", "Artem", ""], ["Lahav", "Ori", ""]]}, {"id": "2010.13926", "submitter": "Zesen Qian", "authors": "Zesen Qian, G. A. Kavvos, Lars Birkedal", "title": "Client-Server Sessions in Linear Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce coexponentials, a new set of modalities for Classical Linear\nLogic. As duals to exponentials, the coexponentials codify a distributed form\nof the structural rules of weakening and contraction. This makes them a\nsuitable logical device for encapsulating the pattern of a server receiving\nrequests from an arbitrary number of clients on a single channel. Guided by\nthis intuition we formulate a system of session types based on Classical Linear\nLogic with coexponentials, which is suited to modelling client-server\ninteractions. We also present a session-typed functional programming language\nfor server-client programming, which we translate to our system of\ncoexponentials.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 22:12:53 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 19:37:00 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Qian", "Zesen", ""], ["Kavvos", "G. A.", ""], ["Birkedal", "Lars", ""]]}, {"id": "2010.14032", "submitter": "Robert Sison", "authors": "Robert Sison (1 and 2 and 3), Toby Murray (1) ((1) University of\n  Melbourne, (2) CSIRO's Data61, (3) UNSW Sydney)", "title": "Verified Secure Compilation for Mixed-Sensitivity Concurrent Programs", "comments": "Submitted to the Journal of Functional Programming Special Issue on\n  Secure Compilation. Some errors in the submitted manuscript's description of\n  Lemma 5.25 have been corrected. This paper expands on its conference version\n  arXiv:1907.00713. For supplement material, see http://covern.org/jfpsc.html", "journal-ref": null, "doi": "10.1017/S0956796821000162", "report-no": null, "categories": "cs.PL cs.CR cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Proving only over source code that programs do not leak sensitive data leaves\na gap between reasoning and reality that can only be filled by accounting for\nthe behaviour of the compiler. Furthermore, software does not always have the\nluxury of limiting itself to single-threaded computation with resources\nstatically dedicated to each user to ensure the confidentiality of their data.\nThis results in mixed-sensitivity concurrent programs, which might reuse memory\nshared between their threads to hold data of different sensitivity levels at\ndifferent times; for such programs, a compiler must preserve the\nvalue-dependent coordination of such mixed-sensitivity reuse despite the impact\nof concurrency.\n  Here we demonstrate, using Isabelle/HOL, that it is feasible to verify that a\ncompiler preserves noninterference, the strictest kind of confidentiality\nproperty, for mixed-sensitivity concurrent programs. First, we present notions\nof refinement that preserve a concurrent value-dependent notion of\nnoninterference that we have designed to support such programs. As proving\nnoninterference-preserving refinement can be considerably more complex than the\nstandard refinements typically used to verify semantics -- preserving\ncompilation, our notions include a decomposition principle that separates the\nsemantics -- from the security-preservation concerns. Second, we demonstrate\nthat these refinement notions are applicable to verified secure compilation, by\nexercising them on a single-pass compiler for mixed-sensitivity concurrent\nprograms that synchronise using mutex locks, from a generic imperative language\nto a generic RISC-style assembly language. Finally, we execute our compiler on\na nontrivial mixed-sensitivity concurrent program modelling a real-world use\ncase, thus preserving its source-level noninterference properties down to an\nassembly-level model automatically.\n  (See paper for complete abstract.)\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 03:24:05 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Sison", "Robert", "", "1 and 2 and 3"], ["Murray", "Toby", ""]]}, {"id": "2010.14094", "submitter": "Felipe Ba\\~nados Schwerter", "authors": "Felipe Ba\\~nados Schwerter, Alison M. Clark, Khurram A. Jafery and\n  Ronald Garcia", "title": "Abstracting Gradual Typing Moving Forward: Precise and Space-Efficient\n  (Technical Report)", "comments": "Accepted to POPL '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstracting Gradual Typing (AGT) is a systematic approach to designing\ngradually-typed languages. Languages developed using AGT automatically satisfy\nthe formal semantic criteria for gradual languages identified by Siek et al.\n[2015]. Nonetheless, vanilla AGT semantics can still have important\nshortcomings. First, a gradual language's runtime checks should preserve the\nspace-efficiency guarantees inherent to the underlying static and dynamic\nlanguages. To the contrary, the default operational semantics of AGT break\nproper tail calls. Second, a gradual language's runtime checks should enforce\nbasic modular type-based invariants expected from the static type discipline.\nTo the contrary, the default operational semantics of AGT may fail to enforce\nsome invariants in surprising ways. We demonstrate this in the\n$\\text{GTFL}_\\lesssim$ language of Garcia et al. [2016].\n  This paper addresses both problems at once by refining the theory underlying\nAGT's dynamic checks. Garcia et al. [2016] observe that AGT involves two\nabstractions of static types: one for the static semantics and one for the\ndynamic semantics. We recast the latter as an abstract interpretation of\nsubtyping itself, while gradual types still abstract static types. Then we show\nhow forward-completeness [Giacobazzi and Quintarelli 2001] is key to supporting\nboth space-efficient execution and reliable runtime type enforcement.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 06:32:36 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 20:22:14 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Schwerter", "Felipe Ba\u00f1ados", ""], ["Clark", "Alison M.", ""], ["Jafery", "Khurram A.", ""], ["Garcia", "Ronald", ""]]}, {"id": "2010.14133", "submitter": "Nick Brown", "authors": "Nick Brown, Ludovic Capelli, J. Mark Bull", "title": "Extended abstract: Type oriented programming for task based parallelism", "comments": "Extended abstract in 2017 workshop on Type-Driven Development", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writing parallel codes is difficult and exhibits a fundamental trade-off\nbetween abstraction and performance. The high level language abstractions\ndesigned to simplify the complexities of parallelism make certain assumptions\nthat impacts performance and scalability. On the other hand lower level\nlanguages, providing many opportunities for optimisation, require in-depth\nknowledge and the programmer to consider tricky details of parallelism. An\napproach is required which can bridge the gap and provide both the ease of\nprogramming and opportunities for control and optimisation. By optionally\ndecorating their codes with additional type information, programmers can either\ndirect the compiler to make certain decisions or rely on sensible default\nchoices.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 08:49:19 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Brown", "Nick", ""], ["Capelli", "Ludovic", ""], ["Bull", "J. Mark", ""]]}, {"id": "2010.14548", "submitter": "Kevin Batz", "authors": "Kevin Batz, Benjamin Lucien Kaminski, Joost-Pieter Katoen, Christoph\n  Matheja", "title": "Relatively Complete Verification of Probabilistic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a syntax for specifying quantitative \"assertions\" - functions\nmapping program states to numbers - for probabilistic program verification. We\nprove that our syntax is expressive in the following sense: Given any\nprobabilistic program $C$, if a function $f$ is expressible in our syntax, then\nthe function mapping each initial state $\\sigma$ to the expected value of $f$\nevaluated in the final states reached after termination of $C$ on $\\sigma$\n(also called the weakest preexpectation $\\textit{wp} [C](f)$) is also\nexpressible in our syntax.\n  As a consequence, we obtain a relatively complete verification system for\nreasoning about expected values and probabilities in the sense of Cook: Apart\nfrom proving a single inequality between two functions given by syntactic\nexpressions in our language, given $f$, $g$, and $C$, we can check whether $g\n\\preceq \\textit{wp} [C] (f)$.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 18:37:46 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Batz", "Kevin", ""], ["Kaminski", "Benjamin Lucien", ""], ["Katoen", "Joost-Pieter", ""], ["Matheja", "Christoph", ""]]}, {"id": "2010.14827", "submitter": "Nick Brown", "authors": "Nick Brown", "title": "ePython: An implementation of Python for the many-core Epiphany\n  coprocessor", "comments": "Preprint of article in the 6th Workshop on Python for\n  High-Performance and Scientific Computing", "journal-ref": "In 2016 6th Workshop on Python for High-Performance and Scientific\n  Computing (PyHPC) (pp. 59-66). IEEE", "doi": "10.1109/PyHPC.2016.012", "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Epiphany is a many-core, low power, low on-chip memory architecture and\none can very cheaply gain access to a number of parallel cores which is\nbeneficial for HPC education and prototyping. The very low power nature of\nthese architectures also means that there is potential for their use in future\nHPC machines, however there is a high barrier to entry in programming them due\nto the associated complexities and immaturity of supporting tools.\n  In this paper we present our work on ePython, a subset of Python for the\nEpiphany and similar many-core co-processors. Due to the limited on-chip memory\nper core we have developed a new Python interpreter and this, combined with\nadditional support for parallelism, has meant that novices can take advantage\nof Python to very quickly write parallel codes on the Epiphany and explore\nconcepts of HPC using a smaller scale parallel machine. The high level nature\nof Python opens up new possibilities on the Epiphany, we examine a\ncomputationally intensive Gauss-Seidel code from the programmability and\nperformance perspective, discuss running Python hybrid on both the host CPU and\nEpiphany, and interoperability between a full Python interpreter on the CPU and\nePython on the Epiphany. The result of this work is support for developing\nPython on the Epiphany, which can be applied to other similar architectures,\nthat the community have already started to adopt and use to explore concepts of\nparallelism and HPC.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 09:01:27 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Brown", "Nick", ""]]}, {"id": "2010.15030", "submitter": "Jonas Kastberg Hinrichsen", "authors": "Jonas Kastberg Hinrichsen, Jesper Bengtson and Robbert Krebbers", "title": "Actris 2.0: Asynchronous Session-Type Based Reasoning in Separation\n  Logic", "comments": "52 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Message passing is a useful abstraction for implementing concurrent programs.\nFor real-world systems, however, it is often combined with other programming\nand concurrency paradigms, such as higher-order functions, mutable state,\nshared-memory concurrency, and locks. We present Actris: a logic for proving\nfunctional correctness of programs that use a combination of the aforementioned\nfeatures. Actris combines the power of modern concurrent separation logics with\na first-class protocol mechanism---based on session types---for reasoning about\nmessage passing in the presence of other concurrency paradigms. We show that\nActris provides a suitable level of abstraction by proving functional\ncorrectness of a variety of examples, including a distributed merge sort, a\ndistributed load-balancing mapper, and a variant of the map-reduce model, using\nconcise specifications. While Actris was already presented in a conference\npaper (POPL'20), this paper expands the prior presentation significantly.\nMoreover, it extends Actris to Actris 2.0 with a notion of subprotocols---based\non session-type subtyping---that permits additional flexibility when composing\nchannel endpoints, and that takes full advantage of the asynchronous semantics\nof message passing in Actris. Soundness of Actris 2.0 is proved using a model\nof its protocol mechanism in the Iris framework. We have mechanised the theory\nof Actris, together with custom tactics, as well as all examples in the paper,\nin the Coq proof assistant.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 15:06:50 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Hinrichsen", "Jonas Kastberg", ""], ["Bengtson", "Jesper", ""], ["Krebbers", "Robbert", ""]]}, {"id": "2010.15884", "submitter": "Hongbo Rong", "authors": "Hongbo Rong, Xiaochen Hao, Yun Liang, Lidong Xu, Hong H Jiang, Pradeep\n  Dubey", "title": "Systolic Computing on GPUs for Productive Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a language and compiler to productively build high-performance\n{\\it software systolic arrays} that run on GPUs. Based on a rigorous\nmathematical foundation (uniform recurrence equations and space-time\ntransform), our language has a high abstraction level and covers a wide range\nof applications. A programmer {\\it specifies} a projection of a dataflow\ncompute onto a linear systolic array, while leaving the detailed implementation\nof the projection to a compiler; the compiler implements the specified\nprojection and maps the linear systolic array to the SIMD execution units and\nvector registers of GPUs. In this way, both productivity and performance are\nachieved in the same time. This approach neatly combines loop transformations,\ndata shuffling, and vector register allocation into a single framework.\nMeanwhile, many other optimizations can be applied as well; the compiler\ncomposes the optimizations together to generate efficient code.\n  We implemented the approach on Intel GPUs. This is the first system that\nallows productive construction of systolic arrays on GPUs. We allow multiple\nprojections, arbitrary projection directions and linear schedules, which can\nexpress most, if not all, systolic arrays in practice. Experiments with 1- and\n2-D convolution on an Intel GEN9.5 GPU have demonstrated the generality of the\napproach, and its productivity in expressing various systolic designs for\nfinding the best candidate. Although our systolic arrays are purely software\nrunning on generic SIMD hardware, compared with the GPU's specialized, hardware\nsamplers that perform the same convolutions, some of our best designs are up to\n59\\% faster. Overall, this approach holds promise for productive\nhigh-performance computing on GPUs.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 18:49:54 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Rong", "Hongbo", ""], ["Hao", "Xiaochen", ""], ["Liang", "Yun", ""], ["Xu", "Lidong", ""], ["Jiang", "Hong H", ""], ["Dubey", "Pradeep", ""]]}, {"id": "2010.16016", "submitter": "EPTCS", "authors": "Walther Neuper (Johannes Kepler University Linz, Austria)", "title": "Lucas-Interpretation on Isabelle's Functions", "comments": "In Proceedings ThEdu'20, arXiv:2010.15832", "journal-ref": "EPTCS 328, 2020, pp. 79-95", "doi": "10.4204/EPTCS.328.5", "report-no": null, "categories": "cs.PL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software tools of Automated Reasoning are too sophisticated for general use\nin mathematics education and respective reasoning, while Lucas-Interpretation\nprovides a general concept for integrating such tools into educational software\nwith the purpose to reliably and flexibly check formal input of students. This\npaper gives the first technically concise description of Lucas-Interpretation\nat the occasion of migrating a prototype implementation to the function package\nof the proof assistant Isabelle. The description shows straightforward\nadaptations of Isabelle's programming language and shows, how simple migration\nof the interpreter was, since the design (before the function package has been\nintroduced to Isabelle) recognised appropriateness of Isabelle's terms as\nmiddle end. The paper gives links into the code in an open repository as\ninvitation to readers for re-using the prototyped code or adopt the general\nconcept. And since the prototype has been designed before the function package\nwas implemented, the paper is an opportunity for recording lessons learned from\nIsabelle's development of code structure.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 01:15:44 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Neuper", "Walther", "", "Johannes Kepler University Linz, Austria"]]}, {"id": "2010.16111", "submitter": "Frederic Blanqui", "authors": "Fr\\'ed\\'eric Blanqui (LSV,ENS Paris Saclay)", "title": "Type safety of rewrite rules in dependent types", "comments": null, "journal-ref": "5th International Conference on Formal Structures for Computation\n  and Deduction (FSCD 2020), Jun 2020, Paris, France. pp.14", "doi": "10.4230/LIPIcs.FSCD.2020.13", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expressiveness of dependent type theory can be extended by identifying\ntypes modulo some additional computation rules. But, for preserving the\ndecidability of type-checking or the logical consistency of the system, one\nmust make sure that those user-defined rewriting rules preserve typing. In this\npaper, we give a new method to check that property using Knuth-Bendix\ncompletion.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 08:06:21 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Blanqui", "Fr\u00e9d\u00e9ric", "", "LSV,ENS Paris Saclay"]]}, {"id": "2010.16115", "submitter": "Frederic Blanqui", "authors": "Gabriel Hondet (DEDUCTEAM, Inria, LSV, ENS Paris Saclay, CNRS),\n  Fr\\'ed\\'eric Blanqui (DEDUCTEAM, Inria, LSV, ENS Paris Saclay, CNRS)", "title": "The New Rewriting Engine of Dedukti", "comments": null, "journal-ref": "5th International Conference on Formal Structures for Computation\n  and Deduction (FSCD 2020), Jun 2020, Paris, France. pp.16", "doi": "10.4230/LIPIcs.FSCD.2020.35", "report-no": null, "categories": "cs.PL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dedukti is a type-checker for the $\\lambda$$\\Pi$-calculus modulo rewriting,\nan extension of Edinburgh's logicalframework LF where functions and type\nsymbols can be defined by rewrite rules. It thereforecontains an engine for\nrewriting LF terms and types according to the rewrite rules given by the user.A\nkey component of this engine is the matching algorithm to find which rules can\nbe fired. In thispaper, we describe the class of rewrite rules supported by\nDedukti and the new implementation ofthe matching algorithm. Dedukti supports\nnon-linear rewrite rules on terms with binders usinghigher-order\npattern-matching as in Combinatory Reduction Systems (CRS). The new\nmatchingalgorithm extends the technique of decision trees introduced by Luc\nMaranget in the OCamlcompiler to this more general context.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 08:19:19 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Hondet", "Gabriel", "", "DEDUCTEAM, Inria, LSV, ENS Paris Saclay, CNRS"], ["Blanqui", "Fr\u00e9d\u00e9ric", "", "DEDUCTEAM, Inria, LSV, ENS Paris Saclay, CNRS"]]}, {"id": "2010.16301", "submitter": "Humberto Rodriguez Avila", "authors": "Humberto Rodriguez Avila, Joeri De Koster, Wolfgang De Meuter", "title": "Advanced Join Patterns for the Actor Model based on CEP Techniques", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2021, Vol. 5,\n  Issue 2, Article 10", "doi": "10.22152/programming-journal.org/2021/5/10", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Actor-based programming languages offer many essential features for\ndeveloping modern distributed reactive systems. These systems exploit the actor\nmodel's isolation property to fulfill their performance and scalability\ndemands. Unfortunately, the reliance of the model on isolation as its most\nfundamental property requires programmers to express complex interaction\npatterns between their actors to be expressed manually in terms of complex\ncombinations of messages sent between the isolated actors.\n  Inquiry: In the last three decades, several language design proposals have\nbeen introduced to reduce the complexity that emerges from describing said\ninteraction and coordination of actors. We argue that none of these proposals\nis satisfactory in order to express the many complex interaction patterns\nbetween actors found in modern reactive distributed systems.\n  Approach: We describe seven smart home automation scenarios (in which an\nactor represents every smart home appliance) to motivate the support by actor\nlanguages for five radically different types of message synchronization\npatterns, which are lacking in modern distributed actor-based languages.\nFortunately, these five types of synchronisation patterns have been studied\nextensively by the Complex Event Processing (CEP) community. Our paper\ndescribes how such CEP patterns are elegantly added to an actor-based\nprogramming language.\n  Knowledge: Based on our findings, we propose an extension of the\nsingle-message matching paradigm of contemporary actor-based languages in order\nto support a multiple-message matching way of thinking in the same way as\nproposed by CEP languages. Our proposal thus enriches the actor-model by ways\nof declaratively describing complex message combinations to which an actor can\nrespond.\n  Grounding: We base the problem-statement of the paper on an online poll in\nthe home automation community that has motivated the real need for the\nCEP-based synchronisation operators between actors proposed in the paper.\nFurthermore, we implemented a DSL -- called Sparrow -- that supports said\noperators and we argue quantitatively (in terms of LOC and in terms of a\nreduction of the concerns that have to be handled by programmers) that the DSL\noutperforms existing approaches.\n  Importance: This work aims to provide a set of synchronization operators that\nhelp actor-based languages to handle the complex interaction required by modern\nreactive distributed systems. To the best of our knowledge, our proposal is the\nfirst one to add advanced CEP synchronization operators to the -- relatively\nsimplistic single-message based matching -- mechanisms of most actor-based\nlanguages.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 14:53:11 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Avila", "Humberto Rodriguez", ""], ["De Koster", "Joeri", ""], ["De Meuter", "Wolfgang", ""]]}, {"id": "2010.16302", "submitter": "Hsiang-Shang Ko", "authors": "Hsiang-Shang Ko", "title": "Programming Metamorphic Algorithms: An Experiment in Type-Driven\n  Algorithm Design", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2021, Vol. 5,\n  Issue 2, Article 7", "doi": "10.22152/programming-journal.org/2021/5/7", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dependently typed programming, proofs of basic, structural properties can\nbe embedded implicitly into programs and do not need to be written explicitly.\nBesides saving the effort of writing separate proofs, a most distinguishing and\nfascinating aspect of dependently typed programming is that it makes the idea\nof interactive type-driven development much more powerful, where expressive\ntype information becomes useful hints that help the programmer to complete a\nprogram. There have not been many attempts at exploiting the full potential of\nthe idea, though. As a departure from the usual properties dealt with in\ndependently typed programming, and as a demonstration that the idea of\ninteractive type-driven development has more potential to be discovered, we\nconduct an experiment in ?type-driven algorithm design?: we develop algorithms\nfrom their specifications encoded in sophisticated types, to see how useful the\nhints provided by a type-aware interactive development environment can be. The\nalgorithmic problem we choose is metamorphisms, whose definitional behaviour is\nconsuming a data structure to compute an intermediate value and then producing\na codata structure from that value, but there are other ways to compute\nmetamorphisms. We develop Gibbons?s streaming algorithm and Nakano?s jigsaw\nmodel in the interactive development environment provided by the dependently\ntyped language Agda, turning intuitive ideas about these algorithms into formal\nconditions and programs that are correct by construction.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 14:53:25 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 10:24:41 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 08:37:49 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ko", "Hsiang-Shang", ""]]}, {"id": "2010.16304", "submitter": "Michail Papadimitriou", "authors": "Michail Papadimitriou, Juan Fumero, Athanasios Stratikopoulos, Foivos\n  S. Zakkak, Christos Kotselidis", "title": "Transparent Compiler and Runtime Specializations for Accelerating\n  Managed Languages on FPGAs", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2021, Vol. 5,\n  Issue 2, Article 8", "doi": "10.22152/programming-journal.org/2021/5/8", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, heterogeneous computing has emerged as the vital way to\nincrease computers? performance and energy efficiency by combining diverse\nhardware devices, such as Graphics Processing Units (GPUs) and Field\nProgrammable Gate Arrays (FPGAs). The rationale behind this trend is that\ndifferent parts of an application can be offloaded from the main CPU to diverse\ndevices, which can efficiently execute these parts as co-processors. FPGAs are\na subset of the most widely used co-processors, typically used for accelerating\nspecific workloads due to their flexible hardware and energy-efficient\ncharacteristics. These characteristics have made them prevalent in a broad\nspectrum of computing systems ranging from low-power embedded systems to\nhigh-end data centers and cloud infrastructures.\n  However, these hardware characteristics come at the cost of programmability.\nDevelopers who create their applications using high-level programming languages\n(e.g., Java, Python, etc.) are required to familiarize with a hardware\ndescription language (e.g., VHDL, Verilog) or recently heterogeneous\nprogramming models (e.g., OpenCL, HLS) in order to exploit the co-processors?\ncapacity and tune the performance of their applications. Currently, the\nabove-mentioned heterogeneous programming models support exclusively the\ncompilation from compiled languages, such as C and C++. Thus, the transparent\nintegration of heterogeneous co-processors to the software ecosystem of managed\nprogramming languages (e.g. Java, Python) is not seamless.\n  In this paper we rethink the engineering trade-offs that we encountered, in\nterms of transparency and compilation overheads, while integrating FPGAs into\nhigh-level managed programming languages. We present a novel approach that\nenables runtime code specialization techniques for seamless and\nhigh-performance execution of Java programs on FPGAs. The proposed solution is\nprototyped in the context of the Java programming language and TornadoVM; an\nopen-source programming framework for Java execution on heterogeneous hardware.\nFinally, we evaluate the proposed solution for FPGA execution against both\nsequential and multi-threaded Java implementations showcasing up to 224x and\n19.8x performance speedups, respectively, and up to 13.82x compared to\nTornadoVM running on an Intel integrated GPU. We also provide a break-down\nanalysis of the proposed compiler optimizations for FPGA execution, as a means\nto project their impact on the applications? characteristics.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 14:53:51 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Papadimitriou", "Michail", ""], ["Fumero", "Juan", ""], ["Stratikopoulos", "Athanasios", ""], ["Zakkak", "Foivos S.", ""], ["Kotselidis", "Christos", ""]]}, {"id": "2010.16306", "submitter": "Katsumi Okuda", "authors": "Katsumi Okuda, Shigeru Chiba", "title": "Lake symbols for island parsing", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2021, Vol. 5,\n  Issue 2, Article 11", "doi": "10.22152/programming-journal.org/2021/5/11", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: An island parser reads an input text and builds the parse (or\nabstract syntax) tree of only the programming constructs of interest in the\ntext. These constructs are called islands and the rest of the text is called\nwater, which the parser ignores and skips over. Since an island parser does not\nhave to parse all the details of the input, it is often easy to develop but\nstill useful enough for a number of software engineering tools. When a parser\ngenerator is used, the developer can implement an island parser by just\ndescribing a small number of grammar rules, for example, in Parsing Expression\nGrammar (PEG).\n  Inquiry: In practice, however, the grammar rules are often complicated since\nthe developer must define the water inside the island; otherwise, the island\nparsing will not reduce the total number of grammar rules. When describing the\ngrammar rules for such water, the developer must consider other rules and\nenumerate a set of symbols, which we call alternative symbols. Due to this\ndifficulty, island parsing seems to be not widely used today despite its\nusefulness in many applications.\n  Approach: This paper proposes the lake symbols for addressing this difficulty\nin developing an island parser. It also presents an extension to PEG for\nsupporting the lake symbols. The lake symbols automate the enumeration of the\nalternative symbols for the water inside an island. The paper proposes an\nalgorithm for translating the extended PEG to the normal PEG, which can be\ngiven to an existing parser generator based on PEG.\n  Knowledge: The user can use lake symbols to define water without specifying\neach alternative symbol. Our algorithms can calculate all alternative symbols\nfor a lake symbol, based on where the lake symbol is used in the grammar.\n  Grounding: We implemented a parser generator accepting our extended PEG and\nimplemented 36 island parsers for Java and 20 island parsers for Python. Our\nexperiments show that the lake symbols reduce 42 % of grammar rules for Java\nand 89 % of rules for Python on average, excluding the case where islands are\nexpressions.\n  Importance: This work eases the use of island parsing. Lake symbols enable\nthe user to define the water inside the island simpler than before. Defining\nwater inside the island is essential to apply island parsing for practical\nprogramming languages.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 14:54:14 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Okuda", "Katsumi", ""], ["Chiba", "Shigeru", ""]]}, {"id": "2010.16353", "submitter": "Long Pham", "authors": "Long Pham, Jan Hoffmann", "title": "Typable Fragments of Polynomial Automatic Amortized Resource Analysis", "comments": "This is the full version of our CSL 2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being a fully automated technique for resource analysis, automatic amortized\nresource analysis (AARA) can fail in returning worst-case cost bounds of\nprograms, fundamentally due to the undecidability of resource analysis. For\nprogrammers who are unfamiliar with the technical details of AARA, it is\ndifficult to predict whether a program can be successfully analyzed in AARA.\nMotivated by this problem, this article identifies classes of programs that can\nbe analyzed in type-based polynomial AARA. Firstly, it is shown that the set of\nfunctions that are typable in univariate polynomial AARA coincides with the\ncomplexity class PTIME. Secondly, the article presents a sufficient condition\nfor typability that axiomatically requires every sub-expression of a given\nprogram to be polynomial-time. It is proved that this condition implies\ntypability in multivariate polynomial AARA under some syntactic restrictions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 16:17:49 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Pham", "Long", ""], ["Hoffmann", "Jan", ""]]}, {"id": "2010.16385", "submitter": "Andreas Pavlogiannis", "authors": "Umang Mathur and Andreas Pavlogiannis and Mahesh Viswanathan", "title": "Optimal Prediction of Synchronization-Preserving Races", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concurrent programs are notoriously hard to write correctly, as scheduling\nnondeterminism introduces subtle errors that are both hard to detect and to\nreproduce. The most common concurrency errors are (data) races, which occur\nwhen memory-conflicting actions are executed concurrently. Consequently,\nconsiderable effort has been made towards developing efficient techniques for\nrace detection. The most common approach is dynamic race prediction: given an\nobserved, race-free trace $\\sigma$ of a concurrent program, the task is to\ndecide whether events of $\\sigma$ can be correctly reordered to a trace\n$\\sigma^*$ that witnesses a race hidden in $\\sigma$.\n  In this work we introduce the notion of sync(hronization)-preserving races. A\nsync-preserving race occurs in $\\sigma$ when there is a witness $\\sigma^*$ in\nwhich synchronization operations (e.g., acquisition and release of locks)\nappear in the same order as in $\\sigma$. This is a broad definition that\nstrictly subsumes the famous notion of happens-before races. Our main results\nare as follows. First, we develop a sound and complete algorithm for predicting\nsync-preserving races. For moderate values of parameters like the number of\nthreads, the algorithm runs in $\\widetilde{O}(\\mathcal{N})$ time and space,\nwhere $\\mathcal{N}$ is the length of the trace $\\sigma$. Second, we show that\nthe problem has a $\\Omega(\\mathcal{N}/\\log^2 \\mathcal{N})$ space lower bound,\nand thus our algorithm is essentially time and space optimal. Third, we show\nthat predicting races with even just a single reversal of two sync operations\nis $\\operatorname{NP}$-complete and even $\\operatorname{W}[1]$-hard when\nparameterized by the number of threads. Thus, sync-preservation characterizes\nexactly the tractability boundary of race prediction, and our algorithm is\nnearly optimal for the tractable side.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 17:29:10 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Mathur", "Umang", ""], ["Pavlogiannis", "Andreas", ""], ["Viswanathan", "Mahesh", ""]]}]