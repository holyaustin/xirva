[{"id": "1603.00087", "submitter": "Santiago Escobar", "authors": "Sonia Santiago and Santiago Escobar and Catherine Meadows and Jos\\'e\n  Meseguer", "title": "Effective Sequential Protocol Composition in Maude-NPA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protocols do not work alone, but together, one protocol relying on another to\nprovide needed services. Many of the problems in cryptographic protocols arise\nwhen such composition is done incorrectly or is not well understood. In this\npaper we discuss an extension to the Maude-NPA syntax and its operational\nsemantics to support dynamic sequential composition of protocols, so that\nprotocols can be specified separately and composed when desired. This allows\none to reason about many different compositions with minimal changes to the\nspecification, as well as improving, in terms of both performance and ease of\nspecification, on an earlier composition extension we presented in [18]. We\nshow how compositions can be defined and executed symbolically in Maude-NPA\nusing the compositional syntax and semantics. We also provide an experimental\nanalysis of the performance of Maude-NPA using the compositional syntax and\nsemantics, and compare it to the performance of a syntax and semantics for\ncomposition developed in earlier research. Finally, in the conclusion we give\nsome lessons learned about the best ways of extending narrowing-based state\nreachability tools, as well as comparison with related work and future plans.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 23:12:24 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Santiago", "Sonia", ""], ["Escobar", "Santiago", ""], ["Meadows", "Catherine", ""], ["Meseguer", "Jos\u00e9", ""]]}, {"id": "1603.00307", "submitter": "Christopher M. Poskitt", "authors": "Claudio Corrodi, Alexander Heu{\\ss}ner, Christopher M. Poskitt", "title": "A Graph-Based Semantics Workbench for Concurrent Asynchronous Programs", "comments": "Accepted for publication in the proceedings of FASE 2016 (to appear)", "journal-ref": "Proc. International Conference on Fundamental Approaches to\n  Software Engineering (FASE 2016), volume 9633 of LNCS, pages 31-48. Springer,\n  2016", "doi": "10.1007/978-3-662-49665-7_3", "report-no": null, "categories": "cs.SE cs.DC cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of novel programming languages and libraries have been proposed that\noffer simpler-to-use models of concurrency than threads. It is challenging,\nhowever, to devise execution models that successfully realise their\nabstractions without forfeiting performance or introducing unintended\nbehaviours. This is exemplified by SCOOP---a concurrent object-oriented\nmessage-passing language---which has seen multiple semantics proposed and\nimplemented over its evolution. We propose a \"semantics workbench\" with fully\nand semi-automatic tools for SCOOP, that can be used to analyse and compare\nprograms with respect to different execution models. We demonstrate its use in\nchecking the consistency of semantics by applying it to a set of representative\nprograms, and highlighting a deadlock-related discrepancy between the principal\nexecution models of the language. Our workbench is based on a modular and\nparameterisable graph transformation semantics implemented in the GROOVE tool.\nWe discuss how graph transformations are leveraged to atomically model\nintricate language abstractions, and how the visual yet algebraic nature of the\nmodel can be used to ascertain soundness.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 15:10:21 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Corrodi", "Claudio", ""], ["Heu\u00dfner", "Alexander", ""], ["Poskitt", "Christopher M.", ""]]}, {"id": "1603.00431", "submitter": "Amirali Sanatinia", "authors": "Amirali Sanatinia, Guevara Noubir", "title": "On GitHub's Programming Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GitHub is the most widely used social, distributed version control system. It\nhas around 10 million registered users and hosts over 16 million public\nrepositories. Its user base is also very active as GitHub ranks in the top 100\nAlexa most popular websites. In this study, we collect GitHub's state in its\nentirety. Doing so, allows us to study new aspects of the ecosystem. Although\nGitHub is the home to millions of users and repositories, the analysis of\nusers' activity time-series reveals that only around 10% of them can be\nconsidered active. The collected dataset allows us to investigate the\npopularity of programming languages and existence of pattens in the relations\nbetween users, repositories, and programming languages.\n  By, applying a k-means clustering method to the users-repositories commits\nmatrix, we find that two clear clusters of programming languages separate from\nthe remaining. One cluster forms for \"web programming\" languages (Java Script,\nRuby, PHP, CSS), and a second for \"system oriented programming\" languages (C,\nC++, Python). Further classification, allow us to build a phylogenetic tree of\nthe use of programming languages in GitHub. Additionally, we study the main and\nthe auxiliary programming languages of the top 1000 repositories in more\ndetail. We provide a ranking of these auxiliary programming languages using\nvarious metrics, such as percentage of lines of code, and PageRank.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 20:03:44 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Sanatinia", "Amirali", ""], ["Noubir", "Guevara", ""]]}, {"id": "1603.00536", "submitter": "EPTCS", "authors": "C\\'esar A. Mu\\~noz (NASA Langley Research Center), Jorge A. P\\'erez\n  (University of Groningen)", "title": "Proceedings of the Eleventh International Workshop on Developments in\n  Computational Models", "comments": null, "journal-ref": "EPTCS 204, 2016", "doi": "10.4204/EPTCS.204", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of DCM 2015, the 11th International\nWorkshop on Developments in Computational Models held on October 28, 2015 in\nCali, Colombia. DCM 2015 was organized as a one-day satellite event of the 12th\nInternational Colloquium on Theoretical Aspects of Computing (ICTAC 2015).\n  Several new models of computation have emerged in the last few years, and\nmany developments of traditional computational models have been proposed with\nthe aim of taking into account the new demands of computer systems users and\nthe new capabilities of computation engines. A new computational model, or a\nnew feature in a traditional one, usually is reflected in a new family of\nprogramming languages, and new paradigms of software development.\n  The aim of the DCM workshop series is to bring together researchers who are\ncurrently developing new computational models or new features for traditional\ncomputational models, in order to foster their interaction, to provide a forum\nfor presenting new ideas and work in progress, and to enable newcomers to learn\nabout current activities in this area. Topics of interest include all abstract\nmodels of computation and their applications to the development of programming\nlanguages and systems.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 00:49:28 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Mu\u00f1oz", "C\u00e9sar A.", "", "NASA Langley Research Center"], ["P\u00e9rez", "Jorge A.", "", "University of Groningen"]]}, {"id": "1603.00649", "submitter": "Malte Schwerhoff", "authors": "Peter M\\\"uller, Malte Schwerhoff, Alexander J. Summers", "title": "Automatic Verification of Iterated Separating Conjunctions using\n  Symbolic Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In permission logics such as separation logic, the iterated separating\nconjunction is a quantifier denoting access permission to an unbounded set of\nheap locations. In contrast to recursive predicates, iterated separating\nconjunctions do not prescribe a structure on the locations they range over, and\nso do not restrict how to traverse and modify these locations. This flexibility\nis important for the verification of random-access data structures such as\narrays and data structures that can be traversed in multiple ways such as\ngraphs. Despite its usefulness, no automatic program verifier natively supports\niterated separating conjunctions; they are especially difficult to incorporate\ninto symbolic execution engines, the prevalent technique for building verifiers\nfor these logics.\n  In this paper, we present the first symbolic execution technique to support\ngeneral iterated separating conjunctions. We propose a novel representation of\nsymbolic heaps and flexible support for logical specifications that quantify\nover heap locations. Our technique exhibits predictable and fast performance\ndespite employing quantifiers at the SMT level, by carefully controlling\nquantifier instantiations. It is compatible with other features of permission\nlogics such as fractional permissions, recursive predicates, and abstraction\nfunctions. Our technique is implemented as an extension of the Viper\nverification infrastructure.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 10:41:47 GMT"}, {"version": "v2", "created": "Thu, 3 Mar 2016 09:21:38 GMT"}, {"version": "v3", "created": "Fri, 6 May 2016 10:23:37 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["M\u00fcller", "Peter", ""], ["Schwerhoff", "Malte", ""], ["Summers", "Alexander J.", ""]]}, {"id": "1603.00975", "submitter": "EPTCS", "authors": "Mauricio Ayala-Rinc\\'on (Universidade de Bras\\'ilia)", "title": "Formalising Confluence in PVS", "comments": "In Proceedings DCM 2015, arXiv:1603.00536", "journal-ref": "EPTCS 204, 2016, pp. 11-17", "doi": "10.4204/EPTCS.204.2", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confluence is a critical property of computational systems which is related\nwith determinism and non ambiguity and thus with other relevant computational\nattributes of functional specifications and rewriting system as termination and\ncompletion. Several criteria have been explored that guarantee confluence and\ntheir formalisations provide further interesting information. This work\ndiscusses topics and presents personal positions and views related with the\nformalisation of confluence properties in the Prototype Verification System PVS\ndeveloped at our research group.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 05:33:28 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Ayala-Rinc\u00f3n", "Mauricio", "", "Universidade de Bras\u00edlia"]]}, {"id": "1603.01520", "submitter": "Daniel Rubio Bonilla", "authors": "Daniel Rubio Bonilla, Colin W. Glass, Jan Kuper", "title": "Optimized Polynomial Evaluation with Semantic Annotations", "comments": "Part of the Program Transformation for Programmability in\n  Heterogeneous Architectures (PROHA) workshop, Barcelona, Spain, 12th March\n  2016, 7 pages, LaTeX, 4 PNG figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss how semantic annotations can be used to introduce\nmathematical algorithmic information of the underlying imperative code to\nenable compilers to produce code transformations that will enable better\nperformance. By using this approaches not only good performance is achieved,\nbut also better programmability, maintainability and portability across\ndifferent hardware architectures. To exemplify this we will use polynomial\nequations of different degrees.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 16:13:24 GMT"}, {"version": "v2", "created": "Thu, 10 Mar 2016 14:17:38 GMT"}, {"version": "v3", "created": "Fri, 11 Mar 2016 11:31:59 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Bonilla", "Daniel Rubio", ""], ["Glass", "Colin W.", ""], ["Kuper", "Jan", ""]]}, {"id": "1603.01965", "submitter": "Johann Mogensen", "authors": "Johann Thor Mogensen Ingibergsson, Stefan-Daniel Suvei, Mikkel Kragh\n  Hansen, Peter Christiansen and Ulrik Pagh Schultz", "title": "Towards a DSL for Perception-Based Safety Systems", "comments": "Presented at DSLRob 2015 (arXiv:1601.00877) This paper is a poster\n  submission, an extension to the already accepted article (no abstract):\n  1601.00877 Thus the introduction in this paper, is a compilation of the\n  earlier article, which is referenced as [3]", "journal-ref": null, "doi": null, "report-no": "DSLRob/2015/08", "categories": "cs.RO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is an extension to an early presented programming language, called\na domain specific language. This paper extends the proposed concept with new\nsensors and behaviours to address real-life situations. The functionality was\ntested in lab experiments, and an extension to the earlier concepts is\nproposed.\n", "versions": [{"version": "v1", "created": "Mon, 7 Mar 2016 07:47:27 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Ingibergsson", "Johann Thor Mogensen", ""], ["Suvei", "Stefan-Daniel", ""], ["Hansen", "Mikkel Kragh", ""], ["Christiansen", "Peter", ""], ["Schultz", "Ulrik Pagh", ""]]}, {"id": "1603.03011", "submitter": "Salvador Tamarit", "authors": "Salvador Tamarit and Julio Mari\\~no and Guillermo Vigueras and Manuel\n  Carro", "title": "Towards a Semantics-Aware Transformation Toolchain for Heterogeneous\n  Systems", "comments": "Part of the Program Transformation for Programmability in\n  Heterogeneous Architectures (PROHA) workshop, Barcelona, Spain, 12th March\n  2016, 11 pages, LaTeX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining good performance when programming heterogeneous computing platforms\nposes significant challenges for the programmer. We present a program\ntransformation environment, implemented in Haskell, where architecture-agnostic\nscientific C code with semantic annotations is transformed into functionally\nequivalent code better suited for a given platform. The transformation steps\nare formalized (and implemented) as rules which can be fired when certain\nsyntactic and semantic conditions are met. These conditions are to be fulfilled\nby program properties which can be automatically inferred or, alternatively,\nstated as annotations in the source code. Rule selection can be guided by\nheuristics derived from a machine learning procedure which tries to capture how\nrun-time characteristics (e.g., resource consumption or performance) are\naffected by the transformation steps.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 19:42:59 GMT"}, {"version": "v2", "created": "Thu, 10 Mar 2016 16:11:03 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Tamarit", "Salvador", ""], ["Mari\u00f1o", "Julio", ""], ["Vigueras", "Guillermo", ""], ["Carro", "Manuel", ""]]}, {"id": "1603.03022", "submitter": "Salvador Tamarit", "authors": "Guillermo Vigueras and Manuel Carro and Salvador Tamarit and Julio\n  Mari\\~no", "title": "Towards Automatic Learning of Heuristics for Mechanical Transformations\n  of Procedural Code", "comments": "Part of the Program Transformation for Programmability in\n  Heterogeneous Architectures (PROHA) workshop, Barcelona, Spain, 12th March\n  2016, 9 pages, LaTeX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current trend in next-generation exascale systems goes towards\nintegrating a wide range of specialized (co-)processors into traditional\nsupercomputers. However, the integration of different specialized devices\nincreases the degree of heterogeneity and the complexity in programming such\ntype of systems. Due to the efficiency of heterogeneous systems in terms of\nWatt and FLOPS per surface unit, opening the access of heterogeneous platforms\nto a wider range of users is an important problem to be tackled. In order to\nbridge the gap between heterogeneous systems and programmers, in this paper we\npropose a machine learning-based approach to learn heuristics for defining\ntransformation strategies of a program transformation system. Our approach\nproposes a novel combination of reinforcement learning and classification\nmethods to efficiently tackle the problems inherent to this type of systems.\nPreliminary results demonstrate the suitability of the approach for easing the\nprogrammability of heterogeneous systems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 20:32:00 GMT"}, {"version": "v2", "created": "Thu, 10 Mar 2016 02:07:48 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Vigueras", "Guillermo", ""], ["Carro", "Manuel", ""], ["Tamarit", "Salvador", ""], ["Mari\u00f1o", "Julio", ""]]}, {"id": "1603.03165", "submitter": "Ivan Radi\\v{c}ek", "authors": "Sumit Gulwani, Ivan Radi\\v{c}ek, Florian Zuleger", "title": "Automated Clustering and Program Repair for Introductory Programming\n  Assignments", "comments": "Extended version of the PLDI paper of the same name", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Providing feedback on programming assignments is a tedious task for the\ninstructor, and even impossible in large Massive Open Online Courses with\nthousands of students. Previous research has suggested that program repair\ntechniques can be used to generate feedback in programming education. In this\npaper, we present a novel fully automated program repair algorithm for\nintroductory programming assignments. The key idea of the technique, which\nenables automation and scalability, is to use the existing correct student\nsolutions to repair the incorrect attempts. We evaluate the approach in two\nexperiments: (I) We evaluate the number, size and quality of the generated\nrepairs on 4,293 incorrect student attempts from an existing MOOC. We find that\nour approach can repair 97% of student attempts, while 81% of those are small\nrepairs of good quality. (II) We conduct a preliminary user study on\nperformance and repair usefulness in an interactive teaching setting. We obtain\npromising initial results (the average usefulness grade 3.4 on a scale from 1\nto 5), and conclude that our approach can be used in an interactive setting.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 07:00:06 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2016 14:40:23 GMT"}, {"version": "v3", "created": "Sat, 27 Aug 2016 11:07:42 GMT"}, {"version": "v4", "created": "Wed, 20 Jun 2018 02:16:55 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Gulwani", "Sumit", ""], ["Radi\u010dek", "Ivan", ""], ["Zuleger", "Florian", ""]]}, {"id": "1603.03488", "submitter": "Salvador Tamarit", "authors": "Salvador Tamarit, Julio Mari\\~no, Guillermo Vigueras, Manuel Carro", "title": "Proceedings of the First Workshop on Program Transformation for\n  Programmability in Heterogeneous Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of PROHA 2016, the first workshop on\nProgram Transformation for Programmability in Heterogeneous Architectures, held\non March 12, 2016 in Barcelona, Spain, as an affiliated workshop of CGO 2016,\nthe 14th International Symposium on Code Generation and Optimization.\nDeveloping and maintaining high-performance applications and libraries for\nheterogeneous architectures while preserving its semantics and with a\nreasonable efficiency is a time-consuming task which is often only possible for\nexperts. It often requires manually adapting sequential, platform-agnostic code\nto different infrastructures, and keeping the changes in all of these\ninfrastructures in sync. These program modification tasks are costly and\nerror-prone. Tools to assist in and, if possible, automate such transformations\nare of course of great interest. However, such tools may need significant\nreasoning and knowledge processing capabilities, including, for example, being\nable to process machine-understandable descriptions of the semantics of a piece\nof code is expected to do; to perform program transformations inside a context\nin which they are applicable; to use strategies to identify the sequence of\ntransformations leading to the best resulting code; and others.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 23:44:33 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Tamarit", "Salvador", ""], ["Mari\u00f1o", "Julio", ""], ["Vigueras", "Guillermo", ""], ["Carro", "Manuel", ""]]}, {"id": "1603.03533", "submitter": "Mohammad Abdollahi Azgomi", "authors": "Elaheh Ghassabani and Mohammad Abdollahi Azgomi", "title": "Stateless Code Model Checking of Information Flow Security", "comments": "Trustworthy Computing Laboratory, School of Computer Engineering,\n  Iran University of Science and Technology, Tehran, Iran, 2014", "journal-ref": null, "doi": null, "report-no": "Technical Report No. TWcL-TR-1401", "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observational determinism is a security property that characterizes secure\ninformation flow for multithreaded programs. Most of the methods that have been\nused to verify observational determinism are based on either type systems or\nconventional model checking techniques. A conventional model checker is\nstateful and often verifies a system model usually constructed manually. As\nthese methods are based on stateful model checking, they are confronted with\nthe state space explosion problem. In order to verify and test computer\nprograms, stateless code model checking is more appropriate than conventional\ntechniques. It is an effective method for systematic testing of large and\ncomplicated concurrent programs, and for exploring the state space of such\nprograms. In this paper, we propose a new method for verifying information flow\nsecurity in concurrent programs. For the first time, we use stateless code\nmodel checking to verify observational determinism.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 06:28:50 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Ghassabani", "Elaheh", ""], ["Azgomi", "Mohammad Abdollahi", ""]]}, {"id": "1603.03535", "submitter": "Mohammad Abdollahi Azgomi", "authors": "Elaheh Ghassabani and Mohammad Abdollahi Azgomi", "title": "A New Approach to Stateless Model Checking of LTL Properties", "comments": null, "journal-ref": "Trustworthy Computing Laboratory, School of Computer Engineering,\n  Iran University of Science and Technology, Tehran, Iran, 2014", "doi": null, "report-no": "Technical Report No. TWcL-TR-1402", "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification of large and complicated concurrent programs is an important\nissue in the software world. Stateless model checking is an appropriate method\nfor systematically and automatically testing of large programs, which has\nproved its power in verifying code of large programs. Another well-known method\nin this area is runtime verification. Both stateless model checking and runtime\nverification are similar in some ways. One common approach in runtime\nverification is to construct runtime monitors for properties expressed in\nlinear temporal logic. Currently, there are some semantics to check linear\ntemporal logic formulae on finite paths proposed in the field of runtime\nverification, which can also be applied to stateless model checking. However,\nexisting stateless model checkers do not support LTL formulae. In some\nsettings, it is more advantageous to make use of stateless model checking\ninstead of runtime verification. This paper proposes a novel encoding of one of\nthe recent LTL semantics on finite paths into an actor-based system. We take a\ntruly parallel approach without saving any program states or traces, which not\nonly addresses important problems in runtime verification, but can also be\napplied to stateless model checking.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 06:37:28 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Ghassabani", "Elaheh", ""], ["Azgomi", "Mohammad Abdollahi", ""]]}, {"id": "1603.03536", "submitter": "Mohammad Abdollahi Azgomi", "authors": "Elaheh Ghassabani and Mohammad Abdollahi Azgomi", "title": "DSCMC: Distributed Stateless Code Model Checker", "comments": null, "journal-ref": "Trustworthy Computing Laboratory, School of Computer Engineering,\n  Iran University of Science and Technology, Tehran, Iran, 2014", "doi": null, "report-no": "Technical Report No. TWcL-TR-1403", "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stateless code model checking is an effective verification technique, which\nis more applicable than stateful model checking to the software world. Existing\nstateless model checkers support the verification of neither LTL formulae nor\nthe information flow security properties. This paper proposes a distributed\nstateless code model checker (DSCMC) designed based on the Actor model, and has\nthe capability of verifying code written in different programming languages.\nThis tool is implemented using Erlang, which is an actor-based programming\nlanguage. DSCMC is able to detect deadlocks, livelocks, and data races\nautomatically. In addition, the tool can verify information flow security and\nthe properties specified in LTL. Thanks to its actor-based architecture, DSCMC\nprovides a wide range of capabilities. The parallel architecture of the tool\nexploiting the rich concurrency model of Erlang is suited to the time-intensive\nprocess of stateless code model checking.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 06:49:43 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Ghassabani", "Elaheh", ""], ["Azgomi", "Mohammad Abdollahi", ""]]}, {"id": "1603.03727", "submitter": "Hongwei Xi", "authors": "Hongwei Xi and Zhiqiang Ren and Hanwen Wu and William Blair", "title": "Session Types in a Linearly Typed Multi-Threaded Lambda-Calculus", "comments": "This is the original version of the paper on supporting programming\n  with dyadic session types in ATS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a formalization of session types in a multi-threaded\nlambda-calculus (MTLC) equipped with a linear type system, establishing for the\nMTLC both type preservation and global progress. The latter (global progress)\nimplies that the evaluation of a well-typed program in the MTLC can never reach\na deadlock. As this formulated MTLC can be readily embedded into ATS, a\nfull-fledged language with a functional programming core that supports both\ndependent types (of DML-style) and linear types, we obtain a direct\nimplementation of session types in ATS. In addition, we gain immediate support\nfor a form of dependent session types based on this embedding into ATS.\nCompared to various existing formalizations of session types, we see the one\ngiven in this paper is unique in its closeness to concrete implementation. In\nparticular, we report such an implementation ready for practical use that\ngenerates Erlang code from well-typed ATS source (making use of session types),\nthus taking great advantage of the infrastructural support for distributed\ncomputing in Erlang.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 19:15:03 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Xi", "Hongwei", ""], ["Ren", "Zhiqiang", ""], ["Wu", "Hanwen", ""], ["Blair", "William", ""]]}, {"id": "1603.04298", "submitter": "Matthijs V\\'ak\\'ar", "authors": "Matthijs V\\'ak\\'ar", "title": "An Effectful Treatment of Dependent Types", "comments": "arXiv admin note: substantial text overlap with arXiv:1512.08009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend Levy's call-by-push-value (CBPV) analysis from simple to dependent\ntype theory (DTT) in order to study the interaction between computational\neffects and dependent types. We define the naive system of dependently typed\nCBPV, dCBPV-, and its extension with a principle of Kleisli extensions for\ndependent functions, dCBPV+. We investigate these systems from the points of\nview of syntax, categorical semantics, concrete models and operational\nsemantics, in presence of a range of effects. We observe that, while the\nexpressive power of dCBPV+ is needed if we want well-defined call-by-value\n(CBV) and call-by-name (CBN) translations of DTT, it is a less straightforward\nsystem than dCBPV-, in presence of some effects. Indeed, to be able to\nconstruct specific models and to retain the subject reduction property in the\noperational semantics, we are required to impose certain subtyping conditions,\nthe idea being that the type of a computation may only become more (not less)\nspecified as certain effects are executed.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 15:09:42 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["V\u00e1k\u00e1r", "Matthijs", ""]]}, {"id": "1603.05197", "submitter": "Shayan Najd", "authors": "Shayan Najd, Sam Lindley, Josef Svenningsson, Philip Wadler", "title": "Embedding by Normalisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the insight that practical embedding techniques, commonly\nused for implementing Domain-Specific Languages, correspond to theoretical\nNormalisation-By-Evaluation (NBE) techniques, commonly used for deriving\ncanonical form of terms with respect to an equational theory.\n  NBE constitutes of four components: a syntactic domain, a semantic domain,\nand a pair of translations between the two. Embedding also often constitutes of\nfour components: an object language, a host language, encoding of object terms\nin the host, and extraction of object code from the host.\n  The correspondence is deep in that all four components in embedding and NBE\ncorrespond to each other. Based on this correspondence, this paper introduces\nEmbedding-By-Normalisation (EBN) as a principled approach to study and\nstructure embedding.\n  The correspondence is useful in that solutions from NBE can be borrowed to\nsolve problems in embedding. In particular, based on NBE techniques, such as\nType-Directed Partial Evaluation, this paper presents a solution to the problem\nof extracting object code from embedded programs involving sum types, such as\nconditional expressions, and primitives, such as literals and operations on\nthem.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 18:04:04 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Najd", "Shayan", ""], ["Lindley", "Sam", ""], ["Svenningsson", "Josef", ""], ["Wadler", "Philip", ""]]}, {"id": "1603.05495", "submitter": "Matthew Noonan", "authors": "Matthew Noonan, Alexey Loginov, David Cok", "title": "Polymorphic Type Inference for Machine Code", "comments": "Full version with appendices, for PLDI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many compiled languages, source-level types are erased very early in the\ncompilation process. As a result, further compiler passes may convert type-safe\nsource into type-unsafe machine code. Type-unsafe idioms in the original source\nand type-unsafe optimizations mean that type information in a stripped binary\nis essentially nonexistent. The problem of recovering high-level types by\nperforming type inference over stripped machine code is called type\nreconstruction, and offers a useful capability in support of reverse\nengineering and decompilation.\n  In this paper, we motivate and develop a novel type system and algorithm for\nmachine-code type inference. The features of this type system were developed by\nsurveying a wide collection of common source- and machine-code idioms, building\na catalog of challenging cases for type reconstruction. We found that these\nidioms place a sophisticated set of requirements on the type system, inducing\nfeatures such as recursively-constrained polymorphic types. Many of the\nfeatures we identify are often seen only in expressive and powerful type\nsystems used by high-level functional languages.\n  Using these type-system features as a guideline, we have developed Retypd: a\nnovel static type-inference algorithm for machine code that supports recursive\ntypes, polymorphism, and subtyping. Retypd yields more accurate inferred types\nthan existing algorithms, while also enabling new capabilities such as\nreconstruction of pointer const annotations with 98% recall. Retypd can operate\non weaker program representations than the current state of the art, removing\nthe need for high-quality points-to information that may be impractical to\ncompute.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 14:09:14 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 20:09:56 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Noonan", "Matthew", ""], ["Loginov", "Alexey", ""], ["Cok", "David", ""]]}, {"id": "1603.06129", "submitter": "Rishabh Singh", "authors": "Sahil Bhatia and Rishabh Singh", "title": "Automated Correction for Syntax Errors in Programming Assignments using\n  Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for automatically generating repair feedback for syntax\nerrors for introductory programming problems. Syntax errors constitute one of\nthe largest classes of errors (34%) in our dataset of student submissions\nobtained from a MOOC course on edX. The previous techniques for generating\nautomated feed- back on programming assignments have focused on functional\ncorrectness and style considerations of student programs. These techniques\nanalyze the program AST of the program and then perform some dynamic and\nsymbolic analyses to compute repair feedback. Unfortunately, it is not possible\nto generate ASTs for student pro- grams with syntax errors and therefore the\nprevious feedback techniques are not applicable in repairing syntax errors.\n  We present a technique for providing feedback on syntax errors that uses\nRecurrent neural networks (RNNs) to model syntactically valid token sequences.\nOur approach is inspired from the recent work on learning language models from\nBig Code (large code corpus). For a given programming assignment, we first\nlearn an RNN to model all valid token sequences using the set of syntactically\ncorrect student submissions. Then, for a student submission with syntax errors,\nwe query the learnt RNN model with the prefix to- ken sequence to predict token\nsequences that can fix the error by either replacing or inserting the predicted\ntoken sequence at the error location. We evaluate our technique on over 14, 000\nstudent submissions with syntax errors. Our technique can completely re- pair\n31.69% (4501/14203) of submissions with syntax errors and in addition partially\ncorrect 6.39% (908/14203) of the submissions.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 18:43:28 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Bhatia", "Sahil", ""], ["Singh", "Rishabh", ""]]}, {"id": "1603.06266", "submitter": "G\\\"unter Khyo", "authors": "G\\\"unter Khyo", "title": "Multidimensional Predicates for Prolog", "comments": "17 pages, to be submitted to ICLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2014, Ungar et al. proposed Korz, a new computational model for\nstructuring adaptive (object-oriented) systems. Korz combines implicit\nparameters and multiple dispatch to structure the behavior of objects in a\nmultidimensional space. Korz is a simple yet expressive model which does not\nrequire special programming techniques such as the Visitor or Strategy pattern\nto accommodate a system for emerging contextual requirements. We show how the\nideas of Korz can be integrated in a Prolog system by extending its syntax and\nsemantics with simple meta-programming techniques. We developed a library,\ncalled mdp (multidimensional predicates) which can be used to experiment with\nmultidimensional Prolog systems. We highlight its benefits with numerous\nscenarios such as printing debugging information, memoization, object-oriented\nprogramming and adaptive GUIs. In particular, we point out that we can\nstructure and extend Prolog programs with additional concerns in a clear and\nconcise manner. We also demonstrate how Prolog's unique meta-programming\ncapabilities allow for quick experimentation with syntactical and semantical\nenhancement of the new, multidimensional model. While there are many open\nconcerns, such as efficiency and comprehensibility in the case of larger\nsystems, we will see that we can use the leverage of mdp and Prolog to explore\nnew horizons in the design of adaptive systems.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 20:43:06 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Khyo", "G\u00fcnter", ""]]}, {"id": "1603.07292", "submitter": "Shayak Sen", "authors": "Aleksandar Chakarov, Aditya Nori, Sriram Rajamani, Shayak Sen, and\n  Deepak Vijaykeerthy", "title": "Debugging Machine Learning Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike traditional programs (such as operating systems or word processors)\nwhich have large amounts of code, machine learning tasks use programs with\nrelatively small amounts of code (written in machine learning libraries), but\nvoluminous amounts of data. Just like developers of traditional programs debug\nerrors in their code, developers of machine learning tasks debug and fix errors\nin their data. However, algorithms and tools for debugging and fixing errors in\ndata are less common, when compared to their counterparts for detecting and\nfixing errors in code. In this paper, we consider classification tasks where\nerrors in training data lead to misclassifications in test points, and propose\nan automated method to find the root causes of such misclassifications. Our\nroot cause analysis is based on Pearl's theory of causation, and uses Pearl's\nPS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi,\nencodes the computation of PS as a probabilistic program, and uses recent work\non probabilistic programs and transformations on probabilistic programs (along\nwith gray-box models of machine learning algorithms) to efficiently compute PS.\nPsi is able to identify root causes of data errors in interesting data sets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 18:30:37 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Chakarov", "Aleksandar", ""], ["Nori", "Aditya", ""], ["Rajamani", "Sriram", ""], ["Sen", "Shayak", ""], ["Vijaykeerthy", "Deepak", ""]]}, {"id": "1603.07484", "submitter": "Rodolphe Lepigre", "authors": "Rodolphe Lepigre (LAMA)", "title": "A Classical Realizability Model for a Semantical Value Restriction", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-662-49498-1_19", "report-no": null, "categories": "cs.LO cs.PL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new type system with support for proofs of programs in a\ncall-by-value language with control operators. The proof mechanism relies on\nobservational equivalence of (untyped) programs. It appears in two type\nconstructors, which are used for specifying program properties and for encoding\ndependent products. The main challenge arises from the lack of expressiveness\nof dependent products due to the value restriction. To circumvent this\nlimitation we relax the syntactic restriction and only require equivalence to a\nvalue. The consistency of the system is obtained semantically by constructing a\nclassical realizability model in three layers (values, stacks and terms).\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 09:00:30 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 13:48:16 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Lepigre", "Rodolphe", "", "LAMA"]]}, {"id": "1603.07790", "submitter": "J\\\"urgen Koslowski", "authors": "Yasuhiko Minamide", "title": "Weighted Pushdown Systems with Indexed Weight Domains", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 12, Issue 2 (June 29,\n  2016) lmcs:1641", "doi": "10.2168/LMCS-12(2:9)2016", "report-no": null, "categories": "cs.FL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reachability analysis of weighted pushdown systems is a very powerful\ntechnique in verification and analysis of recursive programs. Each transition\nrule of a weighted pushdown system is associated with an element of a bounded\nsemiring representing the weight of the rule. However, we have realized that\nthe restriction of the boundedness is too strict and the formulation of\nweighted pushdown systems is not general enough for some applications. To\ngeneralize weighted pushdown systems, we first introduce the notion of stack\nsignatures that summarize the effect of a computation of a pushdown system and\nformulate pushdown systems as automata over the monoid of stack signatures. We\nthen generalize weighted pushdown systems by introducing semirings indexed by\nthe monoid and weaken the boundedness to local boundedness.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 00:50:25 GMT"}, {"version": "v2", "created": "Mon, 28 Mar 2016 02:40:53 GMT"}, {"version": "v3", "created": "Tue, 28 Jun 2016 16:04:30 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Minamide", "Yasuhiko", ""]]}, {"id": "1603.08059", "submitter": "Steven Meyer J", "authors": "Steven Meyer", "title": "CVC Verilog Compiler -- Fast Complex Language Compilers Can be Simple", "comments": "7 pages, 24 references. Paper rewritten in an attempt to comply with\n  the new ACM double blind refereeing system (referees should not be able to\n  determine author), but original title used for this second version. Also\n  added discussion of importance of unbounded size tmps and connections to Bell\n  Labs Unix and XPL historical projects", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explains how to develop Verilog hardware description language\n(HDL) optimized flow graph compiled simulators. It is claimed that the methods\nand algorithms described here can be applied in the development of flow graph\ncompilers for other complex computer languages. The method uses the von Neumann\ncomputer architecture (MRAM model) as the best abstract model of computation\nand uses comparison and selection of alternative machine code sequences to\nutilize modern processor low level parallelism. By using the anti formalist\nmethod described here, the fastest available full IEEE 1364 2005 Verilog HDL\nstandard simulators has been developed. The compiler only required 95,000 lines\nof C code and two developers. This paper explains how such a compiled simulator\nvalidates the anti-formalism computer science methodology best expressed by\nPeter Naur's datalogy and provides specific guidelines for applying the method.\nDevelopment history from a slow interpreter into a fast flow graph based\nmachine code compiled simulator is described. The failure of initial efforts\nthat tried to convert a full 1364 compliant interpreter into interpreted\nexecution of possibly auto generated virtual machines is discussed. The\nargument that fast Verilog simulation requires detail removing abstraction is\nshown to be incorrect. Reasons parallel GPU Verilog simulation has not\nsucceeded are given.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 00:25:37 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 21:18:07 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Meyer", "Steven", ""]]}, {"id": "1603.08633", "submitter": "EPTCS", "authors": "Sarmen Keshishzadeh (Eindhoven University of Technology), Arjan J.\n  Mooij (Embedded Systems Innovation by TNO), Jozef Hooman (Embedded Systems\n  Innovation by TNO and Radboud University Nijmegen)", "title": "Industrial Experiences with a Formal DSL Semantics to Check the\n  Correctness of DSL Artifacts", "comments": "In Proceedings FESCA 2016, arXiv:1603.08371", "journal-ref": "EPTCS 205, 2016, pp. 16-30", "doi": "10.4204/EPTCS.205.2", "report-no": null, "categories": "cs.SE cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A domain specific language (DSL) abstracts from implementation details and is\naligned with the way domain experts reason about a software component. The\ndevelopment of DSLs is usually centered around a grammar and transformations\nthat generate implementation code or analysis models. The semantics of the\nlanguage is often defined implicitly and in terms of a transformation to\nimplementation code. In the presence of multiple transformations from the DSL,\nthe correctness of the generated artifacts with respect to the semantics of the\nDSL is a relevant issue. We show that a formal semantics is essential for\nchecking the correctness of the generated artifacts. We exploit the formal\nsemantics in an industrial project and use formal techniques based on\nequivalence checking and model-based testing for validating the correctness of\nthe generated artifacts. We report about our experience with this approach in\nan industrial development project.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 04:34:04 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Keshishzadeh", "Sarmen", "", "Eindhoven University of Technology"], ["Mooij", "Arjan J.", "", "Embedded Systems Innovation by TNO"], ["Hooman", "Jozef", "", "Embedded Systems\n  Innovation by TNO and Radboud University Nijmegen"]]}, {"id": "1603.08648", "submitter": "Moez AbdelGawad", "authors": "Moez A. AbdelGawad", "title": "A Comparison of NOOP to Structural Domain-Theoretic Models of OOP", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mainstream object-oriented programming languages such as Java, C#, C++ and\nScala are all almost entirely nominally-typed. NOOP is a recently developed\ndomain-theoretic model of OOP that was designed to include full nominal\ninformation found in nominally-typed OOP. This paper compares NOOP to the most\nwidely known domain-theoretic models of OOP, namely, the models developed by\nCardelli and Cook, which were structurally-typed models. Leveraging the\ndevelopment of NOOP, the comparison presented in this paper provides a clear\nand precise mathematical account for the relation between nominal and\nstructural OO type systems.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 05:35:34 GMT"}, {"version": "v2", "created": "Sun, 1 May 2016 02:25:18 GMT"}, {"version": "v3", "created": "Fri, 29 Dec 2017 17:39:07 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["AbdelGawad", "Moez A.", ""]]}, {"id": "1603.08769", "submitter": "Andrew Gacek", "authors": "Tuan-Hung Pham, Andrew Gacek, Michael W. Whalen", "title": "Reasoning about Algebraic Data Types with Abstractions", "comments": "To appear in Journal of Automated Reasoning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning about functions that operate over algebraic data types is an\nimportant problem for a large variety of applications. One application of\nparticular interest is network applications that manipulate or reason about\ncomplex message structures, such as XML messages. This paper presents a\ndecision procedure for reasoning about algebraic data types using abstractions\nthat are provided by catamorphisms: fold functions that map instances of\nalgebraic data types to values in a decidable domain. We show that the\nprocedure is sound and complete for a class of catamorphisms that satisfy a\ngeneralized sufficient surjectivity condition. Our work extends a previous\ndecision procedure that unrolls catamorphism functions until a solution is\nfound.\n  We use the generalized sufficient surjectivity condition to address an\nincompleteness in the previous unrolling algorithm (and associated proof). We\nthen propose the categories of monotonic and associative catamorphisms, which\nwe argue provide a more intuitive inclusion test than the generalized\nsufficient surjectivity condition. We use these notions to address two open\nproblems from previous work: (1) we provide a bound, with respect to formula\nsize, on the number of unrollings necessary for completeness, showing that it\nis linear for monotonic catamorphisms and exponentially small for associative\ncatamorphisms, and (2) we demonstrate that associative catamorphisms can be\ncombined within a formula while preserving completeness. Our combination\nresults extend the set of problems that can be reasoned about using the\ncatamorphism-based approach.\n  We also describe an implementation of the approach, called RADA, which\naccepts formulas in an extended version of the SMT-LIB 2.0 syntax. The\nprocedure is quite general and is central to the reasoning infrastructure for\nGuardol, a domain-specific language for reasoning about network guards.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 13:35:24 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Pham", "Tuan-Hung", ""], ["Gacek", "Andrew", ""], ["Whalen", "Michael W.", ""]]}, {"id": "1603.08949", "submitter": "Cl\\'audio Vasconcelos", "authors": "Cl\\'audio Vasconcelos and Ant\\'onio Ravara", "title": "The While language", "comments": "15 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a formalisation of a simple imperative programming\nlanguage. The objective is to study and develop \"hands-on\" a formal\nspecifcation of a programming language, namely its syntax, operational\nsemantics and type system. To have an executable version of the language, we\nimplemented in Racket its operational semantics and type system.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 20:30:27 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 10:49:48 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Vasconcelos", "Cl\u00e1udio", ""], ["Ravara", "Ant\u00f3nio", ""]]}, {"id": "1603.09274", "submitter": "Somnath Mazumdar", "authors": "Somnath Mazumdar, Roberto Giorgi", "title": "A Survey on Hardware and Software Support for Thread Level Parallelism", "comments": "Due to possible unethical use of the paper before publish in journal,\n  we are with drawing the submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To support growing massive parallelism, functional components and also the\ncapabilities of current processors are changing and continue to do so. Todays\ncomputers are built upon multiple processing cores and run applications\nconsisting of a large number of threads, making runtime thread management a\ncomplex process. Further, each core can support multiple, concurrent thread\nexecution. Hence, hardware and software support for threads is more and more\nneeded to improve peak-performance capacity, overall system throughput, and has\ntherefore been the subject of much research. This paper surveys, many of the\nproposed or currently available solutions for executing, distributing and\nmanaging threads both in hardware and software. The nature of current\napplications is diverse. To increase the system performance, all programming\nmodels may not be suitable to harness the built-in massive parallelism of\nmulticore processors. Due to the heterogeneity in hardware, hybrid programming\nmodel (which combines the features of shared and distributed model) currently\nhas become very promising. In this paper, first, we have given an overview of\nthreads, threading mechanisms and its management issues during execution. Next,\nwe discuss about different parallel programming models considering to their\nexplicit thread support. We also review the programming models with respect to\ntheir support to shared-memory, distributed-memory and heterogeneity. Hardware\nsupport at execution time is very crucial to the performance of the system,\nthus different types of hardware support for threads also exist or have been\nproposed, primarily based on widely used programming models. We also further\ndiscuss on software support for threads, to mainly increase the deterministic\nbehavior during runtime. Finally, we conclude the paper by discussing some\ncommon issues related to the thread management.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 16:49:31 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 19:07:08 GMT"}, {"version": "v3", "created": "Wed, 6 Apr 2016 09:21:28 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Mazumdar", "Somnath", ""], ["Giorgi", "Roberto", ""]]}, {"id": "1603.09290", "submitter": "Andres N\\\"otzli", "authors": "Andres N\\\"otzli, Fraser Brown", "title": "LifeJacket: Verifying precise floating-point optimizations in LLVM", "comments": "Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing floating-point arithmetic is vital because it is ubiquitous,\ncostly, and used in compute-heavy workloads. Implementing precise optimizations\ncorrectly, however, is difficult, since developers must account for all the\nesoteric properties of floating-point arithmetic to ensure that their\ntransformations do not alter the output of a program. Manual reasoning is error\nprone and stifles incorporation of new optimizations. We present an approach to\nautomate reasoning about floating-point optimizations using satisfiability\nmodulo theories (SMT) solvers. We implement the approach in LifeJacket, a\nsystem for automatically verifying precise floating-point optimizations for the\nLLVM assembly language. We have used LifeJacket to verify 43 LLVM optimizations\nand to discover eight incorrect ones, including three previously unreported\nproblems. LifeJacket is an open source extension of the Alive system for\noptimization verification.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 17:46:57 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["N\u00f6tzli", "Andres", ""], ["Brown", "Fraser", ""]]}, {"id": "1603.09597", "submitter": "Pritam Gharat", "authors": "Pritam M. Gharat, Uday P. Khedker, Alan Mycroft", "title": "Flow- and Context-Sensitive Points-to Analysis using Generalized\n  Points-to Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing precise (fully flow-sensitive and context-sensitive) and exhaustive\npoints-to information is computationally expensive. Many practical tools\napproximate the points-to information trading precision for efficiency. This\nhas adverse impact on computationally intensive analyses such as model\nchecking. Past explorations in top-down approaches of fully flow- and\ncontext-sensitive points-to analysis (FCPA) have not scaled. We explore the\nalternative of bottom-up interprocedural approach which constructs summary flow\nfunctions for procedures to represent the effect of their calls. This approach\nhas been effectively used for many analyses. However, it is computationally\nexpensive for FCPA which requires modelling unknown locations accessed\nindirectly through pointers. Such accesses are commonly handled by using\nplaceholders to explicate unknown locations or by using multiple call-specific\nsummary flow functions.\n  We generalize the concept of points-to relations by using the counts of\nindirection levels leaving the unknown locations implicit. This allows us to\ncreate summary flow functions in the form of generalized points-to graphs\n(GPGs) without the need of placeholders. By design, GPGs represent both memory\n(in terms of classical points-to facts) and memory transformers (in terms of\ngeneralized points-to facts). We perform FCPA by progressively reducing\ngeneralized points-to facts to classical points-to facts. GPGs distinguish\nbetween may and must pointer updates thereby facilitating strong updates within\ncalling contexts.\n  The size of GPG is linearly bounded by the number of variables and is\nindependent of the number of statements in the procedure. Empirical\nmeasurements on SPEC benchmarks show that GPGs are indeed compact in spite of\nlarge procedure sizes. This allows us to scale FCPA to 158 kLoC using GPGs\n(compared to 35 kLoC reported by liveness-based FCPA).\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2016 14:05:30 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 13:29:37 GMT"}, {"version": "v3", "created": "Thu, 14 Apr 2016 14:51:30 GMT"}, {"version": "v4", "created": "Wed, 20 Apr 2016 15:45:36 GMT"}, {"version": "v5", "created": "Thu, 4 Aug 2016 15:48:56 GMT"}, {"version": "v6", "created": "Fri, 5 Aug 2016 04:46:29 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Gharat", "Pritam M.", ""], ["Khedker", "Uday P.", ""], ["Mycroft", "Alan", ""]]}]