[{"id": "2009.00391", "submitter": "EPTCS", "authors": "Davide Ancona (DIBRIS, University of Genova, Italy), Angelo Ferrando\n  (University of Manchester, UK), Viviana Mascardi (DIBRIS, University of\n  Genova, Italy)", "title": "Can determinism and compositionality coexist in RML?", "comments": "In Proceedings EXPRESS/SOS 2020, arXiv:2008.12414. Author extended\n  version is arXiv:2008.06453", "journal-ref": "EPTCS 322, 2020, pp. 13-32", "doi": "10.4204/EPTCS.322.4", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Runtime verification (RV) consists in dynamically verifying that the event\ntraces generated by single runs of a system under scrutiny (SUS) are compliant\nwith the formal specification of its expected properties. RML (Runtime\nMonitoring Language) is a simple but expressive Domain Specific Language for\nRV; its semantics is based on a trace calculus formalized by a deterministic\nrewriting system which drives the implementation of the interpreter of the\nmonitors generated by the RML compiler from the specifications. While\ndeterminism of the trace calculus ensures better performances of the generated\nmonitors, it makes the semantics of its operators less intuitive. In this paper\nwe move a first step towards a compositional semantics of the RML trace\ncalculus, by interpreting its basic operators as operations on sets of\ninstantiated event traces and by proving that such an interpretation is\nequivalent to the operational semantics of the calculus.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 04:34:11 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Ancona", "Davide", "", "DIBRIS, University of Genova, Italy"], ["Ferrando", "Angelo", "", "University of Manchester, UK"], ["Mascardi", "Viviana", "", "DIBRIS, University of\n  Genova, Italy"]]}, {"id": "2009.00583", "submitter": "Catherine Dubois", "authors": "Catherine Dubois", "title": "Formally Verified Transformation of Non-binary Constraints into Binary\n  Constraints", "comments": "Part of WFLP 2020 pre-proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known in the Constraint Programming community that any non-binary\nconstraint satisfaction problem (with finite domains) can be transformed into\nan equivalent binary one. One of the most well-known translations is the Hidden\nVariable Encoding. In this paper we formalize this encoding in the proof\nassistant Coq and prove that any solution of the binary constraint satisfaction\nproblem makes it possible to build a solution of the original problem and\nvice-versa. This formal development is used to complete the formally verified\nconstraint solver developed in Coq by Carlier, Dubois and Gotlieb in 2012,\nmaking it a tool able to solve any n-ary constraint satisfaction problem, The\nkey of success of the connection between the translator and the Coq binary\nsolver is the genericity of the latter.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 17:15:06 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Dubois", "Catherine", ""]]}, {"id": "2009.00637", "submitter": "Hongbo Rong", "authors": "Hongbo Rong", "title": "Building Application-Specific Overlays on FPGAs with High-Level\n  Customizable IPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overlays are virtual, re-configurable architectures that overlay on top of\nphysical FPGA fabrics. An overlay that is specialized for an application, or a\nclass of applications, offers both fast reconfiguration and minimized\nperformance penalty. Such an overlay is usually implemented by hardware\ndesigners in hardware \"assembly\" languages at register-transfer level (RTL).\n  This short article proposes an idea for a software programmer, instead of\nhardware designers, to quickly implement an application-specific overlay using\nhigh-level customizable IPs. These IPs are expressed succinctly by a\nspecification language, whose abstraction level is much higher than RTL but can\nnonetheless expresses many performance-critical loop and data optimizations on\nFPGAs, and thus would offer competitively high performance at a much lower cost\nof maintenance and much easier customizations.\n  We propose new language features to easily put the IPs together into an\noverlay. A compiler automatically implements the specified optimizations to\ngenerate an efficient overlay, exposes a multi-tasking programming interface\nfor the overlay, and inserts a runtime scheduler for scheduling tasks to run on\nthe IPs of the overlay, respecting the dependences between the tasks. While an\napplication written in any language can take advantage of the overlay through\nthe programming interface, we show a particular usage scenario, where the\napplication itself is also succinctly specified in the same language.\n  We describe the new language features for expressing overlays, and illustrate\nthe features with an LU decomposer and a convolutional neural network. A system\nis under construction to implement the language features and workloads.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2020 18:08:24 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Rong", "Hongbo", ""]]}, {"id": "2009.00997", "submitter": "Juan Manuel Serrano", "authors": "J. L\\'opez-Gonz\\'alez, Juan M. Serrano", "title": "The Optics of Language-Integrated Query", "comments": null, "journal-ref": "Science of Computer Programming, Volume 190, 2020, 102395, ISSN\n  0167-6423", "doi": "10.1016/j.scico.2020.102395", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Monadic comprehensions reign over the realm of language-integrated query\n(LINQ), and for good reasons. Indeed, comprehensions are tightly integrated\nwith general purpose programming languages and close enough to common query\nlanguages, such as SQL, to guarantee their translation into effective queries.\nComprehensions also support features for writing reusable and composable\nqueries, such as the handling of nested data and the use of functional\nabstractions. In parallel to these developments, optics have emerged in recent\nyears as the technology of choice to write programs that manipulate complex\ndata structures with nested components. Optic abstractions are easily\ncomposable and, in principle, permit both data access and updates. This paper\nattempts to exploit the notion of optic for LINQ as a higher-level language\nthat complements comprehension-based approaches. In order to do this, we lift a\nrestricted subset of optics, namely getters, affine folds and folds, into a\nfull-blown DSL. The type system of the resulting language of optics, that we\nhave named Optica, distills their compositional properties, whereas its\ndenotational semantics is given by standard optics. This formal specification\nof the concept of optic enables the definition of non-standard optic\nrepresentations beyond van Laarhoven, profunctor optics, etc. In particular,\nthe paper demonstrates that a restricted subset of XQuery can be understood as\nan optic representation; it introduces Triplets, a non-standard semantic domain\nto normalize optic expressions and facilitate the generation of SQL queries;\nand it describes how to generate comprehension-based queries from optic\nexpressions, thus showing that both approaches can coexist.The paper also\ndescribes S-Optica, a Scala implementation of Optica using the tagless-final\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 12:19:53 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["L\u00f3pez-Gonz\u00e1lez", "J.", ""], ["Serrano", "Juan M.", ""]]}, {"id": "2009.01001", "submitter": "Michael Hanus", "authors": "Michael Hanus, Claudio Sacerdoti Coen", "title": "Pre-Proceedings of the 28th International Workshop on Functional and\n  Logic Programming (WFLP 2020)", "comments": "Pre-Proceedings of WFLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume constitutes the pre-proceedings of the 28th International\nWorkshop on Functional and Logic Programming (WFLP 2020), organized by the\nUniversity of Bologna, Italy, as part of Bologna Federated Conference on\nProgramming Languages 2020. The international Workshop on Functional and\n(constraint) Logic Programming (WFLP) aims at bringing together researchers,\nstudents, and practitioners interested in functional programming, logic\nprogramming, and their integration. WFLP has a reputation for being a lively\nand friendly forum, and it is open for presenting and discussing work in\nprogress, technical contributions, experience reports, experiments, reviews,\nand system descriptions.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 12:33:02 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 10:29:28 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Hanus", "Michael", ""], ["Coen", "Claudio Sacerdoti", ""]]}, {"id": "2009.01020", "submitter": "Samuel Steffen", "authors": "Nick Baumann, Samuel Steffen, Benjamin Bichsel, Petar Tsankov, Martin\n  Vechev", "title": "zkay v0.2: Practical Data Privacy for Smart Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work introduces zkay, a system for specifying and enforcing data\nprivacy in smart contracts. While the original prototype implementation of zkay\n(v0.1) demonstrates the feasibility of the approach, its proof-of-concept\nimplementation suffers from severe limitations such as insecure encryption and\nlack of important language features.\n  In this report, we present zkay v0.2, which addresses its predecessor's\nlimitations. The new implementation significantly improves security, usability,\nmodularity, and performance of the system. In particular, zkay v0.2 supports\nstate-of-the-art asymmetric and hybrid encryption, introduces many new language\nfeatures (such as function calls, private control flow, and extended type\nsupport), allows for different zk-SNARKs backends, and reduces both compilation\ntime and on-chain costs.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 12:51:21 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 06:42:12 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Baumann", "Nick", ""], ["Steffen", "Samuel", ""], ["Bichsel", "Benjamin", ""], ["Tsankov", "Petar", ""], ["Vechev", "Martin", ""]]}, {"id": "2009.01326", "submitter": "Johannes Waldmann", "authors": "Dennis Renz and Sibylle Schwarz and Johannes Waldmann", "title": "Check Your (Students') Proofs-With Holes", "comments": "Part of WFLP 2020 pre-proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyp (Check Your Proofs) (Durner and Noschinski 2013; Traytel 2019) verifies\nproofs about Haskell-like programs. We extended Cyp with a pattern matcher for\nprograms and proof terms, and a type checker.\n  This allows to use Cyp for auto-grading exercises where the goal is to\ncomplete programs and proofs that are partially given by the instructor, as\nterms with holes. Since this allows holes in programs, type-checking becomes\nessential. Before, Cyp assumed that the program was written by a type-correct\ninstructor, and therefore omitted type-checking of proofs. Cyp gracefully\nhandles incomplete student submissions. It accepts holes temporarily, and\nchecks complete subtrees fully.\n  We present basic design decisions, make some remarks on implementation, and\ninclude example exercises from a recent course that used Cyp as part of the\nLeipzig Autotool auto-grading system.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 20:04:02 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Renz", "Dennis", ""], ["Schwarz", "Sibylle", ""], ["Waldmann", "Johannes", ""]]}, {"id": "2009.01399", "submitter": "Jianping Kelvin Li", "authors": "Jianping Kelvin Li and Kwan-Liu Ma", "title": "P6: A Declarative Language for Integrating Machine Learning in Visual\n  Analytics", "comments": "Accepted for presentation at IEEE VIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present P6, a declarative language for building high performance visual\nanalytics systems through its support for specifying and integrating machine\nlearning and interactive visualization methods. As data analysis methods based\non machine learning and artificial intelligence continue to advance, a visual\nanalytics solution can leverage these methods for better exploiting large and\ncomplex data. However, integrating machine learning methods with interactive\nvisual analysis is challenging. Existing declarative programming libraries and\ntoolkits for visualization lack support for coupling machine learning methods.\nBy providing a declarative language for visual analytics, P6 can empower more\ndevelopers to create visual analytics applications that combine machine\nlearning and visualization methods for data analysis and problem solving.\nThrough a variety of example applications, we demonstrate P6's capabilities and\nshow the benefits of using declarative specifications to build visual analytics\nsystems. We also identify and discuss the research opportunities and challenges\nfor declarative visual analytics.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 00:58:02 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Li", "Jianping Kelvin", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2009.01489", "submitter": "Kirshanthan Sundararajah", "authors": "Yuyan Bao, Kirshanthan Sundararajah, Raghav Malik, Qianchuan Ye,\n  Christopher Wagner, Nouraldin Jaber, Fei Wang, Mohammad Hassan Ameri,\n  Donghang Lu, Alexander Seto, Benjamin Delaware, Roopsha Samanta, Aniket Kate,\n  Christina Garman, Jeremiah Blocki, Pierre-David Letourneau, Benoit Meister,\n  Jonathan Springer, Tiark Rompf, Milind Kulkarni", "title": "HACCLE: An Ecosystem for Building Secure Multi-Party Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryptographic techniques have the potential to enable distrusting parties to\ncollaborate in fundamentally new ways, but their practical implementation poses\nnumerous challenges. An important class of such cryptographic techniques is\nknown as secure multi-party computation (MPC). In an effort to provide an\necosystem for building secure MPC applications using higher degrees of\nautomation, we present the HACCLE (High Assurance Compositional Cryptography:\nLanguages and Environments) toolchain. The HACCLE toolchain contains an\nembedded domain-specific language (Harpoon) for software developers without\ncryptographic expertise to write MPC-based programs. Harpoon programs are\ncompiled into acyclic circuits represented in HACCLE's Intermediate\nRepresentation (HIR) that serves as an abstraction for implementing a\ncomputation using different cryptographic protocols such as secret sharing,\nhomomorphic encryption, or garbled circuits. Implementations of different\ncryptographic protocols serve as different backends of our toolchain. The\nextensible design of HIR allows cryptographic experts to plug in new primitives\nand protocols to realize computations.We have implemented HACCLE, and used it\nto program interesting algorithms and applications (e.g., secure auction,\nmatrix-vector multiplication, and merge sort). We show that the performance is\nimproved by using our optimization strategies and heuristics.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 07:08:00 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 15:52:13 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Bao", "Yuyan", ""], ["Sundararajah", "Kirshanthan", ""], ["Malik", "Raghav", ""], ["Ye", "Qianchuan", ""], ["Wagner", "Christopher", ""], ["Jaber", "Nouraldin", ""], ["Wang", "Fei", ""], ["Ameri", "Mohammad Hassan", ""], ["Lu", "Donghang", ""], ["Seto", "Alexander", ""], ["Delaware", "Benjamin", ""], ["Samanta", "Roopsha", ""], ["Kate", "Aniket", ""], ["Garman", "Christina", ""], ["Blocki", "Jeremiah", ""], ["Letourneau", "Pierre-David", ""], ["Meister", "Benoit", ""], ["Springer", "Jonathan", ""], ["Rompf", "Tiark", ""], ["Kulkarni", "Milind", ""]]}, {"id": "2009.01686", "submitter": "Xiang Fu", "authors": "The Quingo Development Team", "title": "Quingo: A Programming Framework for Heterogeneous Quantum-Classical\n  Computing with NISQ Features", "comments": "28 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy Intermediate-Scale Quantum (NISQ) technology proposes requirements that\ncannot be fully satisfied by existing Quantum Programming Languages (QPLs) or\nframeworks. First, noisy qubits require repeatedly-performed quantum\nexperiments, which explicitly operate some low-level configuration, such as\npulses and timing of operations. This requirement is beyond the scope or\ncapability of most existing QPLs. Though multiple existing QPLs or frameworks\nclaim the support for near-term promising Heterogeneous Quantum-Classical\nComputing (HQCC) algorithms, extra code irrelevant to the computational steps\nhas to be introduced, or the corresponding code can hardly be mapped to HQCC\narchitectures while satisfying timing constraints in quantum-classical\ninteraction.\n  In this paper, we propose Quingo, a modular programming framework for HQCC\nwith NISQ features. Quingo highlights an external domain-specific language with\ntimer-based timing control and opaque operation definition. By adopting a\nsix-phase quantum program life-cycle model, Quingo enables aggressive\noptimization over quantum code through Just-In-Time compilation while\npreserving quantum-classical interaction with timing constraints satisfied. We\npropose a runtime system with a prototype design implemented in Python, which\ncan orchestrate both quantum and classical software and hardware according to\nthe six-phase life-cycle model. It allows components of the framework to focus\non their genuine task, thus achieving a modular programming framework.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2020 06:42:51 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["The Quingo Development Team", "", ""]]}, {"id": "2009.01876", "submitter": "Felicien Ihirwe", "authors": "Felicien Ihirwe, Davide Di Ruscio, Silvia Mazzini, Pierluigi Pierini,\n  Alfonso Pierantonio", "title": "Low-code Engineering for Internet of things: A state of research", "comments": "8 pages, 3 figures, 1 table", "journal-ref": "MODELS '20: Proceedings of the 23rd ACM/IEEE International\n  Conference on Model Driven Engineering Languages and Systems: Companion\n  Proceedings. October 2020", "doi": "10.1145/3417990.3420208", "report-no": "74", "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing Internet of Things (IoT) systems has to cope with several\nchallenges mainly because of the heterogeneity of the involved sub-systems and\ncomponents. With the aim of conceiving languages and tools supporting the\ndevelopment of IoT systems, this paper presents the results of the study, which\nhas been conducted to understand the current state of the art of existing\nplatforms, and in particular low-code ones, for developing IoT systems. By\nanalyzing sixteen platforms, a corresponding set of features has been\nidentified to represent the functionalities and the services that each analyzed\nplatform can support. We also identify the limitations of already existing\napproaches and discuss possible ways to improve and address them in the future.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2020 18:41:37 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 12:15:45 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ihirwe", "Felicien", ""], ["Di Ruscio", "Davide", ""], ["Mazzini", "Silvia", ""], ["Pierini", "Pierluigi", ""], ["Pierantonio", "Alfonso", ""]]}, {"id": "2009.02731", "submitter": "Nghi D. Q. Bui", "authors": "Nghi D. Q. Bui, Yijun Yu, Lingxiao Jiang", "title": "Self-Supervised Contrastive Learning for Code Retrieval and\n  Summarization via Semantic-Preserving Transformations", "comments": "Accepted at SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3462840", "report-no": null, "categories": "cs.SE cs.AI cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Corder, a self-supervised contrastive learning framework for\nsource code model. Corder is designed to alleviate the need of labeled data for\ncode retrieval and code summarization tasks. The pre-trained model of Corder\ncan be used in two ways: (1) it can produce vector representation of code which\ncan be applied to code retrieval tasks that do not have labeled data; (2) it\ncan be used in a fine-tuning process for tasks that might still require label\ndata such as code summarization. The key innovation is that we train the source\ncode model by asking it to recognize similar and dissimilar code snippets\nthrough a contrastive learning objective. To do so, we use a set of\nsemantic-preserving transformation operators to generate code snippets that are\nsyntactically diverse but semantically equivalent. Through extensive\nexperiments, we have shown that the code models pretrained by Corder\nsubstantially outperform the other baselines for code-to-code retrieval,\ntext-to-code retrieval, and code-to-text summarization tasks.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 13:31:16 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 05:38:39 GMT"}, {"version": "v3", "created": "Sat, 16 Jan 2021 12:12:51 GMT"}, {"version": "v4", "created": "Tue, 19 Jan 2021 11:46:56 GMT"}, {"version": "v5", "created": "Wed, 20 Jan 2021 09:49:11 GMT"}, {"version": "v6", "created": "Sun, 2 May 2021 19:25:45 GMT"}, {"version": "v7", "created": "Mon, 17 May 2021 18:01:27 GMT"}, {"version": "v8", "created": "Sun, 23 May 2021 12:10:55 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Bui", "Nghi D. Q.", ""], ["Yu", "Yijun", ""], ["Jiang", "Lingxiao", ""]]}, {"id": "2009.02775", "submitter": "Suvam Mukherjee", "authors": "Suvam Mukherjee, Oded Padon, Sharon Shoham, Deepak D'Souza, Noam\n  Rinetzky", "title": "A Thread-Local Semantics and Efficient Static Analyses for Race Free\n  Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data race free (DRF) programs constitute an important class of concurrent\nprograms. In this paper we provide a framework for designing and proving the\ncorrectness of data flow analyses that target this class of programs. These\nanalyses are in the same spirit as the \"sync-CFG\" analysis proposed in earlier\nliterature. To achieve this, we first propose a novel concrete semantics for\nDRF programs, called L-DRF, that is thread-local in nature---each thread\noperates on its own copy of the data state. We show that abstractions of our\nsemantics allow us to reduce the analysis of DRF programs to a sequential\nanalysis. This aids in rapidly porting existing sequential analyses to sound\nand scalable analyses for DRF programs. Next, we parameterize L-DRF with a\npartitioning of the program variables into \"regions\" which are accessed\natomically. Abstractions of the region-parameterized semantics yield more\nprecise analyses for \"region-race\" free concurrent programs. We instantiate\nthese abstractions to devise efficient relational analyses for race free\nprograms, which we have implemented in a prototype tool called RATCOP. On the\nbenchmarks, RATCOP was able to prove up to 65% of the assertions, in comparison\nto 25% proved by our baseline. Moreover, in a comparative study with a recent\nconcurrent static analyzer, RATCOP was up to 5 orders of magnitude faster.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2020 17:01:51 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Mukherjee", "Suvam", ""], ["Padon", "Oded", ""], ["Shoham", "Sharon", ""], ["D'Souza", "Deepak", ""], ["Rinetzky", "Noam", ""]]}, {"id": "2009.03846", "submitter": "Soham Chakraborty", "authors": "Soham Chakraborty", "title": "On Architecture to Architecture Mapping for Concurrency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping programs from one architecture to another plays a key role in\ntechnologies such as binary translation, decompilation, emulation,\nvirtualization, and application migration. Although multicore architectures are\nubiquitous, the state-of-the-art translation tools do not handle concurrency\nprimitives correctly. Doing so is rather challenging because of the subtle\ndifferences in the concurrency models between architectures.\n  In response, we address various aspects of the challenge. First, we develop\ncorrect and efficient translations between the concurrency models of two\nmainstream architecture families: x86 and ARM (versions 7 and 8). We develop\ndirect mappings between x86 and ARMv8 and ARMv7, and fence elimination\nalgorithms to eliminate redundant fences after direct mapping. Although our\nmapping utilizes ARMv8 as an intermediate model for mapping between x86 and\nARMv7, we argue that it should not be used as an intermediate model in a\ndecompiler because it disallows common compiler transformations.\n  Second, we propose and implement a technique for inserting memory fences for\nsafely migrating programs between different architectures. Our technique checks\nrobustness against x86 and ARM, and inserts fences upon robustness violations.\nOur experiments demonstrate that in most of the programs both our techniques\nintroduce significantly fewer fences compared to naive schemes for porting\napplications across these architectures.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2020 16:28:28 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Chakraborty", "Soham", ""]]}, {"id": "2009.04437", "submitter": "Ori Roth", "authors": "Joseph Gil, Ori Roth", "title": "Ties between Parametrically Polymorphic Type Systems and Finite Control\n  Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a correspondence and bisimulation between variants of\nparametrically polymorphic type systems and variants of finite control\nautomata, such as FSA, PDA, tree automata and Turing machine. Within this\ncorrespondence we show that two recent celebrated results on automatic\ngeneration of fluent API are optimal in certain senses, present new results on\nthe studied type systems, formulate open problems, and present potential\nsoftware engineering applications, other than fluent API generation, which may\nbenefit from judicious use of type theory.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2020 17:32:38 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 13:56:15 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 16:27:06 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Gil", "Joseph", ""], ["Roth", "Ori", ""]]}, {"id": "2009.04826", "submitter": "Eytan Singher", "authors": "Eytan Singher, Shachar Itzhaky", "title": "Theory Exploration Powered By Deductive Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen tremendous growth in the amount of verified software.\nProofs for complex properties can now be achieved using higher-order theories\nand calculi. Complex properties lead to an ever-growing number of definitions\nand associated lemmas, which constitute an integral part of proof construction.\nFollowing this -- whether automatic or semi-automatic -- methods for\ncomputer-aided lemma discovery have emerged. In this work, we introduce a new\nsymbolic technique for bottom-up lemma discovery, that is, the generation of a\nlibrary of lemmas from a base set of inductive data types and recursive\ndefinitions. This is known as the theory exploration problem, and so far,\nsolutions have been proposed based either on counter-example generation or the\nmore prevalent random testing combined with first-order solvers. Our new\napproach, being purely deductive, eliminates the need for random testing as a\nfiltering phase and for SMT solvers. Therefore it is amenable compositional\nreasoning and for the treatment of user-defined higher-order functions. Our\nimplementation has shown to find more lemmas than prior art, while avoiding\nredundancy.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 12:53:56 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Singher", "Eytan", ""], ["Itzhaky", "Shachar", ""]]}, {"id": "2009.04909", "submitter": "Alexander Vandenbroucke", "authors": "Alexander Vandenbroucke and Tom Schrijvers", "title": "Disjunctive Delimited Control", "comments": "15 pages, excluding references & appendices; 36 pages total", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Delimited control is a powerful mechanism for programming language extension\nwhich has been recently proposed for Prolog (and implemented in SWI-Prolog). By\nmanipulating the control flow of a program from inside the language, it enables\nthe implementation of powerful features, such as tabling, without modifying the\ninternals of the Prolog engine. However, its current formulation is inadequate:\nit does not capture Prolog's unique non-deterministic nature which allows\nmultiple ways to satisfy a goal. This paper fully embraces Prolog's\nnon-determinism with a novel interface for disjunctive delimited control, which\ngives the programmer not only control over the sequential (conjunctive) control\nflow, but also over the non-deterministic control flow. We provide a\nmeta-interpreter that conservatively extends Prolog with delimited control and\nshow that it enables a range of typical Prolog features and extensions, now at\nthe library level: findall, cut, branch-and-bound optimisation, probabilistic\nprogramming, . . .\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 14:54:38 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Vandenbroucke", "Alexander", ""], ["Schrijvers", "Tom", ""]]}, {"id": "2009.05090", "submitter": "Piyush Gupta", "authors": "Piyush Gupta, Nikita Mehrotra, Rahul Purandare", "title": "JCoffee: Using Compiler Feedback to Make Partial Code Snippets\n  Compilable", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static program analysis tools are often required to work with only a small\npart of a program's source code, either due to the unavailability of the entire\nprogram or the lack of need to analyze the complete code. This makes it\nchallenging to use static analysis tools that require a complete and typed\nintermediate representation (IR). We present JCoffee, a tool that leverages\ncompiler feedback to convert partial Java programs into their compilable\ncounterparts by simulating the presence of missing surrounding code. It works\nwith any well-typed code snippet (class, function, or even an unenclosed group\nof statements) while making minimal changes to the input code fragment. A demo\nof the tool is available here: https://youtu.be/O4h2g_n2Qls\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2020 18:51:00 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Gupta", "Piyush", ""], ["Mehrotra", "Nikita", ""], ["Purandare", "Rahul", ""]]}, {"id": "2009.05314", "submitter": "Thom Fruehwirth", "authors": "Thom Fruehwirth", "title": "Repeated Recursion Unfolding for Super-Linear Speedup within Bounds", "comments": "This is the full version of a paper presented at the 30th\n  International Symposium on Logic-Based Program Synthesis and Transformation\n  (LOPSTR 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Repeated recursion unfolding is a new approach that repeatedly unfolds a\nrecursion with itself and simplifies it while keeping all unfolded rules. Each\nunfolding doubles the number of recursive steps covered. This reduces the\nnumber of recursive rule applications to its logarithm at the expense of\nintroducing a logarithmic number of unfolded rules to the program. Efficiency\ncrucially depends on the amount of simplification inside the unfolded rules. We\nprove a super-linear speedup theorem in the best case, i.e. speedup by more\nthan a constant factor. Our optimization can lower the time complexity class of\na program. In this paper, the super-linear speedup is within bounds: it holds\nup to an arbitrary but chosen upper bound on the number of recursive steps. We\nalso report on the first results with a prototype implementation of repeated\nrecursion unfolding. A simple program transformation completely removes\nrecursion up to the chosen bound. The actual runtime improvement quickly\nreaches several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 09:59:00 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Fruehwirth", "Thom", ""]]}, {"id": "2009.05547", "submitter": "Carlo Angiuli", "authors": "Carlo Angiuli, Evan Cavallo, Anders M\\\"ortberg, Max Zeuner", "title": "Internalizing Representation Independence with Univalence", "comments": "30 pages; ACM SIGPLAN Symposium on Principles of Programming\n  Languages (POPL), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In their usual form, representation independence metatheorems provide an\nexternal guarantee that two implementations of an abstract interface are\ninterchangeable when they are related by an operation-preserving\ncorrespondence. If our programming language is dependently-typed, however, we\nwould like to appeal to such invariance results within the language itself, in\norder to obtain correctness theorems for complex implementations by\ntransferring them from simpler, related implementations. Recent work in proof\nassistants has shown that Voevodsky's univalence principle allows transferring\ntheorems between isomorphic types, but many instances of representation\nindependence in programming involve non-isomorphic representations.\n  In this paper, we develop techniques for establishing internal relational\nrepresentation independence results in dependent type theory, by using higher\ninductive types to simultaneously quotient two related implementation types by\na heterogeneous correspondence between them. The correspondence becomes an\nisomorphism between the quotiented types, thereby allowing us to obtain an\nequality of implementations by univalence. We illustrate our techniques by\nconsidering applications to matrices, queues, and finite multisets. Our results\nare all formalized in Cubical Agda, a recent extension of Agda which supports\nunivalence and higher inductive types in a computationally well-behaved way.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 17:29:29 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 21:18:25 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Angiuli", "Carlo", ""], ["Cavallo", "Evan", ""], ["M\u00f6rtberg", "Anders", ""], ["Zeuner", "Max", ""]]}, {"id": "2009.05632", "submitter": "Erik Wittern", "authors": "Alan Cha and Erik Wittern and Guillaume Baudart and James C. Davis and\n  Louis Mandel and Jim A. Laredo", "title": "A Principled Approach to GraphQL Query Cost Analysis", "comments": "Published at the ACM Joint European Software Engineering Conference\n  and Symposium on the Foundations of Software Engineering (ESEC/FSE) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The landscape of web APIs is evolving to meet new client requirements and to\nfacilitate how providers fulfill them. A recent web API model is GraphQL, which\nis both a query language and a runtime. Using GraphQL, client queries express\nthe data they want to retrieve or mutate, and servers respond with exactly\nthose data or changes. GraphQL's expressiveness is risky for service providers\nbecause clients can succinctly request stupendous amounts of data, and\nresponding to overly complex queries can be costly or disrupt service\navailability. Recent empirical work has shown that many service providers are\nat risk. Using traditional API management methods is not sufficient, and\npractitioners lack principled means of estimating and measuring the cost of the\nGraphQL queries they receive. In this work, we present a linear-time GraphQL\nquery analysis that can measure the cost of a query without executing it. Our\napproach can be applied in a separate API management layer and used with\narbitrary GraphQL backends. In contrast to existing static approaches, our\nanalysis supports common GraphQL conventions that affect query cost, and our\nanalysis is provably correct based on our formal specification of GraphQL\nsemantics. We demonstrate the potential of our approach using a novel GraphQL\nquery-response corpus for two commercial GraphQL APIs. Our query analysis\nconsistently obtains upper cost bounds, tight enough relative to the true\nresponse sizes to be actionable for service providers. In contrast, existing\nstatic GraphQL query analyses exhibit over-estimates and under-estimates\nbecause they fail to support GraphQL conventions.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 19:33:14 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Cha", "Alan", ""], ["Wittern", "Erik", ""], ["Baudart", "Guillaume", ""], ["Davis", "James C.", ""], ["Mandel", "Louis", ""], ["Laredo", "Jim A.", ""]]}, {"id": "2009.05660", "submitter": "Matthew Sotoudeh", "authors": "Matthew Sotoudeh and Aditya V. Thakur", "title": "Abstract Neural Networks", "comments": "Extended version of conference paper at the 27th Static Analysis\n  Symposium (SAS 2020). Code is available at\n  https://github.com/95616ARG/abstract_neural_networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are rapidly being applied to safety-critical\ndomains such as drone and airplane control, motivating techniques for verifying\nthe safety of their behavior. Unfortunately, DNN verification is NP-hard, with\ncurrent algorithms slowing exponentially with the number of nodes in the DNN.\nThis paper introduces the notion of Abstract Neural Networks (ANNs), which can\nbe used to soundly overapproximate DNNs while using fewer nodes. An ANN is like\na DNN except weight matrices are replaced by values in a given abstract domain.\nWe present a framework parameterized by the abstract domain and activation\nfunctions used in the DNN that can be used to construct a corresponding ANN. We\npresent necessary and sufficient conditions on the DNN activation functions for\nthe constructed ANN to soundly over-approximate the given DNN. Prior work on\nDNN abstraction was restricted to the interval domain and ReLU activation\nfunction. Our framework can be instantiated with other abstract domains such as\noctagons and polyhedra, as well as other activation functions such as Leaky\nReLU, Sigmoid, and Hyperbolic Tangent.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2020 21:17:38 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Sotoudeh", "Matthew", ""], ["Thakur", "Aditya V.", ""]]}, {"id": "2009.05865", "submitter": "Sung Kook Kim", "authors": "Sung Kook Kim, Arnaud J. Venet, Aditya V. Thakur", "title": "Memory-Efficient Fixpoint Computation", "comments": "Extended version of conference paper at the 27th Static Analysis\n  Symposium (SAS 2020). Code is available at\n  https://github.com/95616ARG/mikos_sas2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practical adoption of static analysis often requires trading precision for\nperformance. This paper focuses on improving the memory efficiency of abstract\ninterpretation without sacrificing precision or time efficiency.\nComputationally, abstract interpretation reduces the problem of inferring\nprogram invariants to computing a fixpoint of a set of equations. This paper\npresents a method to minimize the memory footprint in Bourdoncle's iteration\nstrategy, a widely-used technique for fixpoint computation. Our technique is\nagnostic to the abstract domain used. We prove that our technique is optimal\n(i.e., it results in minimum memory footprint) for Bourdoncle's iteration\nstrategy while computing the same result. We evaluate the efficacy of our\ntechnique by implementing it in a tool called MIKOS, which extends the\nstate-of-the-art abstract interpreter IKOS. When verifying user-provided\nassertions, MIKOS shows a decrease in peak-memory usage to 4.07% (24.57x) on\naverage compared to IKOS. When performing interprocedural buffer-overflow\nanalysis, MIKOS shows a decrease in peak-memory usage to 43.7% (2.29x) on\naverage compared to IKOS.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2020 20:51:52 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Kim", "Sung Kook", ""], ["Venet", "Arnaud J.", ""], ["Thakur", "Aditya V.", ""]]}, {"id": "2009.05949", "submitter": "Fangke Ye", "authors": "Fangke Ye, Jisheng Zhao, Vivek Sarkar", "title": "Advanced Graph-Based Deep Learning for Probabilistic Type Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamically typed languages such as JavaScript and Python have emerged as the\nmost popular programming languages in use. Important benefits can accrue from\nincluding type annotations in dynamically typed programs. This approach to\ngradual typing is exemplified by the TypeScript programming system which allows\nprogrammers to specify partially typed programs, and then uses static analysis\nto infer the remaining types. However, in general, the effectiveness of static\ntype inference is limited and depends on the complexity of the program's\nstructure and the initial type annotations. As a result, there is a strong\nmotivation for new approaches that can advance the state of the art in\nstatically predicting types in dynamically typed programs, and that do so with\nacceptable performance for use in interactive programming environments.\nPrevious work has demonstrated the promise of probabilistic type inference\nusing deep learning. In this paper, we advance past work by introducing a range\nof graph neural network (GNN) models that operate on a novel type flow graph\n(TFG) representation. The TFG represents an input program's elements as graph\nnodes connected with syntax edges and data flow edges, and our GNN models are\ntrained to predict the type labels in the TFG for a given input program. We\nstudy different design choices for our GNN models for the 100 most common types\nin our evaluation dataset, and show that our best two GNN configurations for\naccuracy achieve a top-1 accuracy of 87.76% and 86.89% respectively,\noutperforming the two most closely related deep learning type inference\napproaches from past work -- DeepTyper with a top-1 accuracy of 84.62% and\nLambdaNet with a top-1 accuracy of 79.45%. Further, the average inference\nthroughputs of those two configurations are 353.8 and 1,303.9 files/second,\ncompared to 186.7 files/second for DeepTyper and 1,050.3 files/second for\nLambdaNet.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 08:13:01 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Ye", "Fangke", ""], ["Zhao", "Jisheng", ""], ["Sarkar", "Vivek", ""]]}, {"id": "2009.06029", "submitter": "Yepeng Ding", "authors": "Yepeng Ding, Hiroyuki Sato", "title": "Transition-Oriented Programming: Developing Verifiable Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is extremely challenging to develop verifiable systems that are regulated\nby formal specifications and checked by formal verification techniques in\npractice. Although formal verification has made significant progress over the\npast decades, the issue caused by the gulf between the system implementation\nand formal verification still has a huge cost. To fundamentally solve the\nissue, we propose transition-oriented programming (TOP), a novel programming\nparadigm, to instruct developers to develop verifiable systems by thinking in a\nformal way. TOP introduces the theories of the transition system as the joint\nof the implementation and formal verification to promote formal thinking during\ndevelopment. Furthermore, we propose a novel programming language named Seni to\nsupport the TOP features. We argue that TOP is useful and usable to develop\nverifiable systems in a wide range of fields.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 16:03:41 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 03:06:58 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Ding", "Yepeng", ""], ["Sato", "Hiroyuki", ""]]}, {"id": "2009.06089", "submitter": "Felicien Ihirwe", "authors": "Felicien Ihirwe and Silvia Mazzini and Pierluigi Pierini and Alberto\n  Debiasi and Stefano Tonetta", "title": "Model-based analysis support for dependable complex systems in CHESS", "comments": "10 pages, 11 figures", "journal-ref": "9th International Conference on Model-Driven Engineering and\n  Software Development - 2021", "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenges related to dependable complex systems are heterogeneous and\ninvolve different aspects of the system. On one hand, the decision-making\nprocesses need to take into account many options. On the other hand, the design\nof the system's logical architecture must consider various dependability\nconcerns such as safety, reliability, and security. Moreover, in case of\nhigh-assurance systems, the analysis of such concerns must be performed with\nrigorous methods. In this paper, we present the new development of CHESS, a\ncross-domain, model-driven, component-based, and open-source tool for the\ndevelopment of high-integrity systems. We focus on the new recently distributed\nversion of CHESS, which supports extended model-based development and analyses\nfor safety and security concerns. Finally, we present contributions of CHESS to\nseveral international research projects.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2020 21:13:58 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 00:12:49 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Ihirwe", "Felicien", ""], ["Mazzini", "Silvia", ""], ["Pierini", "Pierluigi", ""], ["Debiasi", "Alberto", ""], ["Tonetta", "Stefano", ""]]}, {"id": "2009.06541", "submitter": "Fangyi Zhou", "authors": "Fangyi Zhou (1), Francisco Ferreira (1), Raymond Hu (2), Rumyana\n  Neykova (3) and Nobuko Yoshida (1) ((1) Imperial College London, (2)\n  University of Hertfordshire, (3) Brunel University London)", "title": "Statically Verified Refinements for Multiparty Protocols", "comments": "Conditionally Accepted by OOPSLA' 20. Full version with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With distributed computing becoming ubiquitous in the modern era, safe\ndistributed programming is an open challenge. To address this, multiparty\nsession types (MPST) provide a typing discipline for message-passing\nconcurrency, guaranteeing communication safety properties such as deadlock\nfreedom.\n  While originally MPST focus on the communication aspects, and employ a simple\ntyping system for communication payloads, communication protocols in the real\nworld usually contain constraints on the payload. We introduce refined\nmultiparty session types (RMPST), an extension of MPST, that express data\ndependent protocols via refinement types on the data types.\n  We provide an implementation of RMPST, in a toolchain called Session*, using\nScribble, a multiparty protocol description toolchain, and targeting F*, a\nverification-oriented functional programming language. Users can describe a\nprotocol in Scribble and implement the endpoints in F* using refinement-typed\nAPIs generated from the protocol. The F* compiler can then statically verify\nthe refinements. Moreover, we use a novel approach of callback-styled API\ngeneration, providing static linearity guarantees with the inversion of\ncontrol. We evaluate our approach with real world examples and show that it has\nlittle overhead compared to a na\\\"ive implementation, while guaranteeing safety\nproperties from the underlying theory.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 16:09:56 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Zhou", "Fangyi", ""], ["Ferreira", "Francisco", ""], ["Hu", "Raymond", ""], ["Neykova", "Rumyana", ""], ["Yoshida", "Nobuko", ""]]}, {"id": "2009.06592", "submitter": "Matthew Sotoudeh", "authors": "Matthew Sotoudeh and Aditya V. Thakur", "title": "Analogy-Making as a Core Primitive in the Software Engineering Toolbox", "comments": "Conference paper at SPLASH 'Onward!' 2020. Code is available at\n  https://github.com/95616ARG/sifter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An analogy is an identification of structural similarities and\ncorrespondences between two objects. Computational models of analogy making\nhave been studied extensively in the field of cognitive science to better\nunderstand high-level human cognition. For instance, Melanie Mitchell and\nDouglas Hofstadter sought to better understand high-level perception by\ndeveloping the Copycat algorithm for completing analogies between letter\nsequences. In this paper, we argue that analogy making should be seen as a core\nprimitive in software engineering. We motivate this argument by showing how\ncomplex software engineering problems such as program understanding and\nsource-code transformation learning can be reduced to an instance of the\nanalogy-making problem. We demonstrate this idea using Sifter, a new\nanalogy-making algorithm suitable for software engineering applications that\nadapts and extends ideas from Copycat. In particular, Sifter reduces\nanalogy-making to searching for a sequence of update rule applications. Sifter\nuses a novel representation for mathematical structures capable of effectively\nrepresenting the wide variety of information embedded in software. We conclude\nby listing major areas of future work for Sifter and analogy-making in software\nengineering.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 17:24:15 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Sotoudeh", "Matthew", ""], ["Thakur", "Aditya V.", ""]]}, {"id": "2009.06789", "submitter": "Drew Zagieboylo", "authors": "Drew Zagieboylo, G. Edward Suh, Andrew C. Myers", "title": "The Cost of Software-Based Memory Management Without Virtual Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual memory has been a standard hardware feature for more than three\ndecades. At the price of increased hardware complexity, it has simplified\nsoftware and promised strong isolation among colocated processes. In modern\ncomputing systems, however, the costs of virtual memory have increased\nsignificantly. With large memory workloads, virtualized environments, data\ncenter computing, and chips with multiple DMA devices, virtual memory can\ndegrade performance and increase power usage. We therefore explore the\nimplications of building applications and operating systems without relying on\nhardware support for address translation. Primarily, we investigate the\nimplications of removing the abstraction of large contiguous memory segments.\nOur experiments show that the overhead to remove this reliance is surprisingly\nsmall for real programs. We expect this small overhead to be worth the benefit\nof reducing the complexity and energy usage of address translation. In fact, in\nsome cases, performance can even improve when address translation is avoided.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2020 23:28:30 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Zagieboylo", "Drew", ""], ["Suh", "G. Edward", ""], ["Myers", "Andrew C.", ""]]}, {"id": "2009.06865", "submitter": "David Dewhurst", "authors": "David Rushing Dewhurst", "title": "Structural time series grammar over variable blocks", "comments": "7 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A structural time series model additively decomposes into generative,\nsemantically-meaningful components, each of which depends on a vector of\nparameters. We demonstrate that considering each generative component together\nwith its vector of parameters as a single latent structural time series node\ncan simplify reasoning about collections of structural time series components.\nWe then introduce a formal grammar over structural time series nodes and\nparameter vectors. Valid sentences in the grammar can be interpreted as\ngenerative structural time series models. An extension of the grammar can also\nexpress structural time series models that include changepoints, though these\nmodels are necessarily not generative. We demonstrate a preliminary\nimplementation of the language generated by this grammar. We close with a\ndiscussion of possible future work.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 04:45:02 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Dewhurst", "David Rushing", ""]]}, {"id": "2009.06984", "submitter": "Tomasz Drab", "authors": "Ma{\\l}gorzata Biernacka (1), Dariusz Biernacki (1), Witold Charatonik\n  (1), Tomasz Drab (1) ((1) Institute of Computer Science, University of\n  Wroc{\\l}aw, Poland)", "title": "An Abstract Machine for Strong Call by Value", "comments": "24 pages, 6 figures, APLAS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an abstract machine that implements a full-reducing (a.k.a.\nstrong) call-by-value strategy for pure $\\lambda$-calculus. It is derived using\nDanvy et al.'s functional correspondence from Cr\\'egut's KN by: (1)\ndeconstructing KN to a call-by-name normalization-by-evaluation function akin\nto Filinski and Rohde's, (2) modifying the resulting normalizer so that it\nimplements the right-to-left call-by-value function application, and (3)\nconstructing the functionally corresponding abstract machine.\n  This new machine implements a reduction strategy that subsumes the\nfireball-calculus variant of call by value studied by Accattoli et al. We\ndescribe the strong strategy of the machine in terms of a reduction semantics\nand prove the correctness of the machine using a method based on Biernacka et\nal.'s generalized refocusing. As a byproduct, we present an example application\nof the machine to checking term convertibility by discriminating on the basis\nof their partially normalized forms.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2020 11:03:04 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Biernacka", "Ma\u0142gorzata", ""], ["Biernacki", "Dariusz", ""], ["Charatonik", "Witold", ""], ["Drab", "Tomasz", ""]]}, {"id": "2009.07400", "submitter": "Rafael Ravedutti Lucio Machado", "authors": "Rafael Ravedutti L. Machado (1), Jonas Schmitt (1), Sebastian Eibl\n  (1), Jan Eitzinger (2), Roland Lei{\\ss}a (3), Sebastian Hack (3), Ars\\`ene\n  P\\'erard-Gayot (3), Richard Membarth (3 and 4) and Harald K\\\"ostler (1) ((1)\n  Chair for System Simulation at University of Erlangen-N\\\"urnberg, (2)\n  Regional Computer Center Erlangen at University of Erlangen-N\\\"urnberg, (3)\n  Saarland Informatics Campus at Saarland University, (4) German Research\n  Center for Artificial Intelligence at Saarland Informatics Campus)", "title": "tinyMD: A Portable and Scalable Implementation for Pairwise Interactions\n  Simulations", "comments": "35 pages, 8 figures, submitted to Journal of Computational Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.DC cs.PL physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the suitability of the AnyDSL partial evaluation\nframework to implement tinyMD: an efficient, scalable, and portable simulation\nof pairwise interactions among particles. We compare tinyMD with the miniMD\nproxy application that scales very well on parallel supercomputers. We discuss\nthe differences between both implementations and contrast miniMD's performance\nfor single-node CPU and GPU targets, as well as its scalability on SuperMUC-NG\nand Piz Daint supercomputers. Additionaly, we demonstrate tinyMD's flexibility\nby coupling it with the waLBerla multi-physics framework. This allow us to\nexecute tinyMD simulations using the load-balancing mechanism implemented in\nwaLBerla.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 00:29:13 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Machado", "Rafael Ravedutti L.", "", "3 and 4"], ["Schmitt", "Jonas", "", "3 and 4"], ["Eibl", "Sebastian", "", "3 and 4"], ["Eitzinger", "Jan", "", "3 and 4"], ["Lei\u00dfa", "Roland", "", "3 and 4"], ["Hack", "Sebastian", "", "3 and 4"], ["P\u00e9rard-Gayot", "Ars\u00e8ne", "", "3 and 4"], ["Membarth", "Richard", "", "3 and 4"], ["K\u00f6stler", "Harald", ""]]}, {"id": "2009.07628", "submitter": "EPTCS", "authors": "Julien Lange (Royal Holloway, University of London), Anastasia\n  Mavridou (KBR/NASA Ames Research Center), Larisa Safina (INRIA), Alceste\n  Scalas (DTU Compute, Technical University of Denmark)", "title": "Proceedings 13th Interaction and Concurrency Experience", "comments": null, "journal-ref": "EPTCS 324, 2020", "doi": "10.4204/EPTCS.324", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of ICE'20, the 13th Interaction and\nConcurrency Experience, which was held online on the 19th of June 2020, as a\nsatellite event of DisCoTec'20. The ICE workshop series features a\ndistinguishing review and selection procedure, allowing PC members to interact\nanonymously with authors. As in the past 12 editions, this interaction\nconsiderably improved the accuracy of the feedback from the reviewers and the\nquality of accepted papers, and offered the basis for lively discussion during\nthe workshop. The 2020 edition of ICE included double blind reviewing of\noriginal research papers, in order to increase fairness and avoid bias in\nreviewing. Each paper was reviewed by three PC members, and altogether 5 papers\nwere accepted for publication - plus 5 oral presentations which are not part of\nthis volume. We were proud to host 2 invited talks, by Cinzia Di Giusto and\nKaroliina Lehtinen. The abstracts of these talks are included in this volume\ntogether with the regular papers. The final versions of the contributions,\ntaking into account the discussion at the workshop, are included.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 12:25:32 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Lange", "Julien", "", "Royal Holloway, University of London"], ["Mavridou", "Anastasia", "", "KBR/NASA Ames Research Center"], ["Safina", "Larisa", "", "INRIA"], ["Scalas", "Alceste", "", "DTU Compute, Technical University of Denmark"]]}, {"id": "2009.07740", "submitter": "Juan Cruz-Benito", "authors": "Juan Cruz-Benito, Sanjay Vishwakarma, Francisco Martin-Fernandez,\n  Ismael Faro", "title": "Automated Source Code Generation and Auto-completion Using Deep\n  Learning: Comparing and Discussing Current Language-Model-Related Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the use of deep learning in language models gained much\nattention. Some research projects claim that they can generate text that can be\ninterpreted as human-writing, enabling new possibilities in many application\nareas. Among the different areas related to language processing, one of the\nmost notable in applying this type of modeling is programming languages. For\nyears, the Machine Learning community has been researching this software\nengineering area, pursuing goals like applying different approaches to\nauto-complete, generate, fix, or evaluate code programmed by humans.\nConsidering the increasing popularity of the Deep-Learning-enabled language\nmodels approach, we detected a lack of empirical papers that compare different\ndeep learning architectures to create and use language models based on\nprogramming code. This paper compares different neural network architectures\nlike AWD-LSTMs, AWD-QRNNs, and Transformer while using transfer learning and\ndifferent tokenizations to see how they behave in building language models\nusing a Python dataset for code generation and filling mask tasks. Considering\nthe results, we discuss each approach's different strengths and weaknesses and\nwhat gaps we find to evaluate the language models or apply them in a real\nprogramming context.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2020 15:17:04 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 15:37:47 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2020 10:52:51 GMT"}, {"version": "v4", "created": "Tue, 12 Jan 2021 10:54:20 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Cruz-Benito", "Juan", ""], ["Vishwakarma", "Sanjay", ""], ["Martin-Fernandez", "Francisco", ""], ["Faro", "Ismael", ""]]}, {"id": "2009.07989", "submitter": "EPTCS", "authors": "Zorica Savanovi\\'c (IMT School for Advanced Studies Lucca), Letterio\n  Galletta (IMT School for Advanced Studies Lucca), Hugo Torres Vieira (C4 -\n  University of Beira Interior)", "title": "A type language for message passing component-based systems", "comments": "In Proceedings ICE 2020, arXiv:2009.07628", "journal-ref": "EPTCS 324, 2020, pp. 3-24", "doi": "10.4204/EPTCS.324.3", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Component-based development is challenging in a distributed setting, for\nstarters considering programming a task may involve the assembly of\nloosely-coupled remote components. In order for the task to be fulfilled, the\nsupporting interaction among components should follow a well-defined protocol.\nIn this paper we address a model for message passing component-based systems\nwhere components are assembled together with the protocol itself. Components\ncan therefore be independent from the protocol, and reactive to messages in a\nflexible way. Our contribution is at the level of the type language that allows\nto capture component behaviour so as to check its compatibility with a\nprotocol. We show the correspondence of component and type behaviours, which\nentails a progress property for components.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 00:47:31 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Savanovi\u0107", "Zorica", "", "IMT School for Advanced Studies Lucca"], ["Galletta", "Letterio", "", "IMT School for Advanced Studies Lucca"], ["Vieira", "Hugo Torres", "", "C4 -\n  University of Beira Interior"]]}, {"id": "2009.08361", "submitter": "Aaron Bembenek", "authors": "Aaron Bembenek (1), Michael Greenberg (2), Stephen Chong (1) ((1)\n  Harvard University, (2) Pomona College)", "title": "Formulog: Datalog for SMT-Based Static Analysis (Extended Version)", "comments": "Please cite the official published version of this work: Aaron\n  Bembenek, Michael Greenberg, and Stephen Chong. 2020. Formulog: Datalog for\n  SMT-Based Static Analysis. Proc. ACM Program. Lang. 4, OOPSLA, Article 141\n  (November 2020), 31 pages. https://doi.org/10. 1145/3428209", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Satisfiability modulo theories (SMT) solving has become a critical part of\nmany static analyses, including symbolic execution, refinement type checking,\nand model checking. We propose Formulog, a domain-specific language that makes\nit possible to write a range of SMT-based static analyses in a way that is both\nclose to their formal specifications and amenable to high-level optimizations\nand efficient evaluation.\n  Formulog extends the logic programming language Datalog with a first-order\nfunctional language and mechanisms for representing and reasoning about SMT\nformulas; a novel type system supports the construction of expressive formulas,\nwhile ensuring that neither normal evaluation nor SMT solving goes wrong. Our\ncase studies demonstrate that a range of SMT-based analyses can naturally and\nconcisely be encoded in Formulog, and that -- thanks to this encoding --\nhigh-level Datalog-style optimizations can be automatically and advantageously\napplied to these analyses.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2020 15:17:40 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 16:34:49 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Bembenek", "Aaron", ""], ["Greenberg", "Michael", ""], ["Chong", "Stephen", ""]]}, {"id": "2009.08700", "submitter": "Sarah McDaid PhD", "authors": "Edward McDaid, Sarah McDaid", "title": "A Visual Language for Composable Inductive Programming", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Zoea Visual which is a visual programming language based on the\nZoea composable inductive programming language. Zoea Visual allows users to\ncreate software directly from a specification that resembles a set of\nfunctional test cases. Programming with Zoea Visual involves the definition of\na data flow model of test case inputs, optional intermediate values, and\noutputs. Data elements are represented visually and can be combined to create\nstructures of any complexity. Data flows between elements provide additional\ninformation that allows the Zoea compiler to generate larger programs in less\ntime. This paper includes an overview of the language. The benefits of the\napproach and some possible future enhancements are also discussed.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 09:21:31 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["McDaid", "Edward", ""], ["McDaid", "Sarah", ""]]}, {"id": "2009.09117", "submitter": "Vineeth Kashyap", "authors": "Roger Scott, Joseph Ranieri, Lucja Kot, Vineeth Kashyap", "title": "Out of Sight, Out of Place: Detecting and Assessing Swapped Arguments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programmers often add meaningful information about program semantics when\nnaming program entities such as variables, functions, and macros. However,\nstatic analysis tools typically discount this information when they look for\nbugs in a program. In this work, we describe the design and implementation of a\nstatic analysis checker called SwapD, which uses the natural language\ninformation in programs to warn about mistakenly-swapped arguments at call\nsites. SwapD combines two independent detection strategies to improve the\neffectiveness of the overall checker. We present the results of a comprehensive\nevaluation of SwapD over a large corpus of C and C++ programs totaling 417\nmillion lines of code. In this evaluation, SwapD found 154 manually-vetted\nreal-world cases of mistakenly-swapped arguments, suggesting that such errors,\nwhile not pervasive in released code, are a real problem and a worthwhile\ntarget for static analysis.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2020 23:07:39 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Scott", "Roger", ""], ["Ranieri", "Joseph", ""], ["Kot", "Lucja", ""], ["Kashyap", "Vineeth", ""]]}, {"id": "2009.09158", "submitter": "EPTCS", "authors": "Francesco Ricca (University of Calabria), Alessandra Russo (Imperial\n  College London), Sergio Greco (University of Calabria), Nicola Leone\n  (University of Calabria), Alexander Artikis (University of Piraeus), Gerhard\n  Friedrich (Universit\\\"at Klagenfurt), Paul Fodor (Stony Brook University),\n  Angelika Kimmig (Cardiff University), Francesca Lisi (University of Bari Aldo\n  Moro), Marco Maratea (University of Genova), Alessandra Mileo (INSIGHT Centre\n  for Data Analytics), Fabrizio Riguzzi (Universit\\`a di Ferrara)", "title": "Proceedings 36th International Conference on Logic Programming\n  (Technical Communications)", "comments": null, "journal-ref": "EPTCS 325, 2020", "doi": "10.4204/EPTCS.325", "report-no": null, "categories": "cs.LO cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the first conference held in Marseille in 1982, ICLP has been the\npremier international event for presenting research in logic programming.\nContributions are solicited in all areas of logic programming and related\nareas, including but not restricted to:\n  - Foundations: Semantics, Formalisms, Answer-Set Programming, Non-monotonic\nReasoning, Knowledge Representation.\n  - Declarative Programming: Inference engines, Analysis, Type and mode\ninference, Partial evaluation, Abstract interpretation, Transformation,\nValidation, Verification, Debugging, Profiling, Testing, Logic-based\ndomain-specific languages, constraint handling rules.\n  - Related Paradigms and Synergies: Inductive and Co-inductive Logic\nProgramming, Constraint Logic Programming, Interaction with SAT, SMT and CSP\nsolvers, Logic programming techniques for type inference and theorem proving,\nArgumentation, Probabilistic Logic Programming, Relations to object-oriented\nand Functional programming, Description logics, Neural-Symbolic Machine\nLearning, Hybrid Deep Learning and Symbolic Reasoning.\n  - Implementation: Concurrency and distribution, Objects, Coordination,\nMobility, Virtual machines, Compilation, Higher Order, Type systems, Modules,\nConstraint handling rules, Meta-programming, Foreign interfaces, User\ninterfaces.\n  - Applications: Databases, Big Data, Data Integration and Federation,\nSoftware Engineering, Natural Language Processing, Web and Semantic Web,\nAgents, Artificial Intelligence, Bioinformatics, Education, Computational life\nsciences, Education, Cybersecurity, and Robotics.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 04:18:41 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Ricca", "Francesco", "", "University of Calabria"], ["Russo", "Alessandra", "", "Imperial\n  College London"], ["Greco", "Sergio", "", "University of Calabria"], ["Leone", "Nicola", "", "University of Calabria"], ["Artikis", "Alexander", "", "University of Piraeus"], ["Friedrich", "Gerhard", "", "Universit\u00e4t Klagenfurt"], ["Fodor", "Paul", "", "Stony Brook University"], ["Kimmig", "Angelika", "", "Cardiff University"], ["Lisi", "Francesca", "", "University of Bari Aldo\n  Moro"], ["Maratea", "Marco", "", "University of Genova"], ["Mileo", "Alessandra", "", "INSIGHT Centre\n  for Data Analytics"], ["Riguzzi", "Fabrizio", "", "Universit\u00e0 di Ferrara"]]}, {"id": "2009.09215", "submitter": "Yutaka Nagashima", "authors": "Yutaka Nagashima", "title": "Faster Smarter Induction in Isabelle/HOL", "comments": "This is the preprint of our paper of the same title, which is\n  accepted to IJCAI2021. For the formal proceeding, please refer to the\n  IJCAI2021 website", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Proof by induction plays a critical role in formal verification and\nmathematics at large. However, its automation remains as one of the\nlong-standing challenges in Computer Science. To address this problem, we\ndeveloped sem_ind. Given inductive problem, sem_ind recommends what arguments\nto pass to the induct method. To improve the accuracy of sem_ind, we introduced\ndefinitional quantifiers, a new kind of quantifiers that allow us to\ninvestigate not only the syntactic structures of inductive problems but also\nthe definitions of relevant constants in a domain-agnostic style. Our\nevaluation shows that compared to its predecessor sem_ind improves the accuracy\nof recommendation from 20.1% to 38.2% for the most promising candidates within\n5.0 seconds of timeout while decreasing the median value of execution time from\n2.79 seconds to 1.06 seconds.\n", "versions": [{"version": "v1", "created": "Sat, 19 Sep 2020 11:51:54 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 09:05:41 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2020 09:41:12 GMT"}, {"version": "v4", "created": "Sun, 9 May 2021 07:58:26 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Nagashima", "Yutaka", ""]]}, {"id": "2009.09777", "submitter": "Nghi D. Q. Bui", "authors": "Nghi D. Q. Bui, Yijun Yu, Lingxiao Jiang", "title": "TreeCaps: Tree-Based Capsule Networks for Source Code Processing", "comments": "Accepted at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently program learning techniques have been proposed to process source\ncode based on syntactical structures (e.g., Abstract Syntax Trees) and/or\nsemantic information (e.g., Dependency Graphs). Although graphs may be better\nat capturing various viewpoints of code semantics than trees, constructing\ngraph inputs from code needs static code semantic analysis that may not be\naccurate and introduces noise during learning. Although syntax trees are\nprecisely defined according to the language grammar and easier to construct and\nprocess than graphs, previous tree-based learning techniques have not been able\nto learn semantic information from trees to achieve better accuracy than\ngraph-based techniques. We propose a new learning technique, named TreeCaps, by\nfusing together capsule networks with tree-based convolutional neural networks,\nto achieve learning accuracy higher than existing graph-based techniques while\nit is based only on trees. TreeCaps introduces novel variable-to-static routing\nalgorithms into the capsule networks to compensate for the loss of previous\nrouting algorithms. Aside from accuracy, we also find that TreeCaps is the most\nrobust to withstand those semantic-preserving program transformations that\nchange code syntax without modifying the semantics. Evaluated on a large number\nof Java and C/C++ programs, TreeCaps models outperform prior deep learning\nmodels of program source code, in terms of both accuracy and robustness for\nprogram comprehension tasks such as code functionality classification and\nfunction name prediction\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2020 16:37:19 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 20:43:17 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2020 10:05:39 GMT"}, {"version": "v4", "created": "Mon, 14 Dec 2020 15:12:16 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Bui", "Nghi D. Q.", ""], ["Yu", "Yijun", ""], ["Jiang", "Lingxiao", ""]]}, {"id": "2009.10207", "submitter": "Adithya Murali", "authors": "Adithya Murali, Lucas Pe\\~na, Christof L\\\"oding, P. Madhusudan", "title": "Synthesizing Lemmas for Inductive Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursively defined structures and properties about them are naturally\nexpressed in first-order logic with least fixpoint definitions (FO+lfp), but\nautomated reasoning for such logics has not seen much progress. Such logics,\nunlike pure FOL, do not even admit complete procedures, let alone decidable\nones. In this paper, we undertake a foundational study of finding proofs that\nuse induction to reason with these logics. By treating proofs as purely FO\nproofs punctuated by declarations of induction lemmas, we separate proofs into\ndeductively reasoned components that can be automated and statements of lemmas\nthat need to be divined, respectively. While humans divine such lemmas with\nintuition, we propose a counterexample driven technique that guides the\nsynthesis of such lemmas, where counterexamples are finite models that witness\ninability of proving the theorem as well as other proposed lemmas. We develop\nrelatively complete procedures for synthesizing such lemmas using techniques\nand tools from program/expression synthesis, for powerful FO+lfp logics that\nhave background sorts constrained by natural theories such as arithmetic and\nset theory. We evaluate our procedures and show that over a class of theorems\nthat require finding inductive proofs, our automatic synthesis procedure is\neffective in proving them.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2020 22:36:31 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Murali", "Adithya", ""], ["Pe\u00f1a", "Lucas", ""], ["L\u00f6ding", "Christof", ""], ["Madhusudan", "P.", ""]]}, {"id": "2009.10238", "submitter": "EPTCS", "authors": "Joaqu\\'in Arias, Manuel Carro, Zhuo Chen, Gopal Gupta", "title": "Justifications for Goal-Directed Constraint Answer Set Programming", "comments": "In Proceedings ICLP 2020, arXiv:2009.09158", "journal-ref": "EPTCS 325, 2020, pp. 59-72", "doi": "10.4204/EPTCS.325.12", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ethical and legal concerns make it necessary for programs that may directly\ninfluence the life of people (via, e.g., legal or health counseling) to justify\nin human-understandable terms the advice given. Answer Set Programming has a\nrich semantics that makes it possible to very concisely express complex\nknowledge. However, justifying why an answer is a consequence from an ASP\nprogram may be non-trivial -- even more so when the user is an expert in a\ngiven domain, but not necessarily knowledgeable in ASP. Most ASP systems\ngenerate answers using SAT-solving procedures on ground rules that do not match\nhow humans perceive reasoning. We propose using s(CASP), a query-driven,\ntop-down execution model for predicate ASP with constraints to generate\njustification trees of (constrained) answer sets. The operational semantics of\ns(CASP) relies on backward chaining, which is intuitive to follow and lends\nitself to generating explanations that are easier to translate into natural\nlanguage. We show how s(CASP) provides minimal justifications for, among\nothers, relevant examples proposed in the literature, both as search trees but,\nmore importantly, as explanations in natural language. We validate our design\nwith real ASP applications and evaluate the cost of generating s(CASP)\njustification trees.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 00:48:05 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Arias", "Joaqu\u00edn", ""], ["Carro", "Manuel", ""], ["Chen", "Zhuo", ""], ["Gupta", "Gopal", ""]]}, {"id": "2009.10241", "submitter": "EPTCS", "authors": "Paul Tarau (University of North Texas), Valeria de Paiva (Topos\n  Institute)", "title": "Deriving Theorems in Implicational Linear Logic, Declaratively", "comments": "In Proceedings ICLP 2020, arXiv:2009.09158", "journal-ref": "EPTCS 325, 2020, pp. 110-123", "doi": "10.4204/EPTCS.325.18", "report-no": null, "categories": "cs.LO cs.AI cs.PL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem we want to solve is how to generate all theorems of a given size\nin the implicational fragment of propositional intuitionistic linear logic. We\nstart by filtering for linearity the proof terms associated by our Prolog-based\ntheorem prover for Implicational Intuitionistic Logic. This works, but using\nfor each formula a PSPACE-complete algorithm limits it to very small formulas.\nWe take a few walks back and forth over the bridge between proof terms and\ntheorems, provided by the Curry-Howard isomorphism, and derive step-by-step an\nefficient algorithm requiring a low polynomial effort per generated theorem.\nThe resulting Prolog program runs in O(N) space for terms of size N and\ngenerates in a few hours 7,566,084,686 theorems in the implicational fragment\nof Linear Intuitionistic Logic together with their proof terms in normal form.\nAs applications, we generate datasets for correctness and scalability testing\nof linear logic theorem provers and training data for neural networks working\non theorem proving challenges. The results in the paper, organized as a\nliterate Prolog program, are fully replicable.\n  Keywords: combinatorial generation of provable formulas of a given size,\nintuitionistic and linear logic theorem provers, theorems of the implicational\nfragment of propositional linear intuitionistic logic, Curry-Howard\nisomorphism, efficient generation of linear lambda terms in normal form, Prolog\nprograms for lambda term generation and theorem proving.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 00:48:45 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Tarau", "Paul", "", "University of North Texas"], ["de Paiva", "Valeria", "", "Topos\n  Institute"]]}, {"id": "2009.10242", "submitter": "EPTCS", "authors": "Pedro Cabalar (University of Coru\\~na, Spain), Jorge Fandinno\n  (University of Potsdam, Germany), Brais Mu\\~niz (CITIC, University of\n  Coru\\~na, Spain)", "title": "A System for Explainable Answer Set Programming", "comments": "In Proceedings ICLP 2020, arXiv:2009.09158", "journal-ref": "EPTCS 325, 2020, pp. 124-136", "doi": "10.4204/EPTCS.325.19", "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present xclingo, a tool for generating explanations from ASP programs\nannotated with text and labels. These annotations allow tracing the application\nof rules or the atoms derived by them. The input of xclingo is a markup\nlanguage written as ASP comment lines, so the programs annotated in this way\ncan still be accepted by a standard ASP solver. xclingo translates the\nannotations into additional predicates and rules and uses the ASP solver clingo\nto obtain the extension of those auxiliary predicates. This information is used\nafterwards to construct derivation trees containing textual explanations. The\nlanguage allows selecting which atoms to explain and, in its turn, which atoms\nor rules to include in those explanations. We illustrate the basic features\nthrough a diagnosis problem from the literature.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 00:48:59 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Cabalar", "Pedro", "", "University of Coru\u00f1a, Spain"], ["Fandinno", "Jorge", "", "University of Potsdam, Germany"], ["Mu\u00f1iz", "Brais", "", "CITIC, University of\n  Coru\u00f1a, Spain"]]}, {"id": "2009.10254", "submitter": "EPTCS", "authors": "Finn Teegen (Kiel University)", "title": "Research Summary on Implementing Functional Patterns by Synthesizing\n  Inverse Functions", "comments": "In Proceedings ICLP 2020, arXiv:2009.09158", "journal-ref": "EPTCS 325, 2020, pp. 296-302", "doi": "10.4204/EPTCS.325.39", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research summary we present our recent work on implementing\nfunctional patterns with inverse functions in the lazy functional-logic\nprogramming language Curry. Our goal is the synthesis of the inverse of any\ngiven function in Curry itself. The setting of a functional-logic language\nespecially allows the inversion of non-injective functions. In general, inverse\ncomputation is a non-trivial problem in lazy programming languages due to their\nnon-strict semantics. We are so far able to directly derive the inverse\nfunction for a limited class of functions, namely those consisting of rules\nthat do not involve both extra variables and non-linear right-hand sides.\nBecause the synthesized definitions are based on standard code, known\noptimizations techniques can be applied to them. For all other functions we can\nstill provide an inverse function by using non-strict unification.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 00:52:05 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Teegen", "Finn", "", "Kiel University"]]}, {"id": "2009.10272", "submitter": "Shivam Handa", "authors": "Shivam Handa, Martin Rinard", "title": "Inductive Program Synthesis Over Noisy Data", "comments": null, "journal-ref": null, "doi": "10.1145/3368089.3409732", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new framework and associated synthesis algorithms for program\nsynthesis over noisy data, i.e., data that may contain incorrect/corrupted\ninput-output examples. This framework is based on an extension of finite tree\nautomata called {\\em weighted finite tree automata}. We show how to apply this\nframework to formulate and solve a variety of program synthesis problems over\nnoisy data. Results from our implemented system running on problems from the\nSyGuS 2018 benchmark suite highlight its ability to successfully synthesize\nprograms in the face of noisy data sets, including the ability to synthesize a\ncorrect program even when every input-output example in the data set is\ncorrupted.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2020 01:57:48 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 20:23:13 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Handa", "Shivam", ""], ["Rinard", "Martin", ""]]}, {"id": "2009.10929", "submitter": "Pablo Barenbaum", "authors": "Pablo Barenbaum, Federico Lochbaum, Mariana Milicich", "title": "Semantics of a Relational {\\lambda}-Calculus (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We extend the {\\lambda}-calculus with constructs suitable for relational and\nfunctional-logic programming: non-deterministic choice, fresh variable\nintroduction, and unification of expressions. In order to be able to unify\n{\\lambda}-expressions and still obtain a confluent theory, we depart from\nrelated approaches, such as {\\lambda}Prolog, in that we do not attempt to solve\nhigher-order unification. Instead, abstractions are decorated with a location,\nwhich intuitively may be understood as its memory address, and we impose a\nsimple coherence invariant: abstractions in the same location must be equal.\nThis allows us to formulate a confluent small-step operational semantics which\nonly performs first-order unification and does not require strong evaluation\n(below lambdas). We study a simply typed version of the system. Moreover, a\ndenotational semantics for the calculus is proposed and reduction is shown to\nbe sound with respect to the denotational semantics.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 04:39:59 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 01:00:33 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2021 16:31:56 GMT"}, {"version": "v4", "created": "Sun, 28 Feb 2021 18:54:17 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Barenbaum", "Pablo", ""], ["Lochbaum", "Federico", ""], ["Milicich", "Mariana", ""]]}, {"id": "2009.11403", "submitter": "Nathan Fulton", "authors": "Koundinya Vajjha, Avraham Shinnar, Vasily Pestun, Barry Trager, Nathan\n  Fulton", "title": "CertRL: Formalizing Convergence Proofs for Value and Policy Iteration in\n  Coq", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning algorithms solve sequential decision-making problems\nin probabilistic environments by optimizing for long-term reward. The desire to\nuse reinforcement learning in safety-critical settings inspires a recent line\nof work on formally constrained reinforcement learning; however, these methods\nplace the implementation of the learning algorithm in their Trusted Computing\nBase. The crucial correctness property of these implementations is a guarantee\nthat the learning algorithm converges to an optimal policy. This paper begins\nthe work of closing this gap by developing a Coq formalization of two canonical\nreinforcement learning algorithms: value and policy iteration for finite state\nMarkov decision processes. The central results are a formalization of Bellman's\noptimality principle and its proof, which uses a contraction property of\nBellman optimality operator to establish that a sequence converges in the\ninfinite horizon limit. The CertRL development exemplifies how the Giry monad\nand mechanized metric coinduction streamline optimality proofs for\nreinforcement learning algorithms. The CertRL library provides a general\nframework for proving properties about Markov decision processes and\nreinforcement learning algorithms, paving the way for further work on\nformalization of reinforcement learning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2020 22:28:17 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 19:39:30 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Vajjha", "Koundinya", ""], ["Shinnar", "Avraham", ""], ["Pestun", "Vasily", ""], ["Trager", "Barry", ""], ["Fulton", "Nathan", ""]]}, {"id": "2009.12637", "submitter": "Nick Brown", "authors": "Nick Brown", "title": "Applying Type Oriented Programming to the PGAS Memory Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Partitioned Global Address Space memory model has been popularised by a\nnumber of languages and applications. However this abstraction can often result\nin the programmer having to rely on some in built choices and with this\nimplicit parallelism, with little assistance by the programmer, the scalability\nand performance of the code heavily depends on the compiler and choice of\napplication.\n  We propose an approach, type oriented programming, where all aspects of\nparallelism are encoded via types and the type system. The type information\nassociated by the programmer will determine, for instance, how an array is\nallocated, partitioned and distributed. With this rich, high level of\ninformation the compiler can generate an efficient target executable. If the\nprogrammer wishes to omit detailed type information then the compiler will rely\non well documented and safe default behaviour which can be tuned at a later\ndate with the addition of types.\n  The type oriented parallel programming language Mesham, which follows the\nPGAS memory model, is presented. We illustrate how, if so wished, with the use\nof types one can tune all parameters and options associated with this PGAS\nmodel in a clean and consistent manner without rewriting large portions of\ncode. An FFT case study is presented and considered both in terms of\nprogrammability and performance - the latter we demonstrate by a comparison\nwith an existing FFT solver.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2020 16:41:43 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Brown", "Nick", ""]]}, {"id": "2009.12845", "submitter": "Nick Brown", "authors": "Nick Brown", "title": "A Type-Oriented Graph500 Benchmark", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-07518-1_31", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data intensive workloads have become a popular use of HPC in recent years and\nthe question of how data scientists, who might not be HPC experts, can\neffectively program these machines is important to address. Whilst using models\nsuch as Partitioned Global Address Space (PGAS) is attractive from a simplicity\npoint of view, the abstractions that these impose upon the programmer can\nimpact performance. We propose an approach, type-oriented programming, where\nall aspects of parallelism are encoded via types and the type system which\nallows for the programmer to write simple PGAS data intensive HPC codes and\nthen, if they so wish, tune the fundamental aspects by modifying type\ninformation. This paper considers the suitability of using type-oriented\nprogramming, with the PGAS memory model, in data intensive workloads. We\ncompare a type-oriented implementation of the Graph500 benchmark against MPI\nreference implementations both in terms of programmability and performance, and\nevaluate how orienting their parallel codes around types can assist in the data\nintensive HPC field.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2020 13:54:10 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Brown", "Nick", ""]]}, {"id": "2009.13619", "submitter": "Ruofei Chen", "authors": "Ruofei Chen, Stephanie Balzer", "title": "Ferrite: A Judgmental Embedding of Session Types in Rust", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper introduces Ferrite, a shallow embedding of session types in Rust.\nIn contrast to existing session type libraries and embeddings for mainstream\nlanguages, Ferrite not only supports linear session types but also shared\nsession types. Shared session types allow sharing (aliasing) of channels while\npreserving session fidelity (preservation) using type modalities for acquiring\nand releasing sessions. Ferrite adopts a propositions as types approach and\nencodes typing derivations as Rust functions, with the proof of successful\ntype-checking manifesting as a Rust program. We provide an evaluation of\nFerrite using Servo as a practical example, and demonstrate how safe\ncommunication can be achieved in the canvas component using Ferrite.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2020 20:54:56 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 21:09:58 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 13:56:58 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Chen", "Ruofei", ""], ["Balzer", "Stephanie", ""]]}, {"id": "2009.14381", "submitter": "Atefeh Sohrabizadeh", "authors": "Atefeh Sohrabizadeh, Cody Hao Yu, Min Gao, and Jason Cong", "title": "AutoDSE: Enabling Software Programmers Design Efficient FPGA\n  Accelerators", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adopting FPGA as an accelerator in datacenters is becoming mainstream for\ncustomized computing, but the fact that FPGAs are hard to program creates a\nsteep learning curve for software programmers. Even with the help of high-level\nsynthesis (HLS), accelerator designers still have to manually perform code\nreconstruction and cumbersome parameter tuning to achieve the optimal\nperformance. While many learning models have been leveraged by existing work to\nautomate the design of efficient accelerators, the unpredictability of modern\nHLS tools becomes a major obstacle for them to maintain high accuracy. In this\npaper, we address this problem by incorporating an automated DSE\nframework-AutoDSE- that leverages bottleneck-guided gradient optimizer to\nsystematically find abetter design point. AutoDSE finds the bottleneck of the\ndesign in each step and focuses on high-impact parameters to overcome that,\nwhich is similar to the approach an expert would take. The experimental results\nshow that AutoDSE is able to find the design point that achieves, on the\ngeometric mean, 19.9x speedup over one CPU core for Machsuite and Rodinia\nbenchmarks and 1.04x over the manually designed HLS accelerated vision kernels\nin Xilinx Vitis libraries yet with 26x reduction of their optimization pragmas.\nWith less than one optimization pragma per design on average, we are making\nprogress towards democratizing customizable computing by enabling software\nprogrammers to design efficient FPGA accelerators.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 01:38:18 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Sohrabizadeh", "Atefeh", ""], ["Yu", "Cody Hao", ""], ["Gao", "Min", ""], ["Cong", "Jason", ""]]}, {"id": "2009.14705", "submitter": "James Tuck", "authors": "Tiancong Wang and James Tuck", "title": "Persistent Data Retention Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Volatile Memory devices may soon be a part of main memory, and\nprogramming models that give programmers direct access to persistent memory\nthrough loads and stores are sought to maximize the performance benefits of\nthese new devices. Direct access introduces new challenges. In this work, we\nidentify an important aspect of programming for persistent memory: the\npersistent data retention model.\n  A Persistent Data Retention Model describes what happens to persistent data\nwhen code that uses it is modified. We identify two models present in prior\nwork but not described as such, the Reset and Manual Model, and we propose a\nnew one called the Automatic Model. The Reset model discards all persistent\ndata when a program changes leading to performance overheads and write\namplification. In contrast, if data is to be retained, the Manual Model relies\non the programmer to implement code that upgrades data from one version of the\nprogram to the next. This reduces overheads but places a larger burden on the\nprogrammer.\n  We propose the Automatic Model to assist a programmer by automating some or\nall of the conversion. We describe one such automatic approach, Lazily\nExtendible Data Structures, that uses language extensions and compiler support\nto reduce the effort and complexity associated with updating persistent data.\nWe evaluate our PDRMs in the context of the Persistent Memory Development Kit\n(PMDK) using kernels and the TPC-C application. Manual Model shows an overhead\nof 2.90% to 4.10% on average, and LEDS shows overhead of 0.45% to 10.27% on\naverage, depending on the workload. LEDS reduces the number of writes by 26.36%\ncompared to Manual Model. Furthermore, LEDS significantly reduces the\nprogramming complexity by relying on the compiler to migrate persistent data.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 14:39:07 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Wang", "Tiancong", ""], ["Tuck", "James", ""]]}]