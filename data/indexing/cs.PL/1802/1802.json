[{"id": "1802.00061", "submitter": "Thorsten Wissmann", "authors": "Max S. New and Daniel R. Licata", "title": "Call-by-name Gradual Type Theory", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 1 (January\n  31, 2020) lmcs:6063", "doi": "10.23638/LMCS-16(1:7)2020", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present gradual type theory, a logic and type theory for call-by-name\ngradual typing. We define the central constructions of gradual typing (the\ndynamic type, type casts and type error) in a novel way, by universal\nproperties relative to new judgments for gradual type and term dynamism, which\nwere developed in blame calculi and to state the \"gradual guarantee\" theorem of\ngradual typing. Combined with the ordinary extensionality ($\\eta$) principles\nthat type theory provides, we show that most of the standard operational\nbehavior of casts is uniquely determined by the gradual guarantee. This\nprovides a semantic justification for the definitions of casts, and shows that\nnon-standard definitions of casts must violate these principles. Our type\ntheory is the internal language of a certain class of preorder categories\ncalled equipments. We give a general construction of an equipment interpreting\ngradual type theory from a 2-category representing non-gradual types and\nprograms, which is a semantic analogue of Findler and Felleisen's definitions\nof contracts, and use it to build some concrete domain-theoretic models of\ngradual typing.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 20:48:35 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 20:26:53 GMT"}, {"version": "v3", "created": "Thu, 31 Jan 2019 20:46:48 GMT"}, {"version": "v4", "created": "Mon, 5 Aug 2019 18:35:46 GMT"}, {"version": "v5", "created": "Mon, 6 Jan 2020 17:19:47 GMT"}, {"version": "v6", "created": "Wed, 29 Jan 2020 19:50:44 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["New", "Max S.", ""], ["Licata", "Daniel R.", ""]]}, {"id": "1802.00166", "submitter": "Nirmal Prajapati", "authors": "Waruna Ranasinghe, Nirmal Prajapati, Tomofumi Yuki and Sanjay\n  Rajopadhye", "title": "PCOT: Cache Oblivious Tiling of Polyhedral Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies two variants of tiling: iteration space tiling (or loop\nblocking) and cache-oblivious methods that recursively split the iteration\nspace with divide-and-conquer. The key question to answer is when we should be\nusing one over the other. The answer to this question is complicated for modern\narchitecture due to a number of reasons. In this paper, we present a detailed\nempirical study to answer this question for a range of kernels that fit the\npolyhedral model. Our study is based on a generalized cache oblivious code\ngenerator that support this class, which is a superset of those supported by\nexisting tools. The conclusion is that cache oblivious code is most useful when\nthe aim is to have reduced off-chip memory accesses, e.g., lower energy, albeit\ncertain situations that diminish its effectiveness exist.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 05:48:31 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Ranasinghe", "Waruna", ""], ["Prajapati", "Nirmal", ""], ["Yuki", "Tomofumi", ""], ["Rajopadhye", "Sanjay", ""]]}, {"id": "1802.00588", "submitter": "Catalin Hritcu", "authors": "Carmine Abate, Arthur Azevedo de Amorim, Roberto Blanco, Ana Nora\n  Evans, Guglielmo Fachini, Catalin Hritcu, Th\\'eo Laurent, Benjamin C. Pierce,\n  Marco Stronati, J\\'er\\'emy Thibault, Andrew Tolmach", "title": "When Good Components Go Bad: Formally Secure Compilation Despite Dynamic\n  Compromise", "comments": "CCS paper with significant improvement of the proofs, first step\n  towards a journal version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new formal criterion for evaluating secure compilation schemes\nfor unsafe languages, expressing end-to-end security guarantees for software\ncomponents that may become compromised after encountering undefined\nbehavior---for example, by accessing an array out of bounds.\n  Our criterion is the first to model dynamic compromise in a system of\nmutually distrustful components with clearly specified privileges. It\narticulates how each component should be protected from all the others---in\nparticular, from components that have encountered undefined behavior and become\ncompromised. Each component receives secure compilation guarantees---in\nparticular, its internal invariants are protected from compromised\ncomponents---up to the point when this component itself becomes compromised,\nafter which we assume an attacker can take complete control and use this\ncomponent's privileges to attack other components. More precisely, a secure\ncompilation chain must ensure that a dynamically compromised component cannot\nbreak the safety properties of the system at the target level any more than an\narbitrary attacker-controlled component (with the same interface and\nprivileges, but without undefined behaviors) already could at the source level.\n  To illustrate the model, we construct a secure compilation chain for a small\nunsafe language with buffers, procedures, and components, targeting a simple\nabstract machine with built-in compartmentalization. We give a machine-checked\nproof in Coq that this compiler satisfies our secure compilation criterion.\nFinally, we show that the protection guarantees offered by the\ncompartmentalized abstract machine can be achieved at the machine-code level\nusing either software fault isolation or a tag-based reference monitor.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 07:51:13 GMT"}, {"version": "v2", "created": "Thu, 10 May 2018 16:39:23 GMT"}, {"version": "v3", "created": "Fri, 24 Aug 2018 10:44:09 GMT"}, {"version": "v4", "created": "Mon, 17 Sep 2018 12:26:35 GMT"}, {"version": "v5", "created": "Fri, 29 Nov 2019 18:53:35 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Abate", "Carmine", ""], ["de Amorim", "Arthur Azevedo", ""], ["Blanco", "Roberto", ""], ["Evans", "Ana Nora", ""], ["Fachini", "Guglielmo", ""], ["Hritcu", "Catalin", ""], ["Laurent", "Th\u00e9o", ""], ["Pierce", "Benjamin C.", ""], ["Stronati", "Marco", ""], ["Thibault", "J\u00e9r\u00e9my", ""], ["Tolmach", "Andrew", ""]]}, {"id": "1802.00787", "submitter": "Larry Diehl", "authors": "Larry Diehl and Aaron Stump", "title": "Zero-Cost Coercions for Program and Proof Reuse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of identity coercions between non-indexed and indexed\nvariants of inductive datatypes, such as lists and vectors. An identity\ncoercion translates one type to another such that the coercion function\ndefinitionally reduces to the identity function. This allows us to reuse vector\nprograms to derive list programs (and vice versa), without any runtime cost.\nThis also allows us to reuse vector proofs to derive list proofs (and vice\nversa), without the cost of equational reasoning proof obligations. Our work is\nformalized in Cedille, a dependently typed programming language based on a\ntype-annotated Curry-style type the- ory with implicit (or, erased) products\n(or, dependent functions), and relies crucially on erasure to introduce\ndefinitional equalities between underlying untyped terms.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 17:47:16 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Diehl", "Larry", ""], ["Stump", "Aaron", ""]]}, {"id": "1802.00932", "submitter": "Swati Jaiswal", "authors": "Swati Jaiswal, Uday P. Khedker, and Supratik Chakraborty", "title": "Demand-driven Alias Analysis : Formalizing Bidirectional Analyses for\n  Soundness and Precision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A demand-driven approach to program analysis have been viewed as efficient\nalgorithms to compute only the information required to serve a target demand.\nIn contrast, an exhaustive approach computes all information in anticipation of\nit being used later. However, for a given set of demands, demand-driven methods\nare believed to compute the same information that would be computed by the\ncorresponding exhaustive methods. We investigate the precision and\nbidirectional nature of demand-driven methods and show that:\n  (a) demand-driven methods can be formalized inherently as bidirectional data\nflow analysis because the demands are propagated against the control flow and\nthe information to satisfy the demands is propagated along the control flow. We\nextend the formalization of the Meet Over Paths solution to bidirectional\nflows. This formalization helps us to prove the soundness and precision of our\nanalysis, and\n  (b) since a demand-driven method computes only the required information to\nmeet a demand, it should be able to reduce the imprecision caused by data\nabstractions and hence should be more precise than an exhaustive method. We\nshow that while this is indeed the case with Java, for C/C++, the precision\ncritically hinges on how indirect assignments are handled. We use this insight\nand propose a demand-driven alias analysis that is more precise than an\nexhaustive analysis for C/C++ too.\n  We have chosen devirtualization as an application. Our measurements show that\nour method is not only more efficient but more precise than the existing\ndemand-driven method, as well as the corresponding exhaustive method.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 07:21:57 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Jaiswal", "Swati", ""], ["Khedker", "Uday P.", ""], ["Chakraborty", "Supratik", ""]]}, {"id": "1802.01177", "submitter": "Jochen Burghardt", "authors": "Jochen Burghardt", "title": "A Scheme-Driven Approach to Learning Programs from Input/Output\n  Equations", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe an approach to learn, in a term-rewriting setting, function\ndefinitions from input/output equations. By confining ourselves to structurally\nrecursive definitions we obtain a fairly fast learning algorithm that often\nyields definitions close to intuitive expectations. We provide a Prolog\nprototype implementation of our approach, and indicate open issues of further\ninvestigation.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 19:17:57 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Burghardt", "Jochen", ""]]}, {"id": "1802.01220", "submitter": "Xiaoxiao Yang", "authors": "Xiaoxiao Yang", "title": "The Effect Race in Fine-Grained Concurrency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existed work require knowledge about the effect of program instructions\n(or statements) to analyze and verify algorithms. In this paper, by revealing\nsome findings on executions of object programs, we define two basic concepts --\neffect equivalence relation and effect race relation. Further, we show three\neffect theorems about the race and histories. The core result is that the\neffect race relation is the accurate relation to capture the internal steps, of\nwhich precedence orders are the reason to cause chaotic histories. In addition,\nthe concept -- linearization points -- widely used in the object verification,\nis defined formally as the typical effect race relation. These results provide\na clear basis for analyzing intricate fine-grained executions. We conduct a lot\nof experiments on real object algorithms to show the accuracy and efficiency of\nthese definitions in practice. A simple quantitative analysis method for these\nalgorithms is also proposed.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 00:00:01 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 03:55:35 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Yang", "Xiaoxiao", ""]]}, {"id": "1802.01226", "submitter": "Andr\\'e Platzer", "authors": "Andr\\'e Platzer and Yong Kiam Tan", "title": "Differential Equation Axiomatization: The Impressive Power of\n  Differential Ghosts", "comments": "LICS '18: 33rd Annual ACM/IEEE Symposium on Logic in Computer\n  Science, July 9-12, 2018, Oxford, United Kingdom, ACM ISBN\n  978-1-4503-5583-4/18/07", "journal-ref": null, "doi": "10.1145/3209108.3209147", "report-no": "CMU-CS-17-117", "categories": "cs.LO cs.PL math.CA math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the completeness of an axiomatization for differential equation\ninvariants. First, we show that the differential equation axioms in\ndifferential dynamic logic are complete for all algebraic invariants. Our proof\nexploits differential ghosts, which introduce additional variables that can be\nchosen to evolve freely along new differential equations. Cleverly chosen\ndifferential ghosts are the proof-theoretical counterpart of dark matter. They\ncreate new hypothetical state, whose relationship to the original state\nvariables satisfies invariants that did not exist before. The reflection of\nthese new invariants in the original system then enables its analysis.\n  We then show that extending the axiomatization with existence and uniqueness\naxioms makes it complete for all local progress properties, and further\nextension with a real induction axiom makes it complete for all real arithmetic\ninvariants. This yields a parsimonious axiomatization, which serves as the\nlogical foundation for reasoning about invariants of differential equations.\nMoreover, our results are purely axiomatic, and so the axiomatization is\nsuitable for sound implementation in foundational theorem provers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 00:54:36 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 03:38:14 GMT"}, {"version": "v3", "created": "Mon, 10 Jun 2019 22:02:09 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Platzer", "Andr\u00e9", ""], ["Tan", "Yong Kiam", ""]]}, {"id": "1802.01706", "submitter": "Jarrett Holtz", "authors": "Jarrett Holtz, Arjun Guha, Joydeep Biswas", "title": "Interactive Robot Transition Repair With SMT", "comments": "International Joint Conference on Artificial Intelligence (IJCAI),\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex robot behaviors are often structured as state machines, where states\nencapsulate actions and a transition function switches between states. Since\ntransitions depend on physical parameters, when the environment changes, a\nroboticist has to painstakingly readjust the parameters to work in the new\nenvironment. We present interactive SMT-based Robot Transition Repair (SRTR):\ninstead of manually adjusting parameters, we ask the roboticist to identify a\nfew instances where the robot is in a wrong state and what the right state\nshould be. A lightweight automated analysis of the transition function's source\ncode then 1) identifies adjustable parameters, 2) converts the transition\nfunction into a system of logical constraints, and 3) formulates the\nconstraints and user-supplied corrections as MaxSMT problem that yields new\nparameter values. Our evaluation shows that SRTR is effective on real robots\nand in simulation. We show that SRTR finds new parameters 1) quickly, 2) with\nonly a few corrections, and 3) that the parameters generalize to new scenarios.\nWe also show that a simple state machine corrected by SRTR can out-perform a\nmore complex, expert-tuned state machine in the real world.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 21:52:20 GMT"}, {"version": "v2", "created": "Sat, 5 May 2018 17:29:15 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Holtz", "Jarrett", ""], ["Guha", "Arjun", ""], ["Biswas", "Joydeep", ""]]}, {"id": "1802.01738", "submitter": "Andrey Mokhov", "authors": "Andrey Mokhov, Georgy Lukyanov, Jakob Lechner", "title": "Formal Verification of Spacecraft Control Programs Using a Metalanguage\n  for State Transformers", "comments": "Under review, feedback is sought", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification of functional correctness of control programs is an essential\ntask for the development of space electronics; it is difficult and\ntime-consuming and typically outweighs design and programming tasks in terms of\ndevelopment hours. We present a verification approach designed to help\nspacecraft engineers reduce the effort required for formal verification of\nlow-level control programs executed on custom hardware. The approach uses a\nmetalanguage to describe the semantics of a program as a state transformer,\nwhich can be compiled to multiple targets for testing, formal verification, and\ncode generation. The metalanguage itself is embedded in a strongly-typed host\nlanguage (Haskell), providing a way to prove program properties at the type\nlevel, which can shorten the feedback loop and further increase the\nproductivity of engineers.\n  The verification approach is demonstrated on an industrial case study. We\npresent REDFIN, a processing core used in space missions, and its formal\nsemantics expressed using the proposed metalanguage, followed by a detailed\nexample of verification of a simple control program.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 00:18:20 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Mokhov", "Andrey", ""], ["Lukyanov", "Georgy", ""], ["Lechner", "Jakob", ""]]}, {"id": "1802.01790", "submitter": "EPTCS", "authors": "Davide Ancona, Luca Franceschini, Giorgio Delzanno, Maurizio Leotta,\n  Marina Ribaudo, Filippo Ricca", "title": "Towards Runtime Monitoring of Node.js and Its Application to the\n  Internet of Things", "comments": "In Proceedings ALP4IoT 2017, arXiv:1802.00976", "journal-ref": "EPTCS 264, 2018, pp. 27-42", "doi": "10.4204/EPTCS.264.4", "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last years Node.js has emerged as a framework particularly suitable\nfor implementing lightweight IoT applications, thanks to its underlying\nasynchronous event-driven, non blocking I/O model. However, verifying the\ncorrectness of programs with asynchronous nested callbacks is quite difficult,\nand, hence, runtime monitoring can be a valuable support to tackle such a\ncomplex task.\n  Runtime monitoring is a useful software verification technique that\ncomplements static analysis and testing, but has not been yet fully explored in\nthe context of Internet of Things (IoT) systems. Trace expressions have been\nsuccessfully employed for runtime monitoring in widespread multiagent system\nplatforms. Recently, their expressive power has been extended to allow\nparametric specifications on data that can be captured and monitored only at\nruntime. Furthermore, they can be language and system agnostic, through the\nnotion of event domain and type. This paper investigates the use of parametric\ntrace expressions as a first step towards runtime monitoring of programs\ndeveloped in Node.js and Node-RED, a flow-based IoT programming tool built on\ntop of Node.js. Runtime verification of such systems is a task that mostly\nseems to have been overlooked so far in the literature.\n  A prototype implementing the proposed system for Node.js, in order to\ndynamically check with trace expressions the correct usage of API functions, is\npresented. The tool exploits the dynamic analysis framework Jalangi for\nmonitoring Node.js programs and allows detection of errors that would be\ndifficult to catch with other techniques. Furthermore, it offers a simple REST\ninterface which can be exploited for runtime verification of Node-RED\ncomponents, and, more generally, IoT devices.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 04:10:13 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Ancona", "Davide", ""], ["Franceschini", "Luca", ""], ["Delzanno", "Giorgio", ""], ["Leotta", "Maurizio", ""], ["Ribaudo", "Marina", ""], ["Ricca", "Filippo", ""]]}, {"id": "1802.01866", "submitter": "Brijesh Dongol", "authors": "Simon Doherty, John Derrick, Brijesh Dongol, Heike Wehrheim", "title": "Causal Linearizability: Compositionality for Partially Ordered\n  Executions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the interleaving model of concurrency, where events are totally ordered,\nlinearizability is compositional: the composition of two linearizable objects\nis guaranteed to be linearizable. However, linearizability is not compositional\nwhen events are only partially ordered, as in many weak-memory models that\ndescribe multicore memory systems. In this paper, we present causal\nlinearizability, a correctness condition for concurrent objects implemented in\nweak-memory models. We abstract from the details of specific memory models by\ndefining our condition using Lamport's execution structures. We apply our\ncondition to the C11 memory model, providing a correctness condition for C11\nobjects. We develop a proof method for verifying objects implemented in C11 and\nrelated models. Our method is an adaptation of simulation-based methods, but in\ncontrast to other such methods, it does not require that the implementation\ntotally order its events. We also show that causal linearizability reduces to\nlinearizability in the totally ordered case.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 09:57:58 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Doherty", "Simon", ""], ["Derrick", "John", ""], ["Dongol", "Brijesh", ""], ["Wehrheim", "Heike", ""]]}, {"id": "1802.01930", "submitter": "Dmitry Boulytchev", "authors": "Dmitri Boulytchev", "title": "Code Reuse With Transformation Objects", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for a lightweight datatype-generic programming in\nObjective Caml programming language aimed at better code reuse. We show, that a\nlarge class of transformations usually expressed via recursive functions with\npattern matching can be implemented using the single per-type traversal\nfunction and the set of object-encoded transformations, which we call\ntransformation objects. Object encoding allows transformations to be modified,\ninherited and extended in a conventional object-oriented manner. However, the\ndata representation is kept untouched which preserves the ability to construct\nand pattern-match it in the usual way. Our approach equally works for regular\nand polymorphic variant types which makes it possible to combine data types and\ntheir transformations from statically typed and separately compiled components.\nWe also present an implementation which allows us to automatically derive most\nfunctionality from a slightly augmented type descriptions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 13:11:37 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Boulytchev", "Dmitri", ""]]}, {"id": "1802.01957", "submitter": "Nirmal Prajapati", "authors": "Nirmal Prajapati, Sanjay Rajopadhye and Hristo Djidjev", "title": "Analytical Cost Metrics : Days of Future Past", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we move towards the exascale era, the new architectures must be capable of\nrunning the massive computational problems efficiently. Scientists and\nresearchers are continuously investing in tuning the performance of\nextreme-scale computational problems. These problems arise in almost all areas\nof computing, ranging from big data analytics, artificial intelligence, search,\nmachine learning, virtual/augmented reality, computer vision, image/signal\nprocessing to computational science and bioinformatics. With Moore's law\ndriving the evolution of hardware platforms towards exascale, the dominant\nperformance metric (time efficiency) has now expanded to also incorporate\npower/energy efficiency. Therefore, the major challenge that we face in\ncomputing systems research is: \"how to solve massive-scale computational\nproblems in the most time/power/energy efficient manner?\"\n  The architectures are constantly evolving making the current performance\noptimizing strategies less applicable and new strategies to be invented. The\nsolution is for the new architectures, new programming models, and applications\nto go forward together. Doing this is, however, extremely hard. There are too\nmany design choices in too many dimensions. We propose the following strategy\nto solve the problem: (i) Models - Develop accurate analytical models (e.g.\nexecution time, energy, silicon area) to predict the cost of executing a given\nprogram, and (ii) Complete System Design - Simultaneously optimize all the cost\nmodels for the programs (computational problems) to obtain the most\ntime/area/power/energy efficient solution. Such an optimization problem evokes\nthe notion of codesign.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 06:51:02 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Prajapati", "Nirmal", ""], ["Rajopadhye", "Sanjay", ""], ["Djidjev", "Hristo", ""]]}, {"id": "1802.02229", "submitter": "Shumo Chu", "authors": "Shumo Chu, Brendan Murphy, Jared Roesch, Alvin Cheung, and Dan Suciu", "title": "Axiomatic Foundations and Algorithms for Deciding Semantic Equivalences\n  of SQL Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deciding the equivalence of SQL queries is a fundamental problem in data\nmanagement. As prior work has mainly focused on studying the theoretical\nlimitations of the problem, very few implementations for checking such\nequivalences exist. In this paper, we present a new formalism and\nimplementation for reasoning about the equivalences of SQL queries. Our\nformalism, U-semiring, extends SQL's semiring semantics with unbounded\nsummation and duplicate elimination. U-semiring is defined using only very few\naxioms and can thus be easily implemented using proof assistants such as Coq\nfor automated query reasoning. Yet, they are sufficient enough to enable us\nreason about sophisticated SQL queries that are evaluated over bags and sets,\nalong with various integrity constraints. To evaluate the effectiveness of\nU-semiring, we have used it to formally verify 39 query rewrite rules from both\nclassical data management research papers and real-world SQL engines, where\nmany of them have never been proven correct before.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 21:40:50 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 01:01:38 GMT"}, {"version": "v3", "created": "Thu, 24 May 2018 01:20:14 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Chu", "Shumo", ""], ["Murphy", "Brendan", ""], ["Roesch", "Jared", ""], ["Cheung", "Alvin", ""], ["Suciu", "Dan", ""]]}, {"id": "1802.02353", "submitter": "Neel Kant", "authors": "Neel Kant", "title": "Recent Advances in Neural Program Synthesis", "comments": "16 pages (without citations); Literature Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning has made tremendous progress in a number of\nfields that were previously out of reach for artificial intelligence. The\nsuccesses in these problems has led researchers to consider the possibilities\nfor intelligent systems to tackle a problem that humans have only recently\nthemselves considered: program synthesis. This challenge is unlike others such\nas object recognition and speech translation, since its abstract nature and\ndemand for rigor make it difficult even for human minds to attempt. While it is\nstill far from being solved or even competitive with most existing methods,\nneural program synthesis is a rapidly growing discipline which holds great\npromise if completely realized. In this paper, we start with exploring the\nproblem statement and challenges of program synthesis. Then, we examine the\nfascinating evolution of program induction models, along with how they have\nsucceeded, failed and been reimagined since. Finally, we conclude with a\ncontrastive look at program synthesis and future research recommendations for\nthe field.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 08:44:38 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Kant", "Neel", ""]]}, {"id": "1802.02917", "submitter": "Fabrizio Montesi", "authors": "Fabrizio Montesi", "title": "Classical Higher-Order Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical Processes (CP) is a calculus where the proof theory of classical\nlinear logic types communicating processes with mobile channels, a la\npi-calculus. Its construction builds on a recent propositions as types\ncorrespondence between session types and propositions in linear logic.\nDesirable properties such as type preservation under reductions and progress\ncome for free from the metatheory of linear logic.\n  We contribute to this research line by extending CP with code mobility. We\ngeneralise classical linear logic to capture higher-order (linear) reasoning on\nproofs, which yields a logical reconstruction of (a variant of) the\nHigher-Order pi-calculus (HOpi). The resulting calculus is called Classical\nHigher-Order Processes (CHOP). We explore the metatheory of CHOP, proving that\nits semantics enjoys type preservation and progress (terms do not get stuck).\nWe also illustrate the expressivity of CHOP through examples, derivable syntax\nsugar, and an extension to multiparty sessions. Lastly, we define a translation\nfrom CHOP to CP, which encodes mobility of process code into reference passing.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 15:21:22 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Montesi", "Fabrizio", ""]]}, {"id": "1802.02951", "submitter": "Joseph Tassarotti", "authors": "Joseph Tassarotti, Robert Harper", "title": "A Separation Logic for Concurrent Randomized Programs", "comments": "31 pages. Extended version of POPL 2019 paper", "journal-ref": "Proc. ACM Program. Lang. 3, POPL, Article 64 (January 2019)", "doi": "10.1145/3290377", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Polaris, a concurrent separation logic with support for\nprobabilistic reasoning. As part of our logic, we extend the idea of coupling,\nwhich underlies recent work on probabilistic relational logics, to the setting\nof programs with both probabilistic and non-deterministic choice. To\ndemonstrate Polaris, we verify a variant of a randomized concurrent counter\nalgorithm and a two-level concurrent skip list. All of our results have been\nmechanized in Coq.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 16:29:43 GMT"}, {"version": "v2", "created": "Wed, 21 Nov 2018 16:12:53 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Tassarotti", "Joseph", ""], ["Harper", "Robert", ""]]}, {"id": "1802.02974", "submitter": "Arjun Guha", "authors": "Samuel Baxter, Rachit Nigam, Joe Gibbs Politz, Shriram Krishnamurthi\n  and Arjun Guha", "title": "Putting in All the Stops: Execution Control for JavaScript", "comments": "In proceedings of ACM SIGPLAN Conference on Programming Language\n  Design and Implementation (PLDI) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scores of compilers produce JavaScript, enabling programmers to use many\nlanguages on the Web, reuse existing code, and even use Web IDEs.\nUnfortunately, most compilers inherit the browser's compromised execution\nmodel, so long-running programs freeze the browser tab, infinite loops crash\nIDEs, and so on. The few compilers that avoid these problems suffer poor\nperformance and are difficult to engineer.\n  This paper presents Stopify, a source-to-source compiler that extends\nJavaScript with debugging abstractions and blocking operations, and easily\nintegrates with existing compilers. We apply Stopify to 10 programming\nlanguages and develop a Web IDE that supports stopping, single-stepping,\nbreakpointing, and long-running computations. For nine languages, Stopify\nrequires no or trivial compiler changes. For eight, our IDE is the first that\nprovides these features. Two of our subject languages have compilers with\nsimilar features. Stopify's performance is competitive with these compilers and\nit makes them dramatically simpler.\n  Stopify's abstractions rely on first-class continuations, which it provides\nby compiling JavaScript to JavaScript. We also identify sub-languages of\nJavaScript that compilers implicitly use, and exploit these to improve\nperformance. Finally, Stopify needs to repeatedly interrupt and resume program\nexecution. We use a sampling-based technique to estimate program speed that\noutperforms other systems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 17:32:34 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 00:30:27 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Baxter", "Samuel", ""], ["Nigam", "Rachit", ""], ["Politz", "Joe Gibbs", ""], ["Krishnamurthi", "Shriram", ""], ["Guha", "Arjun", ""]]}, {"id": "1802.03155", "submitter": "Novanto Yudistira", "authors": "Aryo Pinandito, Novanto Yudistira, Fajar Pradana", "title": "Web-Based Implementation of Travelling Salesperson Problem Using Genetic\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The world is connected through the Internet. As the abundance of Internet\nusers connected into the Web and the popularity of cloud computing research,\nthe need of Artificial Intelligence (AI) is demanding. In this research,\nGenetic Algorithm (GA) as AI optimization method through natural selection and\ngenetic evolution is utilized. There are many applications of GA such as web\nmining, load balancing, routing, and scheduling or web service selection.\nHence, it is a challenging task to discover whether the code mainly server side\nand web based language technology affects the performance of GA. Travelling\nSalesperson Problem (TSP) as Non Polynomial-hard (NP-hard) problem is provided\nto be a problem domain to be solved by GA. While many scientists prefer Python\nin GA implementation, another popular high-level interpreter programming\nlanguage such as PHP (PHP Hypertext Preprocessor) and Ruby were benchmarked.\nLine of codes, file sizes, and performances based on GA implementation and\nruntime were found varies among these programming languages. Based on the\nresult, the use of Ruby in GA implementation is recommended.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 07:30:38 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Pinandito", "Aryo", ""], ["Yudistira", "Novanto", ""], ["Pradana", "Fajar", ""]]}, {"id": "1802.03478", "submitter": "Bing Li", "authors": "Bing Li", "title": "Programming Requests/Responses with GreatFree in the Cloud Environment", "comments": "20 pages, 16 listings, 4 figures, 4 tables, International Journal of\n  Distributed and Parallel Systems, 2018", "journal-ref": null, "doi": "10.5121/ijdps.2018.9101", "report-no": null, "categories": "cs.PL cs.DC cs.SE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Programming request with GreatFree is an efficient programming technique to\nimplement distributed polling in the cloud computing environment. GreatFree is\na distributed programming environment through which diverse distributed systems\ncan be established through programming rather than configuring or scripting.\nGreatFree emphasizes the importance of programming since it offers developers\nthe opportunities to leverage their distributed knowledge and programming\nskills. Additionally, programming is the unique way to construct creative,\nadaptive and flexible systems to accommodate various distributed computing\nenvironments. With the support of GreatFree code-level Distributed\nInfrastructure Patterns, Distributed Operation Patterns and APIs, the difficult\nprocedure is accomplished in a programmable, rapid and highly-patterned manner,\ni.e., the programming behaviors are simplified as the repeatable operation of\nCopy-Paste-Replace. Since distributed polling is one of the fundamental\ntechniques to construct distributed systems, GreatFree provides developers with\nrelevant APIs and patterns to program requests/responses in the novel\nprogramming environment.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 23:48:44 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Li", "Bing", ""]]}, {"id": "1802.03691", "submitter": "Xinyun Chen", "authors": "Xinyun Chen, Chang Liu, Dawn Song", "title": "Tree-to-tree Neural Networks for Program Translation", "comments": "Published in NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program translation is an important tool to migrate legacy code in one\nlanguage into an ecosystem built in a different language. In this work, we are\nthe first to employ deep neural networks toward tackling this problem. We\nobserve that program translation is a modular procedure, in which a sub-tree of\nthe source tree is translated into the corresponding target sub-tree at each\nstep. To capture this intuition, we design a tree-to-tree neural network to\ntranslate a source tree into a target one. Meanwhile, we develop an attention\nmechanism for the tree-to-tree model, so that when the decoder expands one\nnon-terminal in the target tree, the attention mechanism locates the\ncorresponding sub-tree in the source tree to guide the expansion of the\ndecoder. We evaluate the program translation capability of our tree-to-tree\nmodel against several state-of-the-art approaches. Compared against other\nneural translation models, we observe that our approach is consistently better\nthan the baselines with a margin of up to 15 points. Further, our approach can\nimprove the previous state-of-the-art program translation approaches by a\nmargin of 20 points on the translation of real-world projects.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 04:42:03 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 17:43:13 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 03:51:27 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Chen", "Xinyun", ""], ["Liu", "Chang", ""], ["Song", "Dawn", ""]]}, {"id": "1802.03950", "submitter": "Huyen T.T Nguyen", "authors": "Huyen T.T Nguyen, C\\'esar Rodr\\'iguez, Marcelo Sousa, Camille Coti and\n  Laure Petrucci", "title": "Quasi-Optimal Partial Order Reduction", "comments": "Minor corrections after review for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dynamic partial order reduction (DPOR) algorithm is optimal when it always\nexplores at most one representative per Mazurkiewicz trace. Existing literature\nsuggests that the reduction obtained by the non-optimal, state-of-the-art\nSource-DPOR (SDPOR) algorithm is comparable to optimal DPOR. We show the first\nprogram with $\\mathop{\\mathcal{O}}(n)$ Mazurkiewicz traces where SDPOR explores\n$\\mathop{\\mathcal{O}}(2^n)$ redundant schedules and identify the cause of the\nblow-up as an NP-hard problem. Our main contribution is a new approach, called\nQuasi-Optimal POR, that can arbitrarily approximate an optimal exploration\nusing a provided constant k. We present an implementation of our method in a\nnew tool called Dpu using specialised data structures. Experiments with Dpu,\nincluding Debian packages, show that optimality is achieved with low values of\nk, outperforming state-of-the-art tools.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 09:55:20 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 10:15:56 GMT"}, {"version": "v3", "created": "Fri, 20 Apr 2018 09:47:13 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Nguyen", "Huyen T. T", ""], ["Rodr\u00edguez", "C\u00e9sar", ""], ["Sousa", "Marcelo", ""], ["Coti", "Camille", ""], ["Petrucci", "Laure", ""]]}, {"id": "1802.03998", "submitter": "Salvador Tamarit", "authors": "David Insa, Sergio P\\'erez, Josep Silva, Salvador Tamarit", "title": "Erlang Code Evolution Control (Use Cases)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of this work is to show how SecEr can be used in different\nscenarios. Concretely, we demonstrate how a user can run SecEr to obtain\nreports about the behaviour preservation between versions as well as how a user\ncan use SecEr to find the source of a discrepancy. The use cases presented are\nthree: two completely different versions of the same program, an improvement in\nthe performance of a function and a program where an error has been introduced.\nA complete description of the technique and the tool is available at [1] and\n[2].\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 12:04:49 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Insa", "David", ""], ["P\u00e9rez", "Sergio", ""], ["Silva", "Josep", ""], ["Tamarit", "Salvador", ""]]}, {"id": "1802.04174", "submitter": "Tomas Balyo", "authors": "Marko Kleine B\\\"uning, Tomas Balyo, Carsten Sinz", "title": "Unbounded Software Model Checking with Incremental SAT-Solving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel unbounded software model checking approach to\nfind errors in programs written in the C language based on incremental\nSAT-solving. Instead of using the traditional assumption based API to\nincremental SAT solvers we use the DimSpec format that is used in SAT based\nautomated planning. A DimSpec formula consists of four CNF formulas\nrepresenting the initial, goal and intermediate states and the relations\nbetween each pair of neighboring states of a transition system. We present a\nnew tool called LLUMC which encodes the presence of certain errors in a C\nprogram into a DimSpec formula, which can be solved by either an incremental\nSAT-based DimSpec solver or the IC3 algorithm for invariant checking. We\nevaluate the approach in the context of SAT-based model checking for both the\nincremental SAT-solving and the IC3 algorithm. We show that our encoding\nexpands the functionality of bounded model checkers by also covering large and\ninfinite loops, while still maintaining a feasible time performance.\nFurthermore, we demonstrate that our approach offers the opportunity to\ngenerate runtime-optimizations by utilizing parallel SAT-solving.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 16:43:06 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["B\u00fcning", "Marko Kleine", ""], ["Balyo", "Tomas", ""], ["Sinz", "Carsten", ""]]}, {"id": "1802.04335", "submitter": "Illia Polosukhin", "authors": "Illia Polosukhin, Alexander Skidanov", "title": "Neural Program Search: Solving Programming Tasks from Description and\n  Examples", "comments": "9 pages, 3 figures, ICLR workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Neural Program Search, an algorithm to generate programs from\nnatural language description and a small number of input/output examples. The\nalgorithm combines methods from Deep Learning and Program Synthesis fields by\ndesigning rich domain-specific language (DSL) and defining efficient search\nalgorithm guided by a Seq2Tree model on it. To evaluate the quality of the\napproach we also present a semi-synthetic dataset of descriptions with test\nexamples and corresponding programs. We show that our algorithm significantly\noutperforms a sequence-to-sequence model with attention baseline.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 20:05:26 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Polosukhin", "Illia", ""], ["Skidanov", "Alexander", ""]]}, {"id": "1802.04406", "submitter": "Robert Colvin", "authors": "Robert J. Colvin and Graeme Smith", "title": "A wide-spectrum language for verification of programs on weak memory\n  models", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-95582-7_14", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern processors deploy a variety of weak memory models, which for\nefficiency reasons may (appear to) execute instructions in an order different\nto that specified by the program text. The consequences of instruction\nreordering can be complex and subtle, and can impact on ensuring correctness.\nPrevious work on the semantics of weak memory models has focussed on the\nbehaviour of assembler-level programs. In this paper we utilise that work to\nextract some general principles underlying instruction reordering, and apply\nthose principles to a wide-spectrum language encompassing abstract data types\nas well as low-level assembler code. The goal is to support reasoning about\nimplementations of data structures for modern processors with respect to an\nabstract specification.\n  Specifically, we define an operational semantics, from which we derive some\nproperties of program refinement, and encode the semantics in the rewriting\nengine Maude as a model-checking tool. The tool is used to validate the\nsemantics against the behaviour of a set of litmus tests (small assembler\nprograms) run on hardware, and also to model check implementations of data\nstructures from the literature against their abstract specifications.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 00:23:16 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Colvin", "Robert J.", ""], ["Smith", "Graeme", ""]]}, {"id": "1802.04408", "submitter": "Jeevana Priya Inala", "authors": "Jeevana Priya Inala, Sicun Gao, Soonho Kong and Armando Solar-Lezama", "title": "REAS: Combining Numerical Optimization with SAT Solving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present ReaS, a technique that combines numerical\noptimization with SAT solving to synthesize unknowns in a program that involves\ndiscrete and floating point computation. ReaS makes the program end-to-end\ndifferentiable by smoothing any Boolean expression that introduces\ndiscontinuity such as conditionals and relaxing the Boolean unknowns so that\nnumerical optimization can be performed. On top of this, ReaS uses a SAT solver\nto help the numerical search overcome local solutions by incrementally fixing\nvalues to the Boolean expressions. We evaluated the approach on 5 case studies\ninvolving hybrid systems and show that ReaS can synthesize programs that could\nnot be solved by previous SMT approaches.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 00:29:31 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Inala", "Jeevana Priya", ""], ["Gao", "Sicun", ""], ["Kong", "Soonho", ""], ["Solar-Lezama", "Armando", ""]]}, {"id": "1802.04428", "submitter": "Xiaokang Qiu", "authors": "Kangjing Huang and Xiaokang Qiu and Qi Tian and Yanjun Wang", "title": "Reconciling Enumerative and Symbolic Search in Syntax-Guided Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Syntax-guided synthesis aims to find a program satisfying semantic\nspecification as well as user-provided structural hypothesis. For syntax-guided\nsynthesis there are two main search strategies: concrete search, which\nsystematically or stochastically enumerates all possible solutions, and\nsymbolic search, which interacts with a constraint solver to solve the\nsynthesis problem. In this paper, we propose a concolic synthesis framework\nwhich combines the best of the two worlds. Based on a decision tree\nrepresentation, our framework works by enumerating tree heights from the\nsmallest possible one to larger ones. For each fixed height, the framework\nsymbolically searches a solution through the counterexample-guided inductive\nsynthesis approach. To compensate the exponential blow-up problem with the\nconcolic synthesis framework, we identify two fragments of synthesis problems\nand develop purely symbolic and more efficient procedures. The two fragments\nare decidable as these procedures are terminating and complete. We implemented\nour synthesis procedures and compared with state-of-the-art synthesizers on a\nrange of benchmarks. Experiments show that our algorithms are promising.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 01:53:50 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Huang", "Kangjing", ""], ["Qiu", "Xiaokang", ""], ["Tian", "Qi", ""], ["Wang", "Yanjun", ""]]}, {"id": "1802.04730", "submitter": "Albert Cohen", "authors": "Nicolas Vasilache, Oleksandr Zinenko, Theodoros Theodoridis, Priya\n  Goyal, Zachary DeVito, William S. Moses, Sven Verdoolaege, Andrew Adams,\n  Albert Cohen", "title": "Tensor Comprehensions: Framework-Agnostic High-Performance Machine\n  Learning Abstractions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models with convolutional and recurrent networks are now\nubiquitous and analyze massive amounts of audio, image, video, text and graph\ndata, with applications in automatic translation, speech-to-text, scene\nunderstanding, ranking user preferences, ad placement, etc. Competing\nframeworks for building these networks such as TensorFlow, Chainer, CNTK,\nTorch/PyTorch, Caffe1/2, MXNet and Theano, explore different tradeoffs between\nusability and expressiveness, research or production orientation and supported\nhardware. They operate on a DAG of computational operators, wrapping\nhigh-performance libraries such as CUDNN (for NVIDIA GPUs) or NNPACK (for\nvarious CPUs), and automate memory allocation, synchronization, distribution.\nCustom operators are needed where the computation does not fit existing\nhigh-performance library calls, usually at a high engineering cost. This is\nfrequently required when new operators are invented by researchers: such\noperators suffer a severe performance penalty, which limits the pace of\ninnovation. Furthermore, even if there is an existing runtime call these\nframeworks can use, it often doesn't offer optimal performance for a user's\nparticular network architecture and dataset, missing optimizations between\noperators as well as optimizations that can be done knowing the size and shape\nof data. Our contributions include (1) a language close to the mathematics of\ndeep learning called Tensor Comprehensions, (2) a polyhedral Just-In-Time\ncompiler to convert a mathematical description of a deep learning DAG into a\nCUDA kernel with delegated memory management and synchronization, also\nproviding optimizations such as operator fusion and specialization for specific\nsizes, (3) a compilation cache populated by an autotuner. [Abstract cutoff]\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 16:53:01 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 08:57:44 GMT"}, {"version": "v3", "created": "Fri, 29 Jun 2018 00:16:36 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Vasilache", "Nicolas", ""], ["Zinenko", "Oleksandr", ""], ["Theodoridis", "Theodoros", ""], ["Goyal", "Priya", ""], ["DeVito", "Zachary", ""], ["Moses", "William S.", ""], ["Verdoolaege", "Sven", ""], ["Adams", "Andrew", ""], ["Cohen", "Albert", ""]]}, {"id": "1802.04799", "submitter": "Tianqi Chen", "authors": "Tianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin Zheng, Eddie Yan,\n  Meghan Cowan, Haichen Shen, Leyuan Wang, Yuwei Hu, Luis Ceze, Carlos\n  Guestrin, Arvind Krishnamurthy", "title": "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning", "comments": "Significantly improved version, add automated optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing need to bring machine learning to a wide diversity of\nhardware devices. Current frameworks rely on vendor-specific operator libraries\nand optimize for a narrow range of server-class GPUs. Deploying workloads to\nnew platforms -- such as mobile phones, embedded devices, and accelerators\n(e.g., FPGAs, ASICs) -- requires significant manual effort. We propose TVM, a\ncompiler that exposes graph-level and operator-level optimizations to provide\nperformance portability to deep learning workloads across diverse hardware\nback-ends. TVM solves optimization challenges specific to deep learning, such\nas high-level operator fusion, mapping to arbitrary hardware primitives, and\nmemory latency hiding. It also automates optimization of low-level programs to\nhardware characteristics by employing a novel, learning-based cost modeling\nmethod for rapid exploration of code optimizations. Experimental results show\nthat TVM delivers performance across hardware back-ends that are competitive\nwith state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and\nserver-class GPUs. We also demonstrate TVM's ability to target new accelerator\nback-ends, such as the FPGA-based generic deep learning accelerator. The system\nis open sourced and in production use inside several major companies.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 20:49:34 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 18:44:40 GMT"}, {"version": "v3", "created": "Fri, 5 Oct 2018 18:47:38 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Chen", "Tianqi", ""], ["Moreau", "Thierry", ""], ["Jiang", "Ziheng", ""], ["Zheng", "Lianmin", ""], ["Yan", "Eddie", ""], ["Cowan", "Meghan", ""], ["Shen", "Haichen", ""], ["Wang", "Leyuan", ""], ["Hu", "Yuwei", ""], ["Ceze", "Luis", ""], ["Guestrin", "Carlos", ""], ["Krishnamurthy", "Arvind", ""]]}, {"id": "1802.04970", "submitter": "Aleksandar S. Dimovski", "authors": "Aleksandar S. Dimovski", "title": "Abstract Family-based Model Checking using Modal Featured Transition\n  Systems: Preservation of CTL* (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational systems allow effective building of many custom variants by using\nfeatures (configuration options) to mark the variable functionality. In many of\nthe applications, their quality assurance and formal verification are of\nparamount importance. Family-based model checking allows simultaneous\nverification of all variants of a variational system in a single run by\nexploiting the commonalities between the variants. Yet, its computational cost\nstill greatly depends on the number of variants (often huge).\n  In this work, we show how to achieve efficient family-based model checking of\nCTL* temporal properties using variability abstractions and off-the-shelf\n(single-system) tools. We use variability abstractions for deriving abstract\nfamily-based model checking, where the variability model of a variational\nsystem is replaced with an abstract (smaller) version of it, called modal\nfeatured transition system, which preserves the satisfaction of both universal\nand existential temporal properties, as expressible in CTL*. Modal featured\ntransition systems contain two kinds of transitions, termed may and must\ntransitions, which are defined by the conservative (over-approximating)\nabstractions and their dual (under-approximating) abstractions, respectively.\nThe variability abstractions can be combined with different partitionings of\nthe set of variants to infer suitable divide-and-conquer verification plans for\nthe variational system. We illustrate the practicality of this approach for\nseveral variational systems.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 07:00:01 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2018 12:23:13 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Dimovski", "Aleksandar S.", ""]]}, {"id": "1802.05862", "submitter": "EPTCS", "authors": "Horatiu Cirstea (LORIA, Universit\\'e de Lorraine, France), David Sabel\n  (Goethe-University Frankfurt am Main, Germany)", "title": "Proceedings Fourth International Workshop on Rewriting Techniques for\n  Program Transformations and Evaluation", "comments": null, "journal-ref": "EPTCS 265, 2018", "doi": "10.4204/EPTCS.265", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the formal proceedings of the 4th International Workshop\non Rewriting Techniques for Program Transformations and Evaluation (WPTE 2017),\nheld on 8th September 2017 in Oxford, United Kingdom, and affiliated with the\nSecond International Conference on Formal Structures for Computation and\nDeduction (FSCD 2017).\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 08:31:08 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Cirstea", "Horatiu", "", "LORIA, Universit\u00e9 de Lorraine, France"], ["Sabel", "David", "", "Goethe-University Frankfurt am Main, Germany"]]}, {"id": "1802.05863", "submitter": "Hugo Torres Vieira", "authors": "Jovanka Pantovic (1), Ivan Prokic (1), Hugo Torres Vieira (2) ((1)\n  Faculty of Technical Sciences, University of Novi Sad, (2) IMT School for\n  Advanced Studies Lucca)", "title": "A Calculus for Modeling Floating Authorizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlling resource usage in distributed systems is a challenging task given\nthe dynamics involved in access granting. Consider, for instance, the setting\nof floating licenses where access can be granted if the request originates in a\nlicensed domain and the number of active users is within the license limits,\nand where licenses can be interchanged. Access granting in such scenarios is\ngiven in terms of floating authorizations, addressed in this paper as first\nclass entities of a process calculus model, encompassing the notions of domain,\naccounting and delegation. We present the operational semantics of the model in\ntwo equivalent alternative ways, each informing on the specific nature of\nauthorizations. We also introduce a typing discipline to single out systems\nthat never get stuck due to lacking authorizations, addressing configurations\nwhere authorization assignment is not statically prescribed in the system\nspecification.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 08:34:28 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Pantovic", "Jovanka", ""], ["Prokic", "Ivan", ""], ["Vieira", "Hugo Torres", ""]]}, {"id": "1802.05918", "submitter": "Alexey Gotsman", "authors": "Mike Dodds, Mark Batty, and Alexey Gotsman", "title": "Compositional Verification of Compiler Optimisations on Relaxed Memory", "comments": "Extended version of the paper from ESOP'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A valid compiler optimisation transforms a block in a program without\nintroducing new observable behaviours to the program as a whole. Deciding which\noptimisations are valid can be difficult, and depends closely on the semantic\nmodel of the programming language. Axiomatic relaxed models, such as C++11,\npresent particular challenges for determining validity, because such models\nallow subtle effects of a block transformation to be observed by the rest of\nthe program. In this paper we present a denotational theory that captures\noptimisation validity on an axiomatic model corresponding to a fragment of\nC++11. Our theory allows verifying an optimisation compositionally, by\nconsidering only the block it transforms instead of the whole program. Using\nthis property, we realise the theory in the first push-button tool that can\nverify real-world optimisations under an axiomatic memory model.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 12:58:12 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Dodds", "Mike", ""], ["Batty", "Mark", ""], ["Gotsman", "Alexey", ""]]}, {"id": "1802.06283", "submitter": "Justin Hsu", "authors": "Alejandro Aguirre, Gilles Barthe, Justin Hsu, Alexandra Silva", "title": "Almost Sure Productivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define Almost Sure Productivity (ASP), a probabilistic generalization of\nthe productivity condition for coinductively defined structures. Intuitively, a\nprobabilistic coinductive stream or tree is ASP if it produces infinitely many\noutputs with probability 1. Formally, we define almost sure productivity using\na final coalgebra semantics of programs inspired from Kerstan and K\\\"onig.\nThen, we introduce a core language for probabilistic streams and trees, and\nprovide two approaches to verify ASP: a sufficient syntactic criterion, and a\nreduction to model-checking pCTL* formulas on probabilistic pushdown automata.\nThe reduction shows that ASP is decidable for our core language.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 20:14:10 GMT"}, {"version": "v2", "created": "Sun, 13 May 2018 21:58:17 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Aguirre", "Alejandro", ""], ["Barthe", "Gilles", ""], ["Hsu", "Justin", ""], ["Silva", "Alexandra", ""]]}, {"id": "1802.06375", "submitter": "Jeremy Siek", "authors": "Andre Kuhlenschmidt and Deyaaeldeen Almahallawi and Jeremy G. Siek", "title": "Efficient Gradual Typing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradual typing combines static and dynamic typing in the same program. One\nwould hope that the performance in a gradually typed language would range\nbetween that of a dynamically typed language and a statically typed language.\nExisting implementations of gradually typed languages have not achieved this\ngoal due to overheads associated with runtime casts. Takikawa et al. (2016)\nreport up to 100$\\times$ slowdowns for partially typed programs. In this paper\nwe present a compiler, named Grift, for evaluating implementation techniques\nfor gradual typing. We take a straightforward but surprisingly unexplored\nimplementation approach for gradual typing, that is, ahead-of-time compilation\nto native assembly code with carefully chosen runtime representations and\nspace-efficient coercions.\n  Our experiments show that this approach achieves performance on par with\nOCaml on statically typed programs and performance between that of Gambit and\nRacket on untyped programs. On partially typed code, the geometric mean ranges\nfrom 0.42$\\times$ to 2.36$\\times$ that of (untyped) Racket across the\nbenchmarks. We implement casts using the coercions of Siek, Thiemann, and\nWadler (2015). This technique eliminates all catastrophic slowdowns without\nintroducing significant overhead. Across the benchmarks, coercions range from\n15% slower (fft) to almost 2$\\times$ faster (matmult) than regular casts. We\nalso implement the monotonic references of Siek et al. (2015). Monotonic\nreferences eliminate all overhead in statically typed code, and for partially\ntyped code, they are faster than proxied references, sometimes up to\n1.48$\\times$.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 13:30:45 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Kuhlenschmidt", "Andre", ""], ["Almahallawi", "Deyaaeldeen", ""], ["Siek", "Jeremy G.", ""]]}, {"id": "1802.06493", "submitter": "EPTCS", "authors": "Liyi Li (University of Illinois at Urbana-Champaign), Elsa Gunter\n  (University of Illinois at Urbana-Champaign)", "title": "A Method to Translate Order-Sorted Algebras to Many-Sorted Algebras", "comments": "In Proceedings WPTE 2017, arXiv:1802.05862", "journal-ref": "EPTCS 265, 2018, pp. 20-34", "doi": "10.4204/EPTCS.265.3", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Order-sorted algebras and many sorted algebras exist in a long history with\nmany different implementations and applications. A lot of language\nspecifications have been defined in order-sorted algebra frameworks such as the\nlanguage specifications in K (an order-sorted algebra framework). The biggest\nproblem in a lot of the order-sorted algebra frameworks is that even if they\nmight allow developers to write programs and language specifications easily,\nbut they do not have a large set of tools to provide reasoning infrastructures\nto reason about the specifications built on the frameworks, which are very\ncommon in some many-sorted algebra framework such as Isabelle/HOL, Coq and FDR.\nThis fact brings us the necessity to marry the worlds of order-sorted algebras\nand many sorted algebras. In this paper, we propose an algorithm to translate a\nstrictly sensible order-sorted algebra to a many-sorted one in a restricted\ndomain by requiring the order-sorted algebra to be strictly sensible. The key\nidea of the translation is to add an equivalence relation called core equality\nto the translated many-sorted algebras. By defining this relation, we reduce\nthe complexity of translating a strictly sensible order-sorted algebra to a\nmany-sorted one, make the translated many-sorted algebra equations only\nincreasing by a very small amount of new equations, and keep the number of\nrewrite rules in the algebra in the same amount. We then prove the order-sorted\nalgebra and its translated many-sorted algebra are bisimilar. To the best of\nour knowledge, our translation and bisimilar proof is the first attempt in\ntranslating and relating an order-sorted algebra with a many-sorted one in a\nway that keeps the size of the translated many-sorted algebra relatively small.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 02:06:10 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Li", "Liyi", "", "University of Illinois at Urbana-Champaign"], ["Gunter", "Elsa", "", "University of Illinois at Urbana-Champaign"]]}, {"id": "1802.06495", "submitter": "EPTCS", "authors": "Koko Muroya, Dan R. Ghica", "title": "Efficient Implementation of Evaluation Strategies via Token-Guided Graph\n  Rewriting", "comments": "In Proceedings WPTE 2017, arXiv:1802.05862", "journal-ref": "EPTCS 265, 2018, pp. 52-66", "doi": "10.4204/EPTCS.265.5", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In implementing evaluation strategies of the lambda-calculus, both\ncorrectness and efficiency of implementation are valid concerns. While the\nnotion of correctness is determined by the evaluation strategy, regarding\nefficiency there is a larger design space that can be explored, in particular\nthe trade-off between space versus time efficiency. We contributed to the study\nof this trade-off by the introduction of an abstract machine for call-by-need,\ninspired by Girard's Geometry of Interaction, a machine combining token passing\nand graph rewriting. This work presents an extension of the machine, to\nadditionally accommodate left-to-right and right-to-left call-by-value\nstrategies. We show soundness and completeness of the extended machine with\nrespect to each of the call-by-need and two call-by-value strategies. Analysing\ntime cost of its execution classifies the machine as \"efficient\" in Accattoli's\ntaxonomy of abstract machines.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 02:06:43 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Muroya", "Koko", ""], ["Ghica", "Dan R.", ""]]}, {"id": "1802.06498", "submitter": "EPTCS", "authors": "Manfred Schmidt-Schau{\\ss} (Goethe-University Frankfurt am Main), Nils\n  Dallmeyer (Goethe-University Frankfurt am Main)", "title": "Space Improvements and Equivalences in a Functional Core Language", "comments": "In Proceedings WPTE 2017, arXiv:1802.05862", "journal-ref": "EPTCS 265, 2018, pp. 98-112", "doi": "10.4204/EPTCS.265.8", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore space improvements in LRP, a polymorphically typed call-by-need\nfunctional core language. A relaxed space measure is chosen for the maximal\nsize usage during an evaluation. It abstracts from the details of the\nimplementation via abstract machines, but it takes garbage collection into\naccount and thus can be seen as a realistic approximation of space usage. The\nresults are: a context lemma for space improving translations and for space\nequivalences, all but one reduction rule of the calculus are shown to be space\nimprovements, and for the exceptional one we show bounds on the space increase.\nSeveral further program transformations are shown to be space improvements or\nspace equivalences in particular the translation into machine expressions is a\nspace equivalence. We also classify certain space-worsening transformations as\nspace-leaks or as space-safe. These results are a step forward in making\npredictions about the change in runtime space behavior of optimizing\ntransformations in call-by-need functional languages.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 02:07:27 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Schmidt-Schau\u00df", "Manfred", "", "Goethe-University Frankfurt am Main"], ["Dallmeyer", "Nils", "", "Goethe-University Frankfurt am Main"]]}, {"id": "1802.06504", "submitter": "Charisee Chiw", "authors": "Charisee Chiw and Gordon L. Kindlmann and John Reppy", "title": "Compiling Diderot: From Tensor Calculus to C", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diderot is a parallel domain-specific language for analysis and visualization\nof multidimensional scientific images, such as those produced by CT and MRI\nscanners. In particular, it supports algorithms where tensor fields (i.e.,\nfunctions from 3D points to tensor values) are used to represent the underlying\nphysical objects that were scanned by the imaging device. Diderot supports\nhigher-order programming where tensor fields are first-class values and where\ndifferential operators and lifted linear-algebra operators can be used to\nexpress mathematical reasoning directly in the language. While such lifted\nfield operations are central to the definition and computation of many\nscientific visualization algorithms, to date they have required extensive\nmanual derivations and laborious implementation.\n  The challenge for the Diderot compiler is to effectively translate the\nhigh-level mathematical concepts that are expressible in the surface language\nto a low-level and efficient implementation in C. This paper describes our\napproach to this challenge, which is based around the careful design of an\nintermediate representation (IR), called EIN, and a number of compiler\ntransformations that lower the program from tensor calculus to C while avoiding\ncombinatorial explosion in the size of the IR. We describe the challenges in\ncompiling a language like Diderot, the design of EIN, and the transformation\nused by the compiler. We also present an evaluation of EIN with respect to both\ncompiler efficiency and quality of generated code.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 02:38:51 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Chiw", "Charisee", ""], ["Kindlmann", "Gordon L.", ""], ["Reppy", "John", ""]]}, {"id": "1802.06551", "submitter": "Marcelo Sousa", "authors": "Marcelo Sousa, Isil Dillig and Shuvendu Lahiri", "title": "Verifying Semantic Conflict-Freedom in Three-Way Program Merges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though many programmers rely on 3-way merge tools to integrate changes\nfrom different branches, such tools can introduce subtle bugs in the\nintegration process. This paper aims to mitigate this problem by defining a\nsemantic notion of confict-freedom, which ensures that the merged program does\nnot introduce new unwanted behaviors. We also show how to verify this property\nusing a novel, compositional algorithm that combines lightweight dependence\nanalysis for shared program fragments and precise relational reasoning for the\nmodifications. We evaluate our tool called SafeMerge on 52 real-world merge\nscenarios obtained from Github and compare the results against a textual merge\ntool. The experimental results demonstrate the benefits of our approach over\nsyntactic confict-freedom and indicate that SafeMerge is both precise and\npractical.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 08:51:38 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Sousa", "Marcelo", ""], ["Dillig", "Isil", ""], ["Lahiri", "Shuvendu", ""]]}, {"id": "1802.06571", "submitter": "Alexandr Basov", "authors": "Alexandr Basov, Daniel de Carvalho, Manuel Mazzara", "title": "Implementing distributed {\\lambda}-calculus interpreter", "comments": "8 pages, 4 tables, 1 figure, proceeding AINA-2018 workshops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes how one can implement distributed {\\lambda}-calculus\ninterpreter from scratch. At first, we describe how to implement a monadic\nparser, than the Krivine Machine is introduced for the interpretation part and\nas for distribution, the actor model is used. In this work we are not providing\ngeneral solution for parallelism, but we consider particular patterns, which\nalways can be parallelized. As a result, the basic extensible implementation of\ncall-by-name distributed machine is introduced and prototype is presented. We\nachieved computation speed improvement in some cases, but efficient distributed\nversion is not achieved, problems are discussed in evaluation section. This\nwork provides a foundation for further research, completing the implementation\nit is possible to add concurrency for non-determinism, improve the interpreter\nusing call-by-need semantic or study optimal auto parallelization to generalize\nwhat could be done efficiently in parallel.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 09:46:39 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Basov", "Alexandr", ""], ["de Carvalho", "Daniel", ""], ["Mazzara", "Manuel", ""]]}, {"id": "1802.06651", "submitter": "Domenico Sacca'", "authors": "Domenico Sacca' and Angelo Furfaro", "title": "CalcuList: a Functional Language Extended with Imperative Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CalcuList (Calculator with List manipulation), is an educational language for\nteaching functional programming extended with some imperative and side-effect\nfeatures, which are enabled under explicit request by the programmer. In\naddition to strings and lists, the language natively supports json objects. The\nlanguage adopts a Python-like syntax and enables interactive computation\nsessions with the user through a REPL (Read-Evaluate-Print-Loop) shell. The\nobject code produced by a compilation is a program that will be eventually\nexecuted by the CalcuList Virtual Machine (CLVM).\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 14:42:34 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Sacca'", "Domenico", ""], ["Furfaro", "Angelo", ""]]}, {"id": "1802.06653", "submitter": "Emmanuel Hainry", "authors": "Emmanuel Hainry (CARTE), Romain P\\'echoux (CARTE)", "title": "A Type-Based Complexity Analysis of Object Oriented Programs", "comments": "Information and Computation, Elsevier, A Para\\^itre, pp.60", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A type system is introduced for a generic Object Oriented programming\nlanguage in order to infer resource upper bounds. A sound andcomplete\ncharacterization of the set of polynomial time computable functions is\nobtained. As a consequence, the heap-space and thestack-space requirements of\ntyped programs are also bounded polynomially. This type system is inspired by\nprevious works on ImplicitComputational Complexity, using tiering and\nnon-interference techniques. The presented methodology has several advantages.\nFirst, itprovides explicit big $O$ polynomial upper bounds to the programmer,\nhence its use could allow the programmer to avoid memory errors.Second, type\nchecking is decidable in polynomial time. Last, it has a good expressivity\nsince it analyzes most object oriented featureslike inheritance, overload,\noverride and recursion. Moreover it can deal with loops guarded by objects and\ncan also be extended tostatements that alter the control flow like break or\nreturn.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 14:46:36 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Hainry", "Emmanuel", "", "CARTE"], ["P\u00e9choux", "Romain", "", "CARTE"]]}, {"id": "1802.07284", "submitter": "Yanhong Annie Liu", "authors": "Yanhong A. Liu", "title": "Logic Programming Applications: What Are the Abstractions and\n  Implementations?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents an overview of applications of logic programming,\nclassifying them based on the abstractions and implementations of logic\nlanguages that support the applications. The three key abstractions are join,\nrecursion, and constraint. Their essential implementations are for-loops, fixed\npoints, and backtracking, respectively. The corresponding kinds of applications\nare database queries, inductive analysis, and combinatorial search,\nrespectively. We also discuss language extensions and programming paradigms,\nsummarize example application problems by application areas, and touch on\nexample systems that support variants of the abstractions with different\nimplementations.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 19:04:14 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Liu", "Yanhong A.", ""]]}, {"id": "1802.08234", "submitter": "Ian Sweet", "authors": "Ian Sweet, Jose Manuel Calderon Trilla, Chad Scherrer, Michael Hicks,\n  and Stephen Magill", "title": "What's the Over/Under? Probabilistic Bounds on Information Leakage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative information flow (QIF) is concerned with measuring how much of a\nsecret is leaked to an adversary who observes the result of a computation that\nuses it. Prior work has shown that QIF techniques based on abstract\ninterpretation with probabilistic polyhedra can be used to analyze the\nworst-case leakage of a query, on-line, to determine whether that query can be\nsafely answered. While this approach can provide precise estimates, it does not\nscale well. This paper shows how to solve the scalability problem by augmenting\nthe baseline technique with sampling and symbolic execution. We prove that our\napproach never underestimates a query's leakage (it is sound), and detailed\nexperimental results show that we can match the precision of the baseline\ntechnique but with orders of magnitude better performance.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 18:45:31 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Sweet", "Ian", ""], ["Trilla", "Jose Manuel Calderon", ""], ["Scherrer", "Chad", ""], ["Hicks", "Michael", ""], ["Magill", "Stephen", ""]]}, {"id": "1802.08307", "submitter": "Berkay Celik", "authors": "Z. Berkay Celik, Leonardo Babun, Amit K. Sikder, Hidayet Aksu, Gang\n  Tan, Patrick McDaniel, A. Selcuk Uluagac", "title": "Sensitive Information Tracking in Commodity IoT", "comments": "first submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Broadly defined as the Internet of Things (IoT), the growth of commodity\ndevices that integrate physical processes with digital connectivity has had\nprofound effects on society--smart homes, personal monitoring devices, enhanced\nmanufacturing and other IoT apps have changed the way we live, play, and work.\nYet extant IoT platforms provide few means of evaluating the use (and potential\navenues for misuse) of sensitive information. Thus, consumers and organizations\nhave little information to assess the security and privacy risks these devices\npresent. In this paper, we present SainT, a static taint analysis tool for IoT\napplications. SainT operates in three phases; (a) translation of\nplatform-specific IoT source code into an intermediate representation (IR), (b)\nidentifying sensitive sources and sinks, and (c) performing static analysis to\nidentify sensitive data flows. We evaluate SainT on 230 SmartThings market apps\nand find 138 (60%) include sensitive data flows. In addition, we demonstrate\nSainT on IoTBench, a novel open-source test suite containing 19 apps with 27\nunique data leaks. Through this effort, we introduce a rigorously grounded\nframework for evaluating the use of sensitive information in IoT apps---and\ntherein provide developers, markets, and consumers a means of identifying\npotential threats to security and privacy.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 21:26:44 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Celik", "Z. Berkay", ""], ["Babun", "Leonardo", ""], ["Sikder", "Amit K.", ""], ["Aksu", "Hidayet", ""], ["Tan", "Gang", ""], ["McDaniel", "Patrick", ""], ["Uluagac", "A. Selcuk", ""]]}, {"id": "1802.08492", "submitter": "Eduard Kamburjan", "authors": "Eduard Kamburjan and Tzu-Chun Chen", "title": "Stateful Behavioral Types for ABS", "comments": "Technical report for the paper \"Stateful Behavioral Types for Active\n  Objects\" at iFM'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is notoriously hard to correctly implement a multiparty protocol which\ninvolves asynchronous/concurrent interactions and the constraints on states of\nmultiple participants. To assist developers in implementing such protocols, we\npropose a novel specification language to specify interactions within multiple\nobject-oriented actors and the side-effects on heap memory of those actors; a\nbehavioral-type-based analysis is presented for type checking. Our\nspecification language formalizes a protocol as a global type, which describes\nthe procedure of asynchronous method calls, the usage of futures, and the heap\nside-effects with a first-order logic. To characterize runs of instances of\ntypes, we give a model-theoretic semantics for types and translate them into\nlogical constraints over traces. We prove protocol adherence: If a program is\nwell-typed w.r.t. a protocol, then every trace of the program adheres to the\nprotocol, i.e., every trace is a model for the formula of its type.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 11:53:20 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 15:09:43 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Kamburjan", "Eduard", ""], ["Chen", "Tzu-Chun", ""]]}, {"id": "1802.08748", "submitter": "Eric Koskinen", "authors": "Kshitij Bansal and Eric Koskinen and Omer Tripp", "title": "Automatic Generation of Precise and Useful Commutativity Conditions\n  (Extended Version)", "comments": "Note: This is an extended version of our paper, which appears in\n  TACAS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning about commutativity between data-structure operations is an\nimportant problem with applications including parallelizing compilers,\noptimistic parallelization and, more recently, Ethereum smart contracts. There\nhave been research results on automatic generation of commutativity conditions,\nyet we are unaware of any fully automated technique to generate conditions that\nare both sound and effective.\n  We have designed such a technique, driven by an algorithm that iteratively\nrefines a conservative approximation of the commutativity (and\nnon-commutativity) condition for a pair of methods into an increasingly precise\nversion. The algorithm terminates if/when the entire state space has been\nconsidered, and can be aborted at any time to obtain a partial yet sound\ncommutativity condition. We have generalized our work to left-/right-movers and\nproved relative completeness. We describe aspects of our technique that lead to\nuseful commutativity conditions, including how predicates are selected during\nrefinement and heuristics that impact the output shape of the condition.\n  We have implemented our technique in a prototype open-source tool Servois.\nOur algorithm produces quantifier-free queries that are dispatched to a\nback-end SMT solver. We evaluate Servois through two case studies: (i) We\nsynthesize commutativity conditions for a range of data structures including\nSet, HashTable, Accumulator, Counter, and Stack. (ii) We consider an Ethereum\nsmart contract called BlockKing, and show that Servois can detect serious\nconcurrency-related vulnerabilities and guide developers to construct robust\nand efficient implementations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 21:59:57 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Bansal", "Kshitij", ""], ["Koskinen", "Eric", ""], ["Tripp", "Omer", ""]]}, {"id": "1802.08927", "submitter": "Andrew Ruef", "authors": "Shiyi Wei, Piotr Mardziel, Andrew Ruef, Jeffrey S. Foster, Michael\n  Hicks", "title": "Evaluating Design Tradeoffs in Numeric Static Analysis for Java", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-89884-1_23", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Numeric static analysis for Java has a broad range of potentially useful\napplications, including array bounds checking and resource usage estimation.\nHowever, designing a scalable numeric static analysis for real-world Java\nprograms presents a multitude of design choices, each of which may interact\nwith others. For example, an analysis could handle method calls via either a\ntop-down or bottom-up interprocedural analysis. Moreover, this choice could\ninteract with how we choose to represent aliasing in the heap and/or whether we\nuse a relational numeric domain, e.g., convex polyhedra. In this paper, we\npresent a family of abstract interpretation-based numeric static analyses for\nJava and systematically evaluate the impact of 162 analysis configurations on\nthe DaCapo benchmark suite. Our experiment considered the precision and\nperformance of the analyses for discharging array bounds checks. We found that\ntop-down analysis is generally a better choice than bottom-up analysis, and\nthat using access paths to describe heap objects is better than using summary\nobjects corresponding to points-to analysis locations. Moreover, these two\nchoices are the most significant, while choices about the numeric domain,\nrepresentation of abstract objects, and context-sensitivity make much less\ndifference to the precision/performance tradeoff.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 23:09:33 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Wei", "Shiyi", ""], ["Mardziel", "Piotr", ""], ["Ruef", "Andrew", ""], ["Foster", "Jeffrey S.", ""], ["Hicks", "Michael", ""]]}, {"id": "1802.08984", "submitter": "Kalev Alpernas", "authors": "Kalev Alpernas (Tel Aviv University), Cormac Flanagan (UC Santa Cruz),\n  Sadjad Fouladi (Stanford University), Leonid Ryzhyk (VMware Research), Mooly\n  Sagiv (Tel Aviv University), Thomas Schmitz (UC Santa Cruz) and Keith\n  Winstein (Stanford University)", "title": "Secure Serverless Computing Using Dynamic Information Flow Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of serverless computing provides an opportunity to rethink cloud\nsecurity. We present an approach for securing serverless systems using a novel\nform of dynamic information flow control (IFC).\n  We show that in serverless applications, the termination channel found in\nmost existing IFC systems can be arbitrarily amplified via multiple concurrent\nrequests, necessitating a stronger termination-sensitive non-interference\nguarantee, which we achieve using a combination of static labeling of\nserverless processes and dynamic faceted labeling of persistent data.\n  We describe our implementation of this approach on top of JavaScript for AWS\nLambda and OpenWhisk serverless platforms, and present three realistic case\nstudies showing that it can enforce important IFC security properties with low\noverhead.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 10:36:56 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Alpernas", "Kalev", "", "Tel Aviv University"], ["Flanagan", "Cormac", "", "UC Santa Cruz"], ["Fouladi", "Sadjad", "", "Stanford University"], ["Ryzhyk", "Leonid", "", "VMware Research"], ["Sagiv", "Mooly", "", "Tel Aviv University"], ["Schmitz", "Thomas", "", "UC Santa Cruz"], ["Winstein", "Keith", "", "Stanford University"]]}, {"id": "1802.09480", "submitter": "James Clarkson", "authors": "James Clarkson and Christos Kotselidis", "title": "Tornado: A Practical And Efficient Heterogeneous Programming Framework\n  For Managed Languages", "comments": "Prematurely submitted report before it is complete and facts verified\n  properly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our experiences creating Tornado: a practical and\nefficient heterogeneous programming framework for managed languages. The novel\naspect of Tornado is that it turns the programming of heterogeneous systems\nfrom an activity predominantly based on a priori knowledge into one based on a\nposteriori knowledge. Alternatively put, it simply means developers do not need\nto overcomplicate their code by catering for all possible eventualities.\nInstead, Tornado provides the ability to specialize each application for a\nspecific system in situ which avoids the need for it to be pre-configured by\nthe developer. To enable this, Tornado employs a sophisticated runtime system\nthat can dynamically configure all aspects of the application - from selecting\nwhich parallelization scheme to apply to specifying which accelerators to use.\nBy using this ability, the end-user, and not the developer, can transparently\nmake use of any available multi-/many-core processor or hardware accelerator.\n  To showcase the impact of Tornado, we implement a real-world computer vision\napplication and deploy it across nine accelerators without having to modify the\nsource code or even explicitly re-compile the application. Using dynamic\nconfiguration, we show that our implementation can achieve up to 124 frames per\nsecond (FPS) - up to 166x speedup over the reference implementation. Finally,\nour implementation is always within 21% of a hand-written OpenCL version but\navoids much of the programming tedium.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 17:55:26 GMT"}, {"version": "v2", "created": "Thu, 1 Mar 2018 17:35:46 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Clarkson", "James", ""], ["Kotselidis", "Christos", ""]]}, {"id": "1802.09517", "submitter": "Kostya Serebryany", "authors": "Kostya Serebryany, Evgenii Stepanov, Aleksey Shlyapnikov, Vlad\n  Tsyrklevich, Dmitry Vyukov", "title": "Memory Tagging and how it improves C/C++ memory safety", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory safety in C and C++ remains largely unresolved. A technique usually\ncalled \"memory tagging\" may dramatically improve the situation if implemented\nin hardware with reasonable overhead. This paper describes two existing\nimplementations of memory tagging: one is the full hardware implementation in\nSPARC; the other is a partially hardware-assisted compiler-based tool for\nAArch64. We describe the basic idea, evaluate the two implementations, and\nexplain how they improve memory safety. This paper is intended to initiate a\nwider discussion of memory tagging and to motivate the CPU and OS vendors to\nadd support for it in the near future.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 20:58:46 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Serebryany", "Kostya", ""], ["Stepanov", "Evgenii", ""], ["Shlyapnikov", "Aleksey", ""], ["Tsyrklevich", "Vlad", ""], ["Vyukov", "Dmitry", ""]]}, {"id": "1802.09598", "submitter": "Sam Staton", "authors": "Sam Staton, Dario Stein, Hongseok Yang, Nathanael L. Ackerman, Cameron\n  E. Freer, Daniel M. Roy", "title": "The Beta-Bernoulli process and algebraic effects", "comments": "To appear in Proc. ICALP 2018", "journal-ref": "Proceedings of the 45th International Colloquium on Automata,\n  Languages, and Programming (ICALP 2018), 141:1-141:15, 2018", "doi": "10.4230/LIPIcs.ICALP.2018.141", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we use the framework of algebraic effects from programming\nlanguage theory to analyze the Beta-Bernoulli process, a standard building\nblock in Bayesian models. Our analysis reveals the importance of abstract data\ntypes, and two types of program equations, called commutativity and\ndiscardability. We develop an equational theory of terms that use the\nBeta-Bernoulli process, and show that the theory is complete with respect to\nthe measure-theoretic semantics, and also in the syntactic sense of Post. Our\nanalysis has a potential for being generalized to other stochastic processes\nrelevant to Bayesian modelling, yielding new understanding of these processes\nfrom the perspective of programming.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 20:49:20 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 08:53:38 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Staton", "Sam", ""], ["Stein", "Dario", ""], ["Yang", "Hongseok", ""], ["Ackerman", "Nathanael L.", ""], ["Freer", "Cameron E.", ""], ["Roy", "Daniel M.", ""]]}, {"id": "1802.09737", "submitter": "EPTCS", "authors": "Bob Coecke, Aleks Kissinger", "title": "Proceedings 14th International Conference on Quantum Physics and Logic", "comments": null, "journal-ref": "EPTCS 266, 2018", "doi": "10.4204/EPTCS.266", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of the 14th International Conference on\nQuantum Physics and Logic (QPL 2017), which was held July 3-7, 2017 at the LUX\nCinema Nijmegen, the Netherlands, and was hosted by Radboud University. QPL is\na conference that brings together researchers working on mathematical\nfoundations of quantum physics, quantum computing, and related areas, with a\nfocus on structural perspectives and the use of logical tools, ordered\nalgebraic and category-theoretic structures, formal languages, semantical\nmethods, and other computer science techniques applied to the study of physical\nbehaviour in general. This conference also welcomes work that applies\nstructures and methods inspired by quantum theory to other fields (including\ncomputer science).\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 06:25:29 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Coecke", "Bob", ""], ["Kissinger", "Aleks", ""]]}, {"id": "1802.09787", "submitter": "Alejandro Aguirre", "authors": "Alejandro Aguirre, Gilles Barthe, Lars Birkedal, Ale\\v{s} Bizjak,\n  Marco Gaboardi and Deepak Garg", "title": "Relational Reasoning for Markov Chains in a Probabilistic Guarded Lambda\n  Calculus", "comments": "To appear at ESOP '18 (Extended version with appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the simply-typed guarded $\\lambda$-calculus with discrete\nprobabilities and endow it with a program logic for reasoning about relational\nproperties of guarded probabilistic computations. This provides a framework for\nprogramming and reasoning about infinite stochastic processes like Markov\nchains. We demonstrate the logic sound by interpreting its judgements in the\ntopos of trees and by using probabilistic couplings for the semantics of\nrelational assertions over distributions on discrete types.\n  The program logic is designed to support syntax-directed proofs in the style\nof relational refinement types, but retains the expressiveness of higher-order\nlogic extended with discrete distributions, and the ability to reason\nrelationally about expressions that have different types or syntactic\nstructure. In addition, our proof system leverages a well-known theorem from\nthe coupling literature to justify better proof rules for relational reasoning\nabout probabilistic expressions. We illustrate these benefits with a broad\nrange of examples that were beyond the scope of previous systems, including\nshift couplings and lump couplings between random walks.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 09:19:18 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Aguirre", "Alejandro", ""], ["Barthe", "Gilles", ""], ["Birkedal", "Lars", ""], ["Bizjak", "Ale\u0161", ""], ["Gaboardi", "Marco", ""], ["Garg", "Deepak", ""]]}, {"id": "1802.09984", "submitter": "Victor Marsault", "authors": "Nadime Francis, Alastair Green, Paolo Guagliardo, Leonid Libkin,\n  Tobias Lindaaker, Victor Marsault, Stefan Plantikow, Mats Rydberg, Martin\n  Schuster, Petra Selmer, and Andr\\'es Taylor", "title": "Formal Semantics of the Language Cypher", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cypher is a query language for property graphs. It was originally designed\nand implemented as part of the Neo4j graph database, and it is currently used\nin a growing number of commercial systems, industrial applications and research\nprojects. In this work, we provide denotational semantics of the core fragment\nof the read-only part of Cypher, which features in particular pattern matching,\nfiltering, and most relational operations on tables.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 16:01:36 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 18:27:52 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Francis", "Nadime", ""], ["Green", "Alastair", ""], ["Guagliardo", "Paolo", ""], ["Libkin", "Leonid", ""], ["Lindaaker", "Tobias", ""], ["Marsault", "Victor", ""], ["Plantikow", "Stefan", ""], ["Rydberg", "Mats", ""], ["Schuster", "Martin", ""], ["Selmer", "Petra", ""], ["Taylor", "Andr\u00e9s", ""]]}, {"id": "1802.10433", "submitter": "Benjamin Lucien Kaminski", "authors": "Kevin Batz, Benjamin Lucien Kaminski, Joost-Pieter Katoen, Christoph\n  Matheja", "title": "How long, O Bayesian network, will I sample thee? A program analysis\n  perspective on expected sampling times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks (BNs) are probabilistic graphical models for describing\ncomplex joint probability distributions. The main problem for BNs is inference:\nDetermine the probability of an event given observed evidence. Since exact\ninference is often infeasible for large BNs, popular approximate inference\nmethods rely on sampling.\n  We study the problem of determining the expected time to obtain a single\nvalid sample from a BN. To this end, we translate the BN together with\nobservations into a probabilistic program. We provide proof rules that yield\nthe exact expected runtime of this program in a fully automated fashion. We\nimplemented our approach and successfully analyzed various real-world BNs taken\nfrom the Bayesian network repository.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 14:36:37 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Batz", "Kevin", ""], ["Kaminski", "Benjamin Lucien", ""], ["Katoen", "Joost-Pieter", ""], ["Matheja", "Christoph", ""]]}, {"id": "1802.10574", "submitter": "Fredrik Kjolstad", "authors": "Fredrik Kjolstad and Peter Ahrens and Shoaib Kamil and Saman\n  Amarasinghe", "title": "Sparse Tensor Algebra Optimizations with Workspaces", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows how to optimize sparse tensor algebraic expressions by\nintroducing temporary tensors, called workspaces, into the resulting loop\nnests. We develop a new intermediate language for tensor operations called\nconcrete index notation that extends tensor index notation. Concrete index\nnotation expresses when and where sub-computations occur and what tensor they\nare stored into. We then describe the workspace optimization in this language,\nand how to compile it to sparse code by building on prior work in the\nliterature.\n  We demonstrate the importance of the optimization on several important sparse\ntensor kernels, including sparse matrix-matrix multiplication (SpMM), sparse\ntensor addition (SpAdd), and the matricized tensor times Khatri-Rao product\n(MTTKRP) used to factorize tensors. Our results show improvements over prior\nwork on tensor algebra compilation and brings the performance of these kernels\non par with state-of-the-art hand-optimized implementations. For example, SpMM\nwas not supported by prior tensor algebra compilers, the performance of MTTKRP\non the nell-2 data set improves by 35%, and MTTKRP can for the first time have\nsparse results.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 18:28:10 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 17:55:16 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Kjolstad", "Fredrik", ""], ["Ahrens", "Peter", ""], ["Kamil", "Shoaib", ""], ["Amarasinghe", "Saman", ""]]}, {"id": "1802.10583", "submitter": "William Blum", "authors": "William Blum", "title": "Reducing Lambda Terms with Traversals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to evaluate untyped lambda terms by combining the\ntheory of traversals, a term-tree traversing technique inspired from Game\nSemantics, with judicious use of the eta-conversion rule of the lambda\ncalculus.\n  The traversal theory of the simply-typed lambda calculus relies on the\neta-long transform to ensure that when traversing an application, there is a\nsubterm representing every possible operator's argument. In the untyped\nsetting, we instead exhibit the missing operand via ad-hoc instantiation of the\neta-expansion rule, which allows the traversal to proceed as if the operand\nexisted in the original term. This gives rise to a more generic concept of\ntraversals for lambda terms. A notable improvement, in addition to handling\nuntyped terms, is that no preliminary transformation is required: the original\nunaltered lambda term is traversed.\n  We show that by bounding the non-determinism of the traversal rule for free\nvariables, one can effectively compute a set of traversals characterizing the\npaths in the tree representation of the beta-normal form, when it exists. This\nyields an evaluation algorithm for untyped lambda-terms. We prove correctness\nby showing that traversals implement leftmost linear reduction, a\ngeneralization of the head linear reduction of Danos et. al.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 18:46:19 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Blum", "William", ""]]}]