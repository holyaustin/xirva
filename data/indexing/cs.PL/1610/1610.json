[{"id": "1610.00097", "submitter": "Matthew Hammer", "authors": "Matthew A. Hammer, Jana Dunfield, Dimitrios J. Economou and Monal\n  Narasimhamurthy", "title": "Refinement types for precisely named cache locations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many programming language techniques for incremental computation employ\nprogrammer-specified names for cached information. At runtime, each name\nidentifies a \"cache location\" for a dynamic data value or a sub-computation; in\nsum, these cache location choices guide change propagation and incremental\n(re)execution.\n  We call a cache location name precise when it identifies at most one value or\nsubcomputation; we call all other names imprecise, or ambiguous. At a minimum,\ncache location names must be precise to ensure that change propagation works\ncorrectly; yet, reasoning statically about names in incremental programs\nremains an open problem.\n  As a first step, this paper defines and solves the precise name problem,\nwhere we verify that incremental programs with explicit names use them\nprecisely. To do so, we give a refinement type and effect system, and prove it\nsound (every well-typed program uses names precisely). We also demonstrate that\nthis type system is expressive by verifying example programs that compute over\nefficient representations of incremental sequences and sets. Beyond verifying\nthese programs, our type system also describes their dynamic naming strategies,\ne.g., for library documentation purposes.\n", "versions": [{"version": "v1", "created": "Sat, 1 Oct 2016 07:01:43 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 15:33:41 GMT"}, {"version": "v3", "created": "Tue, 28 Feb 2017 18:42:57 GMT"}, {"version": "v4", "created": "Tue, 11 Jul 2017 22:32:35 GMT"}, {"version": "v5", "created": "Sat, 21 Oct 2017 14:46:18 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Hammer", "Matthew A.", ""], ["Dunfield", "Jana", ""], ["Economou", "Dimitrios J.", ""], ["Narasimhamurthy", "Monal", ""]]}, {"id": "1610.00502", "submitter": "Emilio Coppa", "authors": "Roberto Baldoni, Emilio Coppa, Daniele Cono D'Elia, Camil Demetrescu,\n  Irene Finocchi", "title": "A Survey of Symbolic Execution Techniques", "comments": "This is the authors pre-print copy. If you are considering citing\n  this survey, we would appreciate if you could use the following BibTeX entry:\n  http://goo.gl/Hf5Fvc", "journal-ref": "ACM Computing Surveys 51(3), 2018. BibTeX entry:\n  http://goo.gl/Hf5Fvc", "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many security and software testing applications require checking whether\ncertain properties of a program hold for any possible usage scenario. For\ninstance, a tool for identifying software vulnerabilities may need to rule out\nthe existence of any backdoor to bypass a program's authentication. One\napproach would be to test the program using different, possibly random inputs.\nAs the backdoor may only be hit for very specific program workloads, automated\nexploration of the space of possible inputs is of the essence. Symbolic\nexecution provides an elegant solution to the problem, by systematically\nexploring many possible execution paths at the same time without necessarily\nrequiring concrete inputs. Rather than taking on fully specified input values,\nthe technique abstractly represents them as symbols, resorting to constraint\nsolvers to construct actual instances that would cause property violations.\nSymbolic execution has been incubated in dozens of tools developed over the\nlast four decades, leading to major practical breakthroughs in a number of\nprominent software reliability applications. The goal of this survey is to\nprovide an overview of the main ideas, challenges, and solutions developed in\nthe area, distilling them for a broad audience.\n  The present survey has been accepted for publication at ACM Computing\nSurveys. If you are considering citing this survey, we would appreciate if you\ncould use the following BibTeX entry: http://goo.gl/Hf5Fvc\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 11:34:31 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 13:06:04 GMT"}, {"version": "v3", "created": "Wed, 2 May 2018 16:14:14 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Baldoni", "Roberto", ""], ["Coppa", "Emilio", ""], ["D'Elia", "Daniele Cono", ""], ["Demetrescu", "Camil", ""], ["Finocchi", "Irene", ""]]}, {"id": "1610.00831", "submitter": "Michael Bukatin", "authors": "Michael Bukatin and Steve Matthews and Andrey Radul", "title": "Notes on Pure Dataflow Matrix Machines: Programming with\n  Self-referential Matrix Transformations", "comments": "7 pages (v3 - update page 7)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dataflow matrix machines are self-referential generalized recurrent neural\nnets. The self-referential mechanism is provided via a stream of matrices\ndefining the connectivity and weights of the network in question. A natural\nquestion is: what should play the role of untyped lambda-calculus for this\nprogramming architecture? The proposed answer is a discipline of programming\nwith only one kind of streams, namely the streams of appropriately shaped\nmatrices. This yields Pure Dataflow Matrix Machines which are networks of\ntransformers of streams of matrices capable of defining a pure dataflow matrix\nmachine.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 03:10:55 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 01:49:32 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2018 17:51:57 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Bukatin", "Michael", ""], ["Matthews", "Steve", ""], ["Radul", "Andrey", ""]]}, {"id": "1610.01004", "submitter": "Brijesh Dongol", "authors": "Alasdair Armstrong, Brijesh Dongol, Simon Doherty", "title": "Reducing Opacity to Linearizability: A Sound and Complete Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC cs.DS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transactional memory is a mechanism that manages thread synchronisation on\nbehalf of a programmer so that blocks of code execute with an illusion of\natomicity. The main safety criterion for transactional memory is opacity, which\ndefines conditions for serialising concurrent transactions.\n  Proving opacity is complicated because it allows concurrent transactions to\nobserve distinct memory states, while TM implementations are typically based on\none single shared store. This paper presents a sound and complete method, based\non coarse-grained abstraction, for reducing proofs of opacity to the relatively\nsimpler correctness condition: linearizability. We use our methods to verify\nTML and NORec from the literature and show our techniques extend to relaxed\nmemory models by showing that both are opaque under TSO without requiring\nadditional fences. Our methods also elucidate TM designs at higher level of\nabstraction; as an application, we develop a variation of NORec with fast-path\nreads transactions. All our proofs have been mechanised, either in the Isabelle\ntheorem prover or the PAT model checker.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 14:07:52 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Armstrong", "Alasdair", ""], ["Dongol", "Brijesh", ""], ["Doherty", "Simon", ""]]}, {"id": "1610.01133", "submitter": "Zhoulai Fu", "authors": "Zhoulai Fu, Zhendong Su", "title": "Mathematical Execution: A Unified Approach for Testing Numerical Code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Mathematical Execution (ME), a new, unified approach for\ntesting numerical code. The key idea is to (1) capture the desired testing\nobjective via a representing function and (2) transform the automated testing\nproblem to the minimization problem of the representing function. The\nminimization problem is to be solved via mathematical optimization. The main\nfeature of ME is that it directs input space exploration by only executing the\nrepresenting function, thus avoiding static or symbolic reasoning about the\nprogram semantics, which is particularly challenging for numerical code. To\nillustrate this feature, we develop an ME-based algorithm for coverage-based\ntesting of numerical code. We also show the potential of applying and adapting\nME to other related problems, including path reachability testing, boundary\nvalue analysis, and satisfiability checking.\n  To demonstrate ME's practical benefits, we have implemented CoverMe, a\nproof-of-concept realization for branch coverage based testing, and evaluated\nit on Sun's C math library (used in, for example, Android, Matlab, Java and\nJavaScript). We have compared CoverMe with random testing and Austin, a\npublicly available branch coverage based testing tool that supports numerical\ncode (Austin combines symbolic execution and search-based heuristics). Our\nexperimental results show that CoverMe achieves near-optimal and substantially\nhigher coverage ratios than random testing on all tested programs, across all\nevaluated coverage metrics. Compared with Austin, CoverMe improves branch\ncoverage from 43% to 91%, with significantly less time (6.9 vs. 6058.4 seconds\non average).\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 19:26:28 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Fu", "Zhoulai", ""], ["Su", "Zhendong", ""]]}, {"id": "1610.01188", "submitter": "Andreas Pavlogiannis", "authors": "Marek Chalupa and Krishnendu Chatterjee and Andreas Pavlogiannis and\n  Nishant Sinha and Kapil Vaidya", "title": "Data-centric Dynamic Partial Order Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new dynamic partial-order reduction method for stateless model\nchecking of concurrent programs. A common approach for exploring program\nbehaviors relies on enumerating the traces of the program, without storing the\nvisited states (aka stateless exploration). As the number of distinct traces\ngrows exponentially, dynamic partial-order reduction (DPOR) techniques have\nbeen successfully used to partition the space of traces into equivalence\nclasses (Mazurkiewicz partitioning), with the goal of exploring only few\nrepresentative traces from each class.\n  We introduce a new equivalence on traces under sequential consistency\nsemantics, which we call the observation equivalence. Two traces are\nobservationally equivalent if every read event observes the same write event in\nboth traces. While the traditional Mazurkiewicz equivalence is control-centric,\nour new definition is data-centric. We show that our observation equivalence is\ncoarser than the Mazurkiewicz equivalence, and in many cases even exponentially\ncoarser. We devise a DPOR exploration of the trace space, called data-centric\nDPOR, based on the observation equivalence. For acyclic architectures, our\nalgorithm is guaranteed to explore exactly one representative trace from each\nobservation class, while spending polynomial time per class. Hence, our\nalgorithm is optimal wrt the observation equivalence, and in several cases\nexplores exponentially fewer traces than any enumerative method based on the\nMazurkiewicz equivalence. For cyclic architectures, we consider an equivalence\nbetween traces which is finer than the observation equivalence; but coarser\nthan the Mazurkiewicz equivalence, and in some cases is exponentially coarser.\nOur data-centric DPOR algorithm remains optimal under this trace equivalence.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 20:29:15 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 22:13:53 GMT"}, {"version": "v3", "created": "Mon, 2 Jan 2017 11:28:08 GMT"}, {"version": "v4", "created": "Sun, 22 Oct 2017 12:11:58 GMT"}, {"version": "v5", "created": "Wed, 17 Oct 2018 13:12:44 GMT"}, {"version": "v6", "created": "Fri, 25 Jan 2019 08:32:46 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Chalupa", "Marek", ""], ["Chatterjee", "Krishnendu", ""], ["Pavlogiannis", "Andreas", ""], ["Sinha", "Nishant", ""], ["Vaidya", "Kapil", ""]]}, {"id": "1610.01213", "submitter": "Gabriel Scherer", "authors": "Gabriel Scherer", "title": "Deciding equivalence with sums and the empty type", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The logical technique of focusing can be applied to the $\\lambda$-calculus;\nin a simple type system with atomic types and negative type formers (functions,\nproducts, the unit type), its normal forms coincide with $\\beta\\eta$-normal\nforms. Introducing a saturation phase gives a notion of quasi-normal forms in\npresence of positive types (sum types and the empty type). This rich structure\nlet us prove the decidability of $\\beta\\eta$-equivalence in presence of the\nempty type, the fact that it coincides with contextual equivalence, and a\nfinite model property.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 21:37:45 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2016 02:04:34 GMT"}, {"version": "v3", "created": "Tue, 8 Nov 2016 15:58:09 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Scherer", "Gabriel", ""]]}, {"id": "1610.01331", "submitter": "Quang Loc Le", "authors": "Quang Loc Le", "title": "A Decision Procedure for String Logic with Equations, Regular Membership\n  and Length Constraints", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the satisfiability problem for string logic with\nequations, regular membership and Presburger constraints over length functions.\nThe difficulty comes from multiple occurrences of string variables making\nstate-of-the-art algorithms non-terminating. Our main contribution is to show\nthat the satisfiability problem in a fragment where no string variable occurs\nmore than twice in an equation is decidable. In particular, we propose a\nsemi-decision procedure for arbitrary string formulae with word equations,\nregular membership and length functions. The essence of our procedure is an\nalgorithm to enumerate an equivalent set of solvable disjuncts for the formula.\nWe further show that the algorithm always terminates for the aforementioned\ndecidable fragment. Finally, we provide a complexity analysis of our decision\nprocedure to prove that it runs, in the worst case, in factorial time.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2016 09:37:51 GMT"}, {"version": "v2", "created": "Tue, 11 Oct 2016 14:40:09 GMT"}], "update_date": "2016-10-12", "authors_parsed": [["Le", "Quang Loc", ""]]}, {"id": "1610.02144", "submitter": "Robert O'Callahan", "authors": "Robert O'Callahan and Chris Jones and Nathan Froyd and Kyle Huey and\n  Albert Noll and Nimrod Partush", "title": "Lightweight User-Space Record And Replay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to record and replay program executions with low overhead enables\nmany applications, such as reverse-execution debugging, debugging of\nhard-to-reproduce test failures, and \"black box\" forensic analysis of failures\nin deployed systems. Existing record-and-replay approaches rely on recording an\nentire virtual machine (which is heavyweight), modifying the OS kernel (which\nadds deployment and maintenance costs), or pervasive code instrumentation\n(which imposes significant performance and complexity overhead). We\ninvestigated whether it is possible to build a practical record-and-replay\nsystem avoiding all these issues. The answer turns out to be yes --- if the CPU\nand operating system meet certain non-obvious constraints. Fortunately modern\nIntel CPUs, Linux kernels and user-space frameworks meet these constraints,\nalthough this has only become true recently. With some novel optimizations, our\nsystem RR records and replays real-world workloads with low overhead with an\nentirely user-space implementation running on stock hardware and operating\nsystems. RR forms the basis of an open-source reverse-execution debugger seeing\nsignificant use in practice. We present the design and implementation of RR,\ndescribe its performance on a variety of workloads, and identify constraints on\nhardware and operating system design required to support our approach.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 05:11:32 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["O'Callahan", "Robert", ""], ["Jones", "Chris", ""], ["Froyd", "Nathan", ""], ["Huey", "Kyle", ""], ["Noll", "Albert", ""], ["Partush", "Nimrod", ""]]}, {"id": "1610.02327", "submitter": "Roly Perera", "authors": "Roly Perera, Deepak Garg, James Cheney", "title": "Causally consistent dynamic slicing", "comments": "in Proceedings of 27th International Conference on Concurrency Theory\n  (CONCUR 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We offer a lattice-theoretic account of dynamic slicing for {\\pi}-calculus,\nbuilding on prior work in the sequential setting. For any run of a concurrent\nprogram, we exhibit a Galois connection relating forward slices of the start\nconfiguration to backward slices of the end configuration. We prove that, up to\nlattice isomorphism, the same Galois connection arises for any causally\nequivalent execution, allowing an efficient concurrent implementation of\nslicing via a standard interleaving semantics. Our approach has been formalised\nin the dependently-typed language Agda.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2016 15:35:30 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Perera", "Roly", ""], ["Garg", "Deepak", ""], ["Cheney", "James", ""]]}, {"id": "1610.02952", "submitter": "Aziem Chawdhary Dr", "authors": "Aziem Chawdhary, Ed Robbins, Andy King", "title": "Incrementally Closing Octagons", "comments": "42 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The octagon abstract domain is a widely used numeric abstract domain\nexpressing relational information between variables whilst being both\ncomputationally efficient and simple to implement. Each element of the domain\nis a system of constraints where each constraint takes the restricted form $\\pm\nx_i \\pm x_j \\leq d$. A key family of operations for the octagon domain are\nclosure algorithms, which check satisfiability and provide a normal form for\noctagonal constraint systems. We present new quadratic incremental algorithms\nfor closure, strong closure and integer closure and proofs of their\ncorrectness. We highlight the benefits and measure the performance of these new\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 15:10:01 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 13:38:13 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Chawdhary", "Aziem", ""], ["Robbins", "Ed", ""], ["King", "Andy", ""]]}, {"id": "1610.03148", "submitter": "Qirun Zhang", "authors": "Qirun Zhang, Chengnian Sun, Zhendong Su", "title": "Skeletal Program Enumeration for Rigorous Compiler Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A program can be viewed as a syntactic structure P (syntactic skeleton)\nparameterized by a collection of the identifiers V (variable names). This paper\nintroduces the skeletal program enumeration (SPE) problem: Given a fixed\nsyntactic skeleton P and a set of variables V , enumerate a set of programs P\nexhibiting all possible variable usage patterns within P. It proposes an\neffective realization of SPE for systematic, rigorous compiler testing by\nleveraging three important observations: (1) Programs with different variable\nusage patterns exhibit diverse control- and data-dependence information, and\nhelp exploit different compiler optimizations and stress-test compilers; (2)\nmost real compiler bugs were revealed by small tests (i.e., small-sized P) ---\nthis \"small-scope\" observation opens up SPE for practical compiler validation;\nand (3) SPE is exhaustive w.r.t. a given syntactic skeleton and variable set,\nand thus can offer a level of guarantee that is absent from all existing\ncompiler testing techniques.\n  The key challenge of SPE is how to eliminate the enormous amount of\nequivalent programs w.r.t. $\\alpha$-conversion. Our main technical contribution\nis a novel algorithm for computing the canonical (and smallest) set of all\nnon-$\\alpha$-equivalent programs. We have realized our SPE technique and\nevaluated it using syntactic skeletons derived from GCC's testsuite. Our\nevaluation results on testing GCC and Clang are extremely promising. In less\nthan six months, our approach has led to 217 confirmed bug reports, 104 of\nwhich have already been fixed, and the majority are long latent bugs despite\nthe extensive prior efforts of automatically testing both compilers (e.g.,\nCsmith and EMI). The results also show that our algorithm for enumerating\nnon-$\\alpha$-equivalent programs provides six orders of magnitude reduction,\nenabling processing the GCC test-suite in under a month.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2016 01:02:44 GMT"}, {"version": "v2", "created": "Sat, 15 Apr 2017 07:38:27 GMT"}, {"version": "v3", "created": "Thu, 20 Apr 2017 04:47:07 GMT"}, {"version": "v4", "created": "Tue, 11 Jul 2017 21:35:37 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Zhang", "Qirun", ""], ["Sun", "Chengnian", ""], ["Su", "Zhendong", ""]]}, {"id": "1610.04461", "submitter": "Markus Raab", "authors": "Markus Raab", "title": "Persistent Contextual Values as Inter-Process Layers", "comments": "8 pages Mobile! 16, October 31, 2016, Amsterdam, Netherlands", "journal-ref": null, "doi": "10.1145/3001854.3001855", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Mobile applications today often fail to be context aware when they also need\nto be customizable and efficient at run-time. Context-oriented programming\nallows programmers to develop applications that are more context aware. Its\ncentral construct, the so-called layer, however, is not customizable. We\npropose to use novel persistent contextual values for mobile development.\nPersistent contextual values automatically adapt their value to the context.\nFurthermore they provide access without overhead. Key-value configuration files\ncontain the specification of contextual values and the persisted contextual\nvalues themselves. By modifying the configuration files, the contextual values\ncan easily be customized for every context. From the specification, we generate\ncode to simplify development. Our implementation, called Elektra, permits\ndevelopment in several languages including C++ and Java. In a benchmark we\ncompare layer activations between threads and between applications. In a case\nstudy involving a web-server on a mobile embedded device the performance\noverhead is minimal, even with many context switches.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 13:47:49 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Raab", "Markus", ""]]}, {"id": "1610.04641", "submitter": "Niki Vazou", "authors": "Niki Vazou and Ranjit Jhala", "title": "Refinement Reflection (or, how to turn your favorite language into a\n  proof assistant using SMT)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Refinement Reflection turns your favorite programming language into a proof\nassistant by reflecting the code implementing a user-defined function into the\nfunction's (output) refinement type. As a consequence, at uses of the function,\nthe function definition is unfolded into the refinement logic in a precise,\npredictable and most importantly, programmer controllable way. In the logic, we\nencode functions and lambdas using uninterpreted symbols preserving SMT-based\ndecidable verification. In the language, we provide a library of combinators\nthat lets programmers compose proofs from basic refinements and function\ndefinitions. We have implemented our approach in the Liquid Haskell system,\nthereby converting Haskell into an interactive proof assistant, that we used to\nverify a variety of properties ranging from arithmetic properties of higher\norder, recursive functions to the Monoid, Applicative, Functor and Monad type\nclass laws for a variety of instances.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 21:04:26 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Vazou", "Niki", ""], ["Jhala", "Ranjit", ""]]}, {"id": "1610.04790", "submitter": "Emery Berger", "authors": "Diogenes Nunez, Samuel Z. Guyer, Emery D. Berger", "title": "Prioritized Garbage Collection: Explicit GC Support for Software Caches", "comments": "to appear in OOPSLA 2016", "journal-ref": null, "doi": "10.1145/2983990.2984028", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programmers routinely trade space for time to increase performance, often in\nthe form of caching or memoization. In managed languages like Java or\nJavaScript, however, this space-time tradeoff is complex. Using more space\ntranslates into higher garbage collection costs, especially at the limit of\navailable memory. Existing runtime systems provide limited support for\nspace-sensitive algorithms, forcing programmers into difficult and often\nbrittle choices about provisioning.\n  This paper presents prioritized garbage collection, a cooperative programming\nlanguage and runtime solution to this problem. Prioritized GC provides an\ninterface similar to soft references, called priority references, which\nidentify objects that the collector can reclaim eagerly if necessary. The key\ndifference is an API for defining the policy that governs when priority\nreferences are cleared and in what order. Application code specifies a priority\nvalue for each reference and a target memory bound. The collector reclaims\nreferences, lowest priority first, until the total memory footprint of the\ncache fits within the bound. We use this API to implement a space-aware\nleast-recently-used (LRU) cache, called a Sache, that is a drop-in replacement\nfor existing caches, such as Google's Guava library. The garbage collector\nautomatically grows and shrinks the Sache in response to available memory and\nworkload with minimal provisioning information from the programmer. Using a\nSache, it is almost impossible for an application to experience a memory leak,\nmemory pressure, or an out-of-memory crash caused by software caching.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 22:30:15 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Nunez", "Diogenes", ""], ["Guyer", "Samuel Z.", ""], ["Berger", "Emery D.", ""]]}, {"id": "1610.04799", "submitter": "Shayan Najd", "authors": "Shayan Najd, Simon Peyton Jones", "title": "Trees That Grow", "comments": "EARLY DRAFT", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the notion of extensibility in functional data types, as a new\napproach to the problem of decorating abstract syntax trees with additional\nsets of information. We observed the need for such extensibility while\nredesigning the data types representing Haskell abstract syntax inside GHC.\n  Specifically, we describe our approach to the tree-decoration problem using a\nnovel syntactic machinery in Haskell for expressing extensible data types. We\nshow that the syntactic machinery is complete in that it can express all the\nsyntactically possible forms of extensions to algebraic data type declarations.\nThen, we describe an encoding of the syntactic machinery based on the existing\nfeatures in Glasgow Haskell Compiler(GHC).\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 23:43:48 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Najd", "Shayan", ""], ["Jones", "Simon Peyton", ""]]}, {"id": "1610.05114", "submitter": "Moez AbdelGawad", "authors": "Moez A. AbdelGawad", "title": "Towards an Accurate Mathematical Model of Generic Nominally-Typed OOP", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of GNOOP as a domain-theoretic model of generic\nnominally-typed OOP is currently underway. This extended abstract presents the\nconcepts of `nominal intervals' and `full generication' that are likely to help\nin building GNOOP as an accurate mathematical model of generic nominally-typed\nOOP. The abstract also presents few related category-theoretic suggestions. The\npresented concepts and suggestions are particularly geared towards enabling\nGNOOP to offer a precise and simple view of so-far-hard-to-analyze features of\ngeneric OOP such as variance annotations (e.g., Java wildcard types) and erased\ngenerics (e.g., Java type erasure).\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 09:07:50 GMT"}, {"version": "v2", "created": "Tue, 25 Oct 2016 11:07:17 GMT"}, {"version": "v3", "created": "Sun, 30 Oct 2016 15:55:27 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["AbdelGawad", "Moez A.", ""]]}, {"id": "1610.05593", "submitter": "Yuhui Lin", "authors": "Yuhui Lin, Gudmund Grov and Rob Arthan", "title": "Understanding and maintaining tactics graphically OR how we are learning\n  that a diagram can be worth more than 10K LoC", "comments": "62 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of a functional language to implement proof strategies as proof\ntactics in interactive theorem provers, often provides short, concise and\nelegant implementations. Whilst being elegant, the use of higher order features\nand combinator languages often results in a very procedural view of a strategy,\nwhich may deviate significantly from the high-level ideas behind it. This can\nmake a tactic hard to understand and hence difficult to to debug and maintain\nfor experts and non-experts alike: one often has to tear apart complex\ncombinations of lower level tactics manually in order to analyse a failure in\nthe overall strategy. In an industrial technology transfer project, we have\nbeen working on porting a very large and complex proof tactic into PSGraph, a\ngraphical language for representing proof strategies, supported by the Tinker\ntool. The goal of this work is to improve understandability and maintainability\nof tactics. Motivated by some initial successes with this, we here extend\nPSGraph with additional features for development and debugging. Through the\nre-implementation and refactoring of several existing tactics, we demonstrates\nthe advantages of PSGraph compared with a typical linear (term-based) tactic\nlanguage with respect to debugging, readability and maintenance. In order to\nact as guidance for others, we give a fairly detailed comparison of the user\nexperience with the two approaches. The paper is supported by a web page\nproviding further details about the implementation as well as interactive\nillustrations of the examples.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 10:17:01 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2016 11:04:30 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Lin", "Yuhui", ""], ["Grov", "Gudmund", ""], ["Arthan", "Rob", ""]]}, {"id": "1610.05954", "submitter": "Jan Rochel", "authors": "Jan Rochel", "title": "Unfolding Semantics of the Untyped {\\lambda}-Calculus with letrec", "comments": "dissertation, 250 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We investigate the relationship between finite terms in {\\lambda}-letrec, the\n{\\lambda}-calculus with letrec, and the infinite {\\lambda}-terms they express.\nWe say that a lambda-letrec term expresses a lambda-term if the latter can be\nobtained as an infinite unfolding of the former. Unfolding is the process of\nsubstituting occurrences of function variables by the right-hand side of their\ndefinition.\n  We consider the following questions: (i) How can we characterise those\ninfinite {\\lambda}-terms that are {\\lambda}-letrec-expressible? (ii) Given two\n{\\lambda}-letrec terms, how can we determine whether they have the same\nunfolding? (iii) Given a {\\lambda}-letrec term, can we find a more compact\nversion of the term with the same unfolding? To tackle these questions we\nintroduce and study the following formalisms: (i) a rewriting system for\nunfolding {\\lambda}-letrec terms into {\\lambda}-terms (ii) a rewriting system\nfor `observing' {\\lambda}-terms by dissecting their term structure (iii)\nhigher-order and first-order graph formalisms together with translations\nbetween them as well as translations from and to {\\lambda}-letrec.\n  We identify a first-order term graph formalism on which bisimulation\npreserves and reflects the unfolding semantics of {\\lambda}-letrec and which is\nclosed under functional bisimulation. From this we derive efficient methods to\ndetermine whether two terms are equivalent under infinite unfolding and to\ncompute the maximally shared form of a given {\\lambda}-letrec term.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 10:41:32 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Rochel", "Jan", ""]]}, {"id": "1610.06067", "submitter": "Aws Albarghouthi", "authors": "Aws Albarghouthi and Loris D'Antoni and Samuel Drews and Aditya Nori", "title": "Fairness as a Program Property", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the following question: Is a decision-making program fair, for\nsome useful definition of fairness? First, we describe how several algorithmic\nfairness questions can be phrased as program verification problems. Second, we\ndiscuss an automated verification technique for proving or disproving fairness\nof decision-making programs with respect to a probabilistic model of the\npopulation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 15:31:34 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Albarghouthi", "Aws", ""], ["D'Antoni", "Loris", ""], ["Drews", "Samuel", ""], ["Nori", "Aditya", ""]]}, {"id": "1610.06768", "submitter": "Hiroshi Unno", "authors": "Hiroshi Unno and Sho Torii", "title": "Automating Induction for Solving Horn Clauses", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification problems of programs written in various paradigms (such as\nimperative, logic, concurrent, functional, and object-oriented ones) can be\nreduced to problems of solving Horn clause constraints on predicate variables\nthat represent unknown inductive invariants. This paper presents a novel Horn\nconstraint solving method based on inductive theorem proving: the method\nreduces Horn constraint solving to validity checking of first-order formulas\nwith inductively defined predicates, which are then checked by induction on the\nderivation of the predicates. To automate inductive proofs, we introduce a\nnovel proof system tailored to Horn constraint solving and use an SMT solver to\ndischarge proof obligations arising in the proof search. The main advantage of\nthe proposed method is that it can verify relational specifications across\nprograms in various paradigms where multiple function calls need to be analyzed\nsimultaneously. The class of specifications includes practically important ones\nsuch as functional equivalence, associativity, commutativity, distributivity,\nmonotonicity, idempotency, and non-interference. Furthermore, our novel\ncombination of Horn clause constraints with inductive theorem proving enables\nus to naturally and automatically axiomatize recursive functions that are\npossibly non-terminating, non-deterministic, higher-order, exception-raising,\nand over non-inductively defined data types. We have implemented a relational\nverification tool for the OCaml functional language based on the proposed\nmethod and obtained promising results in preliminary experiments.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2016 12:44:23 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Unno", "Hiroshi", ""], ["Torii", "Sho", ""]]}, {"id": "1610.07033", "submitter": "Martin Leinberger", "authors": "Martin Leinberger and Ralf L\\\"ammel and Steffen Staab", "title": "LambdaDL: Syntax and Semantics (Preliminary Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic data fuels many different applications, but is still lacking proper\nintegration into programming languages. Untyped access is error-prone while\nmapping approaches cannot fully capture the conceptualization of semantic data.\nIn this paper, we present $\\lambda_{DL}$,a $\\lambda$-calculus with a modified\ntype system to provide type-safe integration of semantic data. This is achieved\nby the integration of description logics into the $\\lambda$-calculus for typing\nand data access. It is centered around several key design principles. Among\nthese are (1) the usage of semantic conceptualizations as types, (2) subtype\ninference for these types, and (3) type-checked query access to the data by\nboth ensuring the satisfiability of queries as well as typing query results\nprecisely in $\\lambda_{DL}$. The paper motivates the use of a modified type\nsystem for semantic data and it provides the theoretic foundation for the\nintegration of description logics as well as the core formal specifications of\n$\\lambda_{DL}$ including a proof of type safety.\n", "versions": [{"version": "v1", "created": "Sat, 22 Oct 2016 11:07:59 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Leinberger", "Martin", ""], ["L\u00e4mmel", "Ralf", ""], ["Staab", "Steffen", ""]]}, {"id": "1610.07118", "submitter": "Niki Vazou", "authors": "Niki Vazou and Jeff Polakow", "title": "Verified Parallel String Matching in Haskell", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove correctness of parallelizing a string matcher using\nHaskell as a theorem prover. We use refinement types to specify correctness\nproperties, Haskell terms to express proofs and Liquid Haskell to check\ncorrectness of proofs. First, we specify and prove that a class of monoid\nmorphisms can be parallelized via parallel monoid concatenation. Then, we\nencode string matching as a morphism to get a provably correct parallel\ntransformation. Our 1839LoC prototype proof shows that Liquid Haskell can be\nused as a fully expressive theorem prover on realistic Haskell implementations.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 03:15:04 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Vazou", "Niki", ""], ["Polakow", "Jeff", ""]]}, {"id": "1610.07198", "submitter": "Pierre Ganty", "authors": "Pierre Ganty and Boris K\\\"opf and Pedro Valero", "title": "A Language-theoretic View on Network Protocols", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Input validation is the first line of defense against malformed or malicious\ninputs. It is therefore critical that the validator (which is often part of the\nparser) is free of bugs.\n  To build dependable input validators, we propose using parser generators for\ncontext-free languages. In the context of network protocols, various works have\npointed at context-free languages as falling short to specify precisely or\nconcisely common idioms found in protocols. We review those assessments and\nperform a rigorous, language-theoretic analysis of several common protocol\nidioms. We then demonstrate the practical value of our findings by developing a\nmodular, robust, and efficient input validator for HTTP relying on context-free\ngrammars and regular expressions.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 16:29:30 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 14:37:17 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Ganty", "Pierre", ""], ["K\u00f6pf", "Boris", ""], ["Valero", "Pedro", ""]]}, {"id": "1610.07236", "submitter": "Sanjay Rajopadhye", "authors": "Tian Jin, Nirmal Prajapati, Waruna Ranasinghe, Guillaume Iooss, Yun\n  Zou, Sanjay Rajopadhye and David Wonnacott", "title": "Hybrid Static/Dynamic Schedules for Tiled Polyhedral Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polyhedral compilers perform optimizations such as tiling and\nparallelization; when doing both, they usually generate code that executes\n\"barrier-synchronized wavefronts\" of tiles. We present a system to express and\ngenerate code for hybrid schedules, where some constraints are automatically\nsatisfied through the structure of the code, and the remainder are dynamically\nenforced at run-time with data flow mechanisms. We prove bounds on the added\noverheads that are better, by at least one polynomial degree, than those of\nprevious techniques.\n  We propose a generic mechanism to implement the needed synchronization, and\nshow it can be easily realized for a variety of targets: OpenMP, Pthreads, GPU\n(CUDA or OpenCL) code, languages like X10, Habanero, Cilk, as well as data flow\nplatforms like DAGuE, and OpenStream and MPI. We also provide a simple concrete\nimplementation that works without the need of any sophisticated run-time\nmechanism.\n  Our experiments show our simple implementation to be competitive or better\nthan the wavefront-synchronized code generated by other systems. We also show\nhow the proposed mechanism can achieve 24% to 70% reduction in energy.\n", "versions": [{"version": "v1", "created": "Sun, 23 Oct 2016 21:29:29 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Jin", "Tian", ""], ["Prajapati", "Nirmal", ""], ["Ranasinghe", "Waruna", ""], ["Iooss", "Guillaume", ""], ["Zou", "Yun", ""], ["Rajopadhye", "Sanjay", ""], ["Wonnacott", "David", ""]]}, {"id": "1610.07390", "submitter": "Roberto Bagnara", "authors": "Roberto Bagnara, Michele Chiari, Roberta Gori, Abramo Bagnara", "title": "A Practical Approach to Interval Refinement for math.h/cmath Functions", "comments": "98 pages, 2 figures, 11 tables, 11 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification of C++ programs has seen considerable progress in several areas,\nbut not for programs that use these languages' mathematical libraries. The\nreason is that all libraries in widespread use come with no guarantees about\nthe computed results. This would seem to prevent any attempt at formal\nverification of programs that use them: without a specification for the\nfunctions, no conclusion can be drawn statically about the behavior of the\nprogram. We propose an alternative to surrender. We introduce a pragmatic\napproach that leverages the fact that most math.h/cmath functions are almost\npiecewise monotonic: as we discovered through exhaustive testing, they may have\nglitches, often of very small size and in small numbers. We develop interval\nrefinement techniques for such functions based on a modified dichotomic search,\nthat enable verification via symbolic execution based model checking, abstract\ninterpretation, and test data generation. Our refinement algorithms are the\nfirst in the literature to be able to handle non-correctly rounded function\nimplementations, enabling verification in the presence of the most common\nimplementations. We experimentally evaluate our approach on real-world code,\nshowing its ability to detect or rule out anomalous behaviors.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 12:39:29 GMT"}, {"version": "v2", "created": "Sat, 25 Mar 2017 15:22:17 GMT"}, {"version": "v3", "created": "Tue, 11 Aug 2020 14:25:00 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Bagnara", "Roberto", ""], ["Chiari", "Michele", ""], ["Gori", "Roberta", ""], ["Bagnara", "Abramo", ""]]}, {"id": "1610.07696", "submitter": "EPTCS", "authors": "Mirco Tribastone (IMT School for Advanced Studies Lucca, Italy),\n  Herbert Wiklicky (Imperial College London)", "title": "Proceedings 14th International Workshop Quantitative Aspects of\n  Programming Languages and Systems", "comments": null, "journal-ref": "EPTCS 227, 2016", "doi": "10.4204/EPTCS.227", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the post-proceedings of the 14th International Workshop\non Quantitative Aspects of Programming Languages and Systems (QAPL), held as a\nsatellite workshop of ETAPS 2016 in Eindhoven, The Netherlands, on 2-3 April\n2016.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 01:00:47 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Tribastone", "Mirco", "", "IMT School for Advanced Studies Lucca, Italy"], ["Wiklicky", "Herbert", "", "Imperial College London"]]}, {"id": "1610.07914", "submitter": "Roberto Bagnara", "authors": "Roberto Bagnara, Abramo Bagnara, Alessandro Benedetti, Patricia M.\n  Hill", "title": "The ACPATH Metric: Precise Estimation of the Number of Acyclic Paths in\n  C-like Languages", "comments": "62 pages, 10 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NPATH is a metric introduced by Brian A. Nejmeh in [13] that is aimed at\novercoming some important limitations of McCabe's cyclomatic complexity.\nDespite the fact that the declared NPATH objective is to count the number of\nacyclic execution paths through a function, the definition given for the C\nlanguage in [13] fails to do so even for very simple programs. We show that\ncounting the number of acyclic paths in CFG is unfeasible in general. Then we\ndefine a new metric for C-like languages, called ACPATH, that allows to quickly\ncompute a very good estimation of the number of acyclic execution paths through\nthe given function. We show that, if the function body does not contain\nbackward gotos and does not contain jumps into a loop from outside the loop,\nthen such estimation is actually exact.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 15:11:46 GMT"}, {"version": "v2", "created": "Wed, 26 Oct 2016 05:16:08 GMT"}, {"version": "v3", "created": "Fri, 28 Jul 2017 08:21:24 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Bagnara", "Roberto", ""], ["Bagnara", "Abramo", ""], ["Benedetti", "Alessandro", ""], ["Hill", "Patricia M.", ""]]}, {"id": "1610.07965", "submitter": "Nikos Tzevelekos", "authors": "Andrzej S. Murawski and Nikos Tzevelekos", "title": "Higher-Order Linearisability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linearisability is a central notion for verifying concurrent libraries: a\ngiven library is proven safe if its operational history can be rearranged into\na new sequential one which, in addition, satisfies a given specification.\nLinearisability has been examined for libraries in which method arguments and\nmethod results are of ground type, including libraries parameterised with such\nmethods. In this paper we extend linearisability to the general higher-order\nsetting: methods can be passed as arguments and returned as values. A library\nmay also depend on abstract methods of any order. We use this generalised\nnotion to show correctness of several higher-order example libraries.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 17:01:29 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Murawski", "Andrzej S.", ""], ["Tzevelekos", "Nikos", ""]]}, {"id": "1610.07978", "submitter": "Richard Eisenberg", "authors": "Richard A. Eisenberg", "title": "Dependent Types in Haskell: Theory and Practice", "comments": "PhD dissertation, University of Pennsylvania", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Haskell, as implemented in the Glasgow Haskell Compiler (GHC), has been\nadding new type-level programming features for some time. Many of these\nfeatures---chiefly: generalized algebraic datatypes (GADTs), type families,\nkind polymorphism, and promoted datatypes---have brought Haskell to the\ndoorstep of dependent types. Many dependently typed programs can even currently\nbe encoded, but often the constructions are painful.\n  In this dissertation, I describe Dependent Haskell, which supports full\ndependent types via a backward-compatible extension to today's Haskell. An\nimportant contribution of this work is an implementation, in GHC, of a portion\nof Dependent Haskell, with the rest to follow. The features I have implemented\nare already released, in GHC 8.0. This dissertation contains several practical\nexamples of Dependent Haskell code, a full description of the differences\nbetween Dependent Haskell and today's Haskell, a novel type-safe dependently\ntyped lambda-calculus (called Pico) suitable for use as an intermediate\nlanguage for compiling Dependent Haskell, and a type inference and elaboration\nalgorithm, Bake, that translates Dependent Haskell to type-correct Pico.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 17:28:46 GMT"}, {"version": "v2", "created": "Sat, 12 Aug 2017 18:03:12 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Eisenberg", "Richard A.", ""]]}, {"id": "1610.08115", "submitter": "Zhuo Chen", "authors": "Zhuo Chen, Kyle Marple, Elmer Salazar, Gopal Gupta, Lakshman Tamil", "title": "A Physician Advisory System for Chronic Heart Failure Management Based\n  on Knowledge Patterns", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 14 pages,\n  LaTeX", "journal-ref": null, "doi": "10.1017/S1471068416000429", "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Management of chronic diseases such as heart failure, diabetes, and chronic\nobstructive pulmonary disease (COPD) is a major problem in health care. A\nstandard approach that the medical community has devised to manage widely\nprevalent chronic diseases such as chronic heart failure (CHF) is to have a\ncommittee of experts develop guidelines that all physicians should follow.\nThese guidelines typically consist of a series of complex rules that make\nrecommendations based on a patient's information. Due to their complexity,\noften the guidelines are either ignored or not complied with at all, which can\nresult in poor medical practices. It is not even clear whether it is humanly\npossible to follow these guidelines due to their length and complexity. In the\ncase of CHF management, the guidelines run nearly 80 pages. In this paper we\ndescribe a physician-advisory system for CHF management that codes the entire\nset of clinical practice guidelines for CHF using answer set programming. Our\napproach is based on developing reasoning templates (that we call knowledge\npatterns) and using these patterns to systemically code the clinical guidelines\nfor CHF as ASP rules. Use of the knowledge patterns greatly facilitates the\ndevelopment of our system. Given a patient's medical information, our system\ngenerates a recommendation for treatment just as a human physician would, using\nthe guidelines. Our system will work even in the presence of incomplete\ninformation. Our work makes two contributions: (i) it shows that highly complex\nguidelines can be successfully coded as ASP rules, and (ii) it develops a\nseries of knowledge patterns that facilitate the coding of knowledge expressed\nin a natural language and that can be used for other application domains. This\npaper is under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 23:05:03 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Chen", "Zhuo", ""], ["Marple", "Kyle", ""], ["Salazar", "Elmer", ""], ["Gupta", "Gopal", ""], ["Tamil", "Lakshman", ""]]}, {"id": "1610.08170", "submitter": "EPTCS", "authors": "Dominic Duggan (Stevens Institute of Technology), Jianhua Yao (Stevens\n  Institute of Technology)", "title": "Parameterized Dataflow (Extended Abstract)", "comments": "In Proceedings QAPL'16, arXiv:1610.07696", "journal-ref": "EPTCS 227, 2016, pp. 63-81", "doi": "10.4204/EPTCS.227.5", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dataflow networks have application in various forms of stream processing, for\nexample for parallel processing of multimedia data. The description of dataflow\ngraphs, including their firing behavior, is typically non-compositional and not\namenable to separate compilation. This article considers a dataflow language\nwith a type and effect system that captures the firing behavior of actors. This\nsystem allows definitions to abstract over actor firing rates, supporting the\ndefinition and safe composition of actor definitions where firing rates are not\ninstantiated until a dataflow graph is launched.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 05:00:33 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Duggan", "Dominic", "", "Stevens Institute of Technology"], ["Yao", "Jianhua", "", "Stevens\n  Institute of Technology"]]}, {"id": "1610.08419", "submitter": "Christoph Rauch", "authors": "Chiara Bodei, Pierpaolo Degano, Gian-Luigi Ferrari and Letterio\n  Galletta", "title": "Tracing where IoT data are collected and aggregated", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 13, Issue 3 (July 19,\n  2017) lmcs:3799", "doi": "10.23638/LMCS-13(3:5)2017", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) offers the infrastructure of the information\nsociety. It hosts smart objects that automatically collect and exchange data of\nvarious kinds, directly gathered from sensors or generated by aggregations.\nSuitable coordination primitives and analysis mechanisms are in order to design\nand reason about IoT systems, and to intercept the implied technological\nshifts. We address these issues from a foundational point of view. To study\nthem, we define IoT-LySa, a process calculus endowed with a static analysis\nthat tracks the provenance and the manipulation of IoT data, and how they flow\nin the system. The results of the analysis can be used by a designer to check\nthe behaviour of smart objects, in particular to verify non-functional\nproperties, among which security.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 16:55:14 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 09:17:23 GMT"}, {"version": "v3", "created": "Tue, 18 Jul 2017 07:58:19 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Bodei", "Chiara", ""], ["Degano", "Pierpaolo", ""], ["Ferrari", "Gian-Luigi", ""], ["Galletta", "Letterio", ""]]}, {"id": "1610.08476", "submitter": "Michael Vitousek", "authors": "Michael M. Vitousek and Jeremy G. Siek", "title": "Gradual Typing in an Open World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradual typing combines static and dynamic typing in the same language,\noffering the benefits of both to programmers. Static typing provides error\ndetection and strong guarantees while dynamic typing enables rapid prototyping\nand flexible programming idioms. For programmers to fully take advantage of a\ngradual type system, however, they must be able to trust their type\nannotations, and so runtime checks must be performed at the boundaries of\nstatic and dynamic code to ensure that static types are respected. Higher order\nand mutable values cannot be completely checked at these boundaries, and so\nadditional checks must be performed at their use sites. Traditionally, this has\nbeen achieved by installing wrappers or proxies on such values that moderate\nthe flow of data between static and dynamic, but these can cause problems if\nthe language supports comparison of object identity or has a foreign function\ninterface.\n  Reticulated Python is a gradually typed variant of Python implemented via a\nsource-to-source translator for Python 3. It implements a proxy-free\nalternative design named transient casts. This paper presents a formal\nsemantics for transient casts and shows that not only are they sound, but they\nwork in an open-world setting in which the Reticulated translator has only been\napplied to some of the program; the rest is untranslated Python. We formalize\nthis open world soundness property and use Coq to prove that it holds for\nAnthill Python, a calculus that models Reticulated Python.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 19:36:31 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Vitousek", "Michael M.", ""], ["Siek", "Jeremy G.", ""]]}, {"id": "1610.08691", "submitter": "Nick Brown", "authors": "Nick Brown", "title": "Type oriented parallel programming for Exascale", "comments": "As presented at the Exascale Applications and Software Conference\n  (EASC), 9th-11th April 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst there have been great advances in HPC hardware and software in recent\nyears, the languages and models that we use to program these machines have\nremained much more static. This is not from a lack of effort, but instead by\nvirtue of the fact that the foundation that many programming languages are\nbuilt on is not sufficient for the level of expressivity required for parallel\nwork. The result is an implicit trade-off between programmability and\nperformance which is made worse due to the fact that, whilst many scientific\nusers are experts within their own fields, they are not HPC experts.\n  Type oriented programming looks to address this by encoding the complexity of\na language via the type system. Most of the language functionality is contained\nwithin a loosely coupled type library that can be flexibly used to control many\naspects such as parallelism. Due to the high level nature of this approach\nthere is much information available during compilation which can be used for\noptimisation and, in the absence of type information, the compiler can apply\nsensible default options thus supporting both the expert programmer and novice\nalike.\n  We demonstrate that, at no performance or scalability penalty when running on\nup to 8196 cores of a Cray XE6 system, codes written in this type oriented\nmanner provide improved programmability. The programmer is able to write\nsimple, implicit parallel, HPC code at a high level and then explicitly tune by\nadding additional type information if required.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 10:28:53 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Brown", "Nick", ""]]}, {"id": "1610.08843", "submitter": "Julien Lange", "authors": "Julien Lange and Nicholas Ng and Bernardo Toninho and Nobuko Yoshida", "title": "Fencing off Go: Liveness and Safety for Channel-based Programming\n  (extended version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Go is a production-level statically typed programming language whose design\nfeatures explicit message-passing primitives and lightweight threads, enabling\n(and encouraging) programmers to develop concurrent systems where components\ninteract through communication more so than by lock-based shared memory\nconcurrency. Go can only detect global deadlocks at runtime, but provides no\ncompile-time protection against all too common communication mismatches or\npartial deadlocks. This work develops a static verification framework for\nliveness and safety in Go programs, able to detect communication errors and\npartial deadlocks in a general class of realistic concurrent programs,\nincluding those with dynamic channel creation, unbounded thread creation and\nrecursion. Our approach infers from a Go program a faithful representation of\nits communication patterns as a behavioural type. By checking a syntactic\nrestriction on channel usage, dubbed fencing, we ensure that programs are made\nup of finitely many different communication patterns that may be repeated\ninfinitely many times. This restriction allows us to implement procedures to\ncheck for liveness and safety in types which in turn approximates liveness and\nsafety in Go programs. We have implemented a type inference and liveness and\nsafety checks in a tool-chain and tested it against publicly available Go\nprograms.\n  Note: This is a revised extended version of a paper that appeared in POPL\n2017, see page 13 for details.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 15:40:45 GMT"}, {"version": "v2", "created": "Mon, 16 Jan 2017 18:06:37 GMT"}, {"version": "v3", "created": "Tue, 28 Feb 2017 13:55:03 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Lange", "Julien", ""], ["Ng", "Nicholas", ""], ["Toninho", "Bernardo", ""], ["Yoshida", "Nobuko", ""]]}, {"id": "1610.09161", "submitter": "Ohad Kammar", "authors": "Yannick Forster, Ohad Kammar, Sam Lindley, Matija Pretnar", "title": "On the Expressive Power of User-Defined Effects: Effect Handlers,\n  Monadic Reflection, Delimited Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare the expressive power of three programming abstractions for\nuser-defined computational effects: Bauer and Pretnar's effect handlers,\nFilinski's monadic reflection, and delimited control without\nanswer-type-modification. This comparison allows a precise discussion about the\nrelative expressiveness of each programming abstraction. It also demonstrates\nthe sensitivity of the relative expressiveness of user-defined effects to\nseemingly orthogonal language features. We present three calculi, one per\nabstraction, extending Levy's call-by-push-value. For each calculus, we present\nsyntax, operational semantics, a natural type-and-effect system, and, for\neffect handlers and monadic reflection, a set-theoretic denotational semantics.\nWe establish their basic meta-theoretic properties: safety, termination, and,\nwhere applicable, soundness and adequacy. Using Felleisen's notion of a macro\ntranslation, we show that these abstractions can macro-express each other, and\nshow which translations preserve typeability. We use the adequate finitary\nset-theoretic denotational semantics for the monadic calculus to show that\neffect handlers cannot be macro-expressed while preserving typeability either\nby monadic reflection or by delimited control. We supplement our development\nwith a mechanised Abella formalisation.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 10:36:42 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 15:12:00 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Forster", "Yannick", ""], ["Kammar", "Ohad", ""], ["Lindley", "Sam", ""], ["Pretnar", "Matija", ""]]}, {"id": "1610.09166", "submitter": "Amir Shaikhha", "authors": "Amir Shaikhha, Mohammad Dashti, Christoph Koch", "title": "Push vs. Pull-Based Loop Fusion in Query Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database query engines use pull-based or push-based approaches to avoid the\nmaterialization of data across query operators. In this paper, we study these\ntwo types of query engines in depth and present the limitations and advantages\nof each engine. Similarly, the programming languages community has developed\nloop fusion techniques to remove intermediate collections in the context of\ncollection programming. We draw parallels between the DB and PL communities by\ndemonstrating the connection between pipelined query engines and loop fusion\ntechniques. Based on this connection, we propose a new type of pull-based\nengine, inspired by a loop fusion technique, which combines the benefits of\nboth approaches. Then we experimentally evaluate the various engines, in the\ncontext of query compilation, for the first time in a fair environment,\neliminating the biasing impact of ancillary optimizations that have\ntraditionally only been used with one of the approaches. We show that for\nrealistic analytical workloads, there is no considerable advantage for either\nform of pipelined query engine, as opposed to what recent research suggests.\nAlso, by using microbenchmarks we show that our proposed engine dominates the\nexisting engines by combining the benefits of both.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 11:00:02 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Shaikhha", "Amir", ""], ["Dashti", "Mohammad", ""], ["Koch", "Christoph", ""]]}, {"id": "1610.09183", "submitter": "Christoph Rauch", "authors": "Ludovic Henrio and Justine Rochas", "title": "Multiactive objects and their applications", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 13, Issue 4 (November\n  21, 2017) lmcs:4079", "doi": "10.23638/LMCS-13(4:12)2017", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to tackle the development of concurrent and distributed systems, the\nactive object programming model provides a high-level abstraction to program\nconcurrent behaviours. There exists already a variety of active object\nframeworks targeted at a large range of application domains: modelling,\nverification, efficient execution. However, among these frameworks, very few\nconsider a multi-threaded execution of active objects. Introducing controlled\nparallelism within active objects enables overcoming some of their limitations.\nIn this paper, we present a complete framework around the multi-active object\nprogramming model. We present it through ProActive, the Java library that\noffers multi-active objects, and through MultiASP, the programming language\nthat allows the formalisation of our developments. We then show how to compile\nan active object language with cooperative multi-threading into multi-active\nobjects. This paper also presents different use cases and the development\nsupport to illustrate the practical usability of our language. Formalisation of\nour work provides the programmer with guarantees on the behaviour of the\nmulti-active object programming model and of the compiler.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 12:15:59 GMT"}, {"version": "v2", "created": "Sun, 14 May 2017 22:59:35 GMT"}, {"version": "v3", "created": "Fri, 3 Nov 2017 13:33:52 GMT"}, {"version": "v4", "created": "Mon, 20 Nov 2017 12:58:14 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Henrio", "Ludovic", ""], ["Rochas", "Justine", ""]]}, {"id": "1610.09543", "submitter": "Pin-Yu Chen", "authors": "Pai-Shun Ting, Chun-Chen Tu, Pin-Yu Chen, Ya-Yun Lo, Shin-Ming Cheng", "title": "FEAST: An Automated Feature Selection Framework for Compilation Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of the application of machine-learning techniques to compilation\ntasks can be largely attributed to the recent development and advancement of\nprogram characterization, a process that numerically or structurally quantifies\na target program. While great achievements have been made in identifying key\nfeatures to characterize programs, choosing a correct set of features for a\nspecific compiler task remains an ad hoc procedure. In order to guarantee a\ncomprehensive coverage of features, compiler engineers usually need to select\nexcessive number of features. This, unfortunately, would potentially lead to a\nselection of multiple similar features, which in turn could create a new\nproblem of bias that emphasizes certain aspects of a program's characteristics,\nhence reducing the accuracy and performance of the target compiler task. In\nthis paper, we propose FEAture Selection for compilation Tasks (FEAST), an\nefficient and automated framework for determining the most relevant and\nrepresentative features from a feature pool. Specifically, FEAST utilizes\nwidely used statistics and machine-learning tools, including LASSO, sequential\nforward and backward selection, for automatic feature selection, and can in\ngeneral be applied to any numerical feature set. This paper further proposes an\nautomated approach to compiler parameter assignment for assessing the\nperformance of FEAST. Intensive experimental results demonstrate that, under\nthe compiler parameter assignment task, FEAST can achieve comparable results\nwith about 18% of features that are automatically selected from the entire\nfeature pool. We also inspect these selected features and discuss their roles\nin program execution.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 17:10:15 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Ting", "Pai-Shun", ""], ["Tu", "Chun-Chen", ""], ["Chen", "Pin-Yu", ""], ["Lo", "Ya-Yun", ""], ["Cheng", "Shin-Ming", ""]]}, {"id": "1610.09787", "submitter": "Dustin Tran", "authors": "Dustin Tran, Alp Kucukelbir, Adji B. Dieng, Maja Rudolph, Dawen Liang,\n  David M. Blei", "title": "Edward: A library for probabilistic modeling, inference, and criticism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI cs.PL stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic modeling is a powerful approach for analyzing empirical\ninformation. We describe Edward, a library for probabilistic modeling. Edward's\ndesign reflects an iterative process pioneered by George Box: build a model of\na phenomenon, make inferences about the model given data, and criticize the\nmodel's fit to the data. Edward supports a broad class of probabilistic models,\nefficient algorithms for inference, and many techniques for model criticism.\nThe library builds on top of TensorFlow to support distributed training and\nhardware such as GPUs. Edward enables the development of complex probabilistic\nmodels and their algorithms at a massive scale.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 04:56:13 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 06:47:13 GMT"}, {"version": "v3", "created": "Wed, 1 Feb 2017 01:37:04 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Tran", "Dustin", ""], ["Kucukelbir", "Alp", ""], ["Dieng", "Adji B.", ""], ["Rudolph", "Maja", ""], ["Liang", "Dawen", ""], ["Blei", "David M.", ""]]}, {"id": "1610.10050", "submitter": "Lu\\'is Cruz-Filipe", "authors": "Lu\\'is Cruz-Filipe, Kim S. Larsen, Fabrizio Montesi", "title": "The Paths to Choreography Extraction", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-662-54458-7_25", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choreographies are global descriptions of interactions among concurrent\ncomponents, most notably used in the settings of verification (e.g., Multiparty\nSession Types) and synthesis of correct-by-construction software (Choreographic\nProgramming). They require a top-down approach: programmers first write\nchoreographies, and then use them to verify or synthesize their programs.\nHowever, most existing software does not come with choreographies yet, which\nprevents their application.\n  To attack this problem, we propose a novel methodology (called choreography\nextraction) that, given a set of programs or protocol specifications,\nautomatically constructs a choreography that describes their behavior. The key\nto our extraction is identifying a set of paths in a graph that represents the\nsymbolic execution of the programs of interest. Our method improves on previous\nwork in several directions: we can now deal with programs that are equipped\nwith a state and internal computation capabilities; time complexity is\ndramatically better; we capture programs that are correct but not necessarily\nsynchronizable, i.e., they work because they exploit asynchronous\ncommunication.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 18:22:40 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2017 11:45:34 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Cruz-Filipe", "Lu\u00eds", ""], ["Larsen", "Kim S.", ""], ["Montesi", "Fabrizio", ""]]}]