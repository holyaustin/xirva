[{"id": "2006.00131", "submitter": "Evandro Ribeiro Da Rosa", "authors": "Evandro Chagas Ribeiro da Rosa, Rafael de Santiago", "title": "Classical and Quantum Data Interaction in Programming Languages: A\n  Runtime Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a runtime architecture that can be used in the development of a\nquantum programming language and its programming environment. The proposed\nruntime architecture enables dynamic interaction between classical and quantum\ndata following the restriction that a quantum computer is available in the\ncloud as a batch computer, with no interaction with the classical computer\nduring its execution. It is done by leaving the quantum code generation for the\nruntime and introducing the concept of futures for quantum measurements. When\nimplemented in a quantum programming language, those strategies aim to\nfacilitate the development of quantum applications, especially for beginning\nprogrammers and students. Being suitable for the current Noisy\nIntermediate-Scale Quantum (NISQ) Computers, the runtime architecture is also\nappropriate for simulation and future Fault-Tolerance Quantum Computers.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 23:51:24 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["da Rosa", "Evandro Chagas Ribeiro", ""], ["de Santiago", "Rafael", ""]]}, {"id": "2006.01491", "submitter": "Andreas Pavlogiannis", "authors": "Anders Alnor Mathiasen and Andreas Pavlogiannis", "title": "The Fine-Grained and Parallel Complexity of Andersen's Pointer Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pointer analysis is one of the fundamental problems in static program\nanalysis. Given a set of pointers, the task is to produce a useful\nover-approximation of the memory locations that each pointer may point-to at\nruntime. The most common formulation is Andersen's Pointer Analysis (APA),\ndefined as an inclusion-based set of $m$ pointer constraints over a set of $n$\npointers. Existing algorithms solve APA in $O(n^2\\cdot m)$ time, while it has\nbeen conjectured that the problem has no truly sub-cubic algorithm, with a\nproof so far having remained elusive.\n  In this work we draw a rich fine-grained and parallel complexity landscape of\nAPA, and present upper and lower bounds. First, we establish an $O(n^3)$\nupper-bound for general APA, improving over $O(n^2\\cdot m)$ as $n=O(m)$.\nSecond, we show that even on-demand APA (\"may a specific pointer $a$ point to a\nspecific location $b$?\") has an $\\Omega(n^3)$ (combinatorial) lower bound under\nstandard complexity-theoretic hypotheses. This formally establishes the\nlong-conjectured \"cubic bottleneck\" of APA, and shows that our $O(n^3)$-time\nalgorithm is optimal. Third, we show that under mild restrictions, APA is\nsolvable in $\\tilde{O}(n^{\\omega})$ time, where $\\omega<2.373$ is the\nmatrix-multiplication exponent. It is believed that $\\omega=2+o(1)$, in which\ncase this bound becomes quadratic. Fourth, we show that even under such\nrestrictions, even the on-demand problem has an $\\Omega(n^2)$ lower bound under\nstandard complexity-theoretic hypotheses, and hence our algorithm is optimal\nwhen $\\omega=2+o(1)$. Fifth, we study the parallelizability of APA and\nestablish lower and upper bounds: (i) in general, the problem is P-complete and\nhence unlikely parallelizable, whereas (ii) under mild restrictions, the\nproblem is parallelizable. Our theoretical treatment formalizes several\ninsights that can lead to practical improvements in the future.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 09:45:31 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 10:34:21 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 10:15:51 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Mathiasen", "Anders Alnor", ""], ["Pavlogiannis", "Andreas", ""]]}, {"id": "2006.01531", "submitter": "Sandra Dylus", "authors": "Sandra Dylus", "title": "Effectful Programming in Declarative Languages with an Emphasis on\n  Non-Determinism: Applications and Formal Reasoning", "comments": "in German, Dissertation as submitted for publication at University of\n  Kiel", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This thesis investigates effectful declarative programming with an emphasis\non non-determinism as an effect. On the one hand, we are interested in\ndeveloping applications using non-determinism as underlying implementation\nidea. We discuss two applications using the functional logic programming\nlanguage Curry. The key idea of these implementations is to exploit the\ninterplay of non-determinism and non-strictness that Curry employs. The first\napplication investigates sorting algorithms parametrised over a comparison\nfunction. By applying a non-deterministic predicate to these sorting functions,\nwe gain a permutation enumeration function. We compare the implementation in\nCurry with an implementation in Haskell that uses a monadic interface to model\nnon-determinism. The other application that we discuss in this work is a\nlibrary for probabilistic programming. Instead of modelling distributions as\nlist of event and probability pairs, we model distributions using Curry's\nbuilt-in non-determinism. In both cases we observe that the combination of\nnon-determinism and non-strictness has advantages over an implementation using\nlists to model non-determinism. On the other hand, we present an idea to apply\nformal reasoning on effectful declarative programming languages. In order to\nstart with simple effects, we focus on modelling a functional subset first.\nThat is, the effects of interest are totality and partiality. We then observe\nthat the general scheme to model these two effects can be generalised to\ncapture a wide range of effects. Obviously, the next step is to apply the idea\nto model non-determinism. More precisely, we implement a model for the\nnon-determinism of Curry: non-strict non-determinism with call-time choice.\nTherefore, we finally discuss why the current representation models\ncall-by-name rather than Curry's call-by-need semantics and give an outlook on\nideas to tackle this problem.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 11:33:53 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Dylus", "Sandra", ""]]}, {"id": "2006.01608", "submitter": "Sander Huyghebaert", "authors": "Sander Huyghebaert, Thomas Van Strydonck, Steven Keuchel, Dominique\n  Devriese", "title": "Uninitialized Capabilities", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This technical report describes a new extension to capability machines.\nCapability machines are a special type of processors that include better\nsecurity primitives at the hardware level. In capability machines, every word\nhas an associated tag bit that indicates whether the value it contains is a\ncapability or a regular data value. Capabilities enable fine-grained control of\nthe authority over memory that program components have. Conceptually,\ncapabilities can be viewed as being an unforgeable token carrying authority\nover a resource. CHERI is a recently developed capability machine that aims to\nprovide fine-grained memory protection, software compartmentalization and\nbackwards compatibility. While our ideas are implemented on CHERI, they are not\nlimited to it and should be applicable to other capability machines as well. In\nthis technical report we propose a new type of capabilities, which represent\nthe authority to access (read and write to) a block of memory but not view its\ninitial contents. Our main goal is to use this new type of capability as part\nof a secure calling convention, but other applications may be possible too.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 13:47:28 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Huyghebaert", "Sander", ""], ["Van Strydonck", "Thomas", ""], ["Keuchel", "Steven", ""], ["Devriese", "Dominique", ""]]}, {"id": "2006.02204", "submitter": "Dimitur Krustev", "authors": "Dimitur Krustev", "title": "Controlling the Size of Supercompiled Programs using Multi-result\n  Supercompilation", "comments": "Extended version of paper submitted to VPT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Supercompilation is a powerful program transformation technique with numerous\ninteresting applications. Existing methods of supercompilation, however, are\noften very unpredictable with respect to the size of the resulting programs. We\nconsider an approach for controlling result size, based on a combination of\nmulti-result supercompilation and a specific generalization strategy, which\navoids code duplication. The current early experiments with this method show\npromising results -- we can keep the size of the result small, while still\nperforming powerful optimizations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 12:20:45 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Krustev", "Dimitur", ""]]}, {"id": "2006.02230", "submitter": "Sanket Tavarageri", "authors": "Sanket Tavarageri, Alexander Heinecke, Sasikanth Avancha, Gagandeep\n  Goyal, Ramakrishna Upadrasta, Bharat Kaul", "title": "PolyDL: Polyhedral Optimizations for Creation of High Performance DL\n  primitives", "comments": "arXiv admin note: substantial text overlap with arXiv:2002.02145", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) have revolutionized many aspects of our lives.\nThe use of DNNs is becoming ubiquitous including in softwares for image\nrecognition, speech recognition, speech synthesis, language translation, to\nname a few. he training of DNN architectures however is computationally\nexpensive. Once the model is created, its use in the intended application - the\ninference task, is computationally heavy too and the inference needs to be fast\nfor real time use. For obtaining high performance today, the code of Deep\nLearning (DL) primitives optimized for specific architectures by expert\nprogrammers exposed via libraries is the norm. However, given the constant\nemergence of new DNN architectures, creating hand optimized code is expensive,\nslow and is not scalable.\n  To address this performance-productivity challenge, in this paper we present\ncompiler algorithms to automatically generate high performance implementations\nof DL primitives that closely match the performance of hand optimized\nlibraries. We develop novel data reuse analysis algorithms using the polyhedral\nmodel to derive efficient execution schedules automatically. In addition,\nbecause most DL primitives use some variant of matrix multiplication at their\ncore, we develop a flexible framework where it is possible to plug in library\nimplementations of the same in lieu of a subset of the loops. We show that such\na hybrid compiler plus a minimal library-use approach results in\nstate-of-the-art performance. We develop compiler algorithms to also perform\noperator fusions that reduce data movement through the memory hierarchy of the\ncomputer system.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 06:44:09 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 15:43:42 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Tavarageri", "Sanket", ""], ["Heinecke", "Alexander", ""], ["Avancha", "Sasikanth", ""], ["Goyal", "Gagandeep", ""], ["Upadrasta", "Ramakrishna", ""], ["Kaul", "Bharat", ""]]}, {"id": "2006.02670", "submitter": "Danny B{\\o}gsted Poulsen", "authors": "Axel Legay, Dirk Nowotka, Danny B{\\o}gsted Poulsen", "title": "Automatic Verification of LLVM Code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present our work in developing a software verification tool\nfor llvm-code - Lodin - that incorporates both explicit-state model checking,\nstatistical model checking and symbolic state model checking algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 07:08:45 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Legay", "Axel", ""], ["Nowotka", "Dirk", ""], ["Poulsen", "Danny B\u00f8gsted", ""]]}, {"id": "2006.03031", "submitter": "Haichen Shen", "authors": "Haichen Shen, Jared Roesch, Zhi Chen, Wei Chen, Yong Wu, Mu Li, Vin\n  Sharma, Zachary Tatlock, Yida Wang", "title": "Nimble: Efficiently Compiling Dynamic Neural Networks for Model\n  Inference", "comments": null, "journal-ref": "In Proceedings of the 4th Conference on Machine Learning and\n  Systems (MLSys 2021)", "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep neural networks increasingly make use of features such as dynamic\ncontrol flow, data structures and dynamic tensor shapes. Existing deep learning\nsystems focus on optimizing and executing static neural networks which assume a\npre-determined model architecture and input data shapes--assumptions which are\nviolated by dynamic neural networks. Therefore, executing dynamic models with\ndeep learning systems is currently both inflexible and sub-optimal, if not\nimpossible. Optimizing dynamic neural networks is more challenging than static\nneural networks; optimizations must consider all possible execution paths and\ntensor shapes. This paper proposes Nimble, a high-performance and flexible\nsystem to optimize, compile, and execute dynamic neural networks on multiple\nplatforms. Nimble handles model dynamism by introducing a dynamic type system,\na set of dynamism-oriented optimizations, and a light-weight virtual machine\nruntime. Our evaluation demonstrates that Nimble outperforms state-of-the-art\ndeep learning frameworks and runtime systems for dynamic neural networks by up\nto 20x on hardware platforms including Intel CPUs, ARM CPUs, and Nvidia GPUs.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 17:39:58 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 07:20:49 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Shen", "Haichen", ""], ["Roesch", "Jared", ""], ["Chen", "Zhi", ""], ["Chen", "Wei", ""], ["Wu", "Yong", ""], ["Li", "Mu", ""], ["Sharma", "Vin", ""], ["Tatlock", "Zachary", ""], ["Wang", "Yida", ""]]}, {"id": "2006.03102", "submitter": "Bertrand Petit", "authors": "Bertrand Petit (INRIA, France), Manuel Serrano (INRIA, France)", "title": "Interactive Music and Synchronous Reactive Programming", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2021, Vol. 5,\n  Issue 1, Article 2", "doi": "10.22152/programming-journal.org/2021/5/2", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Skini, a programming methodology and an execution\nenvironment for interactive structured music. With this system, the composer\nprograms his scores in the HipHop.js synchronous reactive language. They are\nthen executed, or played, in live concerts, in interaction with the audience.\nThe system aims at helping composers to find a good balance between the\ndeterminism of the compositions and the nondeterminism of the interactions with\nthe public. Each execution of a Skini score yields to a different but\naesthetically consistent interpretation. This work raises many questions in the\nmusical fields. How to combine composition and interaction? How to control the\nmusical style when the audience influences what is to play next? What are the\npossible connections with generative music? These are important questions for\nthe Skini system but they are out of the scope of this paper that focuses\nexclusively on the computer science aspects of the system. From that\nperspective, the main questions are how to program the scores and in which\nlanguage? General purpose languages are inappropriate because their elementary\nconstructs (i.e., variables, functions, loops, etc.) do not match the\nconstructions needed to express music and musical constraints. We show that\nsynchronous programming languages are a much better fit because they rely on\ntemporal constructs that can be directly used to represent musical scores and\nbecause their malleability enables composers to experiment easily with artistic\nvariations of their initial scores. The paper mostly focuses on scores\nprogramming. It exposes the process a composer should follow from his very\nfirst musical intuitions up to the generation of a musical artifact. The paper\npresents some excerpts of the programming of a classical music composition that\nit then precisely relates to an actual recording. Examples of techno music and\njazz are also presented, with audio artifact, to demonstrate the versatility of\nthe system. Finally, brief presentations of past live concerts are presented as\nan evidence of viability of the system.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 19:08:45 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Petit", "Bertrand", "", "INRIA, France"], ["Serrano", "Manuel", "", "INRIA, France"]]}, {"id": "2006.03103", "submitter": "Stefan Monnier", "authors": "Stefan Monnier (University of Montreal, Canada)", "title": "SMIE: Weakness is Power!: Auto-indentation with incomplete information", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2021, Vol. 5,\n  Issue 1, Article 1", "doi": "10.22152/programming-journal.org/2021/5/1", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic indentation of source code is fundamentally a simple matter of\nparsing the code and then applying language- and style-specific rules about\nrelative indentation of the various constructs. Yet, in practice, full parsing\nis not always an option, either because of quirks of the language, or because\nthe code is temporarily syntactically incorrect, or because of an incomplete or\nbroken grammar. I present the design of Emacs's Simple-Minded Indentation\nEngine (SMIE), which gets its power from the weakness of the underlying parsing\ntechnique. It makes it possible to perform local parsing, which is hence\nunaffected by irrelevant surrounding code. This provides a form of graceful\ndegradation in the face of incomplete, erroneous, or just plain problematic\ninformation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 19:08:59 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Monnier", "Stefan", "", "University of Montreal, Canada"]]}, {"id": "2006.03511", "submitter": "Baptiste Roziere", "authors": "Marie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, Guillaume\n  Lample", "title": "Unsupervised Translation of Programming Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A transcompiler, also known as source-to-source translator, is a system that\nconverts source code from a high-level programming language (such as C++ or\nPython) to another. Transcompilers are primarily used for interoperability, and\nto port codebases written in an obsolete or deprecated language (e.g. COBOL,\nPython 2) to a modern one. They typically rely on handcrafted rewrite rules,\napplied to the source code abstract syntax tree. Unfortunately, the resulting\ntranslations often lack readability, fail to respect the target language\nconventions, and require manual modifications in order to work properly. The\noverall translation process is timeconsuming and requires expertise in both the\nsource and target languages, making code-translation projects expensive.\nAlthough neural models significantly outperform their rule-based counterparts\nin the context of natural language translation, their applications to\ntranscompilation have been limited due to the scarcity of parallel data in this\ndomain. In this paper, we propose to leverage recent approaches in unsupervised\nmachine translation to train a fully unsupervised neural transcompiler. We\ntrain our model on source code from open source GitHub projects, and show that\nit can translate functions between C++, Java, and Python with high accuracy.\nOur method relies exclusively on monolingual source code, requires no expertise\nin the source or target languages, and can easily be generalized to other\nprogramming languages. We also build and release a test set composed of 852\nparallel functions, along with unit tests to check the correctness of\ntranslations. We show that our model outperforms rule-based commercial\nbaselines by a significant margin.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 15:28:01 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 10:44:08 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 08:58:24 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Lachaux", "Marie-Anne", ""], ["Roziere", "Baptiste", ""], ["Chanussot", "Lowik", ""], ["Lample", "Guillaume", ""]]}, {"id": "2006.03879", "submitter": "Emery Berger", "authors": "Emery D. Berger", "title": "Scalene: Scripting-Language Aware Profiling for Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing profilers for scripting languages (a.k.a. \"glue\" languages) like\nPython suffer from numerous problems that drastically limit their usefulness.\nThey impose order-of-magnitude overheads, report information at too coarse a\ngranularity, or fail in the face of threads. Worse, past\nprofilers---essentially variants of their counterparts for C---are oblivious to\nthe fact that optimizing code in scripting languages requires information about\ncode spanning the divide between the scripting language and libraries written\nin compiled languages.\n  This paper introduces scripting-language aware profiling, and presents\nScalene, an implementation of scripting-language aware profiling for Python.\nScalene employs a combination of sampling, inference, and disassembly of\nbyte-codes to efficiently and precisely attribute execution time and memory\nusage to either Python, which developers can optimize, or library code, which\nthey cannot. It includes a novel sampling memory allocator that reports\nline-level memory consumption and trends with low overhead, helping developers\nreduce footprints and identify leaks. Finally, it introduces a new metric, copy\nvolume, to help developers root out insidious copying costs across the\nPython/library boundary, which can drastically degrade performance. Scalene\nworks for single or multi-threaded Python code, is precise, reporting detailed\ninformation at the line granularity, while imposing modest overheads\n(26%--53%).\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 14:43:09 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 21:59:52 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Berger", "Emery D.", ""]]}, {"id": "2006.03918", "submitter": "Massimo Bartoletti", "authors": "Massimo Bartoletti and Stefano Lande and Roberto Zunino", "title": "Bitcoin covenants unchained", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covenants are linguistic primitives that extend the Bitcoin script language,\nallowing transactions to constrain the scripts of the redeeming ones. Advocated\nas a way of improving the expressiveness of Bitcoin contracts while preserving\nthe simplicity of the UTXO design, various forms of covenants have been\nproposed over the years. A common drawback of the existing descriptions is the\nlack of formalization, making it difficult to reason about properties and\nsupported use cases. In this paper we propose a formal model of covenants,\nwhich can be implemented with minor modifications to Bitcoin. We use our model\nto specify some complex Bitcoin contracts, and we discuss how to exploit\ncovenants to design high-level language primitives for Bitcoin contracts.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 17:04:00 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 13:14:19 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Bartoletti", "Massimo", ""], ["Lande", "Stefano", ""], ["Zunino", "Roberto", ""]]}, {"id": "2006.04686", "submitter": "Fabio Rinaldi", "authors": "Rinaldi Fabio, Dolci Daniele", "title": "RBF Solver for Quaternions Interpolation", "comments": "23 pages, 21 figures, 2 txt files", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we adapt the RBF Solver to work with quaternions by taking\nadvantage of their Lie Algebra and exponential map. This will allow to work\nwith quaternions as if they were normal vectors in R^3 and blend them in a very\nefficient way.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 15:36:07 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Fabio", "Rinaldi", ""], ["Daniele", "Dolci", ""]]}, {"id": "2006.04757", "submitter": "Markus N Rabe", "authors": "Markus N. Rabe and Dennis Lee and Kshitij Bansal and Christian Szegedy", "title": "Mathematical Reasoning via Self-supervised Skip-tree Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine whether self-supervised language modeling applied to mathematical\nformulas enables logical reasoning. We suggest several logical reasoning tasks\nthat can be used to evaluate language models trained on formal mathematical\nstatements, such as type inference, suggesting missing assumptions and\ncompleting equalities. To train language models for formal mathematics, we\npropose a novel skip-tree task. We find that models trained on the skip-tree\ntask show surprisingly strong mathematical reasoning abilities, and outperform\nmodels trained on standard skip-sequence tasks. We also analyze the models'\nability to formulate new conjectures by measuring how often the predictions are\nprovable and useful in other proofs.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 17:12:08 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 04:30:28 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2020 07:48:41 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Rabe", "Markus N.", ""], ["Lee", "Dennis", ""], ["Bansal", "Kshitij", ""], ["Szegedy", "Christian", ""]]}, {"id": "2006.05080", "submitter": "Pierre Clairambault", "authors": "Pierre Clairambault (LIP, PLUME)", "title": "Learning to Count up to Symmetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop the theory of how to count, in thin concurrent\ngames, the configurations of a strategy witnessing that it reaches a certain\nconfiguration of the game. This plays a central role in many recent\ndevelopments in concurrent games, whenever one aims to relate concurrent\nstrategies with weighted relational models. The difficulty, of course, is\nsymmetry: in the presence of symmetry many configurations of the strategy are,\nmorally, different instances of the same, only differing on the inessential\nchoice of copy indices. How do we know which ones to count? The purpose of the\npaper is to clarify that, uncovering many strange phenomena and fascinating\npathological examples along the way. To illustrate the results, we show that a\ncollapse operation to a simple weighted relational model simply counting\nwitnesses is preserved under composition, provided the strategies involved do\nnot deadlock.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 07:19:44 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Clairambault", "Pierre", "", "LIP, PLUME"]]}, {"id": "2006.05862", "submitter": "Mathias Bourgoin", "authors": "Mathias Bourgoin, Benjamin Canou, Emmanuel Chailloux, Adrien Jonquet\n  and Philippe Wang", "title": "Objective Caml for Multicore Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective Caml is a famous dialect of the ML family languages. It is\nwell-known for its performance as a compiled programming language, notably\nthanks to its incremental generational automatic memory collection. However,\nfor historical reasons, the latter was built for monocore processors. One\nconsequence is the runtime library assumes there is effectively no more than\none thread running at a time, which allows many optimisations for monocore\narchitectures: very few thread mutexes are sufficient to prevent more than a\nsingle thread to run at a time. This makes memory allocation and collection\nquite easier. The way it was built makes it not possible to take advantage of\nnow widespread multicore CPU architectures.\n  This paper presents our feedback on removing Objective Caml's garbage\ncollector and designing a \"Stop-The-World Stop&Copy\" garbage collector to\npermit threads to take advantage of multicore architectures.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 14:38:41 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Bourgoin", "Mathias", ""], ["Canou", "Benjamin", ""], ["Chailloux", "Emmanuel", ""], ["Jonquet", "Adrien", ""], ["Wang", "Philippe", ""]]}, {"id": "2006.05875", "submitter": "Bruce Collie", "authors": "Bruce Collie, Michael O'Boyle", "title": "Retrofitting Symbolic Holes to LLVM IR", "comments": "Accepted to TyDe 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic holes are one of the fundamental building blocks of solver-aided and\ninteractive programming. Unknown values can be soundly integrated into\nprograms, and automated tools such as SAT solvers can be used to prove\nproperties of programs containing them. However, supporting symbolic holes in a\nprogramming language is challenging; specifying interactions of holes with the\ntype system and execution semantics requires careful design.\n  This paper motivates and introduces the implementation of symbolic holes with\nunknown type to LLVM IR, a strongly-typed compiler intermediate language. We\ndescribe how such holes can be implemented safely by abstracting unsound and\ntype-unsafe details behind a new primitive IR manipulation. Our implementation\nco-operates well with existing features such as type and dependency checking.\nFinally, we highlight potentially fruitful areas for investigation using our\nimplementation.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 15:01:08 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Collie", "Bruce", ""], ["O'Boyle", "Michael", ""]]}, {"id": "2006.06077", "submitter": "W{\\l}odzimierz Drabent", "authors": "W{\\l}odzimierz Drabent", "title": "S-semantics -- an example", "comments": "15 pages, 1 figure. This version -- some errors corrected + small\n  modifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The s-semantics makes it possible to explicitly deal with variables in\nprogram answers. So it seems suitable for programs using nonground data\nstructures, like open lists. However it is difficult to find published examples\nof using the s-semantics to reason about particular programs.\n  Here we apply s-semantics to prove correctness and completeness of\nFr\\\"uhwirth's $n$ queens program. This is compared with a proof, published\nelsewhere, based on the standard semantics and Herbrand interpretations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 21:45:25 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 22:06:45 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Drabent", "W\u0142odzimierz", ""]]}, {"id": "2006.06762", "submitter": "Lianmin Zheng", "authors": "Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao Yu, Ameer\n  Haj-Ali, Yida Wang, Jun Yang, Danyang Zhuo, Koushik Sen, Joseph E. Gonzalez,\n  Ion Stoica", "title": "Ansor : Generating High-Performance Tensor Programs for Deep Learning", "comments": "Published in OSDI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.PF cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-performance tensor programs are crucial to guarantee efficient execution\nof deep neural networks. However, obtaining performant tensor programs for\ndifferent operators on various hardware platforms is notoriously challenging.\nCurrently, deep learning systems rely on vendor-provided kernel libraries or\nvarious search strategies to get performant tensor programs. These approaches\neither require significant engineering effort to develop platform-specific\noptimization code or fall short of finding high-performance programs due to\nrestricted search space and ineffective exploration strategy.\n  We present Ansor, a tensor program generation framework for deep learning\napplications. Compared with existing search strategies, Ansor explores many\nmore optimization combinations by sampling programs from a hierarchical\nrepresentation of the search space. Ansor then fine-tunes the sampled programs\nwith evolutionary search and a learned cost model to identify the best\nprograms. Ansor can find high-performance programs that are outside the search\nspace of existing state-of-the-art approaches. In addition, Ansor utilizes a\ntask scheduler to simultaneously optimize multiple subgraphs in deep neural\nnetworks. We show that Ansor improves the execution performance of deep neural\nnetworks relative to the state-of-the-art on the Intel CPU, ARM CPU, and NVIDIA\nGPU by up to $3.8\\times$, $2.6\\times$, and $1.7\\times$, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 19:40:09 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 16:21:22 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 18:32:44 GMT"}, {"version": "v4", "created": "Wed, 4 Nov 2020 07:29:29 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Zheng", "Lianmin", ""], ["Jia", "Chengfan", ""], ["Sun", "Minmin", ""], ["Wu", "Zhao", ""], ["Yu", "Cody Hao", ""], ["Haj-Ali", "Ameer", ""], ["Wang", "Yida", ""], ["Yang", "Jun", ""], ["Zhuo", "Danyang", ""], ["Sen", "Koushik", ""], ["Gonzalez", "Joseph E.", ""], ["Stoica", "Ion", ""]]}, {"id": "2006.07193", "submitter": "Benjamin Kowarsch", "authors": "Benjamin Kowarsch", "title": "A Proposal for a Revision of ISO Modula-2", "comments": "Note: This paper contains the same *terms of reference* as\n  arXiv:1809.07080 (a different paper by the same author) which is erroneously\n  marked as duplication by arXiv's automated process. Changes: This version\n  adds several footnotes and two more references, some formatting has been\n  modified, content remains the same as v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Modula-2 language was first specified in [Wir78] by N. Wirth at ETH\nZurich in 1978 and then revised several times. The last revision [Wir88] was\npublished in 1988. The resulting language reports included ambiguities and\nlacked a comprehensive standard library. To resolve the ambiguities and specify\na comprehensive standard library an ISO/IEC working group was formed and\ncommenced work in 1987. A base standard was then ratified and published as IS\n10514-1 in 1996 [JTC96]. Several conforming compilers have since been\ndeveloped. At least five remain available of which at least three are actively\nmaintained and one has been open sourced. Meanwhile, various deficiencies of\nthe standard have become apparent but since its publication, no revision and no\nmaintenance has been carried out. This paper discusses some of the deficiencies\nof IS 10514-1 and proposes a limited revision that could be carried out with\nmoderate effort. The scope of the paper has been deliberately limited to the\ncore language of the base standard and therefore excludes the standard library.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 15:12:45 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 09:25:16 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Kowarsch", "Benjamin", ""]]}, {"id": "2006.07674", "submitter": "Andr\\'es Ezequiel Viso", "authors": "Alexis Mart\\'in, Alejandro R\\'ios, Andr\\'es Viso", "title": "Pure Pattern Calculus \\`a la de Bruijn", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known in the field of programming languages that dealing with\nvariable names and binders may lead to conflicts such as undesired captures\nwhen implementing interpreters or compilers. This situation has been overcome\nby resorting to de Bruijn indices for calculi where binders capture only one\nvariable name, like the $\\lambda$-calculus. The advantage of this approach\nrelies on the fact that so-called $\\alpha$-equivalence becomes syntactical\nequality when working with indices.\n  In recent years pattern calculi have gained considerable attention given\ntheir expressiveness. They turn out to be notoriously convenient to study the\nfoundations of modern functional programming languages modeling features like\npattern matching, path polymorphism, pattern polymorphism, etc. However, the\nliterature falls short when it comes to dealing with $\\alpha$-conversion and\nbinders capturing simultaneously several variable names. Such is the case of\nthe Pure Pattern Calculus (PPC): a natural extension of $\\lambda$-calculus that\nallows to abstract virtually any term.\n  This paper extends de Bruijn's ideas to properly overcome the multi-binding\nproblem by introducing a novel presentation of PPC with bidimensional indices,\nin an effort to implement a prototype for a typed functional programming\nlanguage based on PPC that captures path polymorphism.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 16:27:35 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 20:20:18 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Mart\u00edn", "Alexis", ""], ["R\u00edos", "Alejandro", ""], ["Viso", "Andr\u00e9s", ""]]}, {"id": "2006.08084", "submitter": "Yujun Yan", "authors": "Yujun Yan, Kevin Swersky, Danai Koutra, Parthasarathy Ranganathan,\n  Milad Hashemi", "title": "Neural Execution Engines: Learning to Execute Subroutines", "comments": "Accepted at 34th Conference on Neural Information Processing Systems\n  (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant effort has been made to train neural networks that replicate\nalgorithmic reasoning, but they often fail to learn the abstract concepts\nunderlying these algorithms. This is evidenced by their inability to generalize\nto data distributions that are outside of their restricted training sets,\nnamely larger inputs and unseen data. We study these generalization issues at\nthe level of numerical subroutines that comprise common algorithms like\nsorting, shortest paths, and minimum spanning trees. First, we observe that\ntransformer-based sequence-to-sequence models can learn subroutines like\nsorting a list of numbers, but their performance rapidly degrades as the length\nof lists grows beyond those found in the training set. We demonstrate that this\nis due to attention weights that lose fidelity with longer sequences,\nparticularly when the input numbers are numerically similar. To address the\nissue, we propose a learned conditional masking mechanism, which enables the\nmodel to strongly generalize far outside of its training range with\nnear-perfect accuracy on a variety of algorithms. Second, to generalize to\nunseen data, we show that encoding numbers with a binary representation leads\nto embeddings with rich structure once trained on downstream tasks like\naddition or multiplication. This allows the embedding to handle missing data by\nfaithfully interpolating numbers not seen during training.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 01:51:37 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 00:44:56 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 22:20:54 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Yan", "Yujun", ""], ["Swersky", "Kevin", ""], ["Koutra", "Danai", ""], ["Ranganathan", "Parthasarathy", ""], ["Hashemi", "Milad", ""]]}, {"id": "2006.08479", "submitter": "Ryan Kavanagh", "authors": "Ryan Kavanagh", "title": "Parametrized Fixed Points on O-Categories and Applications to Session\n  Types", "comments": "Accepted at the 36th International Conference on Mathematical\n  Foundations of Programming Semantics --- MFPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  O-categories generalize categories of domains to provide just the structure\nrequired to compute fixed points of locally continuous functors. Parametrized\nfixed points are of particular interest to denotational semantics and are often\ngiven by \"dagger operations\". We generalize existing techniques to define a\nfunctorial dagger operation on locally continuous functors between\nO-categories. We show that this dagger operation satisfies the Conway\nidentities, a collection of identities used to axiomatize iteration theories.\nWe study the behaviour of this dagger operation on natural transformations and\nconsider applications to semantics of session-typed languages.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 15:29:09 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Kavanagh", "Ryan", ""]]}, {"id": "2006.08614", "submitter": "Sahil Suneja", "authors": "Sahil Suneja, Yunhui Zheng, Yufan Zhuang, Jim Laredo, Alessandro\n  Morari", "title": "Learning to map source code to software vulnerability using\n  code-as-a-graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CR cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the applicability of Graph Neural Networks in learning the nuances\nof source code from a security perspective. Specifically, whether signatures of\nvulnerabilities in source code can be learned from its graph representation, in\nterms of relationships between nodes and edges. We create a pipeline we call\nAI4VA, which first encodes a sample source code into a Code Property Graph. The\nextracted graph is then vectorized in a manner which preserves its semantic\ninformation. A Gated Graph Neural Network is then trained using several such\ngraphs to automatically extract templates differentiating the graph of a\nvulnerable sample from a healthy one. Our model outperforms static analyzers,\nclassic machine learning, as well as CNN and RNN-based deep learning models on\ntwo of the three datasets we experiment with. We thus show that a code-as-graph\nencoding is more meaningful for vulnerability detection than existing\ncode-as-photo and linear sequence encoding approaches. (Submitted Oct 2019,\nPaper #28, ICST)\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 16:05:27 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Suneja", "Sahil", ""], ["Zheng", "Yunhui", ""], ["Zhuang", "Yufan", ""], ["Laredo", "Jim", ""], ["Morari", "Alessandro", ""]]}, {"id": "2006.08854", "submitter": "Harley Eades PhD", "authors": "Harley Eades III and Dominic Orchard", "title": "Grading Adjoint Logic", "comments": "Extended abstract of a talk presented at LINEARITY/TLLA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new logic that combines Adjoint Logic with Graded Necessity\nModalities. This results in a very expressive system capable of controlling\nwhen and how structural rules are used. We give a sequent calculus, natural\ndeduction, and term assignment for Graded Adjoint Logic.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 01:11:08 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Eades", "Harley", "III"], ["Orchard", "Dominic", ""]]}, {"id": "2006.09265", "submitter": "Wenda Li", "authors": "Wenda Li and Lei Yu and Yuhuai Wu and Lawrence C. Paulson", "title": "IsarStep: a Benchmark for High-level Mathematical Reasoning", "comments": "9 pages, published at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.CL cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-defined benchmark is essential for measuring and accelerating research\nprogress of machine learning models. In this paper, we present a benchmark for\nhigh-level mathematical reasoning and study the reasoning capabilities of\nneural sequence-to-sequence models. We build a non-synthetic dataset from the\nlargest repository of proofs written by human experts in a theorem prover. The\ndataset has a broad coverage of undergraduate and research-level mathematical\nand computer science theorems. In our defined task, a model is required to fill\nin a missing intermediate proposition given surrounding proofs. This task\nprovides a starting point for the long-term goal of having machines generate\nhuman-readable proofs automatically. Our experiments and analysis reveal that\nwhile the task is challenging, neural models can capture non-trivial\nmathematical reasoning. We further design a hierarchical transformer that\noutperforms the transformer baseline.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 21:09:23 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 16:45:18 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Li", "Wenda", ""], ["Yu", "Lei", ""], ["Wu", "Yuhuai", ""], ["Paulson", "Lawrence C.", ""]]}, {"id": "2006.09294", "submitter": "Xiang Fu", "authors": "Xiang Fu", "title": "CC-Light eQASM Architecture Specification", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is the specification of the CC-Light instantiation of\nexecutable QASM (eQASM), a quantum instruction set architecture (QISA)\ndeveloped in QuTech targeting to control a seven-qubit superconducting quantum\nprocessor. This document can serve as a reference manual for low-level\nprogrammers, compiler backend developers, and microarchitecture implementers of\neQASM. The design of CC-Light eQASM is under the Apache 2.0 License.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 07:55:07 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Fu", "Xiang", ""]]}, {"id": "2006.09616", "submitter": "Steven Lyubomirsky", "authors": "Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer Brennan,\n  Mike He, Jared Roesch, Tianqi Chen, and Zachary Tatlock", "title": "Dynamic Tensor Rematerialization", "comments": "31 pages, 12 figures, implementation available here:\n  https://github.com/uwsampl/dtr-prototype, OpenReview:\n  https://openreview.net/forum?id=Vfs_2RnOD0H", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Checkpointing enables the training of deep learning models under restricted\nmemory budgets by freeing intermediate activations from memory and recomputing\nthem on demand. Current checkpointing techniques statically plan these\nrecomputations offline and assume static computation graphs. We demonstrate\nthat a simple online algorithm can achieve comparable performance by\nintroducing Dynamic Tensor Rematerialization (DTR), a greedy online algorithm\nfor checkpointing that is extensible and general, is parameterized by eviction\npolicy, and supports dynamic models. We prove that DTR can train an $N$-layer\nlinear feedforward network on an $\\Omega(\\sqrt{N})$ memory budget with only\n$\\mathcal{O}(N)$ tensor operations. DTR closely matches the performance of\noptimal static checkpointing in simulated experiments. We incorporate a DTR\nprototype into PyTorch merely by interposing on tensor allocations and operator\ncalls and collecting lightweight metadata on tensors.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 02:49:59 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 03:02:50 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 21:32:29 GMT"}, {"version": "v4", "created": "Thu, 18 Mar 2021 06:20:23 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Kirisame", "Marisa", ""], ["Lyubomirsky", "Steven", ""], ["Haan", "Altan", ""], ["Brennan", "Jennifer", ""], ["He", "Mike", ""], ["Roesch", "Jared", ""], ["Chen", "Tianqi", ""], ["Tatlock", "Zachary", ""]]}, {"id": "2006.09973", "submitter": "Diego Elias Costa", "authors": "Diego Elias Costa, Suhaib Mujahid, Rabe Abdalkareem, Emad Shihab", "title": "Breaking Type Safety in Go: An Empirical Study on the Usage of the\n  unsafe Package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A decade after its first release, the Go programming language has become a\nmajor programming language in the development landscape. While praised for its\nclean syntax and C-like performance, Go also contains a strong static\ntype-system that prevents arbitrary type casting and arbitrary memory access,\nmaking the language type-safe by design. However, to give developers the\npossibility of implementing low-level code, Go ships with a special package\ncalled unsafe that offers developers a way around the type-safety of Go\nprograms. The package gives greater flexibility to developers but comes at a\nhigher risk of runtime errors, chances of non-portability, and the loss of\ncompatibility guarantees for future versions of Go.\n  In this paper, we present the first large-scale study on the usage of the\nunsafe package in 2,438 popular Go projects. Our investigation shows that\nunsafe is used in 24% of Go projects, motivated primarily by communicating with\noperating systems and C code, but is also commonly used as a source of\nperformance optimization. Developers are willing to use unsafe to break\nlanguage specifications (e.g., string immutability) for better performance and\n6% of analyzed projects that use unsafe perform risky pointer conversions that\ncan lead to program crashes and unexpected behavior. Furthermore, we report a\nseries of real issues faced by projects that use unsafe, from crashing errors\nand non-deterministic behavior to having their deployment restricted from\ncertain popular environments. Our findings can be used to understand how and\nwhy developers break type-safety in Go, and help motivate further tools and\nlanguage development that could make the usage of unsafe in Go even safer.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 16:38:40 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 03:57:28 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 01:34:11 GMT"}, {"version": "v4", "created": "Thu, 22 Jul 2021 18:51:48 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Costa", "Diego Elias", ""], ["Mujahid", "Suhaib", ""], ["Abdalkareem", "Rabe", ""], ["Shihab", "Emad", ""]]}, {"id": "2006.10226", "submitter": "Animesh Jain", "authors": "Animesh Jain, Shoubhik Bhattacharya, Masahiro Masuda, Vin Sharma and\n  Yida Wang", "title": "Efficient Execution of Quantized Deep Learning Models: A Compiler\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of applications implement predictive functions using deep\nlearning models, which require heavy use of compute and memory. One popular\ntechnique for increasing resource efficiency is 8-bit integer quantization, in\nwhich 32-bit floating point numbers (fp32) are represented using shorter 8-bit\ninteger numbers. Although deep learning frameworks such as TensorFlow, TFLite,\nMXNet, and PyTorch enable developers to quantize models with only a small drop\nin accuracy, they are not well suited to execute quantized models on a variety\nof hardware platforms. For example, TFLite is optimized to run inference on ARM\nCPU edge devices but it does not have efficient support for Intel CPUs and\nNvidia GPUs. In this paper, we address the challenges of executing quantized\ndeep learning models on diverse hardware platforms by proposing an augmented\ncompiler approach. A deep learning compiler such as Apache TVM can enable the\nefficient execution of model from various frameworks on various targets. Many\ndeep learning compilers today, however, are designed primarily for fp32\ncomputation and cannot optimize a pre-quantized INT8 model. To address this\nissue, we created a new dialect called Quantized Neural Network (QNN) that\nextends the compiler's internal representation with a quantization context.\nWith this quantization context, the compiler can generate efficient code for\npre-quantized models on various hardware platforms. As implemented in Apache\nTVM, we observe that the QNN-augmented deep learning compiler achieves speedups\nof 2.35x, 2.15x, 1.35x and 1.40x on Intel Xeon Cascade Lake CPUs, Nvidia Tesla\nT4 GPUs, ARM Raspberry Pi3 and Pi4 respectively against well optimized fp32\nexecution, and comparable performance to the state-of-the-art\nframework-specific solutions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 01:38:10 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Jain", "Animesh", ""], ["Bhattacharya", "Shoubhik", ""], ["Masuda", "Masahiro", ""], ["Sharma", "Vin", ""], ["Wang", "Yida", ""]]}, {"id": "2006.10604", "submitter": "Margherita Zorzi", "authors": "Davide Trotta and Margherita Zorzi", "title": "Compositional theories for embedded languages", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedded programming style allows to split the syntax in two parts,\nrepresenting respectively a host language H and a core language C embedded in\nH. This formally models several situations in which a user writes code in a\nmain language and delegates some tasks to an ad hoc domain specific language.\nMoreover, as showed in recent years, a particular case of the host-core\napproach allows a flexible management of data linearity, which is particularly\nuseful in non-classical computational paradigms such as quantum computing. The\ndefinition of a systematised type theory to capture and standardize common\nproperties of embedded languages is unexplored. The aim of this paper is to\npresent a flexible fragment of such a type theory, together with its\ncategorical semantics in terms of enriched categories, following previous\ninvestigations. We present the calculus HC0 and we use the notion of internal\nlanguage of a category to relate the language to the class of its models,\nshowing the equivalence between the category of models and the one of theories.\nThis provides a stronger result w.r.t. standard soundness and completeness\nsince it involves not only the models but also morphisms between models. We\nobserve that the definition of the morphisms between models highlights further\nadvantages of the embedded languages and we discuss some concrete instances,\nextensions and specializations of the syntax and the semantics.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 15:18:25 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 16:42:36 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Trotta", "Davide", ""], ["Zorzi", "Margherita", ""]]}, {"id": "2006.10720", "submitter": "Hossein Hajipour", "authors": "Hossein Hajipour, Mateusz Malinowski, Mario Fritz", "title": "IReEn: Iterative Reverse-Engineering of Black-Box Functions via Neural\n  Program Synthesis", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the problem of revealing the functionality of a\nblack-box agent. Notably, we are interested in the interpretable and formal\ndescription of the behavior of such an agent. Ideally, this description would\ntake the form of a program written in a high-level language. This task is also\nknown as reverse engineering and plays a pivotal role in software engineering,\ncomputer security, but also most recently in interpretability. In contrast to\nprior work, we do not rely on privileged information on the black box, but\nrather investigate the problem under a weaker assumption of having only access\nto inputs and outputs of the program. We approach this problem by iteratively\nrefining a candidate set using a generative neural program synthesis approach\nuntil we arrive at a functionally equivalent program. We assess the performance\nof our approach on the Karel dataset. Our results show that the proposed\napproach outperforms the state-of-the-art on this challenge by finding a\nfunctional equivalent program in 78% of cases -- even exceeding prior work that\nhad privileged information on the black-box.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:50:48 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Hajipour", "Hossein", ""], ["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""]]}, {"id": "2006.11479", "submitter": "Jongouk Choi", "authors": "Jongouk Choi, Qingrui Liu, Changhee Jung", "title": "Compiler Directed Speculative Intermittent Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents CoSpec, a new architecture/compiler co-design scheme that\nworks for commodity in-order processors used in energy-harvesting systems. To\nachieve crash consistency without requiring unconventional architectural\nsupport, CoSpec leverages speculation assuming that power failure is not going\nto occur and thus holds all committed stores in a store buffer (SB), as if they\nwere speculative, in case of mispeculation. CoSpec compiler first partitions a\ngiven program into a series of recoverable code regions with the SB size in\nmind, so that no region overflows the SB. When the program control reaches the\nend of each region, the speculation turns out to be successful, thus releasing\nall the buffered stores of the region to NVM. If power failure occurs during\nthe execution of a region, all its speculative stores disappear in the volatile\nSB, i.e., they never affect program states in NVM. Consequently, the\ninterrupted region can be restarted with consistent program states in the wake\nof power failure. To hide the latency of the SB release, i.e., essentially NVM\nwrites, at each region boundary, CoSpec overlaps the NVM writes of the current\nregion with the speculative execution of the next region. Such instruction\nlevel parallelism gives an illusion of out-of-order execution on top of the\nin-order processor, achieving a speedup of more than 1.2X when there is no\npower outage. Our experiments on a set of real energy harvesting traces with\nfrequent outages demonstrate that CoSpec outperforms the state-of-the-art\nscheme by 1.8~3X on average.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 02:50:12 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Choi", "Jongouk", ""], ["Liu", "Qingrui", ""], ["Jung", "Changhee", ""]]}, {"id": "2006.11639", "submitter": "Shu-Hung You", "authors": "Shu-Hung You (1), Robert Bruce Findler (1) and Christos Dimoulas (1)\n  ((1) PLT@Northwestern)", "title": "Dynamic Symbolic Execution of Higher-Order Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of concolic testing deteriorates as the size of programs\nincreases. A promising way out is to test programs modularly, e.g., on a per\nfunction or class basis. Alas, this idea hits a roadblock in modern programming\nlanguages In modern languages, components expect functions, objects, and even\nclasses as inputs. The crux of the problem is that existing concolic testing\ntechniques cannot faithfully capture the complex interactions between a\nhigher-order program and its inputs in order to distill it in a first-order\nformula that an SMT solver can work with. In this paper, we take the first step\ntowards solving the problem; we offer a design, semantics, and prototype for\nconcolic testing of higher-order functions. Inspired by work on higher-order\nsymbolic execution, our model constructs inputs for higher-order functions with\na canonical shape. This enables the concolic tester to keep track of which\npieces of the control-flow path of the higher-order function depend on the\nshape of its input and which do not. The concolic tester encodes the pieces\nthat do not depend on the shape of the input as a first-order formula.\nSubsequently, similar to a first-order concolic tester, it leverages an SMT\nsolver to produce another input with the same shape but that explores a\ndifferent control-flow path of the higher-order function. As a separate\ndimension, the concolic tester iteratively explores the canonical shapes for\nthe input and, investigating all the ways a higher-order function can interact\nwith its input, searching for bugs. To validate our design, we prove that if a\nhigher-order function has a bug, our concolic tester will eventually construct\nan input that triggers the bug. Using our design as a blueprint, we implement a\nprototype concolic tester and confirm that it discovers bugs in a variety of\nhigher-order programs from the literature.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 19:29:50 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["You", "Shu-Hung", "", "PLT@Northwestern"], ["Findler", "Robert Bruce", "", "PLT@Northwestern"], ["Dimoulas", "Christos", "", "PLT@Northwestern"]]}, {"id": "2006.12638", "submitter": "Ashish Tiwari", "authors": "Ashish Tiwari, Arjun Radhakrishna, Sumit Gulwani, and Daniel Perelman", "title": "Information-theoretic User Interaction: Significant Inputs for Program\n  Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming-by-example technologies are being deployed in industrial products\nfor real-time synthesis of various kinds of data transformations. These\ntechnologies rely on the user to provide few representative examples of the\ntransformation task. Motivated by the need to find the most pertinent question\nto ask the user, in this paper, we introduce the {\\em significant questions\nproblem}, and show that it is hard in general. We then develop an\ninformation-theoretic greedy approach for solving the problem. We justify the\ngreedy algorithm using the conditional entropy result, which informally says\nthat the question that achieves the maximum information gain is the one that we\nknow least about.\n  In the context of interactive program synthesis, we use the above result to\ndevelop an {\\em{active program learner}} that generates the significant inputs\nto pose as queries to the user in each iteration. The procedure requires\nextending a {\\em{passive program learner}} to a {\\em{sampling program learner}}\nthat is able to sample candidate programs from the set of all consistent\nprograms to enable estimation of information gain. It also uses clustering of\ninputs based on features in the inputs and the corresponding outputs to sample\na small set of candidate significant inputs. Our active learner is able to\ntradeoff false negatives for false positives and converge in a small number of\niterations on a real-world dataset of %around 800 string transformation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 21:46:40 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Tiwari", "Ashish", ""], ["Radhakrishna", "Arjun", ""], ["Gulwani", "Sumit", ""], ["Perelman", "Daniel", ""]]}, {"id": "2006.12641", "submitter": "Saurabh Pujar", "authors": "Luca Buratti, Saurabh Pujar, Mihaela Bornea, Scott McCarley, Yunhui\n  Zheng, Gaetano Rossiello, Alessandro Morari, Jim Laredo, Veronika Thost,\n  Yufan Zhuang, Giacomo Domeniconi", "title": "Exploring Software Naturalness through Neural Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Software Naturalness hypothesis argues that programming languages can be\nunderstood through the same techniques used in natural language processing. We\nexplore this hypothesis through the use of a pre-trained transformer-based\nlanguage model to perform code analysis tasks. Present approaches to code\nanalysis depend heavily on features derived from the Abstract Syntax Tree (AST)\nwhile our transformer-based language models work on raw source code. This work\nis the first to investigate whether such language models can discover AST\nfeatures automatically. To achieve this, we introduce a sequence labeling task\nthat directly probes the language models understanding of AST. Our results show\nthat transformer based language models achieve high accuracy in the AST tagging\ntask. Furthermore, we evaluate our model on a software vulnerability\nidentification task. Importantly, we show that our approach obtains\nvulnerability identification results comparable to graph based approaches that\nrely heavily on compilers for feature extraction.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 21:56:14 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 13:55:50 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Buratti", "Luca", ""], ["Pujar", "Saurabh", ""], ["Bornea", "Mihaela", ""], ["McCarley", "Scott", ""], ["Zheng", "Yunhui", ""], ["Rossiello", "Gaetano", ""], ["Morari", "Alessandro", ""], ["Laredo", "Jim", ""], ["Thost", "Veronika", ""], ["Zhuang", "Yufan", ""], ["Domeniconi", "Giacomo", ""]]}, {"id": "2006.12645", "submitter": "Vinod Grover", "authors": "Somashekaracharya G. Bhaskaracharya, Julien Demouth, Vinod Grover", "title": "Automatic Kernel Generation for Volta Tensor Cores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A commonly occurring computation idiom in neural networks is to perform some\npointwise operations on the result of a matrix multiplication. Such a sequence\nof operations is typically represented as a computation graph in deep learning\ncompilers. When compiling to a GPU target, these computations can be\nindividually mapped to manually tuned implementations provided by libraries\nsuch as cuBLAS and cuDNN. These libraries also provide off-the-shelf support\nfor targeting tensor cores in NVIDIA GPUs, which can lead to huge performance\nboosts through their specialized support for mixed-precision matrix math.\nAlternatively, tensor cores can be programmed directly using CUDA APIs or\ninline assembly instructions, which opens up the possibility of generating\nefficient CUDA kernels automatically for such computations.\n  Automatic kernel generation is particularly crucial when it is beneficial to\ngenerate efficient code for an entire computation graph by fusing several\noperations into a single device function instead of invoking a separate kernel\nfor each of them. Polyhedral compilation techniques provide a systematic\napproach for the analysis and transformation of a sequence of affine\nloop-nests. In this paper, we describe a polyhedral approach to generate\nefficient CUDA kernels for matrix multiplication using inline assembly\ninstructions for programming tensor cores on NVIDIA Volta GPUs. Furthermore, we\nbuild on this approach to generate fused kernels for computation sequences\ninvolving matrix multiplication and pointwise operations such as bias addition,\nReLU activation etc. Experimental evaluation of these techniques show that\nautomatically generated kernels can provide significantly better performance\nthan manually tuned library implementations, with speedups ranging up to 2.55X.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 22:16:00 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 22:20:24 GMT"}, {"version": "v3", "created": "Sat, 1 Aug 2020 21:41:41 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Bhaskaracharya", "Somashekaracharya G.", ""], ["Demouth", "Julien", ""], ["Grover", "Vinod", ""]]}, {"id": "2006.13635", "submitter": "Dan Frumin", "authors": "Dan Frumin, Robbert Krebbers, Lars Birkedal", "title": "ReLoC Reloaded: A Mechanized Relational Logic for Fine-Grained\n  Concurrency and Logical Atomicity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new version of ReLoC: a relational separation logic for proving\nrefinements of programs with higher-order state, fine-grained concurrency,\npolymorphism and recursive types. The core of ReLoC is its refinement judgment\n$e \\precsim e' : \\tau$, which states that a program $e$ refines a program $e'$\nat type $\\tau$. ReLoC provides type-directed structural rules and symbolic\nexecution rules in separation-logic style for manipulating the judgment,\nwhereas in prior work on refinements for languages with higher-order state and\nconcurrency, such proofs were carried out by unfolding the judgment into its\ndefinition in the model. ReLoC's abstract proof rules make it simpler to carry\nout refinement proofs, and enable us to generalize the notion of logically\natomic specifications to the relational case, which we call logically atomic\nrelational specifications.\n  We build ReLoC on top of the Iris framework for separation logic in Coq,\nallowing us to leverage features of Iris to prove soundness of ReLoC, and to\ncarry out refinement proofs in ReLoC. We implement tactics for interactive\nproofs in ReLoC, allowing us to mechanize several case studies in Coq, and\nthereby demonstrate the practicality of ReLoC.\n  ReLoC Reloaded extends ReLoC (LICS'18) with various technical improvements, a\nnew Coq mechanization, and support for Iris's prophecy variables. The latter\nallows us to carry out refinement proofs that involve reasoning about the\nprogram's future. We also expand ReLoC's notion of logically atomic relational\nspecifications with a new flavor based on the HOCAP pattern by Svendsen et al.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 11:15:20 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 17:40:07 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 16:55:56 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Frumin", "Dan", ""], ["Krebbers", "Robbert", ""], ["Birkedal", "Lars", ""]]}, {"id": "2006.13652", "submitter": "Peter Hegedus Dr", "authors": "G\\'abor Antal, M\\'arton Keleti, P\\'eter Heged\\H{u}s", "title": "Exploring the Security Awareness of the Python and JavaScript Open\n  Source Communities", "comments": "17th International Conference on Mining Software Repositories", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software security is undoubtedly a major concern in today's software\nengineering. Although the level of awareness of security issues is often high,\npractical experiences show that neither preventive actions nor reactions to\npossible issues are always addressed properly in reality. By analyzing large\nquantities of commits in the open-source communities, we can categorize the\nvulnerabilities mitigated by the developers and study their distribution,\nresolution time, etc. to learn and improve security management processes and\npractices. With the help of the Software Heritage Graph Dataset, we\ninvestigated the commits of two of the most popular script languages -- Python\nand JavaScript -- projects collected from public repositories and identified\nthose that mitigate a certain vulnerability in the code (i.e. vulnerability\nresolution commits). On the one hand, we identified the types of\nvulnerabilities (in terms of CWE groups) referred to in commit messages and\ncompared their numbers within the two communities. On the other hand, we\nexamined the average time elapsing between the publish date of a vulnerability\nand the first reference to it in a commit. We found that there is a large\nintersection in the vulnerability types mitigated by the two communities, but\nmost prevalent vulnerabilities are specific to language. Moreover, neither the\nJavaScript nor the Python community reacts very fast to appearing security\nvulnerabilities in general with only a couple of exceptions for certain CWE\ngroups.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 11:59:36 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Antal", "G\u00e1bor", ""], ["Keleti", "M\u00e1rton", ""], ["Heged\u0171s", "P\u00e9ter", ""]]}, {"id": "2006.14010", "submitter": "David Kahn", "authors": "Di Wang, David M Kahn, Jan Hoffmann", "title": "Raising Expectations: Automating Expected Cost Analysis with Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a type-based analysis for deriving upper bounds on the\nexpected execution cost of probabilistic programs. The analysis is naturally\ncompositional, parametric in the cost model, and supports higher order\nfunctions and inductive data types. The derived bounds are multivariate\npolynomials that are functions of data structures. Bound inference is enabled\nby local type rules that reduce type inference to linear constraint solving.\nThe type system is based on the potential method of amortized analysis and\nextends automatic amortized resource analysis (AARA) for deterministic\nprograms. A main innovation is that bounds can contain symbolic probabilities,\nwhich may appear in data structures and function arguments. Another\ncontribution is a novel soundness proof that establishes the correctness of the\nderived bounds with respect to a distribution-based operational cost semantics\nthat also includes nontrivial diverging behavior. For cost models like time,\nderived bounds imply termination with probability one. To highlight the novel\nideas, the presentation focuses on linear potential and a core language.\nHowever, the analysis is implemented as an extension of Resource Aware ML and\nsupports polynomial bounds and user defined data structures. The effectiveness\nof the technique is evaluated by analyzing the sample complexity of discrete\ndistributions and with a novel average-case estimation for deterministic\nprograms that combines expected cost analysis with statistical methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 19:27:24 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 03:52:07 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Wang", "Di", ""], ["Kahn", "David M", ""], ["Hoffmann", "Jan", ""]]}, {"id": "2006.14476", "submitter": "Alberto Sim\\~oes", "authors": "Alberto Sim\\~oes and Ricardo Queir\\'os", "title": "On the Nature of Programming Exercises", "comments": null, "journal-ref": null, "doi": "10.4230/OASIcs.ICPEC.2020.24", "report-no": null, "categories": "cs.CY cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are countless reasons cited in scientific studies to explain the\ndifficulties in programming learning. The reasons range from the subject's\ncomplexity, the ineffective teaching and study methods, to psychological\naspects such as demotivation. Still, learning programming often boils down to\npractice on exercise solving. Hence, it is essential to understand that the\nnature of a programming exercise is an important factor for the success and\nconsistent learning.\n  This paper explores different approaches on the creation of a programming\nexercise, starting with realizing how it is currently formalized, presented and\nevaluated. From there, authors suggest variations that seek to broaden the way\nan exercise is solved and, with this diversity, increase student engagement and\nlearning outcome. The several types of exercises presented can use gamification\ntechniques fostering student motivation. To contextualize the student with his\npeers, we finish presenting metrics that can be obtained by existing automatic\nassessment tools.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 15:22:26 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Sim\u00f5es", "Alberto", ""], ["Queir\u00f3s", "Ricardo", ""]]}, {"id": "2006.14969", "submitter": "Matteo Busi", "authors": "Carmine Abate and Matteo Busi and Stelios Tsampas", "title": "Fully Abstract and Robust Compilation and How to Reconcile the Two,\n  Abstractly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most prominent formal criterion for secure compilation is full\nabstraction, the preservation and reflection of contextual equivalence. Recent\nwork introduced robust compilation, defined as the preservation of robust\nsatisfaction of hyperproperties, i.e., their satisfaction against arbitrary\nattackers. In this paper, we initially set out to compare these two approaches\nto secure compilation. To that end, we provide an exact description of the\nhyperproperties that are robustly satisfied by programs compiled with a fully\nabstract compiler, and show that they can be meaningless or trivial. We then\npropose a novel criterion for secure compilation formulated in the framework of\nMathematical Operational Semantics (MOS), guaranteeing both full abstraction\nand the preservation of robust satisfaction of hyperproperties in a more\nsensible manner.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 13:15:35 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 16:29:29 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 22:16:01 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Abate", "Carmine", ""], ["Busi", "Matteo", ""], ["Tsampas", "Stelios", ""]]}, {"id": "2006.15036", "submitter": "Norman Danner", "authors": "Joseph W. Cutler, Daniel R. Licata, Norman Danner", "title": "Denotational recurrence extraction for amortized analysis", "comments": "To appear in ICFP 2020; formatting changes", "journal-ref": "Proc. ACM Program. Lang. 4, ICFP, Article 97 (August 2020)", "doi": "10.1145/3408979", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A typical way of analyzing the time complexity of functional programs is to\nextract a recurrence expressing the running time of the program in terms of the\nsize of its input, and then to solve the recurrence to obtain a big-O bound.\nFor recurrence extraction to be compositional, it is also necessary to extract\nrecurrences for the size of outputs of helper functions. Previous work has\ndeveloped techniques for using logical relations to state a formal correctness\ntheorem for a general recurrence extraction translation: a program is bounded\nby a recurrence when the operational cost is bounded by the extracted cost, and\nthe output value is bounded, according to a value bounding relation defined by\ninduction on types, by the extracted size. This previous work supports\nhigher-order functions by viewing recurrences as programs in a lambda-calculus,\nor as mathematical entities in a denotational semantics thereof. In this paper,\nwe extend these techniques to support amortized analysis, where costs are\nrearranged from one portion of a program to another to achieve more precise\nbounds. We give an intermediate language in which programs can be annotated\naccording to the banker's method of amortized analysis; this language has an\naffine type system to ensure credits are not spent more than once. We give a\nrecurrence extraction translation of this language into a recurrence language,\na simply-typed lambda-calculus with a cost type, and state and prove a bounding\nlogical relation expressing the correctness of this translation. The recurrence\nlanguage has a denotational semantics in preorders, and we use this semantics\nto solve recurrences, e.g analyzing binary counters and splay trees.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 15:05:57 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 14:34:17 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Cutler", "Joseph W.", ""], ["Licata", "Daniel R.", ""], ["Danner", "Norman", ""]]}, {"id": "2006.16233", "submitter": "Tristan Knoth", "authors": "Tristan Knoth, Di Wang, Adam Reynolds, Jan Hoffmann, and Nadia\n  Polikarpova", "title": "Liquid Resource Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents liquid resource types, a technique for automatically\nverifying the resource consumption of functional programs. Existing resource\nanalysis techniques trade automation for flexibility -- automated techniques\nare restricted to relatively constrained families of resource bounds, while\nmore expressive proof techniques admitting value-dependent bounds rely on\nhandwritten proofs. Liquid resource types combine the best of these approaches,\nusing logical refinements to automatically prove precise bounds on a program's\nresource consumption. The type system augments refinement types with potential\nannotations to conduct an amortized resource analysis. Importantly, users can\nannotate data structure declarations to indicate how potential is allocated\nwithin the type, allowing the system to express bounds with polynomials and\nexponentials, as well as more precise expressions depending on program values.\nWe prove the soundness of the type system, provide a library of flexible and\nreusable data structures for conducting resource analysis, and use our\nprototype implementation to automatically verify resource bounds that\npreviously required a manual proof.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 17:54:24 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 21:50:26 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Knoth", "Tristan", ""], ["Wang", "Di", ""], ["Reynolds", "Adam", ""], ["Hoffmann", "Jan", ""], ["Polikarpova", "Nadia", ""]]}, {"id": "2006.16601", "submitter": "Andrea Fioraldi", "authors": "Andrea Fioraldi", "title": "Symbolic Execution and Debugging Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this thesis, we introduce the idea of combining symbolic execution with\ndynamic analysis for reverse engineering. Differently from DSE, we devise an\napproach where the reverse engineer can use a debugger to drive and inspect a\nconcrete execution engine of the application code and then, when needed,\ntransfer the execution into a symbolic executor in order to automatically\nidentify the input values required to reach a target point in the code. After\nthat, the user can also transfer back the correct input values found with\nsymbolic execution in order to continue the debugging. The synchronization\nbetween a debugger and a symbolic executor can enhance manual dynamic analysis\nand allow a reverser to easily solve small portions of code without leaving the\ndebugger. We implemented a synchronization mechanism on top of the binary\nanalysis framework angr, allowing for transferring the state of the debugged\nprocess to the angr environment and back. The backend library is debugger\nagnostic and can be extended to work with various frontends. We implemented a\nfrontend for the IDA Pro debugger and one for the GNU Debugger, which are both\nwidely popular among reverse engineers.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 08:29:55 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Fioraldi", "Andrea", ""]]}, {"id": "2006.16743", "submitter": "Pengyu Nie", "authors": "Pengyu Nie, Karl Palmskog, Junyi Jessy Li, Milos Gligoric", "title": "Learning to Format Coq Code Using Language Models", "comments": "Accepted in the Coq Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Should the final right bracket in a record declaration be on a separate line?\nShould arguments to the rewrite tactic be separated by a single space? Coq code\ntends to be written in distinct manners by different people and teams. The\nexpressiveness, flexibility, and extensibility of Coq's languages and notations\nmeans that Coq projects have a wide variety of recognizable coding styles,\nsometimes explicitly documented as conventions on naming and formatting. In\nparticular, even inexperienced users can distinguish vernacular using the\nstandard library and plain Ltac from idiomatic vernacular using the\nMathematical Components (MathComp) library and SSReflect.\n  While coding conventions are important for comprehension and maintenance,\nthey are costly to document and enforce. Rule-based formatters, such as Coq's\nbeautifier, have limited flexibility and only capture small fractions of\ndesired conventions in large verification projects. We believe that application\nof language models - a class of Natural Language Processing (NLP) techniques\nfor capturing regularities in corpora - can provide a solution to this\nconundrum. More specifically, we believe that an approach based on\nautomatically learning conventions from existing Coq code, and then suggesting\nidiomatic code to users in the proper context, can be superior to manual\napproaches and static analysis tools - both in terms of effort and results.\n  As a first step, we here outline initial models to learn and suggest space\nformatting in Coq files, with a preliminary implementation for Coq 8.10, and\nevaluated on a corpus based on MathComp 1.9.0 which comprises 164k lines of Coq\ncode from four core projects.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 14:46:15 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Nie", "Pengyu", ""], ["Palmskog", "Karl", ""], ["Li", "Junyi Jessy", ""], ["Gligoric", "Milos", ""]]}]