[{"id": "1912.00429", "submitter": "Jyoti Prakash", "authors": "Jyoti Prakash, Abhishek Tiwari, Christian Hammer", "title": "PointEval: On the Impact of Pointer Analysis Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pointer analysis is a foundational analysis leveraged by various static\nanalyses. Therefore, it gathered wide attention in research for decades. Some\npointer analysis frameworks are based on succinct declarative specifications.\nHowever, these tools are heterogeneous in terms of the underlying intermediate\nrepresentation (IR), heap abstraction, and programming methodology. This\nsituation complicates a fair comparison of these frameworks and thus hinders\nfurther research. Consequently, the literature lacks an evaluation of the\nstrengths and weaknesses of these tools.\n  In this work, we evaluate two major frameworks for pointer analysis, WALA and\nDoop, on the DaCapo set of benchmarks. We compare the pointer analyses\navailable in Wala and Doop, and conclude that---even though based on a\ndeclarative specification---Doop provides a better pointer analysis than Wala\nin terms of precision and scalability. We also compare the two IRs used in\nDoop, i.e., Jimple from the Soot framework and IR from the Wala framework. Our\nevaluation shows that in the majority of the benchmarks Soot's IR gives a more\nprecise and scalable pointer analysis. Finally, we propose a micro-benchmark\n\\emph{PointerBench}, for which we manually validate the points-to statistics to\nevaluate the results of these tools.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2019 15:34:31 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Prakash", "Jyoti", ""], ["Tiwari", "Abhishek", ""], ["Hammer", "Christian", ""]]}, {"id": "1912.00781", "submitter": "Mircea-Dan Hernest", "authors": "Dan Hernest", "title": "Experiments with a PCCoder extension", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in synthesis of programs written in some Domain Specific\nLanguage (DSL) by means of neural networks from a limited set of inputs-output\ncorrespondences such as DeepCoder and its PCCoder reimplementation/optimization\nproved the efficiency of this kind of approach to automatic program generation\nin a DSL language that although limited in scope is universal in the sense that\nprograms can be translated to basically any programming language. We experiment\nwith the extension of the DSL of DeepCoder/PCCoder with symbols IFI and IFL\nwhich denote functional expressions of the If ramification (test) instruction\nfor types Int and List. We notice an increase (doubling) of the size of the\ntraining set, the number of parameters of the trained neural network and of the\ntime spent looking for the program synthesized from limited sets of\ninputs-output correspondences. The result is positive in the sense of\npreserving the accuracy of applying synthesis on randomly generated test sets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 15:57:33 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 05:46:01 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Hernest", "Dan", ""]]}, {"id": "1912.00867", "submitter": "Fredrik Dahlqvist", "authors": "Fredrik Dahlqvist and Rocco Salvia and George A Constantinides", "title": "A Probabilistic Approach to Floating-Point Arithmetic", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite-precision floating point arithmetic unavoidably introduces rounding\nerrors which are traditionally bounded using a worst-case analysis. However,\nworst-case analysis might be overly conservative because worst-case errors can\nbe extremely rare events in practice. Here we develop a probabilistic model of\nrounding errors with which it becomes possible to estimate the likelihood that\nthe rounding error of an algorithm lies within a given interval. Given an input\ndistribution, we show how to compute the distribution of rounding errors. We do\nthis exactly for low precision arithmetic, for high precision arithmetic we\nderive a simple approximation. The model is then entirely compositional: given\na numerical program written in a simple imperative programming language we can\nrecursively compute the distribution of rounding errors at each step of the\ncomputation and propagate it through each program instruction. This is done by\napplying a formalism originally developed by Kozen to formalize the semantics\nof probabilistic programs. We then discuss an implementation of the model and\nuse it to perform probabilistic range analyses on some benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 15:32:57 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 10:09:17 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Dahlqvist", "Fredrik", ""], ["Salvia", "Rocco", ""], ["Constantinides", "George A", ""]]}, {"id": "1912.00981", "submitter": "Samuel Drews", "authors": "Samuel Drews and Aws Albarghouthi and Loris D'Antoni", "title": "Proving Data-Poisoning Robustness in Decision Trees", "comments": "Changes: revisions to main text for clarity of presentation, and\n  corrections to proofs in the appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are brittle, and small changes in the training data\ncan result in different predictions. We study the problem of proving that a\nprediction is robust to data poisoning, where an attacker can inject a number\nof malicious elements into the training set to influence the learned model. We\ntarget decision-tree models, a popular and simple class of machine learning\nmodels that underlies many complex learning techniques. We present a sound\nverification technique based on abstract interpretation and implement it in a\ntool called Antidote. Antidote abstractly trains decision trees for an\nintractably large space of possible poisoned datasets. Due to the soundness of\nour abstraction, Antidote can produce proofs that, for a given input, the\ncorresponding prediction would not have changed had the training set been\ntampered with or not. We demonstrate the effectiveness of Antidote on a number\nof popular datasets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 18:20:54 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 22:40:42 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Drews", "Samuel", ""], ["Albarghouthi", "Aws", ""], ["D'Antoni", "Loris", ""]]}, {"id": "1912.01289", "submitter": "Francesco Tiezzi", "authors": "Rocco De Nicola, Gianluigi Ferrari, Rosario Pugliese, Francesco Tiezzi", "title": "A Formal Approach to the Engineering of Domain-Specific Distributed\n  Systems", "comments": "In Press", "journal-ref": "Journal of Logical and Algebraic Methods in Programming, Elsevier,\n  2019", "doi": "10.1016/j.jlamp.2019.100511", "report-no": null, "categories": "cs.PL cs.FL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review some results regarding specification, programming and verification\nof different classes of distributed systems which stemmed from the research of\nthe Concurrency and Mobility Group at University of Firenze. More specifically,\nwe examine the distinguishing features of network-aware programming,\nservice-oriented computing, autonomic computing, and collective adaptive\nsystems programming. We then present an overview of four different languages,\nnamely Klaim, Cows, Scel and AbC. For each language, we discuss design choices,\npresent syntax and semantics, show how the different formalisms can be used to\nmodel and program a travel booking scenario, and describe programming\nenvironments and verification techniques.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 10:45:53 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["De Nicola", "Rocco", ""], ["Ferrari", "Gianluigi", ""], ["Pugliese", "Rosario", ""], ["Tiezzi", "Francesco", ""]]}, {"id": "1912.01914", "submitter": "Daniel Ventura", "authors": "Sandra Alves and Delia Kesner and Daniel Ventura", "title": "A Quantitative Understanding of Pattern Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper shows that the recent approach to quantitative typing systems for\nprogramming languages can be extended to pattern matching features. Indeed, we\ndefine two resource aware type systems, named U and E, for a lambda-calculus\nequipped with pairs for both patterns and terms. Our typing systems borrow some\nbasic ideas from [BKRDR15], which characterises (head) normalisation in a\nqualitative way, in the sense that typability and normalisation coincide. But\nin contrast to [BKRDR15], our (static) systems also provides quantitative\ninformation about the dynamics of the calculus. Indeed, system U provides upper\nbounds for the length of normalisation sequences plus the size of their\ncorresponding normal forms, while system E, which can be seen as a refinement\nof system U, produces exact bounds for each of them. This is achieved by means\nof a non-idempotent intersection type system equipped with different technical\ntools. First of all, we use product types to type pairs, instead of the\ndisjoint unions in [BKRDR15], thus avoiding an overlap between \"being a pair\"\nand \"being duplicable\", resulting in an essential tool to reason about\nquantitativity. Secondly, typing sequents in system E are decorated with tuples\nof integers, which provide quantitative information about normalisation\nsequences, notably time (c.f. length) and space (c.f. size). Another key tool\nof system E is that the type system distinguishes between consuming\n(contributing to time) and persistent (contributing to space) constructors.\nMoreover, the time resource information is remarkably refined, because it\ndiscriminates between different kinds of reduction steps performed during\nevaluation, so that beta reduction, substitution and matching steps are counted\nseparately.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 11:54:15 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Alves", "Sandra", ""], ["Kesner", "Delia", ""], ["Ventura", "Daniel", ""]]}, {"id": "1912.02211", "submitter": "Abhishek Kr Singh", "authors": "Abhishek Kr Singh and Raja Natarajan", "title": "A Constructive Formalization of the Weak Perfect Graph Theorem", "comments": "The 9th ACM SIGPLAN International Conference on Certified Programs\n  and Proofs (CPP 2020)", "journal-ref": null, "doi": "10.1145/3372885.3373819", "report-no": null, "categories": "cs.LO cs.DM cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Perfect Graph Theorems are important results in graph theory describing\nthe relationship between clique number $\\omega(G) $ and chromatic number\n$\\chi(G) $ of a graph $G$. A graph $G$ is called \\emph{perfect} if\n$\\chi(H)=\\omega(H)$ for every induced subgraph $H$ of $G$. The Strong Perfect\nGraph Theorem (SPGT) states that a graph is perfect if and only if it does not\ncontain an odd hole (or an odd anti-hole) as its induced subgraph. The Weak\nPerfect Graph Theorem (WPGT) states that a graph is perfect if and only if its\ncomplement is perfect. In this paper, we present a formal framework for working\nwith finite simple graphs. We model finite simple graphs in the Coq Proof\nAssistant by representing its vertices as a finite set over a countably\ninfinite domain. We argue that this approach provides a formal framework in\nwhich it is convenient to work with different types of graph constructions (or\nexpansions) involved in the proof of the Lov\\'{a}sz Replication Lemma (LRL),\nwhich is also the key result used in the proof of Weak Perfect Graph Theorem.\nFinally, we use this setting to develop a constructive formalization of the\nWeak Perfect Graph Theorem.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 19:03:53 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Singh", "Abhishek Kr", ""], ["Natarajan", "Raja", ""]]}, {"id": "1912.02250", "submitter": "Kesha Hietala", "authors": "Kesha Hietala, Robert Rand, Shih-Han Hung, Xiaodi Wu, Michael Hicks", "title": "A Verified Optimizer for Quantum Circuits", "comments": "This paper supercedes arXiv:1904.06319; version 2 includes additional\n  results and improved formatting; version 3 is the final draft with additional\n  formatting improvements and some restructuring", "journal-ref": null, "doi": "10.1145/3434318", "report-no": null, "categories": "cs.PL cs.ET cs.LO quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present VOQC, the first fully verified optimizer for quantum circuits,\nwritten using the Coq proof assistant. Quantum circuits are expressed as\nprograms in a simple, low-level language called SQIR, a simple quantum\nintermediate representation, which is deeply embedded in Coq. Optimizations and\nother transformations are expressed as Coq functions, which are proved correct\nwith respect to a semantics of SQIR programs. SQIR uses a semantics of matrices\nof complex numbers, which is the standard for quantum computation, but treats\nmatrices symbolically in order to reason about programs that use an arbitrary\nnumber of quantum bits. SQIR's careful design and our provided automation make\nit possible to write and verify a broad range of optimizations in VOQC,\nincluding full-circuit transformations from cutting-edge optimizers.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 21:07:00 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2020 16:54:52 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 02:45:31 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Hietala", "Kesha", ""], ["Rand", "Robert", ""], ["Hung", "Shih-Han", ""], ["Wu", "Xiaodi", ""], ["Hicks", "Michael", ""]]}, {"id": "1912.02499", "submitter": "Caterina Urban", "authors": "Caterina Urban, Maria Christakis, Valentin W\\\"ustholz, Fuyuan Zhang", "title": "Perfectly Parallel Fairness Certification of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CY cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there is growing concern that machine-learning models, which\ncurrently assist or even automate decision making, reproduce, and in the worst\ncase reinforce, bias of the training data. The development of tools and\ntechniques for certifying fairness of these models or describing their biased\nbehavior is, therefore, critical. In this paper, we propose a perfectly\nparallel static analysis for certifying causal fairness of feed-forward neural\nnetworks used for classification of tabular data. When certification succeeds,\nour approach provides definite guarantees, otherwise, it describes and\nquantifies the biased behavior. We design the analysis to be sound, in practice\nalso exact, and configurable in terms of scalability and precision, thereby\nenabling pay-as-you-go certification. We implement our approach in an\nopen-source tool and demonstrate its effectiveness on models trained with\npopular datasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 10:59:28 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 13:31:02 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Urban", "Caterina", ""], ["Christakis", "Maria", ""], ["W\u00fcstholz", "Valentin", ""], ["Zhang", "Fuyuan", ""]]}, {"id": "1912.02951", "submitter": "Suhabe Bugrara", "authors": "Suhabe Bugrara", "title": "User Experience with Language-Independent Formal Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to help mainstream programmers routinely use formal\nverification on their smart contracts by 1) proposing a new YAML-format for\nwriting general-purpose formal specifications, 2) demonstrating how a formal\nspecification can be incrementally built up without needing advanced training,\nand 3) showing how formal specifications can be tested by using program\nmutation.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 02:08:07 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Bugrara", "Suhabe", ""]]}, {"id": "1912.03584", "submitter": "Stacy Patterson", "authors": "Matthew Obetz, Stacy Patterson, Ana Milanova", "title": "Formalizing Event-Driven Behavior of Serverless Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new operational semantics for serverless computing that model the\nevent-driven relationships between serverless functions, as well as their\ninteraction with platforms services such as databases and object stores. These\nsemantics precisely encapsulate how control transfers between functions, both\ndirectly and through reads and writes to platform services. We use these\nsemantics to define the notion of the service call graph for serverless\napplications that captures program flows through functions and services.\nFinally, we construct service call graphs for twelve serverless JavaScript\napplications, using a prototype of our call graph construction algorithm, and\nwe evaluate their accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 01:05:17 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Obetz", "Matthew", ""], ["Patterson", "Stacy", ""], ["Milanova", "Ana", ""]]}, {"id": "1912.03854", "submitter": "Ramy Shahin", "authors": "Ramy Shahin, Marsha Chechik", "title": "Variability-aware Datalog", "comments": "PADL'20 paper", "journal-ref": null, "doi": "10.1007/978-3-030-39197-3_14", "report-no": null, "categories": "cs.PL cs.DB cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variability-aware computing is the efficient application of programs to\ndifferent sets of inputs that exhibit some variability. One example is program\nanalyses applied to Software Product Lines (SPLs). In this paper we present the\ndesign and development of a variability-aware version of the Souffl\\'{e}\nDatalog engine. The engine can take facts annotated with Presence Conditions\n(PCs) as input, and compute the PCs of its inferred facts, eliminating facts\nthat do not exist in any valid configuration. We evaluate our variability-aware\nSouffl\\'{e} implementation on several fact sets annotated with PCs to measure\nthe associated overhead in terms of processing time and database size.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 05:05:07 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Shahin", "Ramy", ""], ["Chechik", "Marsha", ""]]}, {"id": "1912.04719", "submitter": "Michael Coblenz", "authors": "Michael Coblenz, Gauri Kambhatla, Paulette Koronkevich, Jenna L. Wise,\n  Celeste Barnaby, Joshua Sunshine, Jonathan Aldrich, Brad A. Myers", "title": "PLIERS: A Process that Integrates User-Centered Methods into Programming\n  Language Design", "comments": "50 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming language design requires making many usability-related design\ndecisions. However, existing HCI methods can be impractical to apply to\nprogramming languages: they have high iteration costs, programmers require\nsignificant learning time, and user performance has high variance. To address\nthese problems, we adapted both formative and summative HCI methods to make\nthem more suitable for programming language design. We integrated these methods\ninto a new process, PLIERS, for designing programming languages in a\nuser-centered way. We evaluated PLIERS by using it to design two new\nprogramming languages. Glacier extends Java to enable programmers to express\nimmutability properties effectively and easily. Obsidian is a language for\nblockchains that includes verification of critical safety properties. Summative\nusability studies showed that programmers were able to program effectively in\nboth languages after short training periods.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 14:45:18 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 21:19:57 GMT"}, {"version": "v3", "created": "Thu, 6 Feb 2020 04:07:39 GMT"}, {"version": "v4", "created": "Tue, 25 Aug 2020 17:24:49 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Coblenz", "Michael", ""], ["Kambhatla", "Gauri", ""], ["Koronkevich", "Paulette", ""], ["Wise", "Jenna L.", ""], ["Barnaby", "Celeste", ""], ["Sunshine", "Joshua", ""], ["Aldrich", "Jonathan", ""], ["Myers", "Brad A.", ""]]}, {"id": "1912.05036", "submitter": "Nico Reissmann", "authors": "Nico Reissmann, Jan Christian Meyer, Helge Bahmann, Magnus Sj\\\"alander", "title": "RVSDG: An Intermediate Representation for Optimizing Compilers", "comments": null, "journal-ref": null, "doi": "10.1145/3391902", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Intermediate Representations (IRs) are central to optimizing compilers as the\nway the program is represented may enhance or limit analyses and\ntransformations. Suitable IRs focus on exposing the most relevant information\nand establish invariants that different compiler passes can rely on. While\ncontrol-flow centric IRs appear to be a natural fit for imperative programming\nlanguages, analyses required by compilers have increasingly shifted to\nunderstand data dependencies and work at multiple abstraction layers at the\nsame time. This is partially evidenced in recent developments such as the MLIR\nproposed by Google. However, rigorous use of data flow centric IRs in general\npurpose compilers has not been evaluated for feasibility and usability as\nprevious works provide no practical implementations. We present the\nRegionalized Value State Dependence Graph (RVSDG) IR for optimizing compilers.\nThe RVSDG is a data flow centric IR where nodes represent computations, edges\nrepresent computational dependencies, and regions capture the hierarchical\nstructure of programs. It represents programs in demand-dependence form,\nimplicitly supports structured control flow, and models entire programs within\na single IR. We provide a complete specification of the RVSDG, construction and\ndestruction methods, as well as exemplify its utility by presenting Dead Node\nand Common Node Elimination optimizations. We implemented a prototype compiler\nand evaluate it in terms of performance, code size, compilation time, and\nrepresentational overhead. Our results indicate that the RVSDG can serve as a\ncompetitive IR in optimizing compilers while reducing complexity.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 22:43:19 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 22:27:27 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Reissmann", "Nico", ""], ["Meyer", "Jan Christian", ""], ["Bahmann", "Helge", ""], ["Sj\u00e4lander", "Magnus", ""]]}, {"id": "1912.05234", "submitter": "Artjoms Sinkarovs PhD", "authors": "Artjoms \\v{S}inkarovs, Hans-Nikolai Vie{\\ss}mann, Sven-Bodo Scholz", "title": "Array Languages Make Neural Networks Fast", "comments": null, "journal-ref": null, "doi": "10.1145/3460944.3464312", "report-no": null, "categories": "cs.PL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern machine learning frameworks are complex: they are typically organised\nin multiple layers each of which is written in a different language and they\ndepend on a number of external libraries, but at their core they mainly consist\nof tensor operations. As array-oriented languages provide perfect abstractions\nto implement tensor operations, we consider a minimalistic machine learning\nframework that is shallowly embedded in an array-oriented language and we study\nits productivity and performance. We do this by implementing a state of the art\nConvolutional Neural Network (CNN) and compare it against implementations in\nTensorFlow and PyTorch --- two state of the art industrial-strength frameworks.\nIt turns out that our implementation is 2 and 3 times faster, even after\nfine-tuning the TensorFlow and PyTorch to our hardware --- a 64-core\nGPU-accelerated machine. The size of all three CNN specifications is the same,\nabout 150 lines of code. Our mini framework is 150 lines of highly reusable\nhardware-agnostic code that does not depend on external libraries. The compiler\nfor a host array language automatically generates parallel code for a chosen\narchitecture. The key to such a balance between performance and portability\nlies in the design of the array language; in particular, the ability to express\nrank-polymorphic operations concisely, yet being able to do optimisations\nacross them. This design builds on very few assumptions, and it is readily\ntransferable to other contexts offering a clean approach to high-performance\nmachine learning.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 11:17:51 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["\u0160inkarovs", "Artjoms", ""], ["Vie\u00dfmann", "Hans-Nikolai", ""], ["Scholz", "Sven-Bodo", ""]]}, {"id": "1912.05601", "submitter": "Jonathan Chan", "authors": "Jonathan Chan and William J. Bowman", "title": "Practical Sized Typing for Coq", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Termination of recursive functions and productivity of corecursive functions\nare important for maintaining logical consistency in proof assistants. However,\ncontemporary proof assistants, such as Coq, rely on syntactic criteria that\nprevent users from easily writing obviously terminating or productive programs,\nsuch as quicksort. This is troublesome, since there exist theories for\ntype-based termination- and productivity-checking. In this paper, we present a\ndesign and implementation of sized type checking and inference for Coq. We\nextend past work on sized types for the Calculus of (Co)Inductive Constructions\n(CIC) with support for global definitions found in Gallina, and extend the\nsized-type inference algorithm to support completely unannotated Gallina terms.\nThis allows our design to maintain complete backward compatibility with\nexisting Coq developments. We provide an implementation that extends the Coq\nkernel with optional support for sized types.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 20:18:21 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 16:45:03 GMT"}, {"version": "v3", "created": "Sun, 31 May 2020 20:56:02 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Chan", "Jonathan", ""], ["Bowman", "William J.", ""]]}, {"id": "1912.05823", "submitter": "Abhik Roychoudhury", "authors": "Xiao Liang Yu, Omar Al-Bataineh, David Lo, Abhik Roychoudhury", "title": "Smart Contract Repair", "comments": "32 pages. ACM Transactions on Software Engineering and Methodology\n  (TOSEM), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart contracts are automated or self-enforcing contracts that can be used to\nexchange assets without having to place trust in third parties. Many commercial\ntransactions use smart contracts due to their potential benefits in terms of\nsecure peer-to-peer transactions independent of external parties. Experience\nshows that many commonly used smart contracts are vulnerable to serious\nmalicious attacks which may enable attackers to steal valuable assets of\ninvolving parties. There is therefore a need to apply analysis and automated\nrepair techniques to detect and repair bugs in smart contracts before being\ndeployed. In this work, we present the first general-purpose automated smart\ncontract repair approach that is also gas-aware. Our repair method is\nsearch-based and searches among mutations of the buggy contract. Our method\nalso considers the gas usage of the candidate patches by leveraging our novel\nnotion of gas dominance relationship. We have made our smart contract repair\ntool SCRepair available open-source, for investigation by the wider community.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 08:11:00 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 14:13:43 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 13:52:13 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Yu", "Xiao Liang", ""], ["Al-Bataineh", "Omar", ""], ["Lo", "David", ""], ["Roychoudhury", "Abhik", ""]]}, {"id": "1912.05937", "submitter": "Rahul Gopinath", "authors": "Rahul Gopinath, Bj\\\"orn Mathis, Andreas Zeller", "title": "Inferring Input Grammars from Dynamic Control Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A program is characterized by its input model, and a formal input model can\nbe of use in diverse areas including vulnerability analysis, reverse\nengineering, fuzzing and software testing, clone detection and refactoring.\nUnfortunately, input models for typical programs are often unavailable or out\nof date. While there exist algorithms that can mine the syntactical structure\nof program inputs, they either produce unwieldy and incomprehensible grammars,\nor require heuristics that target specific parsing patterns.\n  In this paper, we present a general algorithm that takes a program and a\nsmall set of sample inputs and automatically infers a readable context-free\ngrammar capturing the input language of the program. We infer the syntactic\ninput structure only by observing access of input characters at different\nlocations of the input parser. This works on all program stack based recursive\ndescent input parsers, including PEG and parser combinators, and can do\nentirely without program specific heuristics. Our Mimid prototype produced\naccurate and readable grammars for a variety of evaluation subjects, including\nexpr, URLparse, and microJSON.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 13:35:09 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Gopinath", "Rahul", ""], ["Mathis", "Bj\u00f6rn", ""], ["Zeller", "Andreas", ""]]}, {"id": "1912.06791", "submitter": "Ekansh Sharma", "authors": "Ekansh Sharma and Daniel M. Roy", "title": "Approximations in Probabilistic Programs", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the first-order probabilistic programming language introduced by\nStaton et al. (2016), but with an additional language construct,\n$\\mathbf{stat}$, that, like the fixpoint operator of Atkinson et al. (2018),\nconverts the description of the Markov kernel of an ergodic Markov chain into a\nsample from its unique stationary distribution. Up to minor changes in how\ncertain error conditions are handled, we show that $\\mathbf{norm}$ and\n$\\mathbf{score}$ are eliminable from the extended language, in the sense of\nFelleisen (1991). We do so by giving an explicit program transformation and\nproof of correctness. In fact, our program transformation implements a Markov\nchain Monte Carlo algorithm, in the spirit of the \"Trace-MH\" algorithm of\nWingate et al. (2011) and Goodman et al. (2008), but less sophisticated to\nenable analysis. We then explore the problem of approximately implementing the\nsemantics of the language with potentially nested $\\mathbf{stat}$ expressions,\nin a language without $\\mathbf{stat}$. For a single $\\mathbf{stat}$ term, the\nerror introduced by the finite unrolling proposed by Atkinson et al. (2018)\nvanishes only asymptotically. In the general case, no guarantees exist. Under\nuniform ergodicity assumptions, we are able to give quantitative error bounds\nand convergence results for the approximate implementation of the extended\nfirst-order language.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 05:58:21 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Sharma", "Ekansh", ""], ["Roy", "Daniel M.", ""]]}, {"id": "1912.06878", "submitter": "Qingkai Shi", "authors": "Qingkai Shi and Rongxin Wu and Gang Fan and Charles Zhang", "title": "Conquering the Extensional Scalability Problem for Value-Flow Analysis\n  Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an increasing number of value-flow properties to check, existing static\nprogram analysis still tends to have scalability issues when high precision is\nrequired. We observe that the key design flaw behind the scalability problem is\nthat the core static analysis engine is oblivious of the mutual synergies among\ndifferent properties being checked and, thus, inevitably loses many\noptimization opportunities. Our approach is inter-property-aware and able to\ncapture possible overlaps and inconsistencies among different properties. Thus,\nbefore analyzing a program, we can make optimization plans which decide how to\nreuse the specific analysis results of a property to speed up checking other\nproperties. Such a synergistic interaction among the properties significantly\nimproves the analysis performance. We have evaluated our approach by checking\ntwenty value-flow properties in standard benchmark programs and ten real-world\nsoftware systems. The results demonstrate that our approach is more than 8x\nfaster than existing ones but consumes only 1/7 memory. Such a substantial\nimprovement in analysis efficiency is not achieved by sacrificing the\neffectiveness: at the time of writing, 39 bugs found by our approach have been\nfixed by developers and four of them have been assigned CVE IDs due to their\nsecurity impact.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2019 16:51:33 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Shi", "Qingkai", ""], ["Wu", "Rongxin", ""], ["Fan", "Gang", ""], ["Zhang", "Charles", ""]]}, {"id": "1912.08255", "submitter": "Julia Belyakova", "authors": "Julia Belyakova", "title": "Decidable Tag-Based Semantic Subtyping for Nominal Types, Tuples, and\n  Unions", "comments": "Published at FTfJP'19", "journal-ref": "Proceedings of the 21st Workshop on Formal Techniques for\n  Java-like Programs (FTfJP '19). ACM, New York, NY, USA, 2019, Article 3, 11\n  pages", "doi": "10.1145/3340672.3341115", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic subtyping enables simple, set-theoretical reasoning about types by\ninterpreting a type as the set of its values. Previously, semantic subtyping\nhas been studied primarily in the context of statically typed languages with\nstructural typing. In this paper, we explore the applicability of semantic\nsubtyping in the context of a dynamic language with nominal types. Instead of\nstatic type checking, dynamic languages rely on run-time checking of type tags\nassociated with values, so we propose using the tags for semantic subtyping. We\nbase our work on a fragment of the Julia language and present tag-based\nsemantic subtyping for nominal types, tuples, and unions, where types are\ninterpreted set-theoretically, as sets of type tags. The proposed subtyping\nrelation is shown to be decidable, and a corresponding analytic definition is\nprovided. The implications of using semantic subtyping for multiple dispatch\nare also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 19:57:47 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Belyakova", "Julia", ""]]}, {"id": "1912.09611", "submitter": "EPTCS", "authors": "Rosemary Monahan (Maynooth University, Ireland), Virgile Prevosto\n  (Universit\\'e Paris-Saclay, France), Jose Proen\\c{c}a (HASLab/INESC-TEC &\n  CISTER/ISEP, Portugal)", "title": "Proceedings Fifth Workshop on Formal Integrated Development Environment", "comments": null, "journal-ref": "EPTCS 310, 2019", "doi": "10.4204/EPTCS.310", "report-no": null, "categories": "cs.SE cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of F-IDE 2019, the fifth international\nworkshop on Formal Integrated Development Environment, which was held on\nOctober 7, 2019 in Porto, Portugal, as part of FM'19, the 3rd World Congress on\nFormal Methods. High levels of safety, security and privacy standards require\nthe use of formal methods to specify and develop compliant software\n(sub)systems. Any standard comes with an assessment process, which requires a\ncomplete documentation of the application in order to ease the justification of\ndesign choices and the review of code and proofs. Thus tools are needed for\nhandling specifications, program constructs and verification artifacts. The aim\nof the F-IDE workshop is to provide a forum for presenting and discussing\nresearch efforts as well as experience returns on design, development and usage\nof formal IDE aiming at making formal methods \"easier\" for both specialists and\nnon-specialists.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 01:49:53 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Monahan", "Rosemary", "", "Maynooth University, Ireland"], ["Prevosto", "Virgile", "", "Universit\u00e9 Paris-Saclay, France"], ["Proen\u00e7a", "Jose", "", "HASLab/INESC-TEC &\n  CISTER/ISEP, Portugal"]]}, {"id": "1912.09715", "submitter": "Andrzej Szalas", "authors": "Andrzej Szalas", "title": "A Paraconsistent ASP-like Language with Tractable Model Generation", "comments": null, "journal-ref": "Journal of Applied Logic - IfColog Journal of Logic and their\n  Applications, vol. 7, No. 3, 2020, 361-389,\n  http://www.collegepublications.co.uk/downloads/ifcolog00039.pdf", "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) is nowadays a dominant rule-based knowledge\nrepresentation tool. Though existing ASP variants enjoy efficient\nimplementations, generating an answer set remains intractable. The goal of this\nresearch is to define a new \\asp-like rule language, 4SP, with tractable model\ngeneration. The language combines ideas of ASP and a paraconsistent rule\nlanguage 4QL. Though 4SP shares the syntax of \\asp and for each program all its\nanswer sets are among 4SP models, the new language differs from ASP in its\nlogical foundations, the intended methodology of its use and complexity of\ncomputing models.\n  As we show in the paper, 4QL can be seen as a paraconsistent counterpart of\nASP programs stratified with respect to default negation. Although model\ngeneration of well-supported models for 4QL programs is tractable, dropping\nstratification makes both 4QL and ASP intractable. To retain tractability while\nallowing non-stratified programs, in 4SP we introduce trial expressions\ninterlacing programs with hypotheses as to the truth values of default\nnegations. This allows us to develop a~model generation algorithm with\ndeterministic polynomial time complexity.\n  We also show relationships among 4SP, ASP and 4QL.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 09:35:29 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Szalas", "Andrzej", ""]]}, {"id": "1912.09770", "submitter": "Pepe Vila", "authors": "Pepe Vila, Pierre Ganty, Marco Guarnieri, and Boris K\\\"opf", "title": "CacheQuery: Learning Replacement Policies from Hardware Caches", "comments": "17 pages, 5 tables, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to infer deterministic cache replacement policies using\noff-the-shelf automata learning and program synthesis techniques. For this, we\nconstruct and chain two abstractions that expose the cache replacement policy\nof any set in the cache hierarchy as a membership oracle to the learning\nalgorithm, based on timing measurements on a silicon CPU. Our experiments\ndemonstrate an advantage in scope and scalability over prior art and uncover 2\npreviously undocumented cache replacement policies.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 11:31:28 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 10:11:02 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Vila", "Pepe", ""], ["Ganty", "Pierre", ""], ["Guarnieri", "Marco", ""], ["K\u00f6pf", "Boris", ""]]}, {"id": "1912.10041", "submitter": "Kees Middelburg", "authors": "C. A. Middelburg", "title": "Probabilistic process algebra and strategic interleaving", "comments": "30 pages, major revision with adaptation of example from\n  arXiv:2003.00473 incorporated (also text overlap with arXiv:1703.06822)", "journal-ref": "Scientific Annals of Computer Science 30(2):205--243 (2020)", "doi": "10.7561/SACS.2020.2.205", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first present a probabilistic version of ACP that rests on the principle\nthat probabilistic choices are always resolved before choices involved in\nalternative composition and parallel composition are resolved and then extend\nthis probabilistic version of ACP with a form of interleaving in which parallel\nprocesses are interleaved according to what is known as a process-scheduling\npolicy in the field of operating systems. We use the term strategic\ninterleaving for this more constrained form of interleaving. The extension\ncovers probabilistic process-scheduling policies.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 15:54:45 GMT"}, {"version": "v2", "created": "Sun, 26 Jan 2020 09:38:57 GMT"}, {"version": "v3", "created": "Sun, 1 Mar 2020 13:12:43 GMT"}, {"version": "v4", "created": "Sun, 15 Mar 2020 15:09:22 GMT"}, {"version": "v5", "created": "Tue, 21 Apr 2020 13:42:04 GMT"}, {"version": "v6", "created": "Tue, 8 Sep 2020 14:17:35 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Middelburg", "C. A.", ""]]}, {"id": "1912.10135", "submitter": "Apoorv Ingle", "authors": "Apoorv Ingle", "title": "QuB: A Resource Aware Functional Programming Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Managing resources---file handles, database connections, etc.---is a hard\nproblem. Debugging resource leaks and runtime errors due to resource\nmismanagement are difficult in evolving production code. Programming languages\nwith static type systems are great tools to ensure erroneous code is detected\nat compile time. However, modern static type systems do little in the aspect of\nresource management as resources are treated as normal values. We propose a\ntype system, Qub, based on the logic of bunched implications (BI) which models\nresources as first class citizens. We distinguish two kinds of program\nobjects---restricted and unrestricted---and two kinds of functions---sharing\nand separating. Our approach guarantees resource correctness without\ncompromising existing functional abstractions.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 22:59:39 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Ingle", "Apoorv", ""]]}, {"id": "1912.10629", "submitter": "EPTCS", "authors": "Denis Cousineau (Mitsubishi Electric R&D Centre Europe (MERCE) Rennes,\n  France), David Mentr\\'e (Mitsubishi Electric R&D Centre Europe (MERCE)\n  Rennes, France), Hiroaki Inoue (Mitsubishi Electric Corporation Amagasaki,\n  Japan)", "title": "Automated Deductive Verification for Ladder Programming", "comments": "In Proceedings F-IDE 2019, arXiv:1912.09611", "journal-ref": "EPTCS 310, 2019, pp. 7-12", "doi": "10.4204/EPTCS.310.2", "report-no": null, "categories": "cs.SE cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ladder Logics is a programming language standardized in IEC 61131-3 and\nwidely used for programming industrial Programmable Logic Controllers (PLC). A\nPLC program consists of inputs (whose values are given at runtime by factory\nsensors), outputs (whose values are given at runtime to factory actuators), and\nthe logical expressions computing output values from input values. Due to the\ngraphical form of Ladder programs, and the amount of inputs and outputs in\ntypical industrial programs, debugging such programs is time-consuming and\nerror-prone. We present, in this paper, a Why3-based tool prototype we have\nimplemented for automating the use of deductive verification in order to\nprovide an easy-to-use and robust debugging tool for Ladder programmers.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 05:40:01 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Cousineau", "Denis", "", "Mitsubishi Electric R&D Centre Europe"], ["Mentr\u00e9", "David", "", "Mitsubishi Electric R&D Centre Europe"], ["Inoue", "Hiroaki", "", "Mitsubishi Electric Corporation Amagasaki,\n  Japan"]]}, {"id": "1912.10630", "submitter": "EPTCS", "authors": "Fr\\'ed\\'eric Tuong (LRI, Universit\\'e Paris-Saclay), Burkhart Wolff\n  (LRI, Universit\\'e Paris-Saclay)", "title": "Deeply Integrating C11 Code Support into Isabelle/PIDE", "comments": "In Proceedings F-IDE 2019, arXiv:1912.09611", "journal-ref": "EPTCS 310, 2019, pp. 13-28", "doi": "10.4204/EPTCS.310.3", "report-no": null, "categories": "cs.PL cs.LO cs.SC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for C code in C11 syntax deeply integrated into the\nIsabelle/PIDE development environment. Our framework provides an abstract\ninterface for verification back-ends to be plugged-in independently. Thus,\nvarious techniques such as deductive program verification or white-box testing\ncan be applied to the same source, which is part of an integrated PIDE document\nmodel. Semantic back-ends are free to choose the supported C fragment and its\nsemantics. In particular, they can differ on the chosen memory model or the\nspecification mechanism for framing conditions.\n  Our framework supports semantic annotations of C sources in the form of\ncomments. Annotations serve to locally control back-end settings, and can\nexpress the term focus to which an annotation refers. Both the logical and the\nsyntactic context are available when semantic annotations are evaluated. As a\nconsequence, a formula in an annotation can refer both to HOL or C variables.\n  Our approach demonstrates the degree of maturity and expressive power the\nIsabelle/PIDE subsystem has achieved in recent years. Our integration technique\nemploys Lex and Yacc style grammars to ensure efficient deterministic parsing.\nWe present two case studies for the integration of (known) semantic back-ends\nin order to validate the design decisions for our back-end interface.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 05:40:20 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Tuong", "Fr\u00e9d\u00e9ric", "", "LRI, Universit\u00e9 Paris-Saclay"], ["Wolff", "Burkhart", "", "LRI, Universit\u00e9 Paris-Saclay"]]}, {"id": "1912.10631", "submitter": "EPTCS", "authors": "Peter D. Mosses", "title": "A Component-Based Formal Language Workbench", "comments": "In Proceedings F-IDE 2019, arXiv:1912.09611", "journal-ref": "EPTCS 310, 2019, pp. 29-34", "doi": "10.4204/EPTCS.310.4", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CBS framework supports component-based specification of programming\nlanguages. It aims to significantly reduce the effort of formal language\nspecification, and thereby encourage language developers to exploit formal\nsemantics more widely. CBS provides an extensive library of reusable language\nspecification components, facilitating co-evolution of languages and their\nspecifications.\n  After introducing CBS and its formal definition, this short paper reports\nwork in progress on generating an IDE for CBS from the definition. It also\nconsiders the possibility of supporting component-based language specification\nin other formal language workbenches.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 05:40:43 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Mosses", "Peter D.", ""]]}, {"id": "1912.10817", "submitter": "Rene Haberland", "authors": "Ren\\'e Haberland", "title": "Using Prolog for Transforming XML Documents", "comments": "49 pages, 54 figures, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.IR cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Proponents of the programming language Prolog share the opinion Prolog is\nmore appropriate for transforming XML-documents as other well-established\ntechniques and languages like XSLT. In order to clarify this position this work\nproposes a tuProlog-styled interpreter for parsing XML-documents into\nProlog-internal lists and vice versa for serialising lists into XML-documents.\nBased on this implementation a comparison between XSLT and Prolog follows.\nFirst, criteria are researched, such as considered language features of XSLT,\nusability and expressibility. These criteria are validated. Second, it is\nassessed when Prolog distinguishes between input and output parameters towards\nreversible transformation.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 18:24:17 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 10:40:40 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Haberland", "Ren\u00e9", ""]]}, {"id": "1912.11281", "submitter": "Frederik Gossen", "authors": "Frederik Gossen, Marc Jasper, Alnis Murtovi, Bernhard Steffen", "title": "Aggressive Aggregation: a New Paradigm for Program Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new paradigm for program optimization which is\nbased on aggressive aggregation, i.e., on a partial evaluation-based\ndecomposition of acyclic program fragments into a pair of computationally\noptimal structures: an Algebraic Decision Diagram (ADD) to capture conditional\nbranching and a parallel assignment that refers to an Expression DAG (ED) which\nrealizes redundancy-free computation. The point of this decomposition into, in\nfact, side-effect-free component structures allows for powerful optimization\nthat semantically comprise effects traditionally aimed at by SSA form\ntransformation, code specialization, common subexpression elimination, and\n(partial) redundancy elimination. We illustrate our approach along an\noptimization of the well-known iterative Fibonacci program, which, typically,\nis considered to lack any optimization potential. The point here is that our\ntechnique supports loop unrolling as a first class optimization technique and\nis tailored to optimally aggregate large program fragments, especially those\nresulting from multiple loop unrollings. For the Fibonacci program, this\nresults in a performance improvement beyond an order of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 10:24:22 GMT"}], "update_date": "2019-12-25", "authors_parsed": [["Gossen", "Frederik", ""], ["Jasper", "Marc", ""], ["Murtovi", "Alnis", ""], ["Steffen", "Bernhard", ""]]}, {"id": "1912.11308", "submitter": "Frederik Gossen", "authors": "Frederik Gossen, Alnis Murtovi, Philip Zweihoff, Bernhard Steffen", "title": "ADD-Lib: Decision Diagrams in Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper, we present the ADD-Lib, our efficient and easy to use framework\nfor Algebraic Decision Diagrams (ADDs). The focus of the ADD-Lib is not so much\non its efficient implementation of individual operations, which are taken by\nother established ADD frameworks, but its ease and flexibility, which arise at\ntwo levels: the level of individual ADD-tools, which come with a dedicated\nuser-friendly web-based graphical user interface, and at the meta level, where\nsuch tools are specified. Both levels are described in the paper: the meta\nlevel by explaining how we can construct an ADD-tool tailored for Random Forest\nrefinement and evaluation, and the accordingly generated Web-based\ndomain-specific tool, which we also provide as an artifact for cooperative\nexperimentation. In particular, the artifact allows readers to combine a given\nRandom Forest with their own ADDs regarded as expert knowledge and to\nexperience the corresponding effect.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 12:11:00 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Gossen", "Frederik", ""], ["Murtovi", "Alnis", ""], ["Zweihoff", "Philip", ""], ["Steffen", "Bernhard", ""]]}, {"id": "1912.11554", "submitter": "Neeraj Pradhan", "authors": "Du Phan, Neeraj Pradhan, Martin Jankowiak", "title": "Composable Effects for Flexible and Accelerated Probabilistic\n  Programming in NumPyro", "comments": "10 pages, 2 figures; NeurIPS 2019 Program Transformations for Machine\n  Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NumPyro is a lightweight library that provides an alternate NumPy backend to\nthe Pyro probabilistic programming language with the same modeling interface,\nlanguage primitives and effect handling abstractions. Effect handlers allow\nPyro's modeling API to be extended to NumPyro despite its being built atop a\nfundamentally different JAX-based functional backend. In this work, we\ndemonstrate the power of composing Pyro's effect handlers with the program\ntransformations that enable hardware acceleration, automatic differentiation,\nand vectorization in JAX. In particular, NumPyro provides an iterative\nformulation of the No-U-Turn Sampler (NUTS) that can be end-to-end JIT\ncompiled, yielding an implementation that is much faster than existing\nalternatives in both the small and large dataset regimes.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2019 22:09:36 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Phan", "Du", ""], ["Pradhan", "Neeraj", ""], ["Jankowiak", "Martin", ""]]}, {"id": "1912.11929", "submitter": "Pablo Gordillo", "authors": "Elvira Albert and Jes\\'us Correas and Pablo Gordillo and Guillermo\n  Rom\\'an-D\\'iez and Albert Rubio", "title": "GASOL: Gas Analysis and Optimization for Ethereum Smart Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the main concepts, components, and usage of GASOL, a Gas AnalysiS\nand Optimization tooL for Ethereum smart contracts. GASOL offers a wide variety\nof cost models that allow inferring the gas consumption associated to selected\ntypes of EVM instructions and/or inferring the number of times that such types\nof bytecode instructions are executed. Among others, we have cost models to\nmeasure only storage opcodes, to measure a selected family of gas-consumption\nopcodes following the Ethereum's classification, to estimate the cost of a\nselected program line, etc. After choosing the desired cost model and the\nfunction of interest, GASOL returns to the user an upper bound of the cost for\nthis function. As the gas consumption is often dominated by the instructions\nthat access the storage, GASOL uses the gas analysis to detect under-optimized\nstorage patterns, and includes an (optional) automatic optimization of the\nselected function. Our tool can be used within an Eclipse plugin for Solidity\nwhich displays the gas and instructions bounds and, when applicable, the\ngas-optimized Solidity function.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 20:35:57 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Albert", "Elvira", ""], ["Correas", "Jes\u00fas", ""], ["Gordillo", "Pablo", ""], ["Rom\u00e1n-D\u00edez", "Guillermo", ""], ["Rubio", "Albert", ""]]}, {"id": "1912.11951", "submitter": "Roshan Dathathri", "authors": "Roshan Dathathri, Blagovesta Kostova, Olli Saarikivi, Wei Dai, Kim\n  Laine, Madanlal Musuvathi", "title": "EVA: An Encrypted Vector Arithmetic Language and Compiler for Efficient\n  Homomorphic Computation", "comments": null, "journal-ref": "Programming Language Design and Implementation (PLDI 2020) 546-561", "doi": "10.1145/3385412.3386023", "report-no": null, "categories": "cs.CR cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully-Homomorphic Encryption (FHE) offers powerful capabilities by enabling\nsecure offloading of both storage and computation, and recent innovations in\nschemes and implementations have made it all the more attractive. At the same\ntime, FHE is notoriously hard to use with a very constrained programming model,\na very unusual performance profile, and many cryptographic constraints.\nExisting compilers for FHE either target simpler but less efficient FHE schemes\nor only support specific domains where they can rely on expert-provided\nhigh-level runtimes to hide complications.\n  This paper presents a new FHE language called Encrypted Vector Arithmetic\n(EVA), which includes an optimizing compiler that generates correct and secure\nFHE programs, while hiding all the complexities of the target FHE scheme.\nBolstered by our optimizing compiler, programmers can develop efficient\ngeneral-purpose FHE applications directly in EVA. For example, we have\ndeveloped image processing applications using EVA, with a very few lines of\ncode.\n  EVA is designed to also work as an intermediate representation that can be a\ntarget for compiling higher-level domain-specific languages. To demonstrate\nthis, we have re-targeted CHET, an existing domain-specific compiler for neural\nnetwork inference, onto EVA. Due to the novel optimizations in EVA, its\nprograms are on average 5.3x faster than those generated by CHET. We believe\nthat EVA would enable a wider adoption of FHE by making it easier to develop\nFHE applications and domain-specific FHE compilers.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 00:24:26 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 16:15:19 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Dathathri", "Roshan", ""], ["Kostova", "Blagovesta", ""], ["Saarikivi", "Olli", ""], ["Dai", "Wei", ""], ["Laine", "Kim", ""], ["Musuvathi", "Madanlal", ""]]}, {"id": "1912.12189", "submitter": "Utpal Bora", "authors": "Utpal Bora, Santanu Das, Pankaj Kukreja, Saurabh Joshi, Ramakrishna\n  Upadrasta, Sanjay Rajopadhye", "title": "LLOV: A Fast Static Data-Race Checker for OpenMP Programs", "comments": "Accepted in ACM TACO, August 2020", "journal-ref": null, "doi": "10.1145/3418597", "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of Exascale computing, writing efficient parallel programs is\nindispensable and at the same time, writing sound parallel programs is very\ndifficult. Specifying parallelism with frameworks such as OpenMP is relatively\neasy, but data races in these programs are an important source of bugs. In this\npaper, we propose LLOV, a fast, lightweight, language agnostic, and static data\nrace checker for OpenMP programs based on the LLVM compiler framework. We\ncompare LLOV with other state-of-the-art data race checkers on a variety of\nwell-established benchmarks. We show that the precision, accuracy, and the F1\nscore of LLOV is comparable to other checkers while being orders of magnitude\nfaster. To the best of our knowledge, LLOV is the only tool among the\nstate-of-the-art data race checkers that can verify a C/C++ or FORTRAN program\nto be data race free.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 15:53:53 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 17:34:09 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Bora", "Utpal", ""], ["Das", "Santanu", ""], ["Kukreja", "Pankaj", ""], ["Joshi", "Saurabh", ""], ["Upadrasta", "Ramakrishna", ""], ["Rajopadhye", "Sanjay", ""]]}, {"id": "1912.12345", "submitter": "Richard Shin", "authors": "Richard Shin, Neel Kant, Kavi Gupta, Christopher Bender, Brandon\n  Trabucco, Rishabh Singh, Dawn Song", "title": "Synthetic Datasets for Neural Program Synthesis", "comments": "ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of program synthesis is to automatically generate programs in a\nparticular language from corresponding specifications, e.g. input-output\nbehavior. Many current approaches achieve impressive results after training on\nrandomly generated I/O examples in limited domain-specific languages (DSLs), as\nwith string transformations in RobustFill. However, we empirically discover\nthat applying test input generation techniques for languages with control flow\nand rich input space causes deep networks to generalize poorly to certain data\ndistributions; to correct this, we propose a new methodology for controlling\nand evaluating the bias of synthetic data distributions over both programs and\nspecifications. We demonstrate, using the Karel DSL and a small Calculator DSL,\nthat training deep networks on these distributions leads to improved\ncross-distribution generalization performance.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 21:28:10 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Shin", "Richard", ""], ["Kant", "Neel", ""], ["Gupta", "Kavi", ""], ["Bender", "Christopher", ""], ["Trabucco", "Brandon", ""], ["Singh", "Rishabh", ""], ["Song", "Dawn", ""]]}, {"id": "1912.12659", "submitter": "Osbert Bastani", "authors": "Osbert Bastani and Xin Zhang and Armando Solar-Lezama", "title": "Synthesizing Queries via Interactive Sketching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to program synthesis, focusing on synthesizing\ndatabase queries. At a high level, our proposed algorithm takes as input a\nsketch with soft constraints encoding user intent, and then iteratively\ninteracts with the user to refine the sketch. At each step, our algorithm\nproposes a candidate refinement of the sketch, which the user can either accept\nor reject. By leveraging this rich form of user feedback, our algorithm is able\nto both resolve ambiguity in user intent and improve scalability. In\nparticular, assuming the user provides accurate inputs and responses, then our\nalgorithm is guaranteed to converge to the true program (i.e., one that the\nuser approves) in polynomial time. We perform a qualitative evaluation of our\nalgorithm, showing how it can be used to synthesize a variety of queries on a\ndatabase of academic publications.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 14:26:07 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Bastani", "Osbert", ""], ["Zhang", "Xin", ""], ["Solar-Lezama", "Armando", ""]]}, {"id": "1912.12700", "submitter": "Jeremie Lagraviere", "authors": "J\\'er\\'emie Lagravi\\`ere, Johannes Langguth, Mohammed Sourouri, Phuong\n  H. Ha, Xing Cai", "title": "On the Performance and Energy Efficiency of the PGAS Programming Model\n  on Multicore Architectures", "comments": null, "journal-ref": "Published in: 2016 International Conference on High Performance\n  Computing & Simulation (HPCS) Date of Conference: 18-22 July 2016 Conference\n  Location: Innsbruck, Austria", "doi": "10.1109/HPCSim.2016.7568416", "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Using large-scale multicore systems to get the maximum performance and energy\nefficiency with manageable programmability is a major challenge. The\npartitioned global address space (PGAS) programming model enhances\nprogrammability by providing a global address space over large-scale computing\nsystems. However, so far the performance and energy efficiency of the PGAS\nmodel on multicore-based parallel architectures have not been investigated\nthoroughly. In this paper we use a set of selected kernels from the well-known\nNAS Parallel Benchmarks to evaluate the performance and energy efficiency of\nthe UPC programming language, which is a widely used implementation of the PGAS\nmodel. In addition, the MPI and OpenMP versions of the same parallel kernels\nare used for comparison with their UPC counterparts. The investigated hardware\nplatforms are based on multicore CPUs, both within a single 16-core node and\nacross multiple nodes involving up to 1024 physical cores. On the multi-node\nplatform we used the hardware measurement solution called High definition\nEnergy Efficiency Monitoring tool in order to measure energy. On the\nsingle-node system we used the hybrid measurement solution to make an effort\ninto understanding the observed performance differences, we use the Intel\nPerformance Counter Monitor to quantify in detail the communication time, cache\nhit/miss ratio and memory usage. Our experiments show that UPC is competitive\nwith OpenMP and MPI on single and multiple nodes, with respect to both the\nperformance and energy efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 17:51:10 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Lagravi\u00e8re", "J\u00e9r\u00e9mie", ""], ["Langguth", "Johannes", ""], ["Sourouri", "Mohammed", ""], ["Ha", "Phuong H.", ""], ["Cai", "Xing", ""]]}, {"id": "1912.13122", "submitter": "Andres Garcia-Camino", "authors": "Andr\\'es Garc\\'ia-Camino", "title": "Declarative Mechanism Design", "comments": null, "journal-ref": null, "doi": null, "report-no": "Report-no: 01", "categories": "cs.AI cs.LG cs.LO cs.MA cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regulation of Multi-Agent Systems (MAS) and Declarative Electronic\nInstitutions (DEIs) was a multidisciplinary research topic of the past decade\ninvolving (Physical and Software) Agents and Law since the beginning, but\nrecently evolved towards News-claimed Robot Lawyer since 2016. One of these\nfirst proposals of restricting the behaviour of Software Agentswas Electronic\nInstitutions.However, with the recent reformulation of Artificial Neural\nNetworks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal\nissues regarding the use of DL has raised concerns in the Artificial\nIntelligence (AI) Community. Now that the Regulation of MAS is almost correctly\naddressed, we propose the Regulation of Artificial Neural Networks as\nAgent-based Training of a special type of regulated Artificial Neural Network\nthat we call Institutional Neural Network (INN).The main purpose of this paper\nis to bring attention to Artificial Teaching (AT) and to give a tentative\nanswer showing a proof-of-concept implementation of Regulated Deep Learning\n(RDL). This paper introduces the former concept and provide sI, a language\npreviously used to model declaratively and extend Electronic Institutions, as a\nmeans to regulate the execution of Artificial Neural Networks and their\ninteractions with Artificial Teachers (ATs)\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 00:10:50 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 22:36:52 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 17:19:26 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Garc\u00eda-Camino", "Andr\u00e9s", ""]]}, {"id": "1912.13451", "submitter": "Olin Shivers Third", "authors": "Olin Shivers, Justin Slepak and Panagiotis Manolios", "title": "Introduction to Rank-polymorphic Programming in Remora (Draft)", "comments": "52 pages; fixes some errors in previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remora is a higher-order, rank-polymorphic array-processing programming\nlanguage, in the same general class of languages as APL and J. It is intended\nfor writing programs to be executed on parallel hardware.\n  We provide an example-driven introduction to the language, and its general\ncomputational model, originally developed by Iverson for APL. We begin with\nDynamic Remora, a variant of the language with a dynamic type system (as in\nScheme or Lisp), to introduce the fundamental computational mechanisms of the\nlanguage, then shift to Explicitly Typed Remora, a variant of the language with\na static, dependent type system that permits the shape of the arrays being\ncomputed to be captured at compile time.\n  This article can be considered an introduction to the general topic of the\nrank-polymorphic array-processing computational model, above and beyond the\nspecific details of the Remora language.\n  We do not address the details of type inference in Remora, that is, the\nassignment of explicit types to programs written without such annotations; this\nis ongoing research.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 17:45:43 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 06:33:42 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Shivers", "Olin", ""], ["Slepak", "Justin", ""], ["Manolios", "Panagiotis", ""]]}, {"id": "1912.13477", "submitter": "Tarmo Uustalu", "authors": "Shin-ya Katsumata, Exequiel Rivas and Tarmo Uustalu", "title": "Interaction laws of monads and comonads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study functor-functor and monad-comonad interaction laws as\nmathematical objects to describe interaction of effectful computations with\nbehaviors of effect-performing machines. Monad-comonad interaction laws are\nmonoid objects of the monoidal category of functor-functor interaction laws. We\nshow that, for suitable generalizations of the concepts of dual and Sweedler\ndual, the greatest functor resp. monad interacting with a given functor or\ncomonad is its dual while the greatest comonad interacting with a given monad\nis its Sweedler dual. We relate monad-comonad interaction laws to stateful\nrunners. We show that functor-functor interaction laws are Chu spaces over the\ncategory of endofunctors taken with the Day convolution monoidal structure.\nHasegawa's glueing endows the category of these Chu spaces with a monoidal\nstructure whose monoid objects are monad-comonad interaction laws.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 18:27:22 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Katsumata", "Shin-ya", ""], ["Rivas", "Exequiel", ""], ["Uustalu", "Tarmo", ""]]}]