[{"id": "1604.00346", "submitter": "EPTCS", "authors": "Ferruccio Damiani (University of Torino, Italy), Michael Lienhardt\n  (University of Torino, Italy)", "title": "Refactoring Delta-Oriented Product Lines to achieve Monotonicity", "comments": "In Proceedings FMSPLE 2016, arXiv:1603.08577", "journal-ref": "EPTCS 206, 2016, pp. 2-16", "doi": "10.4204/EPTCS.206.2", "report-no": null, "categories": "cs.SE cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delta-oriented programming (DOP) is a flexible transformational approach to\nimplement software product lines. In delta-oriented product lines, variants are\ngenerated by applying operations contained in delta modules to a (possibly\nempty) base program. These operations can add, remove or modify named elements\nin a program (e.g., classes, methods and fields in a Java program). This paper\npresents algorithms for refactoring a delta-oriented product line into\nmonotonic form, i.e., either to contain add and modify operations only\n(monotonic increasing) or to contain remove and modify operations only\n(monotonic decreasing). Because of their simpler structure, monotonic\ndelta-oriented product lines are easier to analyze. The algorithms are\nformalized by means of a core calculus for DOP of product lines of Java\nprograms and their correctness and complexity are given.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 18:25:58 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Damiani", "Ferruccio", "", "University of Torino, Italy"], ["Lienhardt", "Michael", "", "University of Torino, Italy"]]}, {"id": "1604.00384", "submitter": "EPTCS", "authors": "Robert Atkey (University of Strathclyde), Neelakantan Krishnaswami\n  (University of Birmingham)", "title": "Proceedings 6th Workshop on Mathematically Structured Functional\n  Programming", "comments": null, "journal-ref": "EPTCS 207, 2016", "doi": "10.4204/EPTCS.207", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sixth workshop on Mathematically Structured Functional Programming is\ndevoted to the derivation of functionality from structure. It is a celebration\nof the direct impact of Theoretical Computer Science on programs as we write\nthem today. Modern programming languages, and in particular functional\nlanguages, support the direct expression of mathematical structures, equipping\nprogrammers with tools of remarkable power and abstraction. Where would Haskell\nbe without monads? Functional reactive programming without arrows?\nCall-by-push-value without adjunctions? The list goes on. This workshop is a\nforum for researchers who seek to reflect mathematical phenomena in data and\ncontrol.\n  The sixth workshop on Mathematically Structured Functional Programming was\nheld on 8th April 2016 as a part of ETAPS 2016 in Eindhoven, the Netherlands.\nThere were five contributed talks and one invited talk.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2016 19:52:28 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Atkey", "Robert", "", "University of Strathclyde"], ["Krishnaswami", "Neelakantan", "", "University of Birmingham"]]}, {"id": "1604.01184", "submitter": "EPTCS", "authors": "Maciej Pir\\'og (Department of Computer Science, KU Leuven, Belgium)", "title": "Eilenberg--Moore Monoids and Backtracking Monad Transformers", "comments": "In Proceedings MSFP 2016, arXiv:1604.00384", "journal-ref": "EPTCS 207, 2016, pp. 23-56", "doi": "10.4204/EPTCS.207.2", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an algebraic underpinning of backtracking monad transformers in\nthe general setting of monoidal categories. As our main technical device, we\nintroduce Eilenberg--Moore monoids, which combine monoids with algebras for\nstrong monads. We show that Eilenberg--Moore monoids coincide with algebras for\nthe list monad transformer ('done right') known from Haskell libraries.\n  From this, we obtain a number of results, including the facts that the list\nmonad transformer is indeed a monad, a transformer, and an instance of the\nMonadPlus class. Finally, we construct an Eilenberg--Moore monoid of\nendomorphisms, which, via the codensity monad construction, yields a\ncontinuation-based implementation a la Hinze.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 09:03:53 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Pir\u00f3g", "Maciej", "", "Department of Computer Science, KU Leuven, Belgium"]]}, {"id": "1604.01185", "submitter": "EPTCS", "authors": "Bartek Klin (University of Warsaw), Micha{\\l} Szynwelski (University\n  of Warsaw)", "title": "SMT Solving for Functional Programming over Infinite Structures", "comments": "In Proceedings MSFP 2016, arXiv:1604.00384", "journal-ref": "EPTCS 207, 2016, pp. 57-75", "doi": "10.4204/EPTCS.207.3", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a simple functional programming language aimed at manipulating\ninfinite, but first-order definable structures, such as the countably infinite\nclique graph or the set of all intervals with rational endpoints. Internally,\nsuch sets are represented by logical formulas that define them, and an external\nsatisfiability modulo theories (SMT) solver is regularly run by the interpreter\nto check their basic properties.\n  The language is implemented as a Haskell module.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 09:04:03 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Klin", "Bartek", "", "University of Warsaw"], ["Szynwelski", "Micha\u0142", "", "University\n  of Warsaw"]]}, {"id": "1604.01186", "submitter": "EPTCS", "authors": "Denis Firsov, Tarmo Uustalu, Niccol\\`o Veltri", "title": "Variations on Noetherianness", "comments": "In Proceedings MSFP 2016, arXiv:1604.00384", "journal-ref": "EPTCS 207, 2016, pp. 76-88", "doi": "10.4204/EPTCS.207.4", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In constructive mathematics, several nonequivalent notions of finiteness\nexist. In this paper, we continue the study of Noetherian sets in the\ndependently typed setting of the Agda programming language. We want to say that\na set is Noetherian, if, when we are shown elements from it one after another,\nwe will sooner or later have seen some element twice. This idea can be made\nprecise in a number of ways. We explore the properties and connections of some\nof the possible encodings. In particular, we show that certain implementations\nimply decidable equality while others do not, and we construct counterexamples\nin the latter case. Additionally, we explore the relation between\nNoetherianness and other notions of finiteness.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 09:04:13 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Firsov", "Denis", ""], ["Uustalu", "Tarmo", ""], ["Veltri", "Niccol\u00f2", ""]]}, {"id": "1604.01187", "submitter": "EPTCS", "authors": "Danel Ahman, Tarmo Uustalu", "title": "Directed Containers as Categories", "comments": "In Proceedings MSFP 2016, arXiv:1604.00384", "journal-ref": "EPTCS 207, 2016, pp. 89-98", "doi": "10.4204/EPTCS.207.5", "report-no": null, "categories": "cs.LO cs.PL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed containers make explicit the additional structure of those\ncontainers whose set functor interpretation carries a comonad structure. The\ndata and laws of a directed container resemble those of a monoid, while the\ndata and laws of a directed container morphism those of a monoid morphism in\nthe reverse direction. With some reorganization, a directed container is the\nsame as a small category, but a directed container morphism is opcleavage-like.\nWe draw some conclusions for comonads from this observation, considering in\nparticular basic constructions and concepts like the opposite category and a\ngroupoid.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 09:04:22 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Ahman", "Danel", ""], ["Uustalu", "Tarmo", ""]]}, {"id": "1604.01290", "submitter": "Vladimir Makarov", "authors": "Vladimir N. Makarov", "title": "Implementation of the Programming Language Dino -- A Case Study in\n  Dynamic Language Performance", "comments": "10 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The article gives a brief overview of the current state of programming\nlanguage Dino in order to see where its stands between other dynamic\nprogramming languages. Then it describes the current implementation, used tools\nand major implementation decisions including how to implement a stable,\nportable and simple JIT compiler.\n  We study the effect of major implementation decisions on the performance of\nDino on x86-64, AARCH64, and Powerpc64. In brief, the performance of some model\nbenchmark on x86-64 was improved by $\\textbf{3.1}$ times after moving from a\nstack based virtual machine to a register-transfer architecture, a further\n$\\textbf{1.5}$ times by adding byte code combining, a further $\\textbf{2.3}$\ntimes through the use of JIT, and a further $\\textbf{4.4}$ times by performing\ntype inference with byte code specialization, with a resulting overall\nperformance improvement of about $\\textbf{47}$ times. To put these results in\ncontext, we include performance comparisons of Dino with widely used\nimplementations of Ruby, Python 3, PyPy and JavaScript on the three platforms\nmentioned above.\n  The goal of this article is to share the experience of Dino implementation\nwith other dynamic language implementors in hope that it can help them to\nimprove implementation of popular dynamic languages to make them probably\nfaster and more portable, using less developer resources, and may be to avoid\nsome mistakes and wrong directions which were experienced during Dino\ndevelopment.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 15:12:06 GMT"}], "update_date": "2016-04-06", "authors_parsed": [["Makarov", "Vladimir N.", ""]]}, {"id": "1604.01401", "submitter": "Thomas H\\\"aner", "authors": "Thomas H\\\"aner, Damian S. Steiger, Krysta Svore, Matthias Troyer", "title": "A Software Methodology for Compiling Quantum Programs", "comments": null, "journal-ref": "Quantum Sci. Technol. 3 (2018) 020501", "doi": "10.1088/2058-9565/aaa5cc", "report-no": null, "categories": "cs.PL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum computers promise to transform our notions of computation by offering\na completely new paradigm. To achieve scalable quantum computation, optimizing\ncompilers and a corresponding software design flow will be essential. We\npresent a software architecture for compiling quantum programs from a\nhigh-level language program to hardware-specific instructions. We describe the\nnecessary layers of abstraction and their differences and similarities to\nclassical layers of a computer-aided design flow. For each layer of the stack,\nwe discuss the underlying methods for compilation and optimization. Our\nsoftware methodology facilitates more rapid innovation among quantum algorithm\ndesigners, quantum hardware engineers, and experimentalists. It enables\nscalable compilation of complex quantum algorithms and can be targeted to any\nspecific quantum hardware implementation.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2016 20:00:03 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 11:57:40 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["H\u00e4ner", "Thomas", ""], ["Steiger", "Damian S.", ""], ["Svore", "Krysta", ""], ["Troyer", "Matthias", ""]]}, {"id": "1604.01990", "submitter": "Christophe Raffalli", "authors": "Rodolphe Lepigre (1), Christophe Raffalli (1) ((1) LAMA)", "title": "Practical Subtyping for System F with Sized (Co-)Induction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a rich type system with subtyping for an extension of System F.\nOur type constructors include sum and product types, universal and existential\nquantifiers, inductive and coinductive types. The latter two size annotations\nallowing the preservation of size invariants. For example it is possible to\nderive the termination of the quicksort by showing that partitioning a list\ndoes not increase its size. The system deals with complex programs involving\nmixed induction and coinduction, or even mixed (co-)induction and polymorphism\n(as for Scott-encoded datatypes). One of the key ideas is to completely\nseparate the induction on sizes from the notion of recursive programs. We use\nthe size change principle to check that the proof is well-founded, not that the\nprogram terminates. Termination is obtained by a strong normalization proof.\nAnother key idea is the use symbolic witnesses to handle quantifiers of all\nsorts. To demonstrate the practicality of our system, we provide an\nimplementation that accepts all the examples discussed in the paper and much\nmore.\n", "versions": [{"version": "v1", "created": "Thu, 7 Apr 2016 13:32:13 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 13:23:40 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 07:41:46 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Lepigre", "Rodolphe", "", "LAMA"], ["Raffalli", "Christophe", "", "LAMA"]]}, {"id": "1604.02474", "submitter": "Michael Greenberg", "authors": "Michael Greenberg", "title": "Space-Efficient Latent Contracts", "comments": "In post-proceedings of TFP 2016. Second revision extends to\n  dependency; third fixes typos in the abstract; fourth is long version of what\n  will be the final TFP publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard higher-order contract monitoring breaks tail recursion and leads to\nspace leaks that can change a program's asymptotic complexity; space-efficiency\nrestores tail recursion and bounds the amount of space used by contracts.\nSpace-efficient contract monitoring for contracts enforcing simple type\ndisciplines (a/k/a gradual typing) is well studied. Prior work establishes a\nspace-efficient semantics for manifest contracts without dependency (Greenberg\n2015); we adapt that work to a latent calculus with dependency. We guarantee\nspace efficiency when no dependency is used; we cannot generally guarantee\nspace efficiency when dependency is used, but instead offer a framework for\nmaking such programs space efficient on a case-by-case basis.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 20:30:12 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 20:08:52 GMT"}, {"version": "v3", "created": "Wed, 1 Jun 2016 15:58:21 GMT"}, {"version": "v4", "created": "Tue, 6 Jun 2017 00:53:32 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Greenberg", "Michael", ""]]}, {"id": "1604.02480", "submitter": "Panagiotis Vekris", "authors": "Panagiotis Vekris, Benjamin Cosman, Ranjit Jhala", "title": "Refinement Types for TypeScript", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Refined TypeScript (RSC), a lightweight refinement type system for\nTypeScript, that enables static verification of higher-order, imperative\nprograms. We develop a formal core of RSC that delineates the interaction\nbetween refinement types and mutability. Next, we extend the core to account\nfor the imperative and dynamic features of TypeScript. Finally, we evaluate RSC\non a set of real world benchmarks, including parts of the Octane benchmarks,\nD3, Transducers, and the TypeScript compiler.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2016 20:50:15 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Vekris", "Panagiotis", ""], ["Cosman", "Benjamin", ""], ["Jhala", "Ranjit", ""]]}, {"id": "1604.03020", "submitter": "Hongwei Xi", "authors": "Hongwei Xi and Hanwen Wu", "title": "Linearly Typed Dyadic Group Sessions for Building Multiparty Sessions", "comments": "This paper can be seen as the pre-sequel to classical linear\n  multirole logic (CLML). arXiv admin note: substantial text overlap with\n  arXiv:1603.03727", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, each party in a (dyadic or multiparty) session implements\nexactly one role specified in the type of the session. We refer to this kind of\nsession as an individual session (i-session). As a generalization of i-session,\na group session (g-session) is one in which each party may implement a group of\nroles based on one channel. In particular, each of the two parties involved in\na dyadic g-session implements either a group of roles or its complement. In\nthis paper, we present a formalization of g-sessions in a multi-threaded\nlambda-calculus (MTLC) equipped with a linear type system, establishing for the\nMTLC both type preservation and global progress. As this formulated MTLC can be\nreadily embedded into ATS, a full-fledged language with a functional\nprogramming core that supports both dependent types (of DML-style) and linear\ntypes, we obtain a direct implementation of linearly typed g-sessions in ATS.\nThe primary contribution of the paper lies in both of the identification of\ng-sessions as a fundamental building block for multiparty sessions and the\ntheoretical development in support of this identification.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 16:23:24 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Xi", "Hongwei", ""], ["Wu", "Hanwen", ""]]}, {"id": "1604.03211", "submitter": "Alcides Fonseca", "authors": "Alcides Fonseca, Bruno Cabral, Jo\\~ao Rafael, Ivo Correia", "title": "Automatic Parallelization: Executing Sequential Programs on a Task-Based\n  Parallel Runtime", "comments": "Accepted for Publication", "journal-ref": "International Journal of Parallel Programming, 2016", "doi": "10.1007/s10766-016-0426-5", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are billions of lines of sequential code inside nowadays' software\nwhich do not benefit from the parallelism available in modern multicore\narchitectures. Automatically parallelizing sequential code, to promote an\nefficient use of the available parallelism, has been a research goal for some\ntime now. This work proposes a new approach for achieving such goal. We created\na new parallelizing compiler that analyses the read and write instructions, and\ncontrol-flow modifications in programs to identify a set of dependencies\nbetween the instructions in the program. Afterwards, the compiler, based on the\ngenerated dependencies graph, rewrites and organizes the program in a\ntask-oriented structure. Parallel tasks are composed by instructions that\ncannot be executed in parallel. A work-stealing-based parallel runtime is\nresponsible for scheduling and managing the granularity of the generated tasks.\nFurthermore, a compile-time granularity control mechanism also avoids creating\nunnecessary data-structures. This work focuses on the Java language, but the\ntechniques are general enough to be applied to other programming languages. We\nhave evaluated our approach on 8 benchmark programs against OoOJava, achieving\nhigher speedups. In some cases, values were close to those of a manual\nparallelization. The resulting parallel code also has the advantage of being\nreadable and easily configured to improve further its performance manually.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2016 18:22:03 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Fonseca", "Alcides", ""], ["Cabral", "Bruno", ""], ["Rafael", "Jo\u00e3o", ""], ["Correia", "Ivo", ""]]}, {"id": "1604.03410", "submitter": "Tim Besard", "authors": "Tim Besard, Pieter Verstraete and Bjorn De Sutter", "title": "High-level GPU programming in Julia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPUs are popular devices for accelerating scientific calculations. However,\nas GPU code is usually written in low-level languages, it breaks the\nabstractions of high-level languages popular with scientific programmers. To\novercome this, we present a framework for CUDA GPU programming in the\nhigh-level Julia programming language. This framework compiles Julia source\ncode for GPU execution, and takes care of the necessary low-level interactions\nusing modern code generation techniques to avoid run-time overhead.\n  Evaluating the framework and its APIs on a case study comprising the trace\ntransform from the field of image processing, we find that the impact on\nperformance is minimal, while greatly increasing programmer productivity. The\nmetaprogramming capabilities of the Julia language proved invaluable for\nenabling this. Our framework significantly improves usability of GPUs, making\nthem accessible for a wide range of programmers. It is available as free and\nopen-source software licensed under the MIT License.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 13:52:17 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Besard", "Tim", ""], ["Verstraete", "Pieter", ""], ["De Sutter", "Bjorn", ""]]}, {"id": "1604.03607", "submitter": "Dylan Hutchison", "authors": "Dylan Hutchison, Bill Howe, Dan Suciu", "title": "Lara: A Key-Value Algebra underlying Arrays and Relations", "comments": "Working draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data processing systems roughly group into families such as relational,\narray, graph, and key-value. Many data processing tasks exceed the capabilities\nof any one family, require data stored across families, or run faster when\npartitioned onto multiple families. Discovering ways to execute computation\namong multiple available systems, let alone discovering an optimal execution\nplan, is challenging given semantic differences between disparate families of\nsystems. In this paper we introduce a new algebra, Lara, which underlies and\nunifies algebras representing the families above in order to facilitate\ntranslation between systems. We describe the operations and objects of\nLara---union, join, and ext on associative tables---and show her properties and\nequivalences to other algebras. Multi-system optimization has a bright future,\nin which we proffer Lara for the role of universal connector.\n", "versions": [{"version": "v1", "created": "Tue, 12 Apr 2016 22:22:16 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Hutchison", "Dylan", ""], ["Howe", "Bill", ""], ["Suciu", "Dan", ""]]}, {"id": "1604.03641", "submitter": "Brianna Ren", "authors": "Brianna M. Ren, Jeffrey S. Foster", "title": "Just-in-Time Static Type Checking for Dynamic Languages", "comments": "19 pages, 6 figures, 2 tables, 1 appendix, This is a preprint of a\n  paper to appear in Programming Language Design and Implementation (PLDI 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic languages such as Ruby, Python, and JavaScript have many compelling\nbenefits, but the lack of static types means subtle errors can remain latent in\ncode for a long time. While many researchers have developed various systems to\nbring some of the benefits of static types to dynamic languages, prior\napproaches have trouble dealing with metaprogramming, which generates code as\nthe program executes. In this paper, we propose Hummingbird, a new system that\nuses a novel technique, just-in-time static type checking, to type check Ruby\ncode even in the presence of metaprogramming. In Hummingbird, method type\nsignatures are gathered dynamically at run-time, as those methods are created.\nWhen a method is called, Hummingbird statically type checks the method body\nagainst current type signatures. Thus, Hummingbird provides thorough static\nchecks on a per-method basis, while also allowing arbitrarily complex\nmetaprogramming. For performance, Hummingbird memoizes the static type checking\npass, invalidating cached checks only if necessary. We formalize Hummingbird\nusing a core, Ruby-like language and prove it sound. To evaluate Hummingbird,\nwe applied it to six apps, including three that use Ruby on Rails, a powerful\nframework that relies heavily on metaprogramming. We found that all apps\ntypecheck successfully using Hummingbird, and that Hummingbird's performance\noverhead is reasonable. We applied Hummingbird to earlier versions of one Rails\napp and found several type errors that had been introduced and then fixed.\nLastly, we demonstrate using Hummingbird in Rails development mode to typecheck\nan app as live updates are applied to it.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 03:04:17 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Ren", "Brianna M.", ""], ["Foster", "Jeffrey S.", ""]]}, {"id": "1604.04591", "submitter": "Christopher M. Poskitt", "authors": "Mischael Schill, Christopher M. Poskitt, Bertrand Meyer", "title": "An Interference-Free Programming Model for Network Objects", "comments": null, "journal-ref": "In Proc. International Conference on Coordination Models and\n  Languages (COORDINATION 2016), volume 9686 of LNCS, pages 227-244. Springer,\n  2016", "doi": "10.1007/978-3-319-39519-7_14", "report-no": null, "categories": "cs.DC cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network objects are a simple and natural abstraction for distributed\nobject-oriented programming. Languages that support network objects, however,\noften leave synchronization to the user, along with its associated pitfalls,\nsuch as data races and the possibility of failure. In this paper, we present\nD-SCOOP, a distributed programming model that allows for interference-free and\ntransaction-like reasoning on (potentially multiple) network objects, with\nsynchronization handled automatically, and network failures managed by a\ncompensation mechanism. We achieve this by leveraging the runtime semantics of\na multi-threaded object-oriented concurrency model, directly generalizing it\nwith a message-based protocol for efficiently coordinating remote objects. We\npresent our pathway to fusing these contrasting but complementary ideas, and\nevaluate the performance overhead of the automatic synchronization in D-SCOOP,\nfinding that it comes close to---or outperforms---explicit locking-based\nsynchronization in Java RMI.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 18:32:52 GMT"}], "update_date": "2016-05-25", "authors_parsed": [["Schill", "Mischael", ""], ["Poskitt", "Christopher M.", ""], ["Meyer", "Bertrand", ""]]}, {"id": "1604.04639", "submitter": "Dylan Hutchison", "authors": "Dylan Hutchison", "title": "ModelWizard: Toward Interactive Model Construction", "comments": "Master's Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data scientists engage in model construction to discover machine learning\nmodels that well explain a dataset, in terms of predictiveness,\nunderstandability and generalization across domains. Questions such as \"what if\nwe model common cause Z\" and \"what if Y's dependence on X reverses\" inspire\nmany candidate models to consider and compare, yet current tools emphasize\nconstructing a final model all at once.\n  To more naturally reflect exploration when debating numerous models, we\npropose an interactive model construction framework grounded in composable\noperations. Primitive operations capture core steps refining data and model\nthat, when verified, form an inductive basis to prove model validity. Derived,\ncomposite operations enable advanced model families, both generic and\nspecialized, abstracted away from low-level details.\n  We prototype our envisioned framework in ModelWizard, a domain-specific\nlanguage embedded in F# to construct Tabular models. We enumerate language\ndesign and demonstrate its use through several applications, emphasizing how\nlanguage may facilitate creation of complex models. To future engineers\ndesigning data science languages and tools, we offer ModelWizard's design as a\nnew model construction paradigm, speeding discovery of our universe's\nstructure.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2016 20:43:20 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Hutchison", "Dylan", ""]]}, {"id": "1604.04695", "submitter": "Michael D. Adams", "authors": "Michael D. Adams (University of Utah, USA), Celeste Hollenbeck\n  (University of Utah, USA), Matthew Might (University of Utah, USA)", "title": "On the Complexity and Performance of Parsing with Derivatives", "comments": "13 pages; 12 figures; implementation at\n  http://bitbucket.org/ucombinator/parsing-with-derivatives/ ; published in\n  PLDI '16, Proceedings of the 37th ACM SIGPLAN Conference on Programming\n  Language Design and Implementation, June 13 - 17, 2016, Santa Barbara, CA,\n  USA", "journal-ref": null, "doi": "10.1145/2908080.2908128", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current algorithms for context-free parsing inflict a trade-off between ease\nof understanding, ease of implementation, theoretical complexity, and practical\nperformance. No algorithm achieves all of these properties simultaneously.\n  Might et al. (2011) introduced parsing with derivatives, which handles\narbitrary context-free grammars while being both easy to understand and simple\nto implement. Despite much initial enthusiasm and a multitude of independent\nimplementations, its worst-case complexity has never been proven to be better\nthan exponential. In fact, high-level arguments claiming it is fundamentally\nexponential have been advanced and even accepted as part of the folklore.\nPerformance ended up being sluggish in practice, and this sluggishness was\ntaken as informal evidence of exponentiality.\n  In this paper, we reexamine the performance of parsing with derivatives. We\nhave discovered that it is not exponential but, in fact, cubic. Moreover,\nsimple (though perhaps not obvious) modifications to the implementation by\nMight et al. (2011) lead to an implementation that is not only easy to\nunderstand but also highly performant in practice.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 05:20:44 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Adams", "Michael D.", "", "University of Utah, USA"], ["Hollenbeck", "Celeste", "", "University of Utah, USA"], ["Might", "Matthew", "", "University of Utah, USA"]]}, {"id": "1604.04729", "submitter": "Rohin Shah", "authors": "Rohin Shah, Emina Torlak, Rastislav Bodik", "title": "SIMPL: A DSL for Automatic Specialization of Inference Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference algorithms in probabilistic programming languages (PPLs) can be\nthought of as interpreters, since an inference algorithm traverses a model\ngiven evidence to answer a query. As with interpreters, we can improve the\nefficiency of inference algorithms by compiling them once the model, evidence\nand query are known. We present SIMPL, a domain specific language for inference\nalgorithms, which uses this idea in order to automatically specialize annotated\ninference algorithms. Due to the approach of specialization, unlike a\ntraditional compiler, with SIMPL new inference algorithms can be added easily,\nand still be optimized using domain-specific information. We evaluate SIMPL and\nshow that partial evaluation gives a 2-6x speedup, caching provides an\nadditional 1-1.5x speedup, and generating C code yields an additional 13-20x\nspeedup, for an overall speedup of 30-150x for several inference algorithms and\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2016 11:45:03 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Shah", "Rohin", ""], ["Torlak", "Emina", ""], ["Bodik", "Rastislav", ""]]}, {"id": "1604.04983", "submitter": "Tahiry  Rabehaja", "authors": "N. Bordenabe, A. McIver, C Morgan and T. Rabehaja", "title": "Compositional security and collateral leakage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In quantitative information flow we say that program $Q$ is \"at least as\nsecure as\" $P$ just when the amount of secret information flowing from $Q$ is\nnever more than flows from $P$, with of course a suitable quantification of\n\"flow\". This secure-refinement order $\\sqsubseteq$ is compositional just when\n$P{\\sqsubseteq}Q$ implies ${\\cal C}(P){\\sqsubseteq}{\\cal C}(Q)$ for any context\n${\\cal C}$, again with a suitable definition of \"context\".\n  Remarkable however is that leaks caused by executing $P,Q$ might not be\nlimited to their declared variables: they might impact correlated secrets in\nvariables declared and initialised in some broader context to which $P,Q$ do\nnot refer even implicitly. We call such leaks collateral because their effect\nis felt in domains of which (the programmers of) $P, Q$ might be wholly\nunaware: our inspiration is the \"Dalenius\" phenomenon for statistical\ndatabases.\n  We show that a proper treatment of these collateral leaks is necessary for a\ncompositional program semantics for read/write \"open\" programs. By adapting a\nrecent Hidden-Markov denotational model for non-interference security, so that\nit becomes \"collateral aware\", we give techniques and examples (e.g.\\\npublic-key encryption) to show how collateral leakage can be calculated and\nthen bounded in its severity.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 04:52:20 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Bordenabe", "N.", ""], ["McIver", "A.", ""], ["Morgan", "C", ""], ["Rabehaja", "T.", ""]]}, {"id": "1604.05044", "submitter": "Marco Patrignani", "authors": "Marco Patrignani, Dominique Devriese, Frank Piessens", "title": "On Modular and Fully-Abstract Compilation -- Technical Appendix", "comments": "technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secure compilation studies compilers that generate target-level components\nthat are as secure as their source-level counterparts. Full abstraction is the\nmost widely-proven property when defining a secure compiler. A compiler is\nmodular if it allows different components to be compiled independently and then\nto be linked together to form a whole program. Unfortunately, many existing\nfully-abstract compilers to untyped machine code are not modular. So, while\nfully-abstractly compiled components are secure from malicious attackers, if\nthey are linked against each other the resulting component may become\nvulnerable to attacks. This paper studies how to devise modular, fully-abstract\ncompilers. It first analyses the attacks arising when compiled programs are\nlinked together, identifying security threats that are due to linking. Then, it\ndefines a compiler from an object-based language with method calls and dynamic\nmemory allocation to untyped assembly language extended with a memory isolation\nmechanism. The paper provides a proof sketch that the defined compiler is\nfully-abstract and modular, so its output can be linked together without\nintroducing security violations.\n", "versions": [{"version": "v1", "created": "Mon, 18 Apr 2016 08:54:33 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Patrignani", "Marco", ""], ["Devriese", "Dominique", ""], ["Piessens", "Frank", ""]]}, {"id": "1604.05841", "submitter": "Prasanna Kumar", "authors": "Prasanna Kumar. K and Amitabha Sanyal and Amey Karkare", "title": "Liveness-Based Garbage Collection for Lazy Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reducing the memory required to run lazy\nfirst-order functional programs. Our approach is to analyze programs for\nliveness of heap-allocated data. The result of the analysis is used to preserve\nonly live data---a subset of reachable data---during garbage collection. The\nresult is an increase in the garbage reclaimed and a reduction in the peak\nmemory requirement of programs. While this technique has already been shown to\nyield benefits for eager first-order languages, the lack of a statically\ndeterminable execution order and the presence of closures pose new challenges\nfor lazy languages. These require changes both in the liveness analysis itself\nand in the design of the garbage collector.\n  To show the effectiveness of our method, we implemented a copying collector\nthat uses the results of the liveness analysis to preserve live objects, both\nevaluated (i.e., in WHNF) and closures. Our experiments confirm that for\nprograms running with a liveness-based garbage collector, there is a\nsignificant decrease in peak memory requirements. In addition, a sizable\nreduction in the number of collections ensures that in spite of using a more\ncomplex garbage collector, the execution times of programs running with\nliveness and reachability-based collectors remain comparable.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 06:40:55 GMT"}, {"version": "v2", "created": "Mon, 2 May 2016 10:53:49 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["K", "Prasanna Kumar.", ""], ["Sanyal", "Amitabha", ""], ["Karkare", "Amey", ""]]}, {"id": "1604.05903", "submitter": "Nabarun Mondal Mr", "authors": "Nabarun Mondal, Jatin Puri, Mrunal Lohia", "title": "A declarative Language for Rapid Business Development", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The motivation for ZoomBA are domain specific languages (DSL) like VERILOG,\nVHDL, Spice. DSL for Software testing is not a new idea, many commercial tools\nlike Silk Suite use them, while Selenese, the DSL for Selenium IDE [6] is open\nsource. ZoomBA is a functionally motivated, embeddable, Turing Complete\nlanguage. It's philosophy is to expose the existing Java echo system in a\ndeclarative fashion for the purpose of System Integration and software\nvalidation. By design ZoomBA script size is meagre compared to Python or even\nto Scala for business automation problems. Bayestree uses ZoomBA for system\nintegration/adapter/data manipulation purposes.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2016 11:52:11 GMT"}, {"version": "v2", "created": "Sat, 5 May 2018 07:35:30 GMT"}, {"version": "v3", "created": "Fri, 11 May 2018 05:48:10 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Mondal", "Nabarun", ""], ["Puri", "Jatin", ""], ["Lohia", "Mrunal", ""]]}, {"id": "1604.06245", "submitter": "Cl\\'audio Vasconcelos", "authors": "Cl\\'audio Vasconcelos and Ant\\'onio Ravara", "title": "A Revision of the Mool Language", "comments": "34 pages, 15 figures, 11 listings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present here in a thorough analysis of the Mool language, covering not\nonly its implementation but also the formalisation (syntax, operational\nsemantics, and type system). The objective is to detect glitches in both the\nimplementation and in the formal definitions, proposing as well new features\nand added expressiveness. To test our proposals we implemented the revision\ndeveloped in the Racket platform.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2016 10:24:17 GMT"}, {"version": "v2", "created": "Fri, 1 Jul 2016 13:02:42 GMT"}, {"version": "v3", "created": "Mon, 4 Jul 2016 09:44:49 GMT"}, {"version": "v4", "created": "Thu, 22 Sep 2016 10:15:47 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Vasconcelos", "Cl\u00e1udio", ""], ["Ravara", "Ant\u00f3nio", ""]]}, {"id": "1604.06525", "submitter": "Matthias Nie{\\ss}ner", "authors": "Zachary DeVito, Michael Mara, Michael Zollh\\\"ofer, Gilbert Bernstein,\n  Jonathan Ragan-Kelley, Christian Theobalt, Pat Hanrahan, Matthew Fisher,\n  Matthias Nie{\\ss}ner", "title": "Opt: A Domain Specific Language for Non-linear Least Squares\n  Optimization in Graphics and Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many graphics and vision problems can be expressed as non-linear least\nsquares optimizations of objective functions over visual data, such as images\nand meshes. The mathematical descriptions of these functions are extremely\nconcise, but their implementation in real code is tedious, especially when\noptimized for real-time performance on modern GPUs in interactive applications.\nIn this work, we propose a new language, Opt (available under\nhttp://optlang.org), for writing these objective functions over image- or\ngraph-structured unknowns concisely and at a high level. Our compiler\nautomatically transforms these specifications into state-of-the-art GPU solvers\nbased on Gauss-Newton or Levenberg-Marquardt methods. Opt can generate\ndifferent variations of the solver, so users can easily explore tradeoffs in\nnumerical precision, matrix-free methods, and solver approaches. In our\nresults, we implement a variety of real-world graphics and vision applications.\nTheir energy functions are expressible in tens of lines of code, and produce\nhighly-optimized GPU solver implementations. These solver have performance\ncompetitive with the best published hand-tuned, application-specific GPU\nsolvers, and orders of magnitude beyond a general-purpose auto-generated\nsolver.\n", "versions": [{"version": "v1", "created": "Fri, 22 Apr 2016 03:02:59 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 00:19:31 GMT"}, {"version": "v3", "created": "Sat, 9 Sep 2017 13:23:33 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["DeVito", "Zachary", ""], ["Mara", "Michael", ""], ["Zollh\u00f6fer", "Michael", ""], ["Bernstein", "Gilbert", ""], ["Ragan-Kelley", "Jonathan", ""], ["Theobalt", "Christian", ""], ["Hanrahan", "Pat", ""], ["Fisher", "Matthew", ""], ["Nie\u00dfner", "Matthias", ""]]}, {"id": "1604.07169", "submitter": "Krishnendu Chatterjee", "authors": "Krishnendu Chatterjee and Hongfei Fu and Amir Kafshdar Goharshady", "title": "Termination Analysis of Probabilistic Programs through\n  Positivstellensatz's", "comments": "A conference version will appear in CAV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider nondeterministic probabilistic programs with the most basic\nliveness property of termination. We present efficient methods for termination\nanalysis of nondeterministic probabilistic programs with polynomial guards and\nassignments. Our approach is through synthesis of polynomial ranking\nsupermartingales, that on one hand significantly generalizes linear ranking\nsupermartingales and on the other hand is a counterpart of polynomial\nranking-functions for proving termination of nonprobabilistic programs. The\napproach synthesizes polynomial ranking-supermartingales through\nPositivstellensatz's, yielding an efficient method which is not only sound, but\nalso semi-complete over a large subclass of programs. We show experimental\nresults to demonstrate that our approach can handle several classical programs\nwith complex polynomial guards and assignments, and can synthesize efficient\nquadratic ranking-supermartingales when a linear one does not exist even for\nsimple affine programs.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 08:57:28 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Fu", "Hongfei", ""], ["Goharshady", "Amir Kafshdar", ""]]}, {"id": "1604.07201", "submitter": "Kohei Suenaga", "authors": "Kensuke Kojima and Minoru Kinoshita and Kohei Suenaga", "title": "Generalized Homogeneous Polynomials for Efficient Template-Based\n  Nonlinear Invariant Synthesis", "comments": "Presented in the 23rd Static Analysis Symposium (SAS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.SC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The template-based method is one of the most successful approaches to\nalgebraic invariant synthesis. In this method, an algorithm designates a\ntemplate polynomial p over program variables, generates constraints for p=0 to\nbe an invariant, and solves the generated constraints. However, this approach\noften suffers from an increasing template size if the degree of a template\npolynomial is too high.\n  We propose a technique to make template-based methods more efficient. Our\ntechnique is based on the following finding: If an algebraic invariant exists,\nthen there is a specific algebraic invariant that we call a generalized\nhomogeneous algebraic invariant that is often smaller. This finding justifies\nusing only a smaller template that corresponds to a generalized homogeneous\nalgebraic invariant.\n  Concretely, we state our finding above formally based on the abstract\nsemantics of an imperative program proposed by Cachera et al. Then, we modify\ntheir template-based invariant synthesis so that it generates only generalized\nhomogeneous algebraic invariants. This modification is proved to be sound.\nFurthermore, we also empirically demonstrate the merit of the restriction to\ngeneralized homogeneous algebraic invariants. Our implementation outperforms\nthat of Cachera et al. for programs that require a higher-degree template.\n", "versions": [{"version": "v1", "created": "Mon, 25 Apr 2016 11:07:05 GMT"}, {"version": "v2", "created": "Fri, 8 Jul 2016 00:50:35 GMT"}, {"version": "v3", "created": "Wed, 14 Sep 2016 00:06:16 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Kojima", "Kensuke", ""], ["Kinoshita", "Minoru", ""], ["Suenaga", "Kohei", ""]]}, {"id": "1604.08080", "submitter": "Germ\\'an Andr\\'es Delbianco", "authors": "Germ\\'an Andr\\'es Delbianco, Ilya Sergey, Aleksandar Nanevski and\n  Anindya Banerjee", "title": "Concurrent Data Structures Linked in Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arguments about correctness of a concurrent data structure are typically\ncarried out by using the notion of linearizability and specifying the\nlinearization points of the data structure's procedures. Such arguments are\noften cumbersome as the linearization points' position in time can be dynamic\n(depend on the interference, run-time values and events from the past, or even\nfuture), non-local (appear in procedures other than the one considered), and\nwhose position in the execution trace may only be determined after the\nconsidered procedure has already terminated.\n  In this paper we propose a new method, based on a separation-style logic, for\nreasoning about concurrent objects with such linearization points. We embrace\nthe dynamic nature of linearization points, and encode it as part of the data\nstructure's auxiliary state, so that it can be dynamically modified in place by\nauxiliary code, as needed when some appropriate run-time event occurs. We name\nthe idea linking-in-time, because it reduces temporal reasoning to spatial\nreasoning. For example, modifying a temporal position of a linearization point\ncan be modeled similarly to a pointer update in separation logic. Furthermore,\nthe auxiliary state provides a convenient way to concisely express the\nproperties essential for reasoning about clients of such concurrent objects. We\nillustrate the method by verifying (mechanically in Coq) an intricate optimal\nsnapshot algorithm due to Jayanti, as well as some clients.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2016 14:13:46 GMT"}, {"version": "v2", "created": "Tue, 3 May 2016 00:08:37 GMT"}, {"version": "v3", "created": "Mon, 24 Oct 2016 17:22:22 GMT"}, {"version": "v4", "created": "Wed, 18 Jan 2017 13:23:29 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Delbianco", "Germ\u00e1n Andr\u00e9s", ""], ["Sergey", "Ilya", ""], ["Nanevski", "Aleksandar", ""], ["Banerjee", "Anindya", ""]]}, {"id": "1604.08345", "submitter": "James Cheney", "authors": "James Cheney and Alberto Momigliano and Matteo Pessina", "title": "Advances in Property-Based Testing for $\\alpha$Prolog", "comments": "To appear, Tests and Proofs 2016; includes appendix with details not\n  in the conference version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $\\alpha$Check is a light-weight property-based testing tool built on top of\n$\\alpha$Prolog, a logic programming language based on nominal logic.\n$\\alpha$Prolog is particularly suited to the validation of the meta-theory of\nformal systems, for example correctness of compiler translations involving\nname-binding, alpha-equivalence and capture-avoiding substitution. In this\npaper we describe an alternative to the negation elimination algorithm\nunderlying $\\alpha$Check that substantially improves its effectiveness. To\nsubstantiate this claim we compare the checker performances w.r.t. two of its\nmain competitors in the logical framework niche, namely the QuickCheck/Nitpick\ncombination offered by Isabelle/HOL and the random testing facility in\nPLT-Redex.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 08:43:17 GMT"}, {"version": "v2", "created": "Fri, 29 Apr 2016 14:45:01 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Cheney", "James", ""], ["Momigliano", "Alberto", ""], ["Pessina", "Matteo", ""]]}, {"id": "1604.08501", "submitter": "Andreas Kl\\\"ockner", "authors": "Andreas Kl\\\"ockner and Lucas C. Wilcox and T. Warburton", "title": "Array Program Transformation with Loo.py by Example: High-Order Finite\n  Elements", "comments": null, "journal-ref": "ARRAY 2016 Proceedings of the 3rd ACM SIGPLAN International\n  Workshop on Libraries, Languages, and Compilers for Array Programming Pages\n  9-16", "doi": "10.1145/2935323.2935325", "report-no": null, "categories": "cs.PL cs.PF math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To concisely and effectively demonstrate the capabilities of our program\ntransformation system Loo.py, we examine a transformation path from two\nreal-world Fortran subroutines as found in a weather model to a single\nhigh-performance computational kernel suitable for execution on modern GPU\nhardware. Along the transformation path, we encounter kernel fusion,\nvectorization, prefetch- ing, parallelization, and algorithmic changes achieved\nby mechanized conversion between imperative and functional/substitution- based\ncode, among a number more. We conclude with performance results that\ndemonstrate the effects and support the effectiveness of the applied\ntransformations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2016 20:56:15 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Kl\u00f6ckner", "Andreas", ""], ["Wilcox", "Lucas C.", ""], ["Warburton", "T.", ""]]}]