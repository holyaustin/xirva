[{"id": "1708.00551", "submitter": "Kartik Chandra", "authors": "Kartik Chandra and Rastislav Bodik", "title": "Bonsai: Synthesis-Based Reasoning for Type Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe algorithms for symbolic reasoning about executable models of type\nsystems, supporting three queries intended for designers of type systems.\nFirst, we check for type soundness bugs and synthesize a counterexample program\nif such a bug is found. Second, we compare two versions of a type system,\nsynthesizing a program accepted by one but rejected by the other. Third, we\nminimize the size of synthesized counterexample programs.\n  These algorithms symbolically evaluate typecheckers and interpreters,\nproducing formulas that characterize the set of programs that fail or succeed\nin the typechecker and the interpreter. However, symbolically evaluating\ninterpreters poses efficiency challenges, which are caused by having to merge\nexecution paths of the various possible input programs. Our main contribution\nis the Bonsai tree, a novel symbolic representation of programs and program\nstates which addresses these challenges. Bonsai trees encode complex syntactic\ninformation in terms of logical constraints, enabling more efficient merging.\n  We implement these algorithms in the Bonsai tool, an assistant for type\nsystem designers. We perform case studies on how Bonsai helps test and explore\na variety of type systems. Bonsai efficiently synthesizes counterexamples for\nsoundness bugs that have been inaccessible to automatic tools, and is the first\nautomated tool to find a counterexample for the recently discovered Scala\nsoundness bug SI-9633.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 23:31:35 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Chandra", "Kartik", ""], ["Bodik", "Rastislav", ""]]}, {"id": "1708.00817", "submitter": "Guilherme B. de P\\'adua", "authors": "Guilherme B. de P\\'adua, Weiyi Shang", "title": "Revisiting Exception Handling Practices with Exception Flow Analysis", "comments": "Pre-print for SCAM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern programming languages, such as Java and C#, typically provide features\nthat handle exceptions. These features separate error-handling code from\nregular source code and aim to assist in the practice of software comprehension\nand maintenance. Having acknowledged the advantages of exception handling\nfeatures, their misuse can still cause reliability degradation or even\ncatastrophic software failures. Prior studies on exception handling aim to\nunderstand the practices of exception handling in its different components,\nsuch as the origin of the exceptions and the handling code of the exceptions.\nYet, the observed findings were scattered and diverse. In this paper, to\ncomplement prior research findings on exception handling, we study its features\nby enriching the knowledge of handling code with a flow analysis of exceptions.\nOur case study is conducted with over 10K exception handling blocks, and over\n77K related exception flows from 16 open-source Java and C# (.NET) libraries\nand applications. Our case study results show that each try block has up to 12\npossible potentially recoverable yet propagated exceptions. More importantly,\n22% of the distinct possible exceptions can be traced back to multiple methods\n(average of 1.39 and max of 34). Such results highlight the additional\nchallenge of composing quality exception handling code. To make it worse, we\nconfirm that there is a lack of documentation of the possible exceptions and\ntheir sources. However, such critical information can be identified by\nexception flow analysis on well- documented API calls (e.g., JRE and .NET\ndocumentation). Finally, we observe different strategies in exception handling\ncode between Java and C#. Our findings highlight the opportunities of\nleveraging automated software analysis to assist in exception handling\npractices and signify the need of more further in-depth studies on exception\nhandling practice.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 16:50:11 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["de P\u00e1dua", "Guilherme B.", ""], ["Shang", "Weiyi", ""]]}, {"id": "1708.01679", "submitter": "Guillermo Polito", "authors": "Guillermo Polito (Centre de Recherche en Informatique Signal et\n  Automatique de Lille, France), Camille Teruel (Inria, France), St\\'ephane\n  Ducasse (INRIA, France), Luc Fabresse (Mines Telecom Institute, Douai,\n  France)", "title": "Scoped Extension Methods in Dynamically-Typed Languages", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2018, Vol. 2,\n  Issue 1, Article 1", "doi": "10.22152/programming-journal.org/2018/2/1", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context. An extension method is a method declared in a package other than the\npackage of its host class. Thanks to extension methods, developers can adapt to\ntheir needs classes they do not own: adding methods to core classes is a\ntypical use case. This is particularly useful for adapting software and\ntherefore to increase reusability.\n  Inquiry. In most dynamically-typed languages, extension methods are globally\nvisible. Because any developer can define extension methods for any class,\nnaming conflicts occur: if two developers define an extension method with the\nsame signature in the same class, only one extension method is visible and\noverwrites the other. Similarly, if two developers each define an extension\nmethod with the same name in a class hierarchy, one overrides the other. To\navoid such \"accidental overrides\", some dynamically-typed languages limit the\nvisibility of an extension method to a particular scope. However, their\nsemantics have not been fully described and compared. In addition, these\nsolutions typically rely on a dedicated and slow method lookup algorithm to\nresolve conflicts at runtime.\n  Approach. In this article, we present a formalization of the underlying\nmodels of Ruby refinements, Groovy categories, Classboxes, and Method Shelters\nthat are scoping extension method solutions in dynamically-typed languages.\n  Knowledge. Our formal framework allows us to objectively compare and analyze\nthe shortcomings of the studied solutions and other different approaches such\nas MultiJava. In addition, language designers can use our formal framework to\ndetermine which mechanism has less risk of \"accidental overrides\".\n  Grounding. Our comparison and analysis of existing solutions is grounded\nbecause it is based on denotational semantics formalizations.\n  Importance. Extension methods are widely used in programming languages that\nsupport them, especially dynamically-typed languages such as Pharo, Ruby or\nPython. However, without a carefully designed mechanism, this feature can cause\ninsidious hidden bugs or can be voluntarily used to gain access to protected\noperations, violate encapsulation or break fundamental invariants.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 23:04:25 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Polito", "Guillermo", "", "Centre de Recherche en Informatique Signal et\n  Automatique de Lille, France"], ["Teruel", "Camille", "", "Inria, France"], ["Ducasse", "St\u00e9phane", "", "INRIA, France"], ["Fabresse", "Luc", "", "Mines Telecom Institute, Douai,\n  France"]]}, {"id": "1708.02318", "submitter": "Vikraman Choudhury", "authors": "Chao-Hong Chen, Vikraman Choudhury and Ryan R. Newton", "title": "Adaptive Lock-Free Data Structures in Haskell: A General Method for\n  Concurrent Implementation Swapping", "comments": "To be published in ACM SIGPLAN Haskell Symposium 2017", "journal-ref": null, "doi": "10.1145/3122955.3122973", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key part of implementing high-level languages is providing built-in and\ndefault data structures. Yet selecting good defaults is hard. A mutable data\nstructure's workload is not known in advance, and it may shift over its\nlifetime - e.g., between read-heavy and write-heavy, or from heavy contention\nby multiple threads to single-threaded or low-frequency use. One idea is to\nswitch implementations adaptively, but it is nontrivial to switch the\nimplementation of a concurrent data structure at runtime. Performing the\ntransition requires a concurrent snapshot of data structure contents, which\nnormally demands special engineering in the data structure's design. However,\nin this paper we identify and formalize an relevant property of lock-free\nalgorithms. Namely, lock-freedom is sufficient to guarantee that freezing\nmemory locations in an arbitrary order will result in a valid snapshot. Several\nfunctional languages have data structures that freeze and thaw, transitioning\nbetween mutable and immutable, such as Haskell vectors and Clojure transients,\nbut these enable only single-threaded writers. We generalize this approach to\naugment an arbitrary lock-free data structure with the ability to gradually\nfreeze and optionally transition to a new representation. This augmentation\ndoesn't require changing the algorithm or code for the data structure, only\nreplacing its datatype for mutable references with a freezable variant. In this\npaper, we present an algorithm for lifting plain to adaptive data and prove\nthat the resulting hybrid data structure is itself lock-free, linearizable, and\nsimulates the original. We also perform an empirical case study in the context\nof heating up and cooling down concurrent maps.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 22:00:49 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Chen", "Chao-Hong", ""], ["Choudhury", "Vikraman", ""], ["Newton", "Ryan R.", ""]]}, {"id": "1708.02319", "submitter": "EPTCS", "authors": "Dan R. Ghica (1), Khulood Alyahya (2) ((1) University of Birmingham,\n  (2) University of Exeter)", "title": "On the Learnability of Programming Language Semantics", "comments": "In Proceedings ICE 2017, arXiv:1711.10708", "journal-ref": "EPTCS 261, 2017, pp. 57-75", "doi": "10.4204/EPTCS.261.7", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game semantics is a powerful method of semantic analysis for programming\nlanguages. It gives mathematically accurate models (\"fully abstract\") for a\nwide variety of programming languages. Game semantic models are combinatorial\ncharacterisations of all possible interactions between a term and its syntactic\ncontext. Because such interactions can be concretely represented as sets of\nsequences, it is possible to ask whether they can be learned from examples.\nConcretely, we are using long short-term memory neural nets (LSTM), a technique\nwhich proved effective in learning natural languages for automatic translation\nand text synthesis, to learn game-semantic models of sequential and concurrent\nversions of Idealised Algol (IA), which are algorithmically complex yet can be\nconcisely described. We will measure how accurate the learned models are as a\nfunction of the degree of the term and the number of free variables involved.\nFinally, we will show how to use the learned model to perform latent semantic\nanalysis between concurrent and sequential Idealised Algol.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 22:04:30 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 05:19:42 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Ghica", "Dan R.", ""], ["Alyahya", "Khulood", ""]]}, {"id": "1708.02328", "submitter": "Vikraman Choudhury", "authors": "Ryan Scott, Vikraman Choudhury, Ryan Newton, Niki Vazou and Ranjit\n  Jhala", "title": "Deriving Law-Abiding Instances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liquid Haskell's refinement-reflection feature augments the Haskell language\nwith theorem proving capabilities, allowing programmers to retrofit their\nexisting code with proofs. But many of these proofs require routine,\nboilerplate code that is tedious to write. Moreover, many such proofs do not\nscale well, as the size of proof terms can grow superlinearly with the size of\nthe datatypes involved in the proofs.\n  We present a technique for programming with refinement reflection which\nsolves this problem by leveraging datatype-generic programming. Our observation\nis that we can take any algebraic datatype, generate an equivalent\nrepresentation type, and have Liquid Haskell automatically construct (and\nprove) an isomorphism between the original type and the representation type.\nThis reduces many proofs down to easy theorems over simple algebraic \"building\nblock\" types, allowing programmers to write generic proofs cheaply and\ncheerfully.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 22:56:20 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Scott", "Ryan", ""], ["Choudhury", "Vikraman", ""], ["Newton", "Ryan", ""], ["Vazou", "Niki", ""], ["Jhala", "Ranjit", ""]]}, {"id": "1708.02512", "submitter": "Daniele Cono D'Elia", "authors": "Daniele Cono D'Elia, Camil Demetrescu", "title": "On-Stack Replacement \\`a la Carte", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-stack replacement (OSR) dynamically transfers execution between different\ncode versions. This mechanism is used in mainstream runtime systems to support\nadaptive and speculative optimizations by running code tailored to provide the\nbest expected performance for the actual workload. Current approaches either\nrestrict the program points where OSR can be fired or require complex\noptimization-specific operations to realign the program's state during a\ntransition. The engineering effort to implement OSR and the lack of\nabstractions make it rarely accessible to the research community, leaving\nfundamental question regarding its flexibility largely unexplored.\n  In this article we make a first step towards a provably sound abstract\nframework for OSR. We show that compiler optimizations can be made OSR-aware in\nisolation, and then safely composed. We identify a class of transformations,\nwhich we call live-variable equivalent (LVE), that captures a natural property\nof fundamental compiler optimizations, and devise an algorithm to automatically\ngenerate the OSR machinery required for an LVE transition at arbitrary program\nlocations.\n  We present an implementation of our ideas in LLVM and evaluate it against\nprominent benchmarks, showing that bidirectional OSR transitions are possible\nalmost everywhere in the code in the presence of common, unhindered global\noptimizations. We then discuss the end-to-end utility of our techniques in\nsource-level debugging of optimized code, showing how our algorithms can\nprovide novel building blocks for debuggers for both executables and managed\nruntimes.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 15:03:01 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["D'Elia", "Daniele Cono", ""], ["Demetrescu", "Camil", ""]]}, {"id": "1708.02537", "submitter": "Justin Hsu", "authors": "Gilles Barthe, Thomas Espitau, Benjamin Gr\\'egoire, Justin Hsu,\n  Pierre-Yves Strub", "title": "Proving Expected Sensitivity of Probabilistic Programs", "comments": null, "journal-ref": null, "doi": "10.1145/3158145", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program sensitivity, also known as Lipschitz continuity, describes how small\nchanges in a program's input lead to bounded changes in the output. We propose\nan average notion of program sensitivity for probabilistic programs---expected\nsensitivity---that averages a distance function over a probabilistic coupling\nof two output distributions from two similar inputs. By varying the distance,\nexpected sensitivity recovers useful notions of probabilistic function\nsensitivity, including stability of machine learning algorithms and convergence\nof Markov chains.\n  Furthermore, expected sensitivity satisfies clean compositional properties\nand is amenable to formal verification. We develop a relational program logic\ncalled $\\mathbb{E}$pRHL for proving expected sensitivity properties. Our logic\nfeatures two key ideas. First, relational pre-conditions and post-conditions\nare expressed using distances, a real-valued generalization of typical\nboolean-valued (relational) assertions. Second, judgments are interpreted in\nterms of expectation coupling, a novel, quantitative generalization of\nprobabilistic couplings which supports compositional reasoning.\n  We demonstrate our logic on examples beyond the reach of prior relational\nlogics. Our main example formalizes uniform stability of the stochastic\ngradient method. Furthermore, we prove rapid mixing for a probabilistic model\nof population dynamics. We also extend our logic with a transitivity principle\nfor expectation couplings to capture the path coupling proof technique by\nBubley and Dyer, and formalize rapid mixing of the Glauber dynamics from\nstatistical physics.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 15:58:45 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 20:31:33 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Barthe", "Gilles", ""], ["Espitau", "Thomas", ""], ["Gr\u00e9goire", "Benjamin", ""], ["Hsu", "Justin", ""], ["Strub", "Pierre-Yves", ""]]}, {"id": "1708.02651", "submitter": "Ioana Cristescu", "authors": "Pierre Boutillier (1), Ioana Cristescu (1) ((1) HMS)", "title": "When rule-based models need to count", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.OT cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rule-based modelers dislike direct enumeration of cases when more efficient\nmeans of enumeration are available. We present an extension of the Kappa\nlanguage which attaches to agents a notion of level. We detail two encodings\nthat are more concise than the former practice.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 15:17:22 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Boutillier", "Pierre", "", "HMS"], ["Cristescu", "Ioana", "", "HMS"]]}, {"id": "1708.02710", "submitter": "Vikraman Choudhury", "authors": "Jacques Carette, Chao-Hong Chen, Vikraman Choudhury and Amr Sabry", "title": "From Reversible Programs to Univalent Universes and Back", "comments": null, "journal-ref": null, "doi": "10.1016/j.entcs.2018.03.013", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a close connection between a reversible programming language\nbased on type isomorphisms and a formally presented univalent universe. The\ncorrespondence relates combinators witnessing type isomorphisms in the\nprogramming language to paths in the univalent universe; and combinator\noptimizations in the programming language to 2-paths in the univalent universe.\nThe result suggests a simple computational interpretation of paths and of\nunivalence in terms of familiar programming constructs whenever the universe in\nquestion is computable.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 04:03:13 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Carette", "Jacques", ""], ["Chen", "Chao-Hong", ""], ["Choudhury", "Vikraman", ""], ["Sabry", "Amr", ""]]}, {"id": "1708.02801", "submitter": "Ahmed Rezine", "authors": "Zeinab Ganjei, Ahmed Rezine, Petru Eles and Zebo Peng", "title": "Safety Verification of Phaser Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of statically checking control state reachability (as\nin possibility of assertion violations, race conditions or runtime errors) and\nplain reachability (as in deadlock-freedom) of phaser programs. Phasers are a\nmodern non-trivial synchronization construct that supports dynamic parallelism\nwith runtime registration and deregistration of spawned tasks. They allow for\ncollective and point-to-point synchronizations. For instance, phasers can\nenforce barriers or producer-consumer synchronization schemes among all or\nsubsets of the running tasks. Implementations %of these recent and dynamic\nsynchronization are found in modern languages such as X10 or Habanero Java.\nPhasers essentially associate phases to individual tasks and use their runtime\nvalues to restrict possible concurrent executions. Unbounded phases may result\nin infinite transition systems even in the case of programs only creating\nfinite numbers of tasks and phasers. We introduce an exact gap-order based\nprocedure that always terminates when checking control reachability for\nprograms generating bounded numbers of coexisting tasks and phasers. We also\nshow verifying plain reachability is undecidable even for programs generating\nfew tasks and phasers. We then explain how to turn our procedure into a sound\nanalysis for checking plain reachability (including deadlock freedom). We\nreport on preliminary experiments with our open source tool.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 12:17:01 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Ganjei", "Zeinab", ""], ["Rezine", "Ahmed", ""], ["Eles", "Petru", ""], ["Peng", "Zebo", ""]]}, {"id": "1708.03536", "submitter": "Guido Mart\\'inez", "authors": "Alejandro D\\'iaz-Caro and Guido Mart\\'inez", "title": "Confluence in Probabilistic Rewriting", "comments": "LSFA 2017 Final version", "journal-ref": "ENTCS 338:115-131, 2018", "doi": "10.1016/j.entcs.2018.10.008", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by the interest of reasoning about probabilistic programming\nlanguages, we set out to study a notion of unicity of normal forms for them. To\nprovide a tractable proof method for it, we define a property of distribution\nconfluence which is shown to imply the desired uniqueness (even for infinite\nsequences of reduction) and further properties. We then carry over several\ncriteria from the classical case, such as Newman's lemma, to simplify proving\nconfluence in concrete languages. Using these criteria, we obtain simple proofs\nof confluence for $\\lambda_1$, an affine probabilistic $\\lambda$-calculus, and\nfor Q$^*$, a quantum programming language for which a related property has\nalready been proven in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 13:26:43 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 03:12:16 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["D\u00edaz-Caro", "Alejandro", ""], ["Mart\u00ednez", "Guido", ""]]}, {"id": "1708.03786", "submitter": "Ryo Suzuki", "authors": "Ryo Suzuki, Gustavo Soares, Andrew Head, Elena Glassman, Ruan Reis,\n  Melina Mongiovi, Loris D'Antoni, Bjoern Hartmann", "title": "TraceDiff: Debugging Unexpected Code Behavior Using Trace Divergences", "comments": "VL/HCC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in program synthesis offer means to automatically debug\nstudent submissions and generate personalized feedback in massive programming\nclassrooms. When automatically generating feedback for programming assignments,\na key challenge is designing pedagogically useful hints that are as effective\nas the manual feedback given by teachers. Through an analysis of teachers'\nhint-giving practices in 132 online Q&A posts, we establish three design\nguidelines that an effective feedback design should follow. Based on these\nguidelines, we develop a feedback system that leverages both program synthesis\nand visualization techniques. Our system compares the dynamic code execution of\nboth incorrect and fixed code and highlights how the error leads to a\ndifference in behavior and where the incorrect code trace diverges from the\nexpected solution. Results from our study suggest that our system enables\nstudents to detect and fix bugs that are not caught by students using another\nexisting visual debugging tool.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 14:48:32 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Suzuki", "Ryo", ""], ["Soares", "Gustavo", ""], ["Head", "Andrew", ""], ["Glassman", "Elena", ""], ["Reis", "Ruan", ""], ["Mongiovi", "Melina", ""], ["D'Antoni", "Loris", ""], ["Hartmann", "Bjoern", ""]]}, {"id": "1708.03882", "submitter": "Raphael Jolly", "authors": "Raphael Jolly", "title": "Monadic Remote Invocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to achieve Separation of Concerns in the domain of remote method\ninvocation, a small functional adapter is added atop Java RMI, eliminating the\nneed for every remote object to implement java.rmi.Remote and making it\npossible to remotely access existing code, unchanged. The Remote monad is\nintroduced, and its implementation and usage are detailed. Reusing the\nexisting, proven technology of RMI allows not to re-invent the underlying\nnetwork protocol. As a result, orthogonal remote invocation is achieved with\nlittle or no implementation effort.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 10:29:06 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Jolly", "Raphael", ""]]}, {"id": "1708.04976", "submitter": "Jasine Babu", "authors": "Jasine Babu, K. Murali Krishnan and Vineeth Paleri", "title": "A fix-point characterization of Herbrand equivalence of expressions in\n  data flow frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of determining Herbrand equivalence of terms at each program\npoint in a data flow framework is a central and well studied question in\nprogram analysis. Most of the well-known algorithms for the computation of\nHerbrand equivalence in data flow frameworks proceed via iterative fix-point\ncomputation on some abstract lattice of short expressions relevant to the given\nflow graph. However the mathematical definition of Herbrand equivalence is\nbased on a meet over all path characterization over the (infinite) set of all\npossible expressions. The aim of this paper is to develop a lattice theoretic\nfix-point formulation of Herbrand equivalence on the (infinite) concrete\nlattice defined over the set of all terms constructible from variables,\nconstants and operators of a program. The present characterization uses an\naxiomatic formulation of the notion of Herbrand congruence and defines the\n(infinite) concrete lattice of Herbrand congruences. Transfer functions and\nnon-deterministic assignments are formulated as monotone functions over this\nconcrete lattice. Herbrand equivalence is defined as the maximum fix point of a\ncomposite transfer function defined over an appropriate product lattice of the\nabove concrete lattice. A re-formulation of the classical meet-over-all-paths\ndefinition of Herbrand equivalence in the above lattice theoretic framework is\nalso presented and is proven to be equivalent to the new lattice theoretic\nfix-point characterization.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 17:05:01 GMT"}, {"version": "v2", "created": "Fri, 20 Oct 2017 14:51:44 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Babu", "Jasine", ""], ["Krishnan", "K. Murali", ""], ["Paleri", "Vineeth", ""]]}, {"id": "1708.05194", "submitter": "Dimitri Racordon", "authors": "Dimitri Racordon, Didier Buchs", "title": "Extracting Formal Specifications to Strenghten Type Behaviour Testing", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing has become an indispensable activity of software development, yet\nwriting good and relevant tests remains a quite challenging task. One\nwell-known problem is that it often is impossible or unrealistic to test for\nevery outcome, as the input and/or output of a program component can represent\nincredbly large, unless infinite domains. A common approach to tackle this\nissue it to only test classes of cases, and to assume that those classes cover\nall (or at least most) of the cases a component is susceptible to be exposed\nto. Unfortunately, those kind of assumptions can prove wrong in many\nsituations, causing a yet well-tested program to fail upon a particular input.\nIn this short paper, we propose to leverage formal verification, in particular\nmodel checking techniques, as a way to better identify cases for which the\naforementioned assumptions do not hold, and ultimately strenghten the\nconfidence one can have in a test suite. The idea is to extract a formal\nspecification of the data types of a program, in the form of a term rewriting\nsystem, and to check that specification against a set of properties specified\nby the programmer. Cases for which tose properties do not hold can then be\nidentified using model checking, and selected as test cases.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 10:23:32 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Racordon", "Dimitri", ""], ["Buchs", "Didier", ""]]}, {"id": "1708.05437", "submitter": "Abel Nieto", "authors": "Abel Nieto", "title": "Towards Algorithmic Typing for DOT", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dependent Object Types (DOT) calculus formalizes key features of Scala.\nThe D$_{<: }$ calculus is the core of DOT. To date, presentations of D$_{<: }$\nhave used declarative typing and subtyping rules, as opposed to algorithmic.\nUnfortunately, algorithmic typing for full D$_{<: }$ is known to be an\nundecidable problem.\n  We explore the design space for a restricted version of D$_{<: }$ that has\ndecidable typechecking. Even in this simplified D$_{<: }$ , algorithmic typing\nand subtyping are tricky, due to the \"bad bounds\" problem. The Scala compiler\nbypasses bad bounds at the cost of a loss in expressiveness in its type system.\nBased on the approach taken in the Scala compiler, we present the Step Typing\nand Step Subtyping relations for D$_{<: }$. We prove these relations sound and\ndecidable. They are not complete with respect to the original D$_{<: }$ rules.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 20:49:35 GMT"}, {"version": "v2", "created": "Tue, 22 Aug 2017 17:35:31 GMT"}, {"version": "v3", "created": "Wed, 27 Sep 2017 20:48:59 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Nieto", "Abel", ""]]}, {"id": "1708.05904", "submitter": "Kalev Alpernas", "authors": "Kalev Alpernas (1), Roman Manevich (2), Aurojit Panda (3), Mooly Sagiv\n  (1), Scott Shenker (4), Sharon Shoham (1), Yaron Velner (5) ((1) Tel Aviv\n  University, (2) Ben-Gurion University of the Negev, (3) NYU, (4) UC Berkeley,\n  (5) Hebrew University of Jerusalem)", "title": "Abstract Interpretation of Stateful Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern networks achieve robustness and scalability by maintaining states on\ntheir nodes. These nodes are referred to as middleboxes and are essential for\nnetwork functionality. However, the presence of middleboxes drastically\ncomplicates the task of network verification. Previous work showed that the\nproblem is undecidable in general and EXPSPACE-complete when abstracting away\nthe order of packet arrival.\n  We describe a new algorithm for conservatively checking isolation properties\nof stateful networks. The asymptotic complexity of the algorithm is polynomial\nin the size of the network, albeit being exponential in the maximal number of\nqueries of the local state that a middlebox can do, which is often small.\n  Our algorithm is sound, i.e., it can never miss a violation of safety but may\nfail to verify some properties. The algorithm performs on-the fly abstract\ninterpretation by (1) abstracting away the order of packet processing and the\nnumber of times each packet arrives, (2) abstracting away correlations between\nstates of different middleboxes and channel contents, and (3) representing\nmiddlebox states by their effect on each packet separately, rather than taking\ninto account the entire state space. We show that the abstractions do not lose\nprecision when middleboxes may reset in any state. This is encouraging since\nmany real middleboxes reset, e.g., after some session timeout is reached or due\nto hardware failure.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 22:06:47 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 12:38:31 GMT"}, {"version": "v3", "created": "Wed, 4 Jul 2018 16:00:56 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Alpernas", "Kalev", ""], ["Manevich", "Roman", ""], ["Panda", "Aurojit", ""], ["Sagiv", "Mooly", ""], ["Shenker", "Scott", ""], ["Shoham", "Sharon", ""], ["Velner", "Yaron", ""]]}, {"id": "1708.06799", "submitter": "Barak Pearlmutter", "authors": "Jeffrey Mark Siskind and Barak A. Pearlmutter", "title": "Divide-and-Conquer Checkpointing for Arbitrary Programs with No User\n  Annotation", "comments": null, "journal-ref": "Optimization Methods and Software 33(04-06):1288-1330, 2018", "doi": "10.1080/10556788.2018.1459621", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical reverse-mode automatic differentiation (AD) imposes only a small\nconstant-factor overhead in operation count over the original computation, but\nhas storage requirements that grow, in the worst case, in proportion to the\ntime consumed by the original computation. This storage blowup can be\nameliorated by checkpointing, a process that reorders application of classical\nreverse-mode AD over an execution interval to tradeoff space \\vs\\ time.\nApplication of checkpointing in a divide-and-conquer fashion to strategically\nchosen nested execution intervals can break classical reverse-mode AD into\nstages which can reduce the worst-case growth in storage from linear to\nsublinear. Doing this has been fully automated only for computations of\nparticularly simple form, with checkpoints spanning execution intervals\nresulting from a limited set of program constructs. Here we show how the\ntechnique can be automated for arbitrary computations. The essential innovation\nis to apply the technique at the level of the language implementation itself,\nthus allowing checkpoints to span any execution interval.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 20:00:41 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 20:53:50 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Siskind", "Jeffrey Mark", ""], ["Pearlmutter", "Barak A.", ""]]}, {"id": "1708.06887", "submitter": "EPTCS", "authors": "Alexei Lisitsa (University of Liverpool, UK), Andrei P. Nemytykh\n  (ISPRAS, Russia), Maurizio Proietti (CNR-IASI, Italy)", "title": "Proceedings Fifth International Workshop on Verification and Program\n  Transformation", "comments": null, "journal-ref": "EPTCS 253, 2017", "doi": "10.4204/EPTCS.253", "report-no": null, "categories": "cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of the Fifth International Workshop on\nVerification and Program Transformation (VPT 2017). The workshop took place in\nUppsala, Sweden, on April 29th, 2017, affiliated with the European Joint\nConferences on Theory and Practice of Software (ETAPS). The aim of the VPT\nworkshop series is to provide a forum where people from the areas of program\ntransformation and program verification can fruitfully exchange ideas and gain\na deeper understanding of the interactions between those two fields. Seven\npapers were presented at the workshop. Additionally, three invited talks were\ngiven by Javier Esparza (Technische Universit\\\"at M\\\"unchen, Germany), Manuel\nHermenegildo (IMDEA Software Institute, Madrid, Spain), and Alexey Khoroshilov\n(Linux Verification Center, ISPRAS, Moscow, Russia).\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 05:39:02 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Lisitsa", "Alexei", "", "University of Liverpool, UK"], ["Nemytykh", "Andrei P.", "", "ISPRAS, Russia"], ["Proietti", "Maurizio", "", "CNR-IASI, Italy"]]}, {"id": "1708.07081", "submitter": "Samer Abdallah", "authors": "Samer Abdallah", "title": "More declarative tabling in Prolog using multi-prompt delimited control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several Prolog implementations include a facility for tabling, an alternative\nresolution strategy which uses memoisation to avoid redundant duplication of\ncomputations. Until relatively recently, tabling has required either low-level\nsupport in the underlying Prolog engine, or extensive program transormation (de\nGuzman et al., 2008). An alternative approach is to augment Prolog with low\nlevel support for continuation capturing control operators, particularly\ndelimited continuations, which have been investigated in the field of\nfunctional programming and found to be capable of supporting a wide variety of\ncomputational effects within an otherwise declarative language.\n  This technical report describes an implementation of tabling in SWI Prolog\nbased on delimited control operators for Prolog recently introduced by\nSchrijvers et al. (2013). In comparison with a previous implementation of\ntabling for SWI Prolog using delimited control (Desouter et al., 2015), this\napproach, based on the functional memoising parser combinators of Johnson\n(1995), stays closer to the declarative core of Prolog, requires less code, and\nis able to deliver solutions from systems of tabled predicates incrementally\n(as opposed to finding all solutions before delivering any to the rest of the\nprogram).\n  A collection of benchmarks shows that a small number of carefully targeted\noptimisations yields performance within a factor of about 2 of the optimised\nversion of Desouter et al.'s system currently included in SWI Prolog.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 16:26:15 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 08:22:39 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Abdallah", "Samer", ""]]}, {"id": "1708.07222", "submitter": "EPTCS", "authors": "Vincent Nys (KU Leuven), Danny De Schreye (KU Leuven)", "title": "Transforming Coroutining Logic Programs into Equivalent CHR Programs", "comments": "In Proceedings VPT 2017, arXiv:1708.06887", "journal-ref": "EPTCS 253, 2017, pp. 9-35", "doi": "10.4204/EPTCS.253.4", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend a technique called Compiling Control. The technique transforms\ncoroutining logic programs into logic programs that, when executed under the\nstandard left-to-right selection rule (and not using any delay features) have\nthe same computational behavior as the coroutining program. In recent work, we\nrevised Compiling Control and reformulated it as an instance of Abstract\nConjunctive Partial Deduction. This work was mostly focused on the program\nanalysis performed in Compiling Control. In the current paper, we focus on the\nsynthesis of the transformed program. Instead of synthesizing a new logic\nprogram, we synthesize a CHR(Prolog) program which mimics the coroutining\nprogram. The synthesis to CHR yields programs containing only simplification\nrules, which are particularly amenable to certain static analysis techniques.\nThe programs are also more concise and readable and can be ported to CHR\nimplementations embedded in other languages than Prolog.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 00:18:39 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Nys", "Vincent", "", "KU Leuven"], ["De Schreye", "Danny", "", "KU Leuven"]]}, {"id": "1708.07224", "submitter": "EPTCS", "authors": "Gyula Sallai (Department of Measurement and Information Systems,\n  Budapest University of Technology and Economics), \\'Akos Hajdu (Department of\n  Measurement and Information Systems, Budapest University of Technology and\n  Economics / MTA-BME Lend\\\"ulet Cyber-Physical Systems Research Group),\n  Tam\\'as T\\'oth (Department of Measurement and Information Systems, Budapest\n  University of Technology and Economics), Zolt\\'an Micskei (Department of\n  Measurement and Information Systems, Budapest University of Technology and\n  Economics)", "title": "Towards Evaluating Size Reduction Techniques for Software Model Checking", "comments": "In Proceedings VPT 2017, arXiv:1708.06887", "journal-ref": "EPTCS 253, 2017, pp. 75-91", "doi": "10.4204/EPTCS.253.7", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal verification techniques are widely used for detecting design flaws in\nsoftware systems. Formal verification can be done by transforming an already\nimplemented source code to a formal model and attempting to prove certain\nproperties of the model (e.g. that no erroneous state can occur during\nexecution). Unfortunately, transformations from source code to a formal model\noften yield large and complex models, making the verification process\ninefficient and costly. In order to reduce the size of the resulting model,\noptimization transformations can be used. Such optimizations include common\nalgorithms known from compiler design and different program slicing techniques.\nOur paper describes a framework for transforming C programs to a formal model,\nenhanced by various optimizations for size reduction. We evaluate and compare\nseveral optimization algorithms regarding their effect on the size of the model\nand the efficiency of the verification. Results show that different\noptimizations are more suitable for certain models, justifying the need for a\nframework that includes several algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 00:19:33 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Sallai", "Gyula", "", "Department of Measurement and Information Systems,\n  Budapest University of Technology and Economics"], ["Hajdu", "\u00c1kos", "", "Department of\n  Measurement and Information Systems, Budapest University of Technology and\n  Economics / MTA-BME Lend\u00fclet Cyber-Physical Systems Research Group"], ["T\u00f3th", "Tam\u00e1s", "", "Department of Measurement and Information Systems, Budapest\n  University of Technology and Economics"], ["Micskei", "Zolt\u00e1n", "", "Department of\n  Measurement and Information Systems, Budapest University of Technology and\n  Economics"]]}, {"id": "1708.07225", "submitter": "EPTCS", "authors": "D\\'aniel Horp\\'acsi, Judit K\\H{o}szegi, Zolt\\'an Horv\\'ath", "title": "Trustworthy Refactoring via Decomposition and Schemes: A Complex Case\n  Study", "comments": "In Proceedings VPT 2017, arXiv:1708.06887", "journal-ref": "EPTCS 253, 2017, pp. 92-108", "doi": "10.4204/EPTCS.253.8", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Widely used complex code refactoring tools lack a solid reasoning about the\ncorrectness of the transformations they implement, whilst interest in proven\ncorrect refactoring is ever increasing as only formal verification can provide\ntrue confidence in applying tool-automated refactoring to industrial-scale\ncode. By using our strategic rewriting based refactoring specification\nlanguage, we present the decomposition of a complex transformation into smaller\nsteps that can be expressed as instances of refactoring schemes, then we\ndemonstrate the semi-automatic formal verification of the components based on a\ntheoretical understanding of the semantics of the programming language. The\nextensible and verifiable refactoring definitions can be executed in our\ninterpreter built on top of a static analyser framework.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 00:19:52 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Horp\u00e1csi", "D\u00e1niel", ""], ["K\u0151szegi", "Judit", ""], ["Horv\u00e1th", "Zolt\u00e1n", ""]]}, {"id": "1708.07226", "submitter": "EPTCS", "authors": "Allan Blanchard (Univ. Orl\\'eans, INSA Centre Val de Loire, France),\n  Fr\\'ed\\'eric Loulergue (Northern Arizona University, School of Informatics\n  Computing and Cyber Systems, Flagstaff, USA), Nikolai Kosmatov (Software\n  Reliability Laboratory, CEA LIST, France)", "title": "From Concurrent Programs to Simulating Sequential Programs: Correctness\n  of a Transformation", "comments": "In Proceedings VPT 2017, arXiv:1708.06887", "journal-ref": "EPTCS 253, 2017, pp. 109-123", "doi": "10.4204/EPTCS.253.9", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frama-C is a software analysis framework that provides a common\ninfrastructure and a common behavioral specification language to plugins that\nimplement various static and dynamic analyses of C programs. Most plugins do\nnot support concurrency. We have proposed Conc2Seq, a Frama-C plugin based on\nprogram transformation, capable to leverage the existing huge code base of\nplugins and to handle concurrent C programs.\n  In this paper we formalize and sketch the proof of correctness of the program\ntransformation principle behind Conc2Seq, and present an effort towards the\nfull mechanization of both the formalization and proofs with the proof\nassistant Coq.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 00:20:08 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Blanchard", "Allan", "", "Univ. Orl\u00e9ans, INSA Centre Val de Loire, France"], ["Loulergue", "Fr\u00e9d\u00e9ric", "", "Northern Arizona University, School of Informatics\n  Computing and Cyber Systems, Flagstaff, USA"], ["Kosmatov", "Nikolai", "", "Software\n  Reliability Laboratory, CEA LIST, France"]]}, {"id": "1708.07233", "submitter": "EPTCS", "authors": "Ian Cassar (Reykjavik University), Adrian Francalanza (University of\n  Malta), Claudio Antares Mezzina (IMT School for Advanced Studies Lucca,\n  Italy), Emilio Tuosto (University of Leicester, UK)", "title": "Reliability and Fault-Tolerance by Choreographic Design", "comments": "In Proceedings PrePost 2017, arXiv:1708.06889", "journal-ref": "EPTCS 254, 2017, pp. 69-80", "doi": "10.4204/EPTCS.254.6", "report-no": null, "categories": "cs.PL cs.DC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed programs are hard to get right because they are required to be\nopen, scalable, long-running, and tolerant to faults. In particular, the recent\napproaches to distributed software based on (micro-)services where different\nservices are developed independently by disparate teams exacerbate the problem.\nIn fact, services are meant to be composed together and run in open context\nwhere unpredictable behaviours can emerge. This makes it necessary to adopt\nsuitable strategies for monitoring the execution and incorporate recovery and\nadaptation mechanisms so to make distributed programs more flexible and robust.\nThe typical approach that is currently adopted is to embed such mechanisms in\nthe program logic, which makes it hard to extract, compare and debug. We\npropose an approach that employs formal abstractions for specifying failure\nrecovery and adaptation strategies. Although implementation agnostic, these\nabstractions would be amenable to algorithmic synthesis of code, monitoring and\ntests. We consider message-passing programs (a la Erlang, Go, or MPI) that are\ngaining momentum both in academia and industry. Our research agenda consists of\n(1) the definition of formal behavioural models encompassing failures, (2) the\nspecification of the relevant properties of adaptation and recovery strategy,\n(3) the automatic generation of monitoring, recovery, and adaptation logic in\ntarget languages of interest.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 00:39:55 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Cassar", "Ian", "", "Reykjavik University"], ["Francalanza", "Adrian", "", "University of\n  Malta"], ["Mezzina", "Claudio Antares", "", "IMT School for Advanced Studies Lucca,\n  Italy"], ["Tuosto", "Emilio", "", "University of Leicester, UK"]]}, {"id": "1708.07366", "submitter": "Martin Sulzmann", "authors": "Martin Sulzmann and Peter Thiemann", "title": "A Computational Interpretation of Context-Free Expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We phrase parsing with context-free expressions as a type inhabitation\nproblem where values are parse trees and types are context-free expressions. We\nfirst show how containment among context-free and regular expressions can be\nreduced to a reachability problem by using a canonical representation of\nstates. The proofs-as-programs principle yields a computational interpretation\nof the reachability problem in terms of a coercion that transforms the parse\ntree for a context-free expression into a parse tree for a regular expression.\nIt also yields a partial coercion from regular parse trees to context-free\nones. The partial coercion from the trivial language of all words to a\ncontext-free expression corresponds to a predictive parser for the expression.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 12:02:28 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Sulzmann", "Martin", ""], ["Thiemann", "Peter", ""]]}, {"id": "1708.07583", "submitter": "Eric Seidel", "authors": "Eric L. Seidel, Huma Sibghat, Kamalika Chaudhuri, Westley Weimer,\n  Ranjit Jhala", "title": "Learning to Blame: Localizing Novice Type Errors with Data-Driven\n  Diagnosis", "comments": "OOPSLA '17", "journal-ref": null, "doi": "10.1145/3133884", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localizing type errors is challenging in languages with global type\ninference, as the type checker must make assumptions about what the programmer\nintended to do. We introduce Nate, a data-driven approach to error localization\nbased on supervised learning. Nate analyzes a large corpus of training data --\npairs of ill-typed programs and their \"fixed\" versions -- to automatically\nlearn a model of where the error is most likely to be found. Given a new\nill-typed program, Nate executes the model to generate a list of potential\nblame assignments ranked by likelihood. We evaluate Nate by comparing its\nprecision to the state of the art on a set of over 5,000 ill-typed OCaml\nprograms drawn from two instances of an introductory programming course. We\nshow that when the top-ranked blame assignment is considered, Nate's\ndata-driven model is able to correctly predict the exact sub-expression that\nshould be changed 72% of the time, 28 points higher than OCaml and 16 points\nhigher than the state-of-the-art SHErrLoc tool. Furthermore, Nate's accuracy\nsurpasses 85% when we consider the top two locations and reaches 91% if we\nconsider the top three.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 00:34:24 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 00:39:45 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Seidel", "Eric L.", ""], ["Sibghat", "Huma", ""], ["Chaudhuri", "Kamalika", ""], ["Weimer", "Westley", ""], ["Jhala", "Ranjit", ""]]}, {"id": "1708.07854", "submitter": "John Gallagher", "authors": "Fabio Fioravanti and John P. Gallagher", "title": "Pre-proceedings of the 27th International Symposium on Logic-Based\n  Program Synthesis and Transformation (LOPSTR 2017)", "comments": "Papers selected for presentation at LOPSTR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume constitutes the pre-proceedings of the 27th International\nSymposium on Logic-Based Program Synthesis and Transformation (LOPSTR 2016),\nheld on 10-12th October 2017 in Namur, Belgium, and co-located with the 19th\nInternational Symposium on Principles and Practice of Declarative Programming\n(PPDP 2017). After discussion at the symposium papers will go through a second\nround of refereeing and selection for the formal proceedings.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 14:54:20 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 11:57:03 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Fioravanti", "Fabio", ""], ["Gallagher", "John P.", ""]]}, {"id": "1708.08003", "submitter": "Jos\\'e Mar\\'ia Rey Poza", "authors": "Jos\\'e Mar\\'ia Rey-Poza, Julio Mari\\~no-Carballo", "title": "The Unfolding Semantics of Functional Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea of using unfolding as a way of computing a program semantics has\nbeen applied successfully to logic programs and has shown itself a powerful\ntool that provides concrete, implementable results, as its outcome is actually\nsource code. Thus, it can be used for characterizing not-so-declarative\nconstructs in mostly declarative languages, or for static analysis. However,\nunfolding-based semantics has not yet been applied to higher-order, lazy\nfunctional programs, perhaps because some functional features absent in logic\nprograms make the correspondence between execution and unfolding not as\nstraightforward. This work presents an unfolding semantics for higher-order,\nlazy functional programs and proves its adequacy with respect to a given\noperational semantics. Finally, we introduce some applications of our\nsemantics.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 18:00:15 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Rey-Poza", "Jos\u00e9 Mar\u00eda", ""], ["Mari\u00f1o-Carballo", "Julio", ""]]}, {"id": "1708.08021", "submitter": "Panagiotis Vekris", "authors": "Avik Chaudhuri, Panagiotis Vekris, Sam Goldman, Marshall Roch, Gabriel\n  Levi", "title": "Fast and Precise Type Checking for JavaScript", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the design and implementation of Flow, a fast and\nprecise type checker for JavaScript that is used by thousands of developers on\nmillions of lines of code at Facebook every day. Flow uses sophisticated type\ninference to understand common JavaScript idioms precisely. This helps it find\nnon-trivial bugs in code and provide code intelligence to editors without\nrequiring significant rewriting or annotations from the developer. We formalize\nan important fragment of Flow's analysis and prove its soundness. Furthermore,\nFlow uses aggressive parallelization and incrementalization to deliver\nnear-instantaneous response times. This helps it avoid introducing any latency\nin the usual edit-refresh cycle of rapid JavaScript development. We describe\nthe algorithms and systems infrastructure that we built to scale Flow's\nanalysis.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 23:17:36 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 08:06:39 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Chaudhuri", "Avik", ""], ["Vekris", "Panagiotis", ""], ["Goldman", "Sam", ""], ["Roch", "Marshall", ""], ["Levi", "Gabriel", ""]]}, {"id": "1708.08319", "submitter": "Jim Pivarski", "authors": "Jim Pivarski, Peter Elmer, Brian Bockelman, Zhe Zhang", "title": "Fast Access to Columnar, Hierarchically Nested Data via Code\n  Transformation", "comments": "10 pages, 2 figures, submitted to IEEE Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Data query systems represent data in a columnar format for fast,\nselective access, and in some cases (e.g. Apache Drill), perform calculations\ndirectly on the columnar data without row materialization, avoiding runtime\ncosts.\n  However, many analysis procedures cannot be easily or efficiently expressed\nas SQL. In High Energy Physics, the majority of data processing requires nested\nloops with complex dependencies. When faced with tasks like these, the\nconventional approach is to convert the columnar data back into an object form,\nusually with a performance price.\n  This paper describes a new technique to transform procedural code so that it\noperates on hierarchically nested, columnar data natively, without row\nmaterialization. It can be viewed as a compiler pass on the typed abstract\nsyntax tree, rewriting references to objects as columnar array lookups.\n  We will also present performance comparisons between transformed code and\nconventional object-oriented code in a High Energy Physics context.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 23:41:13 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 16:02:33 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Pivarski", "Jim", ""], ["Elmer", "Peter", ""], ["Bockelman", "Brian", ""], ["Zhang", "Zhe", ""]]}, {"id": "1708.08323", "submitter": "Liangze Yin", "authors": "Liangze Yin, Wei Dong, Wanwei Liu, Ji Wang", "title": "Scheduling Constraint Based Abstraction Refinement for Multi-Threaded\n  Program Verification", "comments": "27 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bounded model checking is among the most efficient techniques for the\nautomatic verification of concurrent programs. However, encoding all possible\ninterleavings often requires a huge and complex formula, which significantly\nlimits the salability. This paper proposes a novel and efficient abstraction\nrefinement method for multi-threaded program verification. Observing that the\nhuge formula is usually dominated by the exact encoding of the scheduling\nconstraint, this paper proposes a \\tsc based abstraction refinement method,\nwhich avoids the huge and complex encoding of BMC. In addition, to obtain an\neffective refinement, we have devised two graph-based algorithms over event\norder graph for counterexample validation and refinement generation, which can\nalways obtain a small yet effective refinement constraint. Enhanced by two\nconstraint-based algorithms for counterexample validation and refinement\ngeneration, we have proved that our method is sound and complete w.r.t. the\ngiven loop unwinding depth. Experimental results on \\svcompc benchmarks\nindicate that our method is promising and significantly outperforms the\nexisting state-of-the-art tools.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 07:00:28 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 11:26:25 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Yin", "Liangze", ""], ["Dong", "Wei", ""], ["Liu", "Wanwei", ""], ["Wang", "Ji", ""]]}, {"id": "1708.08340", "submitter": "Niklas Grimm", "authors": "V\\'eronique Cortier, Niklas Grimm, Joseph Lallemand, Matteo Maffei", "title": "A Type System for Privacy Properties (Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mature push button tools have emerged for checking trace properties (e.g.\nsecrecy or authentication) of security protocols. The case of\nindistinguishability-based privacy properties (e.g. ballot privacy or\nanonymity) is more complex and constitutes an active research topic with\nseveral recent propositions of techniques and tools. We explore a novel\napproach based on type systems and provide a (sound) type system for proving\nequivalence of protocols, for a bounded or an unbounded number of sessions. The\nresulting prototype implementation has been tested on various protocols of the\nliterature. It provides a significant speed-up (by orders of magnitude)\ncompared to tools for a bounded number of sessions and complements in terms of\nexpressiveness other state-of-the-art tools, such as ProVerif and Tamarin:\ne.g., we show that our analysis technique is the first one to handle a faithful\nencoding of the Helios e-voting protocol in the context of an untrusted ballot\nbox.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 14:36:05 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Cortier", "V\u00e9ronique", ""], ["Grimm", "Niklas", ""], ["Lallemand", "Joseph", ""], ["Maffei", "Matteo", ""]]}, {"id": "1708.08596", "submitter": "Ethan Cecchetti", "authors": "Ethan Cecchetti and Andrew C. Myers and Owen Arden", "title": "Nonmalleable Information Flow: Technical Report", "comments": null, "journal-ref": null, "doi": "10.1145/3133956.3134054", "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noninterference is a popular semantic security condition because it offers\nstrong end-to-end guarantees, it is inherently compositional, and it can be\nenforced using a simple security type system. Unfortunately, it is too\nrestrictive for real systems. Mechanisms for downgrading information are needed\nto capture real-world security requirements, but downgrading eliminates the\nstrong compositional security guarantees of noninterference.\n  We introduce nonmalleable information flow, a new formal security condition\nthat generalizes noninterference to permit controlled downgrading of both\nconfidentiality and integrity. While previous work on robust declassification\nprevents adversaries from exploiting the downgrading of confidentiality, our\nkey insight is transparent endorsement, a mechanism for downgrading integrity\nwhile defending against adversarial exploitation. Robust declassification\nappeared to break the duality of confidentiality and integrity by making\nconfidentiality depend on integrity, but transparent endorsement makes\nintegrity depend on confidentiality, restoring this duality. We show how to\nextend a security-typed programming language with transparent endorsement and\nprove that this static type system enforces nonmalleable information flow, a\nnew security property that subsumes robust declassification and transparent\nendorsement. Finally, we describe an implementation of this type system in the\ncontext of Flame, a flow-limited authorization plugin for the Glasgow Haskell\nCompiler.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 06:06:29 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 17:51:12 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Cecchetti", "Ethan", ""], ["Myers", "Andrew C.", ""], ["Arden", "Owen", ""]]}, {"id": "1708.08731", "submitter": "Matthias H\\\"oschele", "authors": "Matthias H\\\"oschele, Alexander Kampmann, Andreas Zeller", "title": "Active Learning of Input Grammars", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing the precise format of a program's input is a necessary prerequisite\nfor systematic testing. Given a program and a small set of sample inputs, we\n(1) track the data flow of inputs to aggregate input fragments that share the\nsame data flow through program execution into lexical and syntactic entities;\n(2) assign these entities names that are based on the associated variable and\nfunction identifiers; and (3) systematically generalize production rules by\nmeans of membership queries. As a result, we need only a minimal set of sample\ninputs to obtain human-readable context-free grammars that reflect valid input\nstructure. In our evaluation on inputs like URLs, spreadsheets, or\nconfiguration files, our AUTOGRAM prototype obtains input grammars that are\nboth accurate and very readable - and that can be directly fed into test\ngenerators for comprehensive automated testing.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 13:02:44 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["H\u00f6schele", "Matthias", ""], ["Kampmann", "Alexander", ""], ["Zeller", "Andreas", ""]]}, {"id": "1708.08895", "submitter": "Lucas Waye", "authors": "Lucas Waye, Pablo Buiras, Owen Arden, Alejandro Russo, Stephen Chong", "title": "Cryptographically Secure Information Flow Control on Key-Value Stores", "comments": "Full version of conference paper appearing in CCS 2017", "journal-ref": null, "doi": "10.1145/3133956.3134036", "report-no": null, "categories": "cs.CR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Clio, an information flow control (IFC) system that transparently\nincorporates cryptography to enforce confidentiality and integrity policies on\nuntrusted storage. Clio insulates developers from explicitly manipulating keys\nand cryptographic primitives by leveraging the policy language of the IFC\nsystem to automatically use the appropriate keys and correct cryptographic\noperations. We prove that Clio is secure with a novel proof technique that is\nbased on a proof style from cryptography together with standard programming\nlanguages results. We present a prototype Clio implementation and a case study\nthat demonstrates Clio's practicality.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 17:25:58 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Waye", "Lucas", ""], ["Buiras", "Pablo", ""], ["Arden", "Owen", ""], ["Russo", "Alejandro", ""], ["Chong", "Stephen", ""]]}, {"id": "1708.09002", "submitter": "EPTCS", "authors": "Alexei P. Lisitsa (Department of Computer Science, The University of\n  Liverpool), Andrei P. Nemytykh (Program Systems Institute, Russian Academy of\n  Sciences)", "title": "Verification of Programs via Intermediate Interpretation", "comments": "In Proceedings VPT 2017, arXiv:1708.06887. The author's extended\n  version is arXiv:1705.06738", "journal-ref": "EPTCS 253, 2017, pp. 54-74", "doi": "10.4204/EPTCS.253.6", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore an approach to verification of programs via program transformation\napplied to an interpreter of a programming language. A specialization technique\nknown as Turchin's supercompilation is used to specialize some interpreters\nwith respect to the program models. We show that several safety properties of\nfunctional programs modeling a class of cache coherence protocols can be proved\nby a supercompiler and compare the results with our earlier work on direct\nverification via supercompilation not using intermediate interpretation.\n  Our approach was in part inspired by an earlier work by E. De Angelis et al.\n(2014-2015) where verification via program transformation and intermediate\ninterpretation was studied in the context of specialization of constraint logic\nprograms.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 00:19:15 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Lisitsa", "Alexei P.", "", "Department of Computer Science, The University of\n  Liverpool"], ["Nemytykh", "Andrei P.", "", "Program Systems Institute, Russian Academy of\n  Sciences"]]}, {"id": "1708.09158", "submitter": "Tyng-Ruey Chuang", "authors": "Ting-Yan Lai (1), Tyng-Ruey Chuang (1), Shin-Cheng Mu (1) ((1)\n  Institute of Information Science, Academia Sinica, Taiwan)", "title": "Type Safe Redis Queries: A Case Study of Type-Level Programming in\n  Haskell", "comments": "This work is to be presented at the 2nd Workshop on Type-Driven\n  Development (TyDe 2017), September 3, 2017, Oxford, UK. This paper is not\n  included in the workshop proceedings published by the ACM. The authors choose\n  to place it in the public domain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Redis is an in-memory data structure store, often used as a database, with a\nHaskell interface Hedis. Redis is dynamically typed --- a key can be discarded\nand re-associated to a value of a different type, and a command, when fetching\na value of a type it does not expect, signals a runtime error. We develop a\ndomain-specific language that, by exploiting Haskell type-level programming\ntechniques including indexed monad, type-level literals and closed type\nfamilies, keeps track of types of values in the database and statically\nguarantees that type errors cannot happen for a class of Redis programs.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 08:26:44 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Lai", "Ting-Yan", ""], ["Chuang", "Tyng-Ruey", ""], ["Mu", "Shin-Cheng", ""]]}]