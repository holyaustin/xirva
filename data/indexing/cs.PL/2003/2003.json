[{"id": "2003.00151", "submitter": "John Demme PhD", "authors": "John Demme", "title": "A Compiler Infrastructure for FPGA and ASIC Development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This whitepaper proposes a unified framework for hardware design tools to\nease the development and inter-operability of said tools. By creating a large\necosystem of hardware development tools across vendors, academia, and the open\nsource community, we hope to significantly increase much need productivity in\nhardware design.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 01:52:00 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Demme", "John", ""]]}, {"id": "2003.00175", "submitter": "Daliang Xu", "authors": "Daliang Xu and Dongwei Chen and Chun Yang and KangSun and Xu Cheng and\n  Dong Tong", "title": "DangKiller: Eliminating Dangling Pointers Efficiently via Implicit\n  Identifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Use-After-Free vulnerabilities, allowing the attacker to access unintended\nmemory via dangling pointers, are more threatening. However, most detection\nschemes can only detect dangling pointers and invalid them, but not provide a\ntolerance mechanism to repair the errors at runtime. Also, these techniques\nobtain and manage the metadata inefficiently with complex structures and too\nmuch scan (sweep). The goal of this paper is to use compiler instrumentation to\neliminate dangling pointers automatically and efficiently. In this paper, we\nobserve that most techniques lack accurate efficient pointer graph metadata\nmaintaining methods, so they need to scan the log to reduce the redundancy and\nsweep the whole address space to find dangling pointers. Also, they lack a\ndirect, efficiently obtaining metadata approach. The key insight of this paper\nis that a unique identifier can be used as a key to a hash or direct-map\nalgorithm. Thus, this paper maintains the same implicit identifier with each\nmemory object and its corresponding referent. Associating the unique ID with\nmetadata for memory objects, obtaining and managing the pointer graph metadata\ncan be efficiently. Therefore, with the delayed free technique adopted into\nC/C++, we present the DangKiller as a novel and lightweight dangling pointer\nelimination solution. We first demonstrate the MinFat Pointer, which can\ncalculate unique implicit ID for each object and pointer quickly, and use hash\nalgorithm to obtain metadata. Secondly, we propose the Log Cache and Log\nCompression mechanism based on the ID to decrease the redundancy of dangling\npointer candidates. Coupled with the Address Tagging architecture on an ARM64\nsystem, our experiments show that the DangKiller can eliminate use-after-free\nvulnerabilities at only 11% and 3% runtime overheads for the SPEC CPU2006 and\n2017 benchmarks respectively, except for unique cases.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 04:11:27 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Xu", "Daliang", ""], ["Chen", "Dongwei", ""], ["Yang", "Chun", ""], ["KangSun", "", ""], ["Cheng", "Xu", ""], ["Tong", "Dong", ""]]}, {"id": "2003.00290", "submitter": "Gus Henry Smith", "authors": "Gus Smith, Zachary Tatlock and Luis Ceze (University of Washington)", "title": "Enumerating Hardware-Software Splits with Program Rewriting", "comments": "Accepted in the Second Young Architect Workshop, in conjunction with\n  ASPLOS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A core problem in hardware-software codesign is in the sheer size of the\ndesign space. Without a set ISA to constrain the hardware-software interface,\nthe design space explodes. This work presents a strategy for managing the\nmassive hardware-software design space within the domain of machine learning\ninference workloads and accelerators. We first propose EngineIR, a new language\nfor representing machine learning hardware and software in a single program.\nThen, using equality graphs -- a data structure from the compilers literature\n-- we suggest a method for efficiently enumerating the design space by\nperforming rewrites over our representation.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 16:15:51 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Smith", "Gus", "", "University of Washington"], ["Tatlock", "Zachary", "", "University of Washington"], ["Ceze", "Luis", "", "University of Washington"]]}, {"id": "2003.00296", "submitter": "Massimo Bartoletti", "authors": "Massimo Bartoletti, Maurizio Murgia, and Roberto Zunino", "title": "Renegotiation and recursion in Bitcoin contracts", "comments": "Full version of the paper presented at COORDINATION 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BitML is a process calculus to express smart contracts that can be run on\nBitcoin. One of its current limitations is that, once a contract has been\nstipulated, the participants cannot renegotiate its terms: this prevents\nexpressing common financial contracts, where funds have to be added by\nparticipants at run-time. In this paper, we extend BitML with a new primitive\nfor contract renegotiation. At the same time, the new primitive can be used to\nwrite recursive contracts, which was not possible in the original BitML. We\nshow that, despite the increased expressiveness, it is still possible to\nexecute BitML on standard Bitcoin, preserving the security guarantees of BitML.\n", "versions": [{"version": "v1", "created": "Sat, 29 Feb 2020 16:42:13 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 21:01:46 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Bartoletti", "Massimo", ""], ["Murgia", "Maurizio", ""], ["Zunino", "Roberto", ""]]}, {"id": "2003.00399", "submitter": "Fabio Petrillo", "authors": "Hugo Tremblay and Fabio Petrillo", "title": "The cross cyclomatic complexity: a bi-dimensional measure for program\n  complexity on graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reduce and control complexity is an essential practice in software design.\nCyclomatic complexity (CC) is one of the most popular software metrics, applied\nfor more than 40 years. Despite CC is an interesting metric to highlight the\nnumber of branches in a program, it clearly not sufficient to represent the\ncomplexity in a piece of software. In this paper, we introduce the cross\ncyclomatic complexity (CCC), a new bi-dimensional complexity measure on graphs\nthat combines the cyclomatic complexity and the weight of a minimum-weight\ncycle basis in as pair on the Cartesian plan to characterize program complexity\nusing control flow graphs. Our postulates open a new venue to represent program\ncomplexity, and we discuss its implications and opportunities.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 04:32:41 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Tremblay", "Hugo", ""], ["Petrillo", "Fabio", ""]]}, {"id": "2003.00473", "submitter": "Kees Middelburg", "authors": "C. A. Middelburg", "title": "Process algebra, process scheduling, and mutual exclusion", "comments": "15 pages, there is noticeable text overlap with earlier papers\n  (arXiv:1912.10041, arXiv:1703.06822); 15 pages, Section 3.2 improved; 15\n  pages, minor improvements including replacement of reference at end Section\n  3.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the case of multi-threading as found in contemporary programming\nlanguages, parallel processes are interleaved according to what is known as a\nprocess-scheduling policy in the field of operating systems. In a previous\npaper, we extend ACP with this form of interleaving. In the current paper, we\ndo so with the variant of ACP known as ACP$_\\epsilon$. The choice of\nACP$_\\epsilon$ stems from the need to cover more process-scheduling policies.\nWe show that a process-scheduling policy supporting mutual exclusion of\ncritical subprocesses is now covered.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 12:13:24 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 12:41:33 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 12:48:13 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Middelburg", "C. A.", ""]]}, {"id": "2003.00671", "submitter": "Qijing Huang", "authors": "Qijing Huang, Ameer Haj-Ali, William Moses, John Xiang, Ion Stoica,\n  Krste Asanovic, John Wawrzynek", "title": "AutoPhase: Juggling HLS Phase Orderings in Random Forests with Deep\n  Reinforcement Learning", "comments": "arXiv admin note: text overlap with arXiv:1901.04615", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of the code a compiler generates depends on the order in\nwhich it applies the optimization passes. Choosing a good order--often referred\nto as the phase-ordering problem, is an NP-hard problem. As a result, existing\nsolutions rely on a variety of heuristics. In this paper, we evaluate a new\ntechnique to address the phase-ordering problem: deep reinforcement learning.\nTo this end, we implement AutoPhase: a framework that takes a program and uses\ndeep reinforcement learning to find a sequence of compilation passes that\nminimizes its execution time. Without loss of generality, we construct this\nframework in the context of the LLVM compiler toolchain and target high-level\nsynthesis programs. We use random forests to quantify the correlation between\nthe effectiveness of a given pass and the program's features. This helps us\nreduce the search space by avoiding phase orderings that are unlikely to\nimprove the performance of a given program. We compare the performance of\nAutoPhase to state-of-the-art algorithms that address the phase-ordering\nproblem. In our evaluation, we show that AutoPhase improves circuit performance\nby 28% when compared to using the -O3 compiler flag, and achieves competitive\nresults compared to the state-of-the-art solutions, while requiring fewer\nsamples. Furthermore, unlike existing state-of-the-art solutions, our deep\nreinforcement learning solution shows promising result in generalizing to real\nbenchmarks and 12,874 different randomly generated programs, after training on\na hundred randomly generated programs.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 05:35:32 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 19:48:50 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Huang", "Qijing", ""], ["Haj-Ali", "Ameer", ""], ["Moses", "William", ""], ["Xiang", "John", ""], ["Stoica", "Ion", ""], ["Asanovic", "Krste", ""], ["Wawrzynek", "John", ""]]}, {"id": "2003.01331", "submitter": "Yuepeng Wang", "authors": "Yuepeng Wang, Rushi Shah, Abby Criswell, Rong Pan, Isil Dillig", "title": "Data Migration using Datalog Program Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new technique for migrating data between different\nschemas. Our method expresses the schema mapping as a Datalog program and\nautomatically synthesizes a Datalog program from simple input-output examples\nto perform data migration. This approach can transform data between different\ntypes of schemas (e.g., relational-to-graph, document-to-relational) and\nperforms synthesis efficiently by leveraging the semantics of Datalog. We\nimplement the proposed technique as a tool called Dynamite and show its\neffectiveness by evaluating Dynamite on 28 realistic data migration scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 04:48:40 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Wang", "Yuepeng", ""], ["Shah", "Rushi", ""], ["Criswell", "Abby", ""], ["Pan", "Rong", ""], ["Dillig", "Isil", ""]]}, {"id": "2003.01422", "submitter": "W{\\l}odzimierz Drabent", "authors": "W{\\l}odzimierz Drabent", "title": "The Prolog Debugger and Declarative Programming. Examples", "comments": "11 pages, 8 figures (an example added to the previous version + a few\n  corrections)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contains examples for a companion paper \"The Prolog Debugger and\nDeclarative Programming\", which discusses (in)adequacy of the Prolog debugger\nfor declarative programming.\n  Logic programming is a declarative programming paradigm. Programming language\nProlog makes logic programming possible, at least to a substantial extent.\nHowever the Prolog debugger works solely in terms of the operational semantics.\nSo it is incompatible with declarative programming. The companion paper tries\nto find methods of using it from the declarative point of view. Here we provide\nexamples of applying them.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 10:16:52 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 21:01:01 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Drabent", "W\u0142odzimierz", ""]]}, {"id": "2003.01685", "submitter": "Daniel Selsam", "authors": "Daniel Selsam, Simon Hudon and Leonardo de Moura", "title": "Sealing Pointer-Based Optimizations Behind Pure Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional programming languages are particularly well-suited for building\nautomated reasoning systems, since (among other reasons) a logical term is well\nmodeled by an inductive type, traversing a term can be implemented generically\nas a higher-order combinator, and backtracking is dramatically simplified by\npersistent datastructures. However, existing pure functional programming\nlanguages all suffer a major limitation in these domains: traversing a term\nrequires time proportional to the tree size of the term as opposed to its graph\nsize. This limitation would be particularly devastating when building\nautomation for interactive theorem provers such as Lean and Coq, for which the\nexponential blowup of term-tree sizes has proved to be both common and\ndifficult to prevent. All that is needed to recover the optimal scaling is the\nability to perform simple operations on the memory addresses of terms, and yet\nallowing these operations to be used freely would clearly violate the basic\npremise of referential transparency. We show how to use dependent types to seal\nthe necessary pointer-address manipulations behind pure functional interfaces\nwhile requiring only a negligible amount of additional trust. We have\nimplemented our approach for the upcoming version (v4) of Lean, and our\napproach could be adopted by other languages based on dependent type theory as\nwell.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 18:10:45 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 01:13:02 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Selsam", "Daniel", ""], ["Hudon", "Simon", ""], ["de Moura", "Leonardo", ""]]}, {"id": "2003.02110", "submitter": "Danel Ahman", "authors": "Danel Ahman, Matija Pretnar", "title": "Asynchronous Effects", "comments": "POPL 2021 camera ready version", "journal-ref": null, "doi": "10.1145/3434305", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We explore asynchronous programming with algebraic effects. We complement\ntheir conventional synchronous treatment by showing how to naturally also\naccommodate asynchrony within them, namely, by decoupling the execution of\noperation calls into signalling that an operation's implementation needs to be\nexecuted, and interrupting a running computation with the operation's result,\nto which the computation can react by installing interrupt handlers. We\nformalise these ideas in a small core calculus, called $\\lambda_{\\text{ae}}$.\nWe demonstrate the flexibility of $\\lambda_{\\text{ae}}$ using examples ranging\nfrom a multi-party web application, to preemptive multi-threading, to remote\nfunction calls, to a parallel variant of runners of algebraic effects. In\naddition, the paper is accompanied by a formalisation of\n$\\lambda_{\\text{ae}}$'s type safety proofs in Agda, and a prototype\nimplementation of $\\lambda_{\\text{ae}}$ in OCaml.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 14:50:32 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 14:26:45 GMT"}, {"version": "v3", "created": "Sat, 14 Nov 2020 20:04:48 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Ahman", "Danel", ""], ["Pretnar", "Matija", ""]]}, {"id": "2003.02191", "submitter": "Simon Fowler", "authors": "Rudi Horn, Simon Fowler, and James Cheney", "title": "Language-Integrated Updatable Views (Extended version)", "comments": "Extended version of paper accepted for IFL'19 post-proceedings", "journal-ref": null, "doi": "10.1145/3412932.3412945", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational lenses are a modern approach to the view update problem in\nrelational databases. As introduced by Bohannon et al. (2006), relational\nlenses allow the definition of updatable views by the composition of lenses\nperforming individual transformations. Horn et al. (2018) provided the first\nimplementation of incremental relational lenses, which demonstrated that\nrelational lenses can be implemented efficiently by propagating changes to the\ndatabase rather than replacing the entire database state.\n  However, neither approach proposes a concrete language design; consequently,\nit is unclear how to integrate lenses into a general-purpose programming\nlanguage, or how to check that lenses satisfy the well-formedness conditions\nneeded for predictable behaviour. In this paper, we propose the first full\naccount of relational lenses in a functional programming language, by extending\nthe Links web programming language. We provide support for higher-order\npredicates, and provide the first account of typechecking relational lenses\nwhich is amenable to implementation. We prove the soundness of our typing\nrules, and illustrate our approach by implementing a curation interface for a\nscientific database application.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 17:06:15 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 19:18:22 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Horn", "Rudi", ""], ["Fowler", "Simon", ""], ["Cheney", "James", ""]]}, {"id": "2003.02989", "submitter": "Masoud Mohseni", "authors": "Michael Broughton, Guillaume Verdon, Trevor McCourt, Antonio J.\n  Martinez, Jae Hyeon Yoo, Sergei V. Isakov, Philip Massey, Murphy Yuezhen Niu,\n  Ramin Halavati, Evan Peters, Martin Leib, Andrea Skolik, Michael Streif,\n  David Von Dollen, Jarrod R. McClean, Sergio Boixo, Dave Bacon, Alan K. Ho,\n  Hartmut Neven, and Masoud Mohseni", "title": "TensorFlow Quantum: A Software Framework for Quantum Machine Learning", "comments": "39 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.dis-nn cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce TensorFlow Quantum (TFQ), an open source library for the rapid\nprototyping of hybrid quantum-classical models for classical or quantum data.\nThis framework offers high-level abstractions for the design and training of\nboth discriminative and generative quantum models under TensorFlow and supports\nhigh-performance quantum circuit simulators. We provide an overview of the\nsoftware architecture and building blocks through several examples and review\nthe theory of hybrid quantum-classical neural networks. We illustrate TFQ\nfunctionalities via several basic applications including supervised learning\nfor quantum classification, quantum control, and quantum approximate\noptimization. Moreover, we demonstrate how one can apply TFQ to tackle advanced\nquantum learning tasks including meta-learning, Hamiltonian learning, and\nsampling thermal states. We hope this framework provides the necessary tools\nfor the quantum computing and machine learning research communities to explore\nmodels of both natural and artificial quantum systems, and ultimately discover\nnew quantum algorithms which could potentially yield a quantum advantage.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 01:31:43 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Broughton", "Michael", ""], ["Verdon", "Guillaume", ""], ["McCourt", "Trevor", ""], ["Martinez", "Antonio J.", ""], ["Yoo", "Jae Hyeon", ""], ["Isakov", "Sergei V.", ""], ["Massey", "Philip", ""], ["Niu", "Murphy Yuezhen", ""], ["Halavati", "Ramin", ""], ["Peters", "Evan", ""], ["Leib", "Martin", ""], ["Skolik", "Andrea", ""], ["Streif", "Michael", ""], ["Von Dollen", "David", ""], ["McClean", "Jarrod R.", ""], ["Boixo", "Sergio", ""], ["Bacon", "Dave", ""], ["Ho", "Alan K.", ""], ["Neven", "Hartmut", ""], ["Mohseni", "Masoud", ""]]}, {"id": "2003.03170", "submitter": "Christian Graulund", "authors": "Patrick Bahr, Christian Uldal Graulund, Rasmus M{\\o}gelberg", "title": "Diamonds are not forever: Liveness in reactive programming with guarded\n  recursion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When designing languages for functional reactive programming (FRP) the main\nchallenge is to provide the user with a simple, flexible interface for writing\nprograms on a high level of abstraction while ensuring that all programs can be\nimplemented efficiently in a low-level language. To meet this challenge, a new\nfamily of modal FRP languages has been proposed, in which variants of Nakano's\nguarded fixed point operator are used for writing recursive programs\nguaranteeing properties such as causality and productivity. As an apparent\nextension to this it has also been suggested to use Linear Temporal Logic (LTL)\nas a language for reactive programming through the Curry-Howard isomorphism,\nallowing properties such as termination, liveness and fairness to be encoded in\ntypes. However, these two ideas are in conflict with each other, since the\nfixed point operator introduces non-termination into the inductive types that\nare supposed to provide termination guarantees.\n  In this paper we show that by regarding the modal time step operator of LTL a\nsubmodality of the one used for guarded recursion (rather than equating them),\none can obtain a modal type system capable of expressing liveness properties\nwhile retaining the power of the guarded fixed point operator. We introduce the\nlanguage Lively RaTT, a modal FRP language with a guarded fixed point operator\nand an `until' type constructor as in LTL, and show how to program with events\nand fair streams. Using a step-indexed Kripke logical relation we prove\noperational properties of Lively RaTT including productivity and causality as\nwell as the termination and liveness properties expected of types from LTL.\nFinally, we prove that the type system of Lively RaTT guarantees the absence of\nimplicit space leaks.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 12:59:03 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 12:00:28 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Bahr", "Patrick", ""], ["Graulund", "Christian Uldal", ""], ["M\u00f8gelberg", "Rasmus", ""]]}, {"id": "2003.03296", "submitter": "Hui Xu", "authors": "Hui Xu, Zhuangbin Chen, Mingshen Sun, Yangfan Zhou, Michael Lyu", "title": "Memory-Safety Challenge Considered Solved? An In-Depth Study with All\n  Rust CVEs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rust is an emerging programing language that aims at preventing memory-safety\nbugs without sacrificing much efficiency. The claimed property is very\nattractive to developers, and many projects start using the language. However,\ncan Rust achieve the memory-safety promise? This paper studies the question by\nsurveying 186 real-world bug reports collected from several origins which\ncontain all existing Rust CVEs (common vulnerability and exposures) of\nmemory-safety issues by 2020-12-31. We manually analyze each bug and extract\ntheir culprit patterns. Our analysis result shows that Rust can keep its\npromise that all memory-safety bugs require unsafe code, and many memory-safety\nbugs in our dataset are mild soundness issues that only leave a possibility to\nwrite memory-safety bugs without unsafe code. Furthermore, we summarize three\ntypical categories of memory-safety bugs, including automatic memory reclaim,\nunsound function, and unsound generic or trait. While automatic memory claim\nbugs are related to the side effect of Rust newly-adopted ownership-based\nresource management scheme, unsound function reveals the essential challenge of\nRust development for avoiding unsound code, and unsound generic or trait\nintensifies the risk of introducing unsoundness. Based on these findings, we\npropose two promising directions towards improving the security of Rust\ndevelopment, including several best practices of using specific APIs and\nmethods to detect particular bugs involving unsafe code. Our work intends to\nraise more discussions regarding the memory-safety issues of Rust and\nfacilitate the maturity of the language.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 16:16:45 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 14:44:13 GMT"}, {"version": "v3", "created": "Sat, 9 May 2020 11:05:15 GMT"}, {"version": "v4", "created": "Wed, 30 Sep 2020 06:57:44 GMT"}, {"version": "v5", "created": "Sat, 3 Oct 2020 06:00:27 GMT"}, {"version": "v6", "created": "Thu, 25 Feb 2021 01:45:19 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Xu", "Hui", ""], ["Chen", "Zhuangbin", ""], ["Sun", "Mingshen", ""], ["Zhou", "Yangfan", ""], ["Lyu", "Michael", ""]]}, {"id": "2003.03449", "submitter": "Dominik Picheta", "authors": "Dominik Picheta", "title": "Code Obfuscation for the C/C++ Language", "comments": "Document also available in https://picheta.me/c_cpp_obfuscator.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Obfuscation is the action of making something unintelligible. In software\ndevelopment, this action can be applied to source code or binary applications.\nThe aim of this dissertation was to implement a tool for the obfuscation of C\nand C++ source code. The motivation was to allow proprietary code to be\ndistributed to third-parties without risking a recreation of the intellectual\nproperty within it. While many obfuscators exist, they seldom focus on software\nthat is distributed in source code form. This dissertation presents the\nchallenges and successes that arose during the development of a C and C++\nsource code obfuscator using the Nim programming language.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 21:31:21 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Picheta", "Dominik", ""]]}, {"id": "2003.03831", "submitter": "Marco Antoniotti", "authors": "Marco Antoniotti", "title": "Why You Cannot (Yet) Write an \"Interval Arithmetic\" Library in Common\n  Lisp", "comments": "11 pages, paper submitted to the European Lisp Symposium 2020,\n  Zurich, CH", "journal-ref": null, "doi": "10.5281/zenodo.3759522", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Interval Arithmetic\" (IA) appears to be a useful numerical tool to have at\nhand in several applications. Alas, the current IA descriptions and proposed\nstandards are always formulated in terms of the IEEE-754 standard, and the\nstatus of IEEE-754 compliance of most Common Lisp implementations is not up to\npar. A solution would be for Common Lisp implementations to adhere to the\nLanguage Independent Arithmetic} (LIA) IEC standard, which includes IEEE/754.\nWhile the LIA standard provides a set of proposed bindings for Common Lisp, the\nformat and depth of the specification documents is not readily usable by a\nCommon Lisp programmer, should an implementation decide to comply with the\nprovisions. Moreover, much latitude is left to each implementation to provide\nthe LIA \"environmental\" setup. It would be most beneficial if more precision\nwere agreed upon by the Common Lisp community about how to provide LIA\ncompliance in the implementations. In that case, a new set of documentation or\nmanuals in the style of the HyperSpec could be provided, for the benefit of the\nCommon Lisp programmer.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 19:18:08 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 15:55:44 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Antoniotti", "Marco", ""]]}, {"id": "2003.03845", "submitter": "Simon Fowler", "authors": "Simon Fowler, Simon D. Harding, Joanna Sharman, and James Cheney", "title": "Cross-tier web programming for curated databases: A case study", "comments": "Accepted to International Journal of Digital Curation", "journal-ref": "International Journal of Digital Curation 2021 , Vol. 16 , Iss. 1\n  , 21 pp", "doi": "10.2218/ijdc.v16i1.735", "report-no": null, "categories": "cs.PL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Curated databases have become important sources of information across\nscientific disciplines, and due to the manual work of experts, often become\nimportant reference works. Features such as provenance tracking, archiving, and\ndata citation are widely regarded as important features for curated databases,\nbut implementing such features is challenging, and small database projects\noften lack the resources to do so. A scientific database application is not\njust the database itself, but also an ecosystem of web applications to display\nthe data, and applications supporting data curation. Supporting advanced\ncuration features requires changing all of these components, and there is\ncurrently no way to provide such capabilities in a reusable way. Cross-tier\nprogramming languages have been proposed to simplify the creation of web\napplications, where developers write an application in a single, uniform\nlanguage. Consequently, database queries and updates can be written in the same\nlanguage as the rest of the program, and at least in principle, it should be\npossible to provide curation features reusably via program transformations. As\na first step, it is important to establish that realistic curated databases can\nbe implemented in a cross-tier programming language. In this paper, we describe\nsuch a case study: reimplementing the web frontend of a real-world scientific\ndatabase, the IUPHAR/BPS Guide to Pharmacology (GtoPdb), in the Links\nprogramming language. We show how features such as language-integrated query\nsimplify the development process, and rule out common errors. We show that the\nLinks implementation performs fewer database queries, while the time needed to\nhandle the queries is comparable to the Java version. While there is some\noverhead to using Links because of its comparative immaturity compared to Java,\nthe Links version is viable as a proof-of-concept case study.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 20:35:21 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 12:29:36 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Fowler", "Simon", ""], ["Harding", "Simon D.", ""], ["Sharman", "Joanna", ""], ["Cheney", "James", ""]]}, {"id": "2003.04228", "submitter": "Piotr Padlewski", "authors": "Piotr Padlewski, Krzysztof Pszeniczny and Richard Smith", "title": "Modeling the Invariance of Virtual Pointers in LLVM", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Devirtualization is a compiler optimization that replaces indirect (virtual)\nfunction calls with direct calls. It is particularly effective in\nobject-oriented languages, such as Java or C++, in which virtual methods are\ntypically abundant.\n  We present a novel abstract model to express the lifetimes of C++ dynamic\nobjects and invariance of virtual table pointers in the LLVM intermediate\nrepresentation. The model and the corresponding implementation in Clang and\nLLVM enable full devirtualization of virtual calls whenever the dynamic type is\nstatically known and elimination of redundant virtual table loads in other\ncases.\n  Due to the complexity of C++, this has not been achieved by any other C++\ncompiler so far. Although our model was designed for C++, it is also applicable\nto other languages that use virtual dispatch. Our benchmarks show an average of\n0.8% performance improvement on real-world C++ programs, with more than 30%\nspeedup in some cases. The implementation is already a part of the upstream\nLLVM/Clang and can be enabled with the -fstrict-vtable-pointers flag.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 12:38:14 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Padlewski", "Piotr", ""], ["Pszeniczny", "Krzysztof", ""], ["Smith", "Richard", ""]]}, {"id": "2003.04294", "submitter": "Zheng Wang", "authors": "Peng Zhang, Jianbin Fang, Canqun Yang, Chun Huang, Tao Tang, Zheng\n  Wang", "title": "Optimizing Streaming Parallelism on Heterogeneous Many-Core\n  Architectures: A Machine Learning Based Approach", "comments": "Accepted to be published at IEEE TPDS. arXiv admin note: substantial\n  text overlap with arXiv:1802.02760", "journal-ref": null, "doi": "10.1109/TPDS.2020.2978045", "report-no": null, "categories": "cs.DC cs.LG cs.PF cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article presents an automatic approach to quickly derive a good solution\nfor hardware resource partition and task granularity for task-based parallel\napplications on heterogeneous many-core architectures. Our approach employs a\nperformance model to estimate the resulting performance of the target\napplication under a given resource partition and task granularity\nconfiguration. The model is used as a utility to quickly search for a good\nconfiguration at runtime. Instead of hand-crafting an analytical model that\nrequires expert insights into low-level hardware details, we employ machine\nlearning techniques to automatically learn it. We achieve this by first\nlearning a predictive model offline using training programs. The learnt model\ncan then be used to predict the performance of any unseen program at runtime.\nWe apply our approach to 39 representative parallel applications and evaluate\nit on two representative heterogeneous many-core platforms: a CPU-XeonPhi\nplatform and a CPU-GPU platform. Compared to the single-stream version, our\napproach achieves, on average, a 1.6x and 1.1x speedup on the XeonPhi and the\nGPU platform, respectively. These results translate to over 93% of the\nperformance delivered by a theoretically perfect predictor.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 21:18:21 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Zhang", "Peng", ""], ["Fang", "Jianbin", ""], ["Yang", "Canqun", ""], ["Huang", "Chun", ""], ["Tang", "Tao", ""], ["Wang", "Zheng", ""]]}, {"id": "2003.04617", "submitter": "JinGuo Liu", "authors": "Jin-Guo Liu and Taine Zhao", "title": "Differentiate Everything with a Reversible Embeded Domain-Specific\n  Language", "comments": "Github: https://github.com/GiggleLiu/NiLang.jl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reverse-mode automatic differentiation (AD) suffers from the issue of having\ntoo much space overhead to trace back intermediate computational states for\nback-propagation. The traditional method to trace back states is called\ncheckpointing that stores intermediate states into a global stack and restore\nstate through either stack pop or re-computing. The overhead of stack\nmanipulations and re-computing makes the general purposed (not tensor-based) AD\nengines unable to meet many industrial needs. Instead of checkpointing, we\npropose to use reverse computing to trace back states by designing and\nimplementing a reversible programming eDSL, where a program can be executed\nbi-directionally without implicit stack operations. The absence of implicit\nstack operations makes the program compatible with existing compiler features,\nincluding utilizing existing optimization passes and compiling the code as GPU\nkernels. We implement AD for sparse matrix operations and some machine learning\napplications to show that our framework has the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 10:16:51 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 00:19:55 GMT"}, {"version": "v3", "created": "Sun, 31 Jan 2021 23:03:03 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Liu", "Jin-Guo", ""], ["Zhao", "Taine", ""]]}, {"id": "2003.04892", "submitter": "Yatin Manerkar", "authors": "Yatin A. Manerkar, Daniel Lustig, Margaret Martonosi", "title": "RealityCheck: Bringing Modularity, Hierarchy, and Abstraction to\n  Automated Microarchitectural Memory Consistency Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern SoCs are heterogeneous parallel systems comprised of components\ndeveloped by distinct teams and possibly even different vendors. The memory\nconsistency model (MCM) of processors in such SoCs specifies the ordering rules\nwhich constrain the values that can be read by load instructions in parallel\nprograms running on such systems. The implementation of required MCM orderings\ncan span components which may be designed and implemented by many different\nteams. Ideally, each team would be able to specify the orderings enforced by\ntheir components independently and then connect them together when conducting\nMCM verification. However, no prior automated approach for formal hardware MCM\nverification provided this.\n  To bring automated hardware MCM verification in line with the realities of\nthe design process, we present RealityCheck, a methodology and tool for\nautomated formal MCM verification of modular microarchitectural ordering\nspecifications. RealityCheck allows users to specify their designs as a\nhierarchy of distinct modules connected to each other rather than a single flat\nspecification. It can then automatically verify litmus test programs against\nthese modular specifications. RealityCheck also provides support for\nabstraction, which enables scalable verification by breaking up the\nverification of the entire design into smaller verification problems. We\npresent results for verifying litmus tests on 7 different designs using\nRealityCheck. These include in-order and out-of-order pipelines, a non-blocking\ncache, and a heterogeneous processor. Our case studies cover the TSO and RISC-V\n(RVWMO) weak memory models. RealityCheck is capable of verifying 98 RVWMO\nlitmus tests in under 4 minutes each, and its capability for abstraction\nenables up to a 32.1% reduction in litmus test verification time for RVWMO.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 22:48:37 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Manerkar", "Yatin A.", ""], ["Lustig", "Daniel", ""], ["Martonosi", "Margaret", ""]]}, {"id": "2003.05191", "submitter": "Daniel Lund\\'en", "authors": "Daniel Lund\\'en, Johannes Borgstr\\\"om, David Broman", "title": "Correctness of Sequential Monte Carlo Inference for Probabilistic\n  Programming Languages", "comments": null, "journal-ref": "In: Programming Languages and Systems. ESOP 2021. Lecture Notes in\n  Computer Science, vol 12648 (2021)", "doi": "10.1007/978-3-030-72019-3_15", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic programming is an approach to reasoning under uncertainty by\nencoding inference problems as programs. In order to solve these inference\nproblems, probabilistic programming languages (PPLs) employ different inference\nalgorithms, such as sequential Monte Carlo (SMC), Markov chain Monte Carlo\n(MCMC), or variational methods. Existing research on such algorithms mainly\nconcerns their implementation and efficiency, rather than the correctness of\nthe algorithms themselves when applied in the context of expressive PPLs. To\nremedy this, we give a correctness proof for SMC methods in the context of an\nexpressive PPL calculus, representative of popular PPLs such as WebPPL,\nAnglican, and Birch. Previous work have studied correctness of MCMC using an\noperational semantics, and correctness of SMC and MCMC in a denotational\nsetting without term recursion. However, for SMC inference -- one of the most\ncommonly used algorithms in PPLs as of today -- no formal correctness proof\nexists in an operational setting. In particular, an open question is if the\nresample locations in a probabilistic program affects the correctness of SMC.\nWe solve this fundamental problem, and make four novel contributions: (i) we\nextend an untyped PPL lambda calculus and operational semantics to include\nexplicit resample terms, expressing synchronization points in SMC inference;\n(ii) we prove, for the first time, that subject to mild restrictions, any\nplacement of the explicit resample terms is valid for a generic form of SMC\ninference; (iii) as a result of (ii), our calculus benefits from classic\nresults from the SMC literature: a law of large numbers and an unbiased\nestimate of the model evidence; and (iv) we formalize the bootstrap particle\nfilter for the calculus and discuss how our results can be further extended to\nother SMC algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 09:53:38 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 18:25:46 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Lund\u00e9n", "Daniel", ""], ["Borgstr\u00f6m", "Johannes", ""], ["Broman", "David", ""]]}, {"id": "2003.05836", "submitter": "Letterio Galletta", "authors": "Matteo Busi and Pierpaolo Degano and Letterio Galletta", "title": "Control-flow Flattening Preserves the Constant-Time Policy (Extended\n  Version)", "comments": "Extended version of ITASEC20 camera ready paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obfuscating compilers protect a software by obscuring its meaning and\nimpeding the reconstruction of its original source code. The typical concern\nwhen defining such compilers is their robustness against reverse engineering\nand the performance of the produced code. Little work has been done in studying\nwhether the security properties of a program are preserved under obfuscation.\nIn this paper we start addressing this problem: we consider control-flow\nflattening, a popular obfuscation technique used in industrial compilers, and a\nspecific security policy, namely constant-time. We prove that this obfuscation\npreserves the policy, i.e., that every program satisfying the policy still does\nafter the transformation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 15:08:31 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Busi", "Matteo", ""], ["Degano", "Pierpaolo", ""], ["Galletta", "Letterio", ""]]}, {"id": "2003.05841", "submitter": "Christophe Chareton", "authors": "Christophe Chareton, S\\'ebastien Bardin, Fran\\c{c}ois Bobot, Valentin\n  Perrelle, Benoit Valiron", "title": "A Deductive Verification Framework for Circuit-building Quantum Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While recent progress in quantum hardware open the door for significant\nspeedup in certain key areas, quantum algorithms are still hard to implement\nright, and the validation of such quantum programs is a challenge. Early\nattempts either suffer from the lack of automation or parametrized reasoning,\nor target high-level abstract algorithm description languages far from the\ncurrent de facto consensus of circuit-building quantum programming languages.\nAs a consequence, no significant quantum algorithm implementation has been\ncurrently verified in a scale-invariant manner. We propose Qbricks, the first\nformal verification environment for circuit-building quantum programs,\nfeaturing clear separation between code and proof, parametric specifications\nand proofs, high degree of proof automation and allowing to encode quantum\nprograms in a natural way, i.e. close to textbook style. Qbricks builds on best\npractice of formal verification for the classical case and tailor them to the\nquantum case: we bring a new domain-specific circuit-building language for\nquantum programs, namely Qbricks-DSL, together with a new logical specification\nlanguage Qbricks-Spec and a dedicated Hoare-style deductive verification rule\nnamed Hybrid Quantum Hoare Logic. Especially, we introduce and intensively\nbuild upon HOPS, a higher-order extension of the recent path-sum symbolic\nrepresentation, used for both specification and automation. To illustrate the\nopportunity of Qbricks, we implement the first verified parametric\nimplementations of several famous and non-trivial quantum algorithms, including\nthe quantum part of Shor integer factoring (Order Finding - Shor-OF), quantum\nphase estimation (QPE) - a basic building block of many quantum algorithms, and\nGrover search. These breakthroughs were amply facilitated by the specification\nand automated deduction principles introduced within Qbricks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 15:21:11 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 13:20:35 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Chareton", "Christophe", ""], ["Bardin", "S\u00e9bastien", ""], ["Bobot", "Fran\u00e7ois", ""], ["Perrelle", "Valentin", ""], ["Valiron", "Benoit", ""]]}, {"id": "2003.06324", "submitter": "Vinod Grover", "authors": "Bastian Hagedorn, Archibald Samuel Elliott, Henrik Barthels, Rastislav\n  Bodik, Vinod Grover", "title": "Fireiron: A Scheduling Language for High-Performance Linear Algebra on\n  GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving high-performance GPU kernels requires optimizing algorithm\nimplementations to the targeted GPU architecture. It is of utmost importance to\nfully use the compute and memory hierarchy, as well as available specialised\nhardware. Currently, vendor libraries like cuBLAS and cuDNN provide the best\nperforming implementations of GPU algorithms. However the task of the library\nprogrammer is incredibly challenging: for each provided algorithm,\nhigh-performance implementations have to be developed for all commonly used\narchitectures, input sizes, and different storage formats. These\nimplementations are generally provided as optimized assembly code because\nperformance-critical architectural features are only exposed at this level.\nThis prevents reuse between different implementations of even the same\nalgorithm, as simple differences can have major effects on low-level\nimplementation details. In this paper we introduce Fireiron, a DSL and compiler\nwhich allows the specification of high-performance GPU implementations as\ncompositions of simple and reusable building blocks. We show how to use\nFireiron to optimize matrix multiplication implementations, achieving\nperformance matching hand-coded CUDA kernels, even when using specialised\nhardware such as NIVIDA Tensor Cores, and outperforming state-of-the-art\nimplementations provided by cuBLAS by more than 2x.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 14:40:30 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Hagedorn", "Bastian", ""], ["Elliott", "Archibald Samuel", ""], ["Barthels", "Henrik", ""], ["Bodik", "Rastislav", ""], ["Grover", "Vinod", ""]]}, {"id": "2003.06458", "submitter": "Karl Palmskog", "authors": "Talia Ringer, Karl Palmskog, Ilya Sergey, Milos Gligoric, Zachary\n  Tatlock", "title": "QED at Large: A Survey of Engineering of Formally Verified Software", "comments": "183 pages, for errata see\n  https://proofengineering.org/qed_errata.html", "journal-ref": "Foundations and Trends in Programming Languages, Vol. 5, No. 2-3\n  (Sept. 2019), pp. 102-281", "doi": "10.1561/2500000045", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Development of formal proofs of correctness of programs can increase actual\nand perceived reliability and facilitate better understanding of program\nspecifications and their underlying assumptions. Tools supporting such\ndevelopment have been available for over 40 years, but have only recently seen\nwide practical use. Projects based on construction of machine-checked formal\nproofs are now reaching an unprecedented scale, comparable to large software\nprojects, which leads to new challenges in proof development and maintenance.\nDespite its increasing importance, the field of proof engineering is seldom\nconsidered in its own right; related theories, techniques, and tools span many\nfields and venues. This survey of the literature presents a holistic\nunderstanding of proof engineering for program correctness, covering impact in\npractice, foundations, proof automation, proof organization, and practical\nproof development.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 19:35:25 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Ringer", "Talia", ""], ["Palmskog", "Karl", ""], ["Sergey", "Ilya", ""], ["Gligoric", "Milos", ""], ["Tatlock", "Zachary", ""]]}, {"id": "2003.06893", "submitter": "Roberto Bagnara", "authors": "Roberto Bagnara, Michael Barr, Patricia M. Hill", "title": "BARR-C:2018 and MISRA C:2012: Synergy Between the Two Most Widely Used C\n  Coding Standards", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Barr Group's Embedded C Coding Standard (BARR-C:2018, which originates\nfrom the 2009 Netrino's Embedded C Coding Standard) is, for coding standards\nused by the embedded system industry, second only in popularity to MISRA C.\nHowever, the choice between MISRA C:2012 and BARR-C:2018 needs not be a hard\ndecision since they are complementary in two quite different ways. On the one\nhand, BARR-C:2018 has removed all the incompatibilities with respect to MISRA\nC:2012 that were present in the previous edition (BARR-C:2013). As a result,\ndisregarding programming style, BARR-C:2018 defines a subset of C that, while\npreventing a significant number of programming errors, is larger than the one\ndefined by MISRA C:2012. On the other hand, concerning programming style,\nwhereas MISRA C leaves this to individual organizations, BARR-C:2018 defines a\nprogramming style aimed primarily at minimizing programming errors. As a\nresult, BARR-C:2018 can be seen as a first, dramatically useful step to C\nlanguage subsetting that is suitable for all kinds of projects; critical\nprojects can then evolve toward MISRA C:2012 compliance smoothly while\nmaintaining the BARR-C programming style. In this paper, we introduce\nBARR-C:2018, we describe its relationship with MISRA C:2012, and we discuss the\nparallel and serial adoption of the two coding standards.\n", "versions": [{"version": "v1", "created": "Sun, 15 Mar 2020 18:53:07 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Bagnara", "Roberto", ""], ["Barr", "Michael", ""], ["Hill", "Patricia M.", ""]]}, {"id": "2003.07395", "submitter": "Thomas Dickerson", "authors": "Thomas Dickerson", "title": "Adapting Persistent Data Structures for Concurrency and Speculation", "comments": "PhD Thesis, Brown University (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work unifies insights from the systems and functional programming\ncommunities, in order to enable compositional reasoning about software which is\nnonetheless efficiently realizable in hardware. It exploits a correspondence\nbetween design goals for efficient concurrent data structures and efficient\nimmutable persistent data structures, to produce novel implementations of\nmutable concurrent trees with low contention and an efficient snapshot\noperation to support speculative execution models. It also exploits\ncommutativity to characterize a design space for integrating traditional\nhigh-performance concurrent data structures into Software Transactional Memory\n(STM) runtimes, and extends this technique to yield a novel algorithm for\nconcurrent execution of so-called ``smart contracts'' (specialized programs\nwhich manipulate the state of blockchain ledgers).\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 18:26:08 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Dickerson", "Thomas", ""]]}, {"id": "2003.07959", "submitter": "Jianan Yao", "authors": "Jianan Yao, Gabriel Ryan, Justin Wong, Suman Jana, Ronghui Gu", "title": "Learning Nonlinear Loop Invariants with Gated Continuous Logic Networks\n  (Extended Version)", "comments": null, "journal-ref": "In Proceedings of the 41st ACM SIGPLAN Conference on Programming\n  Language Design and Implementation, pp. 106-120. 2020", "doi": "10.1145/3385412.3385986", "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verifying real-world programs often requires inferring loop invariants with\nnonlinear constraints. This is especially true in programs that perform many\nnumerical operations, such as control systems for avionics or industrial\nplants. Recently, data-driven methods for loop invariant inference have shown\npromise, especially on linear invariants. However, applying data-driven\ninference to nonlinear loop invariants is challenging due to the large numbers\nof and magnitudes of high-order terms, the potential for overfitting on a small\nnumber of samples, and the large space of possible inequality bounds.\n  In this paper, we introduce a new neural architecture for general SMT\nlearning, the Gated Continuous Logic Network (G-CLN), and apply it to nonlinear\nloop invariant learning. G-CLNs extend the Continuous Logic Network (CLN)\narchitecture with gating units and dropout, which allow the model to robustly\nlearn general invariants over large numbers of terms. To address overfitting\nthat arises from finite program sampling, we introduce fractional sampling---a\nsound relaxation of loop semantics to continuous functions that facilitates\nunbounded sampling on real domain. We additionally design a new CLN activation\nfunction, the Piecewise Biased Quadratic Unit (PBQU), for naturally learning\ntight inequality bounds.\n  We incorporate these methods into a nonlinear loop invariant inference system\nthat can learn general nonlinear loop invariants. We evaluate our system on a\nbenchmark of nonlinear loop invariants and show it solves 26 out of 27\nproblems, 3 more than prior work, with an average runtime of 53.3 seconds. We\nfurther demonstrate the generic learning ability of G-CLNs by solving all 124\nproblems in the linear Code2Inv benchmark. We also perform a quantitative\nstability evaluation and show G-CLNs have a convergence rate of $97.5\\%$ on\nquadratic problems, a $39.2\\%$ improvement over CLN models.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 21:44:37 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2020 22:32:57 GMT"}, {"version": "v3", "created": "Fri, 10 Apr 2020 16:50:31 GMT"}, {"version": "v4", "created": "Thu, 25 Jun 2020 16:43:51 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Yao", "Jianan", ""], ["Ryan", "Gabriel", ""], ["Wong", "Justin", ""], ["Jana", "Suman", ""], ["Gu", "Ronghui", ""]]}, {"id": "2003.08138", "submitter": "Taro Sekiyama", "authors": "Taro Sekiyama and Takeshi Tsukada and Atsushi Igarashi", "title": "Signature Restriction for Polymorphic Algebraic Effects", "comments": null, "journal-ref": "Proceedings of the ACM on Programming Languages (ICFP) Article No.\n  117 (2020 August)", "doi": "10.1145/3408999", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The naive combination of polymorphic effects and polymorphic type assignment\nhas been well known to break type safety. Existing approaches to this problem\nare classified into two groups: one for restricting how effects are triggered\nand the other for restricting how they are implemented. This work explores a\nnew approach to ensuring the safety of polymorphic effects in polymorphic type\nassignment. A novelty of our work lies in finding a restriction on effect\ninterfaces. To formalize our idea, we employ algebraic effects and handlers,\nwhere an effect interface is given by a set of operations coupled with type\nsignatures. We propose signature restriction, a new notion to restrict the type\nsignatures of operations, and show that signature restriction is sufficient to\nensure type safety of an effectful language equipped with unrestricted\npolymorphic type assignment. We also develop a type-and-effect system to enable\nthe use of both operations that satisfy and do not satisfy the signature\nrestriction in a single program.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 10:39:12 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 05:24:23 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Sekiyama", "Taro", ""], ["Tsukada", "Takeshi", ""], ["Igarashi", "Atsushi", ""]]}, {"id": "2003.08408", "submitter": "Giulia Meuli", "authors": "Giulia Meuli, Mathias Soeken, Martin Roetteler and Thomas H\\\"aner", "title": "Enabling Accuracy-Aware Quantum Compilers using Symbolic Resource\n  Estimation", "comments": "26 pages", "journal-ref": "Proc. ACM Program. Lang. 4, OOPSLA, Article 130 (November 2020)", "doi": "10.1145/3428198", "report-no": null, "categories": "quant-ph cs.ET cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximation errors must be taken into account when compiling quantum\nprograms into a low-level gate set. We present a methodology that tracks such\nerrors automatically and then optimizes accuracy parameters to guarantee a\nspecified overall accuracy while aiming to minimize the implementation cost in\nterms of quantum gates. The core idea of our approach is to extract functions\nthat specify the optimization problem directly from the high-level description\nof the quantum program. Then, custom compiler passes optimize these functions,\nturning them into (near-)symbolic expressions for (1) the total error and (2)\nthe implementation cost (e.g., total quantum gate count). All unspecified\nparameters of the quantum program will show up as variables in these\nexpressions, including accuracy parameters. After solving the corresponding\noptimization problem, a circuit can be instantiated from the found solution. We\ndevelop two prototype implementations, one in C++ based on Clang/LLVM, and\nanother using the Q# compiler infrastructure. We benchmark our prototypes on\ntypical quantum computing programs, including the quantum Fourier transform,\nquantum phase estimation, and Shor's algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 18:00:24 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 11:49:52 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Meuli", "Giulia", ""], ["Soeken", "Mathias", ""], ["Roetteler", "Martin", ""], ["H\u00e4ner", "Thomas", ""]]}, {"id": "2003.09040", "submitter": "Kensen Shi", "authors": "Kensen Shi, David Bieber, Rishabh Singh", "title": "TF-Coder: Program Synthesis for Tensor Manipulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success and popularity of deep learning is on the rise, partially due to\npowerful deep learning frameworks such as TensorFlow and PyTorch that make it\neasier to develop deep learning models. However, these libraries also come with\nsteep learning curves, since programming in these frameworks is quite different\nfrom traditional imperative programming with explicit loops and conditionals.\nIn this work, we present a tool called TF-Coder for programming by example in\nTensorFlow. TF-Coder uses a bottom-up weighted enumerative search, with\nvalue-based pruning of equivalent expressions and flexible type- and\nvalue-based filtering to ensure that expressions adhere to various requirements\nimposed by the TensorFlow library. We also train models that predict TensorFlow\noperations from features of the input and output tensors and natural language\ndescriptions of tasks, and use the models to prioritize relevant operations\nduring the search. TF-Coder solves 63 of 70 real-world tasks within 5 minutes,\noften achieving superhuman performance -- finding solutions that are simpler\nthan those written by TensorFlow experts, in less time as well.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2020 22:53:47 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 00:06:20 GMT"}, {"version": "v3", "created": "Sat, 25 Jul 2020 00:51:38 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Shi", "Kensen", ""], ["Bieber", "David", ""], ["Singh", "Rishabh", ""]]}, {"id": "2003.09769", "submitter": "Leonidas Fegaras", "authors": "Leonidas Fegaras and Md Hasanuzzaman Noor", "title": "Translation of Array-Based Loops to Distributed Data-Parallel Programs", "comments": "This is the extended version of a paper that will appear at VLDB 2020\n  (PVLDB Vol. 13)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large volumes of data generated by scientific experiments and simulations\ncome in the form of arrays, while programs that analyze these data are\nfrequently expressed in terms of array operations in an imperative, loop-based\nlanguage. But, as datasets grow larger, new frameworks in distributed Big Data\nanalytics have become essential tools to large-scale scientific computing.\nScientists, who are typically comfortable with numerical analysis tools but are\nnot familiar with the intricacies of Big Data analytics, must now learn to\nconvert their loop-based programs to distributed data-parallel programs. We\npresent a novel framework for translating programs expressed as array-based\nloops to distributed data parallel programs that is more general and efficient\nthan related work. Although our translations are over sparse arrays, we extend\nour framework to handle packed arrays, such as tiled matrices, without\nsacrificing performance. We report on a prototype implementation on top of\nSpark and evaluate the performance of our system relative to hand-written\nprograms.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 23:40:44 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Fegaras", "Leonidas", ""], ["Noor", "Md Hasanuzzaman", ""]]}, {"id": "2003.10536", "submitter": "Chris Cummins", "authors": "Chris Cummins, Zacharias V. Fisches, Tal Ben-Nun, Torsten Hoefler,\n  Hugh Leather", "title": "ProGraML: Graph-based Deep Learning for Program Optimization and\n  Analysis", "comments": "20 pages, author preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PF cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing complexity of computing systems places a tremendous burden on\noptimizing compilers, requiring ever more accurate and aggressive\noptimizations. Machine learning offers significant benefits for constructing\noptimization heuristics but there remains a gap between what state-of-the-art\nmethods achieve and the performance of an optimal heuristic. Closing this gap\nrequires improvements in two key areas: a representation that accurately\ncaptures the semantics of programs, and a model architecture with sufficient\nexpressiveness to reason about this representation.\n  We introduce ProGraML - Program Graphs for Machine Learning - a novel\ngraph-based program representation using a low level, language agnostic, and\nportable format; and machine learning models capable of performing complex\ndownstream tasks over these graphs. The ProGraML representation is a directed\nattributed multigraph that captures control, data, and call relations, and\nsummarizes instruction and operand types and ordering. Message Passing Neural\nNetworks propagate information through this structured representation, enabling\nwhole-program or per-vertex classification tasks.\n  ProGraML provides a general-purpose program representation that equips\nlearnable models to perform the types of program analysis that are fundamental\nto optimization. To this end, we evaluate the performance of our approach first\non a suite of traditional compiler analysis tasks: control flow reachability,\ndominator trees, data dependencies, variable liveness, and common subexpression\ndetection. On a benchmark dataset of 250k LLVM-IR files covering six source\nprogramming languages, ProGraML achieves an average 94.0 F1 score,\nsignificantly outperforming the state-of-the-art approaches. We then apply our\napproach to two high-level tasks - heterogeneous device mapping and program\nclassification - setting new state-of-the-art performance in both.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 20:27:00 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Cummins", "Chris", ""], ["Fisches", "Zacharias V.", ""], ["Ben-Nun", "Tal", ""], ["Hoefler", "Torsten", ""], ["Leather", "Hugh", ""]]}, {"id": "2003.11118", "submitter": "Justin Gottschlich", "authors": "Fangke Ye, Shengtian Zhou, Anand Venkat, Ryan Marcus, Paul Petersen,\n  Jesmin Jahan Tithi, Tim Mattson, Tim Kraska, Pradeep Dubey, Vivek Sarkar,\n  Justin Gottschlich", "title": "Context-Aware Parse Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simplified parse tree (SPT) presented in Aroma, a state-of-the-art code\nrecommendation system, is a tree-structured representation used to infer code\nsemantics by capturing program \\emph{structure} rather than program\n\\emph{syntax}. This is a departure from the classical abstract syntax tree,\nwhich is principally driven by programming language syntax. While we believe a\nsemantics-driven representation is desirable, the specifics of an SPT's\nconstruction can impact its performance. We analyze these nuances and present a\nnew tree structure, heavily influenced by Aroma's SPT, called a\n\\emph{context-aware parse tree} (CAPT). CAPT enhances SPT by providing a richer\nlevel of semantic representation. Specifically, CAPT provides additional\nbinding support for language-specific techniques for adding\nsemantically-salient features, and language-agnostic techniques for removing\nsyntactically-present but semantically-irrelevant features. Our research\nquantitatively demonstrates the value of our proposed semantically-salient\nfeatures, enabling a specific CAPT configuration to be 39\\% more accurate than\nSPT across the 48,610 programs we analyzed.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 21:19:14 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Ye", "Fangke", ""], ["Zhou", "Shengtian", ""], ["Venkat", "Anand", ""], ["Marcus", "Ryan", ""], ["Petersen", "Paul", ""], ["Tithi", "Jesmin Jahan", ""], ["Mattson", "Tim", ""], ["Kraska", "Tim", ""], ["Dubey", "Pradeep", ""], ["Sarkar", "Vivek", ""], ["Gottschlich", "Justin", ""]]}, {"id": "2003.11517", "submitter": "Shafiuddin Rehan Ahmed", "authors": "Adam Wiemerslage and Shafiuddin Rehan Ahmed", "title": "From Algebraic Word Problem to Program: A Formalized Approach", "comments": "9 pages, 6 figures, Course project of Programming Languages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a pipeline to convert grade school level algebraic\nword problem into program of a formal languageA-IMP. Using natural language\nprocessing tools, we break the problem into sentence fragments which can then\nbe reduced to functions. The functions are categorized by the head verb of the\nsentence and its structure, as defined by (Hosseini et al., 2014). We define\nthe function signature and extract its arguments from the text using dependency\nparsing. We have a working implementation of the entire pipeline which can be\nfound on our github repository.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 20:55:01 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Wiemerslage", "Adam", ""], ["Ahmed", "Shafiuddin Rehan", ""]]}, {"id": "2003.12106", "submitter": "Anders Miltner", "authors": "Anders Miltner, Saswat Padhi, Todd Millstein, David Walker", "title": "Data-Driven Inference of Representation Invariants", "comments": "18 Pages, Full version of PLDI 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A representation invariant is a property that holds of all values of abstract\ntype produced by a module. Representation invariants play important roles in\nsoftware engineering and program verification. In this paper, we develop a\ncounterexample-driven algorithm for inferring a representation invariant that\nis sufficient to imply a desired specification for a module. The key novelty is\na type-directed notion of visible inductiveness, which ensures that the\nalgorithm makes progress toward its goal as it alternates between weakening and\nstrengthening candidate invariants. The algorithm is parameterized by an\nexample-based synthesis engine and a verifier, and we prove that it is sound\nand complete for first-order modules over finite types, assuming that the\nsynthesizer and verifier are as well. We implement these ideas in a tool called\nHanoi, which synthesizes representation invariants for recursive data types.\nHanoi not only handles invariants for first-order code, but higher-order code\nas well. In its back end, Hanoi uses an enumerative synthesizer called Myth and\nan enumerative testing tool as a verifier. Because Hanoi uses testing for\nverification, it is not sound, though our empirical evaluation shows that it is\nsuccessful on the benchmarks we investigated.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 18:40:07 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Miltner", "Anders", ""], ["Padhi", "Saswat", ""], ["Millstein", "Todd", ""], ["Walker", "David", ""]]}, {"id": "2003.12209", "submitter": "Michael Coblenz", "authors": "Michael Coblenz, Jonathan Aldrich, Joshua Sunshine, Brad A. Myers", "title": "Can Advanced Type Systems Be Usable? An Empirical Study of Ownership,\n  Assets, and Typestate in Obsidian", "comments": "Published open access in PACMPL Issue OOPSLA 2020", "journal-ref": "In Proceedings of PACMPL Issue OOPSLA 2020 (OOPSLA 2020). Article\n  132, 28 pages", "doi": "10.1145/3428200", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some blockchain programs (smart contracts) have included serious security\nvulnerabilities. Obsidian is a new typestate-oriented programming language that\nuses a strong type system to rule out some of these vulnerabilities. Although\nObsidian was designed to promote usability to make it as easy as possible to\nwrite programs, strong type systems can cause a language to be difficult to\nuse. In particular, ownership, typestate, and assets, which Obsidian uses to\nprovide safety guarantees, have not seen broad adoption together in popular\nlanguages and result in significant usability challenges. We performed an\nempirical study with 20 participants comparing Obsidian to Solidity, which is\nthe language most commonly used for writing smart contracts today. We observed\nthat Obsidian participants were able to successfully complete more of the\nprogramming tasks than the Solidity participants. We also found that the\nSolidity participants commonly inserted asset-related bugs, which Obsidian\ndetects at compile time.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 02:38:30 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 02:25:22 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Coblenz", "Michael", ""], ["Aldrich", "Jonathan", ""], ["Sunshine", "Joshua", ""], ["Myers", "Brad A.", ""]]}, {"id": "2003.12819", "submitter": "Abhishek Bichhawat", "authors": "Abhishek Bichhawat and McKenna McCall and Limin Jia", "title": "First-order Gradual Information Flow Types with Gradual Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information flow type systems enforce the security property of\nnoninterference by detecting unauthorized data flows at compile-time. However,\nthey require precise type annotations, making them difficult to use in practice\nas much of the legacy infrastructure is written in untyped or dynamically-typed\nlanguages. Gradual typing seamlessly integrates static and dynamic typing,\nproviding the best of both approaches, and has been applied to information flow\ncontrol, where information flow monitors are derived from gradual security\ntypes. Prior work on gradual information flow typing uncovered tensions between\nnoninterference and the dynamic gradual guarantee -- the property that less\nprecise security type annotations in a program should not cause more runtime\nerrors. This paper re-examines the connection between gradual information flow\ntypes and information flow monitors to identify the root cause of the tension\nbetween the gradual guarantees and noninterference. We develop runtime\nsemantics for a simple imperative language with gradual information flow types\nthat provides both noninterference and gradual guarantees. We leverage a proof\ntechnique developed for FlowML and reduce noninterference proofs to\npreservation proofs.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 15:40:50 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 01:38:24 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2021 06:03:11 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Bichhawat", "Abhishek", ""], ["McCall", "McKenna", ""], ["Jia", "Limin", ""]]}, {"id": "2003.13515", "submitter": "Jason Breck", "authors": "Jason Breck (1), John Cyphert (1), Zachary Kincaid (2) and Thomas Reps\n  (1) ((1) University of Wisconsin-Madison, (2) Princeton University)", "title": "Templates and Recurrences: Better Together", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is the confluence of two streams of ideas in the literature on\ngenerating numerical invariants, namely: (1) template-based methods, and (2)\nrecurrence-based methods. A template-based method begins with a template that\ncontains unknown quantities, and finds invariants that match the template by\nextracting and solving constraints on the unknowns. A disadvantage of\ntemplate-based methods is that they require fixing the set of terms that may\nappear in an invariant in advance. This disadvantage is particularly prominent\nfor non-linear invariant generation, because the user must supply maximum\ndegrees on polynomials, bases for exponents, etc. On the other hand,\nrecurrence-based methods are able to find sophisticated non-linear mathematical\nrelations, including polynomials, exponentials, and logarithms, because such\nrelations arise as the solutions to recurrences. However, a disadvantage of\npast recurrence-based invariant-generation methods is that they are primarily\nloop-based analyses: they use recurrences to relate the pre-state and\npost-state of a loop, so it is not obvious how to apply them to a recursive\nprocedure, especially if the procedure is non-linearly recursive (e.g., a\ntree-traversal algorithm). In this paper, we combine these two approaches and\nobtain a technique that uses templates in which the unknowns are functions\nrather than numbers, and the constraints on the unknowns are recurrences. The\ntechnique synthesizes invariants involving polynomials, exponentials, and\nlogarithms, even in the presence of arbitrary control-flow, including any\ncombination of loops, branches, and (possibly non-linear) recursion. For\ninstance, it is able to show that (i) the time taken by merge-sort is $O(n\n\\log(n))$, and (ii) the time taken by Strassen's algorithm is\n$O(n^{\\log_2(7)})$.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 14:39:56 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Breck", "Jason", "", "University of Wisconsin-Madison"], ["Cyphert", "John", "", "University of Wisconsin-Madison"], ["Kincaid", "Zachary", "", "Princeton University"], ["Reps", "Thomas", "", "University of Wisconsin-Madison"]]}, {"id": "2003.13961", "submitter": "Robert Smith", "authors": "Robert S. Smith, Eric C. Peterson, Mark G. Skilbeck, Erik J. Davis", "title": "An Open-Source, Industrial-Strength Optimizing Compiler for Quantum\n  Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quilc is an open-source, optimizing compiler for gate-based quantum programs\nwritten in Quil or QASM, two popular quantum programming languages. The\ncompiler was designed with attention toward NISQ-era quantum computers,\nspecifically recognizing that each quantum gate has a non-negligible and often\nirrecoverable cost toward a program's successful execution. Quilc's primary\ngoal is to make authoring quantum software a simpler exercise by making\narchitectural details less burdensome to the author. Using Quilc allows one to\nwrite programs faster while usually not compromising---and indeed sometimes\nimproving---their execution fidelity on a given hardware architecture. In this\npaper, we describe many of the principles behind Quilc's design, and\ndemonstrate the compiler with various examples.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 05:54:15 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Smith", "Robert S.", ""], ["Peterson", "Eric C.", ""], ["Skilbeck", "Mark G.", ""], ["Davis", "Erik J.", ""]]}]