[{"id": "1902.00319", "submitter": "Gregor Ulm", "authors": "Gregor Ulm, Simon Smith, Adrian Nilsson, Emil Gustavsson, Mats\n  Jirstrand", "title": "OODIDA: On-board/Off-board Distributed Real-Time Data Analytics for\n  Connected Vehicles", "comments": "28 pages, 9 figures, 2 algorithms, 2 code listings, 1 table", "journal-ref": "Data Science and Engineering Vol. 6 (2021)", "doi": "10.1007/s41019-021-00152-6", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fleet of connected vehicles easily produces many gigabytes of data per\nhour, making centralized (off-board) data processing impractical. In addition,\nthere is the issue of distributing tasks to on-board units in vehicles and\nprocessing them efficiently. Our solution to this problem is OODIDA\n(On-board/Off-board Distributed Data Analytics), which is a platform that\ntackles both task distribution to connected vehicles as well as concurrent\nexecution of tasks on arbitrary subsets of edge clients. Its message-passing\ninfrastructure has been implemented in Erlang/OTP, while the end points use a\nlanguage-independent JSON interface. Computations can be carried out in\narbitrary programming languages. The message-passing infrastructure of OODIDA\nis highly scalable, facilitating the execution of large numbers of concurrent\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 13:31:33 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 15:06:20 GMT"}, {"version": "v3", "created": "Sun, 31 Jan 2021 18:36:57 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Ulm", "Gregor", ""], ["Smith", "Simon", ""], ["Nilsson", "Adrian", ""], ["Gustavsson", "Emil", ""], ["Jirstrand", "Mats", ""]]}, {"id": "1902.00524", "submitter": "Florian Myter", "authors": "Florian Myter (Vrije Unversiteit Brussel, Belgium), Christophe\n  Scholliers (Ghent University, Belgium), Wolfgang De Meuter (Vrije\n  Universiteit Brussel, Belgium)", "title": "Distributed Reactive Programming for Reactive Distributed Systems", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2019, Vol. 3,\n  Issue 3, Article 5", "doi": "10.22152/programming-journal.org/2019/3/5", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: The term reactivity is popular in two areas of research: programming\nlanguages and distributed systems. On one hand, reactive programming is a\nparadigm which provides programmers with the means to declaratively write\nevent-driven applications. On the other hand, reactive distributed systems\nhandle client requests in a timely fashion regardless of load or failures.\nInquiry: Reactive programming languages and frameworks tailored to the\nimplementation of distributed systems have previously been proposed. However,\nwe argue that these approaches are ill fit to implement reactive distributed\nsystems. Approach: We analyse state of the art runtimes for distributed\nreactive programming and identify two key issues with regards to reactive\ndistributed systems. They rely on single, central points of coordination and/or\nassume a lack of partial failures in the systems they support. Knowledge: Based\non our analysis we propose a novel runtime for distributed reactive programming\nlanguages and frameworks. This runtime supports reactive distributed systems by\ndesign. Grounding: We implement a proof of concept framework for reactive\ndistributed systems in JavaScript which builds atop our runtime. Using this\nframework we implement a case study application which highlights the\napplicability of our approach. Moreover, we benchmark our runtime against a\nsimilar approach in order to showcase its runtime properties and we prove its\ncorrectness. Importance: This work aims to bridge the gap between two kinds of\nreactivity: reactive distributed systems and distributed reactive programming.\nCurrent distributed reactive programming approaches do not support reactive\ndistributed systems. Our runtime is the first to bridge this reactivity gap: it\nallows for reactive distributed systems to be implemented using distributed\nreactive programming.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 19:06:08 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Myter", "Florian", "", "Vrije Unversiteit Brussel, Belgium"], ["Scholliers", "Christophe", "", "Ghent University, Belgium"], ["De Meuter", "Wolfgang", "", "Vrije\n  Universiteit Brussel, Belgium"]]}, {"id": "1902.00525", "submitter": "S. Tucker Taft", "authors": "S. Tucker Taft (AdaCore, United States)", "title": "ParaSail: A Pointer-Free Pervasively-Parallel Language for Irregular\n  Computations", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2019, Vol. 3,\n  Issue 3, Article 7", "doi": "10.22152/programming-journal.org/2019/3/7", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ParaSail is a language specifically designed to simplify the construction of\nprograms that make full, safe use of parallel hardware even while manipulating\npotentially irregular data structures. As parallel hardware has proliferated,\nthere has been an urgent need for languages that ease the writing of correct\nparallel programs. ParaSail achieves these goals largely through simplification\nof the language, rather than by adding numerous rules. In particular, ParaSail\neliminates global variables, parameter aliasing, and most significantly,\nre-assignable pointers. ParaSail has adopted a pointer-free approach to\ndefining complex data structures. Rather than using pointers, ParaSail supports\nflexible data structuring using expandable (and shrinkable) objects implemented\nusing region-based storage management, along with generalized indexing. By\neliminating global variables, parameter aliasing, and pointers, ParaSail\nreduces the complexity for the programmer, while still allowing ParaSail to\nprovide flexible, pervasive, safe, parallel programming for irregular\ncomputations. Perhaps the most interesting discovery in this language\ndevelopment effort, based on over six years of use by the author and a group of\nParaSail users, has been that it is possible to simultaneously simplify the\nlanguage, support parallel programming with advanced data structures, and\nmaintain flexibility and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 19:06:40 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Taft", "S. Tucker", "", "AdaCore, United States"]]}, {"id": "1902.00543", "submitter": "Rodin Aarssen", "authors": "Rodin Aarssen (CWI, Netherlands), Jurgen Vinju (CWI, Netherlands),\n  Tijs van der Storm (CWI, Netherlands)", "title": "Concrete Syntax with Black Box Parsers", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2019, Vol. 3,\n  Issue 3, Article 15", "doi": "10.22152/programming-journal.org/2019/3/15", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Meta programming consists for a large part of matching, analyzing,\nand transforming syntax trees. Many meta programming systems process abstract\nsyntax trees, but this requires intimate knowledge of the structure of the data\ntype describing the abstract syntax. As a result, meta programming is\nerror-prone, and meta programs are not resilient to evolution of the structure\nof such ASTs, requiring invasive, fault-prone change to these programs.\nInquiry: Concrete syntax patterns alleviate this problem by allowing the meta\nprogrammer to match and create syntax trees using the actual syntax of the\nobject language. Systems supporting concrete syntax patterns, however, require\na concrete grammar of the object language in their own formalism. Creating such\ngrammars is a costly and error-prone process, especially for realistic\nlanguages such as Java and C++. Approach: In this paper we present Concretely,\na technique to extend meta programming systems with pluggable concrete syntax\npatterns, based on external, black box parsers. We illustrate Concretely in the\ncontext of Rascal, an open-source meta programming system and language\nworkbench, and show how to reuse existing parsers for Java, JavaScript, and\nC++. Furthermore, we propose Tympanic, a DSL to declaratively map external AST\nstructures to Rascal's internal data structures. Tympanic allows implementors\nof Concretely to solve the impedance mismatch between object-oriented class\nhierarchies in Java and Rascal's algebraic data types. Both the algebraic data\ntype and AST marshalling code is automatically generated. Knowledge: The\nconceptual architecture of Concretely and Tympanic supports the reuse of\npre-existing, external parsers, and their AST representation in meta\nprogramming systems that feature concrete syntax patterns for matching and\nconstructing syntax trees. As such this opens up concrete syntax pattern\nmatching for a host of realistic languages for which writing a grammar from\nscratch is time consuming and error-prone, but for which industry-strength\nparsers exist in the wild. Grounding: We evaluate Concretely in terms of source\nlines of code (SLOC), relative to the size of the AST data type and marshalling\ncode. We show that for real programming languages such as C++ and Java, adding\nsupport for concrete syntax patterns takes an effort only in the order of\ndozens of SLOC. Similarly, we evaluate Tympanic in terms of SLOC, showing an\norder of magnitude of reduction in SLOC compared to manual implementation of\nthe AST data types and marshalling code. Importance: Meta programming has\napplications in reverse engineering, reengineering, source code analysis,\nstatic analysis, software renovation, domain-specific language engineering, and\nmany others. Processing of syntax trees is central to all of these tasks.\nConcrete syntax patterns improve the practice of constructing meta programs.\nThe combination of Concretely and Tympanic has the potential to make concrete\nsyntax patterns available with very little effort, thereby improving and\npromoting the application of meta programming in the general software\nengineering context.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 20:14:45 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Aarssen", "Rodin", "", "CWI, Netherlands"], ["Vinju", "Jurgen", "", "CWI, Netherlands"], ["van der Storm", "Tijs", "", "CWI, Netherlands"]]}, {"id": "1902.00544", "submitter": "Andi Bejleri", "authors": "Andi Bejleri (TU Darmstadt, Germany), Elton Domnori (Epoka University,\n  Albania), Malte Viering (TU Darmstadt, Germany), Patrick Eugster (Universita\n  della Svizzera Italiana, Switzerland), Mira Mezini (TU Darmstadt, Germany)", "title": "Comprehensive Multiparty Session Types", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2019, Vol. 3,\n  Issue 3, Article 6", "doi": "10.22152/programming-journal.org/2019/3/6", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiparty session types (MST) are a well-established type theory that\ndescribes the interactive structure of a fixed number of components from a\nglobal point of view and type-checks the components through projection of the\nglobal type onto the participants of the session. They guarantee\ncommunicationsafety for a language of multiparty sessions (LMS), i.e.,\ndistributed, parallel components can exchange values without deadlocking and\nunexpected message types. Several variants of MST and LMS have been proposed to\nstudy key features of distributed and parallel programming. We observe that the\npopulation of the considered variants follows from only one ancestor, i.e., the\noriginal LMS/MST, and there are overlapping traits between features of the\nconsidered variants and the original. These hamper evolution of session types\nand languages and their adoption in practice. This paper addresses the\nfollowing question: What are the essential features for MST and LMS, and how\ncan these be modelled with simple constructs? To the best of our knowledge,\nthis is the first time this question has been addressed. We performed a\nsystematic analysis of the features and the constructs in MST, LMS, and the\nconsidered variants to identify the essential features. The variants are among\nthe most influential (according to Google Scholar) and well-established systems\nthat cover a wide set of areas in distributed, parallel programming. We used\nclassical techniques of formal models such as BNF, structural congruence, small\nstep operational semantics and typing judgments to build our language and type\nsystem. Lastly, the coherence of operational semantics and type system is\nproven by induction. This paper proposes a set of essential features, a\nlanguage of structured interactions and a type theory of comprehensive\nmultiparty session types, including global types and type system. The analysis\nremoves overlapping features and captures the shared traits, thereby\nintroducing the essential features. The constructs of the language are simple\nand fundamental, based on the $\\lambda$ and $\\pi$ calculi. Analogously, our\nglobal types reflect what is omitted and introduced in the language. Our system\ncovers all the features of the original and variants, with a better ratio of\nthe number of language and type constructs over the number of covered features.\nThe features of the original, variants, and our system along with the number of\nconstructs in the respective language and global types to model them are\npresented through a table. The syntax, operational semantics, meta-theory and\ntype system of our system are given. We modelled all the motivating examples of\nthe variants in our model, describing the reduction and typing steps. The work\ndiscusses how new features, in particular the non-essential ones (formerly\nexcluded) and advanced ones can be either modelled atop the essential ones or\nadded with minimal efforts, i.e. without modifying the existing ones. The\nfundamental properties of typed processes such as subject reduction,\ncommunication safety, and progress are established.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 20:15:01 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Bejleri", "Andi", "", "TU Darmstadt, Germany"], ["Domnori", "Elton", "", "Epoka University,\n  Albania"], ["Viering", "Malte", "", "TU Darmstadt, Germany"], ["Eugster", "Patrick", "", "Universita\n  della Svizzera Italiana, Switzerland"], ["Mezini", "Mira", "", "TU Darmstadt, Germany"]]}, {"id": "1902.00545", "submitter": "Philipp Seifer", "authors": "Philipp Seifer (University of Koblenz-Landau, Germany), Martin\n  Leinberger (University of Koblenz-Landau, Germany), Ralf L\\\"ammel (University\n  of Koblenz-Landau, Germany), Steffen Staab (University of Koblenz-Landau and\n  University of Southampton, Germany)", "title": "Semantic Query Integration With Reason", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2019, Vol. 3,\n  Issue 3, Article 13", "doi": "10.22152/programming-journal.org/2019/3/13", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-based data models allow for flexible data representation. In\nparticular, semantic data based on RDF and OWL fuels use cases ranging from\ngeneral knowledge graphs to domain specific knowledge in various technological\nor scientific domains. The flexibility of such approaches, however, makes\nprogramming with semantic data tedious and error-prone. In particular the\nlogics-based data descriptions employed by OWL are problematic for existing\nerror-detecting techniques, such as type systems. In this paper, we present\nDOTSpa, an advanced integration of semantic data into programming. We embed\ndescription logics, the logical foundations of OWL, into the type checking\nprocess of a statically typed programming language and provide typed data\naccess through an embedding of the query language SPARQL. In addition, we\ndemonstrate a concrete implementation of the approach, by extending the Scala\nprogramming language. We qualitatively compare programs using our approach to\nequivalent programs using a state-of-the-art library, in terms of how both\nframeworks aid users in the handling of typical failure scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 20:16:13 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 13:55:11 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Seifer", "Philipp", "", "University of Koblenz-Landau, Germany"], ["Leinberger", "Martin", "", "University of Koblenz-Landau, Germany"], ["L\u00e4mmel", "Ralf", "", "University\n  of Koblenz-Landau, Germany"], ["Staab", "Steffen", "", "University of Koblenz-Landau and\n  University of Southampton, Germany"]]}, {"id": "1902.00546", "submitter": "Hrshikesh Arora", "authors": "Hrshikesh Arora (Victoria University of Wellington, New Zealand),\n  Marco Servetto (Victoria University Wellington, New Zealand), Bruno C. D. S.\n  Oliveira (The University of Hong Kong, Hong Kong)", "title": "Separating Use and Reuse to Improve Both", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2019, Vol. 3,\n  Issue 3, Article 12", "doi": "10.22152/programming-journal.org/2019/3/12", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Trait composition has inspired new research in the area of code\nreuse for object oriented (OO) languages. One of the main advantages of this\nkind of composition is that it makes possible to separate subtyping from\nsubclassing; which is good for code-reuse, design and reasoning. However,\nhandling of state within traits is difficult, verbose or inelegant. Inquiry: We\nidentify the this-leaking problem as the fundamental limitation that prevents\nthe separation of subtyping from subclassing in conventional OO languages. We\nexplain that the concept of trait composition addresses this problem, by\ndistinguishing code designed for use (as a type) from code designed for reuse\n(i.e. inherited). We are aware of at least 3 concrete independently designed\nresearch languages following this methodology: TraitRecordJ, Package Templates\nand DeepFJig. Approach: In this paper, we design $42_\\mu$ a new language, where\nwe improve use and reuse and support the This type and family polymorphism by\ndistinguishing code designed for use from code designed for reuse. In this way\n$42_\\mu$ synthesise the 3 approaches above, and improves them with abstract\nstate operations: a new elegant way to handle state composition in trait based\nlanguages. Knowledge and Grounding: Using case studies, we show that $42_\\mu$'s\nmodel of traits with abstract state operations is more usable and compact than\nprior work. We formalise our work and prove that type errors cannot arise from\ncomposing well typed code. Importance: This work is the logical core of the\nprogramming language 42. This shows that the ideas presented in this paper can\nbe applicable to a full general purpose language. This form of composition is\nvery flexible and could be used in many new languages.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 20:16:32 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Arora", "Hrshikesh", "", "Victoria University of Wellington, New Zealand"], ["Servetto", "Marco", "", "Victoria University Wellington, New Zealand"], ["Oliveira", "Bruno C. D. S.", "", "The University of Hong Kong, Hong Kong"]]}, {"id": "1902.00548", "submitter": "Weixin Zhang", "authors": "Weixin Zhang (The University of Hong Kong, Hong Kong), Bruno Oliveira\n  (The University of Hong Kong, Hong Kong)", "title": "Shallow EDSLs and Object-Oriented Programming: Beyond Simple\n  Compositionality", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2019, Vol. 3,\n  Issue 3, Article 10", "doi": "10.22152/programming-journal.org/2019/3/10", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Embedded Domain-Specific Languages (EDSLs) are a common and widely\nused approach to DSLs in various languages, including Haskell and Scala. There\nare two main implementation techniques for EDSLs: shallow embeddings and deep\nembeddings. Inquiry: Shallow embeddings are quite simple, but they have been\ncriticized in the past for being quite limited in terms of modularity and\nreuse. In particular, it is often argued that supporting multiple DSL\ninterpretations in shallow embeddings is difficult. Approach: This paper argues\nthat shallow EDSLs and Object-Oriented Programming (OOP) are closely related.\nGibbons and Wu already discussed the relationship between shallow EDSLs and\nprocedural abstraction, while Cook discussed the connection between procedural\nabstraction and OOP. We make the transitive step in this paper by connecting\nshallow EDSLs directly to OOP via procedural abstraction. The knowledge about\nthis relationship enables us to improve on implementation techniques for EDSLs.\nKnowledge: This paper argues that common OOP mechanisms (including inheritance,\nsubtyping, and type-refinement) increase the modularity and reuse of shallow\nEDSLs when compared to classical procedural abstraction by enabling a simple\nway to express multiple, possibly dependent, interpretations. Grounding: We\nmake our arguments by using Gibbons and Wu's examples, where procedural\nabstraction is used in Haskell to model a simple shallow EDSL. We recode that\nEDSL in Scala and with an improved OO-inspired Haskell encoding. We further\nillustrate our approach with a case study on refactoring a deep external SQL\nquery processor to make it more modular, shallow, and embedded. Importance:\nThis work is important for two reasons. Firstly, from an intellectual point of\nview, this work establishes the connection between shallow embeddings and OOP,\nwhich enables a better understanding of both concepts. Secondly, this work\nillustrates programming techniques that can be used to improve the modularity\nand reuse of shallow EDSLs.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 20:16:52 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Zhang", "Weixin", "", "The University of Hong Kong, Hong Kong"], ["Oliveira", "Bruno", "", "The University of Hong Kong, Hong Kong"]]}, {"id": "1902.00660", "submitter": "James Larus", "authors": "Nachshon Cohen, David T. Aksun, Hillel Avni, James R. Larus", "title": "Fine-Grain Checkpointing with In-Cache-Line Logging", "comments": "In 2019 Architectural Support for Programming Languages and Operating\n  Systems (ASPLOS 19), April 13, 2019, Providence, RI, USA", "journal-ref": null, "doi": "10.1145/3297858.3304046", "report-no": null, "categories": "cs.OS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Volatile Memory offers the possibility of implementing high-performance,\ndurable data structures. However, achieving performance comparable to\nwell-designed data structures in non-persistent (transient) memory is\ndifficult, primarily because of the cost of ensuring the order in which memory\nwrites reach NVM. Often, this requires flushing data to NVM and waiting a full\nmemory round-trip time.\n  In this paper, we introduce two new techniques: Fine-Grained Checkpointing,\nwhich ensures a consistent, quickly recoverable data structure in NVM after a\nsystem failure, and In-Cache-Line Logging, an undo-logging technique that\nenables recovery of earlier state without requiring cache-line flushes in the\nnormal case. We implemented these techniques in the Masstree data structure,\nmaking it persistent and demonstrating the ease of applying them to a highly\noptimized system and their low (5.9-15.4\\%) runtime overhead cost.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 07:22:25 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Cohen", "Nachshon", ""], ["Aksun", "David T.", ""], ["Avni", "Hillel", ""], ["Larus", "James R.", ""]]}, {"id": "1902.00735", "submitter": "Joe Politz", "authors": "Joe Politz (University of California San Diego, United States),\n  Benjamin Lerner (Northeastern University, United States), Sorawee\n  Porncharoenwase (Brown University, United States), Shriram Krishnamurthi\n  (Brown University, United States)", "title": "Event Loops as First-Class Values: A Case Study in Pedagogic Language\n  Design", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2019, Vol. 3,\n  Issue 3, Article 11", "doi": "10.22152/programming-journal.org/2019/3/11", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World model is an existing functional input-output mechanism for\nevent-driven programming. It is used in numerous popular textbooks and\ncurricular settings. The World model conflates two different tasks -- the\ndefinition of an event processor and its execution -- into one. This conflation\nimposes a significant (even unacceptable) burden on student users in several\neducational settings where we have tried to use it, e.g., for teaching physics.\nWhile it was tempting to pile on features to address these issues, we instead\nused the Scheme language design dictum of removing weaknesses that made them\nseem necessary. By separating the two tasks above, we arrived at a slightly\ndifferent primitive, the reactor, as our basis. This only defines the event\nprocessor, and a variety of execution operators dictate how it runs. The new\ndesign enables programmatic control over event-driven programs. This simplifies\nreflecting on program behavior, and eliminates many unnecessary curricular\ndependencies imposed by the old design. This work has been implemented in the\nPyret programming language. The separation of concerns has enabled new\ncurricula, such as the Bootstrap:Physics curriculum, to take flight. Thousands\nof students use this new mechanism every year. We believe that reducing\nimpedance mismatches improves their educational experience.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 15:28:27 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Politz", "Joe", "", "University of California San Diego, United States"], ["Lerner", "Benjamin", "", "Northeastern University, United States"], ["Porncharoenwase", "Sorawee", "", "Brown University, United States"], ["Krishnamurthi", "Shriram", "", "Brown University, United States"]]}, {"id": "1902.01510", "submitter": "EPTCS", "authors": "Maribel Fern\\'andez, Ian Mackie", "title": "Proceedings Tenth International Workshop on Computing with Terms and\n  Graphs", "comments": null, "journal-ref": "EPTCS 288, 2019", "doi": "10.4204/EPTCS.288", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains a selection of the papers presented at TERMGRAPH 2018,\nthe tenth edition of the international workshop on computing with terms and\ngraphs. Graphs, and graph transformation systems, are used in many areas within\nComputer Science: to represent data structures and algorithms, to define\ncomputation models, as a general modelling tool to study complex systems, etc.\nResearch in this area addresses a range of theoretical and practical issues,\nincluding the modelling of first- and higher-order term rewriting by (acyclic\nor cyclic) graph rewriting, graphical frameworks such as interaction nets and\nsharing graphs (optimal reduction), rewrite calculi for the analysis of\nfunctional programs, graph reduction implementations of programming languages,\ngraphical calculi modelling concurrent and mobile computations, object-oriented\nsystems, graphs as a model of biological or chemical systems, and automated\nreasoning and symbolic computation systems working on shared structures.\nPrevious editions of TERMGRAPH took place in Barcelona (2002), Rome (2004),\nVienna (2006), Braga (2007), York (2009), Saarbrucken (2011), Rome (2013),\nVienna (2014) and Eindhoven (2016). TERMGRAPH 2018 is affiliated with FSCD,\nwhich is part of FLOC.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 01:17:47 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Fern\u00e1ndez", "Maribel", ""], ["Mackie", "Ian", ""]]}, {"id": "1902.01906", "submitter": "Linhai Song", "authors": "Zeming Yu and Linhai Song and Yiying Zhang", "title": "Fearless Concurrency? Understanding Concurrent Programming Safety in\n  Real-World Rust Software", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rust is a popular programming language in building various low-level software\nin recent years. It aims to provide safe concurrency when implementing\nmulti-threaded software through a suite of compiler checking rules.\nUnfortunately, there is limited understanding of how the checking rules\ninfluence the safety of concurrent programming in Rust applications.\n  In this paper, we perform a preliminary study on Rust's concurrency safety\nfrom two aspects: concurrency usage and concurrency bugs. Our study can provide\nbetter understanding on Rust's concurrency and can guide future researchers and\npractitioners in writing better, more reliable Rust software and in developing\ndebugging and bug detection tools for Rust.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 20:56:23 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Yu", "Zeming", ""], ["Song", "Linhai", ""], ["Zhang", "Yiying", ""]]}, {"id": "1902.02012", "submitter": "EPTCS", "authors": "Mitsuhiro Okada (Keio University), Yuta Takahashi (Nagoya University)", "title": "On Quasi Ordinal Diagram Systems", "comments": "In Proceedings TERMGRAPH 2018, arXiv:1902.01510", "journal-ref": "EPTCS 288, 2019, pp. 38-49", "doi": "10.4204/EPTCS.288.4", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purposes of this note are the following two; we first generalize\nOkada-Takeuti's well quasi ordinal diagram theory, utilizing the recent result\nof Dershowitz-Tzameret's version of tree embedding theorem with gap conditions.\nSecond, we discuss possible use of such strong ordinal notation systems for the\npurpose of a typical traditional termination proof method for term rewriting\nsystems, especially for second-order (pattern-matching-based) rewriting systems\nincluding a rewrite-theoretic version of Buchholz's hydra game.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 03:23:34 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Okada", "Mitsuhiro", "", "Keio University"], ["Takahashi", "Yuta", "", "Nagoya University"]]}, {"id": "1902.02816", "submitter": "Ajay Jain", "authors": "Charith Mendis, Ajay Jain, Paras Jain and Saman Amarasinghe", "title": "Revec: Program Rejuvenation through Revectorization", "comments": "The first two authors contributed equally to this work. Published at\n  CC 2019", "journal-ref": "Compiler Construction (CC) 2019", "doi": "10.1145/3302516.3307357", "report-no": null, "categories": "cs.PL cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern microprocessors are equipped with Single Instruction Multiple Data\n(SIMD) or vector instructions which expose data level parallelism at a fine\ngranularity. Programmers exploit this parallelism by using low-level vector\nintrinsics in their code. However, once programs are written using vector\nintrinsics of a specific instruction set, the code becomes non-portable. Modern\ncompilers are unable to analyze and retarget the code to newer vector\ninstruction sets. Hence, programmers have to manually rewrite the same code\nusing vector intrinsics of a newer generation to exploit higher data widths and\ncapabilities of new instruction sets. This process is tedious, error-prone and\nrequires maintaining multiple code bases. We propose Revec, a compiler\noptimization pass which revectorizes already vectorized code, by retargeting it\nto use vector instructions of newer generations. The transformation is\ntransparent, happening at the compiler intermediate representation level, and\nenables performance portability of hand-vectorized code.\n  Revec can achieve performance improvements in real-world performance critical\nkernels. In particular, Revec achieves geometric mean speedups of 1.160$\\times$\nand 1.430$\\times$ on fast integer unpacking kernels, and speedups of\n1.145$\\times$ and 1.195$\\times$ on hand-vectorized x265 media codec kernels\nwhen retargeting their SSE-series implementations to use AVX2 and AVX-512\nvector instructions respectively. We also extensively test Revec's impact on\n216 intrinsic-rich implementations of image processing and stencil kernels\nrelative to hand-retargeting.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 19:37:45 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Mendis", "Charith", ""], ["Jain", "Ajay", ""], ["Jain", "Paras", ""], ["Amarasinghe", "Saman", ""]]}, {"id": "1902.04373", "submitter": "Amir Kafshdar Goharshady", "authors": "Krishnendu Chatterjee and Hongfei Fu and Amir Kafshdar Goharshady and\n  Ehsan Kafshdar Goharshady", "title": "Polynomial Invariant Generation for Non-deterministic Recursive Programs", "comments": "A conference version of this article appears in PLDI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical problem of invariant generation for programs with\npolynomial assignments and focus on synthesizing invariants that are a\nconjunction of strict polynomial inequalities. We present a sound and\nsemi-complete method based on positivstellensaetze, i.e. theorems in\nsemi-algebraic geometry that characterize positive polynomials over a\nsemi-algebraic set. To the best of our knowledge, this is the first invariant\ngeneration method to provide completeness guarantees for invariants consisting\nof polynomial inequalities. Moreover, on the theoretical side, the worst-case\ncomplexity of our approach is subexponential, whereas the worst-case complexity\nof the previously-known complete method (Colon et al, CAV 2003), which could\nonly handle linear invariants, is exponential. On the practical side, we reduce\nthe invariant generation problem to quadratic programming (QCLP), which is a\nclassical optimization problem with many industrial solvers. Finally, we\ndemonstrate the applicability of our approach by providing experimental results\non several academic benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 13:17:25 GMT"}, {"version": "v2", "created": "Sun, 7 Jul 2019 16:55:34 GMT"}, {"version": "v3", "created": "Mon, 6 Apr 2020 14:18:22 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Fu", "Hongfei", ""], ["Goharshady", "Amir Kafshdar", ""], ["Goharshady", "Ehsan Kafshdar", ""]]}, {"id": "1902.04645", "submitter": "Cristina Matache", "authors": "Cristina Matache", "title": "Program Equivalence for Algebraic Effects via Modalities", "comments": "Master Thesis, University of Oxford (September 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This dissertation is concerned with the study of program equivalence and\nalgebraic effects as they arise in the theory of programming languages.\nAlgebraic effects represent impure behaviour in a functional programming\nlanguage, such as input and output, exceptions, nondeterminism etc. all treated\nin a generic way. Program equivalence aims to identify which programs can be\nconsidered equal in some sense. This question has been studied for a long time\nbut has only recently been extended to languages with algebraic effects, which\nare a newer development. Much work remains to be done in order to understand\nprogram equivalence in the presence of algebraic effects. In particular, there\nis no characterisation of contextual equivalence using a logic. We define a\nlogic whose formulas express properties of higher-order programs with algebraic\neffects. We then investigate three notions of program equivalence for algebraic\neffects: logical equivalence induced by the aforementioned logic, applicative\nbisimilarity and contextual equivalence. For the programming language used in\nthis dissertation, we prove that they all coincide. Therefore, the main novel\ncontribution of the dissertation is defining the first logic for algebraic\neffects whose induced program equivalence coincides with contextual\nequivalence.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 21:35:51 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Matache", "Cristina", ""]]}, {"id": "1902.04659", "submitter": "Amir Kafshdar Goharshady", "authors": "Peixin Wang and Hongfei Fu and Amir Kafshdar Goharshady and Krishnendu\n  Chatterjee and Xudong Qin and Wenjun Shi", "title": "Cost Analysis of Nondeterministic Probabilistic Programs", "comments": "A conference version will appear in the 40th ACM Conference on\n  Programming Language Design and Implementation (PLDI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of expected cost analysis over nondeterministic\nprobabilistic programs, which aims at automated methods for analyzing the\nresource-usage of such programs. Previous approaches for this problem could\nonly handle nonnegative bounded costs. However, in many scenarios, such as\nqueuing networks or analysis of cryptocurrency protocols, both positive and\nnegative costs are necessary and the costs are unbounded as well.\n  In this work, we present a sound and efficient approach to obtain polynomial\nbounds on the expected accumulated cost of nondeterministic probabilistic\nprograms. Our approach can handle (a) general positive and negative costs with\nbounded updates in variables; and (b) nonnegative costs with general updates to\nvariables. We show that several natural examples which could not be handled by\nprevious approaches are captured in our framework.\n  Moreover, our approach leads to an efficient polynomial-time algorithm, while\nno previous approach for cost analysis of probabilistic programs could\nguarantee polynomial runtime. Finally, we show the effectiveness of our\napproach by presenting experimental results on a variety of programs, motivated\nby real-world applications, for which we efficiently synthesize tight\nresource-usage bounds.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 22:18:26 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 09:59:09 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Wang", "Peixin", ""], ["Fu", "Hongfei", ""], ["Goharshady", "Amir Kafshdar", ""], ["Chatterjee", "Krishnendu", ""], ["Qin", "Xudong", ""], ["Shi", "Wenjun", ""]]}, {"id": "1902.04738", "submitter": "Emery Berger", "authors": "Bobby Powers, David Tench, Emery D. Berger, Andrew McGregor", "title": "Mesh: Compacting Memory Management for C/C++ Applications", "comments": "Draft version, accepted at PLDI 2019", "journal-ref": null, "doi": "10.1145/3314221.3314582", "report-no": null, "categories": "cs.PL cs.DS cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programs written in C/C++ can suffer from serious memory fragmentation,\nleading to low utilization of memory, degraded performance, and application\nfailure due to memory exhaustion. This paper introduces Mesh, a plug-in\nreplacement for malloc that, for the first time, eliminates fragmentation in\nunmodified C/C++ applications. Mesh combines novel randomized algorithms with\nwidely-supported virtual memory operations to provably reduce fragmentation,\nbreaking the classical Robson bounds with high probability. Mesh generally\nmatches the runtime performance of state-of-the-art memory allocators while\nreducing memory consumption; in particular, it reduces the memory of\nconsumption of Firefox by 16% and Redis by 39%.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 04:40:05 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 19:03:03 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Powers", "Bobby", ""], ["Tench", "David", ""], ["Berger", "Emery D.", ""], ["McGregor", "Andrew", ""]]}, {"id": "1902.04744", "submitter": "Peixin Wang", "authors": "Peixin Wang, Hongfei Fu, Krishnendu Chatterjee, Yuxin Deng, Ming Xu", "title": "Proving Expected Sensitivity of Probabilistic Programs with Randomized\n  Variable-Dependent Termination Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of program sensitivity (aka Lipschitz continuity) specifies that\nchanges in the program input result in proportional changes to the program\noutput. For probabilistic programs the notion is naturally extended to expected\nsensitivity. A previous approach develops a relational program logic framework\nfor proving expected sensitivity of probabilistic while loops, where the number\nof iterations is fixed and bounded. In this work, we consider probabilistic\nwhile loops where the number of iterations is not fixed, but randomized and\ndepends on the initial input values. We present a sound approach for proving\nexpected sensitivity of such programs. Our sound approach is martingale-based\nand can be automated through existing martingale-synthesis algorithms.\nFurthermore, our approach is compositional for sequential composition of while\nloops under a mild side condition. We demonstrate the effectiveness of our\napproach on several classical examples from Gambler's Ruin, stochastic hybrid\nsystems and stochastic gradient descent. We also present experimental results\nshowing that our automated approach can handle various probabilistic programs\nin the literature.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 05:32:09 GMT"}, {"version": "v2", "created": "Sat, 13 Jul 2019 09:31:56 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 13:59:53 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Wang", "Peixin", ""], ["Fu", "Hongfei", ""], ["Chatterjee", "Krishnendu", ""], ["Deng", "Yuxin", ""], ["Xu", "Ming", ""]]}, {"id": "1902.04836", "submitter": "Thomas Ehrhard", "authors": "Thomas Ehrhard (IRIF)", "title": "Differentials and distances in probabilistic coherence spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In probabilistic coherence spaces, a denotational model of probabilistic\nfunctional languages, mor-phisms are analytic and therefore smooth. We explore\ntwo related applications of the corresponding derivatives. First we show how\nderivatives allow to compute the expectation of execution time in the weak head\nreduction of probabilistic PCF (pPCF). Next we apply a general notion of\n\"local\" differential of morphisms to the proof of a Lipschitz property of these\nmorphisms allowing in turn to relate the observational distance on pPCF terms\nto a distance the model is naturally equipped with. This suggests that\nextending probabilistic programming languages with derivatives, in the spirit\nof the differential lambda-calculus, could be quite meaningful.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 10:12:36 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Ehrhard", "Thomas", "", "IRIF"]]}, {"id": "1902.05178", "submitter": "Ben Titzer", "authors": "Ross Mcilroy, Jaroslav Sevcik, Tobias Tebbi, Ben L. Titzer, Toon\n  Verwaest", "title": "Spectre is here to stay: An analysis of side-channels and speculative\n  execution", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent discovery of the Spectre and Meltdown attacks represents a\nwatershed moment not just for the field of Computer Security, but also of\nProgramming Languages. This paper explores speculative side-channel attacks and\ntheir implications for programming languages. These attacks leak information\nthrough micro-architectural side-channels which we show are not mere bugs, but\nin fact lie at the foundation of optimization. We identify three open problems,\n(1) finding side-channels, (2) understanding speculative vulnerabilities, and\n(3) mitigating them. For (1) we introduce a mathematical meta-model that\nclarifies the source of side-channels in simulations and CPUs. For (2) we\nintroduce an architectural model with speculative semantics to study\nrecently-discovered vulnerabilities. For (3) we explore and evaluate software\nmitigations and prove one correct for this model. Our analysis is informed by\nextensive offensive research and defensive implementation work for V8, the\nproduction JavaScript virtual machine in Chrome. Straightforward extensions to\nmodel real hardware suggest these vulnerabilities present formidable challenges\nfor effective, efficient mitigation. As a result of our work, we now believe\nthat speculative vulnerabilities on today's hardware defeat all\nlanguage-enforced confidentiality with no known comprehensive software\nmitigations, as we have discovered that untrusted code can construct a\nuniversal read gadget to read all memory in the same address space through\nside-channels. In the face of this reality, we have shifted the security model\nof the Chrome web browser and V8 to process isolation.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 01:20:52 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Mcilroy", "Ross", ""], ["Sevcik", "Jaroslav", ""], ["Tebbi", "Tobias", ""], ["Titzer", "Ben L.", ""], ["Verwaest", "Toon", ""]]}, {"id": "1902.05205", "submitter": "Luis Garcia", "authors": "Luis Garcia, Stefan Mitsch, Andre Platzer", "title": "HyPLC: Hybrid Programmable Logic Controller Program Translation for\n  Verification", "comments": "13 pages, 9 figures. ICCPS 2019", "journal-ref": null, "doi": "10.1145/3302509.3311036", "report-no": null, "categories": "cs.PL cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programmable Logic Controllers (PLCs) provide a prominent choice of\nimplementation platform for safety-critical industrial control systems. Formal\nverification provides ways of establishing correctness guarantees, which can be\nquite important for such safety-critical applications. But since PLC code does\nnot include an analytic model of the system plant, their verification is\nlimited to discrete properties. In this paper, we, thus, start the other way\naround with hybrid programs that include continuous plant models in addition to\ndiscrete control algorithms. Even deep correctness properties of hybrid\nprograms can be formally verified in the theorem prover KeYmaera X that\nimplements differential dynamic logic, dL, for hybrid programs. After verifying\nthe hybrid program, we now present an approach for translating hybrid programs\ninto PLC code. The new tool, HyPLC, implements this translation of discrete\ncontrol code of verified hybrid program models to PLC controller code and, vice\nversa, the translation of existing PLC code into the discrete control actions\nfor a hybrid program given an additional input of the continuous dynamics of\nthe system to be verified. This approach allows for the generation of real\ncontroller code while preserving, by compilation, the correctness of a valid\nand verified hybrid program. PLCs are common cyber-physical interfaces for\nsafety-critical industrial control applications, and HyPLC serves as a\npragmatic tool for bridging formal verification of complex cyber-physical\nsystems at the algorithmic level of hybrid programs with the execution layer of\nconcrete PLC implementations.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 03:48:04 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Garcia", "Luis", ""], ["Mitsch", "Stefan", ""], ["Platzer", "Andre", ""]]}, {"id": "1902.05283", "submitter": "Lau Skorstengaard", "authors": "Lau Skorstengaard, Dominique Devriese, Lars Birkedal", "title": "Reasoning About a Machine with Local Capabilities: Provably Safe Stack\n  and Return Pointer Management - Technical Appendix Including Proofs and\n  Details", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a calling convention for capability machines with local\ncapabilities. The calling convention ensures local-state encapsulation and\nwell-bracketed control flow. We use the calling convention in a hand-full of\nprogram examples and prove that they behave correctly. The correctness proofs\nuse a logical relation that is also presented in this appendix. This is the\ntechnical appendix for the paper with the same name and authors accepted at\nESOP18 and under review for TOPLAS.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 09:55:35 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Skorstengaard", "Lau", ""], ["Devriese", "Dominique", ""], ["Birkedal", "Lars", ""]]}, {"id": "1902.05369", "submitter": "Luca Roversi", "authors": "Claudio Grandi, Dariush Moshiri and Luca Roversi", "title": "Introducing Yet Another REversible Language", "comments": "6 pages. On-line at https://yarel-di.github.io/yarel/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Yarel is a core reversible programming language that implements a class of\npermutations, defined recursively, which are primitive recursive complete. The\ncurrent release of Yarel syntax and operational semantics, implemented by\ncompiling Yarel to Java, is 0.1.0, according to Semantic Versioning 2.0.0.\nYarel comes with Yarel-IDE, developed as an Eclipse plug-in by means of XText.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 14:24:59 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Grandi", "Claudio", ""], ["Moshiri", "Dariush", ""], ["Roversi", "Luca", ""]]}, {"id": "1902.05436", "submitter": "Himanshu Arora", "authors": "Himanshu Arora (1), Raghavan Komondoor (1), G. Ramalingam (2) ((1)\n  Indian Institute of Science, Bangalore, (2) Microsoft Research)", "title": "Checking Observational Purity of Procedures", "comments": "FASE 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verifying whether a procedure is observationally pure is useful in many\nsoftware engineering scenarios. An observationally pure procedure always\nreturns the same value for the same argument, and thus mimics a mathematical\nfunction. The problem is challenging when procedures use private mutable global\nvariables, e.g., for memoization of frequently returned answers, and when they\ninvolve recursion.\n  We present a novel verification approach for this problem. Our approach\ninvolves encoding the procedure's code as a formula that is a disjunction of\npath constraints, with the recursive calls being replaced in the formula with\nreferences to a mathematical function symbol. Then, a theorem prover is invoked\nto check whether the formula that has been constructed agrees with the function\nsymbol referred to above in terms of input-output behavior for all arguments.\n  We evaluate our approach on a set of realistic examples, using the Boogie\nintermediate language and theorem prover. Our evaluation shows that the\ninvariants are easy to construct manually, and that our approach is effective\nat verifying observationally pure procedures.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 15:30:01 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Arora", "Himanshu", ""], ["Komondoor", "Raghavan", ""], ["Ramalingam", "G.", ""]]}, {"id": "1902.05462", "submitter": "Pengfei Su", "authors": "Pengfei Su, Shasha Wen, Hailong Yang, Milind Chabbi and Xu Liu", "title": "Redundant Loads: A Software Inefficiency Indicator", "comments": "This paper is a full-version of our ICSE paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern software packages have become increasingly complex with millions of\nlines of code and references to many external libraries. Redundant operations\nare a common performance limiter in these code bases. Missed compiler\noptimization opportunities, inappropriate data structure and algorithm choices,\nand developers' inattention to performance are some common reasons for the\nexistence of redundant operations. Developers mainly depend on compilers to\neliminate redundant operations. However, compilers' static analysis often\nmisses optimization opportunities due to ambiguities and limited analysis\nscope; automatic optimizations to algorithmic and data structural problems are\nout of scope.\n  We develop LoadSpy, a whole-program profiler to pinpoint redundant memory\nload operations, which are often a symptom of many redundant operations. The\nstrength of LoadSpy exists in identifying and quantifying redundant load\noperations in programs and associating the redundancies with program execution\ncontexts and scopes to focus developers' attention on problematic code. LoadSpy\nworks on fully optimized binaries, adopts various optimization techniques to\nreduce its overhead, and provides a rich graphic user interface, which make it\na complete developer tool. Applying LoadSpy showed that a large fraction of\nredundant loads is common in modern software packages despite highest levels of\nautomatic compiler optimizations. Guided by LoadSpy, we optimize several\nwell-known benchmarks and real-world applications, yielding significant\nspeedups.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 15:54:17 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Su", "Pengfei", ""], ["Wen", "Shasha", ""], ["Yang", "Hailong", ""], ["Chabbi", "Milind", ""], ["Liu", "Xu", ""]]}, {"id": "1902.05464", "submitter": "Daco Harkes", "authors": "Daco Harkes (Delft University of Technology, Netherlands)", "title": "We should Stop Claiming Generality in our Domain-Specific Language\n  Papers", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2019, Vol. 3,\n  Essays, Article 18", "doi": "10.22152/programming-journal.org/2019/3/18", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our community believes that new domain-specific languages should be as\ngeneral as possible to increase their impact. However, I argue in this essay\nthat we should stop claiming generality for new domain-specific languages. More\ngeneral domain-specific languages induce more boilerplate code. Moreover,\ndomain-specific languages are co-developed with their applications in practice,\nand tend to be specific for these applications. Thus, I argue we should stop\nclaiming generality in favor of documenting how domain-specific language based\nsoftware development is beneficial to the overall software development process.\nThe acceptance criteria for scientific literature should make the same shift:\naccepting good domain-specific language engineering practice, instead of the\nnext language to rule them all.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 15:57:22 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Harkes", "Daco", "", "Delft University of Technology, Netherlands"]]}, {"id": "1902.05594", "submitter": "Aleksandar S. Dimovski", "authors": "Aleksandar S. Dimovski and Axel Legay and Andrzej Wasowski", "title": "Variability Abstraction and Refinement for Game-based Lifted Model\n  Checking of full CTL (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variability models allow effective building of many custom model variants for\nvarious configurations. Lifted model checking for a variability model is\ncapable of verifying all its variants simultaneously in a single run by\nexploiting the similarities between the variants. The computational cost of\nlifted model checking still greatly depends on the number of variants (the size\nof configuration space), which is often huge.\n  One of the most promising approaches to fighting the configuration space\nexplosion problem in lifted model checking are variability abstractions. In\nthis work, we define a novel game-based approach for variability-specific\nabstraction and refinement for lifted model checking of the full CTL,\ninterpreted over 3-valued semantics. We propose a direct algorithm for solving\na 3-valued (abstract) lifted model checking game. In case the result of model\nchecking an abstract variability model is indefinite, we suggest a new notion\nof refinement, which eliminates indefinite results. This provides an iterative\nincremental variability-specific abstraction and refinement framework, where\nrefinement is applied only where indefinite results exist and definite results\nfrom previous iterations are reused.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 20:23:43 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Dimovski", "Aleksandar S.", ""], ["Legay", "Axel", ""], ["Wasowski", "Andrzej", ""]]}, {"id": "1902.05870", "submitter": "Arjun Guha", "authors": "Abhinav Jangda, Donald Pinckney, Yuriy Brun and Arjun Guha", "title": "Formal Foundations of Serverless Computing", "comments": null, "journal-ref": "PACMPL, OOPSLA issue, vol. 3, October 2019, pp. 149:1-149:26", "doi": "10.1145/3360575", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing (also known as functions as a service) is a new cloud\ncomputing abstraction that makes it easier to write robust, large-scale web\nservices. In serverless computing, programmers write what are called serverless\nfunctions, and the cloud platform transparently manages the operating system,\nresource allocation, load-balancing, and fault tolerance. When demand for the\nservice spikes, the platform automatically allocates additional hardware to the\nservice and manages load-balancing; when demand falls, the platform silently\ndeallocates idle resources; and when the platform detects a failure, it\ntransparently retries affected requests. In 2014, Amazon Web Services\nintroduced the first serverless platform, AWS Lambda, and similar abstractions\nare now available on all major cloud computing platforms.\n  Unfortunately, the serverless computing abstraction exposes several low-level\noperational details that make it hard for programmers to write and reason about\ntheir code. This paper sheds light on this problem by presenting\n$\\lambda_\\Lambda$, an operational semantics of the essence of serverless\ncomputing. Despite being a small (half a page) core calculus, $\\lambda_\\Lambda$\nmodels all the low-level details that serverless functions can observe. To show\nthat $\\lambda_\\Lambda$ is useful, we present three applications. First, to ease\nreasoning about code, we present a simplified naive semantics of serverless\nexecution and precisely characterize when the naive semantics and\n$\\lambda_\\Lambda$ coincide. Second, we augment $\\lambda_\\Lambda$ with a\nkey-value store to allow reasoning about stateful serverless functions. Third,\nsince a handful of serverless platforms support serverless function\ncomposition, we show how to extend $\\lambda_\\Lambda$ with a composition\nlanguage. We have implemented this composition language and show that it\noutperforms prior work.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 16:16:41 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 02:30:30 GMT"}, {"version": "v3", "created": "Mon, 1 Jul 2019 16:22:48 GMT"}, {"version": "v4", "created": "Sun, 20 Oct 2019 15:09:08 GMT"}, {"version": "v5", "created": "Wed, 20 Nov 2019 11:58:35 GMT"}, {"version": "v6", "created": "Sun, 4 Oct 2020 18:42:44 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Jangda", "Abhinav", ""], ["Pinckney", "Donald", ""], ["Brun", "Yuriy", ""], ["Guha", "Arjun", ""]]}, {"id": "1902.05945", "submitter": "Giulio Guerrieri", "authors": "Beniamino Accattoli, Giulio Guerrieri, Maico Leberle", "title": "Types by Need (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cornerstone of the theory of lambda-calculus is that intersection types\ncharacterise termination properties. They are a flexible tool that can be\nadapted to various notions of termination, and that also induces adequate\ndenotational models.\n  Since the seminal work of de Carvalho in 2007, it is known that multi types\n(i.e. non-idempotent intersection types) refine intersection types with\nquantitative information and a strong connection to linear logic. Typically,\ntype derivations provide bounds for evaluation lengths, and minimal type\nderivations provide exact bounds.\n  De Carvalho studied call-by-name evaluation, and Kesner used his system to\nshow the termination equivalence of call-by-need and call-by-name. De\nCarvalho's system, however, cannot provide exact bounds on call-by-need\nevaluation lengths.\n  In this paper we develop a new multi type system for call-by-need. Our system\nproduces exact bounds and induces a denotational model of call-by-need,\nproviding the first tight quantitative semantics of call-by-need.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 18:57:33 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Accattoli", "Beniamino", ""], ["Guerrieri", "Giulio", ""], ["Leberle", "Maico", ""]]}, {"id": "1902.05983", "submitter": "Ravi Mangal", "authors": "Ravi Mangal, Aditya V. Nori, Alessandro Orso", "title": "Robustness of Neural Networks: A Probabilistic and Practical Approach", "comments": "Accepted for publication at ICSE-NIER 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are becoming increasingly prevalent in software, and it is\ntherefore important to be able to verify their behavior. Because verifying the\ncorrectness of neural networks is extremely challenging, it is common to focus\non the verification of other properties of these systems. One important\nproperty, in particular, is robustness. Most existing definitions of\nrobustness, however, focus on the worst-case scenario where the inputs are\nadversarial. Such notions of robustness are too strong, and unlikely to be\nsatisfied by-and verifiable for-practical neural networks. Observing that\nreal-world inputs to neural networks are drawn from non-adversarial probability\ndistributions, we propose a novel notion of robustness: probabilistic\nrobustness, which requires the neural network to be robust with at least $(1 -\n\\epsilon)$ probability with respect to the input distribution. This\nprobabilistic approach is practical and provides a principled way of estimating\nthe robustness of a neural network. We also present an algorithm, based on\nabstract interpretation and importance sampling, for checking whether a neural\nnetwork is probabilistically robust. Our algorithm uses abstract interpretation\nto approximate the behavior of a neural network and compute an\noverapproximation of the input regions that violate robustness. It then uses\nimportance sampling to counter the effect of such overapproximation and compute\nan accurate estimate of the probability that the neural network violates the\nrobustness property.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 20:44:17 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Mangal", "Ravi", ""], ["Nori", "Aditya V.", ""], ["Orso", "Alessandro", ""]]}, {"id": "1902.06056", "submitter": "Ankush Das", "authors": "Ankush Das, Stephanie Balzer, Jan Hoffmann, Frank Pfenning and Ishani\n  Santurkar", "title": "Resource-Aware Session Types for Digital Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming digital contracts comes with unique challenges, which include (i)\nexpressing and enforcing protocols of interaction, (ii) controlling resource\nusage, and (iii) preventing the duplication or deletion of a contract's assets.\nThis article presents the design and type-theoretic foundation of Nomos, a\nprogramming language for digital contracts that addresses these challenges. To\nexpress and enforce protocols, Nomos is based on shared binary session types.\nTo control resource usage, Nomos employs automatic amortized resource analysis.\nTo prevent the duplication or deletion of assets, Nomos uses a linear type\nsystem. A monad integrates the effectful session-typed language with a\ngeneral-purpose functional language. Nomos' prototype implementation features\nlinear-time type checking and efficient type reconstruction that includes\nautomatic inference of resource bounds via off-the-shelf linear optimization.\nThe effectiveness of the language is evaluated with case studies about\nimplementing common smart contracts such as auctions, elections, and\ncurrencies. Nomos is completely formalized, including the type system, a cost\nsemantics, and a transactional semantics to instantiate Nomos contracts on a\nblockchain. The type soundness proof ensures that protocols are followed at\nrun-time and that types establish sound upper bounds on the resource\nconsumption, ruling out re-entrancy and out-of-gas vulnerabilities.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 07:02:21 GMT"}, {"version": "v2", "created": "Sat, 2 Mar 2019 15:53:25 GMT"}, {"version": "v3", "created": "Fri, 22 Nov 2019 21:52:04 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Das", "Ankush", ""], ["Balzer", "Stephanie", ""], ["Hoffmann", "Jan", ""], ["Pfenning", "Frank", ""], ["Santurkar", "Ishani", ""]]}, {"id": "1902.06097", "submitter": "Andreas Abel", "authors": "Andreas Abel and Christian Sattler", "title": "Normalization by Evaluation for Call-by-Push-Value and Polarized\n  Lambda-Calculus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observe that normalization by evaluation for simply-typed lambda-calculus\nwith weak coproducts can be carried out in a weak bi-cartesian closed category\nof presheaves equipped with a monad that allows us to perform case distinction\non neutral terms of sum type. The placement of the monad influences the normal\nforms we obtain: for instance, placing the monad on coproducts gives us\neta-long beta-pi normal forms where pi refers to permutation of case\ndistinctions out of elimination positions. We further observe that placing the\nmonad on every coproduct is rather wasteful, and an optimal placement of the\nmonad can be determined by considering polarized simple types inspired by\nfocalization. Polarization classifies types into positive and negative, and it\nis sufficient to place the monad at the embedding of positive types into\nnegative ones. We consider two calculi based on polarized types: pure\ncall-by-push-value (CBPV) and polarized lambda-calculus, the natural deduction\ncalculus corresponding to focalized sequent calculus. For these two calculi, we\npresent algorithms for normalization by evaluation. We further discuss\ndifferent implementations of the monad and their relation to existing\nnormalization proofs for lambda-calculus with sums. Our developments have been\npartially formalized in the Agda proof assistant.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 12:26:22 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Abel", "Andreas", ""], ["Sattler", "Christian", ""]]}, {"id": "1902.06146", "submitter": "Peter Breuer", "authors": "Peter T. Breuer", "title": "Compiled Obfuscation for Data Structures in Encrypted Computing", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encrypted computing is an emerging technology based on a processor that\n`works encrypted', taking encrypted inputs to encrypted outputs while data\nremains in encrypted form throughout. It aims to secure user data against\npossible insider attacks by the operator and operating system (who do not know\nthe user's encryption key and cannot access it in the processor). Formally\n`obfuscating' compilation for encrypted computing is such that on each\nrecompilation of the source code, machine code of the same structure is emitted\nfor which runtime traces also all have the same structure but each word beneath\nthe encryption differs from nominal with maximal possible entropy across\nrecompilations. That generates classic cryptographic semantic security for\ndata, relative to the security of the encryption, but it guarantees only single\nwords and an adversary has more than that on which to base decryption attempts.\nThis paper extends the existing integer-based technology to doubles, floats,\narrays, structs and unions as data structures, covering ANSI C. A single\nprinciple drives compiler design and improves the existing security theory to\nquantitative results: every arithmetic instruction that writes must vary to the\nmaximal extent possible.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 19:51:31 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Breuer", "Peter T.", ""]]}, {"id": "1902.06590", "submitter": "Simon Oddershede Gregersen", "authors": "Simon Gregersen, S{\\o}ren Eller Thomsen, Aslan Askarov", "title": "A Dependently Typed Library for Static Information-Flow Control in Idris", "comments": "This is an extended version of a paper of the same title presented at\n  POST 2019", "journal-ref": null, "doi": "10.1007/978-3-030-17138-4_3", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safely integrating third-party code in applications while protecting the\nconfidentiality of information is a long-standing problem. Pure functional\nprogramming languages, like Haskell, make it possible to enforce lightweight\ninformation-flow control through libraries like MAC by Russo. This work\npresents DepSec, a MAC inspired, dependently typed library for static\ninformation-flow control in Idris. We showcase how adding dependent types\nincreases the expressiveness of state-of-the-art static information-flow\ncontrol libraries and how DepSec matches a special-purpose dependent\ninformation-flow type system on a key example. Finally, we show novel and\npowerful means of specifying statically enforced declassification policies\nusing dependent types.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 14:39:25 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Gregersen", "Simon", ""], ["Thomsen", "S\u00f8ren Eller", ""], ["Askarov", "Aslan", ""]]}, {"id": "1902.06733", "submitter": "Carsten Fuhs", "authors": "Carsten Fuhs, Cynthia Kop", "title": "A static higher-order dependency pair framework", "comments": "Extended version of a paper which is to appear in the proceedings of\n  ESOP 2019 (28th European Symposium on Programming). arXiv admin note: text\n  overlap with arXiv:1805.09390", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the static dependency pair method for proving termination of\nhigher-order term rewriting and extend it in a number of ways:\n  (1) We introduce a new rewrite formalism designed for general applicability\nin termination proving of higher-order rewriting, Algebraic Functional Systems\nwith Meta-variables. (2) We provide a syntactically checkable soundness\ncriterion to make the method applicable to a large class of rewrite systems.\n(3) We propose a modular dependency pair framework for this higher-order\nsetting. (4) We introduce a fine-grained notion of formative and computable\nchains to render the framework more powerful. (5) We formulate several existing\nand new termination proving techniques in the form of processors within our\nframework.\n  The framework has been implemented in the (fully automatic) higher-order\ntermination tool WANDA.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 18:22:35 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 17:07:15 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Fuhs", "Carsten", ""], ["Kop", "Cynthia", ""]]}, {"id": "1902.06950", "submitter": "Dominic Orchard", "authors": "Li-yao Xia, Dominic Orchard, Meng Wang", "title": "Composing bidirectional programs monadically (with appendices)", "comments": "Provides the appendices of the paper, which appears in the\n  proceedings of European Symposium on Programming (ESOP) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Software frequently converts data from one representation to another and vice\nversa. Naively specifying both conversion directions separately is error prone\nand introduces conceptual duplication. Instead, bidirectional programming\ntechniques allow programs to be written which can be interpreted in both\ndirections. However, these techniques often employ unfamiliar programming\nidioms via restricted, specialised combinator libraries. Instead, we introduce\na framework for composing bidirectional programs monadically, enabling\nbidirectional programming with familiar abstractions in functional languages\nsuch as Haskell. We demonstrate the generality of our approach applied to\nparsers/printers, lenses, and generators/predicates. We show how to leverage\ncompositionality and equational reasoning for the verification of\nround-tripping properties for such monadic bidirectional programs.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 08:47:23 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Xia", "Li-yao", ""], ["Orchard", "Dominic", ""], ["Wang", "Meng", ""]]}, {"id": "1902.07230", "submitter": "Andr\\'e Platzer", "authors": "Andr\\'e Platzer", "title": "Uniform Substitution At One Fell Swoop", "comments": "CADE 2019 Extending arXiv:1804.05880 with differential games\n  arXiv:1507.04943", "journal-ref": null, "doi": "10.1007/978-3-030-29436-6_25", "report-no": null, "categories": "cs.LO cs.PL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uniform substitution of function, predicate, program or game symbols is the\ncore operation in parsimonious provers for hybrid systems and hybrid games. By\npostponing soundness-critical admissibility checks, this paper introduces a\nuniform substitution mechanism that proceeds in a linear pass homomorphically\nalong the formula. Soundness is recovered using a simple variable condition at\nthe replacements performed by the substitution. The setting in this paper is\nthat of differential hybrid games, in which discrete, continuous, and\nadversarial dynamics interact in differential game logic dGL. This paper proves\nsoundness and completeness of one-pass uniform substitutions for dGL.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 19:01:03 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 16:41:35 GMT"}, {"version": "v3", "created": "Wed, 22 May 2019 19:45:51 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Platzer", "Andr\u00e9", ""]]}, {"id": "1902.07515", "submitter": "Fabian Kunze", "authors": "Yannick Forster, Fabian Kunze, Marc Roth", "title": "The Weak Call-By-Value {\\lambda}-Calculus is Reasonable for Both Time\n  and Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the weak call-by-value $\\lambda$-calculus as a model for\ncomputational complexity theory and establish the natural measures for time and\nspace -- the number of beta-reductions and the size of the largest term in a\ncomputation -- as reasonable measures with respect to the invariance thesis of\nSlot and van Emde Boas [STOC~84]. More precisely, we show that, using those\nmeasures, Turing machines and the weak call-by-value $\\lambda$-calculus can\nsimulate each other within a polynomial overhead in time and a constant factor\noverhead in space for all computations that terminate in (encodings) of 'true'\nor 'false'. We consider this result as a solution to the long-standing open\nproblem, explicitly posed by Accattoli [ENTCS~18], of whether the natural\nmeasures for time and space of the $\\lambda$-calculus are reasonable, at least\nin case of weak call-by-value evaluation.\n  Our proof relies on a hybrid of two simulation strategies of reductions in\nthe weak call-by-value $\\lambda$-calculus by Turing machines, both of which are\ninsufficient if taken alone. The first strategy is the most naive one in the\nsense that a reduction sequence is simulated precisely as given by the\nreduction rules; in particular, all substitutions are executed immediately.\nThis simulation runs within a constant overhead in space, but the overhead in\ntime might be exponential. The second strategy is heap-based and relies on\nstructure sharing, similar to existing compilers of eager functional languages.\nThis strategy only has a polynomial overhead in time, but the space consumption\nmight require an additional factor of $\\log n$, which is essentially due to the\nsize of the pointers required for this strategy. Our main contribution is the\nconstruction and verification of a space-aware interleaving of the two\nstrategies, which is shown to yield both a constant overhead in space and a\npolynomial overhead in time.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 11:30:17 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Forster", "Yannick", ""], ["Kunze", "Fabian", ""], ["Roth", "Marc", ""]]}, {"id": "1902.07808", "submitter": "Michael Vitousek", "authors": "Michael M. Vitousek, Jeremy G. Siek, Avik Chaudhuri", "title": "Optimizing and Evaluating Transient Gradual Typing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradual typing enables programmers to combine static and dynamic typing in\nthe same language. However, ensuring a sound interaction between the static and\ndynamic parts can incur significant runtime cost. In this paper, we perform a\ndetailed performance analysis of the transient gradual typing approach\nimplemented in Reticulated Python, a gradually typed variant of Python. The\ntransient approach inserts lightweight checks throughout a program rather than\ninstalling proxies on higher order values. We show that, when running\nReticulated Python and the transient approach on CPython, performance decreases\nas programs evolve from dynamic to static types, up to a 6x slowdown compared\nto equivalent Python programs.\n  To reduce this overhead, we design a static analysis and optimization that\nremoves redundant runtime checks. The optimization employs a static type\ninference algorithm that solves traditional subtyping constraints and also a\nnew kind of check constraint. We evaluate the resulting performance and find\nthat for many programs, the efficiency of partially typed programs is close to\ntheir untyped counterparts, removing most of the slowdown of transient checks.\nFinally, we measure the efficiency of Reticulated Python programs when running\non PyPy, a tracing JIT. We find that combining PyPy with our type inference\nalgorithm reduces the overall overhead to zero.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 23:20:15 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Vitousek", "Michael M.", ""], ["Siek", "Jeremy G.", ""], ["Chaudhuri", "Avik", ""]]}, {"id": "1902.07818", "submitter": "EPTCS", "authors": "Joachim Niehren (Inria Lille), David Sabel (Goethe Universit\\\"at,\n  Frankfurt)", "title": "Proceedings Fifth International Workshop on Rewriting Techniques for\n  Program Transformations and Evaluation", "comments": null, "journal-ref": "EPTCS 289, 2019", "doi": "10.4204/EPTCS.289", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the formal proceedings of the 5th International Workshop\non Rewriting Techniques for Program Transformations and Evaluation (WPTE 2018),\nheld on 8th of Juli 2018 in Oxford, United Kingdom, and affiliated with FLoC\n2018 and FSCD 2018.\n  Scope of WPTE:\n  Rewriting techniques are of great help for studying correctness of program\ntransformations, translations and evaluation, and the aim of WPTE is to bring\ntogether the researchers working on program transformations, evaluation, and\noperationally-based programming language semantics, using rewriting methods, in\norder to share the techniques and recent developments and to exchange ideas to\nencourage further activation of research in this area. Topics in the scope of\nWPTE include the correctness of program transformations, optimisations and\ntranslations; program transformations for proving termination, confluence and\nother properties; correctness of evaluation strategies; operational semantics\nof programs, operationally-based program equivalences such as contextual\nequivalences and bisimulations; cost-models for reasoning about the optimizing\npower of transformations and the costs of evaluation; program transformations\nfor verification and theorem proving purposes; translation, simulation,\nequivalence of programs with different formalisms, and evaluation strategies;\nprogram transformations for applying rewriting techniques to programs in\nspecific programming languages; program transformations for program inversions\nand program synthesis; program transformation and evaluation for Haskell and\nrewriting.\n  Research Paper Selection:\n  At the workshop six research papers were presented of which five were\naccepted for the postproceedings. Each submission was reviewed by three or four\nmembers of the Program Committee in two to three rounds, one round for workshop\npresentation and at most two rounds for publication to the postproceedings.\n  The program also included one invited talk by Jean-Pierre Jouannaud (Polytec,\nPalaiseau, Grand Paris, France) on a framework for graph rewriting; the\nabstract of this talk is included in this volume.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 00:06:49 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Niehren", "Joachim", "", "Inria Lille"], ["Sabel", "David", "", "Goethe Universit\u00e4t,\n  Frankfurt"]]}, {"id": "1902.07986", "submitter": "Amir Kafshdar Goharshady", "authors": "Krishnendu Chatterjee and Amir Kafshdar Goharshady and Arash\n  Pourdamghani", "title": "Probabilistic Smart Contracts: Secure Randomness on the Blockchain", "comments": "Accepted to the 2019 IEEE International Conference on Blockchain and\n  Cryptocurrency (ICBC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's programmable blockchains, smart contracts are limited to being\ndeterministic and non-probabilistic. This lack of randomness is a consequential\nlimitation, given that a wide variety of real-world financial contracts, such\nas casino games and lotteries, depend entirely on randomness. As a result,\nseveral ad-hoc random number generation approaches have been developed to be\nused in smart contracts. These include ideas such as using an oracle or relying\non the block hash. However, these approaches are manipulatable, i.e. their\noutput can be tampered with by parties who might not be neutral, such as the\nowner of the oracle or the miners. We propose a novel game-theoretic approach\nfor generating provably unmanipulatable pseudorandom numbers on the blockchain.\nOur approach allows smart contracts to access a trustworthy source of\nrandomness that does not rely on potentially compromised miners or oracles,\nhence enabling the creation of a new generation of smart contracts that are not\nlimited to being non-probabilistic and can be drawn from the much more general\nclass of probabilistic programs.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 11:56:22 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Goharshady", "Amir Kafshdar", ""], ["Pourdamghani", "Arash", ""]]}, {"id": "1902.08091", "submitter": "Ross Duncan", "authors": "Alexander Cowtan, Silas Dilkes, Ross Duncan, Alexandre Krajenbrink,\n  Will Simmons, Seyon Sivarajah", "title": "On the qubit routing problem", "comments": "v2: wrong chart replaced with correct version; minor edits for\n  clarity", "journal-ref": null, "doi": "10.4230/LIPIcs.TQC.2019.5", "report-no": null, "categories": "quant-ph cs.DS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new architecture-agnostic methodology for mapping abstract\nquantum circuits to realistic quantum computing devices with restricted qubit\nconnectivity, as implemented by Cambridge Quantum Computing's tket compiler. We\npresent empirical results showing the effectiveness of this method in terms of\nreducing two-qubit gate depth and two-qubit gate count, compared to other\nimplementations.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 15:09:20 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 19:06:27 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Cowtan", "Alexander", ""], ["Dilkes", "Silas", ""], ["Duncan", "Ross", ""], ["Krajenbrink", "Alexandre", ""], ["Simmons", "Will", ""], ["Sivarajah", "Seyon", ""]]}, {"id": "1902.08419", "submitter": "EPTCS", "authors": "Andrei-Sebastian Buruian\\u{a} (Alexandru Ioan Cuza University and\n  Bitdefender), \\c{S}tefan Ciob\\^ac\\u{a} (Alexandru Ioan Cuza University)", "title": "Reducing Total Correctness to Partial Correctness by a Transformation of\n  the Language Semantics", "comments": "In Proceedings WPTE 2018, arXiv:1902.07818", "journal-ref": "EPTCS 289, 2019, pp. 1-16", "doi": "10.4204/EPTCS.289.1", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a language-parametric solution to the problem of total correctness,\nby automatically reducing it to the problem of partial correctness, under the\nassumption that an expression whose value decreases with each program step in a\nwell-founded order is provided. Our approach assumes that the programming\nlanguage semantics is given as a rewrite theory. We implement a prototype on\ntop of the RMT tool and we show that it works in practice on a number of\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 10:08:29 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Buruian\u0103", "Andrei-Sebastian", "", "Alexandru Ioan Cuza University and\n  Bitdefender"], ["Ciob\u00e2c\u0103", "\u015etefan", "", "Alexandru Ioan Cuza University"]]}, {"id": "1902.08420", "submitter": "EPTCS", "authors": "David Sabel (Goethe-University Frankfurt am Main)", "title": "Automating the Diagram Method to Prove Correctness of Program\n  Transformations", "comments": "In Proceedings WPTE 2018, arXiv:1902.07818", "journal-ref": "EPTCS 289, 2019, pp. 17-33", "doi": "10.4204/EPTCS.289.2", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on the automation of a technique to prove the correctness of\nprogram transformations in higher-order program calculi which may permit\nrecursive let-bindings as they occur in functional programming languages. A\nprogram transformation is correct if it preserves the observational semantics\nof programs. In our LRSX Tool the so-called diagram method is automated by\ncombining unification, matching, and reasoning on alpha-renamings on the\nhigher-order meta-language, and automating induction proofs via an encoding\ninto termination problems of term rewrite systems. We explain the techniques,\nwe illustrate the usage of the tool, and we report on experiments.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 10:08:48 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Sabel", "David", "", "Goethe-University Frankfurt am Main"]]}, {"id": "1902.08421", "submitter": "EPTCS", "authors": "Yoshiaki Kanazawa (Nagoya University), Naoki Nishida (Nagoya\n  University)", "title": "On Transforming Functions Accessing Global Variables into Logically\n  Constrained Term Rewriting Systems", "comments": "In Proceedings WPTE 2018, arXiv:1902.07818", "journal-ref": "EPTCS 289, 2019, pp. 34-52", "doi": "10.4204/EPTCS.289.3", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show a new approach to transformations of an imperative\nprogram with function calls and global variables into a logically constrained\nterm rewriting system. The resulting system represents transitions of the whole\nexecution environment with a call stack. More precisely, we prepare a function\nsymbol for the whole environment, which stores values for global variables and\na call stack as its arguments. For a function call, we prepare rewrite rules to\npush the frame to the stack and to pop it after the execution. Any running\nframe is located at the top of the stack, and statements accessing global\nvariables are represented by rewrite rules for the environment symbol. We show\na precise transformation based on the approach and prove its correctness.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 10:09:09 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Kanazawa", "Yoshiaki", "", "Nagoya University"], ["Nishida", "Naoki", "", "Nagoya\n  University"]]}, {"id": "1902.08422", "submitter": "EPTCS", "authors": "Manfred Schmidt-Schau{\\ss} (Goethe-University Frankfurt am Main), Nils\n  Dallmeyer (Goethe-University Frankfurt am Main)", "title": "Optimizing Space of Parallel Processes", "comments": "In Proceedings WPTE 2018, arXiv:1902.07818", "journal-ref": "EPTCS 289, 2019, pp. 53-67", "doi": "10.4204/EPTCS.289.4", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a contribution to exploring and analyzing space-improvements in\nconcurrent programming languages, in particular in the functional\nprocess-calculus CHF. Space-improvements are defined as a generalization of the\ncorresponding notion in deterministic pure functional languages. The main part\nof the paper is the O(n*log n) algorithm SpOptN for offline space optimization\nof several parallel independent processes. Applications of this algorithm are:\n(i) affirmation of space improving transformations for particular classes of\nprogram transformations; (ii) support of an interpreter-based method for\nrefuting space-improvements; and (iii) as a stand-alone offline-optimizer for\nspace (or similar resources) of parallel processes.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 10:09:36 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Schmidt-Schau\u00df", "Manfred", "", "Goethe-University Frankfurt am Main"], ["Dallmeyer", "Nils", "", "Goethe-University Frankfurt am Main"]]}, {"id": "1902.08726", "submitter": "Zheng Yang", "authors": "Zheng Yang, Hang Lei, Weizhong Qian", "title": "A Hybrid Formal Verification System in Coq for Ensuring the Reliability\n  and Security of Ethereum-based Service Smart Contracts", "comments": "29 pages, 28 figures, 5 tables", "journal-ref": "IEEE ACCESS (2020)", "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on the development of a formal symbolic process virtual\nmachine (FSPVM) denoted as FSPVM-E for verifying the reliability and security\nof Ethereum-based services at the source code level of smart contracts, and a\nCoq proof assistant is employed for both programming the system and for proving\nits correctness. The current version of FSPVM-E adopts execution-verification\nisomorphism, which is an application extension of Curry-Howard isomorphism, as\nits fundamental theoretical framework to combine symbolic execution and\nhigher-order logic theorem proving. The four primary components of FSPVM-E\ninclude a general, extensible, and reusable formal memory framework, an\nextensible and universal formal intermediate programming language denoted as\nLolisa, which is a large subset of the Solidity programming language using\ngeneralized algebraic datatypes, the corresponding formally verified\ninterpreter of Lolisa, denoted as FEther, and assistant tools and libraries.\nThe self-correctness of all components is certified in Coq. Currently, FSPVM-E\nsupports the ERC20 token standard, and can automatically and symbolically\nexecute Ethereum-based smart contracts, scan their standard vulnerabilities,\nand verify their reliability and security properties with Hoare-style logic in\nCoq. To the best of authors' knowledge, the present work represents the first\nhybrid formal verification system implemented in Coq for Ethereum smart\ncontracts that is applied at the Solidity source code level.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 03:32:06 GMT"}, {"version": "v2", "created": "Sat, 9 Mar 2019 04:49:24 GMT"}, {"version": "v3", "created": "Thu, 23 Jan 2020 04:35:03 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Yang", "Zheng", ""], ["Lei", "Hang", ""], ["Qian", "Weizhong", ""]]}, {"id": "1902.09099", "submitter": "Jingbo Wang", "authors": "Jingbo Wang and Chungha Sung and Chao Wang", "title": "Mitigating Power Side Channels during Compilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The code generation modules inside modern compilers such as GCC and LLVM,\nwhich use a limited number of CPU registers to store a large number of program\nvariables, may introduce side-channel leaks even in software equipped with\nstate-of-the-art countermeasures. We propose a program analysis and\ntransformation based method to eliminate this side channel. Our method has a\ntype-based technique for detecting leaks, which leverages Datalog-based\ndeclarative analysis and domain-specific optimizations to achieve high\nefficiency and accuracy. It also has a mitigation technique for the compiler's\nbackend, more specifically the register allocation modules, to ensure that\npotentially leaky intermediate computation results are always stored in\ndifferent CPU registers or spilled to memory with isolation. We have\nimplemented and evaluated our method in LLVM for the x86 instruction set\narchitecture. Our experiments on cryptographic software show that the method is\neffective in removing the side channel while being efficient, i.e., our\nmitigated code is more compact and runs faster than code mitigated using\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 06:05:38 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Wang", "Jingbo", ""], ["Sung", "Chungha", ""], ["Wang", "Chao", ""]]}, {"id": "1902.09122", "submitter": "Uri Alon", "authors": "Yaniv David, Uri Alon, Eran Yahav", "title": "Neural Reverse Engineering of Stripped Binaries using Augmented Control\n  Flow Graphs", "comments": null, "journal-ref": null, "doi": "10.1145/3428293", "report-no": null, "categories": "cs.LG cs.CR cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of reverse engineering of stripped executables, which\ncontain no debug information. This is a challenging problem because of the low\namount of syntactic information available in stripped executables, and the\ndiverse assembly code patterns arising from compiler optimizations.\n  We present a novel approach for predicting procedure names in stripped\nexecutables. Our approach combines static analysis with neural models. The main\nidea is to use static analysis to obtain augmented representations of call\nsites; encode the structure of these call sites using the control-flow graph\n(CFG) and finally, generate a target name while attending to these call sites.\nWe use our representation to drive graph-based, LSTM-based and\nTransformer-based architectures.\n  Our evaluation shows that our models produce predictions that are difficult\nand time consuming for humans, while improving on existing methods by 28% and\nby 100% over state-of-the-art neural textual models that do not use any static\nanalysis. Code and data for this evaluation are available at\nhttps://github.com/tech-srl/Nero .\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 07:30:39 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 08:21:53 GMT"}, {"version": "v3", "created": "Wed, 27 May 2020 07:22:27 GMT"}, {"version": "v4", "created": "Fri, 16 Oct 2020 08:42:40 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["David", "Yaniv", ""], ["Alon", "Uri", ""], ["Yahav", "Eran", ""]]}, {"id": "1902.09334", "submitter": "Micha\\\"el Marcozzi", "authors": "Micha\\\"el Marcozzi, Qiyi Tang, Alastair F. Donaldson, Cristian Cadar", "title": "A Systematic Impact Study for Fuzzer-Found Compiler Bugs", "comments": "Conference on Object-Oriented Programming, Systems, Languages &\n  Applications (SPLASH 2019 OOPSLA), Athens, Greece, 20-25 October 2019", "journal-ref": "Proceedings of the ACM on Programming Languages, 2019", "doi": "10.1145/3360581", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite much recent interest in compiler randomized testing (fuzzing), the\npractical impact of fuzzer-found compiler bugs on real-world applications has\nbarely been assessed. We present the first quantitative and qualitative study\nof the tangible impact of miscompilation bugs in a mature compiler. We follow a\nrigorous methodology where the bug impact over the compiled application is\nevaluated based on (1) whether the bug appears to trigger during compilation;\n(2) the extent to which generated assembly code changes syntactically due to\ntriggering of the bug; and (3) how much such changes do cause regression test\nsuite failures and could be used to manually trigger divergences during\nexecution. The study is conducted with respect to the compilation of more than\n10 million lines of C/C++ code from 309 Debian packages, using 12% of the\nhistorical and now fixed miscompilation bugs found by four state-of-the-art\nfuzzers in the Clang/LLVM compiler, as well as 18 bugs found by human users\ncompiling real code or by formal verification. The results show that almost\nhalf of the fuzzer-found bugs propagate to the generated binaries for some\npackages, but rarely affect their syntax and cause two failures in total when\nrunning their test suites. User-reported and formal verification bugs do not\nexhibit a higher impact, with less frequently triggered bugs and one test\nfailure. Our manual analysis of a selection of bugs, either fuzzer-found or\nnot, suggests that none can easily trigger a runtime divergence on the packages\nconsidered in the analysis, and that in general they affect only corner cases.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 15:05:27 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 15:38:41 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 12:38:49 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Marcozzi", "Micha\u00ebl", ""], ["Tang", "Qiyi", ""], ["Donaldson", "Alastair F.", ""], ["Cadar", "Cristian", ""]]}, {"id": "1902.09502", "submitter": "Suvam Mukherjee", "authors": "Suvam Mukherjee, Nitin John Raj, Krishnan Govindraj, Pantazis\n  Deligiannis, Chandramouleswaran Ravichandran, Akash Lal, Aseem Rastogi, Raja\n  Krishnaswamy", "title": "Reliable State Machines: A Framework for Programming Reliable Cloud\n  Services", "comments": "R1: This replacement contains minor formatting improvements over the\n  original R2: Anonymized \"popular cloud service provider\" phrase replaced with\n  \"Microsoft Azure\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building reliable applications for the cloud is challenging because of\nunpredictable failures during a program's execution. This paper presents a\nprogramming framework called Reliable State Machines (RSMs), that offers\nfault-tolerance by construction. Using our framework, a programmer can build an\napplication as several (possibly distributed) RSMs that communicate with each\nother via messages, much in the style of actor-based programming. Each RSM is\nadditionally fault-tolerant by design and offers the illusion of being\n\"always-alive\". An RSM is guaranteed to process each input request exactly\nonce, as one would expect in a failure-free environment. The RSM runtime\nautomatically takes care of persisting state and rehydrating it on a failover.\nWe present the core syntax and semantics of RSMs, along with a formal proof of\nfailure-transparency. We provide an implementation of the RSM framework and\nruntime on the .NET platform for deploying services to Microsoft Azure. We\ncarried out an extensive performance evaluation on micro-benchmarks to show\nthat one can build high-throughput applications with RSMs. We also present a\ncase study where we rewrote a significant part of a production cloud service\nusing RSMs. The resulting service has simpler code and exhibits\nproduction-grade performance.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 18:28:54 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 06:10:45 GMT"}, {"version": "v3", "created": "Wed, 27 Feb 2019 18:29:47 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Mukherjee", "Suvam", ""], ["Raj", "Nitin John", ""], ["Govindraj", "Krishnan", ""], ["Deligiannis", "Pantazis", ""], ["Ravichandran", "Chandramouleswaran", ""], ["Lal", "Akash", ""], ["Rastogi", "Aseem", ""], ["Krishnaswamy", "Raja", ""]]}, {"id": "1902.09685", "submitter": "EPTCS", "authors": "Isaac Oscar Gariano (VUW), Marco Servetto (VUW), Alex Potanin (VUW),\n  Hrshikesh Arora (VUW)", "title": "Iteratively Composing Statically Verified Traits", "comments": "In Proceedings VPT 2019, arXiv:1908.06723", "journal-ref": "EPTCS 299, 2019, pp. 49-55", "doi": "10.4204/EPTCS.299.7", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static verification relying on an automated theorem prover can be very slow\nand brittle: since static verification is undecidable, correct code may not\npass a particular static verifier. In this work we use metaprogramming to\ngenerate code that is correct by construction. A theorem prover is used only to\nverify initial \"traits\": units of code that can be used to compose bigger\nprograms.\n  In our work, meta-programming is done by trait composition, which starting\nfrom correct code, is guaranteed to produce correct code. We do this by\nextending conventional traits with pre- and post-conditions for the methods; we\nalso extend the traditional trait composition (+) operator to check the\ncompatibility of contracts. In this way, there is no need to re-verify the\nproduced code.\n  We show how our approach can be applied to the standard \"power\" function\nexample, where metaprogramming generates optimised, and correct, versions when\nthe exponent is known in advance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 01:12:27 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 06:36:17 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Gariano", "Isaac Oscar", "", "VUW"], ["Servetto", "Marco", "", "VUW"], ["Potanin", "Alex", "", "VUW"], ["Arora", "Hrshikesh", "", "VUW"]]}, {"id": "1902.09927", "submitter": "EPTCS", "authors": "Ivan Proki\\'c (Faculty of Technical Sciences, University of Novi Sad,\n  Serbia)", "title": "The Cpi-calculus: a Model for Confidential Name Passing", "comments": "In Proceedings ICE 2019, arXiv:1909.05242", "journal-ref": "EPTCS 304, 2019, pp. 115-136", "doi": "10.4204/EPTCS.304.8", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sharing confidential information in distributed systems is a necessity in\nmany applications, however, it opens the problem of controlling information\nsharing even among trusted parties. In this paper, we present a formal model in\nwhich dissemination of information is disabled at the level of the syntax in a\ndirect way. We introduce a subcalculus of the pi-calculus in which channels are\nconsidered as confidential information. The only difference with respect to the\npi-calculus is that channels once received cannot be forwarded later on. By\nmeans of examples, we give an initial idea of how some privacy notions already\nstudied in the past, such as group creation and name hiding, can be represented\nwithout any additional language constructs. We also present an encoding of the\n(sum-free) pi-calculus in our calculus.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 13:43:37 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 13:53:12 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 22:25:35 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Proki\u0107", "Ivan", "", "Faculty of Technical Sciences, University of Novi Sad,\n  Serbia"]]}, {"id": "1902.10056", "submitter": "Venkatesh-Prasad Ranganath", "authors": "Joydeep Mitra, Venkatesh-Prasad Ranganath", "title": "SeMA: A Design Methodology for Building Secure Android Apps", "comments": "Updates based on AMobile 2019 reviews", "journal-ref": null, "doi": "10.1109/ASEW.2019.00021", "report-no": null, "categories": "cs.SE cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UX (user experience) designers visually capture the UX of an app via\nstoryboards. This method is also used in Android app development to\nconceptualize and design apps.\n  Recently, security has become an integral part of Android app UX because\nmobile apps are used to perform critical activities such as banking,\ncommunication, and health. Therefore, securing user information is imperative\nin mobile apps.\n  In this context, storyboarding tools offer limited capabilities to capture\nand reason about security requirements of an app. Consequently, security cannot\nbe baked into the app at design time. Hence, vulnerabilities stemming from\ndesign flaws can often occur in apps. To address this concern, in this paper,\nwe propose a storyboard based design methodology to enable the specification\nand verification of security properties of an Android app at design time.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 17:03:36 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 03:54:08 GMT"}, {"version": "v3", "created": "Thu, 19 Sep 2019 17:19:56 GMT"}, {"version": "v4", "created": "Fri, 20 Sep 2019 18:21:56 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Mitra", "Joydeep", ""], ["Ranganath", "Venkatesh-Prasad", ""]]}, {"id": "1902.10231", "submitter": "Isaac Oscar Gariano", "authors": "Isaac Oscar Gariano, Marco Servetto and Alex Potanin", "title": "Sound Invariant Checking Using Type Modifiers and Object Capabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we use pre existing language support for type modifiers and\nobject capabilities to enable a system for sound runtime verification of\ninvariants. Our system guarantees that class invariants hold for all objects\ninvolved in execution. Invariants are specified simply as methods whose\nexecution is statically guaranteed to be deterministic and not access any\nexternally mutable state. We automatically call such invariant methods only\nwhen objects are created or the state they refer to may have been mutated. Our\ndesign restricts the range of expressible invariants but improves upon the\nusability and performance of our system compared to prior work. In addition, we\nsoundly support mutation, dynamic dispatch, exceptions, and non determinism,\nwhile requiring only a modest amount of annotation. We present a case study\nshowing that our system requires a lower annotation burden compared to Spec#,\nand performs orders of magnitude less runtime invariant checks compared to the\nwidely used `visible state semantics' protocols of D, Eiffel. We also formalise\nour approach and prove that such pre existing type modifier and object\ncapability support is sufficient to ensure its soundness.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 21:24:17 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Gariano", "Isaac Oscar", ""], ["Servetto", "Marco", ""], ["Potanin", "Alex", ""]]}, {"id": "1902.10345", "submitter": "Tal Ben-Nun", "authors": "Tal Ben-Nun, Johannes de Fine Licht, Alexandros Nikolaos Ziogas, Timo\n  Schneider, Torsten Hoefler", "title": "Stateful Dataflow Multigraphs: A Data-Centric Model for Performance\n  Portability on Heterogeneous Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of accelerators in high-performance computing has driven\nprogramming complexity beyond the skill-set of the average domain scientist. To\nmaintain performance portability in the future, it is imperative to decouple\narchitecture-specific programming paradigms from the underlying scientific\ncomputations. We present the Stateful DataFlow multiGraph (SDFG), a\ndata-centric intermediate representation that enables separating program\ndefinition from its optimization. By combining fine-grained data dependencies\nwith high-level control-flow, SDFGs are both expressive and amenable to program\ntransformations, such as tiling and double-buffering. These transformations are\napplied to the SDFG in an interactive process, using extensible pattern\nmatching, graph rewriting, and a graphical user interface. We demonstrate SDFGs\non CPUs, GPUs, and FPGAs over various motifs --- from fundamental computational\nkernels to graph analytics. We show that SDFGs deliver competitive performance,\nallowing domain scientists to develop applications naturally and port them to\napproach peak hardware performance without modifying the original scientific\ncode.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 06:12:16 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 17:55:12 GMT"}, {"version": "v3", "created": "Thu, 2 Jan 2020 21:43:32 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Ben-Nun", "Tal", ""], ["Licht", "Johannes de Fine", ""], ["Ziogas", "Alexandros Nikolaos", ""], ["Schneider", "Timo", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1902.11189", "submitter": "Fredrik Dahlqvist", "authors": "Fredrik Dahlqvist and Dexter Kozen", "title": "Semantics of higher-order probabilistic programs with conditioning", "comments": "17 pages, proofs in the Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a denotational semantics for higher-order probabilistic programs\nin terms of linear operators between Banach spaces. Our semantics is rooted in\nthe classical theory of Banach spaces and their tensor products, but bears\nsimilarities with the well-known Scott semantics of higher-order programs\nthrough the use ordered Banach spaces which allow definitions in terms of fixed\npoints. Being based on a monoidal rather than cartesian closed structure, our\nsemantics effectively treats randomness as a resource.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 16:17:33 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Dahlqvist", "Fredrik", ""], ["Kozen", "Dexter", ""]]}]