[{"id": "1810.00619", "submitter": "Thomas Deselaers", "authors": "Victor Carbune, Thierry Coppey, Alexander Daryin, Thomas Deselaers,\n  Nikhil Sarda, Jay Yagnik", "title": "SmartChoices: Hybridizing Programming and Machine Learning", "comments": "published at the Reinforcement Learning for Real Life (RL4RealLife)\n  Workshop in the 36th International Conference on Machine Learning (ICML),\n  Long Beach, California, USA, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present SmartChoices, an approach to making machine learning (ML) a first\nclass citizen in programming languages which we see as one way to lower the\nentrance cost to applying ML to problems in new domains. There is a growing\ndivide in approaches to building systems: on the one hand, programming\nleverages human experts to define a system while on the other hand behavior is\nlearned from data in machine learning. We propose to hybridize these two by\nproviding a 3-call API which we expose through an object called SmartChoice. We\ndescribe the SmartChoices-interface, how it can be used in programming with\nminimal code changes, and demonstrate that it is an easy to use but still\npowerful tool by demonstrating improvements over not using ML at all on three\nalgorithmic problems: binary search, QuickSort, and caches. In these three\nexamples, we replace the commonly used heuristics with an ML model entirely\nencapsulated within a SmartChoice and thus requiring minimal code changes. As\nopposed to previous work applying ML to algorithmic problems, our proposed\napproach does not require to drop existing implementations but seamlessly\nintegrates into the standard software development workflow and gives full\ncontrol to the software developer over how ML methods are applied. Our\nimplementation relies on standard Reinforcement Learning (RL) methods. To learn\nfaster, we use the heuristic function, which they are replacing, as an initial\nfunction. We show how this initial function can be used to speed up and\nstabilize learning while providing a safety net that prevents performance to\nbecome substantially worse -- allowing for a safe deployment in critical\napplications in real life.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 11:14:22 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 11:24:58 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 18:20:51 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Carbune", "Victor", ""], ["Coppey", "Thierry", ""], ["Daryin", "Alexander", ""], ["Deselaers", "Thomas", ""], ["Sarda", "Nikhil", ""], ["Yagnik", "Jay", ""]]}, {"id": "1810.00724", "submitter": "Josep Silva", "authors": "Josep Silva", "title": "Pre-proceedings of the 26th International Workshop on Functional and\n  Logic Programming (WFLP 2018)", "comments": "Papers selected for presentation at WFLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume constitutes the pre-proceedings of the 26th International\nWorkshop on Functional and Logic Programming (WFLP 2018). It is formed of those\npapers selected by the program committee for presentation at the workshop.\nAfter discussion at the workshop, the program committee will select a number of\npapers to be invited for the second round of refereeing and selection for the\nformal proceedings.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 14:34:14 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Silva", "Josep", ""]]}, {"id": "1810.00845", "submitter": "Olli Saarikivi", "authors": "Roshan Dathathri, Olli Saarikivi, Hao Chen, Kim Laine, Kristin Lauter,\n  Saeed Maleki, Madanlal Musuvathi, Todd Mytkowicz", "title": "CHET: Compiler and Runtime for Homomorphic Evaluation of Tensor Programs", "comments": "Submitted to ASPLOS2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully Homomorphic Encryption (FHE) refers to a set of encryption schemes that\nallow computations to be applied directly on encrypted data without requiring a\nsecret key. This enables novel application scenarios where a client can safely\noffload storage and computation to a third-party cloud provider without having\nto trust the software and the hardware vendors with the decryption keys. Recent\nadvances in both FHE schemes and implementations have moved such applications\nfrom theoretical possibilities into the realm of practicalities.\n  This paper proposes a compact and well-reasoned interface called the\nHomomorphic Instruction Set Architecture (HISA) for developing FHE\napplications. Just as the hardware ISA interface enabled hardware advances to\nproceed independent of software advances in the compiler and language runtimes,\nHISA decouples compiler optimizations and runtimes for supporting FHE\napplications from advancements in the underlying FHE schemes.\n  This paper demonstrates the capabilities of HISA by building an end-to-end\nsoftware stack for evaluating neural network models on encrypted data. Our\nstack includes an end-to-end compiler, runtime, and a set of optimizations. Our\napproach shows generated code, on a set of popular neural network\narchitectures, is faster than hand-optimized implementations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 17:38:53 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Dathathri", "Roshan", ""], ["Saarikivi", "Olli", ""], ["Chen", "Hao", ""], ["Laine", "Kim", ""], ["Lauter", "Kristin", ""], ["Maleki", "Saeed", ""], ["Musuvathi", "Madanlal", ""], ["Mytkowicz", "Todd", ""]]}, {"id": "1810.00873", "submitter": "Louis Mandel", "authors": "Guillaume Baudart, Javier Burroni, Martin Hirzel, Louis Mandel,\n  Avraham Shinnar", "title": "Compiling Stan to Generative Probabilistic Languages and Extension to\n  Deep Probabilistic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stan is a probabilistic programming language that is popular in the\nstatistics community, with a high-level syntax for expressing probabilistic\nmodels. Stan differs by nature from generative probabilistic programming\nlanguages like Church, Anglican, or Pyro. This paper presents a comprehensive\ncompilation scheme to compile any Stan model to a generative language and\nproves its correctness. We use our compilation scheme to build two new backends\nfor the Stanc3 compiler targeting Pyro and NumPyro. Experimental results show\nthat the NumPyro backend yields a 2.3x speedup compared to Stan in geometric\nmean over 26 benchmarks. Building on Pyro we extend Stan with support for\nexplicit variational inference guides and deep probabilistic models. That way,\nusers familiar with Stan get access to new features without having to learn a\nfundamentally new language.\n", "versions": [{"version": "v1", "created": "Sun, 30 Sep 2018 15:39:53 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 20:45:47 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 16:29:27 GMT"}, {"version": "v4", "created": "Tue, 12 Jan 2021 20:51:14 GMT"}, {"version": "v5", "created": "Sun, 11 Apr 2021 15:34:02 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Baudart", "Guillaume", ""], ["Burroni", "Javier", ""], ["Hirzel", "Martin", ""], ["Mandel", "Louis", ""], ["Shinnar", "Avraham", ""]]}, {"id": "1810.00905", "submitter": "Rahman Lavaee", "authors": "Rahman Lavaee, John Criswell, Chen Ding", "title": "Codestitcher: Inter-Procedural Basic Block Layout Optimization", "comments": "24 pages, 6 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern software executes a large amount of code. Previous techniques of code\nlayout optimization were developed one or two decades ago and have become\ninadequate to cope with the scale and complexity of new types of applications\nsuch as compilers, browsers, interpreters, language VMs and shared libraries.\n  This paper presents Codestitcher, an inter-procedural basic block code layout\noptimizer which reorders basic blocks in an executable to benefit from better\ncache and TLB performance. Codestitcher provides a hierarchical framework which\ncan be used to improve locality in various layers of the memory hierarchy. Our\nevaluation shows that Codestitcher improves the performance of the original\nprogram by 3\\% to 25\\% (on average, by 10\\%) on 5 widely used applications with\nlarge code sizes: MySQL, Clang, Firefox, Apache, and Python. It gives an\nadditional improvement of 4\\% over LLVM's PGO and 3\\% over PGO combined with\nthe best function reordering technique.\n", "versions": [{"version": "v1", "created": "Mon, 1 Oct 2018 18:20:14 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Lavaee", "Rahman", ""], ["Criswell", "John", ""], ["Ding", "Chen", ""]]}, {"id": "1810.00952", "submitter": "Jared Roesch", "authors": "Jared Roesch, Steven Lyubomirsky, Logan Weber, Josh Pollock, Marisa\n  Kirisame, Tianqi Chen, Zachary Tatlock", "title": "Relay: A New IR for Machine Learning Frameworks", "comments": null, "journal-ref": null, "doi": "10.1145/3211346.3211348", "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning powers diverse services in industry including search,\ntranslation, recommendation systems, and security. The scale and importance of\nthese models require that they be efficient, expressive, and portable across an\narray of heterogeneous hardware devices. These constraints are often at odds;\nin order to better accommodate them we propose a new high-level intermediate\nrepresentation (IR) called Relay. Relay is being designed as a\npurely-functional, statically-typed language with the goal of balancing\nefficient compilation, expressiveness, and portability. We discuss the goals of\nRelay and highlight its important design constraints. Our prototype is part of\nthe open source NNVM compiler framework, which powers Amazon's deep learning\nframework MxNet.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 00:09:54 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Roesch", "Jared", ""], ["Lyubomirsky", "Steven", ""], ["Weber", "Logan", ""], ["Pollock", "Josh", ""], ["Kirisame", "Marisa", ""], ["Chen", "Tianqi", ""], ["Tatlock", "Zachary", ""]]}, {"id": "1810.01190", "submitter": "Yura Perov N", "authors": "Yura Perov", "title": "Inference Over Programs That Make Predictions", "comments": "The International Conference on Probabilistic Programming, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This abstract extends on the previous work (arXiv:1407.2646,\narXiv:1606.00075) on program induction using probabilistic programming. It\ndescribes possible further steps to extend that work, such that, ultimately,\nautomatic probabilistic program synthesis can generalise over any reasonable\nset of inputs and outputs, in particular in regard to text, image and video\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 2 Oct 2018 12:00:41 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Perov", "Yura", ""]]}, {"id": "1810.02053", "submitter": "EPTCS", "authors": "Massimo Bartoletti (University of Cagliari, Italy), Sophia Knight\n  (Uppsala University, Sweden)", "title": "Proceedings 11th Interaction and Concurrency Experience", "comments": null, "journal-ref": "EPTCS 279, 2018", "doi": "10.4204/EPTCS.279", "report-no": null, "categories": "cs.LO cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of ICE'18, the 11th Interaction and\nConcurrency Experience, which was held in Madrid, Spain on the 20th and 21st of\nJune 2018 as a satellite event of DisCoTec'18.\n  The ICE workshop series features a distinguishing review and selection\nprocedure, allowing PC members to interact anonymously with authors. As in the\npast ten editions, this interaction considerably improved the accuracy of the\nfeedback from the reviewers and the quality of accepted papers, and offered the\nbasis for lively discussion during the workshop. For the second time, the 2018\nedition of ICE included double blind reviewing of original research papers, in\norder to increase fairness and avoid bias in reviewing.\n  Each paper was reviewed by three PC members, and altogether six papers were\naccepted for publication (the workshop also featured four oral presentations\nwhich are not part of this volume). We were proud to host three invited talks,\nby Elvira Albert, Silvia Crafa, and Alexey Gotsman. The abstracts of these\ntalks are included in this volume together with the regular papers. Final\nversions of the contributions, taking into account the discussion at the\nworkshop, are included.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 04:44:13 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Bartoletti", "Massimo", "", "University of Cagliari, Italy"], ["Knight", "Sophia", "", "Uppsala University, Sweden"]]}, {"id": "1810.02254", "submitter": "Manuel  Hernandez", "authors": "Manuel Hern\\'andez", "title": "Deriving sorting algorithms via abductive logic program transformation", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Logic program transformation by the unfold/fold method ad- vocates the\nwriting of correct logic programs via the application of some rules to a naive\nprogram. This work focuses on how to overcome subgoal- introduction\ndifficulties in synthesizing efficient sorting algorithms from an naive sorting\nalgorithm, through logic program transformation and abductive reasoning.\n", "versions": [{"version": "v1", "created": "Thu, 4 Oct 2018 14:51:58 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Hern\u00e1ndez", "Manuel", ""]]}, {"id": "1810.02720", "submitter": "Pengcheng Yin", "authors": "Pengcheng Yin, Graham Neubig", "title": "TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic\n  Parsing and Code Generation", "comments": "EMNLP 2018 (Demo Track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present TRANX, a transition-based neural semantic parser that maps natural\nlanguage (NL) utterances into formal meaning representations (MRs). TRANX uses\na transition system based on the abstract syntax description language for the\ntarget MR, which gives it two major advantages: (1) it is highly accurate,\nusing information from the syntax of the target MR to constrain the output\nspace and model the information flow, and (2) it is highly generalizable, and\ncan easily be applied to new types of MR by just writing a new abstract syntax\ndescription corresponding to the allowable structures in the MR. Experiments on\nfour different semantic parsing and code generation tasks show that our system\nis generalizable, extensible, and effective, registering strong results\ncompared to existing neural semantic parsers.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2018 14:35:01 GMT"}], "update_date": "2018-10-08", "authors_parsed": [["Yin", "Pengcheng", ""], ["Neubig", "Graham", ""]]}, {"id": "1810.03598", "submitter": "Long Pham", "authors": "Long Pham, Steven J. Ramsay, C.-H. Luke Ong", "title": "Defunctionalization of Higher-Order Constrained Horn Clauses", "comments": "Some minor typos are fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building on the successes of satisfiability modulo theories (SMT), Bj{\\o}rner\net al. initiated a research programme advocating Horn constraints as a suitable\nbasis for automatic program verification. The notion of first-order constrained\nHorn clauses has recently been extended to higher-order logic by Cathcart Burn\net al. To exploit the remarkable efficiency of SMT solving, a natural approach\nto solve systems of higher-order Horn constraints is to reduce them to systems\nof first-order Horn constraints. This paper presents a defunctionalization\nalgorithm to achieve the reduction.\n  Given a well-sorted higher-order constrained Horn clause (HoCHC) problem\ninstance, the defunctionalization algorithm constructs a first-order\nwell-sorted constrained Horn clause problem. In addition to well-sortedness of\nthe algorithm's output, we prove that if an input HoCHC is solvable, then the\nresult of its defunctionalization is solvable. The converse also holds, which\nwe prove using a recent result on the continuous semantics of HoCHC. To our\nknowledge, this defunctionalization algorithm is the first sound and complete\nreduction from systems of higher-order Horn constraints to systems of\nfirst-order Horn constraints.\n  We have constructed DefMono, a prototype implementation of the\ndefunctionalization algorithm. It first defunctionalizes an input HoCHC problem\nand then feeds the result into a backend SMT solver. We have evaluated the\nperformance of DefMono empirically by comparison with two other higher-order\nverification tools.\n", "versions": [{"version": "v1", "created": "Mon, 8 Oct 2018 17:50:15 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 11:36:12 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Pham", "Long", ""], ["Ramsay", "Steven J.", ""], ["Ong", "C. -H. Luke", ""]]}, {"id": "1810.04308", "submitter": "EPTCS", "authors": "Alessandro Coglio (Kestrel Institute)", "title": "A Simple Java Code Generator for ACL2 Based on a Deep Embedding of ACL2\n  in Java", "comments": "In Proceedings ACL2 2018, arXiv:1810.03762", "journal-ref": "EPTCS 280, 2018, pp. 1-17", "doi": "10.4204/EPTCS.280.1", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AIJ (ACL2 In Java) is a deep embedding in Java of an executable,\nside-effect-free, non-stobj-accessing subset of the ACL2 language without\nguards. ATJ (ACL2 To Java) is a simple Java code generator that turns ACL2\nfunctions into AIJ representations that are evaluated by the AIJ interpreter.\nAIJ and ATJ enable possibly verified ACL2 code to run as, and interoperate\nwith, Java code, without much of the ACL2 framework or any of the Lisp runtime.\nThe current speed of the resulting Java code may be adequate to some\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 00:34:48 GMT"}], "update_date": "2018-10-11", "authors_parsed": [["Coglio", "Alessandro", "", "Kestrel Institute"]]}, {"id": "1810.04610", "submitter": "Andreas Abel", "authors": "Andreas Abel and Jan Reineke", "title": "uops.info: Characterizing Latency, Throughput, and Port Usage of\n  Instructions on Intel Microarchitectures", "comments": null, "journal-ref": null, "doi": "10.1145/3297858.3304062", "report-no": null, "categories": "cs.PF cs.AR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern microarchitectures are some of the world's most complex man-made\nsystems. As a consequence, it is increasingly difficult to predict, explain,\nlet alone optimize the performance of software running on such\nmicroarchitectures. As a basis for performance predictions and optimizations,\nwe would need faithful models of their behavior, which are, unfortunately,\nseldom available.\n  In this paper, we present the design and implementation of a tool to\nconstruct faithful models of the latency, throughput, and port usage of x86\ninstructions. To this end, we first discuss common notions of instruction\nthroughput and port usage, and introduce a more precise definition of latency\nthat, in contrast to previous definitions, considers dependencies between\ndifferent pairs of input and output operands. We then develop novel algorithms\nto infer the latency, throughput, and port usage based on\nautomatically-generated microbenchmarks that are more accurate and precise than\nexisting work.\n  To facilitate the rapid construction of optimizing compilers and tools for\nperformance prediction, the output of our tool is provided in a\nmachine-readable format. We provide experimental results for processors of all\ngenerations of Intel's Core architecture, i.e., from Nehalem to Coffee Lake,\nand discuss various cases where the output of our tool differs considerably\nfrom prior work.\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 16:13:31 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 01:42:17 GMT"}, {"version": "v3", "created": "Tue, 5 Mar 2019 18:10:14 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Abel", "Andreas", ""], ["Reineke", "Jan", ""]]}, {"id": "1810.04828", "submitter": "Zheng Yang", "authors": "Zheng Yang, Hang Lei", "title": "FEther: An Extensible Definitional Interpreter for Smart-contract\n  Verifications in Coq", "comments": "33 pages, 18 figures, 8 tables", "journal-ref": "IEEE Access, 2019", "doi": "10.1109/ACCESS.2019.2905428", "report-no": "vol. 7, pp. 37770-37791", "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain technology adds records to a list using cryptographic links.\nTherefore, the security of blockchain smart contracts is among the most popular\ncontemporary research topics. To improve the theorem-proving technology in this\nfield, we are developing an extensible hybrid verification tool chain, denoted\nas FSPVM-E, for Ethereum smart contract verification. This hybrid system\nextends the proof assistants in Coq, a formal proof-management system.\nCombining symbolic execution with higher-order theorem-proving, it solves\nconsistency, automation, and reusability problems by standard theorem-proving\napproaches. This article completes the FSPVM-E by developing its proof engine.\nFSPVM-E is an extensible definitional interpreter based on our previous work\nFEther, which is totally developed in the Coq proof assistant. It supports\nalmost all semantics of the Solidity programing language, and simultaneously\nexecutes multiple types of symbols. FEther also contains a set of automatic\nstrategies that execute and verify the smart contracts in Coq with a high level\nof automation. The functional correctness of FEther was verified in Coq. In\nstandard tutorials, the execution efficiency of FEther far exceeded that of the\ninterpreters developed in Coq.\n", "versions": [{"version": "v1", "created": "Thu, 11 Oct 2018 03:06:38 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 06:11:42 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Yang", "Zheng", ""], ["Lei", "Hang", ""]]}, {"id": "1810.05294", "submitter": "Poornima Gopi", "authors": "Poornima Gopi", "title": "AppIntent: Intuitive Automation Specification Framework for Mobile\n  AppTesting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of mobile apps and reduced time in mobile app releases\nmandates the need for faster and efficient testing of mobile apps, their GUI\nand functional capabilities. Though, there are wide variety of open source\ntools and frameworks that are developed to provide automated test\ninfrastructure for testing mobile apps. Each of these automation tools supports\ndifferent scripting languages for automating the app testing. These frameworks\nfundamentally lacks the ability to directly capture the intent of the users who\nintend to effectively test the mobile app and its cross-app functional\ncapabilities and performance without worrying about the low-level scripting\nlanguage associated with each tool. Hence, to address this limitation, we\npropose a high-level intent-based automation specification language and APIs\nthat could effectively address following aspects: (i)capture the test\nautomation steps to be captured as high-level intents using intuitive\nautomation specification language, and (ii) provides framework support to\neffectively capture the users behavior patterns effectively for testing the\napps. We develop, AppIntent a high level automation specification\nlanguage-based framework that directly captures the test automation intents\nwith in and across multiple apps using high-level and intuitive language\nwithout worrying about how to actually develop scripts for automation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 00:05:33 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Gopi", "Poornima", ""]]}, {"id": "1810.05555", "submitter": "Tommaso Petrucciani", "authors": "Tommaso Petrucciani, Giuseppe Castagna, Davide Ancona, Elena Zucca", "title": "Semantic subtyping for non-strict languages", "comments": "Extended version of a submission to the post-proceedings of TYPES'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic subtyping is an approach to define subtyping relations for type\nsystems featuring union and intersection type connectives. It has been studied\nonly for strict languages, and it is unsound for non-strict semantics. In this\nwork, we study how to adapt this approach to non-strict languages: in\nparticular, we define a type system using semantic subtyping for a functional\nlanguage with a call-by-need semantics. We do so by introducing an explicit\nrepresentation for divergence in the types, so that the type system\ndistinguishes expressions that are results from those which are computations\nthat might diverge.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 14:42:45 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 10:13:18 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Petrucciani", "Tommaso", ""], ["Castagna", "Giuseppe", ""], ["Ancona", "Davide", ""], ["Zucca", "Elena", ""]]}, {"id": "1810.05661", "submitter": "Johannes Kinder", "authors": "Blake Loring, Duncan Mitchell, Johannes Kinder", "title": "Sound Regular Expression Semantics for Dynamic Symbolic Execution of\n  JavaScript", "comments": "This arXiv version (v4) contains fixes for some typographical errors\n  of the PLDI'19 version (the numbering of indices in Section 4.1 and the\n  example in Section 4.3)", "journal-ref": "Proc. ACM SIGPLAN Conf. Programming Language Design and\n  Implementation (PLDI), pp. 425-438, ACM, 2019", "doi": "10.1145/3314221.3314645", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing support for regular expressions in automated test generation or\nverification tools is lacking. Common aspects of regular expression engines\nfound in mainstream programming languages, such as backreferences or greedy\nmatching, are commonly ignored or imprecisely approximated, leading to poor\ntest coverage or failed proofs. In this paper, we present the first complete\nstrategy to faithfully reason about regular expressions in the context of\nsymbolic execution, focusing on the operators found in JavaScript. We model\nregular expression operations using string constraints and classical regular\nexpressions and use a refinement scheme to address the problem of matching\nprecedence and greediness. Our survey of over 400,000 JavaScript packages from\nthe NPM software repository shows that one fifth make use of complex regular\nexpressions features. We implemented our model in a dynamic symbolic execution\nengine for JavaScript and evaluated it on over 1,000 Node.js packages\ncontaining regular expressions, demonstrating that the strategy is effective\nand can increase line coverage of programs by up to 30%\n", "versions": [{"version": "v1", "created": "Wed, 10 Oct 2018 15:51:34 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 12:21:30 GMT"}, {"version": "v3", "created": "Tue, 7 May 2019 12:16:25 GMT"}, {"version": "v4", "created": "Fri, 13 Mar 2020 16:08:14 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Loring", "Blake", ""], ["Mitchell", "Duncan", ""], ["Kinder", "Johannes", ""]]}, {"id": "1810.06600", "submitter": "Colin Gordon", "authors": "Colin S. Gordon", "title": "Synthesizing Program-Specific Static Analyses", "comments": "Late archiving of OBT'18 abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a static analysis is generally a substantial undertaking, requiring\nsignificant expertise in both program analysis and the domain of the program\nanalysis, and significant development resources. As a result, most program\nanalyses target properties that are universallly of interest (e.g., absence of\nnull pointer dereference) or nearly so (e.g., deadlock freedom). However, many\ninteresting program properties that would benefit from static checking are\nspecific to individual programs, or sometimes programs utilizing a certain\nlibrary. It is impractical to devote program analysis and verification experts\nto these problems.\n  We propose instead to work on example-based synthesis of program analyses\nwithin well-understood domains like type qualifier systems and effect systems.\nThe dynamic behaviors behind the classes of problems these systems prevent\ncorrespond to examples that developers who lack expertise in static analysis\ncan readily provide (data flow paths, or stack traces).\n", "versions": [{"version": "v1", "created": "Mon, 15 Oct 2018 18:25:52 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Gordon", "Colin S.", ""]]}, {"id": "1810.06991", "submitter": "Tom Hirschowitz", "authors": "Clovis Eberhart, Tom Hirschowitz and Alexis Laouar", "title": "Simple game semantics and Day convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game semantics has provided adequate models for a variety of programming\nlanguages, in which types are interpreted as two-player games and programs as\nstrategies. Melli\\`es (2018) suggested that such categories of games and\nstrategies may be obtained as instances of a simple abstract construction on\nweak double categories. However, in the particular case of simple games, his\nconstruction slightly differs from the standard category. We refine the\nabstract construction using factorisation systems, and show that the new\nconstruction yields the standard category of simple games and strategies.\nAnother perhaps surprising instance is Day's convolution monoidal structure on\nthe category of presheaves over a strict monoidal category.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 13:39:38 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Eberhart", "Clovis", ""], ["Hirschowitz", "Tom", ""], ["Laouar", "Alexis", ""]]}, {"id": "1810.07517", "submitter": "Hongbo Rong", "authors": "Hongbo Rong", "title": "Expressing Sparse Matrix Computations for Productive Performance on\n  Spatial Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses spatial programming of sparse matrix computations for\nproductive performance. The challenge is how to express an irregular\ncomputation and its optimizations in a regular way.\n  A sparse matrix has (non-zero) values and a structure. In this paper, we\npropose to classify the implementations of a computation on a sparse matrix\ninto two categories: (1) structure-driven, or top-down, approach, which\ntraverses the structure with given row and column indices and locates the\ncorresponding values, and (2) values-driven, or bottom-up, approach, which\nloads and processes the values in parallel streams, and decodes the structure\nfor the values' corresponding row and column indices.\n  On a spatial architecture like FPGAs, the values-driven approach is the norm.\nWe show how to express a sparse matrix computation and its optimizations for a\nvalues-driven implementation. A compiler automatically synthesizes a code to\ndecode the structure. In this way, programmers focus on optimizing the\nprocessing of the values, using familiar optimizations for dense matrices,\nwhile leaving the complex, irregular structure traversal to an automatic\ncompiler. We also attempt to regularize the optimizations of the reduction for\na dynamic number of values, which is common in a sparse matrix computation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Oct 2018 18:37:06 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Rong", "Hongbo", ""]]}, {"id": "1810.07591", "submitter": "Patrick Diehl", "authors": "R. Tohid and Bibek Wagle and Shahrzad Shirzad and Patrick Diehl and\n  Adrian Serio and Alireza Kheirkhahan and Parsa Amini and Katy Williams and\n  Kate Isaacs and Kevin Huck and Steven Brandt and Hartmut Kaiser", "title": "Asynchronous Execution of Python Code on Task Based Runtime Systems", "comments": null, "journal-ref": null, "doi": "10.1109/ESPM2.2018.00009", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite advancements in the areas of parallel and distributed computing, the\ncomplexity of programming on High Performance Computing (HPC) resources has\ndeterred many domain experts, especially in the areas of machine learning and\nartificial intelligence (AI), from utilizing performance benefits of such\nsystems. Researchers and scientists favor high-productivity languages to avoid\nthe inconvenience of programming in low-level languages and costs of acquiring\nthe necessary skills required for programming at this level. In recent years,\nPython, with the support of linear algebra libraries like NumPy, has gained\npopularity despite facing limitations which prevent this code from distributed\nruns. Here we present a solution which maintains both high level programming\nabstractions as well as parallel and distributed efficiency. Phylanx, is an\nasynchronous array processing toolkit which transforms Python and NumPy\noperations into code which can be executed in parallel on HPC resources by\nmapping Python and NumPy functions and variables into a dependency tree\nexecuted by HPX, a general purpose, parallel, task-based runtime system written\nin C++. Phylanx additionally provides introspection and visualization\ncapabilities for debugging and performance analysis. We have tested the\nfoundations of our approach by comparing our implementation of widely used\nmachine learning algorithms to accepted NumPy standards.\n", "versions": [{"version": "v1", "created": "Wed, 17 Oct 2018 14:50:42 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 18:25:14 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Tohid", "R.", ""], ["Wagle", "Bibek", ""], ["Shirzad", "Shahrzad", ""], ["Diehl", "Patrick", ""], ["Serio", "Adrian", ""], ["Kheirkhahan", "Alireza", ""], ["Amini", "Parsa", ""], ["Williams", "Katy", ""], ["Isaacs", "Kate", ""], ["Huck", "Kevin", ""], ["Brandt", "Steven", ""], ["Kaiser", "Hartmut", ""]]}, {"id": "1810.07951", "submitter": "Michael Innes", "authors": "Michael Innes", "title": "Don't Unroll Adjoint: Differentiating SSA-Form Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents reverse-mode algorithmic differentiation (AD) based on\nsource code transformation, in particular of the Static Single Assignment (SSA)\nform used by modern compilers. The approach can support control flow, nesting,\nmutation, recursion, data structures, higher-order functions, and other\nlanguage constructs, and the output is given to an existing compiler to produce\nhighly efficient differentiated code. Our implementation is a new AD tool for\nthe Julia language, called Zygote, which presents high-level dynamic semantics\nwhile transparently compiling adjoint code under the hood. We discuss the\nbenefits of this approach to both the usability and performance of AD tools.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 08:48:12 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 11:46:55 GMT"}, {"version": "v3", "created": "Sat, 9 Feb 2019 00:30:57 GMT"}, {"version": "v4", "created": "Sat, 9 Mar 2019 13:25:32 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Innes", "Michael", ""]]}, {"id": "1810.08061", "submitter": "Andrew Johnson", "authors": "Dan Moldovan and James M Decker and Fei Wang and Andrew A Johnson and\n  Brian K Lee and Zachary Nado and D Sculley and Tiark Rompf and Alexander B\n  Wiltschko", "title": "AutoGraph: Imperative-style Coding with Graph-based Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a perceived trade-off between machine learning code that is easy to\nwrite, and machine learning code that is scalable or fast to execute. In\nmachine learning, imperative style libraries like Autograd and PyTorch are easy\nto write, but suffer from high interpretive overhead and are not easily\ndeployable in production or mobile settings. Graph-based libraries like\nTensorFlow and Theano benefit from whole-program optimization and can be\ndeployed broadly, but make expressing complex models more cumbersome. We\ndescribe how the use of staged programming in Python, via source code\ntransformation, offers a midpoint between these two library design patterns,\ncapturing the benefits of both. A key insight is to delay all type-dependent\ndecisions until runtime, via dynamic dispatch. We instantiate these principles\nin AutoGraph, a software system that improves the programming experience of the\nTensorFlow library, and demonstrate usability improvements with no loss in\nperformance compared to native TensorFlow graphs. We also show that our system\nis backend agnostic, and demonstrate targeting an alternate IR with\ncharacteristics not found in TensorFlow graphs.\n", "versions": [{"version": "v1", "created": "Tue, 16 Oct 2018 19:14:09 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 19:19:51 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Moldovan", "Dan", ""], ["Decker", "James M", ""], ["Wang", "Fei", ""], ["Johnson", "Andrew A", ""], ["Lee", "Brian K", ""], ["Nado", "Zachary", ""], ["Sculley", "D", ""], ["Rompf", "Tiark", ""], ["Wiltschko", "Alexander B", ""]]}, {"id": "1810.08289", "submitter": "Rahul Gopinath", "authors": "Rahul Gopinath, Bj\\\"orn Mathis, Mathias H\\\"oschele, Alexander\n  Kampmann, Andreas Zeller", "title": "Sample-Free Learning of Input Grammars for Comprehensive Software\n  Fuzzing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating valid test inputs for a program is much easier if one knows the\ninput language. We present first successes for a technique that, given a\nprogram P without any input samples or models, learns an input grammar that\nrepresents the syntactically valid inputs for P -- a grammar which can then be\nused for highly effective test generation for P . To this end, we introduce a\ntest generator targeted at input parsers that systematically explores parsing\nalternatives based on dynamic tracking of constraints; the resulting inputs go\ninto a grammar learner producing a grammar that can then be used for fuzzing.\nIn our evaluation on subjects such as JSON, URL, or Mathexpr, our PYGMALION\nprototype took only a few minutes to infer grammars and generate thousands of\nvalid high-quality inputs.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 22:12:26 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Gopinath", "Rahul", ""], ["Mathis", "Bj\u00f6rn", ""], ["H\u00f6schele", "Mathias", ""], ["Kampmann", "Alexander", ""], ["Zeller", "Andreas", ""]]}, {"id": "1810.08739", "submitter": "EPTCS", "authors": "John Derrick (University of Sheffield), Brijesh Dongol (University of\n  Surrey), Steve Reeves (University of Waikato)", "title": "Proceedings 18th Refinement Workshop", "comments": null, "journal-ref": "EPTCS 282, 2018", "doi": "10.4204/EPTCS.282", "report-no": null, "categories": "cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Refinement is one of the cornerstones of a formal approach to software\nengineering. Refinement is the process of developing a more detailed design or\nimplementation from an abstract specification through a sequence of\nmathematically-based steps that maintain correctness with respect to the\noriginal specification. Work on the foundations of languages such as Z, B, VDM\nand CSP have led to their widespread use in certain industrial sectors, e.g.,\nthose with security or safety critical concerns. In addition to precise\nspecification, formal methods also allow the possibility of precise and\nverifiable development, as captured by the concept of refinement.\n  The 18th Refinement Workshop was held as part of FLoC 2018 at Oxford, UK.\n", "versions": [{"version": "v1", "created": "Sat, 20 Oct 2018 02:55:10 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Derrick", "John", "", "University of Sheffield"], ["Dongol", "Brijesh", "", "University of\n  Surrey"], ["Reeves", "Steve", "", "University of Waikato"]]}, {"id": "1810.09065", "submitter": "Scott Stoller", "authors": "Christopher Kane, Bo Lin, Saksham Chand, Scott D. Stoller, Yanhong A.\n  Liu", "title": "High-level Cryptographic Abstractions", "comments": null, "journal-ref": "PLAS 2019: Proceedings of the 14th ACM SIGSAC Workshop on\n  Programming Languages and Analysis for Security. November 2019. Pages 31-43", "doi": "10.1145/3338504.3357343", "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interfaces exposed by commonly used cryptographic libraries are clumsy,\ncomplicated, and assume an understanding of cryptographic algorithms. The\nchallenge is to design high-level abstractions that require minimum knowledge\nand effort to use while also allowing maximum control when needed.\n  This paper proposes such high-level abstractions consisting of simple\ncryptographic primitives and full declarative configuration. These abstractions\ncan be implemented on top of any cryptographic library in any language. We have\nimplemented these abstractions in Python, and used them to write a wide variety\nof well-known security protocols, including Signal, Kerberos, and TLS.\n  We show that programs using our abstractions are much smaller and easier to\nwrite than using low-level libraries, where size of security protocols\nimplemented is reduced by about a third on average. We show our implementation\nincurs a small overhead, less than 5 microseconds for shared key operations and\nless than 341 microseconds (< 1%) for public key operations. We also show our\nabstractions are safe against main types of cryptographic misuse reported in\nthe literature.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 02:59:56 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 19:03:37 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Kane", "Christopher", ""], ["Lin", "Bo", ""], ["Chand", "Saksham", ""], ["Stoller", "Scott D.", ""], ["Liu", "Yanhong A.", ""]]}, {"id": "1810.09367", "submitter": "Thierry Coquand", "authors": "Thierry Coquand", "title": "Canonicity and normalisation for Dependent Type Theory", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show canonicity and normalization for dependent type theory with a\ncumulative sequence of universes and a type of Boolean. The argument follows\nthe usual notion of reducibility, going back to Godel's Dialectica\ninterpretation and the work of Tait. A key feature of our approach is the use\nof a proof relevant notion of reducibility.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 15:36:39 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Coquand", "Thierry", ""]]}, {"id": "1810.09538", "submitter": "Eli Bingham", "authors": "Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer,\n  Neeraj Pradhan, Theofanis Karaletsos, Rohit Singh, Paul Szerlip, Paul\n  Horsfall, Noah D. Goodman", "title": "Pyro: Deep Universal Probabilistic Programming", "comments": "Submitted to JMLR MLOSS track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pyro is a probabilistic programming language built on Python as a platform\nfor developing advanced probabilistic models in AI research. To scale to large\ndatasets and high-dimensional models, Pyro uses stochastic variational\ninference algorithms and probability distributions built on top of PyTorch, a\nmodern GPU-accelerated deep learning framework. To accommodate complex or\nmodel-specific algorithmic behavior, Pyro leverages Poutine, a library of\ncomposable building blocks for modifying the behavior of probabilistic\nprograms.\n", "versions": [{"version": "v1", "created": "Thu, 18 Oct 2018 19:28:32 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Bingham", "Eli", ""], ["Chen", "Jonathan P.", ""], ["Jankowiak", "Martin", ""], ["Obermeyer", "Fritz", ""], ["Pradhan", "Neeraj", ""], ["Karaletsos", "Theofanis", ""], ["Singh", "Rohit", ""], ["Szerlip", "Paul", ""], ["Horsfall", "Paul", ""], ["Goodman", "Noah D.", ""]]}, {"id": "1810.09555", "submitter": "Xiaoran Xu", "authors": "Xiaoran Xu, Keith Cooper, Jacob Brock, Yan Zhang, Handong Ye", "title": "ShareJIT: JIT Code Cache Sharing across Processes and Its Practical\n  Implementation", "comments": "OOPSLA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Just-in-time (JIT) compilation coupled with code caching are widely used to\nimprove performance in dynamic programming language implementations. These code\ncaches, along with the associated profiling data for the hot code, however,\nconsume significant amounts of memory. Furthermore, they incur extra JIT\ncompilation time for their creation. On Android, the current standard JIT\ncompiler and its code caches are not shared among processes---that is, the\nruntime system maintains a private code cache, and its associated data, for\neach runtime process. However, applications running on the same platform tend\nto share multiple libraries in common. Sharing cached code across multiple\napplications and multiple processes can lead to a reduction in memory use. It\ncan directly reduce compile time. It can also reduce the cumulative amount of\ntime spent interpreting code. All three of these effects can improve actual\nruntime performance.\n  In this paper, we describe ShareJIT, a global code cache for JITs that can\nshare code across multiple applications and multiple processes. We implemented\nShareJIT in the context of the Android Runtime (ART), a widely used,\nstate-of-the-art system. To increase sharing, our implementation constrains the\namount of context that the JIT compiler can use to optimize the code. This\nexposes a fundamental tradeoff: increased specialization to a single process'\ncontext decreases the extent to which the compiled code can be shared. In\nShareJIT, we limit some optimization to increase shareability. To evaluate the\nShareJIT, we tested 8 popular Android apps in a total of 30 experiments.\nShareJIT improved overall performance by 9% on average, while decreasing memory\nconsumption by 16% on average and JIT compilation time by 37% on average.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2018 21:05:15 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Xu", "Xiaoran", ""], ["Cooper", "Keith", ""], ["Brock", "Jacob", ""], ["Zhang", "Yan", ""], ["Ye", "Handong", ""]]}, {"id": "1810.09610", "submitter": "EPTCS", "authors": "Eric C.R. Hehner (University of Toronto)", "title": "A Theory of Lazy Imperative Timing", "comments": "In Proceedings Refine 2018, arXiv:1810.08739", "journal-ref": "EPTCS 282, 2018, pp. 1-9", "doi": "10.4204/EPTCS.282.1", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a theory of lazy imperative timing.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 00:47:36 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Hehner", "Eric C. R.", "", "University of Toronto"]]}, {"id": "1810.09611", "submitter": "EPTCS", "authors": "Ian J. Hayes (The University of Queensland)", "title": "Some Challenges of Specifying Concurrent Program Components", "comments": "In Proceedings Refine 2018, arXiv:1810.08739", "journal-ref": "EPTCS 282, 2018, pp. 10-22", "doi": "10.4204/EPTCS.282.2", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to address some of the challenges of formally\nspecifying components of shared-memory concurrent programs. The focus is to\nprovide an abstract specification of a component that is suitable for use both\nby clients of the component and as a starting point for refinement to an\nimplementation of the component. We present some approaches to devising\nspecifications, investigating different forms suitable for different contexts.\nWe examine handling atomicity of access to data structures, blocking operations\nand progress properties, and transactional operations that may fail and need to\nbe retried.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 00:47:50 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Hayes", "Ian J.", "", "The University of Queensland"]]}, {"id": "1810.09612", "submitter": "EPTCS", "authors": "Graeme Smith, Kirsten Winter, Robert J. Colvin", "title": "Correctness of Concurrent Objects under Weak Memory Models", "comments": "In Proceedings Refine 2018, arXiv:1810.08739. arXiv admin note: text\n  overlap with arXiv:1802.04954", "journal-ref": "EPTCS 282, 2018, pp. 53-67", "doi": "10.4204/EPTCS.282.5", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a theory for correctness of concurrent objects under\nweak memory models. Central to our definitions is the concept of observations\nwhich determine when effects of operations become visible, and hence determine\nthe semantics of objects, under a given memory model. The resulting notion of\ncorrectness, called object refinement, is generic as it is parameterised by the\nmemory model under consideration. Our theory enforces the minimal constraints\non the placing of observations and on the semantics of objects that underlie\nobject refinement. Object refinement is suitable as a reference for correctness\nwhen proving new proof methods for objects under weak memory models to be sound\nand complete.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 00:48:06 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Smith", "Graeme", ""], ["Winter", "Kirsten", ""], ["Colvin", "Robert J.", ""]]}, {"id": "1810.09613", "submitter": "EPTCS", "authors": "Emil Sekerinski (McMaster University), Shucai Yao (McMaster\n  University)", "title": "Refining Santa: An Exercise in Efficient Synchronization", "comments": "In Proceedings Refine 2018, arXiv:1810.08739", "journal-ref": "EPTCS 282, 2018, pp. 68-86", "doi": "10.4204/EPTCS.282.6", "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Santa Claus Problem is an intricate exercise for concurrent programming.\nThis paper outlines the refinement steps to develop a highly efficient\nimplementation with concurrent objects, starting from a simple specification.\nThe efficiency of the implementation is compared to those in other languages.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 00:48:21 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Sekerinski", "Emil", "", "McMaster University"], ["Yao", "Shucai", "", "McMaster\n  University"]]}, {"id": "1810.09717", "submitter": "Jakub Bednarek", "authors": "Jakub Bednarek, Karol Piaskowski, Krzysztof Krawiec", "title": "Ain't Nobody Got Time For Coding: Structure-Aware Program Synthesis From\n  Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program synthesis from natural language (NL) is practical for humans and,\nonce technically feasible, would significantly facilitate software development\nand revolutionize end-user programming. We present SAPS, an end-to-end neural\nnetwork capable of mapping relatively complex, multi-sentence NL specifications\nto snippets of executable code. The proposed architecture relies exclusively on\nneural components, and is trained on abstract syntax trees, combined with a\npretrained word embedding and a bi-directional multi-layer LSTM for processing\nof word sequences. The decoder features a doubly-recurrent LSTM, for which we\npropose novel signal propagation schemes and soft attention mechanism. When\napplied to a large dataset of problems proposed in a previous study, SAPS\nperforms on par with or better than the method proposed there, producing\ncorrect programs in over 92% of cases. In contrast to other methods, it does\nnot require post-processing of the resulting programs, and uses a\nfixed-dimensional latent representation as the only interface between the NL\nanalyzer and the source code generator.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 08:29:11 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 07:17:09 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Bednarek", "Jakub", ""], ["Piaskowski", "Karol", ""], ["Krawiec", "Krzysztof", ""]]}, {"id": "1810.09868", "submitter": "Keno Fischer", "authors": "Keno Fischer, Elliot Saba", "title": "Automatic Full Compilation of Julia Programs and ML Models to Cloud TPUs", "comments": "Submitted to SysML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Google's Cloud TPUs are a promising new hardware architecture for machine\nlearning workloads. They have powered many of Google's milestone machine\nlearning achievements in recent years. Google has now made TPUs available for\ngeneral use on their cloud platform and as of very recently has opened them up\nfurther to allow use by non-TensorFlow frontends. We describe a method and\nimplementation for offloading suitable sections of Julia programs to TPUs via\nthis new API and the Google XLA compiler. Our method is able to completely fuse\nthe forward pass of a VGG19 model expressed as a Julia program into a single\nTPU executable to be offloaded to the device. Our method composes well with\nexisting compiler-based automatic differentiation techniques on Julia code, and\nwe are thus able to also automatically obtain the VGG19 backwards pass and\nsimilarly offload it to the TPU. Targeting TPUs using our compiler, we are able\nto evaluate the VGG19 forward pass on a batch of 100 images in 0.23s which\ncompares favorably to the 52.4s required for the original model on the CPU. Our\nimplementation is less than 1000 lines of Julia, with no TPU specific changes\nmade to the core Julia compiler or any other Julia packages.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2018 14:02:11 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Fischer", "Keno", ""], ["Saba", "Elliot", ""]]}, {"id": "1810.10443", "submitter": "Tianhan Lu", "authors": "Tianhan Lu, Pavol Cerny, Bor-Yuh Evan Chang, Ashutosh Trivedi", "title": "Type-directed Bounding of Collections in Reactive Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Our aim is to statically verify that in a given reactive program, the length\nof collection variables does not grow beyond a given bound. We propose a\nscalable type-based technique that checks that each collection variable has a\ngiven refinement type that specifies constraints about its length. A novel\nfeature of our refinement types is that the refinements can refer to AST\ncounters that track how many times an AST node has been executed. This feature\nenables type refinements to track limited flow-sensitive information. We\ngenerate verification conditions that ensure that the AST counters are used\nconsistently, and that the types imply the given bound. The verification\nconditions are discharged by an off-the-shelf SMT solver. Experimental results\ndemonstrate that our technique is scalable, and effective at verifying reactive\nprograms with respect to requirements on length of collections.\n", "versions": [{"version": "v1", "created": "Wed, 24 Oct 2018 15:15:43 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 15:33:18 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Lu", "Tianhan", ""], ["Cerny", "Pavol", ""], ["Chang", "Bor-Yuh Evan", ""], ["Trivedi", "Ashutosh", ""]]}, {"id": "1810.10807", "submitter": "Sebastian Wolff", "authors": "Roland Meyer and Sebastian Wolff", "title": "Decoupling Lock-Free Data Structures from Memory Reclamation for Static\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification of concurrent data structures is one of the most challenging\ntasks in software verification. The topic has received considerable attention\nover the course of the last decade. Nevertheless, human-driven techniques\nremain cumbersome and notoriously difficult while automated approaches suffer\nfrom limited applicability. The main obstacle for automation is the complexity\nof concurrent data structures. This is particularly true in the absence of\ngarbage collection. The intricacy of lock-free memory management paired with\nthe complexity of concurrent data structures makes automated verification\nprohibitive.\n  In this work we present a method for verifying concurrent data structures and\ntheir memory management separately. We suggest two simpler verification tasks\nthat imply the correctness of the data structure. The first task establishes an\nover-approximation of the reclamation behavior of the memory management. The\nsecond task exploits this over-approximation to verify the data structure\nwithout the need to consider the implementation of the memory management\nitself. To make the resulting verification tasks tractable for automated\ntechniques, we establish a second result. We show that a verification tool\nneeds to consider only executions where a single memory location is reused. We\nimplemented our approach and were able to verify linearizability of\nMichael&Scott's queue and the DGLM queue for both hazard pointers and\nepoch-based reclamation. To the best of our knowledge, we are the first to\nverify such implementations fully automatically.\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 09:42:27 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 14:30:55 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Meyer", "Roland", ""], ["Wolff", "Sebastian", ""]]}, {"id": "1810.10826", "submitter": "Thorsten Wissmann", "authors": "Andrei Stefanescu, Stefan Ciobaca, Radu Mereuta, Brandon Moore, Traian\n  Florin Serbanuta, and Grigore Rosu", "title": "All-Path Reachability Logic", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 2 (April 30,\n  2019) lmcs:5408", "doi": "10.23638/LMCS-15(2:5)2019", "report-no": null, "categories": "cs.PL cs.FL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a language-independent proof system for reachability\nproperties of programs written in non-deterministic (e.g., concurrent)\nlanguages, referred to as all-path reachability logic. It derives\npartial-correctness properties with all-path semantics (a state satisfying a\ngiven precondition reaches states satisfying a given postcondition on all\nterminating execution paths). The proof system takes as axioms any\nunconditional operational semantics, and is sound (partially correct) and\n(relatively) complete, independent of the object language. The soundness has\nalso been mechanized in Coq. This approach is implemented in a tool for\nsemantics-based verification as part of the K framework (http://kframework.org)\n", "versions": [{"version": "v1", "created": "Thu, 25 Oct 2018 10:49:00 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 08:18:13 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Stefanescu", "Andrei", ""], ["Ciobaca", "Stefan", ""], ["Mereuta", "Radu", ""], ["Moore", "Brandon", ""], ["Serbanuta", "Traian Florin", ""], ["Rosu", "Grigore", ""]]}, {"id": "1810.11268", "submitter": "Cristian Ramon-Cortes", "authors": "Cristian Ramon-Cortes and Ramon Amela and Jorge Ejarque and Philippe\n  Clauss and Rosa M. Badia", "title": "AutoParallel: A Python module for automatic parallelization and\n  distributed execution of affine loop nests", "comments": "Accepted to the 8th Workshop on Python for High-Performance and\n  Scientific Computing (PyHPC 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last improvements in programming languages, programming models, and\nframeworks have focused on abstracting the users from many programming issues.\nAmong others, recent programming frameworks include simpler syntax, automatic\nmemory management and garbage collection, which simplifies code re-usage\nthrough library packages, and easily configurable tools for deployment. For\ninstance, Python has risen to the top of the list of the programming languages\ndue to the simplicity of its syntax, while still achieving a good performance\neven being an interpreted language. Moreover, the community has helped to\ndevelop a large number of libraries and modules, tuning them to obtain great\nperformance.\n  However, there is still room for improvement when preventing users from\ndealing directly with distributed and parallel computing issues. This paper\nproposes and evaluates AutoParallel, a Python module to automatically find an\nappropriate task-based parallelization of affine loop nests to execute them in\nparallel in a distributed computing infrastructure. This parallelization can\nalso include the building of data blocks to increase task granularity in order\nto achieve a good execution performance. Moreover, AutoParallel is based on\nsequential programming and only contains a small annotation in the form of a\nPython decorator so that anyone with little programming skills can scale up an\napplication to hundreds of cores.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 11:17:21 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Ramon-Cortes", "Cristian", ""], ["Amela", "Ramon", ""], ["Ejarque", "Jorge", ""], ["Clauss", "Philippe", ""], ["Badia", "Rosa M.", ""]]}, {"id": "1810.11334", "submitter": "Yangjia Li", "authors": "Mingsheng Ying, Li Zhou and Yangjia Li", "title": "Reasoning about Parallel Quantum Programs", "comments": "Added an application on formal verification of\n  Bravyi-Gosset-K\\\"onig's algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of parallel quantum programming by defining the\noperational and denotational semantics of parallel quantum programs. The\ntechnical contributions of this paper include: (1) find a series of useful\nproof rules for reasoning about correctness of parallel quantum programs; (2)\nprove a (relative) completeness of our proof rules for partial correctness of\ndisjoint parallel quantum programs; and (3) prove a strong soundness theorem of\nthe proof rules showing that partial correctness is well maintained at each\nstep of transitions in the operational semantics of a general parallel quantum\nprogram (with shared variables). This is achieved by partially overcoming the\nfollowing conceptual challenges that are never present in classical parallel\nprogramming: (i) the intertwining of nondeterminism caused by quantum\nmeasurements and introduced by parallelism; (ii) entanglement between component\nquantum programs; and (iii) combining quantum predicates in the overlap of\nstate Hilbert spaces of component quantum programs with shared variables.\nApplications of the techniques developed in this paper are illustrated by a\nformal verification of Bravyi-Gosset-K\\\"onig's parallel quantum algorithm\nsolving a linear algebra problem, which gives for the first time an\nunconditional proof of a computational quantum advantage.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 14:10:44 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 13:35:51 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Ying", "Mingsheng", ""], ["Zhou", "Li", ""], ["Li", "Yangjia", ""]]}, {"id": "1810.11482", "submitter": "Patrick Diehl", "authors": "Patrick Diehl and Madhavan Seshadri and Thomas Heller and Hartmut\n  Kaiser", "title": "Integration of CUDA Processing within the C++ library for parallelism\n  and concurrency (HPX)", "comments": null, "journal-ref": null, "doi": "10.1109/ESPM2.2018.00006", "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experience shows that on today's high performance systems the utilization of\ndifferent acceleration cards in conjunction with a high utilization of all\nother parts of the system is difficult. Future architectures, like exascale\nclusters, are expected to aggravate this issue as the number of cores are\nexpected to increase and memory hierarchies are expected to become deeper. One\nbig aspect for distributed applications is to guarantee high utilization of all\navailable resources, including local or remote acceleration cards on a cluster\nwhile fully using all the available CPU resources and the integration of the\nGPU work into the overall programming model.\n  For the integration of CUDA code we extended HPX, a general purpose C++ run\ntime system for parallel and distributed applications of any scale, and enabled\nasynchronous data transfers from and to the GPU device and the asynchronous\ninvocation of CUDA kernels on this data. Both operations are well integrated\ninto the general programming model of HPX which allows to seamlessly overlap\nany GPU operation with work on the main cores. Any user defined CUDA kernel can\nbe launched on any (local or remote) GPU device available to the distributed\napplication.\n  We present asynchronous implementations for the data transfers and kernel\nlaunches for CUDA code as part of a HPX asynchronous execution graph. Using\nthis approach we can combine all remotely and locally available acceleration\ncards on a cluster to utilize its full performance capabilities. Overhead\nmeasurements show, that the integration of the asynchronous operations (data\ntransfer + launches of the kernels) as part of the HPX execution graph imposes\nno additional computational overhead and significantly eases orchestrating\ncoordinated and concurrent work on the main cores and the used GPU devices.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 18:23:35 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Diehl", "Patrick", ""], ["Seshadri", "Madhavan", ""], ["Heller", "Thomas", ""], ["Kaiser", "Hartmut", ""]]}, {"id": "1810.11527", "submitter": "Anders Miltner", "authors": "Anders Miltner, Solomon Maina, Kathleen Fisher, Benjamin C. Pierce,\n  David Walker, Steve Zdancewic", "title": "Synthesizing Symmetric Lenses", "comments": "ICFP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lenses are programs that can be run both \"front to back\" and \"back to front,\"\nallowing updates to either their source or their target data to be transferred\nin both directions. Lenses have been extensively studied, extended, and\napplied. Recent work has demonstrated how techniques from type-directed program\nsynthesis can be used to efficiently synthesize a simple class of\nlenses---bijective lenses over string data---given a pair of types (regular\nexpressions) and examples.\n  We extend this synthesis algorithm to a broader class of lenses, called\nsimple symmetric lenses, including all bijective lenses, all of the popular\ncategory of \"asymmetric\" lenses, and a subset of the \"symmetric lenses\"\nproposed by Hofmann et al. Intuitively, simple symmetric lenses allow some\ninformation to be present on one side but not the other and vice versa. They\nare of independent theoretical interest, being the largest class of symmetric\nlenses that do not use persistent internal state.\n  Synthesizing simple symmetric lenses is more challenging than synthesizing\nbijective lenses: Since some of the information on each side can be\n\"disconnected\" from the other side, there will typically be many lenses that\nagree with a given example. To guide the search process, we use stochastic\nregular expressions and information theory to estimate the amount of\ninformation propagated by a candidate lens, preferring lenses that propagate\nmore information, as well as user annotations marking parts of the source and\ntarget formats as either irrelevant or essential.\n  We describe an implementation of simple symmetric lenses and our synthesis\nprocedure as extensions to the Boomerang language. We evaluate its performance\non 48 benchmark examples drawn from Flash Fill, Augeas, and the bidirectional\nprogramming literature. Our implementation can synthesize each of these lenses\nin under 30 seconds.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 20:53:28 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 14:24:23 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Miltner", "Anders", ""], ["Maina", "Solomon", ""], ["Fisher", "Kathleen", ""], ["Pierce", "Benjamin C.", ""], ["Walker", "David", ""], ["Zdancewic", "Steve", ""]]}, {"id": "1810.11530", "submitter": "Pascal Lamblin", "authors": "Bart van Merri\\\"enboer, Olivier Breuleux, Arnaud Bergeron, Pascal\n  Lamblin", "title": "Automatic differentiation in ML: Where we are and where we should be\n  going", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review the current state of automatic differentiation (AD) for array\nprogramming in machine learning (ML), including the different approaches such\nas operator overloading (OO) and source transformation (ST) used for AD,\ngraph-based intermediate representations for programs, and source languages.\nBased on these insights, we introduce a new graph-based intermediate\nrepresentation (IR) which specifically aims to efficiently support\nfully-general AD for array programming. Unlike existing dataflow programming\nrepresentations in ML frameworks, our IR naturally supports function calls,\nhigher-order functions and recursion, making ML models easier to implement. The\nability to represent closures allows us to perform AD using ST without a tape,\nmaking the resulting derivative (adjoint) program amenable to ahead-of-time\noptimization using tools from functional language compilers, and enabling\nhigher-order derivatives. Lastly, we introduce a proof of concept compiler\ntoolchain called Myia which uses a subset of Python as a front end.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2018 21:09:07 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 21:16:54 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["van Merri\u00ebnboer", "Bart", ""], ["Breuleux", "Olivier", ""], ["Bergeron", "Arnaud", ""], ["Lamblin", "Pascal", ""]]}, {"id": "1810.11673", "submitter": "M. H. van Emden", "authors": "M.H. van Emden", "title": "Beyond Structured Programming", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": "DCS-359-IR", "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The correctness of a structured program is, at best, plausible. Though this\nis a step forward compared to what came before, it falls short of verified\ncorrectness. To verify a structured program according to Hoare's method one is\nfaced with the problem of finding assertions to fit existing code. In 1971 this\nmode of verification was declared by Dijkstra as too hard to be of practical\nuse---he advised that proof and code were to grow together. A method for doing\nthis was independently published by Reynolds in 1978 and by van Emden in 1979.\nThe latter was further developed to attain the form of matrix code. This form\nof code not only obviates the need of fitting assertions to existing code, but\nhelps in discovering an algorithm that reaches a given postcondition from a\nfixed precondition. In this paper a keyboard-editable version of matrix code is\npresented that uses E.W. Dijkstra's guarded commands as starting point. The\nresult is reached by using Floyd's method rather than Hoare's as starting\npoint.\n", "versions": [{"version": "v1", "created": "Sat, 27 Oct 2018 16:42:41 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["van Emden", "M. H.", ""]]}, {"id": "1810.11765", "submitter": "Matthias Springer", "authors": "Matthias Springer, Hidehiko Masuhara", "title": "DynaSOAr: A Parallel Memory Allocator for Object-oriented Programming on\n  GPUs with Efficient Memory Access", "comments": "To appear in ECOOP 2019", "journal-ref": null, "doi": "10.4230/LIPIcs.ECOOP.2019.16", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object-oriented programming has long been regarded as too inefficient for\nSIMD high-performance computing, despite the fact that many important HPC\napplications have an inherent object structure. On SIMD accelerators, including\nGPUs, this is mainly due to performance problems with memory allocation and\nmemory access: There are a few libraries that support parallel memory\nallocation directly on accelerator devices, but all of them suffer from\nuncoalesed memory accesses.\n  We discovered a broad class of object-oriented programs with many important\nreal-world applications that can be implemented efficiently on massively\nparallel SIMD accelerators. We call this class Single-Method Multiple-Objects\n(SMMO), because parallelism is expressed by running a method on all objects of\na type.\n  To make fast GPU programming available to average programmers, we developed\nDynaSOAr, a CUDA framework for SMMO applications. DynaSOAr consists of (1) a\nfully-parallel, lock-free, dynamic memory allocator, (2) a data layout DSL and\n(3) an efficient, parallel do-all operation. DynaSOAr achieves performance\nsuperior to state-of-the-art GPU memory allocators by controlling both memory\nallocation and memory access.\n  DynaSOAr improves the usage of allocated memory with a Structure of Arrays\ndata layout and achieves low memory fragmentation through efficient management\nof free and allocated memory blocks with lock-free, hierarchical bitmaps.\nContrary to other allocators, our design is heavily based on atomic operations,\ntrading raw (de)allocation performance for better overall application\nperformance. In our benchmarks, DynaSOAr achieves a speedup of application code\nof up to 3x over state-of-the-art allocators. Moreover, DynaSOAr manages heap\nmemory more efficiently than other allocators, allowing programmers to run up\nto 2x larger problem sizes with the same amount of memory.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 06:19:31 GMT"}, {"version": "v2", "created": "Sat, 12 Jan 2019 15:53:35 GMT"}, {"version": "v3", "created": "Fri, 5 Apr 2019 01:36:51 GMT"}, {"version": "v4", "created": "Sun, 9 Jun 2019 04:34:35 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Springer", "Matthias", ""], ["Masuhara", "Hidehiko", ""]]}, {"id": "1810.11865", "submitter": "John Vilk", "authors": "John Vilk, Emery D. Berger, James Mickens, Mark Marron", "title": "McFly: Time-Travel Debugging for the Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-traveling debuggers offer the promise of simplifying debugging by\nletting developers freely step forwards and backwards through a program's\nexecution. However, web applications present multiple challenges that make\ntime-travel debugging especially difficult. A time-traveling debugger for web\napplications must accurately reproduce all network interactions, asynchronous\nevents, and visual states observed during the original execution, both while\nstepping forwards and backwards. This must all be done in the context of a\ncomplex and highly multithreaded browser runtime. At the same time, to be\npractical, a time-traveling debugger must maintain interactive speeds.\n  This paper presents McFly, the first time-traveling debugger for web\napplications. McFly departs from previous approaches by operating on a\nhigh-level representation of the browser's internal state. This approach lets\nMcFly provide accurate time-travel debugging - maintaining JavaScript and\nvisual state in sync at all times - at interactive speeds. McFly's architecture\nis browser-agnostic, building on web standards supported by all major browsers.\nWe have implemented McFly as an extension to the Microsoft Edge web browser,\nand core parts of McFly have been integrated into a time-traveling debugger\nproduct from Microsoft.\n", "versions": [{"version": "v1", "created": "Sun, 28 Oct 2018 19:27:06 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Vilk", "John", ""], ["Berger", "Emery D.", ""], ["Mickens", "James", ""], ["Marron", "Mark", ""]]}, {"id": "1810.12041", "submitter": "Lucas Carvalho Cordeiro", "authors": "Mikhail R. Gadelha, Enrico Steffinlongo, Lucas C. Cordeiro, Bernd\n  Fischer and Denis A. Nicole", "title": "SMT-Based Refutation of Spurious Bug Reports in the Clang Static\n  Analyzer", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and evaluate a bug refutation extension for the Clang Static\nAnalyzer (CSA) that addresses the limitations of the existing built-in\nconstraint solver. In particular, we complement CSA's existing heuristics that\nremove spurious bug reports. We encode the path constraints produced by CSA as\nSatisfiability Modulo Theories (SMT) problems, use SMT solvers to precisely\ncheck them for satisfiability, and remove bug reports whose associated path\nconstraints are unsatisfiable. Our refutation extension refutes spurious bug\nreports in 8 out of 12 widely used open-source applications; on average, it\nrefutes ca. 7% of all bug reports, and never refutes any true bug report. It\nincurs only negligible performance overheads, and on average adds 1.2% to the\nruntime of the full Clang/LLVM toolchain. A demonstration is available at {\\tt\nhttps://www.youtube.com/watch?v=ylW5iRYNsGA}.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 10:27:24 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 05:32:18 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Gadelha", "Mikhail R.", ""], ["Steffinlongo", "Enrico", ""], ["Cordeiro", "Lucas C.", ""], ["Fischer", "Bernd", ""], ["Nicole", "Denis A.", ""]]}, {"id": "1810.12146", "submitter": "Hanwen Wu", "authors": "Hanwen Wu, Hongwei Xi", "title": "Implementing Linking in Multiparty Sessions (Extended Abstract)", "comments": "AGERE! 2018, Boston, MA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fast growth of service-oriented programming (SOP) is evident in this day\nand age of the Internet, and handling communication is of paramount importance\nin SOP. Session types are a formalism that is proposed to specify interactions\nbetween communicating processes. In essence, a session type system is a kind of\ntype system designed to enforce (through type-checking) that the involved\nprocesses communicate according to a chosen protocol specified as a session\ntype. It is well-known that linear logic plays a pivotal role in the study of\nsession types. For instance, various inference rules in linear logic can be\ninterpreted as ways for constructing channels (used by communicating processes\nto send/receive messages.) A particularly interesting case is the cut-rule in\nlinear logic, which can be interpreted as a way for connecting the ends of two\nmatching channels to form a single new channel. This form of channel\nconstruction is often referred to as linking or (bi-directional) forwarding. We\nhave generalized classical linear logic into classical linear multirole logic\n(LMRL), where the former can be seen as a special case of the latter involving\nonly two roles. In LMRL, there is a cut-rule involving multiple sequents\n(instead of exactly two), which we call multiparty cut (mp-cut). We have also\nformulated a novel multiparty session type system directly based on LMRL. When\nimplementing it, we need to find a way of connecting multiple channels that\ncorresponds to mp-cut. In this paper, we describe an implementation of linking\nfor multiparty sessions in the setting of shared memory. We also describe two\nnovel concepts, two-way linking with residual and three-way linking, which can\nonly be formulated in the setting of multiparty sessions. Notably, linking for\nbinary sessions can be thought of as a specially optimized version of what is\nimplemented for multiparty sessions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 14:27:51 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Wu", "Hanwen", ""], ["Xi", "Hongwei", ""]]}, {"id": "1810.12190", "submitter": "Hongwei Xi", "authors": "Hongwei Xi and Dengping Zhu", "title": "To Memory Safety through Proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a type system capable of guaranteeing the memory safety of\nprograms that may involve (sophisticated) pointer manipulation such as pointer\narithmetic. With its root in a recently developed framework Applied Type System\n(ATS), the type system imposes a level of abstraction on program states through\na novel notion of recursive stateful views and then relies on a form of linear\nlogic to reason about such stateful views. We consider the design and then the\nformalization of the type system to constitute the primary contribution of the\npaper. In addition, we also mention a running implementation of the type system\nand then give some examples in support of the practicality of programming with\nrecursive stateful views.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 15:29:24 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Xi", "Hongwei", ""], ["Zhu", "Dengping", ""]]}, {"id": "1810.12396", "submitter": "Calvin Smith", "authors": "Calvin Smith, Justin Hsu, Aws Albarghouthi", "title": "Trace Abstraction Modulo Probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose trace abstraction modulo probability, a proof technique for\nverifying high-probability accuracy guarantees of probabilistic programs. Our\nproofs overapproximate the set of program traces using failure automata,\nfinite-state automata that upper bound the probability of failing to satisfy a\ntarget specification. We automate proof construction by reducing probabilistic\nreasoning to logical reasoning: we use program synthesis methods to select\naxioms for sampling instructions, and then apply Craig interpolation to prove\nthat traces fail the target specification with only a small probability. Our\nmethod handles programs with unknown inputs, parameterized distributions,\ninfinite state spaces, and parameterized specifications. We evaluate our\ntechnique on a range of randomized algorithms drawn from the differential\nprivacy literature and beyond. To our knowledge, our approach is the first to\nautomatically establish accuracy properties of these algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2018 20:36:12 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Smith", "Calvin", ""], ["Hsu", "Justin", ""], ["Albarghouthi", "Aws", ""]]}, {"id": "1810.12619", "submitter": "Atsushi Igarashi", "authors": "Yusuke Miyazaki and Taro Sekiyama and Atsushi Igarashi", "title": "Dynamic Type Inference for Gradual Hindley--Milner Typing", "comments": "Some typos are corrected in v2", "journal-ref": "PACMPL 3(POPL): 18:1-18:29 (2019)", "doi": "10.1145/3290331", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Garcia and Cimini study a type inference problem for the ITGL, an implicitly\nand gradually typed language with let-polymorphism, and develop a sound and\ncomplete inference algorithm for it. Soundness and completeness mean that, if\nthe algorithm succeeds, the input term can be translated to a well-typed term\nof an explicitly typed blame calculus by cast insertion and vice versa.\nHowever, in general, there are many possible translations depending on how type\nvariables that were left undecided by static type inference are instantiated\nwith concrete static types. Worse, the translated terms may behave\ndifferently---some evaluate to values but others raise blame.\n  In this paper, we propose and formalize a new blame calculus\n$\\lambda^{\\textsf{DTI}}_{\\textsf{B}}$ that avoids such divergence as an\nintermediate language for the ITGL. A main idea is to allow a term to contain\ntype variables (that have not been instantiated during static type inference)\nand defer instantiation of these type variables to run time. We introduce\ndynamic type inference (DTI) into the semantics of\n$\\lambda^{\\textsf{DTI}}_{\\textsf{B}}$ so that type variables are instantiated\nalong reduction. The DTI-based semantics not only avoids the divergence\ndescribed above but also is sound and complete with respect to the semantics of\nfully instantiated terms in the following sense: if the evaluation of a term\nsucceeds (i.e., terminates with a value) in the DTI-based semantics, then there\nis a fully instantiated version of the term that also succeeds in the\nexplicitly typed blame calculus and vice versa.\n  Finally, we prove the gradual guarantee, which is an important correctness\ncriterion of a gradually typed language, for the ITGL.\n", "versions": [{"version": "v1", "created": "Tue, 30 Oct 2018 10:06:46 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 10:27:15 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Miyazaki", "Yusuke", ""], ["Sekiyama", "Taro", ""], ["Igarashi", "Atsushi", ""]]}, {"id": "1810.13430", "submitter": "Jan Malakhovski", "authors": "Jan Malakhovski", "title": "Exceptionally Monadic Error Handling", "comments": "fixed several typos, added more references, better abstract, and a\n  bunch of random changes here and there (mostly clarifications and\n  terminology)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We notice that the type of catch :: c a -> (e -> c a) -> c a operator is a\nspecial case of monadic bind operator (>>=) :: m a -> (a -> m b) -> m b, the\nsemantics (surprisingly) matches, and this observation has many interesting\nconsequences.\n  For instance, the reader is probably aware that the monadic essence of the\n(>>=) operator of the error monad $\\lambda A.E \\lor A$ is to behave like\nidentity monad for \"normal\" values and to stop on \"errors\". The unappreciated\nfact is that handling of said \"errors\" with a catch operator of the \"flipped\"\n\"conjoined\" error monad $\\lambda E.E \\lor A$ is, too, a monadic computation\nthat treats still unhandled \"errors\" as \"normal\" values and stops when an\n\"error\" is finally handled.\n  We show that for an appropriately indexed type of computations such a\n\"conjoined\" structure naturally follows from the conventional operational\nsemantics of throw and catch operators. Consequently, we show that this\nstructure uniformly generalizes all conventional monadic error handling\nmechanisms we are aware of. We also demonstrate several more interesting\ninstances of this structure of which at least bi-indexed monadic parser\ncombinators and conventional exceptions implemented via continuations have\nimmediate practical applications. Finally, we notice that these observations\nprovide surprising perspectives on error handling in general and point to a\nlargely unexplored trail in programming language design space.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 17:42:58 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 07:14:43 GMT"}, {"version": "v3", "created": "Wed, 30 Jan 2019 20:39:54 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Malakhovski", "Jan", ""]]}, {"id": "1810.13432", "submitter": "Daniel Milroy", "authors": "Daniel J. Milroy, Allison H. Baker, Dorit M. Hammerling, Youngsung\n  Kim, Elizabeth R. Jessup, Thomas Hauser", "title": "Making root cause analysis feasible for large code bases: a solution\n  approach for a climate model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For large-scale simulation codes with huge and complex code bases, where\nbit-for-bit comparisons are too restrictive, finding the source of\nstatistically significant discrepancies (e.g., from a previous version,\nalternative hardware or supporting software stack) in output is non-trivial at\nbest. Although there are many tools for program comprehension through debugging\nor slicing, few (if any) scale to a model as large as the Community Earth\nSystem Model (CESM; trademarked), which consists of more than 1.5 million lines\nof Fortran code. Currently for the CESM, we can easily determine whether a\ndiscrepancy exists in the output using a by now well-established statistical\nconsistency testing tool. However, this tool provides no information as to the\npossible cause of the detected discrepancy, leaving developers in a seemingly\nimpossible (and frustrating) situation. Therefore, our aim in this work is to\nprovide the tools to enable developers to trace a problem detected through the\nCESM output to its source. To this end, our strategy is to reduce the search\nspace for the root cause(s) to a tractable size via a series of techniques that\ninclude creating a directed graph of internal CESM variables, extracting a\nsubgraph (using a form of hybrid program slicing), partitioning into\ncommunities, and ranking nodes by centrality. Runtime variable sampling then\nbecomes feasible in this reduced search space. We demonstrate the utility of\nthis process on multiple examples of CESM simulation output by illustrating how\nsampling can be performed as part of an efficient parallel iterative refinement\nprocedure to locate error sources, including sensitivity to CPU instructions.\nBy providing CESM developers with tools to identify and understand the reason\nfor statistically distinct output, we have positively impacted the CESM\nsoftware development cycle and, in particular, its focus on quality assurance.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 17:45:10 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 05:33:50 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Milroy", "Daniel J.", ""], ["Baker", "Allison H.", ""], ["Hammerling", "Dorit M.", ""], ["Kim", "Youngsung", ""], ["Jessup", "Elizabeth R.", ""], ["Hauser", "Thomas", ""]]}]