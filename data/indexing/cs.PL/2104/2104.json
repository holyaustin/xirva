[{"id": "2104.00250", "submitter": "Kc Sivaramakrishnan", "authors": "KC Sivaramakrishnan, Stephen Dolan, Leo White, Tom Kelly, Sadiq\n  Jaffer, Anil Madhavapeddy", "title": "Retrofitting Effect Handlers onto OCaml", "comments": "Accepted to PLDI 2021", "journal-ref": null, "doi": "10.1145/3453483.3454039", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Effect handlers have been gathering momentum as a mechanism for modular\nprogramming with user-defined effects. Effect handlers allow for non-local\ncontrol flow mechanisms such as generators, async/await, lightweight threads\nand coroutines to be composably expressed. We present a design and evaluate a\nfull-fledged efficient implementation of effect handlers for OCaml, an\nindustrial-strength multi-paradigm programming language. Our implementation\nstrives to maintain the backwards compatibility and performance profile of\nexisting OCaml code. Retrofitting effect handlers onto OCaml is challenging\nsince OCaml does not currently have any non-local control flow mechanisms other\nthan exceptions. Our implementation of effect handlers for OCaml: (i) imposes a\nmean 1% overhead on a comprehensive macro benchmark suite that does not use\neffect handlers; (ii) remains compatible with program analysis tools that\ninspect the stack; and (iii) is efficient for new code that makes use of effect\nhandlers.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 04:34:23 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Sivaramakrishnan", "KC", ""], ["Dolan", "Stephen", ""], ["White", "Leo", ""], ["Kelly", "Tom", ""], ["Jaffer", "Sadiq", ""], ["Madhavapeddy", "Anil", ""]]}, {"id": "2104.00378", "submitter": "Seamus Brady", "authors": "Seamus Brady", "title": "The Comprehensive Blub Archive Network: Towards Design Principals for\n  Open Source Programming Language Repositories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many popular open source programming languages (Perl, Ruby or Python for\nexample) have systems for distributing packaged source code that software\ndevelopers can use when working in that particular programming language. This\npaper will consider the design principals that should be followed if designing\nsuch an open source code repository.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 10:13:42 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Brady", "Seamus", ""]]}, {"id": "2104.00379", "submitter": "Seamus Brady", "authors": "Seamus Brady", "title": "Immutability and Design Patterns in Ruby", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Functional Programming has seen a resurgence in interest in the last few\nyears and is often mentioned in opposition to Object-Orientated Programming.\nHowever, Object-Orientated Programming can actually absorb some of the lessons\nof Functional Programming to improve performance and reliability. This paper\nexamines three ways in which Immutability, a common Functional Programming\ntechnique, can be used to develop immutable objects in Ruby.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 10:18:52 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Brady", "Seamus", ""]]}, {"id": "2104.00461", "submitter": "Klaus V. Gleissenthall", "authors": "Rami Gokhan Kici and Klaus v. Gleissenthall and Deian Stefan and\n  Ranjit Jhala", "title": "Solver-Aided Constant-Time Circuit Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Xenon, a solver-aided method for formally verifying that Verilog\nhardware executes in constant-time. Xenon scales to realistic hardware designs\nby drastically reducing the effort needed to localize the root cause of\nverification failures via a new notion of constant-time counterexamples, which\nXenon uses to automatically synthesize a minimal set of secrecy assumptions.\nXenon further exploits modularity in Verilog code via a notion of module\nsummaries, thereby avoiding duplicate work across multiple module\ninstantiations. We show how Xenon's assumption synthesis and summaries enable\nthe verification of a variety of circuits including AES, a highly modular\nAES-256 implementation where modularity cuts verification from six hours to\nunder three seconds, and ScarV, a timing channel hardened RISC-V\nmicro-controller whose size exceeds previously verified designs by an order of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 13:44:53 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Kici", "Rami Gokhan", ""], ["Gleissenthall", "Klaus v.", ""], ["Stefan", "Deian", ""], ["Jhala", "Ranjit", ""]]}, {"id": "2104.00480", "submitter": "Edwin Brady", "authors": "Edwin Brady", "title": "Idris 2: Quantitative Type Theory in Practice", "comments": "To appear in proceedings of ECOOP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dependent types allow us to express precisely what a function is intended to\ndo. Recent work on Quantitative Type Theory (QTT) extends dependent type\nsystems with linearity, also allowing precision in expressing when a function\ncan run. This is promising, because it suggests the ability to design and\nreason about resource usage protocols, such as we might find in distributed and\nconcurrent programming, where the state of a communication channel changes\nthroughout program execution. As yet, however, there has not been a full-scale\nprogramming language with which to experiment with these ideas. Idris 2 is a\nnew version of the dependently typed language Idris, with a new core language\nbased on QTT, supporting linear and dependent types. In this paper, we\nintroduce Idris 2, and describe how QTT has influenced its design. We give\nexamples of the benefits of QTT in practice including: expressing which data is\nerased at run time, at the type level; and, resource tracking in the type\nsystem leading to type-safe concurrent programming with session types.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 14:09:56 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Brady", "Edwin", ""]]}, {"id": "2104.00739", "submitter": "Gopal P. Sarma", "authors": "Gopal Sarma, James Koppel, Gregory Malecha, Patrick Schultz, Eric\n  Drexler, Ramana Kumar, Cody Roux, and Philip Zucker", "title": "Formal Methods for the Informal Engineer: Workshop Recommendations", "comments": "6 pages", "journal-ref": null, "doi": "10.31219/osf.io/t4qs8", "report-no": null, "categories": "cs.SE cs.AI cs.LG cs.PL q-bio.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Formal Methods for the Informal Engineer (FMIE) was a workshop held at the\nBroad Institute of MIT and Harvard in 2021 to explore the potential role of\nverified software in the biomedical software ecosystem. The motivation for\norganizing FMIE was the recognition that the life sciences and medicine are\nundergoing a transition from being passive consumers of software and AI/ML\ntechnologies to fundamental drivers of new platforms, including those which\nwill need to be mission and safety-critical. Drawing on conversations leading\nup to and during the workshop, we make five concrete recommendations to help\nsoftware leaders organically incorporate tools, techniques, and perspectives\nfrom formal methods into their project planning and development trajectories.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 19:22:42 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Sarma", "Gopal", ""], ["Koppel", "James", ""], ["Malecha", "Gregory", ""], ["Schultz", "Patrick", ""], ["Drexler", "Eric", ""], ["Kumar", "Ramana", ""], ["Roux", "Cody", ""], ["Zucker", "Philip", ""]]}, {"id": "2104.01065", "submitter": "Ryan Kavanagh", "authors": "Ryan Kavanagh", "title": "Fairness and Observed Communication Semantics for Session-Typed\n  Languages", "comments": "Significantly expands the workshop version of this paper: Ryan\n  Kavanagh. \"Substructural Observed Communication Semantics\". In: Proceedings\n  of EXPRESS/SOS 2020. Electronic Proceedings in Theoretical Computer Science\n  322. Aug. 27, 2020, pp. 69-87. doi: 10.4204/EPTCS.322.7. arXiv: 2008.13358v1\n  [cs.PL]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observed communication semantics provide an intuitive notion of equivalence\nfor communicating programs. We give the first observed communication semantics\nfor a session-typed programming language with general recursion. This language,\nPolarized SILL, also supports channel and code transmission, synchronous and\nasynchronous communication, and functional programming. We present a framework\ninspired by testing preorders for defining simulations based on observed\ncommunications. We show that the \"external\" simulations coincide with barbed\nprecongruence.\n  Polarized SILL is defined using a multiset-rewriting-style substructural\noperational semantics. To ensure that our observed communication semantics is\nwell-defined, we introduce fairness for multiset rewriting systems. We\nconstruct a fair scheduler and we give sufficient conditions for traces to be\nfair.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 14:31:33 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Kavanagh", "Ryan", ""]]}, {"id": "2104.01189", "submitter": "{\\DJ}or{\\dj}e \\v{Z}ikeli\\'c", "authors": "Krishnendu Chatterjee, Ehsan Kafshdar Goharshady, Petr Novotn\\'y,\n  {\\DJ}or{\\dj}e \\v{Z}ikeli\\'c", "title": "Proving Non-termination by Program Reversal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to proving non-termination of non-deterministic\ninteger programs. Our technique is rather simple but efficient. It relies on a\npurely syntactic reversal of the program's transition system followed by a\nconstraint-based invariant synthesis with constraints coming from both the\noriginal and the reversed transition system. The latter task is performed by a\nsimple call to an off-the-shelf SMT-solver, which allows us to leverage the\nlatest advances in SMT-solving. Moreover, our method offers a combination of\nfeatures not present (as a whole) in previous approaches: it handles programs\nwith non-determinism, provides relative completeness guarantees and supports\nprograms with polynomial arithmetic. The experiments performed with our\nprototype tool RevTerm show that our approach, despite its simplicity and\nstronger theoretical guarantees, is at least on par with the state-of-the-art\ntools, often achieving a non-trivial improvement under a proper configuration\nof its parameters.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 18:13:30 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Goharshady", "Ehsan Kafshdar", ""], ["Novotn\u00fd", "Petr", ""], ["\u017dikeli\u0107", "\u0110or\u0111e", ""]]}, {"id": "2104.01239", "submitter": "Stephanie Weirich", "authors": "Stephanie Weirich and Benjamin Pierce", "title": "ICFP 2020 Post-Conference Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This document describes the ICFP 2020 virtual conference, including the\nplanning process and the criteria that informed its design, plus feedback from\nthe post-conference survey. It is intended to provide a record of the event and\ngive advice to future organizers of virtual conferences.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 18:15:26 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Weirich", "Stephanie", ""], ["Pierce", "Benjamin", ""]]}, {"id": "2104.01241", "submitter": "Darshana Balakrishnan", "authors": "Darshana Balakrishnan, Carl Nuessle, Oliver Kennedy, Lukasz Ziarek", "title": "TreeToaster: Towards an IVM-Optimized Compiler", "comments": "23 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A compiler's optimizer operates over abstract syntax trees (ASTs),\ncontinuously applying rewrite rules to replace subtrees of the AST with more\nefficient ones. Especially on large source repositories, even simply finding\nopportunities for a rewrite can be expensive, as optimizer traverses the AST\nnaively. In this paper, we leverage the need to repeatedly find rewrites, and\nexplore options for making the search faster through indexing and incremental\nview maintenance (IVM). Concretely, we consider bolt-on approaches that make\nuse of embedded IVM systems like DBToaster, as well as two new approaches:\nLabel-indexing and TreeToaster, an AST-specialized form of IVM. We integrate\nthese approaches into an existing just-in-time data structure compiler and show\nexperimentally that TreeToaster can significantly improve performance with\nminimal memory overheads.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 21:13:37 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Balakrishnan", "Darshana", ""], ["Nuessle", "Carl", ""], ["Kennedy", "Oliver", ""], ["Ziarek", "Lukasz", ""]]}, {"id": "2104.01270", "submitter": "Benno Stein", "authors": "Benno Stein, Bor-Yuh Evan Chang, Manu Sridharan", "title": "Demanded Abstract Interpretation (Extended Version)", "comments": "extended version of PLDI'21 paper (with appendices)", "journal-ref": null, "doi": "10.1145/3453483.3454044", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of making expressive static analyzers interactive.\nFormal static analysis is seeing increasingly widespread adoption as a tool for\nverification and bug-finding, but even with powerful cloud infrastructure it\ncan take minutes or hours to get batch analysis results after a code change.\nWhile existing techniques offer some demand-driven or incremental aspects for\ncertain classes of analysis, the fundamental challenge we tackle is doing both\nfor arbitrary abstract interpreters.\n  Our technique, demanded abstract interpretation, lifts program syntax and\nanalysis state to a dynamically evolving graph structure, in which program\nedits, client-issued queries, and evaluation of abstract semantics are all\ntreated uniformly. The key difficulty addressed by our approach is the\napplication of general incremental computation techniques to the complex,\ncyclic dependency structure induced by abstract interpretation of loops with\nwidening operators. We prove that desirable abstract interpretation\nmeta-properties, including soundness and termination, are preserved in our\napproach, and that demanded analysis results are equal to those computed by a\nbatch abstract interpretation. Experimental results suggest promise for a\nprototype demanded abstract interpretation framework: by combining incremental\nand demand-driven techniques, our framework consistently delivers analysis\nresults at interactive speeds, answering 95% of queries within 1.2 seconds.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 23:08:47 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 22:25:43 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Stein", "Benno", ""], ["Chang", "Bor-Yuh Evan", ""], ["Sridharan", "Manu", ""]]}, {"id": "2104.01358", "submitter": "Ugo de'Liguoro", "authors": "Ugo de'Liguoro and Riccardo Treglia", "title": "Intersection Types for a Computational Lambda-Calculus with Global State", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the semantics of an untyped lambda-calculus equipped with operators\nrepresenting read and write operations from and to a global state. We adopt the\nmonadic approach to model side effects and treat read and write as algebraic\noperations over a computational monad. We introduce an operational semantics\nand a type assignment system of intersection types, and prove that types are\ninvariant under reduction and expansion of term and state configurations, and\ncharacterize convergent terms via their typings.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 09:29:27 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["de'Liguoro", "Ugo", ""], ["Treglia", "Riccardo", ""]]}, {"id": "2104.01438", "submitter": "Ayush Bansal", "authors": "Anay Mehrotra, Ayush Bansal, Awanish Pandey, and Subhajit Roy", "title": "Input Validation with Symbolic Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic execution has always been plagued by the inability to handle\nprograms that require highly structured inputs. Most often, the symbolic\nexecution engine gets overwhelmed by the sheer number of infeasible paths and\nfails to explore enough feasible paths to gain any respectable coverage. In\nthis paper, we propose a system, InVaSion, that attempts to solve this problem\nfor forking-based symbolic execution engines. We propose an input specification\nlanguage (ISL) that is based on a finite-state automaton but includes guarded\ntransitions, a set of registers and a set of commands to update the register\nstates. We demonstrate that our language is expressive enough to handle complex\ninput specifications, like the Tiff image format, while not requiring\nsubstantial human effort; even the Tiff image specification could be specified\nin our language with an automaton of about 35 states. InVaSion translates the\ngiven program and the input specification into a non-deterministic program and\nuses symbolic execution to instantiate the non-determinism. This allows our\ntool to work with any forking-based symbolic execution engine and with no\nrequirement of any special theory solver. Over our set of benchmarks, on an\naverage, InVaSion was able to increase branch coverage from 24.97% to 67.84%\nover baseline KLEE.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 16:06:44 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Mehrotra", "Anay", ""], ["Bansal", "Ayush", ""], ["Pandey", "Awanish", ""], ["Roy", "Subhajit", ""]]}, {"id": "2104.01667", "submitter": "Rene Haberland", "authors": "Ren\\'e Haberland", "title": "A Logical Programming Language as an Instrument for Specifying and\n  Verifying Dynamic Memory", "comments": "209 pages, 97 figures, 6 appendices", "journal-ref": "Dissertation, Thesis, 2017", "doi": null, "report-no": null, "categories": "cs.LO cs.FL cs.PL cs.SC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a Prolog-dialect for the found and prioritised problems on\nexpressibility and automation. Given some given C-like program, if dynamic\nmemory is allocated, altered and freed on runtime, then a description of\ndesired dynamic memory is a heap specification. The check of calculated memory\nstate against a given specification is dynamic memory verification. This\ncontribution only considers formal specification and verification in a Hoare\ncalculus. Issues found include: invalid assignment, (temporary) unavailable\ndata in memory cells, excessive memory allocation, (accidental) heap alteration\nin unexpected regions and others. Excessive memory allocation is nowadays\nsuccessfully resolved by memory analysers like Valgrind. Essentially, papers in\nthose areas did not bring any big breakthrough. Possible reasons may also\ninclude the decrease of tension due to more available memory and parallel\nthreads. However, starting with Apt, problems related to variable modes have\nnot yet been resolved -- neither entirely nor in an acceptable way. Research\ncontributions over the last decades show again and again that heap issues\nremain and remain complex and still important. A significant contribution was\nreached in 2016 by Peter O'Hearn, who accepted the G\\\"{o}del prize for his\nparallel approach on a spatial heap operation.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 19:18:07 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Haberland", "Ren\u00e9", ""]]}, {"id": "2104.02443", "submitter": "Ahmed Elnaggar", "authors": "Ahmed Elnaggar, Wei Ding, Llion Jones, Tom Gibbs, Tamas Feher,\n  Christoph Angerer, Silvia Severini, Florian Matthes and Burkhard Rost", "title": "CodeTrans: Towards Cracking the Language of Silicon's Code Through\n  Self-Supervised Deep Learning and High Performance Computing", "comments": "28 pages, 6 tables and 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.CL cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, a growing number of mature natural language processing\napplications make people's life more convenient. Such applications are built by\nsource code - the language in software engineering. However, the applications\nfor understanding source code language to ease the software engineering process\nare under-researched. Simultaneously, the transformer model, especially its\ncombination with transfer learning, has been proven to be a powerful technique\nfor natural language processing tasks. These breakthroughs point out a\npromising direction for process source code and crack software engineering\ntasks. This paper describes CodeTrans - an encoder-decoder transformer model\nfor tasks in the software engineering domain, that explores the effectiveness\nof encoder-decoder transformer models for six software engineering tasks,\nincluding thirteen sub-tasks. Moreover, we have investigated the effect of\ndifferent training strategies, including single-task learning, transfer\nlearning, multi-task learning, and multi-task learning with fine-tuning.\nCodeTrans outperforms the state-of-the-art models on all the tasks. To expedite\nfuture works in the software engineering domain, we have published our\npre-trained models of CodeTrans. https://github.com/agemagician/CodeTrans\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 11:57:12 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 06:51:32 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Elnaggar", "Ahmed", ""], ["Ding", "Wei", ""], ["Jones", "Llion", ""], ["Gibbs", "Tom", ""], ["Feher", "Tamas", ""], ["Angerer", "Christoph", ""], ["Severini", "Silvia", ""], ["Matthes", "Florian", ""], ["Rost", "Burkhard", ""]]}, {"id": "2104.02458", "submitter": "Marco Peressotti", "authors": "Saverio Giallorenzo and Fabrizio Montesi and Marco Peressotti and\n  Florian Rademacher and Sabine Sachweh", "title": "Jolie & LEMMA: Model-Driven Engineering and Programming Languages Meet\n  on Microservices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of microservices, Model-Driven Engineering has emerged as a\npowerful methodology for architectural design, and new programming languages\nhave introduced language abstractions to deal with microservice development\nmore effectively. In this article, we present the first preliminary\ninvestigation of how the two approaches can be married, taking the LEMMA\nframework and the Jolie programming language as respective representatives. By\ndeveloping a conceptual metamodel for Jolie, we elicit a strong link between\nthe two approaches, which shows that there is much to gain. We discuss a few\nlow-hanging fruits that come from our finding and present some interesting\nfuture directions that arise from our new viewpoint.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 12:34:22 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Giallorenzo", "Saverio", ""], ["Montesi", "Fabrizio", ""], ["Peressotti", "Marco", ""], ["Rademacher", "Florian", ""], ["Sachweh", "Sabine", ""]]}, {"id": "2104.02466", "submitter": "Caterina Urban", "authors": "Caterina Urban and Antoine Min\\'e", "title": "A Review of Formal Methods applied to Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We review state-of-the-art formal methods applied to the emerging field of\nthe verification of machine learning systems. Formal methods can provide\nrigorous correctness guarantees on hardware and software systems. Thanks to the\navailability of mature tools, their use is well established in the industry,\nand in particular to check safety-critical applications as they undergo a\nstringent certification process. As machine learning is becoming more popular,\nmachine-learned components are now considered for inclusion in critical\nsystems. This raises the question of their safety and their verification. Yet,\nestablished formal methods are limited to classic, i.e. non machine-learned\nsoftware. Applying formal methods to verify systems that include machine\nlearning has only been considered recently and poses novel challenges in\nsoundness, precision, and scalability.\n  We first recall established formal methods and their current use in an\nexemplar safety-critical field, avionic software, with a focus on abstract\ninterpretation based techniques as they provide a high level of scalability.\nThis provides a golden standard and sets high expectations for machine learning\nverification. We then provide a comprehensive and detailed review of the formal\nmethods developed so far for machine learning, highlighting their strengths and\nlimitations. The large majority of them verify trained neural networks and\nemploy either SMT, optimization, or abstract interpretation techniques. We also\ndiscuss methods for support vector machines and decision tree ensembles, as\nwell as methods targeting training and data preparation, which are critical but\noften neglected aspects of machine learning. Finally, we offer perspectives for\nfuture research directions towards the formal verification of machine learning\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 12:48:17 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 15:32:51 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Urban", "Caterina", ""], ["Min\u00e9", "Antoine", ""]]}, {"id": "2104.03142", "submitter": "Jos\\'e Moreira", "authors": "Jos\\'e E. Moreira, Kit Barton, Steven Battle, Peter Bergner, Ramon\n  Bertran, Puneeth Bhat, Pedro Caldeira, David Edelsohn, Gordon Fossum, Brad\n  Frey, Nemanja Ivanovic, Chip Kerchner, Vincent Lim, Shakti Kapoor, Tulio\n  Machado Filho, Silvia Melitta Mueller, Brett Olsson, Satish Sadasivam,\n  Baptiste Saleil, Bill Schmidt, Rajalakshmi Srinivasaraghavan, Shricharan\n  Srivatsan, Brian Thompto, Andreas Wagner, Nelson Wu", "title": "A matrix math facility for Power ISA(TM) processors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.LG cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power ISA(TM) Version 3.1 has introduced a new family of matrix math\ninstructions, collectively known as the Matrix-Multiply Assist (MMA) facility.\nThe instructions in this facility implement numerical linear algebra operations\non small matrices and are meant to accelerate computation-intensive kernels,\nsuch as matrix multiplication, convolution and discrete Fourier transform.\nThese instructions have led to a power- and area-efficient implementation of a\nhigh throughput math engine in the future POWER10 processor. Performance per\ncore is 4 times better, at constant frequency, than the previous generation\nPOWER9 processor. We also advocate the use of compiler built-ins as the\npreferred way of leveraging these instructions, which we illustrate through\ncase studies covering matrix multiplication and convolution.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 14:17:32 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Moreira", "Jos\u00e9 E.", ""], ["Barton", "Kit", ""], ["Battle", "Steven", ""], ["Bergner", "Peter", ""], ["Bertran", "Ramon", ""], ["Bhat", "Puneeth", ""], ["Caldeira", "Pedro", ""], ["Edelsohn", "David", ""], ["Fossum", "Gordon", ""], ["Frey", "Brad", ""], ["Ivanovic", "Nemanja", ""], ["Kerchner", "Chip", ""], ["Lim", "Vincent", ""], ["Kapoor", "Shakti", ""], ["Filho", "Tulio Machado", ""], ["Mueller", "Silvia Melitta", ""], ["Olsson", "Brett", ""], ["Sadasivam", "Satish", ""], ["Saleil", "Baptiste", ""], ["Schmidt", "Bill", ""], ["Srinivasaraghavan", "Rajalakshmi", ""], ["Srivatsan", "Shricharan", ""], ["Thompto", "Brian", ""], ["Wagner", "Andreas", ""], ["Wu", "Nelson", ""]]}, {"id": "2104.03388", "submitter": "Pengfei Su", "authors": "Bolun Li, Pengfei Su, Milind Chabbi, Shuyin Jiao, Xu Liu", "title": "DJXPerf: Identifying Memory Inefficiencies via Object-centric Profiling\n  for Java", "comments": "13 pages (including 2-page reference), 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Java is the \"go-to\" programming language choice for developing scalable\nenterprise cloud applications. In such systems, even a few percent CPU time\nsavings can offer a significant competitive advantage and cost saving. Although\nperformance tools abound in Java, those that focus on the data locality in the\nmemory hierarchy are rare.\n  In this paper, we present DJXPerf, a lightweight, object-centric memory\nprofiler for Java, which associates memory-hierarchy performance metrics (e.g.,\ncache/TLB misses) with Java objects. DJXPerf uses statistical sampling of\nhardware performance monitoring counters to attribute metrics to not only\nsource code locations but also Java objects. DJXPerf presents Java object\nallocation contexts combined with their usage contexts and presents them\nordered by the poor locality behaviors. DJXPerf's performance measurement,\nobject attribution, and presentation techniques guide optimizing object\nallocation, layout, and access patterns. DJXPerf incurs only ~8% runtime\noverhead and ~5% memory overhead on average, requiring no modifications to\nhardware, OS, Java virtual machine, or application source code, which makes it\nattractive to use in production. Guided by DJXPerf, we study and optimize a\nnumber of Java and Scala programs, including well-known benchmarks and\nreal-world applications, and demonstrate significant speedups.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 20:49:36 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Li", "Bolun", ""], ["Su", "Pengfei", ""], ["Chabbi", "Milind", ""], ["Jiao", "Shuyin", ""], ["Liu", "Xu", ""]]}, {"id": "2104.03425", "submitter": "Josep Silva", "authors": "Marisa Llorens, Javier Oliver, Josep Silva, Salvador Tamarit", "title": "Maximal and minimal dynamic Petri net slicing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Petri net slicing is a technique to reduce the size of a Petri net\nso that it can ease the analysis or understanding of the original Petri net.\n  Objective: Presenting two new Petri net slicing algorithms to isolate those\nplaces and transitions of a Petri net (the slice) that may contribute tokens to\none or more places given (the slicing criterion).\n  Method: The two algorithms proposed are formalized. The completeness of the\nfirst algorithm and the minimality of the second algorithm are formally proven.\nBoth algorithms together with other three state-of-the-art algorithms have been\nimplemented and integrated into a single tool so that we have been able to\ncarry out a fair empirical evaluation.\n  Results: Besides the two new Petri net slicing algorithms, a public, free,\nand open-source implementation of five algorithms is reported. The results of\nan empirical evaluation of the new algorithms and the slices that they produce\nare also presented.\n  Conclusions: The first algorithm collects all places and transitions that may\ninfluence (in any computation) the slicing criterion, while the second\nalgorithm collects a minimum set of places and transitions that may influence\n(in some computation) the slicing criterion. Therefore, the net computed by the\nfirst algorithm can reproduce any computation that contributes tokens to any\nplace of interest. In contrast, the second algorithm loses this possibility but\nit often produces a much more reduced subnet (which still can reproduce some\ncomputations that contribute tokens to some places of interest). The first\nalgorithm is proven complete, and the second one is proven minimal.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 23:01:26 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Llorens", "Marisa", ""], ["Oliver", "Javier", ""], ["Silva", "Josep", ""], ["Tamarit", "Salvador", ""]]}, {"id": "2104.03598", "submitter": "Di Wang", "authors": "Di Wang, Jan Hoffmann, Thomas Reps", "title": "Sound Probabilistic Inference via Guide Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic programming languages aim to describe and automate Bayesian\nmodeling and inference. Modern languages support programmable inference, which\nallows users to customize inference algorithms by incorporating guide programs\nto improve inference performance. For Bayesian inference to be sound, guide\nprograms must be compatible with model programs. One pervasive but challenging\ncondition for model-guide compatibility is absolute continuity, which requires\nthat the model and guide programs define probability distributions with the\nsame support.\n  This paper presents a new probabilistic programming language that guarantees\nabsolute continuity, and features general programming constructs, such as\nbranching and recursion. Model and guide programs are implemented as coroutines\nthat communicate with each other to synchronize the set of random variables\nthey sample during their execution. Novel guide types describe and enforce\ncommunication protocols between coroutines. If the model and guide are\nwell-typed using the same protocol, then they are guaranteed to enjoy absolute\ncontinuity. An efficient algorithm infers guide types from code so that users\ndo not have to specify the types. The new programming language is evaluated\nwith an implementation that includes the type-inference algorithm and a\nprototype compiler that targets Pyro. Experiments show that our language is\ncapable of expressing a variety of probabilistic models with nontrivial control\nflow and recursion, and that the coroutine-based computation does not introduce\nsignificant overhead in actual Bayesian inference.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 08:29:39 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Wang", "Di", ""], ["Hoffmann", "Jan", ""], ["Reps", "Thomas", ""]]}, {"id": "2104.03678", "submitter": "Kouji Matsui", "authors": "Kouji Matsui", "title": "A Proposal for an Interactive Shell Based on a Typed Lambda Calculus", "comments": "26 pages, 6 figures, It has been presented at Information Processing\n  Society of Japan Programming Study Group-132nd Programming Study Group", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents Favalon, a functional programming language built on the\npremise of a lambda calculus for use as an interactive shell replacement.\nFavalon seamlessly integrates with typed versions of existing libraries and\ncommands using type inference, flexible runtime type metadata, and the same\ntechniques employed by shells to link commands together. Much of Favalon's\nsyntax is customizable via user-defined functions, allowing it to be extended\nby anyone who is familiar with a command-line shell. Furthermore, Favalon's\ntype inference engine can be separated from its runtime library and easily\nrepurposed for other applications.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 10:46:28 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Matsui", "Kouji", ""]]}, {"id": "2104.04463", "submitter": "Yurii Kostyukov", "authors": "Yurii Kostyukov, Dmitry Mordvinov and Grigory Fedyukovich", "title": "Beyond the Elementary Representations of Program Invariants over\n  Algebraic Data Types", "comments": "Extended version of a paper appearing in PLDI'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-order logic is a natural way of expressing properties of computation.\nIt is traditionally used in various program logics for expressing the\ncorrectness properties and certificates. Although such representations are\nexpressive for some theories, they fail to express many interesting properties\nof algebraic data types (ADTs). In this paper, we explore three different\napproaches to represent program invariants of ADT-manipulating programs: tree\nautomata, and first-order formulas with or without size constraints. We compare\nthe expressive power of these representations and prove the negative\ndefinability of both first-order representations using the pumping lemmas. We\npresent an approach to automatically infer program invariants of\nADT-manipulating programs by a reduction to a finite model finder. The\nimplementation called RInGen has been evaluated against state-of-the-art\ninvariant synthesizers and has been experimentally shown to be competitive. In\nparticular, program invariants represented by automata are capable of\nexpressing more complex properties of computation and their automatic\nconstruction is often less expensive.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 16:26:00 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 10:14:25 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Kostyukov", "Yurii", ""], ["Mordvinov", "Dmitry", ""], ["Fedyukovich", "Grigory", ""]]}, {"id": "2104.04512", "submitter": "Caleb Stanford", "authors": "Konstantinos Kallas, Filip Niksic, Caleb Stanford, Rajeev Alur", "title": "Stream Processing With Dependency-Guided Synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time data processing applications with low latency requirements have led\nto the increasing popularity of stream processing systems. While such systems\noffer convenient APIs that can be used to achieve data parallelism\nautomatically, they offer limited support for computations that require\nsynchronization between parallel nodes. In this paper, we propose\n*dependency-guided synchronization (DGS)*, an alternative programming model and\nstream processing API for stateful streaming computations with complex\nsynchronization requirements. In the proposed model, the input is viewed as\npartially ordered, and the program consists of a set of parallelization\nconstructs which are applied to decompose the partial order and process events\nindependently. Our API maps to an execution model called *synchronization\nplans* which supports synchronization between parallel nodes. Our evaluation\nshows that APIs offered by two widely used systems -- Flink and Timely Dataflow\n-- cannot suitably expose parallelism in some representative applications. In\ncontrast, DGS enables implementations with scalable performance, the resulting\nsynchronization plans offer throughput improvements when implemented manually\nin existing systems, and the programming overhead is small compared to writing\nsequential code.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 17:50:53 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 05:12:14 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kallas", "Konstantinos", ""], ["Niksic", "Filip", ""], ["Stanford", "Caleb", ""], ["Alur", "Rajeev", ""]]}, {"id": "2104.04576", "submitter": "Max Sponner", "authors": "Max Sponner, Bernd Waschneck and Akash Kumar", "title": "Compiler Toolchains for Deep Learning Workloads on Embedded Platforms", "comments": "tinyML 2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the usage of deep learning becomes increasingly popular in mobile and\nembedded solutions, it is necessary to convert the framework-specific network\nrepresentations into executable code for these embedded platforms. This paper\nconsists of two parts: The first section is made up of a survey and benchmark\nof the available open source deep learning compiler toolchains, which focus on\nthe capabilities and performance of the individual solutions in regard to\ntargeting embedded devices and microcontrollers that are combined with a\ndedicated accelerator in a heterogeneous fashion. The second part explores the\nimplementation and evaluation of a compilation flow for such a heterogeneous\ndevice and reuses one of the existing toolchains to demonstrate the necessary\nsteps for hardware developers that plan to build a software flow for their own\nhardware.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 13:54:25 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Sponner", "Max", ""], ["Waschneck", "Bernd", ""], ["Kumar", "Akash", ""]]}, {"id": "2104.04616", "submitter": "Milijana Surbatovich", "authors": "Milijana Surbatovich, Limin Jia, Brandon Lucia", "title": "Automatically Enforcing Fresh and Consistent Inputs in Intermittent\n  Systems", "comments": "Updates for camera-ready, add DOI", "journal-ref": null, "doi": "10.1145/3453483.3454081", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Intermittently powered energy-harvesting devices enable new applications in\ninaccessible environments. Program executions must be robust to unpredictable\npower failures, introducing new challenges in programmability and correctness.\nOne hard problem is that input operations have implicit constraints, embedded\nin the behavior of continuously powered executions, on when input values can be\ncollected and used. This paper aims to develop a formal framework for enforcing\nthese constraints. We identify two key properties -- freshness (i.e., uses of\ninputs must satisfy the same time constraints as in continuous executions) and\ntemporal consistency (i.e., the collection of a set of inputs must satisfy the\nsame time constraints as in continuous executions). We formalize these\nproperties and show that they can be enforced using atomic regions. We develop\nOcelot, an LLVM-based analysis and transformation tool targeting Rust, to\nenforce these properties automatically. Ocelot provides the programmer with\nannotations to express these constraints and infers atomic region placement in\na program to satisfy them. We then formalize Ocelot's design and show that\nOcelot generates correct programs with little performance cost or code changes.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 21:39:52 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 23:01:28 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Surbatovich", "Milijana", ""], ["Jia", "Limin", ""], ["Lucia", "Brandon", ""]]}, {"id": "2104.04706", "submitter": "Amir M. Mir", "authors": "Amir M. Mir, Evaldas Latoskinas, Georgios Gousios", "title": "ManyTypes4Py: A Benchmark Python Dataset for Machine Learning-based Type\n  Inference", "comments": "MSR'21, Data Showcase To download the dataset, check out its GitHub\n  repo: https://github.com/saltudelft/many-types-4-py-dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we present ManyTypes4Py, a large Python dataset for machine\nlearning (ML)-based type inference. The dataset contains a total of 5,382\nPython projects with more than 869K type annotations. Duplicate source code\nfiles were removed to eliminate the negative effect of the duplication bias. To\nfacilitate training and evaluation of ML models, the dataset was split into\ntraining, validation and test sets by files. To extract type information from\nabstract syntax trees (ASTs), a lightweight static analyzer pipeline is\ndeveloped and accompanied with the dataset. Using this pipeline, the collected\nPython projects were analyzed and the results of the AST analysis were stored\nin JSON-formatted files. The ManyTypes4Py dataset is shared on zenodo and its\ntools are publicly available on GitHub.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 08:10:06 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Mir", "Amir M.", ""], ["Latoskinas", "Evaldas", ""], ["Gousios", "Georgios", ""]]}, {"id": "2104.04741", "submitter": "Nick Brown", "authors": "Nick Brown", "title": "Application specific dataflow machine construction for programming FPGAs\n  via Lucent", "comments": "Accepted at the LATTE (Languages, Tools, and Techniques for\n  Accelerator Design) ASPLOS workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Field Programmable Gate Arrays (FPGAs) have the potential to accelerate\nspecific HPC codes. However even with the advent of High Level Synthesis (HLS),\nwhich enables FPGA programmers to write code in C or C++, programming such\ndevices still requires considerable expertise. Much of this is due to the fact\nthat these architectures are founded on dataflow rather than the Von Neumann\nabstraction of CPUs or GPUs. Thus programming FPGAs via imperative languages is\nnot optimal and can result in very significant performance differences between\nthe first and final versions of algorithms on dataflow architectures with the\nsteps in between often not obvious and requiring considerable expertise.\n  In this position paper we argue that languages built upon dataflow principals\nshould be exploited to enable fast by construction codes for FPGAs, and this is\nakin to the programmer adopting the abstraction of developing a bespoke\ndataflow machine specialised for their application. It is our belief that much\ncan be learnt from the generation of dataflow languages that gained popularity\nin the 1970s and 1980s around programming general purpose dataflow machines,\nand we introduce Lucent which is a modern derivative of Lucid, and used as a\nvehicle to explore this hypothesis. The idea behind Lucent is to provide high\nprogrammer productivity and performance for FPGAs by giving developers the most\nsuitable language level abstractions. The focus of Lucent is very much to\nsupport the acceleration of HPC kernels, rather than the embedded electronics\nand circuit level, and we provide a brief overview of the language driven by\nexamples.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 11:32:54 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Brown", "Nick", ""]]}, {"id": "2104.04955", "submitter": "R. Baghdadi", "authors": "Riyadh Baghdadi, Massinissa Merouani, Mohamed-Hicham Leghettas, Kamel\n  Abdous, Taha Arbaoui, Karima Benatchba, Saman Amarasinghe", "title": "A Deep Learning Based Cost Model for Automatic Code Optimization", "comments": null, "journal-ref": "Proceedings of the 4th MLSys Conference, San Jose, CA, USA, 2021", "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enabling compilers to automatically optimize code has been a longstanding\ngoal for the compiler community. Efficiently solving this problem requires\nusing precise cost models. These models predict whether applying a sequence of\ncode transformations reduces the execution time of the program. Building an\nanalytical cost model to do so is hard in modern x86 architectures due to the\ncomplexity of the microarchitecture. In this paper, we present a novel deep\nlearning based cost model for automatic code optimization. This model was\nintegrated in a search method and implemented in the Tiramisu compiler to\nselect the best code transformations. The input of the proposed model is a set\nof simple features representing the unoptimized code and a sequence of code\ntransformations. The model predicts the speedup expected when the code\ntransformations are applied. Unlike previous models, the proposed one works on\nfull programs and does not rely on any heavy feature engineering. The proposed\nmodel has only 16% of mean absolute percentage error in predicting speedups on\nfull programs. The proposed model enables Tiramisu to automatically find code\ntransformations that match or are better than state-of-the-art compilers\nwithout requiring the same level of heavy feature engineering required by those\ncompilers.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 08:32:42 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Baghdadi", "Riyadh", ""], ["Merouani", "Massinissa", ""], ["Leghettas", "Mohamed-Hicham", ""], ["Abdous", "Kamel", ""], ["Arbaoui", "Taha", ""], ["Benatchba", "Karima", ""], ["Amarasinghe", "Saman", ""]]}, {"id": "2104.04990", "submitter": "Raven Beutner", "authors": "Raven Beutner, Luke Ong", "title": "On Probabilistic Termination of Functional Programs with Continuous\n  Distributions", "comments": "PLDI 2021", "journal-ref": null, "doi": "10.1145/3453483.3454111", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We study termination of higher-order probabilistic functional programs with\nrecursion, stochastic conditioning and sampling from continuous distributions.\n  Reasoning about the termination probability of programs with continuous\ndistributions is hard, because the enumeration of terminating executions cannot\nprovide any non-trivial bounds. We present a new operational semantics based on\ntraces of intervals, which is sound and complete with respect to the standard\nsampling-based semantics, in which (countable) enumeration can provide\narbitrarily tight lower bounds. Consequently we obtain the first proof that\ndeciding almost-sure termination (AST) for programs with continuous\ndistributions is $\\Pi^0_2$-complete. We also provide a compositional\nrepresentation of our semantics in terms of an intersection type system.\n  In the second part, we present a method of proving AST for non-affine\nprograms, i.e., recursive programs that can, during the evaluation of the\nrecursive body, make multiple recursive calls (of a first-order function) from\ndistinct call sites. Unlike in a deterministic language, the number of\nrecursion call sites has direct consequences on the termination probability.\nOur framework supports a proof system that can verify AST for programs that are\nwell beyond the scope of existing methods.\n  We have constructed prototype implementations of our method of computing\nlower bounds of termination probability, and AST verification.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 10:57:17 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Beutner", "Raven", ""], ["Ong", "Luke", ""]]}, {"id": "2104.05310", "submitter": "Chen Wu", "authors": "Dawn Drain, Changran Hu, Chen Wu, Mikhail Breslav, Neel Sundaresan", "title": "Generating Code with the Help of Retrieved Template Functions and Stack\n  Overflow Answers", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We approach the important challenge of code autocompletion as an open-domain\ntask, in which a sequence-to-sequence code generator model is enhanced with the\nability to attend to reference code snippets supplied by a semantic code search\nengine. In this work, we present a novel framework to precisely retrieve\ntemplate functions as well as intent-snippet pairs and effectively train such a\nretrieval-guided code generator. To demonstrate the effectiveness of our model\ndesigns, we perform extensive experiments with CodeSearchNet which contains\ntemplate functions and CoNaLa which contains Stack Overflow intent-snippet\npairs. We also investigate different retrieval models, including Elasticsearch,\nDPR, and our fusion representation search model, which currently holds the\nnumber one spot on the CodeSearchNet leaderboard. We observe improvements by\nleveraging multiple database elements and further gain from retrieving diverse\ndata points by using Maximal Marginal Relevance. Overall, we see a 4%\nimprovement to cross-entropy loss, a 15% improvement to edit distance, and a\n44% improvement to BLEU score when retrieving template functions. We see\nsubtler improvements of 2%, 11%, and 6% respectively when retrieving Stack\nOverflow intent-snippet pairs. We also create a novel Stack Overflow-Function\nAlignment dataset, which consists of 150K tuples of functions and Stack\nOverflow intent-snippet pairs that are of help in writing the associated\nfunction, of which 1.7K are manually curated.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 09:37:32 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 02:46:54 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Drain", "Dawn", ""], ["Hu", "Changran", ""], ["Wu", "Chen", ""], ["Breslav", "Mikhail", ""], ["Sundaresan", "Neel", ""]]}, {"id": "2104.05348", "submitter": "Dmitriy Traytel", "authors": "Basil F\\\"urer, Andreas Lochbihler, Joshua Schneider, Dmitriy Traytel", "title": "Quotients of Bounded Natural Functors", "comments": "Extended version of homonymous IJCAR 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The functorial structure of type constructors is the foundation for many\ndefinition and proof principles in higher-order logic (HOL). For example,\ninductive and coinductive datatypes can be built modularly from bounded natural\nfunctors (BNFs), a class of well-behaved type constructors. Composition,\nfixpoints, and, under certain conditions, subtypes are known to preserve the\nBNF structure. In this article, we tackle the preservation question for\nquotients, the last important principle for introducing new types in HOL. We\nidentify sufficient conditions under which a quotient inherits the BNF\nstructure from its underlying type. Surprisingly, lifting the structure in the\nobvious manner fails for some quotients, a problem that also affects the\nquotients of polynomial functors used in the Lean proof assistant. We provide a\nstrictly more general lifting scheme that supports such problematic quotients.\nWe extend the Isabelle/HOL proof assistant with a command that automates the\nregistration of a quotient type as a BNF, reducing the proof burden on the user\nfrom the full set of BNF axioms to our inheritance conditions. We demonstrate\nthe command's usefulness through several case studies.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 10:56:20 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["F\u00fcrer", "Basil", ""], ["Lochbihler", "Andreas", ""], ["Schneider", "Joshua", ""], ["Traytel", "Dmitriy", ""]]}, {"id": "2104.05372", "submitter": "Adam Paszke", "authors": "Adam Paszke, Daniel Johnson, David Duvenaud, Dimitrios Vytiniotis,\n  Alexey Radul, Matthew Johnson, Jonathan Ragan-Kelley and Dougal Maclaurin", "title": "Getting to the Point. Index Sets and Parallelism-Preserving Autodiff for\n  Pointful Array Programming", "comments": "31 pages with appendix, 11 figures. A conference submission is still\n  under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a novel programming language design that attempts to combine the\nclarity and safety of high-level functional languages with the efficiency and\nparallelism of low-level numerical languages. We treat arrays as\neagerly-memoized functions on typed index sets, allowing abstract function\nmanipulations, such as currying, to work on arrays. In contrast to composing\nprimitive bulk-array operations, we argue for an explicit nested indexing style\nthat mirrors application of functions to arguments. We also introduce a\nfine-grained typed effects system which affords concise and\nautomatically-parallelized in-place updates. Specifically, an associative\naccumulation effect allows reverse-mode automatic differentiation of in-place\nupdates in a way that preserves parallelism. Empirically, we benchmark against\nthe Futhark array programming language, and demonstrate that aggressive\ninlining and type-driven compilation allows array programs to be written in an\nexpressive, \"pointful\" style with little performance penalty.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 11:46:30 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Paszke", "Adam", ""], ["Johnson", "Daniel", ""], ["Duvenaud", "David", ""], ["Vytiniotis", "Dimitrios", ""], ["Radul", "Alexey", ""], ["Johnson", "Matthew", ""], ["Ragan-Kelley", "Jonathan", ""], ["Maclaurin", "Dougal", ""]]}, {"id": "2104.05558", "submitter": "Francesco Dagnino", "authors": "Francesco Dagnino", "title": "A meta-theory for big-step semantics", "comments": "arXiv admin note: text overlap with arXiv:2002.08738", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is well-known that big-step semantics is not able to distinguish stuck and\nnon-terminating computations. This is a strong limitation as it makes very\ndifficult to reason about properties involving infinite computations, such as\ntype soundness, which cannot even be expressed. To face this problem, we\ndevelop a systematic study of big-step semantics: we introduce an abstract\ndefinition of what a big-step semantics is, we formalise the evaluation\nalgorithm implicitly associated with any big-step semantics and we identify\ncomputations with executions of such an algorithm, thus recovering the\ndistinction between stuckness an non-termination. Then, we define constructions\nyielding an extended version of a given arbitrary big-step semantics, where\nsuch a difference is made explicit. Building on such constructions, we describe\na general proof technique to show that a predicate is sound, that is, prevents\nstuck computation, with respect to a big-step semantics. The extended semantics\nare exploited in the meta-theory, notably they are necessary to show that the\nproof technique works. However, they remain transparent when using the proof\ntechnique, since it consists in checking three conditions on the original rules\nonly. We illustrate the technique by several examples, showing that it is\napplicable also in cases where subject reduction does not hold, hence the\nstandard technique for small-step semantics cannot be used.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 15:24:24 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 21:22:20 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dagnino", "Francesco", ""]]}, {"id": "2104.05573", "submitter": "Sanket Tavarageri", "authors": "Sanket Tavarageri, Gagandeep Goyal, Sasikanth Avancha, Bharat Kaul,\n  Ramakrishna Upadrasta", "title": "AI Powered Compiler Techniques for DL Code Optimization", "comments": "arXiv admin note: text overlap with arXiv:2006.02230,\n  arXiv:2002.02145", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating high performance implementations of deep learning primitives on CPUs\nis a challenging task. Multiple considerations including multi-level cache\nhierarchy, and wide SIMD units of CPU platforms influence the choice of program\ntransformations to apply for performance optimization. In this paper, we\npresent machine learning powered compiler techniques to optimize loop nests. We\ntake a two-pronged approach to code optimization: We first apply high level\noptimizations to optimize the code to take optimal advantage of the cache\nmemories. Then, we perform low level, target-specific optimizations to\neffectively vectorize the code to run well on the SIMD units of the machine.\nFor high level optimizations, we use polyhedral compilation techniques and deep\nlearning approaches. For low level optimization, we use a target specific code\ngenerator that generates code using vector intrinsics and Reinforcement\nLearning (RL) techniques to find the optimal parameters for the code generator.\nWe perform experimental evaluation of the developed techniques on various\nmatrix multiplications that occur in popular deep learning workloads. The\nexperimental results show that the compiler techniques presented in the paper\nachieve 7.6X and 8.2X speed-ups over a baseline for sequential and parallel\nruns respectively.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 15:42:47 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Tavarageri", "Sanket", ""], ["Goyal", "Gagandeep", ""], ["Avancha", "Sasikanth", ""], ["Kaul", "Bharat", ""], ["Upadrasta", "Ramakrishna", ""]]}, {"id": "2104.06349", "submitter": "Runzhou Tao", "authors": "Runzhou Tao, Yunong Shi, Jianan Yao, John Hui, Frederic T. Chong,\n  Ronghui Gu", "title": "Gleipnir: Toward Practical Error Analysis for Quantum Programs (Extended\n  Version)", "comments": "typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Practical error analysis is essential for the design, optimization, and\nevaluation of Noisy Intermediate-Scale Quantum(NISQ) computing. However,\nbounding errors in quantum programs is a grand challenge, because the effects\nof quantum errors depend on exponentially large quantum states. In this work,\nwe present Gleipnir, a novel methodology toward practically computing verified\nerror bounds in quantum programs. Gleipnir introduces the\n$(\\hat\\rho,\\delta)$-diamond norm, an error metric constrained by a quantum\npredicate consisting of the approximate state $\\hat\\rho$ and its distance\n$\\delta$ to the ideal state $\\rho$. This predicate $(\\hat\\rho,\\delta)$ can be\ncomputed adaptively using tensor networks based on the Matrix Product States.\nGleipnir features a lightweight logic for reasoning about error bounds in noisy\nquantum programs, based on the $(\\hat\\rho,\\delta)$-diamond norm metric. Our\nexperimental results show that Gleipnir is able to efficiently generate tight\nerror bounds for real-world quantum programs with 10 to 100 qubits, and can be\nused to evaluate the error mitigation performance of quantum compiler\ntransformations.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 16:45:57 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 18:29:00 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Tao", "Runzhou", ""], ["Shi", "Yunong", ""], ["Yao", "Jianan", ""], ["Hui", "John", ""], ["Chong", "Frederic T.", ""], ["Gu", "Ronghui", ""]]}, {"id": "2104.07162", "submitter": "Qiaochu Chen", "authors": "Qiaochu Chen, Aaron Lamoreaux, Xinyu Wang, Greg Durrett, Osbert\n  Bastani, Isil Dillig", "title": "Web Question Answering with Neurosymbolic Program Synthesis", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a new technique based on program synthesis for\nextracting information from webpages. Given a natural language query and a few\nlabeled webpages, our method synthesizes a program that can be used to extract\nsimilar types of information from other unlabeled webpages. To handle websites\nwith diverse structure, our approach employs a neurosymbolic DSL that\nincorporates both neural NLP models as well as standard language constructs for\ntree navigation and string manipulation. We also propose an optimal synthesis\nalgorithm that generates all DSL programs that achieve optimal F1 score on the\ntraining examples. Our synthesis technique is compositional, prunes the search\nspace by exploiting a monotonicity property of the DSL, and uses transductive\nlearning to select programs with good generalization power. We have implemented\nthese ideas in a new tool called WebQA and evaluate it on 25 different tasks\nacross multiple domains. Our experiments show that WebQA significantly\noutperforms existing tools such as state-of-the-art question answering models\nand wrapper induction systems.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 23:23:33 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Chen", "Qiaochu", ""], ["Lamoreaux", "Aaron", ""], ["Wang", "Xinyu", ""], ["Durrett", "Greg", ""], ["Bastani", "Osbert", ""], ["Dillig", "Isil", ""]]}, {"id": "2104.07460", "submitter": "Guixin Ye", "authors": "Guixin Ye, Zhanyong Tang, Shin Hwei Tan, Songfang Huang, Dingyi Fang,\n  Xiaoyang Sun, Lizhong Bian, Haibo Wang, Zheng Wang", "title": "Automated Conformance Testing for JavaScript Engines via Deep Compiler\n  Fuzzing", "comments": "PLDI 2021", "journal-ref": null, "doi": "10.1145/3453483.3454054", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  JavaScript (JS) is a popular, platform-independent programming language. To\nensure the interoperability of JS programs across different platforms, the\nimplementation of a JS engine should conform to the ECMAScript standard.\nHowever, doing so is challenging as there are many subtle definitions of API\nbehaviors, and the definitions keep evolving.\n  We present COMFORT, a new compiler fuzzing framework for detecting JS engine\nbugs and behaviors that deviate from the ECMAScript standard. COMFORT leverages\nthe recent advance in deep learning-based language models to automatically\ngenerate JS test code. As a departure from prior fuzzers, COMFORT utilizes the\nwell-structured ECMAScript specifications to automatically generate test data\nalong with the test programs to expose bugs that could be overlooked by the\ndevelopers or manually written test cases. COMFORT then applies differential\ntesting methodologies on the generated test cases to expose standard\nconformance bugs. We apply COMFORT to ten mainstream JS engines. In 200 hours\nof automated concurrent testing runs, we discover bugs in all tested JS\nengines. We had identified 158 unique JS engine bugs, of which 129 have been\nverified, and 115 have already been fixed by the developers. Furthermore, 21 of\nthe Comfort-generated test cases have been added to Test262, the official\nECMAScript conformance test suite.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 13:47:42 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Ye", "Guixin", ""], ["Tang", "Zhanyong", ""], ["Tan", "Shin Hwei", ""], ["Huang", "Songfang", ""], ["Fang", "Dingyi", ""], ["Sun", "Xiaoyang", ""], ["Bian", "Lizhong", ""], ["Wang", "Haibo", ""], ["Wang", "Zheng", ""]]}, {"id": "2104.07896", "submitter": "Chen Wu", "authors": "Dawn Drain, Chen Wu, Alexey Svyatkovskiy, Neel Sundaresan", "title": "Generating Bug-Fixes Using Pretrained Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting and fixing bugs are two of the most important yet frustrating parts\nof the software development cycle. Existing bug detection tools are based\nmainly on static analyzers, which rely on mathematical logic and symbolic\nreasoning about the program execution to detect common types of bugs. Fixing\nbugs is typically left out to the developer. In this work we introduce\nDeepDebug: a data-driven program repair approach which learns to detect and fix\nbugs in Java methods mined from real-world GitHub repositories. We frame\nbug-patching as a sequence-to-sequence learning task consisting of two steps:\n(i) denoising pretraining, and (ii) supervised finetuning on the target\ntranslation task. We show that pretraining on source code programs improves the\nnumber of patches found by 33% as compared to supervised training from scratch,\nwhile domain-adaptive pretraining from natural language to code further\nimproves the accuracy by another 32%. We refine the standard accuracy\nevaluation metric into non-deletion and deletion-only fixes, and show that our\nbest model generates 75% more non-deletion fixes than the previous state of the\nart. In contrast to prior work, we attain our best results when generating raw\ncode, as opposed to working with abstracted code that tends to only benefit\nsmaller capacity models. Finally, we observe a subtle improvement from adding\nsyntax embeddings along with the standard positional embeddings, as well as\nwith adding an auxiliary task to predict each token's syntactic class. Despite\nfocusing on Java, our approach is language agnostic, requiring only a\ngeneral-purpose parser such as tree-sitter.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 05:27:04 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 23:20:06 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Drain", "Dawn", ""], ["Wu", "Chen", ""], ["Svyatkovskiy", "Alexey", ""], ["Sundaresan", "Neel", ""]]}, {"id": "2104.08130", "submitter": "Maximiliano Cristia", "authors": "Maximiliano Cristi\\'a and Gianfranco Rossi", "title": "$\\{log\\}$: Set Formulas as Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  $\\{log\\}$ is a programming language at the intersection of Constraint Logic\nProgramming, set programming and declarative programming. But $\\{log\\}$ is also\na satisfiability solver for a theory of finite sets and finite binary\nrelations. With $\\{log\\}$ programmers can write abstract programs using all the\npower of set theory and binary relations. These programs are not very efficient\nbut they are very close to specifications. Then, their correctness is more\nevident. Furthermore, $\\{log\\}$ programs are also set formulas. Hence,\nprogrammers can use $\\{log\\}$ again to automatically prove their programs\nverify non trivial properties. In this paper we show this development\nmethodology by means of several examples.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 14:25:06 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Cristi\u00e1", "Maximiliano", ""], ["Rossi", "Gianfranco", ""]]}, {"id": "2104.08366", "submitter": "Marcos Viera", "authors": "Mauricio Cassola, Agust\\'in Talagorria, Alberto Pardo, Marcos Viera", "title": "A Gradual Type System for Elixir", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Elixir is a functional programming language with dynamic typing. We propose a\ngradual type system that makes it possible to perform type-checking on a\nsignificant fragment of the language. An important feature of the type system\nis that it does not require any syntactic change to Elixir. Type information is\nprovided by means of function signatures which are declared in terms of Elixir\ntypespec directives. The proposed type system is based on subtyping and is\nbackward compatible, as it allows the presence of untyped code fragments. We\nhave implemented a prototype of the type-checker in Elixir itself.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 20:59:42 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Cassola", "Mauricio", ""], ["Talagorria", "Agust\u00edn", ""], ["Pardo", "Alberto", ""], ["Viera", "Marcos", ""]]}, {"id": "2104.08638", "submitter": "Priyanka Bose", "authors": "Priyanka Bose, Dipanjan Das, Yanju Chen, Yu Feng, Christopher Kruegel,\n  Giovanni Vigna", "title": "SAILFISH: Vetting Smart Contract State-Inconsistency Bugs in Seconds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents SAILFISH, a scalable system for automatically finding\nstate-inconsistency bugs in smart contracts. To make the analysis tractable, we\nintroduce a hybrid approach that includes (i) a light-weight exploration phase\nthat dramatically reduces the number of instructions to analyze, and (ii) a\nprecise refinement phase based on symbolic evaluation guided by our novel\nvalue-summary analysis, which generates extra constraints to over-approximate\nthe side effects of whole-program execution, thereby ensuring the precision of\nthe symbolic evaluation. We developed a prototype of SAILFISH and evaluated its\nability to detect two state-inconsistency flaws, viz., reentrancy and\ntransaction order dependence (TOD) in Ethereum smart contracts. Further, we\npresent detection rules for other kinds of smart contract flaws that SAILFISH\ncan be extended to detect.\n  Our experiments demonstrate the efficiency of our hybrid approach as well as\nthe benefit of the value summary analysis. In particular, we show that S\nSAILFISH outperforms five state-of-the-art smart contract analyzers (SECURITY,\nMYTHRIL, OYENTE, SEREUM and VANDAL ) in terms of performance, and precision. In\ntotal, SAILFISH discovered 47 previously unknown vulnerable smart contracts out\nof 89,853 smart contracts from ETHERSCAN .\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 20:21:07 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Bose", "Priyanka", ""], ["Das", "Dipanjan", ""], ["Chen", "Yanju", ""], ["Feng", "Yu", ""], ["Kruegel", "Christopher", ""], ["Vigna", "Giovanni", ""]]}, {"id": "2104.09583", "submitter": "Raghav Malik", "authors": "Raghav Malik, Vidush Singhal, Benjamin Gottfried, Milind Kulkarni", "title": "Vectorized Secure Evaluation of Decision Forests", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": "10.1145/3453483.3454094", "report-no": null, "categories": "cs.CR cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As the demand for machine learning-based inference increases in tandem with\nconcerns about privacy, there is a growing recognition of the need for secure\nmachine learning, in which secret models can be used to classify private data\nwithout the model or data being leaked. Fully Homomorphic Encryption (FHE)\nallows arbitrary computation to be done over encrypted data, providing an\nattractive approach to providing such secure inference. While such computation\nis often orders of magnitude slower than its plaintext counterpart, the ability\nof FHE cryptosystems to do \\emph{ciphertext packing} -- that is, encrypting an\nentire vector of plaintexts such that operations are evaluated elementwise on\nthe vector -- helps ameliorate this overhead, effectively creating a SIMD\narchitecture where computation can be vectorized for more efficient evaluation.\nMost recent research in this area has targeted regular, easily vectorizable\nneural network models. Applying similar techniques to irregular ML models such\nas decision forests remains unexplored, due to their complex, hard-to-vectorize\nstructures. In this paper we present COPSE, the first system that exploits\nciphertext packing to perform decision-forest inference. COPSE consists of a\nstaging compiler that automatically restructures and compiles decision forest\nmodels down to a new set of vectorizable primitives for secure inference. We\nfind that COPSE's compiled models outperform the state of the art across a\nrange of decision forest models, often by more than an order of magnitude,\nwhile still scaling well.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 19:32:47 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Malik", "Raghav", ""], ["Singhal", "Vidush", ""], ["Gottfried", "Benjamin", ""], ["Kulkarni", "Milind", ""]]}, {"id": "2104.09669", "submitter": "Thurston Dang", "authors": "Thurston H. Y. Dang, Jose P. Cambronero, Martin C. Rinard", "title": "Inferring Drop-in Binary Parsers from Program Executions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present BIEBER (Byte-IdEntical Binary parsER), the first system to model\nand regenerate a full working parser from instrumented program executions. To\nachieve this, BIEBER exploits the regularity (e.g., header fields and\narray-like data structures) that is commonly found in file formats. Key\ngeneralization steps derive strided loops that parse input file data and\nrewrite concrete loop bounds with expressions over input file header bytes.\nThese steps enable BIEBER to generalize parses of specific input files to\nobtain parsers that operate over input files of arbitrary size. BIEBER also\nincrementally and efficiently infers a decision tree that reads file header\nbytes to route input files of different types to inferred parsers of the\nappropriate type. The inferred parsers and decision tree are expressed in an\nIR; separate backends (C and Perl in our prototype) can translate the IR into\nthe same language as the original program (for a safer drop-in replacement), or\nautomatically port to a different language. An empirical evaluation shows that\nBIEBER can successfully regenerate parsers for six file formats (waveform audio\n[1654 files], MT76x0 .BIN firmware containers [5 files], OS/2 1.x bitmap images\n[9 files], Windows 3.x bitmaps [9971 files], Windows 95/NT4 bitmaps [133\nfiles], and Windows 98/2000 bitmaps [859 files]), correctly parsing 100% (>=\n99.98% when using standard held-out cross-validation) of the corresponding\ncorpora. The regenerated parsers contain automatically inserted safety checks\nthat eliminate common classes of errors such as memory errors. We find that\nBIEBER can help reverse-engineer file formats, because it automatically\nidentifies predicates for the decision tree that relate to key semantics of the\nfile format. We also discuss how BIEBER helped us detect and fix two new bugs\nin stb_image as well as independently rediscover and fix a known bug.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 22:18:57 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Dang", "Thurston H. Y.", ""], ["Cambronero", "Jose P.", ""], ["Rinard", "Martin C.", ""]]}, {"id": "2104.09948", "submitter": "Philip Zweihoff", "authors": "Philip Zweihoff and Bernhard Steffen", "title": "A Generative Approach for User-Centered, Collaborative, Domain-Specific\n  Modeling Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of low- and no-code modeling tools is today an established way in\npractice to give non-programmers an opportunity to master their digital\nchallenges independently, using the means of model-driven software development.\nHowever, the existing tools are limited to a very small number of different\ndomains such as mobile app development, which can be attributed to the enormous\ndemands that a user has on such a tool today. These demands exceed the mere use\nof a modeling environment as such and require cross-cutting concerns such as:\neasy access, direct usability and simultaneous collaboration, which result in\nadditional effort in the realization of such tools. Our solution is based on\nthe idea to support and simplify the creation of new domain-specific holistic\ntools by generating it entirely based on a declarative specification with a\ndomain-specific meta-tool. The meta-tool Pyro demonstrated and analyzed here\nfocuses on graph-based graphical languages to fully generate a complete,\ndirectly executable tool starting from a meta-model in order to meet all\ncross-cutting requirements.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 13:19:28 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zweihoff", "Philip", ""], ["Steffen", "Bernhard", ""]]}, {"id": "2104.10042", "submitter": "Vasiliy Gurianov", "authors": "Vasyliy I. Gurianov", "title": "A Brief Overview of the UML Scientific Profile", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article provides a brief overview of the UML SP (UML Scientific\nProfile). It is an Object-Oriented Simulation Language and may find usage in\nOOS and ABS. UML SP allows for the application of Unified Process methodology\nfor the development of simulation software. It supports the first three stages\nof simulation: (1) definition of simulation objectives, definition of\nrequirements for simulation software, (2) conceptual modelling, (3) formal\ndescription and partially, (4) programming. UML SPs positioning is that of a\nlanguage for scientific simulation. It can easily be implemented in UML editors\nthat support UML profiles. The project is available as open source on the\nGitHub platform.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 18:10:22 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Gurianov", "Vasyliy I.", ""]]}, {"id": "2104.10267", "submitter": "Riccardo Treglia", "authors": "Claudia Faggian, Giulio Guerrieri, Ugo de'Liguoro, Riccardo Treglia", "title": "On reduction and normalization in the computational core", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the reduction in a lambda-calculus derived from Moggi's\ncomputational one, that we call the computational core. The reduction relation\nconsists of rules obtained by orienting three monadic laws. Such laws, in\nparticular associativity and identity, introduce intricacies in the operational\nanalysis. We investigate the central notions of returning a value versus having\na normal form, and address the question of normalizing strategies. Our analysis\nrelies on factorization results.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 22:12:02 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 10:29:44 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Faggian", "Claudia", ""], ["Guerrieri", "Giulio", ""], ["de'Liguoro", "Ugo", ""], ["Treglia", "Riccardo", ""]]}, {"id": "2104.10274", "submitter": "Marco Eilers", "authors": "Christian Br\\\"am and Marco Eilers and Peter M\\\"uller and Robin Sierra\n  and Alexander J. Summers", "title": "Modular Verification of Collaborating Smart Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart contracts are programs that execute inside blockchains such as Ethereum\nto manipulate digital assets. Since bugs in smart contracts may lead to\nsubstantial financial losses, there is considerable interest in formally\nproving their correctness. However, the specification and verification of smart\ncontracts faces challenges that do not arise in other application domains.\nSmart contracts frequently interact with unverified, potentially adversarial\noutside code, which substantially weakens the assumptions that formal analyses\ncan (soundly) make. Moreover, the core functionality of smart contracts is to\nmanipulate and transfer resources; describing this functionality concisely\nrequires dedicated specification support. Current reasoning techniques do not\nfully address these challenges, being restricted in their scope or\nexpressiveness (in particular, in the presence of re-entrant calls), and\noffering limited means of expressing the resource transfers a contract\nperforms.\n  In this paper, we present a novel specification methodology tailored to the\ndomain of smart contracts. Our specification constructs and associated\nreasoning technique are the first to enable: (1) sound and precise reasoning in\nthe presence of unverified code and arbitrary re-entrancy, (2) modular\nreasoning about collaborating smart contracts, and (3) domain-specific\nspecifications based on resources and resource transfers, which allow\nexpressing a contract's behavior in intuitive and concise ways and exclude\ntypical errors by default. We have implemented our approach in 2vyper, an\nSMT-based automated verification tool for Ethereum smart contracts written in\nthe Vyper language, and demonstrated its effectiveness in succinctly capturing\nand verifying strong correctness guarantees for real-world contracts.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 22:36:08 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Br\u00e4m", "Christian", ""], ["Eilers", "Marco", ""], ["M\u00fcller", "Peter", ""], ["Sierra", "Robin", ""], ["Summers", "Alexander J.", ""]]}, {"id": "2104.10379", "submitter": "Owen Arden", "authors": "Owen Arden, Anitha Gollamudi, Ethan Cecchetti, Stephen Chong, and\n  Andrew C. Myers", "title": "A Calculus for Flow-Limited Authorization", "comments": "58 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real-world applications routinely make authorization decisions based on\ndynamic computation. Reasoning about dynamically computed authority is\nchallenging. Integrity of the system might be compromised if attackers can\nimproperly influence the authorizing computation. Confidentiality can also be\ncompromised by authorization, since authorization decisions are often based on\nsensitive data such as membership lists and passwords. Previous formal models\nfor authorization do not fully address the security implications of permitting\ntrust relationships to change, which limits their ability to reason about\nauthority that derives from dynamic computation. Our goal is an approach to\nconstructing dynamic authorization mechanisms that do not violate\nconfidentiality or integrity.\n  The Flow-Limited Authorization Calculus (FLAC) is a simple, expressive model\nfor reasoning about dynamic authorization as well as an information flow\ncontrol language for securely implementing various authorization mechanisms.\nFLAC combines the insights of two previous models: it extends the Dependency\nCore Calculus with features made possible by the Flow-Limited Authorization\nModel. FLAC provides strong end-to-end information security guarantees even for\nprograms that incorporate and implement rich dynamic authorization mechanisms.\nThese guarantees include noninterference and robust declassification, which\nprevent attackers from influencing information disclosures in unauthorized\nways. We prove these security properties formally for all FLAC programs and\nexplore the expressiveness of FLAC with several examples.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 06:40:25 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Arden", "Owen", ""], ["Gollamudi", "Anitha", ""], ["Cecchetti", "Ethan", ""], ["Chong", "Stephen", ""], ["Myers", "Andrew C.", ""]]}, {"id": "2104.10749", "submitter": "Pietro Borrello", "authors": "Pietro Borrello, Daniele Cono D'Elia, Leonardo Querzoni, Cristiano\n  Giuffrida", "title": "Constantine: Automatic Side-Channel Resistance Using Efficient Control\n  and Data Flow Linearization", "comments": "Proceedings of the ACM Conference on Computer and Communications\n  Security (CCS) 2021. Code and BibTeX entry available at\n  https://github.com/pietroborrello/constantine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of microarchitectural side channels, vendors scramble to deploy\nmitigations for transient execution attacks, but leave traditional side-channel\nattacks against sensitive software (e.g., crypto programs) to be fixed by\ndevelopers by means of constant-time programming (i.e., absence of\nsecret-dependent code/data patterns). Unfortunately, writing constant-time code\nby hand is hard, as evidenced by the many flaws discovered in production side\nchannel-resistant code. Prior efforts to automatically transform programs into\nconstant-time equivalents offer limited security or compatibility guarantees,\nhindering their applicability to real-world software.\n  In this paper, we present Constantine, a compiler-based system to\nautomatically harden programs against microarchitectural side channels.\nConstantine pursues a radical design point where secret-dependent control and\ndata flows are completely linearized (i.e., all involved code/data accesses are\nalways executed). This strategy provides strong security and compatibility\nguarantees by construction, but its natural implementation leads to state\nexplosion in real-world programs. To address this challenge, Constantine relies\non carefully designed optimizations such as just-in-time loop linearization and\naggressive function cloning for fully context-sensitive points-to analysis,\nwhich not only address state explosion, but also lead to an efficient and\ncompatible solution. Constantine yields overheads as low as 16% on standard\nbenchmarks and can handle a fully-fledged component from the production wolfSSL\nlibrary.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 20:25:30 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Borrello", "Pietro", ""], ["D'Elia", "Daniele Cono", ""], ["Querzoni", "Leonardo", ""], ["Giuffrida", "Cristiano", ""]]}, {"id": "2104.11324", "submitter": "Kyle Hale", "authors": "Nicholas Wanninger, Joshua J. Bowden, and Kyle C. Hale", "title": "Virtines: Virtualization at Function Call Granularity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual execution environments provide strong isolation, on-demand\ninfrastructure, simplified device models, and many other benefits for systems\nand application programmers. However, these environments are often perceived as\nheavyweight and rife with performance pitfalls for fine-grained or low-latency\ntasks. While others have shown that a virtual environment's footprint can be\ndrastically reduced by paring down the guest and host software stacks, in this\npaper we take a different approach. We probe the limits of fine-grained,\nvirtual execution by investigating the viability of hardware virtualized\nenvironments at function call granularity. We introduce a new abstraction\ncalled a virtine, or virtualized subroutine, and present the design of a new\nmicrohypervisor called Wasp that enables them. Through a series of examples we\nshow that Wasp can enable virtines with start-up latencies as low as 100\n$\\mu$s. We also provide two convenient programming interfaces to virtines,\nusing the Rust language and using extensions to C. Using these extensions we\nimplement a simple HTTP server and integrate virtines into an off-the-shelf\nimplementation of OpenSSL.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 20:18:35 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Wanninger", "Nicholas", ""], ["Bowden", "Joshua J.", ""], ["Hale", "Kyle C.", ""]]}, {"id": "2104.12039", "submitter": "Jedidiah McClurg", "authors": "Jordan Schmerge, Miles Claver, Jackson Garner, Jake Vossen, Jedidiah\n  McClurg", "title": "ReGiS: Regular Expression Simplification via Rewrite-Guided Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expression simplification is an important task necessary in a variety of\ndomains, e.g., compilers, digital logic design, etc. Syntax-guided synthesis\n(SyGuS) with a cost function can be used for this purpose, but ordered\nenumeration through a large space of candidate expressions can be expensive.\nEquality saturation is an alternative approach which allows efficient\nconstruction and maintenance of expression equivalence classes generated by\nrewrite rules, but the procedure may not reach saturation, meaning global\nminimality cannot be confirmed. We present a new approach called rewrite-guided\nsynthesis (ReGiS), in which a unique interplay between SyGuS and equality\nsaturation-based rewriting helps to overcome these problems, resulting in an\nefficient, scalable framework for expression simplification. We demonstrate the\nflexibility and practicality of our approach by applying ReGiS to regular\nexpression denial of service (ReDoS) attack prevention. Many real-world regular\nexpression matching engines are vulnerable to these complexity-based attacks,\nand while much research has focused on detecting vulnerable regular\nexpressions, we provide a way for developers to go further, by automatically\ntransforming their regular expressions to remove vulnerabilities.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 23:43:07 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 17:05:02 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Schmerge", "Jordan", ""], ["Claver", "Miles", ""], ["Garner", "Jackson", ""], ["Vossen", "Jake", ""], ["McClurg", "Jedidiah", ""]]}, {"id": "2104.12156", "submitter": "Christian Anti\\'c", "authors": "Christian Antic", "title": "Algebraic answer set programming", "comments": "arXiv admin note: text overlap with arXiv:2009.05774", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DM cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-monotonic reasoning is an essential part of human intelligence\nprominently formalized in artificial intelligence research via answer set\nprogramming. Describing complex objects as the composition of elementary ones\nis a common strategy in computer science and science in general. This paper\ncontributes to the foundations of answer set programming and artificial\nintelligence by introducing and studying the sequential composition of answer\nset programs. Specifically, we show that the notion of composition gives rise\nto a family of finite monoids and seminearrings, baptized {\\em ASP monoids} and\n{\\em ASP seminearrings} in this paper. Particularly, we show that the\ncombination of composition and union yields the structure of a finite\nidempotent seminearring. We also show that the restricted class of proper\nKrom-Horn programs, which only contain rules with exactly one body atom, yields\na finite idempotent semiring. On the semantic side, we show that the van\nEmden-Kowalski immediate consequence operator of a program can be represented\nvia composition, which allows us to compute the least model semantics of Horn\nprograms without any explicit reference to operators. As a result, we\ncharacterize answer sets algebraically, which bridges the conceptual gap\nbetween the syntax and semantics of an answer set program in a mathematically\nsatisfactory way, and which provides an algebraic characterization of strong\nand uniform equivalence. Moreover, it gives rise to an algebraic meta-calculus\nfor answer set programs. In a broader sense, this paper is a further step\ntowards an algebra of rule-based logical theories and in the future we plan to\nadapt and generalize the methods of this paper to wider classes of formalisms,\nmost importantly to first-order and disjunctive answer set programs and\nextensions thereof.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 13:27:22 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Antic", "Christian", ""]]}, {"id": "2104.12455", "submitter": "Lorenzo Bacchiani", "authors": "Lorenzo Bacchiani, Mario Bravetti, Julien Lange, Gianluigi Zavattaro", "title": "A Session Subtyping Tool (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session types are becoming popular and have been integrated in several\nmainstream programming languages. Nevertheless, while many programming\nlanguages consider asynchronous fifo channel communication, the notion of\nsubtyping used in session type implementations is the one defined by Gay and\nHole for synchronous communication. This might be because there are several\nnotions of asynchronous session subtyping, these notions are usually\nundecidable, and only recently sound (but not complete) algorithmic\ncharacterizations for these subtypings have been proposed. But the fact that\nthe definition of asynchronous session subtyping and the theory behind related\nalgorithms are not easily accessible to non-experts may also prevent further\nintegration. The aim of this paper, and of the tool presented therein, is to\nmake the growing body of knowledge about asynchronous session subtyping more\naccessible, thus promoting its integration in practical applications of session\ntypes.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 10:35:17 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 18:27:53 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Bacchiani", "Lorenzo", ""], ["Bravetti", "Mario", ""], ["Lange", "Julien", ""], ["Zavattaro", "Gianluigi", ""]]}, {"id": "2104.12542", "submitter": "Yorah Bosse", "authors": "Yorah Bosse, Igor Scaliante Wiese, Marco Aur\\'elio Graciotto Silva,\n  Nelson Lago, Le\\^onidas de Oliveira Brand\\~ao, David Redmiles, Fabio Kon,\n  Marco A. Gerosa", "title": "Catalogs of C and Python Antipatterns by CS1 Students", "comments": "114 pages, 3 figures, 1 table, 5 appendices", "journal-ref": null, "doi": null, "report-no": "RT-MAC-2021-01", "categories": "cs.CY cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding students' programming misconceptions is critical. Doing so\ndepends on identifying the reasons why students make errors when learning a new\nprogramming language. Knowing the misconceptions can help students to improve\ntheir reflection about their mistakes and also help instructors to design\nbetter teaching strategies. In this technical report, we propose catalogs of\nantipatterns for two programming languages: C and Python. To accomplish this,\nwe analyzed the codes of 166 CS1 engineering students when they were coding\nsolutions to programming exercises. In our results, we catalog 41 CS1\nantipatterns from 95 cataloged misconceptions in C and Python. These\nantipatterns were separated into three catalogs: C, Python, and antipatterns\nfound in code using both programming languages. For each antipattern, we\npresent code examples, students' solutions (if they are present), a possible\nsolution to avoid the antipattern, among other information.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 02:12:41 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Bosse", "Yorah", ""], ["Wiese", "Igor Scaliante", ""], ["Silva", "Marco Aur\u00e9lio Graciotto", ""], ["Lago", "Nelson", ""], ["Brand\u00e3o", "Le\u00f4nidas de Oliveira", ""], ["Redmiles", "David", ""], ["Kon", "Fabio", ""], ["Gerosa", "Marco A.", ""]]}, {"id": "2104.13315", "submitter": "Shivam Handa", "authors": "Shivam Handa and Martin Rinard", "title": "Inductive Program Synthesis over Noisy Datasets using Abstraction\n  Refinement Based Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.FL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new synthesis algorithm to solve program synthesis over noisy\ndatasets, i.e., data that may contain incorrect/corrupted input-output\nexamples. Our algorithm uses an abstraction refinement based optimization\nprocess to synthesize programs which optimize the tradeoff between the loss\nover the noisy dataset and the complexity of the synthesized program. The\nalgorithm uses abstractions to divide the search space of programs into\nsubspaces by computing an abstract value that represents outputs for all\nprograms in a subspace. The abstract value allows our algorithm to compute, for\neach subspace, a sound approximate lower bound of the loss over all programs in\nthe subspace. It iteratively refines these abstractions to further subdivide\nthe space into smaller subspaces, prune subspaces that do not contain an\noptimal program, and eventually synthesize an optimal program.\n  We implemented this algorithm in a tool called Rose. We compare Rose to a\ncurrent state-of-the-art noisy program synthesis system using the SyGuS 2018\nbenchmark suite. Our evaluation demonstrates that Rose significantly\noutperforms this previous system: on two noisy benchmark program synthesis\nproblems sets drawn from the SyGus 2018 benchmark suite, Rose delivers speedups\nof up to 1587 and 81.7, with median speedups of 20.5 and 81.7. Rose also\nterminates on 20 (out of 54) and 4 (out of 11) more benchmark problems than the\nprevious system. Both Rose and the previous system synthesize programs that are\noptimal over the provided noisy data sets. For the majority of the problems in\nthe benchmark sets ($272$ out of $286$), the synthesized programs also produce\ncorrect outputs for all inputs in the original (unseen) noise-free data set.\nThese results highlight the benefits that Rose can deliver for effective noisy\nprogram synthesis.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 16:45:11 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Handa", "Shivam", ""], ["Rinard", "Martin", ""]]}, {"id": "2104.13324", "submitter": "Paolo Pistone", "authors": "Paolo Pistone", "title": "On Generalized Metric Spaces for the Simply Typed Lambda-Calculus\n  (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL math.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generalized metrics, arising from Lawvere's view of metric spaces as enriched\ncategories, have been widely applied in denotational semantics as a way to\nmeasure to which extent two programs behave in a similar, although non\nequivalent, way. However, the application of generalized metrics to\nhigher-order languages like the simply typed lambda calculus has so far proved\nunsatisfactory. In this paper we investigate a new approach to the construction\nof cartesian closed categories of generalized metric spaces. Our starting point\nis a quantitative semantics based on a generalization of usual logical\nrelations. Within this setting, we show that several families of generalized\nmetrics provide ways to extend the Euclidean metric to all higher-order types.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 16:56:01 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Pistone", "Paolo", ""]]}, {"id": "2104.13795", "submitter": "Gabriele Vanoni", "authors": "Beniamino Accattoli, Ugo Dal Lago, Gabriele Vanoni", "title": "The Space of Interaction (long version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The space complexity of functional programs is not well understood. In\nparticular, traditional implementation techniques are tailored to time\nefficiency, and space efficiency induces time inefficiencies, as it prefers\nre-computing to saving. Girard's geometry of interaction underlies an\nalternative approach based on the interaction abstract machine (IAM), claimed\nas space efficient in the literature. It has also been conjectured to provide a\nreasonable notion of space for the lambda-calculus, but such an important\nresult seems to be elusive.\n  In this paper we introduce a new intersection type system precisely measuring\nthe space consumption of the IAM on the typed term. Intersection types have\nbeen repeatedly used to measure time, which they achieve by dropping\nidempotency, turning intersections into multisets. Here we show that the space\nconsumption of the IAM is connected to a further structural modification,\nturning multisets into trees. Tree intersection types lead to a finer\nunderstanding of some space complexity results from the literature. They also\nshed new light on the conjecture about reasonable space: we show that the usual\nway of encoding Turing machines into the lambda calculus cannot be used to\nprove that the space of the IAM is a reasonable cost model.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 14:33:28 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Accattoli", "Beniamino", ""], ["Lago", "Ugo Dal", ""], ["Vanoni", "Gabriele", ""]]}, {"id": "2104.13979", "submitter": "Beniamino Accattoli", "authors": "Beniamino Accattoli, Giulio Guerrieri, Maico Leberle", "title": "Semantic Bounds and Strong Call-by-Value Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores two topics at once: the use of denotational semantics to\nbound the evaluation length of functional programs, and the semantics of strong\n(that is, possibly under abstractions) call-by-value evaluation.\n  About the first, we analyze de Carvalho's seminal use of relational semantics\nfor bounding the evaluation length of lambda-terms, starting from the\npresentation of the semantics as an intersection types system. We focus on the\npart of his work which is usually neglected in its many recent adaptations,\ndespite being probably the conceptually deeper one: how to transfer the\nbounding power from the type system to the relational semantics itself. We\ndissect this result and re-understand it via the isolation of a simpler size\nrepresentation property.\n  About the second, we use relational semantics to develop a semantical study\nof strong call-by-value evaluation, which is both a delicate and neglected\ntopic. We give a semantic characterization of terms normalizable with respect\nto strong evaluation, providing in particular the first result of adequacy with\nrespect to strong call-by-value. Moreover, we extract bounds about strong\nevaluation from both the type systems and the relational semantics.\n  Essentially, we use strong call-by-value to revisit de Carvalho's semantic\nbounds, and de Carvalho's technique to provide semantical foundations for\nstrong call-by-value.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 19:09:30 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Accattoli", "Beniamino", ""], ["Guerrieri", "Giulio", ""], ["Leberle", "Maico", ""]]}, {"id": "2104.14094", "submitter": "Farzaneh Derakhshan", "authors": "Farzaneh Derakhshan, Stephanie Balzer, Limin Jia", "title": "Session Logical Relations for Noninterference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Information flow control type systems statically restrict the propagation of\nsensitive data to ensure end-to-end confidentiality. The property to be shown\nis noninterference, asserting that an attacker cannot infer any secrets from\nmade observations. Session types delimit the kinds of observations that can be\nmade along a communication channel by imposing a protocol of message exchange.\nThese protocols govern the exchange along a single channel and leave\nunconstrained the propagation along adjacent channels. This paper contributes\nan information flow control type system for linear session types. The type\nsystem stands in close correspondence with intuitionistic linear logic.\nIntuitionistic linear logic typing ensures that process configurations form a\ntree such that client processes are parent nodes and provider processes child\nnodes. To control the propagation of secret messages, the type system is\nenriched with secrecy levels and arranges these levels to be aligned with the\nconfiguration tree. Two levels are associated with every process: the maximal\nsecrecy denoting the process' security clearance and the running secrecy\ndenoting the highest level of secret information obtained so far. The\ncomputational semantics naturally stratifies process configurations such that\nhigher-secrecy processes are parents of lower-secrecy ones, an invariant\nenforced by typing. Noninterference is stated in terms of a logical relation\nthat is indexed by the secrecy-level-enriched session types. The logical\nrelation contributes a novel development of logical relations for session typed\nlanguages as it considers open configurations, allowing for more nuanced\nequivalence statement.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 03:41:19 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Derakhshan", "Farzaneh", ""], ["Balzer", "Stephanie", ""], ["Jia", "Limin", ""]]}, {"id": "2104.14519", "submitter": "Rohit Chadha", "authors": "Rohit Chadha, A. Prasad Sistla and Mahesh Viswanathan", "title": "On Linear Time Decidability of Differential Privacy for Programs with\n  Unbounded Inputs", "comments": "An extended abstract to be published in 36th Annual IEEE Symposium on\n  Logic in Computer Science (LICS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.FL cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an automata model for describing interesting classes of\ndifferential privacy mechanisms/algorithms that include known mechanisms from\nthe literature. These automata can model algorithms whose inputs can be an\nunbounded sequence of real-valued query answers. We consider the problem of\nchecking whether there exists a constant $d$ such that the algorithm described\nby these automata are $d\\epsilon$-differentially private for all positive\nvalues of the privacy budget parameter $\\epsilon$. We show that this problem\ncan be decided in time linear in the automaton's size by identifying a\nnecessary and sufficient condition on the underlying graph of the automaton.\nThis paper's results are the first decidability results known for algorithms\nwith an unbounded number of query answers taking values from the set of reals.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:34:44 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Chadha", "Rohit", ""], ["Sistla", "A. Prasad", ""], ["Viswanathan", "Mahesh", ""]]}]