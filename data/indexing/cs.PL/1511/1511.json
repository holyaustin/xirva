[{"id": "1511.00184", "submitter": "Lukas Holik", "authors": "Fr\\'ed\\'eric Haziza, Luk\\'a\\v{s} Hol\\'ik, Roland Meyer, Sebastian\n  Wolff", "title": "Pointer Race Freedom", "comments": null, "journal-ref": null, "doi": null, "report-no": "FIT-TR-2015-05", "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel notion of pointer race for concurrent programs\nmanipulating a shared heap. A pointer race is an access to a memory address\nwhich was freed, and it is out of the accessor's control whether or not the\ncell has been re-allocated. We establish two results. (1) Under the assumption\nof pointer race freedom, it is sound to verify a program running under explicit\nmemory management as if it was running with garbage collection. (2) Even the\nrequirement of pointer race freedom itself can be verified under the\ngarbage-collected semantics. We then prove analogues of the theorems for a\nstronger notion of pointer race needed to cope with performance-critical code\npurposely using racy comparisons and even racy dereferences of pointers. As a\npractical contribution, we apply our results to optimize a thread-modular\nanalysis under explicit memory management. Our experiments confirm a speed-up\nof up to two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 22:18:40 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 11:08:27 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2015 05:01:10 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Haziza", "Fr\u00e9d\u00e9ric", ""], ["Hol\u00edk", "Luk\u00e1\u0161", ""], ["Meyer", "Roland", ""], ["Wolff", "Sebastian", ""]]}, {"id": "1511.00346", "submitter": "Ichiro Hasuo", "authors": "Ichiro Hasuo, Shunsuke Shimizu, Corina Cirstea", "title": "Lattice-Theoretic Progress Measures and Coalgebraic Model Checking (with\n  Appendices)", "comments": "24 pages, Extended version with appendices of a paper accepted to\n  POPL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of formal verification in general and model checking in\nparticular, parity games serve as a mighty vehicle: many problems are encoded\nas parity games, which are then solved by the seminal algorithm by Jurdzinski.\nIn this paper we identify the essence of this workflow to be the notion of\nprogress measure, and formalize it in general, possibly infinitary,\nlattice-theoretic terms. Our view on progress measures is that they are to\nnested/alternating fixed points what invariants are to safety/greatest fixed\npoints, and what ranking functions are to liveness/least fixed points. That is,\nprogress measures are combination of the latter two notions (invariant and\nranking function) that have been extensively studied in the context of\n(program) verification.\n  We then apply our theory of progress measures to a general model-checking\nframework, where systems are categorically presented as coalgebras. The\nframework's theoretical robustness is witnessed by a smooth transfer from the\nbranching-time setting to the linear-time one. Although the framework can be\nused to derive some decision procedures for finite settings, we also expect the\nproposed framework to form a basis for sound proof methods for some\nundecidable/infinitary problems.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 00:53:41 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 09:22:36 GMT"}, {"version": "v3", "created": "Sat, 9 Jan 2016 08:11:59 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Hasuo", "Ichiro", ""], ["Shimizu", "Shunsuke", ""], ["Cirstea", "Corina", ""]]}, {"id": "1511.00511", "submitter": "Philipp Haller", "authors": "Philipp Haller and Heather Miller", "title": "A Formal Model for Direct-style Asynchronous Observables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Languages like F#, C#, and recently also Scala, provide \"Async\" programming\nmodels which aim to make asynchronous programming easier by avoiding an\ninversion of control that is inherent in callback-based programming models.\nThis paper presents a novel approach to integrate the Async model with\nobservable streams of the Reactive Extensions model. Reactive Extensions are\nbest-known from the .NET platform, and widely-used implementations of its\nprogramming model exist also for Java, Ruby, and other languages. This paper\ncontributes a formalization of the unified \"Reactive Async\" model in the\ncontext of an object-based core calculus. Our formal model captures the essence\nof the protocol of asynchronous observables using a heap evolution property. We\nprove a subject reduction theorem; the theorem implies that reduction preserves\nthe heap evolution property. Thus, for well-typed programs our calculus ensures\nthe protocol of asynchronous observables.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 14:15:51 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Haller", "Philipp", ""], ["Miller", "Heather", ""]]}, {"id": "1511.00825", "submitter": "Kengo Kido", "authors": "Kengo Kido, Swarat Chaudhuri and Ichiro Hasuo", "title": "Abstract Interpretation with Infinitesimals: Towards Scalability in\n  Nonstandard Static Analysis (Extended Version)", "comments": "28 pages, an extended version of a paper accepted in 17th\n  International Conference on Verification, Model Checking, and Abstract\n  Interpretation (VMCAI 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend abstract interpretation for the purpose of verifying hybrid\nsystems. Abstraction has been playing an important role in many verification\nmethodologies for hybrid systems, but some special care is needed for\nabstraction of continuous dynamics defined by ODEs. We apply Cousot and\nCousot's framework of abstract interpretation to hybrid systems, almost as it\nis, by regarding continuous dynamics as an infinite iteration of infinitesimal\ndiscrete jumps. This extension follows the recent line of work by Suenaga,\nHasuo and Sekine, where deductive verification is extended for hybrid systems\nby 1) introducing a constant dt for an infinitesimal value; and 2) employing\nRobinson's nonstandard analysis (NSA) to define mathematically rigorous\nsemantics. Our theoretical results include soundness and termination via\nuniform widening operators; and our prototype implementation successfully\nverifies some benchmark examples.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 09:22:36 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Kido", "Kengo", ""], ["Chaudhuri", "Swarat", ""], ["Hasuo", "Ichiro", ""]]}, {"id": "1511.00915", "submitter": "Jan Wielemaker", "authors": "Jan Wielemaker and Torbj\\\"orn Lager and Fabrizio Riguzzi", "title": "SWISH: SWI-Prolog for Sharing", "comments": "International Workshop on User-Oriented Logic Programming (IULP\n  2015), co-located with the 31st International Conference on Logic Programming\n  (ICLP 2015), Proceedings of the International Workshop on User-Oriented Logic\n  Programming (IULP 2015), Editors: Stefan Ellmauthaler and Claudia Schulz,\n  pages 99-113, August 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, we see a new type of interfaces for programmers based on web\ntechnology. For example, JSFiddle, IPython Notebook and R-studio. Web\ntechnology enables cloud-based solutions, embedding in tutorial web pages,\natractive rendering of results, web-scale cooperative development, etc. This\narticle describes SWISH, a web front-end for Prolog. A public website exposes\nSWI-Prolog using SWISH, which is used to run small Prolog programs for\ndemonstration, experimentation and education. We connected SWISH to the\nClioPatria semantic web toolkit, where it allows for collaborative development\nof programs and queries related to a dataset as well as performing maintenance\ntasks on the running server and we embedded SWISH in the Learn Prolog Now!\nonline Prolog book.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 14:16:31 GMT"}], "update_date": "2016-08-06", "authors_parsed": [["Wielemaker", "Jan", ""], ["Lager", "Torbj\u00f6rn", ""], ["Riguzzi", "Fabrizio", ""]]}, {"id": "1511.00916", "submitter": "Joost Vennekens", "authors": "Joost Vennekens", "title": "Lowering the learning curve for declarative programming: a Python API\n  for the IDP system", "comments": "International Workshop on User-Oriented Logic Programming (IULP\n  2015), co-located with the 31st International Conference on Logic Programming\n  (ICLP 2015), Proceedings of the International Workshop on User-Oriented Logic\n  Programming (IULP 2015), Editors: Stefan Ellmauthaler and Claudia Schulz,\n  pages 83-98, August 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programmers may be hesitant to use declarative systems, because of the\nassociated learning curve. In this paper, we present an API that integrates the\nIDP Knowledge Base system into the Python programming language. IDP is a\nstate-of-the-art logical system, which uses SAT, SMT, Logic Programming and\nAnswer Set Programming technology. Python is currently one of the most widely\nused (teaching) languages for programming. The first goal of our API is to\nallow a Python programmer to use the declarative power of IDP, without needing\nto learn any new syntax or semantics. The second goal is allow IDP to be added\nto/removed from an existing code base with minimal changes.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 14:21:23 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Vennekens", "Joost", ""]]}, {"id": "1511.00920", "submitter": "Ingmar Dasseville", "authors": "Ingmar Dasseville and Gerda Janssens", "title": "A web-based IDE for IDP", "comments": "International Workshop on User-Oriented Logic Programming (IULP\n  2015), co-located with the 31st International Conference on Logic Programming\n  (ICLP 2015), Proceedings of the International Workshop on User-Oriented Logic\n  Programming (IULP 2015), Editors: Stefan Ellmauthaler and Claudia Schulz,\n  pages 21-32, August 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IDP is a knowledge base system based on first order logic. It is finding its\nway to a larger public but is still facing practical challenges. Adoption of\nnew languages requires a newcomer-friendly way for users to interact with it.\nBoth an online presence to try to convince potential users to download the\nsystem and offline availability to develop larger applications are essential.\nWe developed an IDE which can serve both purposes through the use of web\ntechnology. It enables us to provide the user with a modern IDE with relatively\nlittle effort.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 14:30:10 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Dasseville", "Ingmar", ""], ["Janssens", "Gerda", ""]]}, {"id": "1511.00924", "submitter": "Sarah Alice Gaggl", "authors": "Sarah Alice Gaggl and Sebastian Rudolph and Lukas Schweizer", "title": "Bound Your Models! How to Make OWL an ASP Modeling Language", "comments": "International Workshop on User-Oriented Logic Programming (IULP\n  2015), co-located with the 31st International Conference on Logic Programming\n  (ICLP 2015), Proceedings of the International Workshop on User-Oriented Logic\n  Programming (IULP 2015), Editors: Stefan Ellmauthaler and Claudia Schulz,\n  pages 33-49, August 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To exploit the Web Ontology Language OWL as an answer set programming (ASP)\nlanguage, we introduce the notion of bounded model semantics, as an intuitive\nand computationally advantageous alternative to its classical semantics. We\nshow that a translation into ASP allows for solving a wide range of\nbounded-model reasoning tasks, including satisfiability and axiom entailment\nbut also novel ones such as model extraction and enumeration. Ultimately, our\nwork facilitates harnessing advanced semantic web modeling environments for the\nlogic programming community through an \"off-label use\" of OWL.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 14:38:53 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Gaggl", "Sarah Alice", ""], ["Rudolph", "Sebastian", ""], ["Schweizer", "Lukas", ""]]}, {"id": "1511.00928", "submitter": "Ruben Lapauw", "authors": "Ruben Lapauw and Ingmar Dasseville and Marc Denecker", "title": "Visualising interactive inferences with IDPD3", "comments": "International Workshop on User-Oriented Logic Programming (IULP\n  2015), co-located with the 31st International Conference on Logic Programming\n  (ICLP 2015), Proceedings of the International Workshop on User-Oriented Logic\n  Programming (IULP 2015), Editors: Stefan Ellmauthaler and Claudia Schulz,\n  pages 67-81, August 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large part of the use of knowledge base systems is the interpretation of\nthe output by the end-users and the interaction with these users. Even during\nthe development process visualisations can be a great help to the developer. We\ncreated IDPD3 as a library to visualise models of logic theories. IDPD3 is a\nnew version of $ID^{P}_{Draw}$ and adds support for visualised interactive\nsimulations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 14:40:57 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Lapauw", "Ruben", ""], ["Dasseville", "Ingmar", ""], ["Denecker", "Marc", ""]]}, {"id": "1511.01080", "submitter": "Michel Rueher", "authors": "H\\'el\\`ene Collavizza, Claude Michel, Michel Rueher", "title": "Searching input values hitting suspicious Intervals in programs with\n  floating-point operations", "comments": null, "journal-ref": "28th International Conference on Software and Systems\n  (ICTSS-2016)., Oct 2016, Graz, Austria. 2016, LNCS", "doi": null, "report-no": null, "categories": "cs.PL cs.DS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programs with floating-point computations are often derived from mathematical\nmodels or designed with the semantics of the real numbers in mind. However, for\na given input, the computed path with floating-point numbers may differ from\nthe path corresponding to the same computation with real numbers. A common\npractice when validating such programs consists in estimating the accuracy of\nfloating-point computations with respect to the same sequence of operations in\nan ide-alized semantics of real numbers. However, state-of-the-art tools\ncompute an over-approximation of the error introduced by floating-point\noperations. As a consequence, totally inappropriate behaviors of a program may\nbe dreaded but the developer does not know whether these behaviors will\nactually occur, or not. In this paper, we introduce a new constraint-based\napproach that searches for test cases in the part of the over-approximation\nwhere errors due to floating-point arithmetic would lead to inappropriate\nbehaviors.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 20:46:32 GMT"}, {"version": "v2", "created": "Fri, 5 Aug 2016 08:22:12 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Collavizza", "H\u00e9l\u00e8ne", ""], ["Michel", "Claude", ""], ["Rueher", "Michel", ""]]}, {"id": "1511.01223", "submitter": "Philipp Haller", "authors": "Lucas Wiener and Tomas Ekholm and Philipp Haller", "title": "Modular Responsive Web Design using Element Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Responsive Web Design (RWD) enables web applications to adapt to the\ncharacteristics of different devices such as screen size which is important for\nmobile browsing. Today, the only W3C standard to support this adaptability is\nCSS media queries. However, using media queries it is impossible to create\napplications in a modular way, because responsive elements then always depend\non the global context. Hence, responsive elements can only be reused if the\nglobal context is exactly the same, severely limiting their reusability. This\nmakes it extremely challenging to develop large responsive applications,\nbecause the lack of true modularity makes certain requirement changes either\nimpossible or expensive to realize.\n  In this paper we extend RWD to also include responsive modules, i.e., modules\nthat adapt their design based on their local context independently of the\nglobal context. We present the ELQ project which implements our approach. ELQ\nis a novel implementation of so-called element queries which generalize media\nqueries. Importantly, our design conforms to existing web specifications,\nenabling adoption on a large scale. ELQ is designed to be heavily extensible\nusing plugins. Experimental results show speed-ups of the core algorithms of up\nto 37x compared to previous approaches.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 06:52:59 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Wiener", "Lucas", ""], ["Ekholm", "Tomas", ""], ["Haller", "Philipp", ""]]}, {"id": "1511.01261", "submitter": "Torsten Schaub", "authors": "Martin Gebser and Phillip Obermeier and Torsten Schaub", "title": "Interactive Answer Set Programming - Preliminary Report", "comments": "International Workshop on User-Oriented Logic Programming (IULP\n  2015), co-located with the 31st International Conference on Logic Programming\n  (ICLP 2015), Proceedings of the International Workshop on User-Oriented Logic\n  Programming (IULP 2015), invited talk, Editors: Stefan Ellmauthaler and\n  Claudia Schulz, pages 3-17, August 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Answer Set Programming (ASP) rests upon one-shot solving. A logic\nprogram is fed into an ASP system and its stable models are computed. The high\npractical relevance of dynamic applications led to the development of\nmulti-shot solving systems. An operative system solves continuously changing\nlogic programs. Although this was primarily aiming at dynamic applications in\nassisted living, robotics, or stream reasoning, where solvers interact with an\nenvironment, it also opened up the opportunity of interactive ASP, where a\nsolver interacts with a user. We begin with a formal characterization of\ninteractive ASP in terms of states and operations on them. In turn, we describe\nthe interactive ASP shell aspic along with its basic functionalities.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 09:42:59 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Gebser", "Martin", ""], ["Obermeier", "Phillip", ""], ["Schaub", "Torsten", ""]]}, {"id": "1511.01399", "submitter": "\\'Eric Tanter", "authors": "Ronald Garcia and \\'Eric Tanter", "title": "Deriving a Simple Gradual Security Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstracting Gradual Typing (AGT) is an approach to systematically deriving\ngradual counterparts to static type disciplines. The approach consists of\ndefining the semantics of gradual types by interpreting them as sets of static\ntypes, and then defining an optimal abstraction back to gradual types. These\noperations are used to lift the static discipline to the gradual setting. The\nruntime semantics of the gradual language then arises as reductions on gradual\ntyping derivations.\n  To demonstrate the flexibility of AGT, we gradualize $\\lambda_\\text{SEC}$,\nthe prototypical security-typed language, with respect to only security labels\nrather than entire types, yielding a type system that ranges gradually from\nsimply-typed to securely-typed. We establish noninterference for the gradual\nlanguage, called $\\lambda_{\\widetilde{\\text{SEC}}}$, using Zdancewic's logical\nrelation proof method. Whereas prior work presents gradual security cast\nlanguages, which require explicit security casts, this work yields the first\ngradual security source language, which requires no explicit casts.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 17:07:00 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 01:40:26 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2015 15:18:22 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Garcia", "Ronald", ""], ["Tanter", "\u00c9ric", ""]]}, {"id": "1511.01413", "submitter": "Pedro Lopez-Garcia", "authors": "Umer Liqat, Kyriakos Georgiou, Steve Kerrison, Pedro Lopez-Garcia,\n  John P. Gallagher, Manuel V. Hermenegildo and Kerstin Eder", "title": "Inferring Parametric Energy Consumption Functions at Different Software\n  Levels: ISA vs. LLVM IR", "comments": "22 pages, 4 figures, 2 tables", "journal-ref": null, "doi": "10.1007/978-3-319-46559-3_5", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The static estimation of the energy consumed by program executions is an\nimportant challenge, which has applications in program optimization and\nverification, and is instrumental in energy-aware software development. Our\nobjective is to estimate such energy consumption in the form of functions on\nthe input data sizes of programs. We have developed a tool for experimentation\nwith static analysis which infers such energy functions at two levels, the\ninstruction set architecture (ISA) and the intermediate code (LLVM IR) levels,\nand reflects it upwards to the higher source code level. This required the\ndevelopment of a translation from LLVM IR to an intermediate representation and\nits integration with existing components, a translation from ISA to the same\nrepresentation, a resource analyzer, an ISA-level energy model, and a mapping\nfrom this model to LLVM IR. The approach has been applied to programs written\nin the XC language running on XCore architectures, but is general enough to be\napplied to other languages. Experimental results show that our LLVM IR level\nanalysis is reasonably accurate (less than 6.4% average error vs. hardware\nmeasurements) and more powerful than analysis at the ISA level. This paper\nprovides insights into the trade-off of precision versus analyzability at these\nlevels.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 17:46:13 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Liqat", "Umer", ""], ["Georgiou", "Kyriakos", ""], ["Kerrison", "Steve", ""], ["Lopez-Garcia", "Pedro", ""], ["Gallagher", "John P.", ""], ["Hermenegildo", "Manuel V.", ""], ["Eder", "Kerstin", ""]]}, {"id": "1511.01566", "submitter": "EPTCS", "authors": "Samson Abramsky (University of Oxford), Dominic Horsman (University of\n  Oxford)", "title": "DEMONIC programming: a computational language for single-particle\n  equilibrium thermodynamics, and its formal semantics", "comments": "In Proceedings QPL 2015, arXiv:1511.01181. Dominic Horsman published\n  previously as Clare Horsman", "journal-ref": "EPTCS 195, 2015, pp. 1-16", "doi": "10.4204/EPTCS.195.1", "report-no": null, "categories": "cs.LO cond-mat.stat-mech cs.PL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maxwell's Demon, 'a being whose faculties are so sharpened that he can follow\nevery molecule in its course', has been the centre of much debate about its\nabilities to violate the second law of thermodynamics. Landauer's hypothesis,\nthat the Demon must erase its memory and incur a thermodynamic cost, has become\nthe standard response to Maxwell's dilemma, and its implications for the\nthermodynamics of computation reach into many areas of quantum and classical\ncomputing. It remains, however, still a hypothesis. Debate has often centred\naround simple toy models of a single particle in a box. Despite their\nsimplicity, the ability of these systems to accurately represent thermodynamics\n(specifically to satisfy the second law) and whether or not they display\nLandauer Erasure, has been a matter of ongoing argument. The recent\nNorton-Ladyman controversy is one such example.\n  In this paper we introduce a programming language to describe these simple\nthermodynamic processes, and give a formal operational semantics and program\nlogic as a basis for formal reasoning about thermodynamic systems. We formalise\nthe basic single-particle operations as statements in the language, and then\nshow that the second law must be satisfied by any composition of these basic\noperations. This is done by finding a computational invariant of the system. We\nshow, furthermore, that this invariant requires an erasure cost to exist within\nthe system, equal to kTln2 for a bit of information: Landauer Erasure becomes a\ntheorem of the formal system. The Norton-Ladyman controversy can therefore be\nresolved in a rigorous fashion, and moreover the formalism we introduce gives a\nset of reasoning tools for further analysis of Landauer erasure, which are\nprovably consistent with the second law of thermodynamics.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 01:41:13 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Abramsky", "Samson", "", "University of Oxford"], ["Horsman", "Dominic", "", "University of\n  Oxford"]]}, {"id": "1511.01567", "submitter": "EPTCS", "authors": "Costin B\\u{a}descu, Prakash Panangaden", "title": "Quantum Alternation: Prospects and Problems", "comments": "In Proceedings QPL 2015, arXiv:1511.01181", "journal-ref": "EPTCS 195, 2015, pp. 33-42", "doi": "10.4204/EPTCS.195.3", "report-no": null, "categories": "cs.PL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a notion of quantum control in a quantum programming language\nwhich permits the superposition of finitely many quantum operations without\nperforming a measurement. This notion takes the form of a conditional construct\nsimilar to the IF statement in classical programming languages. We show that\nadding such a quantum IF statement to the QPL programming language simplifies\nthe presentation of several quantum algorithms. This motivates the possibility\nof extending the denotational semantics of QPL to include this form of quantum\nalternation. We give a denotational semantics for this extension of QPL based\non Kraus decompositions rather than on superoperators. Finally, we clarify the\nrelation between quantum alternation and recursion, and discuss the possibility\nof lifting the semantics defined by Kraus operators to the superoperator\nsemantics defined by Selinger.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 01:41:34 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["B\u0103descu", "Costin", ""], ["Panangaden", "Prakash", ""]]}, {"id": "1511.01572", "submitter": "EPTCS", "authors": "Kentaro Honda", "title": "Analysis of Quantum Entanglement in Quantum Programs using Stabilizer\n  Formalism", "comments": "In Proceedings QPL 2015, arXiv:1511.01181", "journal-ref": "EPTCS 195, 2015, pp. 262-272", "doi": "10.4204/EPTCS.195.19", "report-no": null, "categories": "quant-ph cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum entanglement plays an important role in quantum computation and\ncommunication. It is necessary for many protocols and computations, but causes\nunexpected disturbance of computational states. Hence, static analysis of\nquantum entanglement in quantum programs is necessary. Several papers studied\nthe problem. They decided qubits were entangled if multiple qubits unitary\ngates are applied to them, and some refined this reasoning using information\nabout the state of each separated qubit. However, they do not care about the\nfact that unitary gate undoes entanglement and that measurement may separate\nmultiple qubits. In this paper, we extend prior work using stabilizer\nformalism. It refines reasoning about separability of quantum variables in\nquantum programs.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 01:44:16 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Honda", "Kentaro", ""]]}, {"id": "1511.01838", "submitter": "St\\'ephane Gimenez", "authors": "St\\'ephane Gimenez, Georg Moser", "title": "The Complexity of Interaction (Long Version)", "comments": null, "journal-ref": null, "doi": "10.1145/2837614.2837646", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the complexity of functional programs written in\nthe interaction-net computation model, an asynchronous, parallel and confluent\nmodel that generalizes linear-logic proof nets. Employing user-defined sized\nand scheduled types, we certify concrete time, space and space-time complexity\nbounds for both sequential and parallel reductions of interaction-net programs\nby suitably assigning complexity potentials to typed nodes. The relevance of\nthis approach is illustrated on archetypal programming examples. The provided\nanalysis is precise, compositional and is, in theory, not restricted to\nparticular complexity classes.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 18:05:10 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Gimenez", "St\u00e9phane", ""], ["Moser", "Georg", ""]]}, {"id": "1511.01874", "submitter": "Radu Grigore", "authors": "Radu Grigore and Hongseok Yang", "title": "Abstraction Refinement Guided by a Learnt Probabilistic Model", "comments": "POPL2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The core challenge in designing an effective static program analysis is to\nfind a good program abstraction -- one that retains only details relevant to a\ngiven query. In this paper, we present a new approach for automatically finding\nsuch an abstraction. Our approach uses a pessimistic strategy, which can\noptionally use guidance from a probabilistic model. Our approach applies to\nparametric static analyses implemented in Datalog, and is based on\ncounterexample-guided abstraction refinement. For each untried abstraction, our\nprobabilistic model provides a probability of success, while the size of the\nabstraction provides an estimate of its cost in terms of analysis time.\nCombining these two metrics, probability and cost, our refinement algorithm\npicks an optimal abstraction. Our probabilistic model is a variant of the\nErdos-Renyi random graph model, and it is tunable by what we call\nhyperparameters. We present a method to learn good values for these\nhyperparameters, by observing past runs of the analysis on an existing\ncodebase. We evaluate our approach on an object sensitive pointer analysis for\nJava programs, with two client analyses (PolySite and Downcast).\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 19:57:03 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 08:15:54 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Grigore", "Radu", ""], ["Yang", "Hongseok", ""]]}, {"id": "1511.02415", "submitter": "Marek Trt\\'ik", "authors": "Marek Trtik", "title": "Anonymous On-line Communication Between Program Analyses", "comments": "Regular paper, 20 pages. arXiv admin note: text overlap with\n  arXiv:1504.07862", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a light-weight client-server model of communication between\nprogram analyses. Clients are individual analyses and the server mediates their\ncommunication. A client cannot see properties of any other and the\ncommunication is anonymous. There is no central algorithm standing above\nclients which would tell them when to communicate what information. Clients\ncommunicate with others spontaneously, according to their actual personal\nneeds. The model is based on our observation that a piece of information\nprovided to an analysis at a right place may (substantially) improve its\nresult. We evaluated the proposed communication model for all possible\ncombinations of three clients on more than 400 benchmarks and the results show\nthat the communication model performs well in practice.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 00:25:07 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Trtik", "Marek", ""]]}, {"id": "1511.02529", "submitter": "Iliano Cervesato", "authors": "Iliano Cervesato (Carnegie Mellon University), Carsten Sch\\\"urmann (IT\n  University of Copenhagen)", "title": "Proceedings First International Workshop on Focusing", "comments": null, "journal-ref": "EPTCS 197, 2015", "doi": "10.4204/EPTCS.197", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume constitutes the proceedings of WoF'15, the First International\nWorkshop on Focusing, held on November 23rd, 2015 in Suva, Fiji. The workshop\nwas a half-day satellite event of LPAR-20, the 20th International Conferences\non Logic for Programming, Artificial Intelligence and Reasoning.\n  The program committee selected four papers for presentation at WoF'15, and\ninclusion in this volume. In addition, the program included an invited talk by\nElaine Pimentel.\n  Focusing is a proof search strategy that alternates two phases: an inversion\nphase where invertible sequent rules are applied exhaustively and a chaining\nphase where it selects a formula and decomposes it maximally using\nnon-invertible rules. Focusing is one of the most exciting recent developments\nin computational logic: it is complete for many logics of interest and provides\na foundation for their use as programming languages and rewriting calculi.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 21:15:13 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 05:49:10 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Cervesato", "Iliano", "", "Carnegie Mellon University"], ["Sch\u00fcrmann", "Carsten", "", "IT\n  University of Copenhagen"]]}, {"id": "1511.02597", "submitter": "Larisa Safina", "authors": "Larisa Safina, Manuel Mazzara, Fabrizio Montesi", "title": "Data-driven Workflows for Microservices", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microservices is an architectural style inspired by service-oriented\ncomputing that has recently started gaining popularity. Jolie is a programming\nlanguage based on the microservices paradigm: the main building block of Jolie\nsystems are services, in contrast to, e.g., functions or objects. The\nprimitives offered by the Jolie language elicit many of the recurring patterns\nfound in microservices, like load balancers and structured processes. However,\nJolie still lacks some useful constructs for dealing with message types and\ndata manipulation that are present in service-oriented computing. In this\npaper, we focus on the possibility of expressing choices at the level of data\ntypes, a feature well represented in standards for Web Services, e.g., WSDL. We\nextend Jolie to support such type choices and show the impact of our\nimplementation on some of the typical scenarios found in microservice systems.\nThis shows how computation can move from a process-driven to a data-driven\napproach, and leads to the preliminary identification of recurring\ncommunication patterns that can be shaped as design patterns.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 08:31:46 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Safina", "Larisa", ""], ["Mazzara", "Manuel", ""], ["Montesi", "Fabrizio", ""]]}, {"id": "1511.02603", "submitter": "Paschalis Mpeis", "authors": "Paschalis Mpeis, Pavlos Petoumenos, Hugh Leather", "title": "Iterative compilation on mobile devices", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of poorly optimized mobile applications coupled with their\nincreasing centrality in our digital lives make a framework for mobile app\noptimization an imperative. While tuning strategies for desktop and server\napplications have a long history, it is difficult to adapt them for use on\nmobile phones.\n  Reference inputs which trigger behavior similar to a mobile application's\ntypical are hard to construct. For many classes of applications the very\nconcept of typical behavior is nonexistent, each user interacting with the\napplication in very different ways. In contexts like this, optimization\nstrategies need to evaluate their effectiveness against real user input, but\ndoing so online runs the risk of user dissatisfaction when suboptimal\noptimizations are evaluated.\n  In this paper we present an iterative compiler which employs a novel capture\nand replay technique in order to collect real user input and use it later to\nevaluate different transformations offline. The proposed mechanism identifies\nand stores only the set of memory pages needed to replay the most heavily used\nfunctions of the application. At idle periods, this minimal state is combined\nwith different binaries of the application, each one build with different\noptimizations enabled. Replaying the targeted functions allows us to evaluate\nthe effectiveness of each set of optimizations for the actual way the user\ninteracts with the application.\n  For the BEEBS benchmark suite, our approach was able to improve performance\nby up to 57%, while keeping the slowdown experienced by the user on average at\n0.8%. By focusing only on heavily used functions, we are able to conserve\nstorage space by between two and three orders of magnitude compared to typical\ncapture and replay implementations.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 09:14:08 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 17:34:46 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2015 16:05:33 GMT"}, {"version": "v4", "created": "Wed, 6 Jan 2016 21:10:43 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Mpeis", "Paschalis", ""], ["Petoumenos", "Pavlos", ""], ["Leather", "Hugh", ""]]}, {"id": "1511.02629", "submitter": "Luke Ong", "authors": "C.-H. Luke Ong", "title": "Normalisation by Traversals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method of computing the beta-normal eta-long form of a\nsimply-typed lambda-term by constructing traversals over a variant abstract\nsyntax tree of the term. In contrast to beta-reduction, which changes the term\nby substitution, this method of normalisation by traversals leaves the original\nterm intact. We prove the correctness of the normalisation procedure by game\nsemantics. As an application, we establish a path-traversal correspondence\ntheorem which is the basis of a key decidability result in higher-order model\nchecking.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 10:51:06 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Ong", "C. -H. Luke", ""]]}, {"id": "1511.02956", "submitter": "Maxime Chevalier-Boisvert", "authors": "Maxime Chevalier-Boisvert and Marc Feeley", "title": "Interprocedural Type Specialization of JavaScript Programs Without Type\n  Analysis", "comments": "10 pages, 10 figures, submitted to CGO 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamically typed programming languages such as Python and JavaScript defer\ntype checking to run time. VM implementations can improve performance by\neliminating redundant dynamic type checks. However, type inference analyses are\noften costly and involve tradeoffs between compilation time and resulting\nprecision. This has lead to the creation of increasingly complex multi-tiered\nVM architectures.\n  Lazy basic block versioning is a simple JIT compilation technique which\neffectively removes redundant type checks from critical code paths. This novel\napproach lazily generates type-specialized versions of basic blocks on-the-fly\nwhile propagating context-dependent type information. This approach does not\nrequire the use of costly program analyses, is not restricted by the precision\nlimitations of traditional type analyses.\n  This paper extends lazy basic block versioning to propagate type information\ninterprocedurally, across function call boundaries. Our implementation in a\nJavaScript JIT compiler shows that across 26 benchmarks, interprocedural basic\nblock versioning eliminates more type tag tests on average than what is\nachievable with static type analysis without resorting to code transformations.\nOn average, 94.3% of type tag tests are eliminated, yielding speedups of up to\n56%. We also show that our implementation is able to outperform Truffle/JS on\nseveral benchmarks, both in terms of execution time and compilation time.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 01:29:01 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Chevalier-Boisvert", "Maxime", ""], ["Feeley", "Marc", ""]]}, {"id": "1511.03213", "submitter": "Pallavi Maiya", "authors": "Pallavi Maiya (1), Rahul Gupta (1), Aditya Kanade (1), Rupak Majumdar\n  (2) ((1) Indian Institute of Science, (2) MPI-SWS)", "title": "A Partial Order Reduction Technique for Event-driven Multi-threaded\n  Programs", "comments": "35 pages, 20 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-driven multi-threaded programming is fast becoming a preferred style of\ndeveloping efficient and responsive applications. In this concurrency model,\nmultiple threads execute concurrently, communicating through shared objects as\nwell as by posting asynchronous events that are executed in their order of\narrival. In this work, we consider partial order reduction (POR) for\nevent-driven multi-threaded programs. The existing POR techniques treat event\nqueues associated with threads as shared objects and thereby, reorder every\npair of events handled on the same thread even if reordering them does not lead\nto different states. We do not treat event queues as shared objects and propose\na new POR technique based on a novel backtracking set called the\ndependence-covering set. Events handled by the same thread are reordered by our\nPOR technique only if necessary. We prove that exploring dependence-covering\nsets suffices to detect all deadlock cycles and assertion violations defined\nover local variables. To evaluate effectiveness of our POR scheme, we have\nimplemented a dynamic algorithm to compute dependence-covering sets. On\nexecution traces obtained from a few Android applications, we demonstrate that\nour technique explores many fewer transitions ---often orders of magnitude\nfewer--- compared to exploration based on persistent sets, wherein, event\nqueues are considered as shared objects.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 18:22:29 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 09:20:01 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Maiya", "Pallavi", "", "Indian Institute of Science"], ["Gupta", "Rahul", "", "Indian Institute of Science"], ["Kanade", "Aditya", "", "Indian Institute of Science"], ["Majumdar", "Rupak", "", "MPI-SWS"]]}, {"id": "1511.03406", "submitter": "Kimio Kuramitsu", "authors": "Shun Honda, Kimio Kuramitsu", "title": "Implementing a Small Parsing Virtual Machine on Embedded Systems", "comments": "An earlier draft for future submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  PEGs are a formal grammar foundation for describing syntax, and are not hard\nto generate parsers with a plain recursive decent parsing. However, the large\namount of C-stack consumption in the recursive parsing is not acceptable\nespecially in resource-restricted embedded systems. Alternatively, we have\nattempted the machine virtualization approach to PEG-based parsing. MiniNez,\nour implemented virtual machine, is presented in this paper with several\ndownsizing techniques, including instruction specialization, inline expansion\nand static flow analysis. As a result, the MiniNez machine achieves both a very\nsmall footprint and competitive performance to generated C parsers. We have\ndemonstrated the experimental results by comparing on two major embedded\nplatforms: Cortex-A7 and Intel Atom processor.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 07:44:04 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Honda", "Shun", ""], ["Kuramitsu", "Kimio", ""]]}, {"id": "1511.04583", "submitter": "Yanhong Annie Liu", "authors": "Yanhong A. Liu, Jon Brandvein, Scott D. Stoller, Bo Lin", "title": "Demand-Driven Incremental Object Queries", "comments": null, "journal-ref": "PPDP 2016: Proceedings of the 18th International Symposium on\n  Principles and Practice of Declarative Programming, September 2016, Pages\n  228-241. ACM Press", "doi": "10.1145/2967973.2968610", "report-no": null, "categories": "cs.PL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object queries are essential in information seeking and decision making in\nvast areas of applications. However, a query may involve complex conditions on\nobjects and sets, which can be arbitrarily nested and aliased. The objects and\nsets involved as well as the demand---the given parameter values of\ninterest---can change arbitrarily. How to implement object queries efficiently\nunder all possible updates, and furthermore to provide complexity guarantees?\n  This paper describes an automatic method. The method allows powerful queries\nto be written completely declaratively. It transforms demand as well as all\nobjects and sets into relations. Most importantly, it defines invariants for\nnot only the query results, but also all auxiliary values about the objects and\nsets involved, including those for propagating demand, and incrementally\nmaintains all of them. Implementation and experiments with problems from a\nvariety of application areas, including distributed algorithms and\nprobabilistic queries, confirm the analyzed complexities, trade-offs, and\nsignificant improvements over prior work.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 17:27:33 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 03:34:09 GMT"}, {"version": "v3", "created": "Fri, 15 Jul 2016 18:06:30 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Liu", "Yanhong A.", ""], ["Brandvein", "Jon", ""], ["Stoller", "Scott D.", ""], ["Lin", "Bo", ""]]}, {"id": "1511.04846", "submitter": "Bor-Yuh Evan Chang", "authors": "Yi-Fan Tsai, Devin Coughlin, Bor-Yuh Evan Chang, and Xavier Rival", "title": "Synthesizing Short-Circuiting Validation of Data Structure Invariants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents incremental verification-validation, a novel approach for\nchecking rich data structure invariants expressed as separation logic\nassertions. Incremental verification-validation combines static verification of\nseparation properties with efficient, short-circuiting dynamic validation of\narbitrarily rich data constraints. A data structure invariant checker is an\ninductive predicate in separation logic with an executable interpretation; a\nshort-circuiting checker is an invariant checker that stops checking whenever\nit detects at run time that an assertion for some sub-structure has been fully\nproven statically. At a high level, our approach does two things: it statically\nproves the separation properties of data structure invariants using a static\nshape analysis in a standard way but then leverages this proof in a novel\nmanner to synthesize short-circuiting dynamic validation of the data\nproperties. As a consequence, we enable dynamic validation to make up for\nimprecision in sound static analysis while simultaneously leveraging the static\nverification to make the remaining dynamic validation efficient. We show\nempirically that short-circuiting can yield asymptotic improvements in dynamic\nvalidation, with low overhead over no validation, even in cases where static\nverification is incomplete.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 07:26:32 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Tsai", "Yi-Fan", ""], ["Coughlin", "Devin", ""], ["Chang", "Bor-Yuh Evan", ""], ["Rival", "Xavier", ""]]}, {"id": "1511.04926", "submitter": "Elena Giachino", "authors": "Elena Giachino (DISI, FOCUS), Cosimo Laneve (DISI, FOCUS), Michael\n  Lienhardt (FOCUS, DISI)", "title": "A framework for deadlock detection in core ABS", "comments": "Software and Systems Modeling, Springer Verlag, 2015", "journal-ref": null, "doi": "10.1007/s10270-014-0444-y", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for statically detecting deadlocks in a concurrent\nobject-oriented language with asynchronous method calls and cooperative\nscheduling of method activations. Since this language features recursion and\ndynamic resource creation, deadlock detection is extremely complex and\nstate-of-the-art solutions either give imprecise answers or do not scale. In\norder to augment precision and scalability we propose a modular framework that\nallows several techniques to be combined. The basic component of the framework\nis a front-end inference algorithm that extracts abstract behavioural\ndescriptions of methods, called contracts, which retain resource dependency\ninformation. This component is integrated with a number of possible different\nback-ends that analyse contracts and derive deadlock information. As a\nproof-of-concept, we discuss two such back-ends: (i) an evaluator that computes\na fixpoint semantics and (ii) an evaluator using abstract model checking.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 11:54:10 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Giachino", "Elena", "", "DISI, FOCUS"], ["Laneve", "Cosimo", "", "DISI, FOCUS"], ["Lienhardt", "Michael", "", "FOCUS, DISI"]]}, {"id": "1511.05104", "submitter": "Elena Giachino", "authors": "Elena Giachino (DISI, FOCUS), Einar Broch Johnsen, Cosimo Laneve\n  (DISI, FOCUS), Ka I Pun", "title": "Time complexity of concurrent programs", "comments": "FACS 2015, Oct 2015, Niter\\'oi, Rio de Janeiro, Brazil", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of automatically computing the time complexity of\nconcurrent object-oriented programs. To determine this complexity we use\nintermediate abstract descriptions that record relevant information for the\ntime analysis (cost of statements, creations of objects, and concurrent\noperations), called behavioural types. Then, we define a translation function\nthat takes behavioural types and makes the parallelism explicit into so-called\ncost equations, which are fed to an automatic off-the-shelf solver for\nobtaining the time complexity.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 19:48:48 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Giachino", "Elena", "", "DISI, FOCUS"], ["Johnsen", "Einar Broch", "", "DISI, FOCUS"], ["Laneve", "Cosimo", "", "DISI, FOCUS"], ["Pun", "Ka I", ""]]}, {"id": "1511.06459", "submitter": "Ryan Wisnesky", "authors": "Patrick Schultz, David I. Spivak, Ryan Wisnesky", "title": "QINL: Query-integrated Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an alternative solution to the impedance-mismatch problem between\nprogramming and query languages: rather than embed queries in a programming\nlanguage, as done in LINQ systems, we embed programs in a query language, and\ndub the result QINL.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 00:09:02 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Schultz", "Patrick", ""], ["Spivak", "David I.", ""], ["Wisnesky", "Ryan", ""]]}, {"id": "1511.06965", "submitter": "David Darais", "authors": "David Darais, David Van Horn", "title": "Constructive Galois Connections: Taming the Galois Connection Framework\n  for Mechanized Metatheory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Galois connections are a foundational tool for structuring abstraction in\nsemantics and their use lies at the heart of the theory of abstract\ninterpretation. Yet, mechanization of Galois connections remains limited to\nrestricted modes of use, preventing their general application in mechanized\nmetatheory and certified programming.\n  This paper presents constructive Galois connections, a variant of Galois\nconnections that is effective both on paper and in proof assistants; is\ncomplete with respect to a large subset of classical Galois connections; and\nenables more general reasoning principles, including the \"calculational\" style\nadvocated by Cousot.\n  To design constructive Galois connection we identify a restricted mode of use\nof classical ones which is both general and amenable to mechanization in\ndependently-typed functional programming languages. Crucial to our metatheory\nis the addition of monadic structure to Galois connections to control a\n\"specification effect\". Effectful calculations may reason classically, while\npure calculations have extractable computational content. Explicitly moving\nbetween the worlds of specification and implementation is enabled by our\nmetatheory.\n  To validate our approach, we provide two case studies in mechanizing existing\nproofs from the literature: one uses calculational abstract interpretation to\ndesign a static analyzer, the other forms a semantic basis for gradual typing.\nBoth mechanized proofs closely follow their original paper-and-pencil\ncounterparts, employ reasoning principles not captured by previous\nmechanization approaches, support the extraction of verified algorithms, and\nare novel.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 04:55:17 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 20:39:13 GMT"}, {"version": "v3", "created": "Thu, 28 Jul 2016 20:54:23 GMT"}, {"version": "v4", "created": "Wed, 26 Oct 2016 17:29:30 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Darais", "David", ""], ["Van Horn", "David", ""]]}, {"id": "1511.06968", "submitter": "Raghu Prabhakar", "authors": "Raghu Prabhakar, David Koeplinger, Kevin Brown, HyoukJoong Lee,\n  Christopher De Sa, Christos Kozyrakis, Kunle Olukotun", "title": "Generating Configurable Hardware from Parallel Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years the computing landscape has seen an in- creasing shift\ntowards specialized accelerators. Field pro- grammable gate arrays (FPGAs) are\nparticularly promising as they offer significant performance and energy\nimprovements compared to CPUs for a wide class of applications and are far more\nflexible than fixed-function ASICs. However, FPGAs are difficult to program.\nTraditional programming models for reconfigurable logic use low-level hardware\ndescription languages like Verilog and VHDL, which have none of the pro-\nductivity features of modern software development languages but produce very\nefficient designs, and low-level software lan- guages like C and OpenCL coupled\nwith high-level synthesis (HLS) tools that typically produce designs that are\nfar less efficient. Functional languages with parallel patterns are a better\nfit for hardware generation because they both provide high-level abstractions\nto programmers with little experience in hard- ware design and avoid many of\nthe problems faced when gen- erating hardware from imperative languages. In\nthis paper, we identify two optimizations that are important when using par-\nallel patterns to generate hardware: tiling and metapipelining. We present a\ngeneral representation of tiled parallel patterns, and provide rules for\nautomatically tiling patterns and gen- erating metapipelines. We demonstrate\nexperimentally that these optimizations result in speedups up to 40x on a set\nof benchmarks from the data analytics domain.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 05:57:27 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Prabhakar", "Raghu", ""], ["Koeplinger", "David", ""], ["Brown", "Kevin", ""], ["Lee", "HyoukJoong", ""], ["De Sa", "Christopher", ""], ["Kozyrakis", "Christos", ""], ["Olukotun", "Kunle", ""]]}, {"id": "1511.07033", "submitter": "Andrew Kent", "authors": "Andrew M. Kent, David Kempe, Sam Tobin-Hochstadt", "title": "Occurrence Typing Modulo Theories", "comments": null, "journal-ref": "SIGPLAN Not. 51, 6 (June 2016), 296-309", "doi": "10.1145/2980983.2908091", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new type system combining occurrence typing, previously used to\ntype check programs in dynamically-typed languages such as Racket, JavaScript,\nand Ruby, with dependent refinement types. We demonstrate that the addition of\nrefinement types allows the integration of arbitrary solver-backed reasoning\nabout logical propositions from external theories. By building on occurrence\ntyping, we can add our enriched type system as an extension of Typed\nRacket---adding dependency and refinement reuses the existing formalism while\nincreasing its expressiveness.\n  Dependent refinement types allow Typed Racket programmers to express rich\ntype relationships, ranging from data structure invariants such as red-black\ntree balance to preconditions such as vector bounds. Refinements allow\nprogrammers to embed the propositions that occurrence typing in Typed Racket\nalready reasons about into their types. Further, extending occurrence typing to\nrefinements allows us to make the underlying formalism simpler and more\npowerful.\n  In addition to presenting the design of our system, we present a formal model\nof the system, show how to integrate it with theories over both linear\narithmetic and bitvectors, and evaluate the system in the context of the full\nTyped Racket implementation. Specifically, we take safe vector access as a case\nstudy, and examine all vector accesses in a 56,000 line corpus of Typed Racket\nprograms. Our system is able to prove that 50% of these are safe with no new\nannotation, and with a few annotations and modifications, we can capture close\nto 80%.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 16:54:32 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2016 17:57:24 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Kent", "Andrew M.", ""], ["Kempe", "David", ""], ["Tobin-Hochstadt", "Sam", ""]]}, {"id": "1511.07163", "submitter": "Thorsten Tarrach", "authors": "Pavol \\v{C}ern\\'y, Edmund M. Clarke, Thomas A. Henzinger, Arjun\n  Radhakrishna, Leonid Ryzhyk, Roopsha Samanta, Thorsten Tarrach", "title": "Optimizing Solution Quality in Synchronization Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a multithreaded program written assuming a friendly, non-preemptive\nscheduler, the goal of synchronization synthesis is to automatically insert\nsynchronization primitives to ensure that the modified program behaves\ncorrectly, even with a preemptive scheduler. In this work, we focus on the\nquality of the synthesized solution: we aim to infer synchronization placements\nthat not only ensure correctness, but also meet some quantitative objectives\nsuch as optimal program performance on a given computing platform.\n  The key step that enables solution optimization is the construction of a set\nof global constraints over synchronization placements such that each model of\nthe constraints set corresponds to a correctness-ensuring synchronization\nplacement. We extract the global constraints from generalizations of\ncounterexample traces and the control-flow graph of the program. The global\nconstraints enable us to choose from among the encoded synchronization\nsolutions using an objective function. We consider two types of objective\nfunctions: ones that are solely dependent on the program (e.g., minimizing the\nsize of critical sections) and ones that are also dependent on the computing\nplatform. For the latter, given a program and a computing platform, we\nconstruct a performance model based on measuring average contention for\ncritical sections and the average time taken to acquire and release a lock\nunder a given average contention.\n  We empirically evaluated that our approach scales to typical module sizes of\nmany real world concurrent programs such as device drivers and multithreaded\nservers, and that the performance predictions match reality. To the best of our\nknowledge, this is the first comprehensive approach for optimizing the\nplacement of synthesized synchronization.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 10:24:30 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["\u010cern\u00fd", "Pavol", ""], ["Clarke", "Edmund M.", ""], ["Henzinger", "Thomas A.", ""], ["Radhakrishna", "Arjun", ""], ["Ryzhyk", "Leonid", ""], ["Samanta", "Roopsha", ""], ["Tarrach", "Thorsten", ""]]}, {"id": "1511.07267", "submitter": "Duc-Hiep Chu", "authors": "Duc-Hiep Chu, Joxan Jaffar", "title": "Automatic Reasoning on Recursive Data-Structures with Sharing", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of automatically verifying programs which manipulate\narbitrary data structures. Our specification language is expressive, contains a\nnotion of \\emph{separation}, and thus enables a precise specification of\n\\emph{frames}. The main contribution then is a program verification method\nwhich combines strongest postcondition reasoning in the form symbolic\nexecution, unfolding recursive definitions of the data structure in question,\nand a new frame rule to achieve \\emph{local reasoning} so that proofs can be\ncompositional. Finally, we present an implementation of our verifier, and\ndemonstrate automation on a number of representative programs. In particular,\nwe present the first automatic proof of a classic graph marking algorithm,\npaving the way for dealing with a class of programs which traverse a complex\ndata structure.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 15:18:19 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 12:45:50 GMT"}, {"version": "v3", "created": "Wed, 15 Nov 2017 14:36:17 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Chu", "Duc-Hiep", ""], ["Jaffar", "Joxan", ""]]}, {"id": "1511.08049", "submitter": "Sarmen Keshishzadeh", "authors": "Sarmen Keshishzadeh and Arjan J. Mooij and Jozef Hooman", "title": "Industrial Experiences with a Formal DSL Semantics to Check Correctness\n  of DSL Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A domain specific language (DSL) abstracts from implementation details and is\naligned with the way domain experts reason about a software component. The\ndevelopment of DSLs is usually centered around a grammar and transformations\nthat generate implementation code or analysis models. The semantics of the\nlanguage is often defined implicitly and in terms of a transformation to\nimplementation code. In the presence of multiple transformations from the DSL,\nthe consistency of the generated artifacts with respect to the semantics of the\nDSL is a relevant issue. We show that a formal semantics is essential for\nchecking the consistency between the generated artifacts. We exploit the formal\nsemantics in an industrial project and use formal techniques based on\nequivalence checking and model-based testing for consistency checking. We\nreport about our experience with this approach in an industrial development\nproject.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 13:09:20 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Keshishzadeh", "Sarmen", ""], ["Mooij", "Arjan J.", ""], ["Hooman", "Jozef", ""]]}, {"id": "1511.08307", "submitter": "Kimio Kuramitsu", "authors": "Kimio Kuramitsu", "title": "Nez: practical open grammar language", "comments": "unpublished draft work (11 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nez is a PEG(Parsing Expressing Grammar)-based open grammar language that\nallows us to describe complex syntax constructs without action code. Since open\ngrammars are declarative and free from a host programming language of parsers,\nsoftware engineering tools and other parser applications can reuse once-defined\ngrammars across programming languages.\n  A key challenge to achieve practical open grammars is the expressiveness of\nsyntax constructs and the resulting parser performance, as the traditional\naction code approach has provided very pragmatic solutions to these two issues.\nIn Nez, we extend the symbol-based state management to recognize\ncontext-sensitive language syntax, which often appears in major programming\nlanguages. In addition, the Abstract Syntax Tree constructor allows us to make\nflexible tree structures, including the left-associative pair of trees. Due to\nthese extensions, we have demonstrated that Nez can parse not all but many\ngrammars.\n  Nez can generate various types of parsers since all Nez operations are\nindependent of a specific parser language. To highlight this feature, we have\nimplemented Nez with dynamic parsing, which allows users to integrate a Nez\nparser as a parser library that loads a grammar at runtime. To achieve its\npractical performance, Nez operators are assembled into low-level virtual\nmachine instructions, including automated state modifications when\nbacktracking, transactional controls of AST construction, and efficient\nmemoization in packrat parsing. We demonstrate that Nez dynamic parsers achieve\nvery competitive performance compared to existing efficient parser generators.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 07:37:10 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Kuramitsu", "Kimio", ""]]}, {"id": "1511.08414", "submitter": "Kimio Kuramitsu", "authors": "Tetsuro Matsumura, Kimio Kuramitsu", "title": "A declarative extension of parsing expression grammars for recognizing\n  most programming languages", "comments": "To appear in Journal of Information Processing, 24(2), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parsing Expression Grammars are a popular foundation for describing syntax.\nUnfortunately, several syntax of programming languages are still hard to\nrecognize with pure PEGs. Notorious cases appears: typedef-defined names in\nC/C++, indentation-based code layout in Python, and HERE document in many\nscripting languages. To recognize such PEG-hard syntax, we have addressed a\ndeclarative extension to PEGs. The \"declarative\" extension means no programmed\nsemantic actions, which are traditionally used to realize the extended parsing\nbehavior. Nez is our extended PEG language, including symbol tables and\nconditional parsing. This paper demonstrates that the use of Nez Extensions can\nrealize many practical programming languages, such as C, C\\#, Ruby, and Python,\nwhich involve PEG-hard syntax.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 15:15:47 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Matsumura", "Tetsuro", ""], ["Kuramitsu", "Kimio", ""]]}]