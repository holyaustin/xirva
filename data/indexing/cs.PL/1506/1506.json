[{"id": "1506.01573", "submitter": "Lance Williams", "authors": "Lance R. Williams", "title": "Programs as Polypeptides", "comments": "in European Conference on Artificial Life (ECAL '15), York, UK, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.ET cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a visual programming language for defining behaviors manifested\nby reified actors in a 2D virtual world that can be compiled into programs\ncomprised of sequences of combinators that are themselves reified as actors.\nThis makes it possible to build programs that build programs from components of\na few fixed types delivered by diffusion using processes that resemble\nchemistry as much as computation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 13:08:04 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Williams", "Lance R.", ""]]}, {"id": "1506.01949", "submitter": "Norman Danner", "authors": "Norman Danner and Daniel R. Licata and Ramyaa Ramyaa", "title": "Denotational cost semantics for functional languages with inductive\n  types", "comments": "To appear in ICFP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central method for analyzing the asymptotic complexity of a functional\nprogram is to extract and then solve a recurrence that expresses evaluation\ncost in terms of input size. The relevant notion of input size is often\nspecific to a datatype, with measures including the length of a list, the\nmaximum element in a list, and the height of a tree. In this work, we give a\nformal account of the extraction of cost and size recurrences from higher-order\nfunctional programs over inductive datatypes. Our approach allows a wide range\nof programmer-specified notions of size, and ensures that the extracted\nrecurrences correctly predict evaluation cost. To extract a recurrence from a\nprogram, we first make costs explicit by applying a monadic translation from\nthe source language to a complexity language, and then abstract datatype values\nas sizes. Size abstraction can be done semantically, working in models of the\ncomplexity language, or syntactically, by adding rules to a preorder judgement.\nWe give several different models of the complexity language, which support\ndifferent notions of size. Additionally, we prove by a logical relations\nargument that recurrences extracted by this process are upper bounds for\nevaluation cost; the proof is entirely syntactic and therefore applies to all\nof the models we consider.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 15:57:26 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Danner", "Norman", ""], ["Licata", "Daniel R.", ""], ["Ramyaa", "Ramyaa", ""]]}, {"id": "1506.02367", "submitter": "Pierre Lescanne", "authors": "Maciej Bendkowski (TCS), Katarzyna Grygiel (TCS), Pierre Lescanne\n  (TCS, LIP), Marek Zaionc (TCS)", "title": "A natural counting of lambda terms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DM cs.PL math.CO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the sequences of numbers corresponding to lambda terms of given\nsizes, where the size is this of lambda terms with de Bruijn indices in a very\nnatural model where all the operators have size 1. For plain lambda terms, the\nsequence corresponds to two families of binary trees for which we exhibit\nbijections. We study also the distribution of normal forms, head normal forms\nand strongly normalizing terms. In particular we show that strongly normalizing\nterms are of density 0 among plain terms.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 06:57:41 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2015 08:19:29 GMT"}, {"version": "v3", "created": "Tue, 17 May 2016 08:30:30 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Bendkowski", "Maciej", "", "TCS"], ["Grygiel", "Katarzyna", "", "TCS"], ["Lescanne", "Pierre", "", "TCS, LIP"], ["Zaionc", "Marek", "", "TCS"]]}, {"id": "1506.02833", "submitter": "Albert Saa-Garriga", "authors": "Albert Sa\\`a-Garriga and David Castells-Rufas and Jordi Carrabina", "title": "OMP2HMPP: Compiler Framework for Energy Performance Trade-off Analysis\n  of Automatically Generated Codes", "comments": null, "journal-ref": "IJCSI International Journal of Computer Science Issues, Volume 12,\n  Issue 2, March 2015", "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present OMP2HMPP, a tool that, in a first step, automatically translates\nOpenMP code into various possible transformations of HMPP. In a second step\nOMP2HMPP executes all variants to obtain the performance and power consumption\nof each transformation. The resulting trade-off can be used to choose the more\nconvenient version. After running the tool on a set of codes from the Polybench\nbenchmark we show that the best automatic transformation is equivalent to a\nmanual one done by an expert. Compared with original OpenMP code running in 2\nquad-core processors we obtain an average speed-up of 31x and 5.86x factor in\noperations per watt.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 09:24:38 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Sa\u00e0-Garriga", "Albert", ""], ["Castells-Rufas", "David", ""], ["Carrabina", "Jordi", ""]]}, {"id": "1506.03032", "submitter": "Hui Xu", "authors": "Hui Xu, Yangfan Zhou, Michael R. Lyu", "title": "N-Version Obfuscation: Impeding Software Tampering Replication with\n  Program Diversity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tamper-resistance is a fundamental software security research area. Many\napproaches have been proposed to thwart specific procedures of tampering, e.g.,\nobfuscation and self-checksumming. However, to our best knowledge, none of them\ncan achieve theoretically tamper-resistance. Our idea is to impede the\nreplication of tampering via program diversification, and thus increasing the\ncomplexity to break the whole software system. To this end, we propose to\ndeliver same featured, but functionally nonequivalent software copies to\ndifferent machines. We formally define the problem as N-version obfuscation,\nand provide a viable means to solve the problem. Our evaluation result shows\nthat the time required for breaking a software system is linearly increased\nwith the number of software versions, which is O(n) complexity.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 10:10:47 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Xu", "Hui", ""], ["Zhou", "Yangfan", ""], ["Lyu", "Michael R.", ""]]}, {"id": "1506.03363", "submitter": "Tony Clark", "authors": "Tony Clark, Paul Sammut, James Willans", "title": "Super-Languages: Developing Languages and Applications with XMF (Second\n  Edition)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this book is to introduce the language XMF. This is done by\ndefining the language, providing some examples of applications that can be\nwritten directly in the XOCL language that comes with XMF, and then by showing\nhow XMF can be used for language engineering. The main focus of this book is on\nlanguage engineering by example.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 15:34:58 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Clark", "Tony", ""], ["Sammut", "Paul", ""], ["Willans", "James", ""]]}, {"id": "1506.03366", "submitter": "Tony Clark", "authors": "Tony Clark", "title": "Processing XML for Domain Specific Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  XML is a standard and universal language for representing information. XML\nprocessing is supported by two key frameworks: DOM and SAX. SAX is efficient,\nbut leaves the developer to encode much of the processing. This paper\nintroduces a language for expressing XML-based languages via grammars that can\nbe used to process XML documents and synthesize arbitrary values. The language\nis declarative and shields the developer from SAX implementation details. The\nlanguage is specified and an efficient implementation is defined as an abstract\nmachine.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 15:47:15 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Clark", "Tony", ""]]}, {"id": "1506.03380", "submitter": "Tony Clark", "authors": "Tony Clark, Dean Kramer, and Samia Oussena", "title": "Model Driven Reactive Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reactive applications (rapps) are of interest because of the explosion of\nmobile, tablet and web-based platforms. The complexity and proliferation of\nimplementation technologies makes it attractive to use model-driven techniques\nto develop rapp systems. This article proposes a domain specific language for\nrapps consisting of stereotyped class models for the structure of the\napplication and state machine models for the application behaviour. The models\nare given a semantics in terms of a transformation to a calculus called Widget.\nThe languages are introduced using an example application for mobile phones.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 16:27:01 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Clark", "Tony", ""], ["Kramer", "Dean", ""], ["Oussena", "Samia", ""]]}, {"id": "1506.03950", "submitter": "Abhishek Bichhawat", "authors": "Abhishek Bichhawat, Vineet Rajani, Deepak Garg, Christian Hammer", "title": "Generalizing Permissive-Upgrade in Dynamic Information Flow Analysis", "comments": null, "journal-ref": null, "doi": "10.1145/2637113.2637116", "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preventing implicit information flows by dynamic program analysis requires\ncoarse approximations that result in false positives, because a dynamic monitor\nsees only the executed trace of the program. One widely deployed method is the\nno-sensitive-upgrade check, which terminates a program whenever a variable's\ntaint is upgraded (made more sensitive) due to a control dependence on tainted\ndata. Although sound, this method is restrictive, e.g., it terminates the\nprogram even if the upgraded variable is never used subsequently. To counter\nthis, Austin and Flanagan introduced the permissive-upgrade check, which allows\na variable upgrade due to control dependence, but marks the variable\n\"partially-leaked\". The program is stopped later if it tries to use the\npartially-leaked variable. Permissive-upgrade handles the dead-variable\nassignment problem and remains sound. However, Austin and Flanagan develop\npermissive-upgrade only for a two-point (low-high) security lattice and\nindicate a generalization to pointwise products of such lattices. In this\npaper, we develop a non-trivial and non-obvious generalization of\npermissive-upgrade to arbitrary lattices. The key difficulty lies in finding a\nsuitable notion of partial leaks that is both sound and permissive and in\ndeveloping a suitable definition of memory equivalence that allows an inductive\nproof of soundness.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 09:25:44 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 12:26:39 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Bichhawat", "Abhishek", ""], ["Rajani", "Vineet", ""], ["Garg", "Deepak", ""], ["Hammer", "Christian", ""]]}, {"id": "1506.04161", "submitter": "David Monniaux", "authors": "David Monniaux (VERIMAG - IMAG), Francesco Alberti", "title": "A simple abstraction of arrays and maps by program translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for the static analysis of programs handling arrays,\nwith a Galois connection between the semantics of the array program and\nsemantics of purely scalar operations. The simplest way to implement it is by\nautomatic, syntactic transformation of the array program into a scalar program\nfollowed analysis of the scalar program with any static analysis technique\n(abstract interpretation, acceleration, predicate abstraction,.. .). The\nscalars invariants thus obtained are translated back onto the original program\nas universally quantified array invariants. We illustrate our approach on a\nvariety of examples, leading to the \" Dutch flag \" algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 20:12:57 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Monniaux", "David", "", "VERIMAG - IMAG"], ["Alberti", "Francesco", ""]]}, {"id": "1506.04205", "submitter": "\\'Eric Tanter", "authors": "\\'Eric Tanter and Nicolas Tabareau", "title": "Gradual Certified Programming in Coq", "comments": "DLS'15 final version, Proceedings of the ACM Dynamic Languages\n  Symposium (DLS 2015)", "journal-ref": null, "doi": "10.1145/2816707.2816710", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expressive static typing disciplines are a powerful way to achieve\nhigh-quality software. However, the adoption cost of such techniques should not\nbe under-estimated. Just like gradual typing allows for a smooth transition\nfrom dynamically-typed to statically-typed programs, it seems desirable to\nsupport a gradual path to certified programming. We explore gradual certified\nprogramming in Coq, providing the possibility to postpone the proofs of\nselected properties, and to check \"at runtime\" whether the properties actually\nhold. Casts can be integrated with the implicit coercion mechanism of Coq to\nsupport implicit cast insertion a la gradual typing. Additionally, when\nextracting Coq functions to mainstream languages, our encoding of casts\nsupports lifting assumed properties into runtime checks. Much to our surprise,\nit is not necessary to extend Coq in any way to support gradual certified\nprogramming. A simple mix of type classes and axioms makes it possible to bring\ngradual certified programming to Coq in a straightforward manner.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2015 00:45:35 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2015 12:51:40 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Tanter", "\u00c9ric", ""], ["Tabareau", "Nicolas", ""]]}, {"id": "1506.04498", "submitter": "Satoshi Egi", "authors": "Satoshi Egi", "title": "Egison: Non-Linear Pattern-Matching against Non-Free Data Types", "comments": "9 pages. arXiv admin note: text overlap with arXiv:1407.0729", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Egison programming language whose feature is strong\npattern-matching facility against not only algebraic data types but also\nnon-free data types whose data have multiple ways of representation such as\nsets and graphs. Our language supports multiple occurrences of the same\nvariables in a pattern, multiple results of pattern-matching, polymorphism of\npattern-constructors and loop-patterns, patterns that contain \"and-so-forth\"\nwhose repeat count can be changed by the parameter. This paper proposes the way\nto design expressions that have all these features and demonstrates how these\nfeatures are useful to express programs concise. Egison has already implemented\nin Haskell.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 07:48:14 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Egi", "Satoshi", ""]]}, {"id": "1506.04857", "submitter": "Keehang Kwon", "authors": "Keehang Kwon", "title": "Mutually Exclusive Modules in Logic Programming", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logic programming has traditionally lacked devices for expressing mutually\nexclusive modules. We address this limitation by adopting choice-conjunctive\nmodules of the form $D_0 \\& D_1$ where $D_0, D_1$ are a conjunction of Horn\nclauses and $\\&$ is a linear logic connective. Solving a goal $G$ using $D_0 \\&\nD_1$ -- $exec(D_0 \\& D_1,G)$ -- has the following operational semantics:\n$choose$ a successful one between $exec(D_0,G)$ and $exec(D_1,G)$. In other\nwords, if $D_0$ is chosen in the course of solving $G$, then $D_1$ will be\ndiscarded and vice versa. Hence, the class of choice-conjunctive modules can\ncapture the notion of mutually exclusive modules.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 07:09:05 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Kwon", "Keehang", ""]]}, {"id": "1506.04945", "submitter": "Mehul Bhatt", "authors": "Carl Schultz and Mehul Bhatt", "title": "Spatial Symmetry Driven Pruning Strategies for Efficient Declarative\n  Spatial Reasoning", "comments": "22 pages. Accepted for publication at: COSIT 2015 - Conference on\n  Spatial Information Theory XII (COSIT), Santa Fe, New Mexico, USA ,October\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Declarative spatial reasoning denotes the ability to (declaratively) specify\nand solve real-world problems related to geometric and qualitative spatial\nrepresentation and reasoning within standard knowledge representation and\nreasoning (KR) based methods (e.g., logic programming and derivatives). One\napproach for encoding the semantics of spatial relations within a declarative\nprogramming framework is by systems of polynomial constraints. However, solving\nsuch constraints is computationally intractable in general (i.e. the theory of\nreal-closed fields).\n  We present a new algorithm, implemented within the declarative spatial\nreasoning system CLP(QS), that drastically improves the performance of deciding\nthe consistency of spatial constraint graphs over conventional polynomial\nencodings. We develop pruning strategies founded on spatial symmetries that\nform equivalence classes (based on affine transformations) at the qualitative\nspatial level. Moreover, pruning strategies are themselves formalised as\nknowledge about the properties of space and spatial symmetries. We evaluate our\nalgorithm using a range of benchmarks in the class of contact problems, and\nproofs in mereology and geometry. The empirical results show that CLP(QS) with\nknowledge-based spatial pruning outperforms conventional polynomial encodings\nby orders of magnitude, and can thus be applied to problems that are otherwise\nunsolvable in practice.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 12:40:30 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Schultz", "Carl", ""], ["Bhatt", "Mehul", ""]]}, {"id": "1506.05270", "submitter": "Aggelos Biboudis", "authors": "Aggelos Biboudis, George Fourtounis, Yannis Smaragdakis", "title": "jUCM: Universal Class Morphing (position paper)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend prior work on class-morphing to provide a more expressive\npattern-based compile-time reflection language. Our MorphJ language offers a\ndisciplined form of metaprogramming that produces types by statically iterating\nover and pattern-matching on fields and methods of other types. We expand such\ncapabilities with \"universal morphing\", which also allows pattern-matching over\ntypes (e.g., all classes nested in another, all supertypes of a class) while\nmaintaining modular type safety for our meta-programs. We present informal\nexamples of the functionality and discuss a design for adding universal\nmorphing to Java.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 10:28:41 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Biboudis", "Aggelos", ""], ["Fourtounis", "George", ""], ["Smaragdakis", "Yannis", ""]]}, {"id": "1506.05893", "submitter": "Daniel Bundala", "authors": "Daniel Bundala, Sanjit A. Seshia", "title": "On Systematic Testing for Execution-Time Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a program and a time deadline, does the program finish before the\ndeadline when executed on a given platform? With the requirement to produce a\ntest case when such a violation can occur, we refer to this problem as the\nworst-case execution-time testing (WCETT) problem.\n  In this paper, we present an approach for solving the WCETT problem for\nloop-free programs by timing the execution of a program on a small number of\ncarefully calculated inputs. We then create a sequence of integer linear\nprograms the solutions of which encode the best timing model consistent with\nthe measurements. By solving the programs we can find the worst-case input as\nwell as estimate execution time of any other input. Our solution is more\naccurate than previous approaches and, unlikely previous work, by increasing\nthe number of measurements we can produce WCETT bounds up to any desired\naccuracy.\n  Timing of a program depends on the properties of the platform it executes on.\nWe further show how our approach can be used to quantify the timing\nrepeatability of the underlying platform.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 07:37:01 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Bundala", "Daniel", ""], ["Seshia", "Sanjit A.", ""]]}, {"id": "1506.06378", "submitter": "Nate Foster", "authors": "Steffen Smolka, Spiridon Eliopoulos, Nate Foster, Arjun Guha", "title": "A Fast Compiler for NetKAT", "comments": null, "journal-ref": "ACM SIGPLAN Notices - ICFP '15, Volume 50 Issue 9, September 2015,\n  Pages 328-341", "doi": "10.1145/2784731.2784761", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-level programming languages play a key role in a growing number of\nnetworking platforms, streamlining application development and enabling precise\nformal reasoning about network behavior. Unfortunately, current compilers only\nhandle \"local\" programs that specify behavior in terms of hop-by-hop forwarding\nbehavior, or modest extensions such as simple paths. To encode richer \"global\"\nbehaviors, programmers must add extra state -- something that is tricky to get\nright and makes programs harder to write and maintain. Making matters worse,\nexisting compilers can take tens of minutes to generate the forwarding state\nfor the network, even on relatively small inputs. This forces programmers to\nwaste time working around performance issues or even revert to using\nhardware-level APIs.\n  This paper presents a new compiler for the NetKAT language that handles rich\nfeatures including regular paths and virtual networks, and yet is several\norders of magnitude faster than previous compilers. The compiler uses symbolic\nautomata to calculate the extra state needed to implement \"global\" programs,\nand an intermediate representation based on binary decision diagrams to\ndramatically improve performance. We describe the design and implementation of\nthree essential compiler stages: from virtual programs (which specify behavior\nin terms of virtual topologies) to global programs (which specify network-wide\nbehavior in terms of physical topologies), from global programs to local\nprograms (which specify behavior in terms of single-switch behavior), and from\nlocal programs to hardware-level forwarding tables. We present results from\nexperiments on real-world benchmarks that quantify performance in terms of\ncompilation time and forwarding table size.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2015 15:37:27 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2015 15:52:52 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Smolka", "Steffen", ""], ["Eliopoulos", "Spiridon", ""], ["Foster", "Nate", ""], ["Guha", "Arjun", ""]]}, {"id": "1506.07635", "submitter": "Chinmay Narayan", "authors": "Chinmay Narayan, Subodh Sharma, Shibashis Guha, S.Arun-Kumar", "title": "From Traces To Proofs: Proving Concurrent Program Safe", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nondeterminism in scheduling is the cardinal reason for difficulty in proving\ncorrectness of concurrent programs. A powerful proof strategy was recently\nproposed [6] to show the correctness of such programs. The approach captured\ndata-flow dependencies among the instructions of an interleaved and error-free\nexecution of threads. These data-flow dependencies were represented by an\ninductive data-flow graph (iDFG), which, in a nutshell, denotes a set of\nexecutions of the concurrent program that gave rise to the discovered data-flow\ndependencies. The iDFGs were further transformed in to alternative finite\nautomatons (AFAs) in order to utilize efficient automata-theoretic tools to\nsolve the problem. In this paper, we give a novel and efficient algorithm to\ndirectly construct AFAs that capture the data-flow dependencies in a concurrent\nprogram execution. We implemented the algorithm in a tool called ProofTraPar to\nprove the correctness of finite state cyclic programs under the sequentially\nconsistent memory model. Our results are encouranging and compare favorably to\nexisting state-of-the-art tools.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 06:56:00 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2016 13:15:12 GMT"}, {"version": "v3", "created": "Thu, 28 Apr 2016 08:40:24 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Narayan", "Chinmay", ""], ["Sharma", "Subodh", ""], ["Guha", "Shibashis", ""], ["Arun-Kumar", "S.", ""]]}, {"id": "1506.07813", "submitter": "Arjun Guha", "authors": "Joe Gibbs Politz, Spiridon Eliopoulos, Arjun Guha, and Shriram\n  Krishnamurthi", "title": "ADsafety: Type-Based Verification of JavaScript Sandboxing", "comments": "in Proceedings of the USENIX Security Symposium (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web sites routinely incorporate JavaScript programs from several sources into\na single page. These sources must be protected from one another, which requires\nrobust sandboxing. The many entry-points of sandboxes and the subtleties of\nJavaScript demand robust verification of the actual sandbox source. We use a\nnovel type system for JavaScript to encode and verify sandboxing properties.\nThe resulting verifier is lightweight and efficient, and operates on actual\nsource. We demonstrate the effectiveness of our technique by applying it to\nADsafe, which revealed several bugs and other weaknesses.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 16:59:52 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Politz", "Joe Gibbs", ""], ["Eliopoulos", "Spiridon", ""], ["Guha", "Arjun", ""], ["Krishnamurthi", "Shriram", ""]]}]