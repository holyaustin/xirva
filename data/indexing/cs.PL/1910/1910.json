[{"id": "1910.00241", "submitter": "Andreas Pavlogiannis", "authors": "Krishnendu Chatterjee, Bhavya Choudhary, Andreas Pavlogiannis", "title": "Optimal Dyck Reachability for Data-Dependence and Alias Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental algorithmic problem at the heart of static analysis is Dyck\nreachability. The input is a graph where the edges are labeled with different\ntypes of opening and closing parentheses, and the reachability information is\ncomputed via paths whose parentheses are properly matched. We present new\nresults for Dyck reachability problems with applications to alias analysis and\ndata-dependence analysis. Our main contributions, that include improved upper\nbounds as well as lower bounds that establish optimality guarantees, are as\nfollows.\n  First, we consider Dyck reachability on bidirected graphs, which is the\nstandard way of performing field-sensitive points-to analysis. Given a\nbidirected graph with $n$ nodes and $m$ edges, we present: (i)~an algorithm\nwith worst-case running time $O(m + n \\cdot \\alpha(n))$, where $\\alpha(n)$ is\nthe inverse Ackermann function, improving the previously known $O(n^2)$ time\nbound; (ii)~a matching lower bound that shows that our algorithm is optimal wrt\nto worst-case complexity; and (iii)~an optimal average-case upper bound of\n$O(m)$ time, improving the previously known $O(m \\cdot \\log n)$ bound.\n  Second, we consider the problem of context-sensitive data-dependence\nanalysis, where the task is to obtain analysis summaries of library code in the\npresence of callbacks. Our algorithm preprocesses libraries in almost linear\ntime, after which the contribution of the library in the complexity of the\nclient analysis is only linear, and only wrt the number of call sites.\n  Third, we prove that combinatorial algorithms for Dyck reachability on\ngeneral graphs with truly sub-cubic bounds cannot be obtained without obtaining\nsub-cubic combinatorial algorithms for Boolean Matrix Multiplication, which is\na long-standing open problem. We also show that the same hardness holds for\ngraphs of constant treewidth.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 08:08:58 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Choudhary", "Bhavya", ""], ["Pavlogiannis", "Andreas", ""]]}, {"id": "1910.00577", "submitter": "Uri Alon", "authors": "Uri Alon, Roy Sadaka, Omer Levy, Eran Yahav", "title": "Structural Language Models of Code", "comments": "Appeared in ICML'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of any-code completion - generating a missing piece of\nsource code in a given program without any restriction on the vocabulary or\nstructure. We introduce a new approach to any-code completion that leverages\nthe strict syntax of programming languages to model a code snippet as a tree -\nstructural language modeling (SLM). SLM estimates the probability of the\nprogram's abstract syntax tree (AST) by decomposing it into a product of\nconditional probabilities over its nodes. We present a neural model that\ncomputes these conditional probabilities by considering all AST paths leading\nto a target node. Unlike previous techniques that have severely restricted the\nkinds of expressions that can be generated in this task, our approach can\ngenerate arbitrary code in any programming language. Our model significantly\noutperforms both seq2seq and a variety of structured approaches in generating\nJava and C# code. Our code, data, and trained models are available at\nhttp://github.com/tech-srl/slm-code-generation/ . An online demo is available\nat http://AnyCodeGen.org .\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 18:54:07 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 09:07:27 GMT"}, {"version": "v3", "created": "Thu, 25 Jun 2020 09:04:07 GMT"}, {"version": "v4", "created": "Wed, 29 Jul 2020 12:15:33 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Alon", "Uri", ""], ["Sadaka", "Roy", ""], ["Levy", "Omer", ""], ["Yahav", "Eran", ""]]}, {"id": "1910.00709", "submitter": "Isaac Oscar Gariano", "authors": "Isaac Oscar Gariano and Marco Servetto", "title": "M{\\mu}l: The Power of Dynamic Multi-Methods", "comments": "Presented at Workshop on Meta-Programming Techniques and Reflection\n  (META'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-methods are a straightforward extension of traditional (single) dynamic\ndispatch, which is the core of most object oriented languages. With\nmulti-methods, a method call will select an appropriate implementation based on\nthe values of multiple arguments, and not just the first/receiver. Language\nsupport for both single and multiple dispatch is typically designed to be used\nin conjunction with other object oriented features, in particular classes and\ninheritance. But are these extra features really necessary?\n  M{\\mu}l is a dynamic language designed to be as simple as possible but still\nsupporting flexible abstraction and polymorphism. M{\\mu}l provides only two\nforms of abstraction: (object) identities and (multi) methods. In M{\\mu}l\nmethod calls are dispatched based on the identity of arguments, as well as what\nother methods are defined on them. In order to keep M{\\mu}ls design simple,\nwhen multiple method definitions are applicable, the most recently defined one\nis chosen, not the most specific (as is conventional with dynamic dispatch).\n  In this paper we show how by defining methods at runtime, we obtain much of\nthe power of classes and meta object protocols, in particular the ability to\ndynamically modify the state and behaviour of 'classes' of objects.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2019 23:07:51 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Gariano", "Isaac Oscar", ""], ["Servetto", "Marco", ""]]}, {"id": "1910.00905", "submitter": "Dan Frumin", "authors": "Dan Frumin, Robbert Krebbers, Lars Birkedal", "title": "Compositional Non-Interference for Fine-Grained Concurrent Programs", "comments": "New example in Section 2 has been added. A simpler security condition\n  is now used in Section 3. Major editing in Sections 1, 2, and 7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-interference is a program property that ensures the absence of\ninformation leaks. In the context of programming languages, there exist two\ncommon approaches for establishing non-interference: type systems and program\nlogics. Type systems provide strong automation (by means of type checking), but\nthey are inherently restrictive in the kind of programs they support. Program\nlogics support challenging programs, but they typically require significant\nhuman assistance, and cannot handle modules or higher-order programs.\n  To connect these two approaches, we present SeLoC---a separation logic for\nnon-interference, on top of which we build a type system using the technique of\nlogical relations. By building a type system on top of separation logic, we can\ncompositionally verify programs that consist of typed and untyped parts. The\nformer parts are verified through type checking, while the latter parts are\nverified through manual proof.\n  The core technical contribution of SeLoC is a relational form of weakest\npreconditions that can track information flow using separation logic resources.\nSeLoC is fully machine-checked, and built on top of the Iris framework for\nconcurrent separation logic in Coq. The integration with Iris provides seamless\nsupport for fine-grained concurrency, which was beyond the reach of prior type\nsystems and program logics for non-interference.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 12:25:28 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 13:51:09 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Frumin", "Dan", ""], ["Krebbers", "Robbert", ""], ["Birkedal", "Lars", ""]]}, {"id": "1910.01755", "submitter": "Sunjay Cauligi", "authors": "Sunjay Cauligi, Craig Disselkoen, Klaus v. Gleissenthall, Dean\n  Tullsen, Deian Stefan, Tamara Rezk, and Gilles Barthe", "title": "Constant-Time Foundations for the New Spectre Era", "comments": "To appear at PLDI '20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The constant-time discipline is a software-based countermeasure used for\nprotecting high assurance cryptographic implementations against timing\nside-channel attacks. Constant-time is effective (it protects against many\nknown attacks), rigorous (it can be formalized using program semantics), and\namenable to automated verification. Yet, the advent of micro-architectural\nattacks makes constant-time as it exists today far less useful.\n  This paper lays foundations for constant-time programming in the presence of\nspeculative and out-of-order execution. We present an operational semantics and\na formal definition of constant-time programs in this extended setting. Our\nsemantics eschews formalization of microarchitectural features (that are\ninstead assumed under adversary control), and yields a notion of constant-time\nthat retains the elegance and tractability of the usual notion. We demonstrate\nthe relevance of our semantics in two ways: First, by contrasting existing\nSpectre-like attacks with our definition of constant-time. Second, by\nimplementing a static analysis tool, Pitchfork, which detects violations of our\nextended constant-time property in real world cryptographic libraries.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2019 23:01:21 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 04:58:03 GMT"}, {"version": "v3", "created": "Fri, 8 May 2020 23:00:40 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Cauligi", "Sunjay", ""], ["Disselkoen", "Craig", ""], ["Gleissenthall", "Klaus v.", ""], ["Tullsen", "Dean", ""], ["Stefan", "Deian", ""], ["Rezk", "Tamara", ""], ["Barthe", "Gilles", ""]]}, {"id": "1910.02146", "submitter": "Tobias Reiher", "authors": "Tobias Reiher, Alexander Senier, Jeronimo Castrillon, Thorsten Strufe", "title": "RecordFlux: Formal Message Specification and Generation of Verifiable\n  Binary Parsers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various vulnerabilities have been found in message parsers of protocol\nimplementations in the past. Even highly sensitive software components like TLS\nlibraries are affected regularly. Resulting issues range from denial-of-service\nattacks to the extraction of sensitive information. The complexity of protocols\nand imprecise specifications in natural language are the core reasons for\nsubtle bugs in implementations, which are hard to find. The lack of precise\nspecifications impedes formal verification.\n  In this paper, we propose a model and a corresponding domain-specific\nlanguage to formally specify message formats of existing real-world binary\nprotocols. A unique feature of the model is the capability to define\ninvariants, which specify relations and dependencies between message fields.\nFurthermore, the model allows defining the relation of messages between\ndifferent protocol layers and thus ensures correct interpretation of payload\ndata. We present a technique to derive verifiable parsers based on the model,\ngenerate efficient code for their implementation, and automatically prove the\nabsence of runtime errors. Examples of parser specifications for Ethernet and\nTLS demonstrate the applicability of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2019 17:19:25 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Reiher", "Tobias", ""], ["Senier", "Alexander", ""], ["Castrillon", "Jeronimo", ""], ["Strufe", "Thorsten", ""]]}, {"id": "1910.02375", "submitter": "Michael Kruse", "authors": "Michael Kruse, Hal Finkel", "title": "Design and Use of Loop-Transformation Pragmas", "comments": "IWOMP 2019, September 11-13, Auckland, preprint", "journal-ref": null, "doi": "10.1007/978-3-030-28596-8_9", "report-no": null, "categories": "cs.PL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adding a pragma directive into the source code is arguably easier than\nrewriting it, for instance for loop unrolling. Moreover, if the application is\nmaintained for multiple platforms, their difference in performance\ncharacteristics may require different code transformations. Code transformation\ndirectives allow replacing the directives depending on the platform, i.e.\nseparation of code semantics and its performance optimization.\n  In this paper, we explore the design space (syntax and semantics) of adding\nsuch directive into a future OpenMP specification. Using a prototype\nimplementation in Clang, we demonstrate the usefulness of such directives on a\nfew benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2019 05:06:38 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Kruse", "Michael", ""], ["Finkel", "Hal", ""]]}, {"id": "1910.02874", "submitter": "Thorsten Wissmann", "authors": "Giorgio Audrito, Jacob Beal, Ferruccio Damiani, Danilo Pianini, Mirko\n  Viroli", "title": "Field-based Coordination with the Share Operator", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 4 (October\n  2, 2020) lmcs:6816", "doi": "10.23638/LMCS-16(4:1)2020", "report-no": null, "categories": "cs.DC cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Field-based coordination has been proposed as a model for coordinating\ncollective adaptive systems, promoting a view of distributed computations as\nfunctions manipulating data structures spread over space and evolving over\ntime, called computational fields. The field calculus is a formal foundation\nfor field computations, providing specific constructs for evolution (time) and\nneighbor interaction (space), which are handled by separate operators (called\nrep and nbr, respectively). This approach, however, intrinsically limits the\nspeed of information propagation that can be achieved by their combined use. In\nthis paper, we propose a new field-based coordination operator called share,\nwhich captures the space-time nature of field computations in a single operator\nthat declaratively achieves: (i) observation of neighbors' values; (ii)\nreduction to a single local value; and (iii) update and converse sharing to\nneighbors of a local variable. We show that for an important class of\nself-stabilising computations, share can replace all occurrences of rep and nbr\nconstructs. In addition to conceptual economy, use of the share operator also\nallows many prior field calculus algorithms to be greatly accelerated, which we\nvalidate empirically with simulations of frequently used network propagation\nand collection algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 15:54:03 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 16:15:57 GMT"}, {"version": "v3", "created": "Sat, 12 Sep 2020 08:33:19 GMT"}, {"version": "v4", "created": "Wed, 30 Sep 2020 19:45:45 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Audrito", "Giorgio", ""], ["Beal", "Jacob", ""], ["Damiani", "Ferruccio", ""], ["Pianini", "Danilo", ""], ["Viroli", "Mirko", ""]]}, {"id": "1910.03001", "submitter": "Vincenzo De Florio", "authors": "Vincenzo De Florio and Chris Blondia", "title": "Trading off Complexity for Expressiveness in Programming Languages:\n  Visions and Preliminary Experiences", "comments": "Appeared in Proc. of the 3rd Int.l Conference on Advanced\n  Communication and Networking (ACN 2011), Lecture Notes in Computer Science.\n  Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When programming resource-scarce embedded smart devices, the designer often\nrequires both the low-level system programming features of a language such as C\nand higher level capability typical of a language like Java. The choice of a\nparticular language typically implies trade offs between conflicting design\ngoals such as performance, costs, and overheads. The large variety of\nlanguages, virtual machines, and translators provides the designer with a dense\ntrade off space, ranging from minimalistic to rich full-fledged approaches, but\nonce a choice is made it is often difficult for the designer to revise it. In\nthis work we propose a system of light-weighted and modular extensions as a\nmethod to flexibly reshape the target programming language as needed, adding\nonly those application layer features that match the current design goals. In\nso doing complexity is made transparent, but not hidden: While the programmer\ncan benefit of higher level constructs, the designer can deal with modular\nbuilding blocks each characterized by a certain algorithmic complexity and\ntherefore each accountable for a given share of the overhead. As a result the\ndesigner is given a finer control on the amount of resources that are consumed\nby the run-time executive of the chosen programming language.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 09:51:09 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["De Florio", "Vincenzo", ""], ["Blondia", "Chris", ""]]}, {"id": "1910.03026", "submitter": "Deepak Puthal", "authors": "Devki Nandan Jha, Khaled Alwasel, Areeb Alshoshan, Xianghua Huang,\n  Ranesh Kumar Naha, Sudheer Kumar Battula, Saurabh Garg, Deepak Puthal, Philip\n  James, Albert Y. Zomaya, Schahram Dustdar and Rajiv Ranjan", "title": "IoTSim-Edge: A Simulation Framework for Modeling the Behaviour of IoT\n  and Edge Computing Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel simulator IoTSim-Edge, which captures the\nbehavior of heterogeneous IoT and edge computing infrastructure and allows\nusers to test their infrastructure and framework in an easy and configurable\nmanner. IoTSim-Edge extends the capability of CloudSim to incorporate the\ndifferent features of edge and IoT devices. The effectiveness of IoTSim-Edge is\ndescribed using three test cases. The results show the varying capability of\nIoTSim-Edge in terms of application composition, battery-oriented modeling,\nheterogeneous protocols modeling and mobility modeling along with the resources\nprovisioning for IoT applications.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 19:10:19 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Jha", "Devki Nandan", ""], ["Alwasel", "Khaled", ""], ["Alshoshan", "Areeb", ""], ["Huang", "Xianghua", ""], ["Naha", "Ranesh Kumar", ""], ["Battula", "Sudheer Kumar", ""], ["Garg", "Saurabh", ""], ["Puthal", "Deepak", ""], ["James", "Philip", ""], ["Zomaya", "Albert Y.", ""], ["Dustdar", "Schahram", ""], ["Ranjan", "Rajiv", ""]]}, {"id": "1910.03704", "submitter": "Casey Casalnuovo", "authors": "Casey Casalnuovo, Kevin Lee, Hulin Wang, Prem Devanbu, Emily Morgan", "title": "Do People Prefer \"Natural\" code?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.IT cs.PL math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural code is known to be very repetitive (much more so than natural\nlanguage corpora); furthermore, this repetitiveness persists, even after\naccounting for the simpler syntax of code. However, programming languages are\nvery expressive, allowing a great many different ways (all clear and\nunambiguous) to express even very simple computations. So why is natural code\nrepetitive? We hypothesize that the reasons for this lie in fact that code is\nbimodal: it is executed by machines, but also read by humans. This bimodality,\nwe argue, leads developers to write code in certain preferred ways that would\nbe familiar to code readers. To test this theory, we 1) model familiarity using\na language model estimated over a large training corpus and 2) run an\nexperiment applying several meaning preserving transformations to Java and\nPython expressions in a distinct test corpus to see if forms more familiar to\nreaders (as predicted by the language models) are in fact the ones actually\nwritten. We find that these transformations generally produce program\nstructures that are less common in practice, supporting the theory that the\nhigh repetitiveness in code is a matter of deliberate preference. Finally, 3)\nwe use a human subject study to show alignment between language model score and\nhuman preference for the first time in code, providing support for using this\nmeasure to improve code.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 22:11:55 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Casalnuovo", "Casey", ""], ["Lee", "Kevin", ""], ["Wang", "Hulin", ""], ["Devanbu", "Prem", ""], ["Morgan", "Emily", ""]]}, {"id": "1910.03784", "submitter": "Kohei Suenaga", "authors": "Kohei Suenaga and Takuya Ishizawa", "title": "Generalized Property-Directed Reachability for Hybrid Systems", "comments": "To appear in VMCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized property-directed reachability (GPDR) belongs to the family of\nthe model-checking techniques called IC3/PDR. It has been successfully applied\nto software verification; for example, it is the core of Spacer, a\nstate-of-the-art Horn-clause solver bundled with Z3. However, it has yet to be\napplied to hybrid systems, which involve a continuous evolution of values over\ntime. As the first step towards GPDR- based model checking for hybrid systems,\nthis paper formalizes HGPDR, an adaptation of GPDR to hybrid systems, and\nproves its soundness. We also implemented a semi-automated proof-of-concept\nverifier, which allows a user to provide hints to guide verification steps.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 04:15:18 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 19:26:09 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Suenaga", "Kohei", ""], ["Ishizawa", "Takuya", ""]]}, {"id": "1910.04137", "submitter": "Rohit Chadha", "authors": "Gilles Barthe, Rohit Chadha, Vishal Jagannath, A. Prasad Sistla and\n  Mahesh Viswanathan", "title": "Deciding Differential Privacy for Programs with Finite Inputs and\n  Outputs", "comments": null, "journal-ref": null, "doi": "10.1145/3373718.339479", "report-no": null, "categories": "cs.CR cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a de facto standard for statistical computations over\ndatabases that contain private data. The strength of differential privacy lies\nin a rigorous mathematical definition that guarantees individual privacy and\nyet allows for accurate statistical results. Thanks to its mathematical\ndefinition, differential privacy is also a natural target for formal analysis.\nA broad line of work uses logical methods for proving privacy. However, these\nmethods are not complete, and only partially automated. A recent and\ncomplementary line of work uses statistical methods for finding privacy\nviolations. However, the methods only provide statistical guarantees (but no\nproofs).\n  We propose the first decision procedure for checking the differential privacy\nof a non-trivial class of probabilistic computations. Our procedure takes as\ninput a program P parametrized by a privacy budget $\\epsilon$, and either\nproves differential privacy for all possible values of $\\epsilon$ or generates\na counterexample. In addition, our procedure applies both to\n$\\epsilon$-differential privacy and $(\\epsilon,\\delta)$-differential privacy.\nTechnically, the decision procedure is based on a novel and judicious encoding\nof the semantics of programs in our class into a decidable fragment of the\nfirst-order theory of the reals with exponentiation. We implement our procedure\nand use it for (dis)proving privacy bounds for many well-known examples,\nincluding randomized response, histogram, report noisy max and sparse vector.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2019 17:23:39 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 22:05:28 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Barthe", "Gilles", ""], ["Chadha", "Rohit", ""], ["Jagannath", "Vishal", ""], ["Sistla", "A. Prasad", ""], ["Viswanathan", "Mahesh", ""]]}, {"id": "1910.05177", "submitter": "Michael Pradel", "authors": "Yaza Wainakh and Moiz Rauf and Michael Pradel", "title": "IdBench: Evaluating Semantic Representations of Identifier Names in\n  Source Code", "comments": "Accepted as full research paper at International Conference on\n  Software Engineering (ICSE) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifier names convey useful information about the intended semantics of\ncode. Name-based program analyses use this information, e.g., to detect bugs,\nto predict types, and to improve the readability of code. At the core of\nname-based analyses are semantic representations of identifiers, e.g., in the\nform of learned embeddings. The high-level goal of such a representation is to\nencode whether two identifiers, e.g., len and size, are semantically similar.\nUnfortunately, it is currently unclear to what extent semantic representations\nmatch the semantic relatedness and similarity perceived by developers. This\npaper presents IdBench, the first benchmark for evaluating semantic\nrepresentations against a ground truth created from thousands of ratings by 500\nsoftware developers. We use IdBench to study state-of-the-art embedding\ntechniques proposed for natural language, an embedding technique specifically\ndesigned for source code, and lexical string distance functions. Our results\nshow that the effectiveness of semantic representations varies significantly\nand that the best available embeddings successfully represent semantic\nrelatedness. On the downside, no existing technique provides a satisfactory\nrepresentation of semantic similarities, among other reasons because\nidentifiers with opposing meanings are incorrectly considered to be similar,\nwhich may lead to fatal mistakes, e.g., in a refactoring tool. Studying the\nstrengths and weaknesses of the different techniques shows that they complement\neach other. As a first step toward exploiting this complementarity, we present\nan ensemble model that combines existing techniques and that clearly\noutperforms the best available semantic representation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 13:34:30 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 10:07:16 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Wainakh", "Yaza", ""], ["Rauf", "Moiz", ""], ["Pradel", "Michael", ""]]}, {"id": "1910.06500", "submitter": "Yasir Hussain", "authors": "Yasir Hussain, Zhiqiu Huang, Yu Zhou and Senzhang Wang", "title": "DeepVS: An Efficient and Generic Approach for Source Code Modeling Usage", "comments": null, "journal-ref": "Electronics Letters ( Volume: 56 , Issue: 12 , 6 11 2020 )\n  Page(s): 604 - 607", "doi": "10.1049/el.2020.0500", "report-no": null, "categories": "cs.NE cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The source code suggestions provided by current IDEs are mostly dependent on\nstatic type learning. These suggestions often end up proposing irrelevant\nsuggestions for a peculiar context. Recently, deep learning-based approaches\nhave shown great potential in the modeling of source code for various software\nengineering tasks. However, these techniques lack adequate generalization and\nresistance to acclimate the use of such models in a real-world software\ndevelopment environment. This letter presents \\textit{DeepVS}, an end-to-end\ndeep neural code completion tool that learns from existing codebases by\nexploiting the bidirectional Gated Recurrent Unit (BiGRU) neural net. The\nproposed tool is capable of providing source code suggestions instantly in an\nIDE by using pre-trained BiGRU neural net. The evaluation of this work is\ntwo-fold, quantitative and qualitative. Through extensive evaluation on ten\nreal-world open-source software systems, the proposed method shows significant\nperformance enhancement and its practicality. Moreover, the results also\nsuggest that \\textit{DeepVS} tool is capable of suggesting zero-day (unseen)\ncode tokens by learning coding patterns from real-world software systems.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 02:59:52 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 12:01:26 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Hussain", "Yasir", ""], ["Huang", "Zhiqiu", ""], ["Zhou", "Yu", ""], ["Wang", "Senzhang", ""]]}, {"id": "1910.06826", "submitter": "Ib\\'eria Medeiros", "authors": "Ib\\'eria Medeiros (1), Nuno Neves (1), Miguel Correia (2) ((1) LASIGE,\n  Faculdade de Ci\\^encias, Universidade de Lisboa, Portugal, (2) INESC-ID,\n  Instituto Superior T\\'ecnico, Universidade de Lisboa, Portugal)", "title": "Statically Detecting Vulnerabilities by Processing Programming Languages\n  as Natural Languages", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web applications continue to be a favorite target for hackers due to a\ncombination of wide adoption and rapid deployment cycles, which often lead to\nthe introduction of high impact vulnerabilities. Static analysis tools are\nimportant to search for bugs automatically in the program source code,\nsupporting developers on their removal. However, building these tools requires\nprogramming the knowledge on how to discover the vulnerabilities. This paper\npresents an alternative approach in which tools learn to detect flaws\nautomatically by resorting to artificial intelligence concepts, more concretely\nto natural language processing. The approach employs a sequence model to learn\nto characterize vulnerabilities based on an annotated corpus. Afterwards, the\nmodel is utilized to discover and identify vulnerabilities in the source code.\nIt was implemented in the DEKANT tool and evaluated experimentally with a large\nset of PHP applications and WordPress plugins. Overall, we found several\nhundred vulnerabilities belonging to 12 classes of input validation\nvulnerabilities, where 62 of them were zero-day.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 18:23:42 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Medeiros", "Ib\u00e9ria", ""], ["Neves", "Nuno", ""], ["Correia", "Miguel", ""]]}, {"id": "1910.07517", "submitter": "Uri Alon", "authors": "Noam Yefet, Uri Alon, Eran Yahav", "title": "Adversarial Examples for Models of Code", "comments": "Accepted to OOPSLA'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural models of code have shown impressive results when performing tasks\nsuch as predicting method names and identifying certain kinds of bugs. We show\nthat these models are vulnerable to adversarial examples, and introduce a novel\napproach for attacking trained models of code using adversarial examples. The\nmain idea of our approach is to force a given trained model to make an\nincorrect prediction, as specified by the adversary, by introducing small\nperturbations that do not change the program's semantics, thereby creating an\nadversarial example. To find such perturbations, we present a new technique for\nDiscrete Adversarial Manipulation of Programs (DAMP). DAMP works by deriving\nthe desired prediction with respect to the model's inputs, while holding the\nmodel weights constant, and following the gradients to slightly modify the\ninput code. We show that our DAMP attack is effective across three neural\narchitectures: code2vec, GGNN, and GNN-FiLM, in both Java and C#. Our\nevaluations demonstrate that DAMP has up to 89% success rate in changing a\nprediction to the adversary's choice (a targeted attack) and a success rate of\nup to 94% in changing a given prediction to any incorrect prediction (a\nnon-targeted attack). To defend a model against such attacks, we empirically\nexamine a variety of possible defenses and discuss their trade-offs. We show\nthat some of these defenses can dramatically drop the success rate of the\nattacker, with a minor penalty of 2% relative degradation in accuracy when they\nare not performing under attack. Our code, data, and trained models are\navailable at https://github.com/tech-srl/adversarial-examples .\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 19:56:42 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 10:31:10 GMT"}, {"version": "v3", "created": "Sun, 8 Dec 2019 07:26:44 GMT"}, {"version": "v4", "created": "Wed, 27 May 2020 07:54:30 GMT"}, {"version": "v5", "created": "Mon, 12 Oct 2020 18:36:43 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Yefet", "Noam", ""], ["Alon", "Uri", ""], ["Yahav", "Eran", ""]]}, {"id": "1910.07519", "submitter": "Frederic Prost", "authors": "Dominique Duval (CASC), Rachid Echahed (LIG Laboratoire d'Informatique\n  de Grenoble), Frederic Prost (LIG)", "title": "On foundational aspects of RDF and SPARQL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.FL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the recommendations of the World Wide Web Consortium (W3C) about\nthe Resource Description Framework (RDF) and the associated query language\nSPARQL. We propose a new formal framework based on category theory which\nprovides clear and concise formal definitions of the main basic features of RDF\nand SPARQL. We propose to define the notions of RDF graphs as well as SPARQL\nbasic graph patterns as objects of some nested categories. This allows one to\nclarify, in particular, the role of blank nodes. Furthermore, we consider basic\nSPARQL CONSTRUCT and SELECT queries and formalize their operational semantics\nfollowing a novel algebraic graph transformation approach called POIM.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 08:21:57 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 16:55:33 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Duval", "Dominique", "", "CASC"], ["Echahed", "Rachid", "", "LIG Laboratoire d'Informatique\n  de Grenoble"], ["Prost", "Frederic", "", "LIG"]]}, {"id": "1910.07583", "submitter": "Andreas Stahlbauer", "authors": "Andreas Stahlbauer", "title": "Abstract Transducers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several abstract machines that operate on symbolic input alphabets have been\nproposed in the last decade, for example, symbolic automata or lattice\nautomata. Applications of these types of automata include software security\nanalysis and natural language processing. While these models provide means to\ndescribe words over infinite input alphabets, there is no considerable work on\nsymbolic output (as present in transducers) alphabets, or even abstraction\n(widening) thereof. Furthermore, established approaches for transforming, for\nexample, minimizing or reducing, finite-state machines that produce output on\nstates or transitions are not applicable. A notion of equivalence of this type\nof machines is needed to make statements about whether or not transformations\nmaintain the semantics. We present abstract transducers as a new form of\nfinite-state transducers. Both their input alphabet and the output alphabet is\ncomposed of abstract words, where one abstract word represents a set of\nconcrete words. The mapping between these representations is described by\nabstract word domains. By using words instead of single letters, abstract\ntransducers provide the possibility of lookaheads to decide on state\ntransitions to conduct. Since both the input symbol and the output symbol on\neach transition is an abstract entity, abstraction techniques can be applied\nnaturally. We apply abstract transducers as the foundation for sharing task\nartifacts for reuse in context of program analysis and verification, and\ndescribe task artifacts as abstract words. A task artifact is any entity that\ncontributes to an analysis task and its solution, for example, candidate\ninvariants or source code to weave.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 19:34:55 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Stahlbauer", "Andreas", ""]]}, {"id": "1910.08091", "submitter": "Yura Perov N", "authors": "Yura Perov, Logan Graham, Kostis Gourgoulias, Jonathan G. Richens,\n  Ciar\\'an M. Lee, Adam Baker, Saurabh Johri", "title": "MultiVerse: Causal Reasoning using Importance Sampling in Probabilistic\n  Programming", "comments": "Logan and Yura have made equal contributions to the paper. Accepted\n  to the 2nd Symposium on Advances in Approximate Bayesian Inference\n  (Vancouver, Canada, 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.PL stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We elaborate on using importance sampling for causal reasoning, in particular\nfor counterfactual inference. We show how this can be implemented natively in\nprobabilistic programming. By considering the structure of the counterfactual\nquery, one can significantly optimise the inference process. We also consider\ndesign choices to enable further optimisations. We introduce MultiVerse, a\nprobabilistic programming prototype engine for approximate causal reasoning. We\nprovide experimental results and compare with Pyro, an existing probabilistic\nprogramming framework with some of causal reasoning tools.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 18:00:24 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 10:05:13 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Perov", "Yura", ""], ["Graham", "Logan", ""], ["Gourgoulias", "Kostis", ""], ["Richens", "Jonathan G.", ""], ["Lee", "Ciar\u00e1n M.", ""], ["Baker", "Adam", ""], ["Johri", "Saurabh", ""]]}, {"id": "1910.08416", "submitter": "Narciso Mart\\'i-Oliet", "authors": "Francisco Dur\\'an, Steven Eker, Santiago Escobar, Narciso\n  Mart\\'i-Oliet, Jos\\'e Meseguer, Rub\\'en Rubio, Carolyn Talcott", "title": "Programming and Symbolic Computation in Maude", "comments": "Journal of Logical and Algebraic Methods in Programming, 2019", "journal-ref": null, "doi": "10.1016/j.jlamp.2019.100497", "report-no": null, "categories": "cs.LO cs.PL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rewriting logic is both a flexible semantic framework within which widely\ndifferent concurrent systems can be naturally specified and a logical framework\nin which widely different logics can be specified. Maude programs are exactly\nrewrite theories. Maude has also a formal environment of verification tools.\nSymbolic computation is a powerful technique for reasoning about the\ncorrectness of concurrent systems and for increasing the power of formal tools.\nWe present several new symbolic features of Maude that enhance formal reasoning\nabout Maude programs and the effectiveness of formal tools. They include: (i)\nvery general unification modulo user-definable equational theories, and (ii)\nsymbolic reachability analysis of concurrent systems using narrowing. The paper\ndoes not focus just on symbolic features: it also describes several other new\nMaude features, including: (iii) Maude's strategy language for controlling\nrewriting, and (iv) external objects that allow flexible interaction of Maude\nobject-based concurrent systems with the external world. In particular,\nmeta-interpreters are external objects encapsulating Maude interpreters that\ncan interact with many other objects. To make the paper self-contained and give\na reasonably complete language overview, we also review the basic Maude\nfeatures for equational rewriting and rewriting with rules, Maude programming\nof concurrent object systems, and reflection. Furthermore, we include many\nexamples illustrating all the Maude notions and features described in the\npaper.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 13:38:57 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Dur\u00e1n", "Francisco", ""], ["Eker", "Steven", ""], ["Escobar", "Santiago", ""], ["Mart\u00ed-Oliet", "Narciso", ""], ["Meseguer", "Jos\u00e9", ""], ["Rubio", "Rub\u00e9n", ""], ["Talcott", "Carolyn", ""]]}, {"id": "1910.08480", "submitter": "Taro Sekiyama", "authors": "Taro Sekiyama and Atsushi Igarashi", "title": "Gradual Typing for Extensibility by Rows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies gradual typing for row types and row polymorphism. Key\ningredients in this work are the dynamic row type, which represents a\nstatically unknown part of a row, and consistency for row types, which allows\ninjecting static row types into the dynamic row type and, conversely,\nprojecting the dynamic row type to any static row type. While consistency\ncaptures the behavior of the dynamic row type statically, it makes the\nsemantics of a gradually typed language incoherent when combined with row\nequivalence which identifies row types up to field reordering. To solve this\nproblem, we develop consistent equivalence, which characterizes composition of\nconsistency and row equivalence. Using consistent equivalence, we propose a\npolymorphic blame calculus for row types and row polymorphism. In our blame\ncalculus, casts perform not only run-time checking with the dynamic row type\nbut also field reordering in row types. To simplify our technical development\nfor row polymorphism, we adopt scoped labels, which are employed by the\nlanguage Koka and are also emerging in the context of effect systems. We give\nthe formal definition of our blame calculus with these technical developments\nand prove its type soundness. We also sketch the gradually typed surface\nlanguage and type-preserving translation from the surface language to the blame\ncalculus and discuss conservativity of the surface language over typing of a\nstatically typed language with row types and row polymorphism.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 15:53:46 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 01:48:53 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Sekiyama", "Taro", ""], ["Igarashi", "Atsushi", ""]]}, {"id": "1910.08607", "submitter": "Marco Patrignani", "authors": "Marco Patrignani, Marco Guarnieri", "title": "Exorcising Spectres with Secure Compilers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attackers can access sensitive information of programs by exploiting the\nside-effects of speculatively-executed instructions using Spectre attacks. To\nmitigate theses attacks, popular compilers deployed a wide range of\ncountermeasures. The security of these countermeasures, however, has not been\nascertained: while some of them are believed to be secure, others are known to\nbe insecure and result in vulnerable programs. To reason about the security\nguarantees of these compiler-inserted countermeasures, this paper presents a\nframework comprising several secure compilation criteria characterizing when\ncompilers produce code resistant against Spectre attacks. With this framework,\nwe perform a comprehensive security analysis of compiler-level countermeasures\nagainst Spectre attacks implemented in major compilers. This work provides\nsound foundations to formally reason about the security of compiler-level\ncountermeasures against Spectre attacks as well as the first proofs of security\nand insecurity of said countermeasures.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 19:58:44 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 04:55:42 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 21:26:45 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Patrignani", "Marco", ""], ["Guarnieri", "Marco", ""]]}, {"id": "1910.08634", "submitter": "Marco Patrignani", "authors": "Marco Patrignani, Riad S. Wahby, Robert K\\\"unnemann", "title": "Universal Composability is Secure Compilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal composability is a framework for the specification and analysis of\ncryptographic protocols with a strong compositionality guarantee: UC protocols\nare secure even when composed with other protocols. Secure compilation tells\nwhether compiled programs are as secure as their source-level counterparts, no\nmatter what target-level code they interact with. These two disciplines are\nstudied in isolation, but we believe there is a deeper connection between them\nwith benefits from both worlds to reap. This paper outlines the connection\nbetween universal composability and robust compilation, the latest of secure\ncompilation theories. We show how to read the universal composability theorem\nin terms of a robust compilation theorem and vice-versa. This, in turn, shows\nwhich elements of one theory corresponds to which element in the other theory.\nWe believe this is the first step towards understanding how can secure\ncompilation theories be used in universal composability settings and\nvice-versa.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 21:28:56 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 16:59:28 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Patrignani", "Marco", ""], ["Wahby", "Riad S.", ""], ["K\u00fcnnemann", "Robert", ""]]}, {"id": "1910.09521", "submitter": "Yanjun Wang", "authors": "Yanjun Wang, Jinwei Liu, Dalin Zhang, and Xiaokang Qiu", "title": "Reasoning About Recursive Tree Traversals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traversals are commonly seen in tree data structures, and\nperformance-enhancing transformations between tree traversals are critical for\nmany applications. Existing approaches to reasoning about tree traversals and\ntheir transformations are ad hoc, with various limitations on the class of\ntraversals they can handle, the granularity of dependence analysis, and the\ntypes of possible transformations. We propose Retreet, a framework in which one\ncan describe general recursive tree traversals, precisely represent iterations,\nschedules and dependences, and automatically check data-race-freeness and\ntransformation correctness. The crux of the framework is a stack-based\nrepresentation for iterations and an encoding to Monadic Second-Order (MSO)\nlogic over trees. Experiments show that our framework can reason about\ntraversals with sophisticated mutual recursion on real-world data structures\nsuch as CSS and cycletrees.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 17:26:59 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 00:00:22 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Wang", "Yanjun", ""], ["Liu", "Jinwei", ""], ["Zhang", "Dalin", ""], ["Qiu", "Xiaokang", ""]]}, {"id": "1910.09579", "submitter": "Steven W.T. Cheung", "authors": "Steven W.T. Cheung, Dan R. Ghica, Koko Muroya", "title": "Transparent Synchronous Dataflow", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2021, Vol. 5,\n  Issue 3, Article 12", "doi": "10.22152/programming-journal.org/2021/5/12", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dataflow programming is a popular and convenient programming paradigm in\nsystems modelling, optimisation, and machine learning. It has a number of\nadvantages, for instance the lacks of control flow allows computation to be\ncarried out in parallel as well as in distributed machines. More recently the\nidea of dataflow graphs has also been brought into the design of various deep\nlearning frameworks. They facilitate an easy and efficient implementation of\nautomatic differentiation, which is the heart of modern deep learning paradigm.\n[abstract abridged]\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 18:12:46 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 21:12:23 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Cheung", "Steven W. T.", ""], ["Ghica", "Dan R.", ""], ["Muroya", "Koko", ""]]}, {"id": "1910.09586", "submitter": "Marco Patrignani", "authors": "Marco Vassena, Marco Patrignani", "title": "Memory Safety Preservation for WebAssembly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  WebAssembly (Wasm) is a next-generation portable compilation target for\ndeploying applications written in high-level languages on the web. In order to\nprotect their memory from untrusted code, web browser engines confine the\nexecution of compiled Wasm programs in a memory-safe sandbox. Unfortunately,\nclassic memory-safety vulnerabilities (e.g., buffer overflows and\nuse-after-free) can still corrupt the memory within the sandbox and allow Wasm\ncode to mount severe attacks. To prevent these attacks, we study a class of\nsecure compilers that eliminate (different kinds of) of memory safety\nviolations. Following a rigorous approach, we discuss memory safety in terms of\nhypersafety properties, which let us identify suitable secure compilation\ncriteria for memory-safety-preserving compilers. We conjecture that, barring\nsome restrictions at module boundaries, the existing security mechanisms of\nWasm may suffice to enforce memory-safety preservation, in the short term. In\nthe long term, we observe that certain features proposed in the design of a\nmemory-safe variant of Wasm could allow compilers to lift these restrictions\nand enforce relaxed forms of memory safety.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 18:24:21 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Vassena", "Marco", ""], ["Patrignani", "Marco", ""]]}, {"id": "1910.09633", "submitter": "Vladimir Zamdzhiev", "authors": "Romain P\\'echoux, Simon Perdrix, Mathys Rennela, Vladimir Zamdzhiev", "title": "Quantum Programming with Inductive Datatypes: Causality and Affine Type\n  Theory", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-45231-5_29", "report-no": null, "categories": "cs.LO cs.PL math.CT quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inductive datatypes in programming languages allow users to define useful\ndata structures such as natural numbers, lists, trees, and others. In this\npaper we show how inductive datatypes may be added to the quantum programming\nlanguage QPL. We construct a sound categorical model for the language and by\ndoing so we provide the first detailed semantic treatment of user-defined\ninductive datatypes in quantum programming. We also show our denotational\ninterpretation is invariant with respect to big-step reduction, thereby\nestablishing another novel result for quantum programming. Compared to\nclassical programming, this property is considerably more difficult to prove\nand we demonstrate its usefulness by showing how it immediately implies\ncomputational adequacy at all types. To further cement our results, our\nsemantics is entirely based on a physically natural model of von Neumann\nalgebras, which are mathematical structures used by physicists to study quantum\nmechanics.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 20:12:43 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["P\u00e9choux", "Romain", ""], ["Perdrix", "Simon", ""], ["Rennela", "Mathys", ""], ["Zamdzhiev", "Vladimir", ""]]}, {"id": "1910.09744", "submitter": "Umang Mathur", "authors": "Paul Krogmeier, Umang Mathur, Adithya Murali, P. Madhusudan, Mahesh\n  Viswanathan", "title": "Decidable Synthesis of Programs with Uninterpreted Functions", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-53291-8_32", "report-no": null, "categories": "cs.PL cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify a decidable synthesis problem for a class of programs of\nunbounded size with conditionals and iteration that work over infinite data\ndomains. The programs in our class use uninterpreted functions and relations,\nand abide by a restriction called coherence that was recently identified to\nyield decidable verification. We formulate a powerful grammar-restricted\n(syntax-guided) synthesis problem for coherent uninterpreted programs, and we\nshow the problem to be decidable, identify its precise complexity, and also\nstudy several variants of the problem.\n", "versions": [{"version": "v1", "created": "Tue, 22 Oct 2019 03:06:11 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 21:36:15 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Krogmeier", "Paul", ""], ["Mathur", "Umang", ""], ["Murali", "Adithya", ""], ["Madhusudan", "P.", ""], ["Viswanathan", "Mahesh", ""]]}, {"id": "1910.10346", "submitter": "Yanhong Annie Liu", "authors": "Yanhong A. Liu and Scott D. Stoller", "title": "Knowledge of Uncertain Worlds: Programming with Logical Constraints", "comments": null, "journal-ref": "Journal of Logic and Computation. 2021", "doi": "10.1093/logcom/exaa077", "report-no": null, "categories": "cs.LO cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming with logic for sophisticated applications must deal with\nrecursion and negation, which together have created significant challenges in\nlogic, leading to many different, conflicting semantics of rules. This paper\ndescribes a unified language, DA logic, for design and analysis logic, based on\nthe unifying founded semantics and constraint semantics, that support the power\nand ease of programming with different intended semantics. The key idea is to\nprovide meta-constraints, supports the use of uncertain information in the form\nof either undefined values or possible combinations of values or both, and\npromote the use of knowledge units that can be instantiated by any new\npredicates, including predicates with additional arguments.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 04:30:51 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 03:49:52 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2020 03:44:58 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Liu", "Yanhong A.", ""], ["Stoller", "Scott D.", ""]]}, {"id": "1910.10421", "submitter": "Keisuke Nakano", "authors": "Keisuke Nakano", "title": "Towards a Complete Picture of Lens Laws", "comments": "Proceedings of the Third Workshop on Software Foundations for Data\n  Interoperability (SFDI2019+), October 28, 2019, Fukuoka, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bidirectional transformation, also called lens, has played important roles in\nmaintaining consistency in many fields of applications. A lens is specified by\na pair of forward and backward functions which relate to each other in a\nconsistent manner. The relation is formalized as a set of equations called lens\nlaws. This report investigates precise dependencies among lens laws: which law\nimplies another and which combination of laws implies another. The set of such\nimplications forms a complicated graph structure. It would be helpful to check\na well-definedness of bidirectional transformation in a lightweight way.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 09:10:56 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Nakano", "Keisuke", ""]]}, {"id": "1910.10851", "submitter": "EPTCS", "authors": "Aaron Stump (The University of Iowa)", "title": "A Weakly Initial Algebra for Higher-Order Abstract Syntax in Cedille", "comments": "In Proceedings LFMTP 2019, arXiv:1910.08712", "journal-ref": "EPTCS 307, 2019, pp. 55-67", "doi": "10.4204/EPTCS.307.6", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cedille is a relatively recent tool based on a Curry-style pure type theory,\nwithout a primitive datatype system. Using novel techniques based on dependent\nintersection types, inductive datatypes with their induction principles are\nderived. One benefit of this approach is that it allows exploration of new or\nadvanced forms of inductive datatypes. This paper reports work in progress on\none such form, namely higher-order abstract syntax (HOAS). We consider the\nnature of HOAS in the setting of pure type theory, comparing with the\ntraditional concept of environment models for lambda calculus. We see an\nalternative, based on what we term Kripke function-spaces, for which we can\nderive a weakly initial algebra in Cedille. Several examples are given using\nthe encoding.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 00:23:26 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Stump", "Aaron", "", "The University of Iowa"]]}, {"id": "1910.10889", "submitter": "Umang Mathur", "authors": "Umang Mathur, P. Madhusudan, Mahesh Viswanathan", "title": "What's Decidable About Program Verification Modulo Axioms?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the decidability of the verification problem of programs\n\\emph{modulo axioms} --- that is, verifying whether programs satisfy their\nassertions, when the functions and relations it uses are assumed to interpreted\nby arbitrary functions and relations that satisfy a set of first-order axioms.\nUnfortunately, verification of entirely uninterpreted programs (with the empty\nset of axioms) is already undecidable. A recent work introduced a subclass of\n\\emph{coherent} uninterpreted programs, and showed that they admit decidable\nverification \\cite{coherence2019}. We undertake a systematic study of various\nnatural axioms for relations and functions, and study the decidability of the\ncoherent verification problem. Axioms include relations being reflexive,\nsymmetric, transitive, or total order relations, %and their combinations,\nfunctions restricted to being associative, idempotent or commutative, and\ncombinations of such axioms as well. Our comprehensive results unearth a rich\nlandscape that shows that though several axiom classes admit decidability for\ncoherent programs, coherence is not a panacea as several others continue to be\nundecidable.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 02:40:49 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 21:29:14 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Mathur", "Umang", ""], ["Madhusudan", "P.", ""], ["Viswanathan", "Mahesh", ""]]}, {"id": "1910.10988", "submitter": "Kwanghoon Choi", "authors": "Kwanghoon Choi, James Cheney, Simon Fowler, Sam Lindley", "title": "A Polymorphic RPC Calculus", "comments": "SBMF-Brazilian Symposium on Formal Methods 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The RPC calculus is a simple semantic foundation for multi-tier programming\nlanguages such as Links in which located functions can be written for the\nclient-server model. Subsequently, the typed RPC calculus is designed to\ncapture the location information of functions by types and to drive location\ntype-directed slicing compilations. However, the use of locations is currently\nlimited to monomorphic ones, which is one of the gaps to overcome to put into\npractice the theory of RPC calculi for client-server model. This paper proposes\na polymorphic RPC calculus to allow programmers to write succinct multi-tier\nprograms using polymorphic location constructs. Then the polymorphic multi-tier\nprograms can be automatically translated into programs only containing location\nconstants amenable to the existing slicing compilation methods. We formulate a\ntype system for the polymorphic RPC calculus, and prove its type soundness.\nAlso, we design a monomorphization translation together with proofs on its type\nand semantic correctness for the translation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 09:30:44 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 00:30:53 GMT"}, {"version": "v3", "created": "Mon, 25 May 2020 12:19:01 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Choi", "Kwanghoon", ""], ["Cheney", "James", ""], ["Fowler", "Simon", ""], ["Lindley", "Sam", ""]]}, {"id": "1910.11108", "submitter": "Simon Fowler", "authors": "Simon Fowler", "title": "Model-View-Update-Communicate: Session Types meet the Elm Architecture", "comments": "Extended version of paper to appear at ECOOP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Session types are a type discipline for communication channel endpoints which\nallow conformance to protocols to be checked statically. Safely implementing\nsession types requires linearity, usually in the form of a linear type system.\nUnfortunately, linear typing is difficult to integrate with graphical user\ninterfaces (GUIs), and to date most programs using session types are command\nline applications.\n  In this paper, we propose the first principled integration of session typing\nand GUI development by building upon the Model-View-Update (MVU) architecture,\npioneered by the Elm programming language. We introduce\n$\\lambda_{\\textsf{MVU}}$, the first formal model of the MVU architecture, and\nprove it sound. By extending $\\lambda_{\\textsf{MVU}}$ with \\emph{commands} as\nfound in Elm, along with \\emph{linearity} and \\emph{model transitions}, we show\nthe first formal integration of session typing and GUI programming. We\nimplement our approach in the Links web programming language, and show examples\nincluding a two-factor authentication workflow and multi-room chat server.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 13:48:33 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 09:56:55 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 15:12:03 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Fowler", "Simon", ""]]}, {"id": "1910.11141", "submitter": "Alexey Radul", "authors": "Alexey Radul, Brian Patton, Dougal Maclaurin, Matthew D. Hoffman and\n  Rif A. Saurous", "title": "Automatically Batching Control-Intensive Programs for Modern\n  Accelerators", "comments": "10 pages; Machine Learning and Systems 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LG cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a general approach to batching arbitrary computations for\naccelerators such as GPUs. We show orders-of-magnitude speedups using our\nmethod on the No U-Turn Sampler (NUTS), a workhorse algorithm in Bayesian\nstatistics. The central challenge of batching NUTS and other Markov chain Monte\nCarlo algorithms is data-dependent control flow and recursion. We overcome this\nby mechanically transforming a single-example implementation into a form that\nexplicitly tracks the current program point for each batch member, and only\nsteps forward those in the same place. We present two different batching\nalgorithms: a simpler, previously published one that inherits recursion from\nthe host Python, and a more complex, novel one that implemenents recursion\ndirectly and can batch across it. We implement these batching methods as a\ngeneral program transformation on Python source. Both the batching system and\nthe NUTS implementation presented here are available as part of the popular\nTensorFlow Probability software package.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 14:06:18 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 15:56:56 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Radul", "Alexey", ""], ["Patton", "Brian", ""], ["Maclaurin", "Dougal", ""], ["Hoffman", "Matthew D.", ""], ["Saurous", "Rif A.", ""]]}, {"id": "1910.11232", "submitter": "Andr\\'e Platzer", "authors": "Andr\\'e Platzer", "title": "Overview of Logical Foundations of Cyber-Physical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": "In Helmut Seidl, editor, Post-proceedings of the Summer School\n  Marktoberdorf: Safety and Security of Software Systems - Logics, Proofs,\n  Applications, TUM University Press, 2020", "categories": "cs.LO cs.PL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-physical systems (CPSs) are important whenever computer technology\ninterfaces with the physical world as it does in self-driving cars or aircraft\ncontrol support systems. Due to their many subtleties, controllers for\ncyber-physical systems deserve to be held to the highest correctness standards.\nTheir correct functioning is crucial, which explains the broad interest in\nsafety analysis technology for their mathematical models, which are called\nhybrid systems because they combine discrete dynamics with continuous dynamics.\nDifferential dynamic logic (dL) provides logical specification and rigorous\nreasoning techniques for hybrid systems. The logic dL is implemented in the\ntheorem prover KeYmaera X, which has been instrumental in verifying ground\nrobot controllers, railway systems, and the next-generation airborne collision\navoidance system ACAS X. This chapter provides an informal overview of this\nlogical approach to CPS safety that is detailed in a recent textbook on Logical\nFoundations of Cyber-Physical Systems. It also explains how safety guarantees\nobtained in the land of verified models reach the level of CPS execution\nunharmed.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 15:38:12 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Platzer", "Andr\u00e9", ""]]}, {"id": "1910.11471", "submitter": "K.M. Tahsin Hassan Rahit", "authors": "K.M. Tahsin Hassan Rahit, Rashidul Hasan Nabil and Md Hasibul Huq", "title": "Machine Translation from Natural Language to Code using Long-Short Term\n  Memory", "comments": "8 pages, 3 figures, conference", "journal-ref": "Proceedings of the Future Technologies Conference (FTC) 2019.\n  Advances in Intelligent Systems and Computing, vol 1069. Springer, Cham", "doi": "10.1007/978-3-030-32520-6_6", "report-no": null, "categories": "cs.CL cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making computer programming language more understandable and easy for the\nhuman is a longstanding problem. From assembly language to present day's\nobject-oriented programming, concepts came to make programming easier so that a\nprogrammer can focus on the logic and the architecture rather than the code and\nlanguage itself. To go a step further in this journey of removing\nhuman-computer language barrier, this paper proposes machine learning approach\nusing Recurrent Neural Network (RNN) and Long-Short Term Memory (LSTM) to\nconvert human language into programming language code. The programmer will\nwrite expressions for codes in layman's language, and the machine learning\nmodel will translate it to the targeted programming language. The proposed\napproach yields result with 74.40% accuracy. This can be further improved by\nincorporating additional techniques, which are also discussed in this paper.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 00:46:07 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Rahit", "K. M. Tahsin Hassan", ""], ["Nabil", "Rashidul Hasan", ""], ["Huq", "Md Hasibul", ""]]}, {"id": "1910.11629", "submitter": "Danel Ahman", "authors": "Danel Ahman and Andrej Bauer", "title": "Runners in action", "comments": "ESOP 2020 final version + online appendix", "journal-ref": null, "doi": "10.1007/978-3-030-44914-8_2", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Runners of algebraic effects, also known as comodels, provide a mathematical\nmodel of resource management. We show that they also give rise to a programming\nconcept that models top-level external resources, as well as allows programmers\nto modularly define their own intermediate \"virtual machines\". We capture the\ncore ideas of programming with runners in an equational calculus\n$\\lambda_{\\mathsf{coop}}$, which we equip with a sound and coherent\ndenotational semantics that guarantees the linear use of resources and\nexecution of finalisation code. We accompany $\\lambda_{\\mathsf{coop}}$ with\nexamples of runners in action, provide a prototype language implementation in\nOCaml, as well as a Haskell library based on $\\lambda_{\\mathsf{coop}}$.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 11:37:35 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 20:26:57 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Ahman", "Danel", ""], ["Bauer", "Andrej", ""]]}, {"id": "1910.11714", "submitter": "Sebastian Wolff", "authors": "Roland Meyer and Sebastian Wolff", "title": "Pointer Life Cycle Types for Lock-Free Data Structures with Memory\n  Reclamation", "comments": null, "journal-ref": null, "doi": "10.1145/3371136", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the verification of lock-free data structures that manually\nmanage their memory with the help of a safe memory reclamation (SMR) algorithm.\nOur first contribution is a type system that checks whether a program properly\nmanages its memory. If the type check succeeds, it is safe to ignore the SMR\nalgorithm and consider the program under garbage collection. Intuitively, our\ntypes track the protection of pointers as guaranteed by the SMR algorithm.\nThere are two design decisions. The type system does not track any shape\ninformation, which makes it extremely lightweight. Instead, we rely on\ninvariant annotations that postulate a protection by the SMR. To this end, we\nintroduce angels, ghost variables with an angelic semantics. Moreover, the SMR\nalgorithm is not hard-coded but a parameter of the type system definition. To\nachieve this, we rely on a recent specification language for SMR algorithms.\nOur second contribution is to automate the type inference and the invariant\ncheck. For the type inference, we show a quadratic-time algorithm. For the\ninvariant check, we give a source-to-source translation that links our programs\nto off-the-shelf verification tools. It compiles away the angelic semantics.\nThis allows us to infer appropriate annotations automatically in a\nguess-and-check manner. To demonstrate the effectiveness of our type-based\nverification approach, we check linearizability for various list and set\nimplementations from the literature with both hazard pointers and epoch-based\nmemory reclamation. For many of the examples, this is the first time they are\nverified automatically. For the ones where there is a competitor, we obtain a\nspeed-up of up to two orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 13:29:33 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 17:14:40 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2019 09:34:51 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Meyer", "Roland", ""], ["Wolff", "Sebastian", ""]]}, {"id": "1910.11717", "submitter": "Sebastian Graf", "authors": "Sebastian Graf and Simon Peyton Jones", "title": "Selective Lambda Lifting", "comments": "Rejected from ICFP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lambda lifting is a well-known transformation, traditionally employed for\ncompiling functional programs to supercombinators. However, more recent\nabstract machines for functional languages like OCaml and Haskell tend to do\nclosure conversion instead for direct access to the environment, so lambda\nlifting is no longer necessary to generate machine code. We propose to revisit\nselective lambda lifting in this context as an optimising code generation\nstrategy and conceive heuristics to identify beneficial lifting opportunities.\nWe give a static analysis for estimating impact on heap allocations of a\nlifting decision. Performance measurements of our implementation within the\nGlasgow Haskell Compiler on a large corpus of Haskell benchmarks suggest modest\nspeedups.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 13:31:21 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 14:53:56 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Graf", "Sebastian", ""], ["Jones", "Simon Peyton", ""]]}, {"id": "1910.11724", "submitter": "Joachim Breitner", "authors": "Antal Spector-Zabusky, Joachim Breitner, Yao Li, Stephanie Weirich", "title": "Embracing a mechanized formalization gap", "comments": "Submitted to CPP'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If a code base is so big and complicated that complete mechanical\nverification is intractable, can we still apply and benefit from verification\nmethods? We show that by allowing a deliberate mechanized formalization gap we\ncan shrink and simplify the model until it is manageable, while still retaining\na meaningful, declaratively documented connection to the original, unmodified\nsource code. Concretely, we translate core parts of the Haskell compiler GHC\ninto Coq, using hs-to-coq, and verify invariants related to the use of term\nvariables.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 13:41:13 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Spector-Zabusky", "Antal", ""], ["Breitner", "Joachim", ""], ["Li", "Yao", ""], ["Weirich", "Stephanie", ""]]}, {"id": "1910.11741", "submitter": "Fabrizio Montesi", "authors": "Lu\\'is Cruz-Filipe, Fabrizio Montesi, Larisa Safina", "title": "Implementing choreography extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choreography extraction deals with the generation of a choreography (a global\ndescription of communication behaviour) from a set of local process behaviours.\nIn this work, we implement a previously proposed theory for extraction and show\nthat, in spite of its theoretical exponential complexity, it is usable in\npractice. We discuss the data structures needed for an efficient\nimplementation, introduce some optimizations, and perform a systematic\npractical evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 14:09:45 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Cruz-Filipe", "Lu\u00eds", ""], ["Montesi", "Fabrizio", ""], ["Safina", "Larisa", ""]]}, {"id": "1910.11924", "submitter": "Matteo Cimini", "authors": "Benjamin Mourad, Matteo Cimini", "title": "A Calculus for Language Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a calculus for expressing algorithms for programming\nlanguages transformations. We present the type system and operational semantics\nof the calculus, and we prove that it is type sound. We have implemented our\ncalculus, and we demonstrate its applicability with common examples in\nprogramming languages. As our calculus manipulates inference systems, our work\ncan, in principle, be applied to logical systems.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 20:31:18 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Mourad", "Benjamin", ""], ["Cimini", "Matteo", ""]]}, {"id": "1910.12256", "submitter": "Yotam Feldman", "authors": "Yotam M. Y. Feldman, Neil Immerman, Mooly Sagiv, Sharon Shoham", "title": "Complexity and Information in Invariant Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the complexity of SAT-based invariant inference, a\nprominent approach to safety verification. We consider the problem of inferring\nan inductive invariant of polynomial length given a transition system and a\nsafety property. We analyze the complexity of this problem in a black-box\nmodel, called the Hoare-query model, which is general enough to capture\nalgorithms such as IC3/PDR and its variants. An algorithm in this model learns\nabout the system's reachable states by querying the validity of Hoare triples.\n  We show that in general an algorithm in the Hoare-query model requires an\nexponential number of queries. Our lower bound is information-theoretic and\napplies even to computationally unrestricted algorithms, showing that no choice\nof generalization from the partial information obtained in a polynomial number\nof Hoare queries can lead to an efficient invariant inference procedure in this\nclass.\n  We then show, for the first time, that by utilizing rich Hoare queries, as\ndone in PDR, inference can be exponentially more efficient than approaches such\nas ICE learning, which only utilize inductiveness checks of candidates. We do\nso by constructing a class of transition systems for which a simple version of\nPDR with a single frame infers invariants in a polynomial number of queries,\nwhereas every algorithm using only inductiveness checks and counterexamples\nrequires an exponential number of queries.\n  Our results also shed light on connections and differences with the classical\ntheory of exact concept learning with queries, and imply that learning from\ncounterexamples to induction is harder than classical exact learning from\nlabeled examples. This demonstrates that the convergence rate of\nCounterexample-Guided Inductive Synthesis depends on the form of\ncounterexamples.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 12:59:48 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 13:27:00 GMT"}, {"version": "v3", "created": "Sat, 18 Jan 2020 20:04:17 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Feldman", "Yotam M. Y.", ""], ["Immerman", "Neil", ""], ["Sagiv", "Mooly", ""], ["Shoham", "Sharon", ""]]}, {"id": "1910.12272", "submitter": "Kazunori Ueda", "authors": "Kazunori Ueda, Hiroshi Hosobe, Daisuke Ishii", "title": "Declarative Semantics of the Hybrid Constraint Language HydLa", "comments": "10 pages, 3 figures. This is an English translation of the paper that\n  originally appeared in Computer Software, Vol.28, No.1 (2011), pp.306-311,\n  doi:10.11309/jssst.28.1_306", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid systems are dynamical systems with continuous evolution of states and\ndiscrete evolution of states and governing equations. We have worked on the\ndesign and implementation of HydLa, a constraint-based modeling language for\nhybrid systems, with a view to the proper handling of uncertainties and the\nintegration of simulation and verification. HydLa's constraint hierarchies\nfacilitate the description of constraints with adequate strength, but its\nsemantical foundations are not obvious due to the interaction of various\nlanguage constructs. This paper gives the declarative semantics of HydLa and\ndiscusses its properties and consequences by means of examples.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2019 14:45:17 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Ueda", "Kazunori", ""], ["Hosobe", "Hiroshi", ""], ["Ishii", "Daisuke", ""]]}, {"id": "1910.12643", "submitter": "Daniel Fava", "authors": "Daniel Schnetzer Fava, Martin Steffen", "title": "Ready, set, Go! Data-race detection and the Go language", "comments": "22nd Brazilian Symposium on Formal Methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data races are often discussed in the context of lock acquisition and\nrelease, with race-detection algorithms routinely relying on vector clocks as a\nmeans of capturing the relative ordering of events from different threads. In\nthis paper, we present a data-race detector for a language with channel\ncommunication as its sole synchronization primitive, and provide a semantics\ndirectly tied to the happens-before relation, thus forging the notion of vector\nclocks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 13:08:22 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 09:34:07 GMT"}, {"version": "v3", "created": "Wed, 15 Jan 2020 12:01:23 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Fava", "Daniel Schnetzer", ""], ["Steffen", "Martin", ""]]}, {"id": "1910.12935", "submitter": "Ming-Ho Yee", "authors": "Ming-Ho Yee, Ayaz Badouraly, Ond\\v{r}ej Lhot\\'ak, Frank Tip, and Jan\n  Vitek", "title": "Precise Dataflow Analysis of Event-Driven Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-driven programming is widely used for implementing user interfaces, web\napplications, and non-blocking I/O. An event-driven program is organized as a\ncollection of event handlers whose execution is triggered by events.\nTraditional static analysis techniques are unable to reason precisely about\nevent-driven code because they conservatively assume that event handlers may\nexecute in any order. This paper proposes an automatic transformation from\nInterprocedural Finite Distributive Subset (IFDS) problems to Interprocedural\nDistributed Environment (IDE) problems as a general solution to obtain precise\nstatic analysis of event-driven applications; problems in both forms can be\nsolved by existing implementations. Our contribution is to show how to improve\nanalysis precision by automatically enriching the former with information about\nthe state of event handlers to filter out infeasible paths. We prove the\ncorrectness of our transformation and report on experiments with a\nproof-of-concept implementation for a subset of JavaScript.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 19:47:56 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Yee", "Ming-Ho", ""], ["Badouraly", "Ayaz", ""], ["Lhot\u00e1k", "Ond\u0159ej", ""], ["Tip", "Frank", ""], ["Vitek", "Jan", ""]]}, {"id": "1910.13324", "submitter": "Yuan Zhou", "authors": "Yuan Zhou, Hongseok Yang, Yee Whye Teh and Tom Rainforth", "title": "Divide, Conquer, and Combine: a New Inference Strategy for Probabilistic\n  Programs with Stochastic Support", "comments": "Published at the 37th International Conference on Machine Learning\n  (ICML 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal probabilistic programming systems (PPSs) provide a powerful\nframework for specifying rich probabilistic models. They further attempt to\nautomate the process of drawing inferences from these models, but doing this\nsuccessfully is severely hampered by the wide range of non--standard models\nthey can express. As a result, although one can specify complex models in a\nuniversal PPS, the provided inference engines often fall far short of what is\nrequired. In particular, we show that they produce surprisingly unsatisfactory\nperformance for models where the support varies between executions, often doing\nno better than importance sampling from the prior. To address this, we\nintroduce a new inference framework: Divide, Conquer, and Combine, which\nremains efficient for such models, and show how it can be implemented as an\nautomated and generic PPS inference engine. We empirically demonstrate\nsubstantial performance improvements over existing approaches on three\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 15:36:56 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 23:21:21 GMT"}, {"version": "v3", "created": "Thu, 16 Jul 2020 17:26:12 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Zhou", "Yuan", ""], ["Yang", "Hongseok", ""], ["Teh", "Yee Whye", ""], ["Rainforth", "Tom", ""]]}, {"id": "1910.13346", "submitter": "Changxi Liu", "authors": "Changxi Liu and Hailong Yang and Xu Liu and Zhongzhi Luan and Depei\n  Qian", "title": "Intelligent-Unrolling: Exploiting Regular Patterns in Irregular\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern optimizing compilers are able to exploit memory access or computation\npatterns to generate vectorization codes. However, such patterns in irregular\napplications are unknown until runtime due to the input dependence. Thus,\neither compiler's static optimization or profile-guided optimization based on\nspecific inputs cannot predict the patterns for any common input, which leads\nto suboptimal code generation. To address this challenge, we develop\nIntelligent-Unroll, a framework to automatically optimize irregular\napplications with vectorization. Intelligent-Unroll allows the users to depict\nthe computation task using \\textit{code seed} with the memory access and\ncomputation patterns represented in \\textit{feature table} and\n\\textit{information-code tree}, and generates highly efficient codes.\nFurthermore, Intelligent-Unroll employs several novel optimization techniques\nto optimize reduction operations and gather/scatter instructions. We evaluate\nIntelligent-Unroll with sparse matrix-vector multiplication (SpMV) and graph\napplications. Experimental results show that Intelligent-Unroll is able to\ngenerate more efficient vectorization codes compared to the state-of-the-art\nimplementations.\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 04:23:49 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Liu", "Changxi", ""], ["Yang", "Hailong", ""], ["Liu", "Xu", ""], ["Luan", "Zhongzhi", ""], ["Qian", "Depei", ""]]}, {"id": "1910.14560", "submitter": "David Naumann", "authors": "Anindya Banerjee and Ramana Nagasamudram and David A. Naumann and\n  Mohammad Nikouei", "title": "A Relational Program Logic with Data Abstraction and Dynamic Framing", "comments": "Submitted for publication. Version 3: Revise exposition with more\n  examples; add case study; add author; revise title, add index, minor changes\n  throughout. Version 4: Improved relational second order frame rule; revised\n  and expanded examples and description of prototype; polish exposition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a paper published in 1972 Hoare articulated the fundamental notions of\nhiding invariants and simulations. Hiding: invariants on encapsulated data\nrepresentations need not be mentioned in specifications that comprise the API\nof a module. Simulation: correctness of a new data representation and\nimplementation can be established by proving simulation between the old and new\nimplementations using a coupling relation defined on the encapsulated state.\nThese results were formalized semantically and for a simple model of state,\nthough the paper claimed this could be extended to encompass dynamically\nallocated objects. In recent years, progress has been made towards formalizing\nthe claim, for simulation, though mainly in semantic developments. In this\npaper, the ideas are combined with the idea in Hoare's 1969 paper: a logic of\nprograms. For a language with dynamically allocated shared mutable objects, we\nintroduce a relational Hoare logic that formalizes encapsulation, hiding of\ninvariants, and relating two implementations via coupling relations. Relations\nand other assertions are expressed in first order logic. Specifications can\nexpress a wide range of relational properties such as conditional equivalence\nand noninterference with declassification. The proof rules facilitate reasoning\nby means of convenient alignments and are shown sound with respect to a\nconventional operational semantics. Applicability to representative examples of\ndata abstraction is demonstrated using an SMT-based implementation.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 16:05:54 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 01:02:26 GMT"}, {"version": "v3", "created": "Fri, 30 Oct 2020 03:23:54 GMT"}, {"version": "v4", "created": "Sun, 30 May 2021 04:59:56 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Banerjee", "Anindya", ""], ["Nagasamudram", "Ramana", ""], ["Naumann", "David A.", ""], ["Nikouei", "Mohammad", ""]]}, {"id": "1910.14619", "submitter": "Anthony Vandikas", "authors": "Azadeh Farzan and Anthony Vandikas", "title": "Reductions for Safety Proofs (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program reductions are used widely to simplify reasoning about the\ncorrectness of concurrent and distributed programs. In this paper, we propose a\ngeneral approach to proof simplification of concurrent programs based on\nexploring generic classes of reductions. We introduce two classes of sound\nprogram reductions, study their theoretical properties, show how they can be\neffectively used in algorithmic verification, and demonstrate that they are\nvery effective in producing proofs of a diverse class of programs without\ntargeting specific syntactic properties of these programs. The most novel\ncontribution of this paper is the introduction of the concept of context in the\ndefinition of program reductions. We demonstrate how commutativity of program\nsteps in some program contexts can be used to define a generic class of sound\nreductions which can be used to automatically produce proofs for programs whose\ncomplete Floyd-Hoare style proofs are theoretically beyond the reach of\nautomated verification technology of today.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 17:05:40 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Farzan", "Azadeh", ""], ["Vandikas", "Anthony", ""]]}]