[{"id": "2012.00556", "submitter": "Joxan Jaffar", "authors": "Joxan Jaffar, Rasool Maghareh, Sangharatna Godboley, Xuan-Linh Ha", "title": "TracerX: Dynamic Symbolic Execution with Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Symbolic Execution (DSE) is an important method for the testing of\nprograms. An important system on DSE is KLEE which inputs a C/C++ program\nannotated with symbolic variables, compiles it into LLVM, and then emulates the\nexecution paths of LLVM using a specified backtracking strategy. The major\nchallenge in symbolic execution is path explosion. The method of abstraction\nlearning has been used to address this. The key step here is the computation of\nan interpolant to represent the learnt abstraction. In this paper, we present a\nnew interpolation algorithm and implement it on top of the KLEE system. The\nmain objective is to address the path explosion problem in pursuit of code\npenetration: to prove that a target program point is either reachable or\nunreachable. That is, our focus is verification. We show that despite the\noverhead of computing interpolants, the pruning of the symbolic execution tree\nthat interpolants provide often brings significant overall benefits. We then\nperformed a comprehensive experimental evaluation against KLEE, as well as\nagainst one well-known system that is based on Static Symbolic Execution, CBMC.\nOur primary experiment shows code penetration success at a new level,\nparticularly so when the target is hard to determine. A secondary experiment\nshows that our implementation is competitive for testing.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 15:02:38 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Jaffar", "Joxan", ""], ["Maghareh", "Rasool", ""], ["Godboley", "Sangharatna", ""], ["Ha", "Xuan-Linh", ""]]}, {"id": "2012.00818", "submitter": "Mat\\'u\\v{s} Sul\\'ir", "authors": "Mat\\'u\\v{s} Sul\\'ir, Jaroslav Porub\\\"an", "title": "Designing Voice-Controllable APIs", "comments": null, "journal-ref": "2019 IEEE 15th International Scientific Conference on Informatics,\n  IEEE, 2019, pp. 393-398", "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main purpose of a voice command system is to process a sentence in\nnatural language and perform the corresponding action. Although there exist\nmany approaches to map sentences to API (application programming interface)\ncalls, this mapping is usually performed after the API is already implemented,\npossibly by other programmers. In this paper, we describe how the API developer\ncan use patterns to map sentences to API calls by utilizing the similarities\nbetween names and types in the sentences and the API. In the cases when the\nmapping is not straightforward, we suggest the usage of suitable annotations\n(attribute-oriented programming).\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 20:40:19 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Sul\u00edr", "Mat\u00fa\u0161", ""], ["Porub\u00e4n", "Jaroslav", ""]]}, {"id": "2012.00829", "submitter": "Mat\\'u\\v{s} Sul\\'ir", "authors": "Milan Nos\\'a\\v{l}, Jaroslav Porub\\\"an, Mat\\'u\\v{s} Sul\\'ir", "title": "Customizing Host IDE for Non-programming Users of Pure Embedded DSLs: A\n  Case Study", "comments": null, "journal-ref": "Computer Languages, Systems and Structures (COMLAN), Vol. 49,\n  2017, pp. 101-118", "doi": "10.1016/j.cl.2017.04.003", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pure embedding as an implementation strategy of domain-specific languages\n(DSLs) benefits from low implementation costs. On the other hand, it introduces\nundesired syntactic noise that impedes involvement of non-programming domain\nexperts. Due to this, pure embedded DSLs are generally not intended for, nor\nused by, non-programmers. In this work, we try to challenge this state by\nexperimenting with inexpensive customizations of the host IDE (Integrated\nDevelopment Environment) to reduce the negative impact of syntactic noise. We\npresent several techniques and recommendations based on standard IDE features\n(e.g., file templates, code folding, etc.) that aim to reduce syntactic noise\nand generally improve the user experience with pure embedded DSLs. The\ntechniques are presented using a NetBeans IDE case study. The goal of the\nproposed techniques is to improve the user experience with pure embedded DSLs\nwith a focus on the involvement of non-programming domain experts (or\nnon-programmers in general). The proposed techniques were evaluated using a\ncontrolled experiment. The experiment compared a group using Ruby and\nnon-modified RubyMine IDE versus a group using Java and NetBeans IDE customized\nto use the proposed techniques. Experiment results indicate that even\ninexpensive host IDE customizations can significantly alleviate issues caused\nby the syntactic noise: Java with its inflexible syntax performed better than\nRuby with its concise syntax.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 21:04:33 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Nos\u00e1\u013e", "Milan", ""], ["Porub\u00e4n", "Jaroslav", ""], ["Sul\u00edr", "Mat\u00fa\u0161", ""]]}, {"id": "2012.01067", "submitter": "Egor Namakonov", "authors": "Ori Lahav (1), Egor Namakonov (2 and 3), Jonas Oberhauser (4 and 5),\n  Anton Podkopaev (3 and 6), Viktor Vafeiadis (7) ((1) Tel Aviv University, (2)\n  St Petersburg University, (3) JetBrains Research, (4) Huawei Dresden Research\n  Center, (5) Huawei OS Kernel Lab, (6) NRU HSE, (7) MPI-SWS)", "title": "Making Weak Memory Models Fair", "comments": "24 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We observe that the standard notion of thread fairness is insufficient for\nguaranteeing termination of even the simplest shared-memory programs under weak\nmemory models. Guaranteeing termination requires additional model-specific\nfairness constraints, which we call memory fairness. In the case of acyclic\ndeclarative memory models, such as TSO and RA, we show that memory fairness can\nbe equivalently expressed in a uniform fashion as prefix-finiteness of an\nextended coherence order. This uniform memory fairness representation yields\nthe first effective way for proving termination of spinloops under weak memory\nconsistency.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 10:22:23 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Lahav", "Ori", "", "2 and 3"], ["Namakonov", "Egor", "", "2 and 3"], ["Oberhauser", "Jonas", "", "4 and 5"], ["Podkopaev", "Anton", "", "3 and 6"], ["Vafeiadis", "Viktor", ""]]}, {"id": "2012.01303", "submitter": "Valentin Iovene", "authors": "Valentin Iovene (NEUROSPIN, PARIETAL), Gaston Zanitti (NEUROSPIN,\n  PARIETAL), Demian Wassermann (NEUROSPIN, PARIETAL)", "title": "Complex Coordinate-Based Meta-Analysis with Probabilistic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing number of published functional magnetic resonance imaging\n(fMRI) studies, meta-analysis databases and models have become an integral part\nof brain mapping research. Coordinate-based meta-analysis (CBMA) databases are\nbuilt by automatically extracting both coordinates of reported peak activations\nand term associations using natural language processing (NLP) techniques.\nSolving term-based queries on these databases make it possible to obtain\nstatistical maps of the brain related to specific cognitive processes. However,\nwith tools like Neurosynth, only singleterm queries lead to statistically\nreliable results. When solving richer queries, too few studies from the\ndatabase contribute to the statistical estimations. We design a probabilistic\ndomain-specific language (DSL) standing on Datalog and one of its probabilistic\nextensions, CP-Logic, for expressing and solving rich logic-based queries. We\nencode a CBMA database into a probabilistic program. Using the joint\ndistribution of its Bayesian network translation, we show that solutions of\nqueries on this program compute the right probability distributions of voxel\nactivations. We explain how recent lifted query processing algorithms make it\npossible to scale to the size of large neuroimaging data, where state of the\nart knowledge compilation (KC) techniques fail to solve queries fast enough for\npractical applications. Finally, we introduce a method for relating studies to\nterms probabilistically, leading to better solutions for conjunctive queries on\nsmaller databases. We demonstrate results for two-term conjunctive queries,\nboth on simulated meta-analysis databases and on the widely-used Neurosynth\ndatabase.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 16:16:26 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 13:36:50 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Iovene", "Valentin", "", "NEUROSPIN, PARIETAL"], ["Zanitti", "Gaston", "", "NEUROSPIN,\n  PARIETAL"], ["Wassermann", "Demian", "", "NEUROSPIN, PARIETAL"]]}, {"id": "2012.01470", "submitter": "Chris Cummins", "authors": "Chris Cummins, Hugh Leather, Zacharias Fisches, Tal Ben-Nun, Torsten\n  Hoefler, Michael O'Boyle", "title": "Deep Data Flow Analysis", "comments": "9 pages, plus appendices. arXiv admin note: text overlap with\n  arXiv:2003.10536", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compiler architects increasingly look to machine learning when building\nheuristics for compiler optimization. The promise of automatic heuristic\ndesign, freeing the compiler engineer from the complex interactions of program,\narchitecture, and other optimizations, is alluring. However, most machine\nlearning methods cannot replicate even the simplest of the abstract\ninterpretations of data flow analysis that are critical to making good\noptimization decisions. This must change for machine learning to become the\ndominant technology in compiler heuristics.\n  To this end, we propose ProGraML - Program Graphs for Machine Learning - a\nlanguage-independent, portable representation of whole-program semantics for\ndeep learning. To benchmark current and future learning techniques for compiler\nanalyses we introduce an open dataset of 461k Intermediate Representation (IR)\nfiles for LLVM, covering five source programming languages, and 15.4M\ncorresponding data flow results. We formulate data flow analysis as an MPNN and\nshow that, using ProGraML, standard analyses can be learned, yielding improved\nperformance on downstream compiler optimization tasks.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 03:29:14 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Cummins", "Chris", ""], ["Leather", "Hugh", ""], ["Fisches", "Zacharias", ""], ["Ben-Nun", "Tal", ""], ["Hoefler", "Torsten", ""], ["O'Boyle", "Michael", ""]]}, {"id": "2012.01658", "submitter": "EPTCS", "authors": "Dominique Duval (CNRS and Univ. Grenoble Alpes, France), Rachid\n  Echahed (CNRS and Univ. Grenoble Alpes, France), Fr\\'ed\\'eric Prost (CNRS and\n  Univ. Grenoble Alpes, France)", "title": "An Algebraic Graph Transformation Approach for RDF and SPARQL", "comments": "In Proceedings GCM 2020, arXiv:2012.01181. arXiv admin note:\n  substantial text overlap with arXiv:1910.07519", "journal-ref": "EPTCS 330, 2020, pp. 55-70", "doi": "10.4204/EPTCS.330.4", "report-no": null, "categories": "cs.DB cs.PL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the recommendations of the World Wide Web Consortium (W3C) about\nRDF framework and its associated query language SPARQL. We propose a new formal\nframework based on category theory which provides clear and concise formal\ndefinitions of the main basic features of RDF and SPARQL. We define RDF graphs\nas well as SPARQL basic graph patterns as objects of some nested categories.\nThis allows one to clarify, in particular, the role of blank nodes.\nFurthermore, we consider basic SPARQL CONSTRUCT and SELECT queries and\nformalize their operational semantics following a novel algebraic graph\ntransformation approach called POIM.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 02:27:57 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Duval", "Dominique", "", "CNRS and Univ. Grenoble Alpes, France"], ["Echahed", "Rachid", "", "CNRS and Univ. Grenoble Alpes, France"], ["Prost", "Fr\u00e9d\u00e9ric", "", "CNRS and\n  Univ. Grenoble Alpes, France"]]}, {"id": "2012.02154", "submitter": "Kartik Singhal", "authors": "Kartik Singhal", "title": "Quantum Hoare Type Theory", "comments": "33 pages, 3 figures, 12 code listings. Draft master's paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.ET cs.LO quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As quantum computers become real, it is high time we come up with effective\ntechniques that help programmers write correct quantum programs. Inspired by\nHoare Type Theory in classical computing, we propose Quantum Hoare Type Theory\n(QHTT) in which precise specifications about the modification to the quantum\nstate can be provided within the type of a computation. These specifications\nwithin a Hoare type are given in the form of Hoare-logic style pre- and\npostconditions following the propositions-as-types principle. The type-checking\nprocess verifies that the implementation conforms to the provided\nspecification. QHTT has the potential to be a unified system for programming,\nspecifying, and reasoning about quantum programs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:41:08 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Singhal", "Kartik", ""]]}, {"id": "2012.02193", "submitter": "EPTCS", "authors": "Brian Courtehoute (University of York, United Kingdom), Detlef Plump\n  (University of York, United Kingdom)", "title": "A Fast Graph Program for Computing Minimum Spanning Trees", "comments": "In Proceedings GCM 2020, arXiv:2012.01181", "journal-ref": "EPTCS 330, 2020, pp. 163-180", "doi": "10.4204/EPTCS.330.10", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When using graph transformation rules to implement graph algorithms, a\nchallenge is to match the efficiency of programs in conventional languages. To\nhelp overcome that challenge, the graph programming language GP 2 features\nrooted rules which, under mild conditions, can match in constant time on\nbounded degree graphs. In this paper, we present an efficient GP 2 program for\ncomputing minimum spanning trees. We provide empirical performance results as\nevidence for the program's subquadratic complexity on bounded degree graphs.\nThis is achieved using depth-first search as well as rooted graph\ntransformation. The program is based on Boruvka's algorithm for minimum\nspanning trees. Our performance results show that the program's time complexity\nis consistent with that of classical implementations of Boruvka's algorithm,\nnamely O(m log n), where m is the number of edges and n the number of nodes.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 02:29:49 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Courtehoute", "Brian", "", "University of York, United Kingdom"], ["Plump", "Detlef", "", "University of York, United Kingdom"]]}, {"id": "2012.03538", "submitter": "Alexander Okhotin", "authors": "Alexander Okhotin", "title": "Describing the syntax of programming languages using conjunctive and\n  Boolean grammars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A classical result by Floyd (\"On the non-existence of a phrase structure\ngrammar for ALGOL 60\", 1962) states that the complete syntax of any sensible\nprogramming language cannot be described by the ordinary kind of formal\ngrammars (Chomsky's ``context-free''). This paper uses grammars extended with\nconjunction and negation operators, known as conjunctive grammars and Boolean\ngrammars, to describe the set of well-formed programs in a simple typeless\nprocedural programming language. A complete Boolean grammar, which defines such\nconcepts as declaration of variables and functions before their use, is\nconstructed and explained. Using the Generalized LR parsing algorithm for\nBoolean grammars, a program can then be parsed in time $O(n^4)$ in its length,\nwhile another known algorithm allows subcubic-time parsing. Next, it is shown\nhow to transform this grammar to an unambiguous conjunctive grammar, with\nsquare-time parsing. This becomes apparently the first specification of the\nsyntax of a programming language entirely by a computationally feasible formal\ngrammar.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 09:05:48 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Okhotin", "Alexander", ""]]}, {"id": "2012.05270", "submitter": "Alessio Colucci", "authors": "Alessio Colucci, D\\'avid Juh\\'asz, Martin Mosbeck, Alberto Marchisio,\n  Semeen Rehman, Manfred Kreutzer, Guenther Nadbath, Axel Jantsch and Muhammad\n  Shafique", "title": "MLComp: A Methodology for Machine Learning-based Performance Estimation\n  and Adaptive Selection of Pareto-Optimal Compiler Optimization Sequences", "comments": "Accepted for publication at the 24th IEEE/ACM Design, Automation and\n  Test in Europe (DATE'21) Conference, February, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedded systems have proliferated in various consumer and industrial\napplications with the evolution of Cyber-Physical Systems and the Internet of\nThings. These systems are subjected to stringent constraints so that embedded\nsoftware must be optimized for multiple objectives simultaneously, namely\nreduced energy consumption, execution time, and code size. Compilers offer\noptimization phases to improve these metrics. However, proper selection and\nordering of them depends on multiple factors and typically requires expert\nknowledge. State-of-the-art optimizers facilitate different platforms and\napplications case by case, and they are limited by optimizing one metric at a\ntime, as well as requiring a time-consuming adaptation for different targets\nthrough dynamic profiling.\n  To address these problems, we propose the novel MLComp methodology, in which\noptimization phases are sequenced by a Reinforcement Learning-based policy.\nTraining of the policy is supported by Machine Learning-based analytical models\nfor quick performance estimation, thereby drastically reducing the time spent\nfor dynamic profiling. In our framework, different Machine Learning models are\nautomatically tested to choose the best-fitting one. The trained Performance\nEstimator model is leveraged to efficiently devise Reinforcement Learning-based\nmulti-objective policies for creating quasi-optimal phase sequences.\n  Compared to state-of-the-art estimation models, our Performance Estimator\nmodel achieves lower relative error (<2%) with up to 50x faster training time\nover multiple platforms and application domains. Our Phase Selection Policy\nimproves execution time and energy consumption of a given code by up to 12% and\n6%, respectively. The Performance Estimator and the Phase Selection Policy can\nbe trained efficiently for any target platform and application domain.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 19:13:39 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 11:53:33 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Colucci", "Alessio", ""], ["Juh\u00e1sz", "D\u00e1vid", ""], ["Mosbeck", "Martin", ""], ["Marchisio", "Alberto", ""], ["Rehman", "Semeen", ""], ["Kreutzer", "Manfred", ""], ["Nadbath", "Guenther", ""], ["Jantsch", "Axel", ""], ["Shafique", "Muhammad", ""]]}, {"id": "2012.05863", "submitter": "Aleksandar S. Dimovski", "authors": "Aleksandar S. Dimovski, Sven Apel, Axel Legay", "title": "A Decision Tree Lifted Domain for Analyzing Program Families with\n  Numerical Features (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lifted (family-based) static analysis by abstract interpretation is capable\nof analyzing all variants of a program family simultaneously, in a single run\nwithout generating any of the variants explicitly. The elements of the\nunderlying lifted analysis domain are tuples, which maintain one property per\nvariant. Still, explicit property enumeration in tuples, one by one for all\nvariants, immediately yields combinatorial explosion. This is particularly\napparent in the case of program families that, apart from Boolean features,\ncontain also numerical features with big domains, thus admitting astronomic\nconfiguration spaces.\n  The key for an efficient lifted analysis is proper handling of\nvariability-specific constructs of the language (e.g., feature-based runtime\ntests and #if directives). In this work, we introduce a new symbolic\nrepresentation of the lifted abstract domain that can efficiently analyze\nprogram families with numerical features. This makes sharing between property\nelements corresponding to different variants explicitly possible. The elements\nof the new lifted domain are constraint-based decision trees, where decision\nnodes are labeled with linear constraints defined over numerical features and\nthe leaf nodes belong to an existing single-program analysis domain. To\nillustrate the potential of this representation, we have implemented an\nexperimental lifted static analyzer, called SPLNUM^2Analyzer, for inferring\ninvariants of C programs. It uses existing numerical domains (e.g., intervals,\noctagons, polyhedra) from the APRON library as parameters. An empirical\nevaluation on benchmarks from SV-COMP and BusyBox yields promising preliminary\nresults indicating that our decision trees-based approach is effective and\noutperforms the tuple-based approach, which is used as a baseline lifted\nanalysis based on abstract interpretation.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 18:21:15 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Dimovski", "Aleksandar S.", ""], ["Apel", "Sven", ""], ["Legay", "Axel", ""]]}, {"id": "2012.06382", "submitter": "Marat Akhin", "authors": "Daniil Stepanov, Marat Akhin, Mikhail Belyaev", "title": "Type-Centric Kotlin Compiler Fuzzing: Preserving Test Program\n  Correctness by Preserving Types", "comments": "Accepted to: 2021 IEEE International Conference on Software Testing,\n  Verification and Validation (ICST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kotlin is a relatively new programming language from JetBrains: its\ndevelopment started in 2010 with release 1.0 done in early 2016. The Kotlin\ncompiler, while slowly and steadily becoming more and more mature, still\ncrashes from time to time on the more tricky input programs, not least because\nof the complexity of its features and their interactions. This makes it a great\ntarget for fuzzing, even the basic forms of which can find a significant number\nof Kotlin compiler crashes.\n  There is a problem with fuzzing, however, closely related to the cause of the\ncrashes: generating a random, non-trivial and semantically valid Kotlin program\nis hard. In this paper, we talk about type-centric compiler fuzzing in the form\nof type-centric enumeration, an approach inspired by skeletal program\nenumeration and based on a combination of generative and mutation-based\nfuzzing, which solves this problem by focusing on program types. After creating\nthe skeleton program, we fill the typed holes with fragments of suitable type,\ncreated via generation and enhanced by semantic-aware mutation.\n  We implemented this approach in our Kotlin compiler fuzzing framework called\nBackend Bug Finder (BBF) and did an extensive evaluation, not only testing the\nreal-world feasibility of our approach, but also comparing it to other compiler\nfuzzing techniques. The results show our approach to be significantly better\ncompared to other fuzzing approaches at generating semantically valid Kotlin\nprograms, while creating more interesting crash-inducing inputs at the same\ntime. We managed to find more than 50 previously unknown compiler crashes, of\nwhich 18 were considered important after their triage by the compiler team.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 14:32:37 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Stepanov", "Daniil", ""], ["Akhin", "Marat", ""], ["Belyaev", "Mikhail", ""]]}, {"id": "2012.06530", "submitter": "Tom Hirschowitz", "authors": "Andr\\'e Hirschowitz, Tom Hirschowitz and Ambroise Lafont", "title": "Modules over monads and operational semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO math.CT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is a contribution to the search for efficient and high-level\nmathematical tools to specify and reason about (abstract) programming languages\nor calculi. Generalising the reduction monads of Ahrens et al., we introduce\ntransition monads, thus covering new applications such as\nlambda-bar-mu-calculus, pi-calculus, Positive GSOS specifications, differential\nlambda-calculus, and the big-step, simply-typed, call-by-value lambda-calculus.\nMoreover, we design a suitable notion of signature for transition monads.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 17:51:50 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Hirschowitz", "Andr\u00e9", ""], ["Hirschowitz", "Tom", ""], ["Lafont", "Ambroise", ""]]}, {"id": "2012.06981", "submitter": "Stephen Macke", "authors": "Stephen Macke, Hongpu Gong, Doris Jung-Lin Lee, Andrew Head, Doris\n  Xin, Aditya Parameswaran", "title": "Fine-Grained Lineage for Safer Notebook Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB cs.HC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computational notebooks have emerged as the platform of choice for data\nscience and analytical workflows, enabling rapid iteration and exploration. By\nkeeping intermediate program state in memory and segmenting units of execution\ninto so-called \"cells\", notebooks allow users to execute their workflows\ninteractively and enjoy particularly tight feedback. However, as cells are\nadded, removed, reordered, and rerun, this hidden intermediate state\naccumulates in a way that is not necessarily correlated with the notebook's\nvisible code, making execution behavior difficult to reason about, and leading\nto errors and lack of reproducibility. We present NBSafety, a custom Jupyter\nkernel that uses runtime tracing and static analysis to automatically manage\nlineage associated with cell execution and global notebook state. NBSafety\ndetects and prevents errors that users make during unaided notebook\ninteractions, all while preserving the flexibility of existing notebook\nsemantics. We evaluate NBSafety's ability to prevent erroneous interactions by\nreplaying and analyzing 666 real notebook sessions. Of these, NBSafety\nidentified 117 sessions with potential safety errors, and in the remaining 549\nsessions, the cells that NBSafety identified as resolving safety issues were\nmore than $7\\times$ more likely to be selected by users for re-execution\ncompared to a random baseline, even though the users were not using NBSafety\nand were therefore not influenced by its suggestions.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 06:50:31 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 20:20:44 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Macke", "Stephen", ""], ["Gong", "Hongpu", ""], ["Lee", "Doris Jung-Lin", ""], ["Head", "Andrew", ""], ["Xin", "Doris", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "2012.07023", "submitter": "Nghi D. Q. Bui", "authors": "Nghi D. Q. Bui, Yijun Yu, Lingxiao Jiang", "title": "InferCode: Self-Supervised Learning of Code Representations by\n  Predicting Subtrees", "comments": "Accepted at ICSE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building deep learning models on source code has found many successful\nsoftware engineering applications, such as code search, code comment\ngeneration, bug detection, code migration, and so on. Current learning\ntechniques, however, have a major drawback that these models are mostly trained\non datasets labeled for particular downstream tasks, and code representations\nmay not be suitable for other tasks. While some techniques produce\nrepresentations from unlabeled code, they are far from satisfactory when\napplied to downstream tasks. Although certain techniques generate\nrepresentations from unlabeled code when applied to downstream tasks they are\nfar from satisfactory. This paper proposes InferCode to overcome the limitation\nby adapting the self-supervised learning mechanism to build source code model.\nThe key novelty lies in training code representations by predicting\nautomatically identified subtrees from the context of the ASTs. Subtrees in\nASTs are treated with InferCode as the labels for training code representations\nwithout any human labeling effort or the overhead of expensive graph\nconstruction, and the trained representations are no longer tied to any\nspecific downstream tasks or code units. We trained an InferCode model instance\nusing the Tree-based CNN as the encoder of a large set of Java code and applied\nit to downstream unsupervised tasks such as code clustering, code clone\ndetection, cross-language code search or reused under a transfer learning\nscheme to continue training the model weights for supervised tasks such as code\nclassification and method name prediction. Compared to previous code learning\ntechniques applied to the same downstream tasks, such as Code2Vec, Code2Seq,\nASTNN, higher performance results are achieved using our pre-trained InferCode\nmodel with a significant margin for most tasks including those involving\ndifferent programming languages.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 10:33:41 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 16:37:23 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Bui", "Nghi D. Q.", ""], ["Yu", "Yijun", ""], ["Jiang", "Lingxiao", ""]]}, {"id": "2012.07145", "submitter": "Luke Anderson", "authors": "Luke Anderson, Andrew Adams, Karima Ma, Tzu-Mao Li, Jonathan\n  Ragan-Kelley", "title": "Learning to Schedule Halide Pipelines for the GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm to automatically generate high-performance GPU\nimplementations of complex imaging and machine learning pipelines, directly\nfrom high-level Halide algorithm code. It is fully automatic, requiring no\nschedule templates or hand-optimized kernels, and it targets a diverse range of\ncomputations which is significantly broader than existing autoschedulers. We\naddress the scalability challenge of extending previous approaches to schedule\nlarge real world programs, while enabling a broad set of program rewrites that\ntake into account the nested parallelism and memory hierarchy introduced by GPU\narchitectures. We achieve this using a hierarchical sampling strategy that\ngroups programs into buckets based on their structural similarity, then samples\nrepresentatives to be evaluated, allowing us to explore a large space by only\nconsidering a subset of the space, and a pre-pass that 'freezes' decisions for\nthe lowest cost sections of a program, allowing more time to be spent on the\nimportant stages. We then apply an efficient cost model combining machine\nlearning, program analysis, and GPU architecture knowledge. Our method scales\ncombinatorially better with respect to the deeper nested parallelism required\nby GPUs compared to previous work. We evaluate its performance on a diverse\nsuite of real-world imaging and machine learning pipelines. We demonstrate\nresults that are on average 1.66X faster than existing automatic solutions (up\nto 5X), and competitive with what the best human experts were able to achieve\nin an active effort to beat our automatic results.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 20:40:08 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Anderson", "Luke", ""], ["Adams", "Andrew", ""], ["Ma", "Karima", ""], ["Li", "Tzu-Mao", ""], ["Ragan-Kelley", "Jonathan", ""]]}, {"id": "2012.07581", "submitter": "Mayank Agarwal", "authors": "Mayank Agarwal, Kartik Talamadupula, Stephanie Houde, Fernando\n  Martinez, Michael Muller, John Richards, Steven Ross, Justin D. Weisz", "title": "Quality Estimation & Interpretability for Code Translation", "comments": "NeurIPS 2020 Workshop on Computer-Assisted Programming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the automated translation of source code from one programming\nlanguage to another by using automatic approaches inspired by Neural Machine\nTranslation (NMT) methods for natural languages has come under study. However,\nsuch approaches suffer from the same problem as previous NMT approaches on\nnatural languages, viz. the lack of an ability to estimate and evaluate the\nquality of the translations; and consequently ascribe some measure of\ninterpretability to the model's choices. In this paper, we attempt to estimate\nthe quality of source code translations built on top of the TransCoder model.\nWe consider the code translation task as an analog of machine translation (MT)\nfor natural languages, with some added caveats. We present our main motivation\nfrom a user study built around code translation; and present a technique that\ncorrelates the confidences generated by that model to lint errors in the\ntranslated code. We conclude with some observations on these correlations, and\nsome ideas for future work.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 19:56:11 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 20:49:06 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Agarwal", "Mayank", ""], ["Talamadupula", "Kartik", ""], ["Houde", "Stephanie", ""], ["Martinez", "Fernando", ""], ["Muller", "Michael", ""], ["Richards", "John", ""], ["Ross", "Steven", ""], ["Weisz", "Justin D.", ""]]}, {"id": "2012.07711", "submitter": "Ji Liu", "authors": "Ji Liu, Luciano Bello, Huiyang Zhou", "title": "Relaxed Peephole Optimization: A Novel Compiler Optimization for Quantum\n  Circuits", "comments": "14 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel quantum compiler optimization, named\nrelaxed peephole optimization (RPO) for quantum computers. RPO leverages the\nsingle-qubit state information that can be determined statically by the\ncompiler. We define that a qubit is in a basis state when, at a given point in\ntime, its state is either in the X-, Y-, or Z-basis. When basis qubits are used\nas inputs to quantum gates, there exist opportunities for strength reduction,\nwhich replaces quantum operations with equivalent but less expensive ones.\nCompared to the existing peephole optimization for quantum programs, the\ndifference is that our proposed optimization does not require an identical\nunitary matrix, thereby named `relaxed' peephole optimization. We also extend\nour approach to optimize the quantum gates when some input qubits are in known\npure states. Both optimizations, namely the Quantum Basis-state Optimization\n(QBO) and the Quantum Pure-state Optimization (QPO), are implemented in the\nIBM's Qiskit transpiler. Our experimental results show that our proposed\noptimization pass is fast and effective. The circuits optimized with our\ncompiler optimizations obtain up to 18.0% (11.7% on average) fewer CNOT gates\nand up to 8.2% (7.1% on average) lower transpilation time than that of the most\naggressive optimization level in the Qiskit compiler. When running on real\nquantum computers, the success rates of 3-qubit quantum phase estimation\nalgorithm improve by 2.30X due to the reduced gate counts.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 17:03:06 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Liu", "Ji", ""], ["Bello", "Luciano", ""], ["Zhou", "Huiyang", ""]]}, {"id": "2012.07990", "submitter": "Changwan Hong", "authors": "Ajay Brahmakshatriya, Yunming Zhang, Changwan Hong, Shoaib Kamil,\n  Julian Shun, Saman Amarasinghe", "title": "Compilation Techniques for Graph Algorithms on GPUs", "comments": "This paper appears in International Symposium on Code Generation and\n  Optimization (CGO) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of graph programs depends highly on the algorithm, the size\nand structure of the input graphs, as well as the features of the underlying\nhardware. No single set of optimizations or one hardware platform works well\nacross all settings. To achieve high performance, the programmer must carefully\nselect which set of optimizations and hardware platforms to use. The GraphIt\nprogramming language makes it easy for the programmer to write the algorithm\nonce and optimize it for different inputs using a scheduling language. However,\nGraphIt currently has no support for generating high performance code for GPUs.\nProgrammers must resort to re-implementing the entire algorithm from scratch in\na low-level language with an entirely different set of abstractions and\noptimizations in order to achieve high performance on GPUs.\n  We propose GG, an extension to the GraphIt compiler framework, that achieves\nhigh performance on both CPUs and GPUs using the same algorithm specification.\nGG significantly expands the optimization space of GPU graph processing\nframeworks with a novel GPU scheduling language and compiler that enables\ncombining graph optimizations for GPUs. GG also introduces two performance\noptimizations, Edge-based Thread Warps CTAs load balancing (ETWC) and\nEdgeBlocking, to expand the optimization space for GPUs. ETWC improves load\nbalancing by dynamically partitioning the edges of each vertex into blocks that\nare assigned to threads, warps, and CTAs for execution. EdgeBlocking improves\nthe locality of the program by reordering the edges and restricting random\nmemory accesses to fit within the L2 cache. We evaluate GG on 5 algorithms and\n9 input graphs on both Pascal and Volta generation NVIDIA GPUs, and show that\nit achieves up to 5.11x speedup over state-of-the-art GPU graph processing\nframeworks, and is the fastest on 66 out of the 90 experiments.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 22:44:03 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 00:30:49 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Brahmakshatriya", "Ajay", ""], ["Zhang", "Yunming", ""], ["Hong", "Changwan", ""], ["Kamil", "Shoaib", ""], ["Shun", "Julian", ""], ["Amarasinghe", "Saman", ""]]}, {"id": "2012.08141", "submitter": "Mingkuan Xu", "authors": "Yuanming Hu, Mingkuan Xu, Ye Kuang, Fr\\'edo Durand", "title": "AsyncTaichi: On-the-fly Inter-kernel Optimizations for Imperative and\n  Spatially Sparse Programming", "comments": "18 pages, 20 figures, submitted to ACM SIGGRAPH Asia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging spatial sparsity has become a popular approach to accelerate 3D\ncomputer graphics applications. Spatially sparse data structures and efficient\nsparse kernels (such as parallel stencil operations on active voxels), are key\nto achieve high performance. Existing work focuses on improving performance\nwithin a single sparse computational kernel. We show that a system that looks\nbeyond a single kernel, plus additional domain-specific sparse data structure\nanalysis, opens up exciting new space for optimizing sparse computations.\nSpecifically, we propose a domain-specific data-flow graph model of imperative\nand sparse computation programs, which describes kernel relationships and\nenables easy analysis and optimization. Combined with an asynchronous execution\nengine that exposes a wide window of kernels, the inter-kernel optimizer can\nthen perform effective sparse computation optimizations, such as eliminating\nunnecessary voxel list generations and removing voxel activation checks. These\ndomain-specific optimizations further make way for classical general-purpose\noptimizations that are originally challenging to directly apply to computations\nwith sparse data structures. Without any computational code modification, our\nnew system leads to $4.02\\times$ fewer kernel launches and $1.87\\times$ speed\nup on our GPU benchmarks, including computations on Eulerian grids, Lagrangian\nparticles, meshes, and automatic differentiation.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 08:09:31 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 15:15:08 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Hu", "Yuanming", ""], ["Xu", "Mingkuan", ""], ["Kuang", "Ye", ""], ["Durand", "Fr\u00e9do", ""]]}, {"id": "2012.08320", "submitter": "Enzo Rucci", "authors": "Roberto Millon and Emmanuel Frati and Enzo Rucci", "title": "A Comparative Study between HLS and HDL on SoC for Image Processing\n  Applications", "comments": null, "journal-ref": null, "doi": "10.37537/rev.elektron.4.2.117.2020", "report-no": "Revista elektronVol 4, No 2 (2020) 100-106", "categories": "cs.AR cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The increasing complexity in today's systems and the limited market times\ndemand new development tools for FPGA. Currently, in addition to traditional\nhardware description languages (HDLs), there are high-level synthesis (HLS)\ntools that increase the abstraction level in system development. Despite the\ngreater simplicity of design and testing, HLS has some drawbacks in describing\nharware. This paper presents a comparative study between HLS and HDL for FPGA,\nusing a Sobel filter as a case study in the image processing field. The results\nshow that the HDL implementation is slightly better than the HLS version\nconsidering resource usage and response time. However, the programming effort\nrequired in the HDL solution is significantly larger than in the HLS\ncounterpart.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 14:28:52 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Millon", "Roberto", ""], ["Frati", "Emmanuel", ""], ["Rucci", "Enzo", ""]]}, {"id": "2012.08626", "submitter": "Roberto Casadei PhD", "authors": "Giorgio Audrito, Roberto Casadei, Ferruccio Damiani, Mirko Viroli", "title": "Computation Against a Neighbour", "comments": "50 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent works in contexts like the Internet of Things (IoT) and large-scale\nCyber-Physical Systems (CPS) propose the idea of programming distributed\nsystems by focussing on their global behaviour across space and time. In this\nview, a potentially vast and heterogeneous set of devices is considered as an\n\"aggregate\" to be programmed as a whole, while abstracting away the details of\nindividual behaviour and exchange of messages, which are expressed\ndeclaratively. One such a paradigm, known as aggregate programming, builds on\ncomputational models inspired by field-based coordination. Existing models such\nas the field calculus capture interaction with neighbours by a so-called\n\"neighbouring field\" (a map from neighbours to values). This requires ad-hoc\nmechanisms to smoothly compose with standard values, thus complicating\nprogramming and introducing clutter in aggregate programs, libraries and\ndomain-specific languages (DSLs). To address this key issue we introduce the\nnovel notion of \"computation against a neighbour\", whereby the evaluation of\ncertain subexpressions of the aggregate program are affected by recent\ncorresponding evaluations in neighbours. We capture this notion in the\nneighbours calculus (NC), a new field calculus variant which is shown to\nsmoothly support declarative specification of interaction with neighbours, and\ncorrespondingly facilitate the embedding of field computations as internal DSLs\nin common general-purpose programming languages -- as exemplified by a Scala\nimplementation, called ScaFi. This paper formalises NC, thoroughly compares it\nwith respect to the classic field calculus, and shows its expressiveness by\nmeans of a case study in edge computing, developed in ScaFi.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 21:33:55 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Audrito", "Giorgio", ""], ["Casadei", "Roberto", ""], ["Damiani", "Ferruccio", ""], ["Viroli", "Mirko", ""]]}, {"id": "2012.09138", "submitter": "Danil Annenkov", "authors": "Danil Annenkov, Mikkel Milo, Jakob Botsch Nielsen, Bas Spitters", "title": "Extracting Smart Contracts Tested and Verified in Coq", "comments": "Coq implementation:\n  https://github.com/AU-COBRA/ConCert/tree/artifact-2020 Update: fixed the\n  \"author running\" list, fixed a mistake in an evaluation rule for lambda-box\n  (Section 3.1, item 2)", "journal-ref": "CPP'2021: Proceedings of the 10th ACM SIGPLAN International\n  Conference on Certified Programs and Proofs, January 18--19, 2021, Virtual,\n  Denmark", "doi": "10.1145/3437992.3439934", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We implement extraction of Coq programs to functional languages based on\nMetaCoq's certified erasure. As part of this, we implement an optimisation pass\nremoving unused arguments. We prove the pass correct wrt. a conventional\ncall-by-value operational semantics of functional languages. We apply this to\ntwo functional smart contract languages, Liquidity and Midlang, and to the\nfunctional language Elm. Our development is done in the context of the ConCert\nframework that enables smart contract verification. We contribute a verified\nboardroom voting smart contract featuring maximum voter privacy such that each\nvote is kept private except under collusion of all other parties. We also\nintegrate property-based testing into ConCert using QuickChick and our\ndevelopment is the first to support testing properties of interacting smart\ncontracts. We test several complex contracts such as a DAO-like contract, an\nescrow contract, an implementation of a Decentralized Finance (DeFi) contract\nwhich includes a custom token standard (Tezos FA2), and more. In total, this\ngives us a way to write dependent programs in Coq, test them\nsemi-automatically, verify, and then extract to functional smart contract\nlanguages, while retaining a small trusted computing base of only MetaCoq and\nthe pretty-printers into these languages.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 18:23:58 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 14:07:52 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Annenkov", "Danil", ""], ["Milo", "Mikkel", ""], ["Nielsen", "Jakob Botsch", ""], ["Spitters", "Bas", ""]]}, {"id": "2012.09155", "submitter": "Maverick Woo", "authors": "Kaiyuan Li and Maverick Woo and Limin Jia", "title": "On the Generation of Disassembly Ground Truth and the Evaluation of\n  Disassemblers", "comments": "Revised and extended version of our publication that first appeared\n  in the 2020 Workshop on Forming an Ecosystem Around Software Transformation\n  (FEAST '20), November 13, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a software transformation or software security task needs to analyze a\ngiven program binary, the first step is often disassembly. Since many modern\ndisassemblers have become highly accurate on many binaries, we believe reliable\ndisassembler benchmarking requires standardizing the set of binaries used and\nthe disassembly ground truth about these binaries. This paper presents (i) a\nfirst version of our work-in-progress disassembly benchmark suite, which\ncomprises 879 binaries from diverse projects compiled with multiple compilers\nand optimization settings, and (ii) a novel disassembly ground truth generator\nleveraging the notion of \"listing files\", which has broad support by Clang,\nGCC, ICC, and MSVC. In additional, it presents our evaluation of four prominent\nopen-source disassemblers using this benchmark suite and a custom evaluation\nsystem. Our entire system and all generated data are maintained openly on\nGitHub to encourage community adoption.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 20:03:40 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Li", "Kaiyuan", ""], ["Woo", "Maverick", ""], ["Jia", "Limin", ""]]}, {"id": "2012.09590", "submitter": "Daniel Graziotin", "authors": "Marvin Wyrich, Andreas Preikschat, Daniel Graziotin, Stefan Wagner", "title": "The Mind Is a Powerful Place: How Showing Code Comprehensibility Metrics\n  Influences Code Understanding", "comments": "To appear in: Proceedings of the 43rd International Conference on\n  Software Engineering (ICSE '21), Madrid, Spain, 12 pages. 12 pages, 1 figure.\n  Postprint, after peer review", "journal-ref": "2021 IEEE/ACM 43rd International Conference on Software\n  Engineering (ICSE), 2021 pp. 512-523", "doi": "10.1109/ICSE43902.2021.00055", "report-no": null, "categories": "cs.SE cs.CY cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static code analysis tools and integrated development environments present\ndevelopers with quality-related software metrics, some of which describe the\nunderstandability of source code. Software metrics influence overarching\nstrategic decisions that impact the future of companies and the prioritization\nof everyday software development tasks. Several software metrics, however, lack\nin validation: we just choose to trust that they reflect what they are supposed\nto measure. Some of them were even shown to not measure the quality aspects\nthey intend to measure. Yet, they influence us through biases in our\ncognitive-driven actions. In particular, they might anchor us in our decisions.\nWhether the anchoring effect exists with software metrics has not been studied\nyet. We conducted a randomized and double-blind experiment to investigate the\nextent to which a displayed metric value for source code comprehensibility\nanchors developers in their subjective rating of source code comprehensibility,\nwhether performance is affected by the anchoring effect when working on\ncomprehension tasks, and which individual characteristics might play a role in\nthe anchoring effect. We found that the displayed value of a comprehensibility\nmetric has a significant and large anchoring effect on a developer's code\ncomprehensibility rating. The effect does not seem to affect the time or\ncorrectness when working on comprehension questions related to the code\nsnippets under study. Since the anchoring effect is one of the most robust\ncognitive biases, and we have limited understanding of the consequences of the\ndemonstrated manipulation of developers by non-validated metrics, we call for\nan increased awareness of the responsibility in code quality reporting and for\ncorresponding tools to be based on scientific evidence.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 14:27:45 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 12:52:32 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Wyrich", "Marvin", ""], ["Preikschat", "Andreas", ""], ["Graziotin", "Daniel", ""], ["Wagner", "Stefan", ""]]}, {"id": "2012.10086", "submitter": "Flemming Nielson", "authors": "Flemming Nielson, Hanne Riis Nielson", "title": "Program Analysis (an Appetizer)", "comments": "208 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This book is an introduction to program analysis that is meant to be\nconsiderably more elementary than our advanced book Principles of Program\nAnalysis (Springer, 2005). Rather than using flow charts as the model of\nprograms, the book follows our introductory book Formal Methods an Appetizer\n(Springer, 2019) using program graphs as the model of programs. In our\nexperience this makes the underlying ideas more accessible to our computer\nscience and computer engineering students on the master course 02242: Program\nAnalysis at The Technical University of Denmark. Here we have gradually\nreplaced our use of the more elementary parts of Principles of Program Analysis\nwith material from the current book.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 07:27:23 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Nielson", "Flemming", ""], ["Nielson", "Hanne Riis", ""]]}, {"id": "2012.10211", "submitter": "Michael Robinson", "authors": "Michael Robinson", "title": "Looking for non-compliant documents using error messages from multiple\n  parsers", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whether a file is accepted by a single parser is not a reliable indication of\nwhether a file complies with its stated format. Bugs within both the parser and\nthe format specification mean that a compliant file may fail to parse, or that\na non-compliant file might be read without any apparent trouble. The latter\nsituation presents a significant security risk, and should be avoided. This\narticle suggests that a better way to assess format specification compliance is\nto examine the set of error messages produced by a set of parsers rather than a\nsingle parser. If both a sample of compliant files and a sample of\nnon-compliant files are available, then we show how a statistical test based on\na pseudo-likelihood ratio can be very effective at determining a file's\ncompliance. Our method is format agnostic, and does not directly rely upon a\nformal specification of the format. Although this article focuses upon the case\nof the PDF format (ISO 32000-2), we make no attempt to use any specific details\nof the format. Furthermore, we show how principal components analysis can be\nuseful for a format specification designer to assess the quality and structure\nof these samples of files and parsers. While these tests are absolutely\nrudimentary, it appears that their use to measure file format variability and\nto identify non-compliant files is both novel and surprisingly effective.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 19:54:58 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Robinson", "Michael", ""]]}, {"id": "2012.10511", "submitter": "Adam Petz", "authors": "Adam Petz and Perry Alexander", "title": "An Infrastructure for Faithful Execution of Remote Attestation Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote attestation is an emerging technology for establishing trust in a\nremote computing system. Copland is a domain-specific language for specifying\nlayered attestation protocols, characterizing attestation-relevant system\nevents, and describing evidence bundling. In this work we formally define and\nverify a Copland Compiler and Copland Virtual Machine for executing Copland\nprotocols. The compiler translates Copland into instructions that are executed\non the virtual machine. The compiler and virtual machine are implemented as\nmonadic, functional programs in the Coq proof assistant and verified with\nrespect to the Copland event and evidence semantics. In addition we introduce\nthe Attestation Manager Monad as an environment for managing Copland term\nexecution providing support for managing nonces, binding results of Copland\nprotocols to variables, and appraising evidence results.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 20:57:34 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Petz", "Adam", ""], ["Alexander", "Perry", ""]]}, {"id": "2012.10662", "submitter": "Md Rafiqul Islam Rabin", "authors": "Md Rafiqul Islam Rabin and Mohammad Amin Alipour", "title": "Configuring Test Generators using Bug Reports: A Case Study of GCC\n  Compiler and Csmith", "comments": "The 36th ACM/SIGAPP Symposium on Applied Computing, Software\n  Verification and Testing Track (SAC-SVT'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The correctness of compilers is instrumental in the safety and reliability of\nother software systems, as bugs in compilers can produce executables that do\nnot reflect the intent of programmers. Such errors are difficult to identify\nand debug. Random test program generators are commonly used in testing\ncompilers, and they have been effective in uncovering bugs. However, the\nproblem of guiding these test generators to produce test programs that are more\nlikely to find bugs remains challenging. In this paper, we use the code\nsnippets in the bug reports to guide the test generation. The main idea of this\nwork is to extract insights from the bug reports about the language features\nthat are more prone to inadequate implementation and using the insights to\nguide the test generators. We use the GCC C compiler to evaluate the\neffectiveness of this approach. In particular, we first cluster the test\nprograms in the GCC bugs reports based on their features. We then use the\ncentroids of the clusters to compute configurations for Csmith, a popular test\ngenerator for C compilers. We evaluated this approach on eight versions of GCC\nand found that our approach provides higher coverage and triggers more\nmiscompilation failures than the state-of-the-art test generation techniques\nfor GCC.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 11:25:13 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 12:36:38 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Rabin", "Md Rafiqul Islam", ""], ["Alipour", "Mohammad Amin", ""]]}, {"id": "2012.11394", "submitter": "Graham Campbell", "authors": "Graham Campbell and Brian Courtehoute and Detlef Plump", "title": "Fast Rule-Based Graph Programs", "comments": "47 pages, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Implementing graph algorithms efficiently in a rule-based language is\nchallenging because graph pattern matching is expensive. In this paper, we\npresent a number of linear-time implementations of graph algorithms in GP 2, an\nexperimental programming language based on graph transformation rules which\naims to facilitate program analysis and verification. We focus on two classes\nof rule-based graph programs: graph reduction programs which check some graph\nproperty, and programs using a depth-first search to test some property or\nperform an operation such as producing a 2-colouring or a topological sorting.\nPrograms of the first type run in linear time without any constraints on input\ngraphs while programs of the second type require input graphs of bounded degree\nto run in linear time. Essential for achieving the linear time complexity are\nso-called rooted rules in GP 2, which, in many situations, can be matched in\nconstant time. For each of our programs, we prove both correctness and\ncomplexity, and also give empirical evidence for their run time.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 14:50:31 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 01:33:19 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Campbell", "Graham", ""], ["Courtehoute", "Brian", ""], ["Plump", "Detlef", ""]]}, {"id": "2012.11401", "submitter": "Daniel Selsam", "authors": "Daniel Selsam, Jesse Michael Han, Leonardo de Moura, Patrice Godefroid", "title": "Universal Policies for Software-Defined MDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new programming paradigm called oracle-guided decision\nprogramming in which a program specifies a Markov Decision Process (MDP) and\nthe language provides a universal policy. We prototype a new programming\nlanguage, Dodona, that manifests this paradigm using a primitive 'choose'\nrepresenting nondeterministic choice. The Dodona interpreter returns either a\nvalue or a choicepoint that includes a lossless encoding of all information\nnecessary in principle to make an optimal decision. Meta-interpreters query\nDodona's (neural) oracle on these choicepoints to get policy and value\nestimates, which they can use to perform heuristic search on the underlying\nMDP. We demonstrate Dodona's potential for zero-shot heuristic guidance by\nmeta-learning over hundreds of synthetic tasks that simulate basic operations\nover lists, trees, Church datastructures, polynomials, first-order terms and\nhigher-order terms.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 15:04:06 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Selsam", "Daniel", ""], ["Han", "Jesse Michael", ""], ["de Moura", "Leonardo", ""], ["Godefroid", "Patrice", ""]]}, {"id": "2012.12143", "submitter": "Paolo Luchini", "authors": "Paolo Luchini", "title": "Introducing CPL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL physics.comp-ph physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CPL here stands for a computer programming language conceived and developed\nby the author since 1993, but published for the first time now. It was born as\na Compiled Programming Language, designed together with its compiler and\ntherefore suitable for computationally intensive numerical applications,\nalthough some years later an interpreter was also provided for interactive\nusage. CPL's distinctive features are Concealed Pointer Lookup, the ability to\nimplicitly dereference pointers based on the type of operands involved,\nConsistent Procedure Linkage, the enforcement of function prototypes without\ndedicated header or interface files, and Custom Parameter Lists, the ability to\noverload function names which are distinguished by the type of their parameters\nand/or parameter separators. Perhaps even more distinctly, CPL's syntax can be\nextended on the fly by the program being compiled; library modules tap this\nfeature to seamlessly add real and complex matrix operations, graphics,\nparallel-computing extensions, and symbolic differentiation. The CPL coding\nsoftware is available for free download at http://CPLcode.net .\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 16:28:10 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Luchini", "Paolo", ""]]}, {"id": "2012.12700", "submitter": "Jingzhe Guo", "authors": "Jingzhe Guo, Mingsheng Ying", "title": "Software Pipelining for Quantum Loop Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for performing software pipelining on quantum for-loop\nprograms, exploiting parallelism in and across iterations. We redefine concepts\nthat are useful in program optimization, including array aliasing, instruction\ndependency and resource conflict, this time in optimization of quantum\nprograms. Using the redefined concepts, we present a software pipelining\nalgorithm exploiting instruction-level parallelism in quantum loop programs.\nThe optimization method is then evaluated on some test cases, including popular\napplications like QAOA, and compared with several baseline results. The\nevaluation results show that our approach outperforms loop optimizers\nexploiting only in-loop optimization chances by reducing total depth of the\nloop program to close to the optimal program depth obtained by full loop\nunrolling, while generating much smaller code in size. This is the first step\ntowards optimization of a quantum program with such loop control flow as far as\nwe know.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 14:27:05 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Guo", "Jingzhe", ""], ["Ying", "Mingsheng", ""]]}, {"id": "2012.12964", "submitter": "Maxwell Nye", "authors": "Maxwell Nye, Yewen Pu, Matthew Bowers, Jacob Andreas, Joshua B.\n  Tenenbaum, Armando Solar-Lezama", "title": "Representing Partial Programs with Blended Abstract Semantics", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing programs from examples requires searching over a vast,\ncombinatorial space of possible programs. In this search process, a key\nchallenge is representing the behavior of a partially written program before it\ncan be executed, to judge if it is on the right track and predict where to\nsearch next. We introduce a general technique for representing partially\nwritten programs in a program synthesis engine. We take inspiration from the\ntechnique of abstract interpretation, in which an approximate execution model\nis used to determine if an unfinished program will eventually satisfy a goal\nspecification. Here we learn an approximate execution model implemented as a\nmodular neural network. By constructing compositional program representations\nthat implicitly encode the interpretation semantics of the underlying\nprogramming language, we can represent partial programs using a flexible\ncombination of concrete execution state and learned neural representations,\nusing the learned approximate semantics when concrete semantics are not known\n(in unfinished parts of the program). We show that these hybrid neuro-symbolic\nrepresentations enable execution-guided synthesizers to use more powerful\nlanguage constructs, such as loops and higher-order functions, and can be used\nto synthesize programs more accurately for a given search budget than pure\nneural approaches in several domains.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 20:40:18 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 18:44:59 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Nye", "Maxwell", ""], ["Pu", "Yewen", ""], ["Bowers", "Matthew", ""], ["Andreas", "Jacob", ""], ["Tenenbaum", "Joshua B.", ""], ["Solar-Lezama", "Armando", ""]]}, {"id": "2012.13129", "submitter": "Ankush Das", "authors": "Ankush Das and Frank Pfenning", "title": "Rast: A Language for Resource-Aware Session Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional session types prescribe bidirectional communication protocols for\nconcurrent computations, where well-typed programs are guaranteed to adhere to\nthe protocols. However, simple session types cannot capture properties beyond\nthe basic type of the exchanged messages. In response, recent work has extended\nsession types with refinements from linear arithmetic, capturing intrinsic\nattributes of processes and data. These refinements then play a central role in\ndescribing sequential and parallel complexity bounds on session-typed programs.\nThe Rast language provides an open-source implementation of session-typed\nconcurrent programs extended with arithmetic refinements as well as ergometric\nand temporal types to capture work and span of program execution. To further\nsupport generic programming, Rast also enhances arithmetically refined session\ntypes with recently developed nested parametric polymorphism. Type checking\nrelies on Cooper's algorithm for quantifier elimination in Presburger\narithmetic with a few significant optimizations, and a heuristic extension to\nnonlinear constraints. Rast furthermore includes a reconstruction engine so\nthat most program constructs pertaining the layers of refinements and resources\nare inserted automatically. We provide a variety of examples to demonstrate the\nexpressivity of the language.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 06:30:00 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Das", "Ankush", ""], ["Pfenning", "Frank", ""]]}, {"id": "2012.13333", "submitter": "Martin Lester", "authors": "M. M. Lester, R. P. Neatherway, C.-H. L. Ong, S. J. Ramsay", "title": "Verifying Liveness Properties of ML Programs", "comments": "Peer-reviewed extended abstract presented at 2011 ACM SIGPLAN\n  Workshop on ML. Full technical report archived at:\n  https://ora.ox.ac.uk/objects/uuid:bf35bab8-b395-4f85-93bd-57ca0328a1d3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher-order recursion schemes are a higher-order analogue of Boolean\nPrograms; they form a natural class of abstractions for functional programs. We\npresent a new, efficient algorithm for checking CTL properties of the trees\ngenerated by higher-order recursion schemes, which is an extension of\nKobayashi's intersection type-based model checking technique. We show that an\nimplementation of this algorithm, THORS, performs well on a number of small\nexamples and we demonstrate how it can be used to verify liveness properties of\nOCaml programs. Example properties include statements such as \"all opened\nsockets are eventually closed\" and \"the lock is held until the file is closed\".\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 17:09:40 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Lester", "M. M.", ""], ["Neatherway", "R. P.", ""], ["Ong", "C. -H. L.", ""], ["Ramsay", "S. J.", ""]]}, {"id": "2012.14133", "submitter": "Brijesh Dongol", "authors": "Sadegh Dalvandi and Brijesh Dongol", "title": "Verifying C11-Style Weak Memory Libraries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deductive verification of concurrent programs under weak memory has thus far\nbeen limited to simple programs over a monolithic state space. For\nscalabiility, we also require modular techniques with verifiable library\nabstractions. This paper addresses this challenge in the context of RC11 RAR, a\nsubset of the C11 memory model that admits relaxed and release-acquire\naccesses, but disallows, so called, load-buffering cycles. We develop a simple\nframework for specifying abstract objects that precisely characterises the\nobservability guarantees of abstract method calls. We show how this framework\ncan be integrated with an operational semantics that enables verification of\nclient programs that execute abstract method calls from a library it uses.\nFinally, we show how implementations of such abstractions in RC11 RAR can be\nverified by developing a (contextual) refinement framework for abstract\nobjects. Our framework, including the operational semantics, verification\ntechnique for client-library programs, and simulation between abstract\nlibraries and their implementations, has been mechanised in Isabelle/HOL.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 08:02:59 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Dalvandi", "Sadegh", ""], ["Dongol", "Brijesh", ""]]}, {"id": "2012.14205", "submitter": "Marco Guarnieri", "authors": "Marco Guarnieri, Marco Patrignani", "title": "Contract-Aware Secure Compilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microarchitectural attacks exploit the abstraction gap between the\nInstruction Set Architecture (ISA) and how instructions are actually executed\nby processors to compromise the confidentiality and integrity of a system. To\nsecure systems against microarchitectural attacks, programmers need to reason\nabout and program against these microarchitectural side-effects. However, we\ncannot -- and should not -- expect programmers to manually tailor programs for\nspecific processors and their security guarantees. Instead, we could rely on\ncompilers (and the secure compilation community), as they can play a prominent\nrole in bridging this gap: compilers should target specific processors\nmicroarchitectural security guarantees and they should leverage these\nguarantees to produce secure code. To achieve this, we outline the idea of\nContract-Aware Secure COmpilation (CASCO) where compilers are parametric with\nrespect to a hardware/software security-contract, an abstraction capturing a\nprocessor's security guarantees. That is, compilers will automatically leverage\nthe guarantees formalized in the contract to ensure that program-level security\nproperties are preserved at microarchitectural level.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 11:57:51 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Guarnieri", "Marco", ""], ["Patrignani", "Marco", ""]]}, {"id": "2012.14542", "submitter": "Ajay Singh", "authors": "Ajay Singh, Trevor Brown, Ali Mashtizadeh", "title": "NBR: Neutralization Based Reclamation", "comments": "Accepted in PPoPP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Safe memory reclamation (SMR) algorithms suffer from a trade-off between\nbounding unreclaimed memory and the speed of reclamation. Hazard pointer (HP)\nbased algorithms bound unreclaimed memory at all times, but tend to be slower\nthan other approaches. Epoch based reclamation (EBR) algorithms are faster, but\ndo not bound memory reclamation. Other algorithms follow hybrid approaches,\nrequiring special compiler or hardware support, changes to record layouts,\nand/or extensive code changes. Not all SMR algorithms can be used to reclaim\nmemory for all data structures.\n  We propose a new neutralization based reclamation (NBR) algorithm that is\nfaster than the best known EBR algorithms and achieves bounded unreclaimed\nmemory. It is non-blocking when used with a non-blocking operating system (OS)\nkernel, and only requires atomic read, write and CAS. NBR is straightforward to\nuse with many different data structures, and in most cases, require similar\nreasoning and programmer effort to two-phased locking. NBR is implemented using\nOS signals and a lightweight handshaking mechanism between participating\nthreads to determine when it is safe to reclaim a record. Experiments on a\nlock-based binary search tree and a lazy linked list show that NBR\nsignificantly outperforms many state of the art reclamation algorithms. In the\ntree NBR is faster than next best algorithm, DEBRA by upto 38% and HP by upto\n17%. And, in the list NBR is 15% and 243% faster than DEBRA and HP,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 00:32:48 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 21:20:03 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Singh", "Ajay", ""], ["Brown", "Trevor", ""], ["Mashtizadeh", "Ali", ""]]}, {"id": "2012.15240", "submitter": "Martin Lester", "authors": "Martin Mariusz Lester", "title": "Analysis of MiniJava Programs via Translation to ML", "comments": "Short paper at Formal Techniques for Java-like Programs (FTfJP) 2019", "journal-ref": "FTfJP 2019: Proceedings of the 21st Workshop on Formal Techniques\n  for Java-like Programs", "doi": "10.1145/3340672.3341119", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MiniJava is a subset of the object-oriented programming language Java.\nStandard ML is the canonical representative of the ML family of functional\nprogramming languages, which includes F# and OCaml. Different program analysis\nand verification tools and techniques have been developed for both Java-like\nand ML-like languages. Naturally, the tools developed for a particular language\nemphasise accurate treatment of language features commonly used in that\nlanguage. In Java, this means objects with mutable properties and dynamic\nmethod dispatch. In ML, this means higher order functions and algebraic\ndatatypes with pattern matching.\n  We propose to translate programs from one language into the other and use the\ntarget language's tools for analysis and verification. By doing so, we hope to\nidentify areas for improvement in the target language's tools and suggest\ntechniques, perhaps as used in the source language's tools, that may guide\ntheir improvement. More generally, we hope to develop tools for reasoning about\nprograms that are more resilient to changes in the style of code and\nrepresentation of data. We begin our programme by outlining a translation from\nMiniJava to ML that uses only the core features of ML; in particular, it avoids\nthe use of ML's mutable references.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 17:35:30 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Lester", "Martin Mariusz", ""]]}, {"id": "2012.15285", "submitter": "Martin Lester", "authors": "Martin Lester", "title": "What can a 1980s BASIC programming textbook teach us today?", "comments": "Peer-reviewed extended abstract presented at Fourth Symposium on the\n  History and Philosophy of Programming (HaPoP), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elementary Basic, published in 1982, is an introductory programming text with\na novel central conceit, namely that the fictional 19th century detective\nSherlock Holmes used a computer to help solve mysteries. It is also novel among\nsimilar books of its time for its focus on program design. In other regards,\nsuch as its use of the language BASIC, it is representative of its time.\n  Over 35 years after it was written, I think it is worth looking back at it to\nsee to what is still relevant today and what would be done differently. We may\neven learn something about teaching programming today. Of particular interest\nis the degree to which the use of BASIC influenced the content of the book.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 18:58:54 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Lester", "Martin", ""]]}, {"id": "2012.15365", "submitter": "Martin Lester", "authors": "Martin Mariusz Lester", "title": "Solving Interactive Fiction Games via Partial Evaluation and Bounded\n  Model Checking", "comments": "Work-in-progress was presented at 7th International Workshop on\n  Rewriting Techniques for Program Transformations and Evaluation (WPTE 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a case study on using program verification tools, specifically\nmodel-checkers for C programs, to solve simple interactive fiction games from\naround 1980. Off-the-shelf model-checking tools are unable to handle the games\nin their original form. In order to work around this, we apply a series of\nprogram transformations that do not change the behaviour of the program. An\ninteresting aspect of these games is that they use a simple, interpreted\nlanguage to script in-game events. This turns out to be the most difficult part\nof the program for verification tools to handle; we tackle this using partial\nevaluation. Our case study thus provides some insights that are applicable more\ngenerally to verification and analysis of programs that interpret scripting\nlanguages.\n  To the best of our knowledge, this is the first example of a commercially\nreleased game being solved by application of a program model-checker to the\ngame's code.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 23:31:41 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Lester", "Martin Mariusz", ""]]}, {"id": "2012.15422", "submitter": "Konstantinos Kallas", "authors": "Shivam Handa (MIT), Konstantinos Kallas (University of Pennsylvania),\n  Nikos Vasilakis (MIT), Martin Rinard (MIT)", "title": "An Order-Aware Dataflow Model for Parallel Unix Pipelines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dataflow model for modelling parallel Unix shell pipelines. To\naccurately capture the semantics of complex Unix pipelines, the dataflow model\nis order-aware, i.e., the order in which a node in the dataflow graph consumes\ninputs from different edges plays a central role in the semantics of the\ncomputation and therefore in the resulting parallelization. We use this model\nto capture the semantics of transformations that exploit data parallelism\navailable in Unix shell computations and prove their correctness. We\nadditionally formalize the translations from the Unix shell to the dataflow\nmodel and from the dataflow model back to a parallel shell script. We implement\nour model and transformations as the compiler and optimization passes of a\nsystem parallelizing shell pipelines, and use it to evaluate the speedup\nachieved on 47 pipelines.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 03:25:29 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 19:23:18 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Handa", "Shivam", "", "MIT"], ["Kallas", "Konstantinos", "", "University of Pennsylvania"], ["Vasilakis", "Nikos", "", "MIT"], ["Rinard", "Martin", "", "MIT"]]}, {"id": "2012.15443", "submitter": "Nikos Vasilakis", "authors": "Nikos Vasilakis (CSAIL, MIT), Jiasi Shen (CSAIL, MIT), Martin Rinard\n  (CSAIL, MIT)", "title": "Automatic Synthesis of Parallel and Distributed Unix Commands with\n  KumQuat", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present KumQuat, a system for automatically synthesizing parallel and\ndistributed versions of Unix shell commands. KumQuat follows a\ndivide-and-conquer approach, decomposing commands into (i) a parallel mapper\napplying the original command to produce partial results, and (ii) an ordered\ncombiner that combines the partial results into the final output. KumQuat\nsynthesizes the combiner by applying repeated rounds of exploration; at each\nround, it compares the results of the synthesized program with those from the\nsequential program to discard invalid candidates. A series of refinements\nimprove the performance of both the synthesis component and the resulting\nsynthesized programs. For 98.2% of Unix commands from real pipelines, KumQuat\neither synthesizes a combiner (92.2%) or reports that a combiner is not\nsynthesizable (7.8%), offering an average speedup of 7.8$\\times$ for the\nparallel version and 3.8$\\times$ for the distributed version.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 04:31:26 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Vasilakis", "Nikos", "", "CSAIL, MIT"], ["Shen", "Jiasi", "", "CSAIL, MIT"], ["Rinard", "Martin", "", "CSAIL, MIT"]]}, {"id": "2012.15489", "submitter": "Yeting Li", "authors": "Yeting Li and Shuaimin Li and Zhiwu Xu and Jialun Cao and Zixuan Chen\n  and Yun Hu and Haiming Chen and Shing-Chi Cheung", "title": "TransRegex: Multi-modal Regular Expression Synthesis by\n  Generate-and-Repair", "comments": "accepted by ICSE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since regular expressions (abbrev. regexes) are difficult to understand and\ncompose, automatically generating regexes has been an important research\nproblem. This paper introduces TransRegex, for automatically constructing\nregexes from both natural language descriptions and examples. To the best of\nour knowledge, TransRegex is the first to treat the NLP-and-example-based regex\nsynthesis problem as the problem of NLP-based synthesis with regex repair. For\nthis purpose, we present novel algorithms for both NLP-based synthesis and\nregex repair. We evaluate TransRegex with ten relevant state-of-the-art tools\non three publicly available datasets. The evaluation results demonstrate that\nthe accuracy of our TransRegex is 17.4%, 35.8% and 38.9% higher than that of\nNLP-based approaches on the three datasets, respectively. Furthermore,\nTransRegex can achieve higher accuracy than the state-of-the-art multi-modal\ntechniques with 10% to 30% higher accuracy on all three datasets. The\nevaluation results also indicate TransRegex utilizing natural language and\nexamples in a more effective way.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 07:36:42 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 06:27:31 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Li", "Yeting", ""], ["Li", "Shuaimin", ""], ["Xu", "Zhiwu", ""], ["Cao", "Jialun", ""], ["Chen", "Zixuan", ""], ["Hu", "Yun", ""], ["Chen", "Haiming", ""], ["Cheung", "Shing-Chi", ""]]}]