[{"id": "1711.00113", "submitter": "Dariusz Biernacki", "authors": "Dariusz Biernacki, Serguei Lenglet, Piotr Polesiuk", "title": "Proving Soundness of Extensional Normal-Form Bisimilarities", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 1 (March 29,\n  2019) lmcs:5323", "doi": "10.23638/LMCS-15(1:31)2019", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Normal-form bisimilarity is a simple, easy-to-use behavioral equivalence that\nrelates terms in $\\lambda$-calculi by decomposing their normal forms into\nbisimilar subterms. Moreover, it typically allows for powerful up-to\ntechniques, such as bisimulation up to context, which simplify bisimulation\nproofs even further. However, proving soundness of these relations becomes\ncomplicated in the presence of $\\eta$-expansion and usually relies on ad hoc\nproof methods which depend on the language. In this paper we propose a more\nsystematic proof method to show that an extensional normal-form bisimilarity\nalong with its corresponding up to context technique are sound. We illustrate\nour technique with three calculi: the call-by-value $\\lambda$-calculus, the\ncall-by-value $\\lambda$-calculus with the delimited-control operators shift and\nreset, and the call-by-value $\\lambda$-calculus with the abortive control\noperators call/cc and abort. In the first two cases, there was previously no\nsound up to context technique validating the $\\eta$-law, whereas no theory of\nnormal-form bisimulations for a calculus with call/cc and abort has been\npresented before. Our results have been fully formalized in the Coq proof\nassistant.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 21:09:16 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 13:33:30 GMT"}, {"version": "v3", "created": "Wed, 27 Mar 2019 09:45:19 GMT"}, {"version": "v4", "created": "Thu, 28 Mar 2019 09:31:39 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Biernacki", "Dariusz", ""], ["Lenglet", "Serguei", ""], ["Polesiuk", "Piotr", ""]]}, {"id": "1711.00740", "submitter": "Miltiadis Allamanis", "authors": "Miltiadis Allamanis, Marc Brockschmidt, Mahmoud Khademi", "title": "Learning to Represent Programs with Graphs", "comments": "Published in ICLR 2018. arXiv admin note: text overlap with\n  arXiv:1705.07867", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning tasks on source code (i.e., formal languages) have been considered\nrecently, but most work has tried to transfer natural language methods and does\nnot capitalize on the unique opportunities offered by code's known syntax. For\nexample, long-range dependencies induced by using the same variable or function\nin distant locations are often not considered. We propose to use graphs to\nrepresent both the syntactic and semantic structure of code and use graph-based\ndeep learning methods to learn to reason over program structures.\n  In this work, we present how to construct graphs from source code and how to\nscale Gated Graph Neural Networks training to such large graphs. We evaluate\nour method on two tasks: VarNaming, in which a network attempts to predict the\nname of a variable given its usage, and VarMisuse, in which the network learns\nto reason about selecting the correct variable that should be used at a given\nprogram location. Our comparison to methods that use less structured program\nrepresentations shows the advantages of modeling known structure, and suggests\nthat our models learn to infer meaningful names and to solve the VarMisuse task\nin many cases. Additionally, our testing showed that VarMisuse identifies a\nnumber of bugs in mature open-source projects.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 09:48:06 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 11:59:03 GMT"}, {"version": "v3", "created": "Fri, 4 May 2018 20:30:53 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Allamanis", "Miltiadis", ""], ["Brockschmidt", "Marc", ""], ["Khademi", "Mahmoud", ""]]}, {"id": "1711.00878", "submitter": "Bernardo Toninho", "authors": "Bernardo Toninho and Nobuko Yoshida", "title": "On Polymorphic Sessions and Functions: A Tale of Two (Fully Abstract)\n  Encodings", "comments": "Extended version of ESOP'18 paper (includes appendix with proofs and\n  additional definitions)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work exploits the logical foundation of session types to determine what\nkind of type discipline for the pi-calculus can exactly capture, and is\ncaptured by, lambda-calculus behaviours. Leveraging the proof theoretic content\nof the soundness and completeness of sequent calculus and natural deduction\npresentations of linear logic, we develop the first mutually inverse and fully\nabstract processes-as-functions and functions-as-processes encodings between a\npolymorphic session pi-calculus and a linear formulation of System F. We are\nthen able to derive results of the session calculus from the theory of the\nlambda-calculus: (1) we obtain a characterisation of inductive and coinductive\nsession types via their algebraic representations in System F; and (2) we\nextend our results to account for value and process passing, entailing strong\nnormalisation.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 18:38:08 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 16:58:55 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Toninho", "Bernardo", ""], ["Yoshida", "Nobuko", ""]]}, {"id": "1711.00931", "submitter": "Ryan Kavanagh", "authors": "Ryan Kavanagh, Stephen Brookes", "title": "A Denotational Semantics for SPARC TSO", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 2 (May 8,\n  2019) lmcs:5443", "doi": "10.23638/LMCS-15(2:10)2019", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The SPARC TSO weak memory model is defined axiomatically, with a\nnon-compositional formulation that makes modular reasoning about programs\ndifficult. Our denotational approach uses pomsets to provide a compositional\nsemantics capturing exactly the behaviours permitted by SPARC TSO. It uses\nbuffered states and an inductive definition of execution to assign an\ninput-output meaning to pomsets. We show that our denotational account is sound\nand complete relative to the axiomatic account, that is, that it captures\nexactly the behaviours permitted by the axiomatic account. Our compositional\napproach facilitates the study of SPARC TSO and supports modular analysis of\nprogram behaviour.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 20:51:10 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 21:14:53 GMT"}, {"version": "v3", "created": "Thu, 18 Apr 2019 02:14:44 GMT"}, {"version": "v4", "created": "Tue, 7 May 2019 08:25:07 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Kavanagh", "Ryan", ""], ["Brookes", "Stephen", ""]]}, {"id": "1711.01024", "submitter": "Bernhard Scholz", "authors": "Wasuwee Sodsong, Bernhard Scholz, Sanjay Chawla", "title": "SPARK: Static Program Analysis Reasoning and Retrieving Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program analysis is a technique to reason about programs without executing\nthem, and it has various applications in compilers, integrated development\nenvironments, and security. In this work, we present a machine learning\npipeline that induces a security analyzer for programs by example. The security\nanalyzer determines whether a program is either secure or insecure based on\nsymbolic rules that were deduced by our machine learning pipeline. The machine\npipeline is two-staged consisting of a Recurrent Neural Networks (RNN) and an\nExtractor that converts an RNN to symbolic rules.\n  To evaluate the quality of the learned symbolic rules, we propose a\nsampling-based similarity measurement between two infinite regular languages.\nWe conduct a case study using real-world data. In this work, we discuss the\nlimitations of existing techniques and possible improvements in the future. The\nresults show that with sufficient training data and a fair distribution of\nprogram paths it is feasible to deducing symbolic security rules for the\nOpenJDK library with millions lines of code.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 04:28:58 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Sodsong", "Wasuwee", ""], ["Scholz", "Bernhard", ""], ["Chawla", "Sanjay", ""]]}, {"id": "1711.02246", "submitter": "Torben Amtoft", "authors": "Torben Amtoft, Anindya Banerjee", "title": "A Theory of Slicing for Probabilistic Control-Flow Graphs", "comments": "Expanded and revised version of a paper originally appearing in\n  Foundations of Software Science and Computation Structures - 19th\n  International Conference, FOSSACS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a theory for slicing probabilistic imperative programs --\ncontaining random assignments, and ``observe'' statements (for conditioning) --\nrepresented as probabilistic control-flow graphs (pCFGs) whose nodes modify\nprobability distributions.\n  We show that such a representation allows direct adaptation of standard\nmachinery such as data and control dependence, postdominators, relevant\nvariables, etc to the probabilistic setting. We separate the specification of\nslicing from its implementation: first we develop syntactic conditions that a\nslice must satisfy; next we prove that any such slice is semantically correct;\nfinally we give an algorithm to compute the least slice. To generate smaller\nslices, we may in addition take advantage of knowledge that certain loops will\nterminate (almost) always.\n  A key feature of our syntactic conditions is that they involve two disjoint\nslices such that the variables of one slice are probabilistically independent\nof the variables of the other. This leads directly to a proof of correctness of\nprobabilistic slicing. In a companion article we show adequacy of the semantics\nof pCFGs with respect to the standard semantics of structured probabilistic\nprograms.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 01:21:33 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Amtoft", "Torben", ""], ["Banerjee", "Anindya", ""]]}, {"id": "1711.02256", "submitter": "Torben Amtoft", "authors": "Torben Amtoft, Anindya Banerjee", "title": "A Semantics for Probabilistic Control-Flow Graphs", "comments": "In a companion paper, also just submitted to arXiv, the semantics is\n  used to reason about slicing. So as to make each paper self-contained, there\n  is some overlap between them", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article develops a novel operational semantics for probabilistic\ncontrol-flow graphs (pCFGs) of probabilistic imperative programs with random\nassignment and \"observe\" (or conditioning) statements. The semantics transforms\nprobability distributions (on stores) as control moves from one node to another\nin pCFGs. We relate this semantics to a standard, expectation-transforming,\ndenotational semantics of structured probabilistic imperative programs, by\ntranslating structured programs into (unstructured) pCFGs, and proving adequacy\nof the translation. This shows that the operational semantics can be used\nwithout loss of information, and is faithful to the \"intended\" semantics and\nhence can be used to reason about, for example, the correctness of\ntransformations (as we do in a companion article).\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 02:04:34 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Amtoft", "Torben", ""], ["Banerjee", "Anindya", ""]]}, {"id": "1711.03016", "submitter": "Lane Schwartz", "authors": "Richard Wei, Lane Schwartz, Vikram Adve", "title": "DLVM: A modern compiler infrastructure for deep learning systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning software demands reliability and performance. However, many of\nthe existing deep learning frameworks are software libraries that act as an\nunsafe DSL in Python and a computation graph interpreter. We present DLVM, a\ndesign and implementation of a compiler infrastructure with a linear algebra\nintermediate representation, algorithmic differentiation by adjoint code\ngeneration, domain-specific optimizations and a code generator targeting GPU\nvia LLVM. Designed as a modern compiler infrastructure inspired by LLVM, DLVM\nis more modular and more generic than existing deep learning compiler\nframeworks, and supports tensor DSLs with high expressivity. With our\nprototypical staged DSL embedded in Swift, we argue that the DLVM system\nenables a form of modular, safe and performant frameworks for deep learning.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 15:33:23 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 14:47:33 GMT"}, {"version": "v3", "created": "Wed, 6 Dec 2017 01:55:59 GMT"}, {"version": "v4", "created": "Mon, 11 Dec 2017 21:49:48 GMT"}, {"version": "v5", "created": "Fri, 2 Feb 2018 21:07:25 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Wei", "Richard", ""], ["Schwartz", "Lane", ""], ["Adve", "Vikram", ""]]}, {"id": "1711.03028", "submitter": "Russell O'Connor", "authors": "Russell O'Connor", "title": "Simplicity: A New Language for Blockchains", "comments": null, "journal-ref": "2017. Proceedings of the 2017 Workshop on Programming Languages\n  and Analysis for Security. ACM, New York, NY, USA", "doi": "10.1145/3139337.3139340", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simplicity is a typed, combinator-based, functional language without loops\nand recursion, designed to be used for crypto-currencies and blockchain\napplications. It aims to improve upon existing crypto-currency languages, such\nas Bitcoin Script and Ethereum's EVM, while avoiding some of the problems they\nface. Simplicity comes with formal denotational semantics defined in Coq, a\npopular, general purpose software proof assistant. Simplicity also includes\noperational semantics that are defined with an abstract machine that we call\nthe Bit Machine. The Bit Machine is used as a tool for measuring the\ncomputational space and time resources needed to evaluate Simplicity programs.\nOwing to its Turing incompleteness, Simplicity is amenable to static analysis\nthat can be used to derive upper bounds on the computational resources needed,\nprior to execution. While Turing incomplete, Simplicity can express any\nfinitary function, which we believe is enough to build useful \"smart contracts\"\nfor blockchain applications.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 16:07:52 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 23:31:38 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["O'Connor", "Russell", ""]]}, {"id": "1711.03050", "submitter": "Gabriel Scherer", "authors": "Olivier Fl\\\"uckiger, Gabriel Scherer, Ming-Ho Yee, Aviral Goel, Amal\n  Ahmed, Jan Vitek", "title": "Correctness of Speculative Optimizations with Dynamic Deoptimization", "comments": null, "journal-ref": "Proceedings of the ACM on Programming Languages (POPL 2018)", "doi": "10.1145/3158137", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-performance dynamic language implementations make heavy use of\nspeculative optimizations to achieve speeds close to statically compiled\nlanguages. These optimizations are typically performed by a just-in-time\ncompiler that generates code under a set of assumptions about the state of the\nprogram and its environment. In certain cases, a program may execute code\ncompiled under assumptions that are no longer valid. The implementation must\nthen deoptimize the program on-the-fly; this entails finding semantically\nequivalent code that does not rely on invalid assumptions, translating program\nstate to that expected by the target code, and transferring control. This paper\nlooks at the interaction between optimization and deoptimization, and shows\nthat reasoning about speculation is surprisingly easy when assumptions are made\nexplicit in the program representation. This insight is demonstrated on a\ncompiler intermediate representation, named \\sourir, modeled after the\nhigh-level representation for a dynamic language. Traditional compiler\noptimizations such constant folding, dead code elimination, and function\ninlining are shown to be correct in the presence of assumptions. Furthermore,\nthe paper establishes the correctness of compiler transformations specific to\ndeoptimization: namely unrestricted deoptimization, predicate hoisting, and\nassume composition.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 16:52:00 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 21:27:10 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Fl\u00fcckiger", "Olivier", ""], ["Scherer", "Gabriel", ""], ["Yee", "Ming-Ho", ""], ["Goel", "Aviral", ""], ["Ahmed", "Amal", ""], ["Vitek", "Jan", ""]]}, {"id": "1711.03147", "submitter": "Clemente Rubio-Manzano", "authors": "Clemente Rubio-Manzano, Martin Pereira-Fari\\~na", "title": "On the incorporation of interval-valued fuzzy sets into the Bousi-Prolog\n  system: declarative semantics, implementation and applications", "comments": null, "journal-ref": "Interactions Between Computational Intelligence and Mathematics\n  Studies in Computational Intelligence, vol 794. Springer 2018", "doi": "10.1007/978-3-030-01632-6_1", "report-no": null, "categories": "cs.AI cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyse the benefits of incorporating interval-valued fuzzy\nsets into the Bousi-Prolog system. A syntax, declarative semantics and im-\nplementation for this extension is presented and formalised. We show, by using\npotential applications, that fuzzy logic programming frameworks enhanced with\nthem can correctly work together with lexical resources and ontologies in order\nto improve their capabilities for knowledge representation and reasoning.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 20:25:43 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Rubio-Manzano", "Clemente", ""], ["Pereira-Fari\u00f1a", "Martin", ""]]}, {"id": "1711.03219", "submitter": "Ohad Kammar", "authors": "Adam \\'Scibior, Ohad Kammar, Matthijs V\\'ak\\'ar, Sam Staton, Hongseok\n  Yang, Yufei Cai, Klaus Ostermann, Sean K. Moss, Chris Heunen, and Zoubin\n  Ghahramani", "title": "Denotational validation of higher-order Bayesian inference", "comments": null, "journal-ref": "Proc. ACM Program. Lang. 2, POPL, Article 60 (January 2018)", "doi": "10.1145/3158148", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a modular semantic account of Bayesian inference algorithms for\nprobabilistic programming languages, as used in data science and machine\nlearning. Sophisticated inference algorithms are often explained in terms of\ncomposition of smaller parts. However, neither their theoretical justification\nnor their implementation reflects this modularity. We show how to conceptualise\nand analyse such inference algorithms as manipulating intermediate\nrepresentations of probabilistic programs using higher-order functions and\ninductive types, and their denotational semantics. Semantic accounts of\ncontinuous distributions use measurable spaces. However, our use of\nhigher-order functions presents a substantial technical difficulty: it is\nimpossible to define a measurable space structure over the collection of\nmeasurable functions between arbitrary measurable spaces that is compatible\nwith standard operations on those functions, such as function application. We\novercome this difficulty using quasi-Borel spaces, a recently proposed\nmathematical structure that supports both function spaces and continuous\ndistributions. We define a class of semantic structures for representing\nprobabilistic programs, and semantic validity criteria for transformations of\nthese representations in terms of distribution preservation. We develop a\ncollection of building blocks for composing representations. We use these\nbuilding blocks to validate common inference algorithms such as Sequential\nMonte Carlo and Markov Chain Monte Carlo. To emphasize the connection between\nthe semantic manipulation and its traditional measure theoretic origins, we use\nKock's synthetic measure theory. We demonstrate its usefulness by proving a\nquasi-Borel counterpart to the Metropolis-Hastings-Green theorem.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 00:32:01 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["\u015acibior", "Adam", ""], ["Kammar", "Ohad", ""], ["V\u00e1k\u00e1r", "Matthijs", ""], ["Staton", "Sam", ""], ["Yang", "Hongseok", ""], ["Cai", "Yufei", ""], ["Ostermann", "Klaus", ""], ["Moss", "Sean K.", ""], ["Heunen", "Chris", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1711.03239", "submitter": "Osbert Bastani", "authors": "Osbert Bastani, Rahul Sharma, Alex Aiken, Percy Liang", "title": "Active Learning of Points-To Specifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When analyzing programs, large libraries pose significant challenges to\nstatic points-to analysis. A popular solution is to have a human analyst\nprovide points-to specifications that summarize relevant behaviors of library\ncode, which can substantially improve precision and handle missing code such as\nnative code. We propose ATLAS, a tool that automatically infers points-to\nspecifications. ATLAS synthesizes unit tests that exercise the library code,\nand then infers points-to specifications based on observations from these\nexecutions. ATLAS automatically infers specifications for the Java standard\nlibrary, and produces better results for a client static information flow\nanalysis on a benchmark of 46 Android apps compared to using existing\nhandwritten specifications.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 03:03:35 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 18:16:05 GMT"}, {"version": "v3", "created": "Tue, 22 May 2018 01:27:47 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Bastani", "Osbert", ""], ["Sharma", "Rahul", ""], ["Aiken", "Alex", ""], ["Liang", "Percy", ""]]}, {"id": "1711.03272", "submitter": "Siddharth Krishna", "authors": "Siddharth Krishna, Dennis Shasha, Thomas Wies", "title": "Go with the Flow: Compositional Abstractions for Concurrent Data\n  Structures (Extended Version)", "comments": "This is an extended version of a POPL 2018 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concurrent separation logics have helped to significantly simplify\ncorrectness proofs for concurrent data structures. However, a recurring problem\nin such proofs is that data structure abstractions that work well in the\nsequential setting are much harder to reason about in a concurrent setting due\nto complex sharing and overlays. To solve this problem, we propose a novel\napproach to abstracting regions in the heap by encoding the data structure\ninvariant into a local condition on each individual node. This condition may\ndepend on a quantity associated with the node that is computed as a fixpoint\nover the entire heap graph. We refer to this quantity as a flow. Flows can\nencode both structural properties of the heap (e.g. the reachable nodes from\nthe root form a tree) as well as data invariants (e.g. sortedness). We then\nintroduce the notion of a flow interface, which expresses the relies and\nguarantees that a heap region imposes on its context to maintain the local flow\ninvariant with respect to the global heap. Our main technical result is that\nthis notion leads to a new semantic model of separation logic. In this model,\nflow interfaces provide a general abstraction mechanism for describing complex\ndata structures. This abstraction mechanism admits proof rules that generalize\nover a wide variety of data structures. To demonstrate the versatility of our\napproach, we show how to extend the logic RGSep with flow interfaces. We have\nused this new logic to prove linearizability and memory safety of nontrivial\nconcurrent data structures. In particular, we obtain parametric linearizability\nproofs for concurrent dictionary algorithms that abstract from the details of\nthe underlying data structure representation. These proofs cannot be easily\nexpressed using the abstraction mechanisms provided by existing separation\nlogics.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 06:45:24 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Krishna", "Siddharth", ""], ["Shasha", "Dennis", ""], ["Wies", "Thomas", ""]]}, {"id": "1711.03424", "submitter": "Cynthia Kop", "authors": "Cynthia Kop", "title": "Cons-free Programming with Immutable Functions", "comments": "workshop proceedings for DICE 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the power of non-determinism in purely functional programming\nlanguages with higher-order types. Specifically, we set out to characterise the\nhierarchy NP $\\subseteq$ NEXP $\\subseteq$ NEXP$^{(2)}$ $\\subseteq \\cdots\n\\subseteq$ NEXP$^{(k)}$ $\\subseteq \\cdots$ solely in terms of higher-typed,\npurely functional programs. Although the work is incomplete, we present an\ninitial approach using cons-free programs with immutable functions.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 15:28:39 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Kop", "Cynthia", ""]]}, {"id": "1711.03433", "submitter": "Cynthia Kop", "authors": "Cynthia Kop, Kristoffer Rose", "title": "h: A Plank for Higher-order Attribute Contraction Schemes", "comments": "workshop proceedings for HOR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and formalize h, a core (or \"plank\") calculus that can serve as\nthe foundation for several compiler specification languages, notably CRSX\n(Combinatory Reductions Systems with eXtensions), HACS (Higher-order Attribute\nContraction Schemes), and TransScript. We discuss how the h typing and\nformation rules introduce the necessary restrictions to ensure that rewriting\nis well-defined, even in the presence of h's powerful extensions for\nmanipulating free variables and environments as first class elements (including\nin pattern matching).\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 15:52:18 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Kop", "Cynthia", ""], ["Rose", "Kristoffer", ""]]}, {"id": "1711.03436", "submitter": "Osbert Bastani", "authors": "Osbert Bastani and Lazaro Clapp and Saswat Anand and Rahul Sharma and\n  Alex Aiken", "title": "Eventually Sound Points-To Analysis with Missing Code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static analyses make the increasingly tenuous assumption that all source code\nis available for analysis; for example, large libraries often call into native\ncode that cannot be analyzed. We propose a points-to analysis that initially\nmakes optimistic assumptions about missing code, and then inserts runtime\nchecks that report counterexamples to these assumptions that occur during\nexecution. Our approach guarantees eventual soundness, i.e., the static\nanalysis is sound for the available code after some finite number of\ncounterexamples. We implement Optix, an eventually sound points-to analysis for\nAndroid apps, where the Android framework is missing. We show that the runtime\nchecks added by Optix incur low overhead on real programs, and demonstrate how\nOptix improves a client information flow analysis for detecting Android\nmalware.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 15:54:49 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Bastani", "Osbert", ""], ["Clapp", "Lazaro", ""], ["Anand", "Saswat", ""], ["Sharma", "Rahul", ""], ["Aiken", "Alex", ""]]}, {"id": "1711.03588", "submitter": "Carroll Morgan", "authors": "Annabelle McIver, Carroll Morgan, Benjamin Lucien Kaminski,\n  Joost-Pieter Katoen", "title": "A New Proof Rule for Almost-Sure Termination", "comments": "V1 to appear in PoPL18. This version collects some existing text into\n  new example subsection 5.5 and adds a new example 5.6 and makes further\n  remarks about uncountable branching. The new example 5.6 relates to work on\n  lexicographic termination methods, also to appear in PoPL18 [Agrawal et al,\n  2018]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important question for a probabilistic program is whether the probability\nmass of all its diverging runs is zero, that is that it terminates \"almost\nsurely\". Proving that can be hard, and this paper presents a new method for\ndoing so; it is expressed in a program logic, and so applies directly to source\ncode. The programs may contain both probabilistic- and demonic choice, and the\nprobabilistic choices may depend on the current state.\n  As do other researchers, we use variant functions (a.k.a.\n\"super-martingales\") that are real-valued and probabilistically might decrease\non each loop iteration; but our key innovation is that the amount as well as\nthe probability of the decrease are parametric.\n  We prove the soundness of the new rule, indicate where its applicability goes\nbeyond existing rules, and explain its connection to classical results on\ndenumerable (non-demonic) Markov chains.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 20:29:00 GMT"}, {"version": "v2", "created": "Tue, 26 Dec 2017 01:09:43 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["McIver", "Annabelle", ""], ["Morgan", "Carroll", ""], ["Kaminski", "Benjamin Lucien", ""], ["Katoen", "Joost-Pieter", ""]]}, {"id": "1711.03620", "submitter": "David Van Horn", "authors": "Phuc C. Nguyen, Thomas Gilray, Sam Tobin-Hochstadt, and David Van Horn", "title": "Soft Contract Verification for Higher-Order Stateful Programs", "comments": "ACM SIGPLAN Symposium on Principles of Programming Language (POPL)", "journal-ref": "Proceedings of the ACM on Programming Languages, Vol. 2, No. POPL,\n  Article 51. Publication date: January 2018", "doi": "10.1145/3158139", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software contracts allow programmers to state rich program properties using\nthe full expressive power of an object language. However, since they are\nenforced at runtime, monitoring contracts imposes significant overhead and\ndelays error discovery. So contract verification aims to guarantee all or most\nof these properties ahead of time, enabling valuable optimizations and yielding\na more general assurance of correctness. Existing methods for static contract\nverification satisfy the needs of more restricted target languages, but fail to\naddress the challenges unique to those conjoining untyped, dynamic programming,\nhigher-order functions, modularity, and statefulness. Our approach tackles all\nthese features at once, in the context of the full Racket system---a mature\nenvironment for stateful, higher-order, multi-paradigm programming with or\nwithout types. Evaluating our method using a set of both pure and stateful\nbenchmarks, we are able to verify 99.94% of checks statically (all but 28 of\n49, 861).\n  Stateful, higher-order functions pose significant challenges for static\ncontract verification in particular. In the presence of these features, a\nmodular analysis must permit code from the current module to escape permanently\nto an opaque context (unspecified code from outside the current module) that\nmay be stateful and therefore store a reference to the escaped closure. Also,\ncontracts themselves, being predicates wri en in unrestricted Racket, may\nexhibit stateful behavior; a sound approach must be robust to contracts which\nare arbitrarily expressive and interwoven with the code they monitor. In this\npaper, we present and evaluate our solution based on higher-order symbolic\nexecution, explain the techniques we used to address such thorny issues,\nformalize a notion of behavioral approximation, and use it to provide a\nmechanized proof of soundness.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 22:05:47 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Nguyen", "Phuc C.", ""], ["Gilray", "Thomas", ""], ["Tobin-Hochstadt", "Sam", ""], ["Van Horn", "David", ""]]}, {"id": "1711.03829", "submitter": "Maximilian Schwenger", "authors": "Peter Faymonville (1), Bernd Finkbeiner (1), Maximilian Schwenger (1),\n  Hazem Torfah (1) ((1) Saarland University)", "title": "Real-time Stream-based Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce RTLola, a new stream-based specification language for the\ndescription of real-time properties of reactive systems. The key feature is the\nintegration of sliding windows over real-time intervals with aggregation\nfunctions into the language. Using sliding windows we can detach fixed-rate\noutput streams from the varying rate input streams. We provide an efficient\nevaluation algorithm of the sliding windows by partitioning the windows into\nintervals according to a given monitor frequency. For useful aggregation\nfunctions, the intervals allow a more efficient way to compute the aggregation\nvalue by dynamically reusing interval summaries. In general, the number of\ninput values within a single window instance can grow arbitrarily large\ndisallowing any guarantees on the expected memory consumption. Assuming a fixed\nmonitor output rate, we can provide memory guarantees which can be computed\na-priori. Additionally, for specifications using certain classes of aggregation\nfunctions, we can perform a more precise, better memory analysis. We\ndemonstrate the applicability of the new language on practical examples.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 14:27:19 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 11:52:34 GMT"}, {"version": "v3", "created": "Thu, 20 Sep 2018 08:17:13 GMT"}, {"version": "v4", "created": "Wed, 12 Jun 2019 09:37:17 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Faymonville", "Peter", "", "Saarland University"], ["Finkbeiner", "Bernd", "", "Saarland University"], ["Schwenger", "Maximilian", "", "Saarland University"], ["Torfah", "Hazem", "", "Saarland University"]]}, {"id": "1711.03842", "submitter": "Anish Tondwalkar", "authors": "Niki Vazou, Anish Tondwalkar, Vikraman Choudhury, Ryan G. Scott, Ryan\n  R. Newton, Philip Wadler, Ranjit Jhala", "title": "Refinement Reflection: Complete Verification with SMT", "comments": "29 pages plus appendices, to appear in POPL 2018. arXiv admin note:\n  text overlap with arXiv:1610.04641", "journal-ref": null, "doi": "10.1145/3158141", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We introduce Refinement Reflection, a new framework for building SMT-based\ndeductive verifiers. The key idea is to reflect the code implementing a\nuser-defined function into the function's (output) refinement type. As a\nconsequence, at uses of the function, the function definition is instantiated\nin the SMT logic in a precise fashion that permits decidable verification.\nReflection allows the user to write equational proofs of programs just by\nwriting other programs using pattern-matching and recursion to perform\ncase-splitting and induction. Thus, via the propositions-as-types principle, we\nshow that reflection permits the specification of arbitrary functional\ncorrectness properties. Finally, we introduce a proof-search algorithm called\nProof by Logical Evaluation that uses techniques from model checking and\nabstract interpretation, to completely automate equational reasoning. We have\nimplemented reflection in Liquid Haskell and used it to verify that the widely\nused instances of the Monoid, Applicative, Functor, and Monad typeclasses\nactually satisfy key algebraic laws required to make the clients safe, and have\nused reflection to build the first library that actually verifies assumptions\nabout associativity and ordering that are crucial for safe deterministic\nparallelism.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 17:51:16 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Vazou", "Niki", ""], ["Tondwalkar", "Anish", ""], ["Choudhury", "Vikraman", ""], ["Scott", "Ryan G.", ""], ["Newton", "Ryan R.", ""], ["Wadler", "Philip", ""], ["Jhala", "Ranjit", ""]]}, {"id": "1711.03871", "submitter": "Daniel Patterson", "authors": "Daniel Patterson, Jamie Perconti, Christos Dimoulas, Amal Ahmed", "title": "FunTAL: Reasonably Mixing a Functional Language with Assembly", "comments": "15 pages; implementation at https://dbp.io/artifacts/funtal/;\n  published in PLDI '17, Proceedings of the 38th ACM SIGPLAN Conference on\n  Programming Language Design and Implementation, June 18 - 23, 2017,\n  Barcelona, Spain", "journal-ref": null, "doi": "10.1145/3140587.3062347", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present FunTAL, the first multi-language system to formalize safe\ninteroperability between a high-level functional language and low-level\nassembly code while supporting compositional reasoning about the mix. A central\nchallenge in developing such a multi-language is bridging the gap between\nassembly, which is staged into jumps to continuations, and high-level code,\nwhere subterms return a result. We present a compositional stack-based typed\nassembly language that supports components, comprised of one or more basic\nblocks, that may be embedded in high-level contexts. We also present a logical\nrelation for FunTAL that supports reasoning about equivalence of high-level\ncomponents and their assembly replacements, mixed-language programs with\ncallbacks between languages, and assembly components comprised of different\nnumbers of basic blocks.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 15:25:50 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Patterson", "Daniel", ""], ["Perconti", "Jamie", ""], ["Dimoulas", "Christos", ""], ["Ahmed", "Amal", ""]]}, {"id": "1711.04001", "submitter": "Navid Yaghmazadeh", "authors": "Navid Yaghmazadeh, Xinyu Wang, Isil Dillig", "title": "Automated Migration of Hierarchical Data to Relational Tables using\n  Programming-by-Example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many applications export data in hierarchical formats like XML and\nJSON, it is often necessary to convert such hierarchical documents to a\nrelational representation. This paper presents a novel programming-by-example\napproach, and its implementation in a tool called Mitra, for automatically\nmigrating tree-structured documents to relational tables. We have evaluated the\nproposed technique using two sets of experiments. In the first experiment, we\nused Mitra to automate 98 data transformation tasks collected from\nStackOverflow. Our method can generate the desired program for 94% of these\nbenchmarks with an average synthesis time of 3.8 seconds. In the second\nexperiment, we used Mitra to generate programs that can convert real-world XML\nand JSON datasets to full-fledged relational databases. Our evaluation shows\nthat Mitra can automate the desired transformation for all datasets.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 20:08:18 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Yaghmazadeh", "Navid", ""], ["Wang", "Xinyu", ""], ["Dillig", "Isil", ""]]}, {"id": "1711.04422", "submitter": "John Regehr", "authors": "Raimondas Sasnauskas, Yang Chen, Peter Collingbourne, Jeroen Ketema,\n  Gratian Lup, Jubi Taneja, John Regehr", "title": "Souper: A Synthesizing Superoptimizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If we can automatically derive compiler optimizations, we might be able to\nsidestep some of the substantial engineering challenges involved in creating\nand maintaining a high-quality compiler. We developed Souper, a synthesizing\nsuperoptimizer, to see how far these ideas might be pushed in the context of\nLLVM. Along the way, we discovered that Souper's intermediate representation\nwas sufficiently similar to the one in Microsoft Visual C++ that we applied\nSouper to that compiler as well. Shipping, or about-to-ship, versions of both\ncompilers contain optimizations suggested by Souper but implemented by hand.\nAlternately, when Souper is used as a fully automated optimization pass it\ncompiles a Clang compiler binary that is about 3 MB (4.4%) smaller than the one\ncompiled by LLVM.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 05:01:56 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 01:50:19 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Sasnauskas", "Raimondas", ""], ["Chen", "Yang", ""], ["Collingbourne", "Peter", ""], ["Ketema", "Jeroen", ""], ["Lup", "Gratian", ""], ["Taneja", "Jubi", ""], ["Regehr", "John", ""]]}, {"id": "1711.04559", "submitter": "Daniel Patterson", "authors": "Daniel Patterson and Amal Ahmed", "title": "Linking Types for Multi-Language Software: Have Your Cake and Eat It Too", "comments": "14 pages; published in SNAPL '17, 2nd Summit on Advances in\n  Programming Languages, May 7-10, 2017, Pacific Grove, California", "journal-ref": null, "doi": "10.4230/LIPIcs.SNAPL.2017.12", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Software developers compose systems from components written in many different\nlanguages. A business-logic component may be written in Java or OCaml, a\nresource-intensive component in C or Rust, and a high-assurance component in\nCoq. In this multi-language world, program execution sends values from one\nlinguistic context to another. This boundary-crossing exposes values to\ncontexts with unforeseen behavior---that is, behavior that could not arise in\nthe source language of the value. For example, a Rust function may end up being\napplied in an ML context that violates the memory usage policy enforced by\nRust's type system. This leads to the question of how developers ought to\nreason about code in such a multi-language world where behavior inexpressible\nin one language is easily realized in another.\n  This paper proposes the novel idea of linking types to address the problem of\nreasoning about single-language components in a multi-lingual setting.\nSpecifically, linking types allow programmers to annotate where in a program\nthey can link with components inexpressible in their unadulterated language.\nThis enables developers to reason about (behavioral) equality using only their\nown language and the annotations, even though their code may be linked with\ncode written in a language with more expressive power.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 13:02:25 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Patterson", "Daniel", ""], ["Ahmed", "Amal", ""]]}, {"id": "1711.04718", "submitter": "Peng Fu", "authors": "Peng Fu", "title": "A Type Checking Algorithm for Higher-rank, Impredicative and\n  Second-order Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a type checking algorithm that is able to type check a nontrivial\nsubclass of functional programs that use features such as higher-rank,\nimpredicative and second-order types. The only place the algorithm requires\ntype annotation is before each function declaration. We prove the soundness of\nthe type checking algorithm with respect to System $\\mathbf{F}_{\\omega}$, i.e.\nif the program is type checked, then the type checker will produce a well-typed\nannotated System $\\mathbf{F}_{\\omega}$ term. We extend the basic algorithm to\nhandle pattern matching and let-bindings. We implement a prototype type checker\nand test it on a variety of functional programs.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 17:30:12 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Fu", "Peng", ""]]}, {"id": "1711.05019", "submitter": "Stefan Wagner", "authors": "Stefan Wagner, Jan J\\\"urjens, Claudia Koller, Peter Trischberger", "title": "Comparing Bug Finding Tools with Reviews and Tests", "comments": "16 pages, 1 figure", "journal-ref": "Khendek F., Dssouli R. (eds) Testing of Communicating Systems.\n  TestCom 2005. Lecture Notes in Computer Science, vol 3502. Springer, Berlin,\n  Heidelberg", "doi": "10.1007/11430230_4", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bug finding tools can find defects in software source code us- ing an\nautomated static analysis. This automation may be able to reduce the time spent\nfor other testing and review activities. For this we need to have a clear\nunderstanding of how the defects found by bug finding tools relate to the\ndefects found by other techniques. This paper describes a case study using\nseveral projects mainly from an industrial environment that were used to\nanalyse the interrelationships. The main finding is that the bug finding tools\npredominantly find different defects than testing but a subset of defects found\nby reviews. However, the types that can be detected are analysed more\nthoroughly. Therefore, a combination is most advisable if the high number of\nfalse positives of the tools can be tolerated.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 09:33:49 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Wagner", "Stefan", ""], ["J\u00fcrjens", "Jan", ""], ["Koller", "Claudia", ""], ["Trischberger", "Peter", ""]]}, {"id": "1711.05159", "submitter": "Mathys Rennela", "authors": "Mathys Rennela and Sam Staton", "title": "Classical Control, Quantum Circuits and Linear Logic in Enriched\n  Category Theory", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 1 (March 10,\n  2020) lmcs:6192", "doi": "10.23638/LMCS-16(1:30)2020", "report-no": null, "categories": "cs.LO cs.PL math.CT math.OA quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe categorical models of a circuit-based (quantum) functional\nprogramming language. We show that enriched categories play a crucial role.\nFollowing earlier work on QWire by Paykin et al., we consider both a simple\nfirst-order linear language for circuits, and a more powerful host language,\nsuch that the circuit language is embedded inside the host language. Our\ncategorical semantics for the host language is standard, and involves cartesian\nclosed categories and monads. We interpret the circuit language not in an\nordinary category, but in a category that is enriched in the host category. We\nshow that this structure is also related to linear/non-linear models. As an\nextended example, we recall an earlier result that the category of W*-algebras\nis dcpo-enriched, and we use this model to extend the circuit language with\nsome recursive types.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 15:59:21 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 17:49:53 GMT"}, {"version": "v3", "created": "Thu, 22 Aug 2019 13:19:22 GMT"}, {"version": "v4", "created": "Fri, 17 Jan 2020 21:38:22 GMT"}, {"version": "v5", "created": "Mon, 9 Mar 2020 16:39:50 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Rennela", "Mathys", ""], ["Staton", "Sam", ""]]}, {"id": "1711.05787", "submitter": "Rishabh Singh", "authors": "Jeevana Priya Inala and Rishabh Singh", "title": "WebRelate: Integrating Web Data with Spreadsheets using Examples", "comments": "To appear in POPL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data integration between web sources and relational data is a key challenge\nfaced by data scientists and spreadsheet users. There are two main challenges\nin programmatically joining web data with relational data. First, most websites\ndo not expose a direct interface to obtain tabular data, so the user needs to\nformulate a logic to get to different webpages for each input row in the\nrelational table. Second, after reaching the desired webpage, the user needs to\nwrite complex scripts to extract the relevant data, which is often conditioned\non the input data. Since many data scientists and end-users come from diverse\nbackgrounds, writing such complex regular-expression based logical scripts to\nperform data integration tasks is unfortunately often beyond their programming\nexpertise.\n  We present WebRelate, a system that allows users to join semi-structured web\ndata with relational data in spreadsheets using input-output examples.\nWebRelate decomposes the web data integration task into two sub-tasks of i) URL\nlearning and ii) input-dependent web extraction. The first sub-task generates\nthe URLs for the webpages containing the desired data for all rows in the\nrelational table. WebRelate achieves this by learning a string transformation\nprogram using a few example URLs. The second sub-task uses examples of desired\ndata to be extracted from the corresponding webpages and learns a program to\nextract the data for the other rows. We design expressive domain-specific\nlanguages for URL generation and web data extraction, and present efficient\nsynthesis algorithms for learning programs in these DSLs from few input-output\nexamples. We evaluate WebRelate on 88 real-world web data integration tasks\ntaken from online help forums and Excel product team, and show that WebRelate\ncan learn the desired programs within few seconds using only 1 example for the\nmajority of the tasks.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 20:17:07 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Inala", "Jeevana Priya", ""], ["Singh", "Rishabh", ""]]}, {"id": "1711.06092", "submitter": "Yehia Abd Alrahman", "authors": "Yehia Abd Alrahman, Rocco De Nicola and Michele Loreti", "title": "Programming the Interactions of Collective Adaptive Systems by Relying\n  on Attribute-based Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective adaptive systems are new emerging computational systems consisting\nof a large number of interacting components and featuring complex behaviour.\nThese systems are usually distributed, heterogeneous, decentralised and\ninterdependent, and are operating in dynamic and possibly unpredictable\nenvironments. Finding ways to understand and design these systems and, most of\nall, to model the interactions of their components, is a difficult but\nimportant endeavour. In this article we propose a language-based approach for\nprogramming the interactions of collective-adaptive systems by relying on\nattribute-based communication; a paradigm that permits a group of partners to\ncommunicate by considering their run-time properties and capabilities. We\nintroduce AbC, a foundational calculus for attribute-based communication and\nshow how its linguistic primitives can be used to program a complex and\nsophisticated variant of the well-known problem of Stable Allocation in Content\nDelivery Networks. Also other interesting case studies, from the realm of\ncollective-adaptive systems, are considered. We also illustrate the expressive\npower of attribute-based communication by showing the natural encoding of other\nexisting communication paradigms into AbC.\n", "versions": [{"version": "v1", "created": "Thu, 26 Oct 2017 17:23:16 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 09:35:11 GMT"}, {"version": "v3", "created": "Wed, 29 Nov 2017 09:17:30 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Alrahman", "Yehia Abd", ""], ["De Nicola", "Rocco", ""], ["Loreti", "Michele", ""]]}, {"id": "1711.06115", "submitter": "M. Ammar Ben Khadra", "authors": "M. Ammar Ben Khadra", "title": "An introduction to approximate computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate computing is a research area where we investigate a wide spectrum\nof techniques to trade off computation accuracy for better performance or\nenergy consumption. In this work, we provide a general introduction to\napproximate computing. Also, we propose a taxonomy to make it easier to discuss\nthe merits of different approximation techniques. Our taxonomy emphasizes the\nexpected cost of tackling approximate computing across the entire system stack.\nWe conclude by discussing the unique opportunities as well as challenges of\nnondeterministic approximate computing.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 12:15:09 GMT"}, {"version": "v2", "created": "Sun, 10 Dec 2017 17:05:21 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Khadra", "M. Ammar Ben", ""]]}, {"id": "1711.06467", "submitter": "Aseem Rastogi", "authors": "Aseem Rastogi, Nikhil Swamy, Michael Hicks", "title": "WYS*: A DSL for Verified Secure Multi-party Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secure multi-party computation (MPC) enables a set of mutually distrusting\nparties to cooperatively compute, using a cryptographic protocol, a function\nover their private data. This paper presents Wys*, a new domain-specific\nlanguage (DSL) for writing mixed-mode MPCs. Wys* is an embedded DSL hosted in\nF*, a verification-oriented, effectful programming language. Wys* source\nprograms are essentially F* programs written in a custom MPC effect, meaning\nthat the programmers can use F*'s logic to verify the correctness and security\nproperties of their programs. To reason about the distributed runtime semantics\nof these programs, we formalize a deep embedding of Wys*, also in F*. We\nmechanize the necessary metatheory to prove that the properties verified for\nthe Wys* source programs carry over to the distributed, multi-party semantics.\nFinally, we use F*'s extraction to extract an interpreter that we have proved\nmatches this semantics, yielding a partially verified implementation. Wys* is\nthe first DSL to enable formal verification of MPC programs. We have\nimplemented several MPC protocols in Wys*, including private set intersection,\njoint median, and an MPC card dealing application, and have verified their\ncorrectness and security.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 09:36:16 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 17:18:17 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Rastogi", "Aseem", ""], ["Swamy", "Nikhil", ""], ["Hicks", "Michael", ""]]}, {"id": "1711.07148", "submitter": "Ke Wang", "authors": "Ke Wang, RIshabh Singh, Zhendong Su", "title": "Data-Driven Feedback Generation for Introductory Programming Exercises", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CY cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the \"Search, Align, and Repair\" data-driven program\nrepair framework to automate feedback generation for introductory programming\nexercises. Distinct from existing techniques, our goal is to develop an\nefficient, fully automated, and problem-agnostic technique for large or\nMOOC-scale introductory programming courses. We leverage the large amount of\navailable student submissions in such settings and develop new algorithms for\nidentifying similar programs, aligning correct and incorrect programs, and\nrepairing incorrect programs by finding minimal fixes. We have implemented our\ntechnique in the SARFGEN system and evaluated it on thousands of real student\nattempts from the Microsoft-DEV204.1X edX course and the Microsoft CodeHunt\nplatform. Our results show that SARFGEN can, within two seconds on average,\ngenerate concise, useful feedback for 89.7% of the incorrect student\nsubmissions. It has been integrated with the Microsoft-DEV204.1X edX class and\ndeployed for production use.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 05:08:48 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Wang", "Ke", ""], ["Singh", "RIshabh", ""], ["Su", "Zhendong", ""]]}, {"id": "1711.07163", "submitter": "Ke Wang", "authors": "Ke Wang, Rishabh Singh, Zhendong Su", "title": "Dynamic Neural Program Embedding for Program Repair", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural program embeddings have shown much promise recently for a variety of\nprogram analysis tasks, including program synthesis, program repair, fault\nlocalization, etc. However, most existing program embeddings are based on\nsyntactic features of programs, such as raw token sequences or abstract syntax\ntrees. Unlike images and text, a program has an unambiguous semantic meaning\nthat can be difficult to capture by only considering its syntax (i.e.\nsyntactically similar pro- grams can exhibit vastly different run-time\nbehavior), which makes syntax-based program embeddings fundamentally limited.\nThis paper proposes a novel semantic program embedding that is learned from\nprogram execution traces. Our key insight is that program states expressed as\nsequential tuples of live variable values not only captures program semantics\nmore precisely, but also offer a more natural fit for Recurrent Neural Networks\nto model. We evaluate different syntactic and semantic program embeddings on\npredicting the types of errors that students make in their submissions to an\nintroductory programming class and two exercises on the CodeHunt education\nplatform. Evaluation results show that our new semantic program embedding\nsignificantly outperforms the syntactic program embeddings based on token\nsequences and abstract syntax trees. In addition, we augment a search-based\nprogram repair system with the predictions obtained from our se- mantic\nembedding, and show that search efficiency is also significantly improved.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 06:02:06 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 22:15:32 GMT"}, {"version": "v3", "created": "Sun, 25 Feb 2018 22:48:31 GMT"}, {"version": "v4", "created": "Sat, 30 Jun 2018 00:33:27 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Wang", "Ke", ""], ["Singh", "Rishabh", ""], ["Su", "Zhendong", ""]]}, {"id": "1711.07257", "submitter": "Giuseppe Lipari", "authors": "Cl\\'ement Ballabriga and Julien Forget and Giuseppe Lipari", "title": "Abstract Interpretation of Binary Code with Memory Accesses using\n  Polyhedra", "comments": "An earlier version of this paper has been submitted to TACAS 2018\n  (http://www.etaps.org/index.php/2018/tacas) for peer-review. Compared to the\n  submitted paper, this version contains more up-to-date benchmarks in Section\n  6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel methodology for static analysis of binary\ncode using abstract interpretation. We use an abstract domain based on\npolyhedra and two mapping functions that associate polyhedra variables with\nregisters and memory. We demonstrate our methodology to the problem of\ncomputing upper bounds to loop iterations in the code. This problem is\nparticularly important in the domain of Worst-Case Execution Time (WCET)\nanalysis of safety-critical real-time code. However, our approach is general\nand it can applied to other static analysis problems.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 11:14:24 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Ballabriga", "Cl\u00e9ment", ""], ["Forget", "Julien", ""], ["Lipari", "Giuseppe", ""]]}, {"id": "1711.07606", "submitter": "Hongbo Rong", "authors": "Hongbo Rong", "title": "Programmatic Control of a Compiler for Generating High-performance\n  Spatial Hardware", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This methodology paper addresses high-performance high-productivity\nprogramming on spatial architectures. Spatial architectures are efficient for\nexecuting dataflow algorithms, yet for high-performance programming, the\nproductivity is low and verification is painful. We show that coding and\nverification are the biggest obstacle to the wide adoption of spatial\narchitectures. We propose a new programming methodology, T2S (Temporal to\nSpatial), to remove this obstacle. A programmer specifies a temporal definition\nand a spatial mapping. The temporal definition defines the functionality to\ncompute, while the spatial mapping defines how to decompose the functionality\nand map the decomposed pieces onto a spatial architecture. The specification\nprecisely controls a compiler to actually implement the loop and data\ntransformations specified in the mapping. The specification is loop-nest- and\nmatrix-oriented, and thus lends itself to the compiler for automatic, static\nverification. Many generic, strategic loop and data optimizations can be\nsystematically expressed. Consequently, high performance is expected with\nsubstantially higher productivity: compared with high-performance programming\nin today's high-level synthesis (HLS) languages or hardware description\nlanguages (HDLs), the engineering effort on coding and verification is expected\nto be reduced from months to hours, a reduction of 2 or 3 orders of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 02:30:11 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 20:36:49 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Rong", "Hongbo", ""]]}, {"id": "1711.08029", "submitter": "Yu Feng", "authors": "Yu Feng, Ruben Martins, Osbert Bastani, and Isil Dillig", "title": "Program Synthesis using Conflict-Driven Learning", "comments": "This was a miscommunication and one author did not want to make it\n  publicly available right now", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new conflict-driven program synthesis technique that is capable\nof learning from past mistakes. Given a spurious program that violates the\ndesired specification, our synthesis algorithm identifies the root cause of the\nconflict and learns new lemmas that can prevent similar mistakes in the future.\nSpecifically, we introduce the notion of equivalence modulo conflict and show\nhow this idea can be used to learn useful lemmas that allow the synthesizer to\nprune large parts of the search space. We have implemented a general-purpose\nCDCL-style program synthesizer called Neo and evaluate it in two different\napplication domains, namely data wrangling in R and functional programming over\nlists. Our experiments demonstrate the substantial benefits of conflict-driven\nlearning and show that Neo outperforms two state-of-the-art synthesis tools,\nMorpheus and Deepcoder, that target these respective domains.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 20:34:58 GMT"}, {"version": "v2", "created": "Sat, 25 Nov 2017 01:49:11 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Feng", "Yu", ""], ["Martins", "Ruben", ""], ["Bastani", "Osbert", ""], ["Dillig", "Isil", ""]]}, {"id": "1711.08349", "submitter": "Gian Pietro Farina", "authors": "Gian Pietro Farina, Stephen Chong, Marco Gaboardi", "title": "Relational Symbolic Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic execution is a classical program analysis technique used to show\nthat programs satisfy or violate given specifications. In this work we\ngeneralize symbolic execution to support program analysis for relational\nspecifications in the form of relational properties - these are properties\nabout two runs of two programs on related inputs, or about two executions of a\nsingle program on related inputs. Relational properties are useful to formalize\nnotions in security and privacy, and to reason about program optimizations. We\ndesign a relational symbolic execution engine, named RelSym which supports\ninteractive refutation, as well as proving of relational properties for\nprograms written in a language with arrays and for-like loops.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 15:48:05 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 21:22:40 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 21:24:16 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Farina", "Gian Pietro", ""], ["Chong", "Stephen", ""], ["Gaboardi", "Marco", ""]]}, {"id": "1711.08699", "submitter": "Pawel Sobocinski", "authors": "Filippo Bonchi and Dusko Pavlovic and Pawel Sobocinski", "title": "Functorial Semantics for Relational Theories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of Frobenius theory as a generalisation of Lawvere's\nfunctorial semantics approach to categorical universal algebra. Whereas the\nuniverse for models of Lawvere theories is the category of sets and functions,\nor more generally cartesian categories, Frobenius theories take their models in\nthe category of sets and relations, or more generally in cartesian\nbicategories.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 14:15:58 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Bonchi", "Filippo", ""], ["Pavlovic", "Dusko", ""], ["Sobocinski", "Pawel", ""]]}, {"id": "1711.08700", "submitter": "Lu\\'is Cruz-Filipe", "authors": "Lu\\'is Cruz-Filipe, Fabrizio Montesi", "title": "That's Enough: Asynchrony with Standard Choreography Primitives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choreographies are widely used for the specification of concurrent and\ndistributed software architectures. Since asynchronous communications are\nubiquitous in real-world systems, previous works have proposed different\napproaches for the formal modelling of asynchrony in choreographies. Such\napproaches typically rely on ad-hoc syntactic terms or semantics for capturing\nthe concept of messages in transit, yielding different formalisms that have to\nbe studied separately.\n  In this work, we take a different approach, and show that such extensions are\nnot needed to reason about asynchronous communications in choreographies.\nRather, we demonstrate how a standard choreography calculus already has all the\nneeded expressive power to encode messages in transit (and thus asynchronous\ncommunications) through the primitives of process spawning and name mobility.\nThe practical consequence of our results is that we can reason about real-world\nsystems within a choreography formalism that is simpler than those hitherto\nproposed.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 14:16:16 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 09:21:57 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Cruz-Filipe", "Lu\u00eds", ""], ["Montesi", "Fabrizio", ""]]}, {"id": "1711.08847", "submitter": "Van Chan Ngo", "authors": "Van Chan Ngo, Quentin Carbonneaux, Jan Hoffmann", "title": "Bounded Expectations: Resource Analysis for Probabilistic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new static analysis for deriving upper bounds on the\nexpected resource consumption of probabilistic programs. The analysis is fully\nautomatic and derives symbolic bounds that are multivariate polynomials of the\ninputs. The new technique combines manual state-of-the-art reasoning techniques\nfor probabilistic programs with an effective method for automatic\nresource-bound analysis of deterministic programs. It can be seen as both, an\nextension of automatic amortized resource analysis (AARA) to probabilistic\nprograms and an automation of manual reasoning for probabilistic programs that\nis based on weakest preconditions. As a result, bound inference can be reduced\nto off-the-shelf LP solving in many cases and automatically-derived bounds can\nbe interactively extended with standard program logics if the automation fails.\nBuilding on existing work, the soundness of the analysis is proved with respect\nto an operational semantics that is based on Markov decision processes. The\neffectiveness of the technique is demonstrated with a prototype implementation\nthat is used to automatically analyze 39 challenging probabilistic programs and\nrandomized algorithms. Experimental results indicate that the derived constant\nfactors in the bounds are very precise and even optimal for many programs.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 00:55:12 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Ngo", "Van Chan", ""], ["Carbonneaux", "Quentin", ""], ["Hoffmann", "Jan", ""]]}, {"id": "1711.09084", "submitter": "Martin Jon\\'a\\v{s}", "authors": "Jan Mr\\'azek, Martin Jon\\'a\\v{s}, Ji\\v{r}\\'i Barnat", "title": "SMT Queries Decomposition and Caching in Semi-Symbolic Model Checking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In semi-symbolic (control-explicit data-symbolic) model checking the\nstate-space explosion problem is fought by representing sets of states by\nfirst-order formulas over the bit-vector theory. In this model checking\napproach, most of the verification time is spent in an SMT solver on deciding\nsatisfiability of quantified queries, which represent equality of symbolic\nstates. In this paper, we introduce a new scheme for decomposition of symbolic\nstates, which can be used to significantly improve the performance of any\nsemi-symbolic model checker. Using the decomposition, a model checker can issue\nmuch simpler and smaller queries to the solver when compared to the original\ncase. Some SMT calls may be even avoided completely, as the satisfaction of\nsome of the simplified formulas can be decided syntactically. Moreover, the\ndecomposition allows for an efficient caching scheme for quantified formulas.\nTo support our theoretical contribution, we show the performance gain of our\nmodel checker SymDIVINE on a set of examples from the Software Verification\nCompetition.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 22:56:00 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Mr\u00e1zek", "Jan", ""], ["Jon\u00e1\u0161", "Martin", ""], ["Barnat", "Ji\u0159\u00ed", ""]]}, {"id": "1711.09197", "submitter": "Attila Egri-Nagy", "authors": "Attila Egri-Nagy", "title": "Declarativeness: the work done by something else", "comments": "11 pages, first public draft, final version will be published\n  elsewhere", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being declarative means that we do computer programming on higher levels of\nabstraction. This vague definition identifies declarativeness with the act of\nignoring details, but it is a special case of abstraction. The unspecified part\nis some computational work. Automating computations and offloading mental\nprocessing are essentially the same concept, which is fundamental for both\ncomputational and mathematical thinking. This shows that declarativeness is not\njust a particular style, but it is the core idea of programming. Here we\ndemonstrate this argument and examine its consequences for teaching by a\nsystematic study of coding examples from an introductory programming course.\nThe chosen language is Clojure, as it is proven to be accessible for novices.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 06:20:10 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Egri-Nagy", "Attila", ""]]}, {"id": "1711.09281", "submitter": "Milod Kazerounian", "authors": "Milod Kazerounian, Niki Vazou, Austin Bourgerie, Jeffrey S. Foster,\n  Emina Torlak", "title": "Refinement Types for Ruby", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Refinement types are a popular way to specify and reason about key program\nproperties. In this paper, we introduce RTR, a new system that adds refinement\ntypes to Ruby. RTR is built on top of RDL, a Ruby type checker that provides\nbasic type information for the verification process. RTR works by encoding its\nverification problems into Rosette, a solver-aided host language. RTR handles\nmixins through assume-guarantee reasoning and uses just-in-time verification\nfor metaprogramming. We formalize RTR by showing a translation from a core,\nRuby-like language with refinement types into Rosette. We apply RTR to check a\nrange of functional correctness properties on six Ruby programs. We find that\nRTR can successfully verify key methods in these programs, taking only a few\nminutes to perform verification.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 20:18:50 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Kazerounian", "Milod", ""], ["Vazou", "Niki", ""], ["Bourgerie", "Austin", ""], ["Foster", "Jeffrey S.", ""], ["Torlak", "Emina", ""]]}, {"id": "1711.09286", "submitter": "Joachim Breitner", "authors": "Antal Spector-Zabusky and Joachim Breitner and Christine Rizkallah and\n  Stephanie Weirich", "title": "Total Haskell is Reasonable Coq", "comments": "13 pages plus references. Published at CPP'18, In Proceedings of 7th\n  ACM SIGPLAN International Conference on Certified Programs and Proofs\n  (CPP'18). ACM, New York, NY, USA, 2018", "journal-ref": null, "doi": "10.1145/3167092", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We would like to use the Coq proof assistant to mechanically verify\nproperties of Haskell programs. To that end, we present a tool, named\nhs-to-coq, that translates total Haskell programs into Coq programs via a\nshallow embedding. We apply our tool in three case studies -- a lawful Monad\ninstance, \"Hutton's razor\", and an existing data structure library -- and prove\ntheir correctness. These examples show that this approach is viable: both that\nhs-to-coq applies to existing Haskell code, and that the output it produces is\namenable to verification.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 20:49:58 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Spector-Zabusky", "Antal", ""], ["Breitner", "Joachim", ""], ["Rizkallah", "Christine", ""], ["Weirich", "Stephanie", ""]]}, {"id": "1711.09305", "submitter": "David Darais", "authors": "David Darais, Ian Sweet, Chang Liu, Michael Hicks", "title": "A Language for Probabilistically Oblivious Computation", "comments": null, "journal-ref": null, "doi": "10.1145/3371118", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An oblivious computation is one that is free of direct and indirect\ninformation leaks, e.g., due to observable differences in timing and memory\naccess patterns. This paper presents Lambda Obliv, a core language whose type\nsystem enforces obliviousness. Prior work on type-enforced oblivious\ncomputation has focused on deterministic programs. Lambda Obliv is new in its\nconsideration of programs that implement probabilistic algorithms, such as\nthose involved in cryptography. Lambda Obliv employs a substructural type\nsystem and a novel notion of probability region to ensure that information is\nnot leaked via the observed distribution of visible events. Probability regions\nsupport reasoning about probabilistic correlation and independence between\nvalues, and our use of probability regions is motivated by a source of\nunsoundness that we discovered in the type system of ObliVM, a language for\nimplementing state of the art oblivious algorithms. We prove that Lambda\nObliv's type system enforces obliviousness and show that it is expressive\nenough to typecheck advanced tree-based oblivious RAMs.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 23:15:11 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 19:37:34 GMT"}, {"version": "v3", "created": "Thu, 11 Jul 2019 18:37:14 GMT"}, {"version": "v4", "created": "Fri, 19 Jul 2019 19:29:30 GMT"}, {"version": "v5", "created": "Tue, 12 Nov 2019 20:00:43 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Darais", "David", ""], ["Sweet", "Ian", ""], ["Liu", "Chang", ""], ["Hicks", "Michael", ""]]}, {"id": "1711.09640", "submitter": "Michele Pagani", "authors": "Thomas Ehrhard (IRIF), Michele Pagani (IRIF), Christine Tasson (IRIF)", "title": "Measurable Cones and Stable, Measurable Functions", "comments": null, "journal-ref": null, "doi": "10.1145/3158147", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a notion of stable and measurable map between cones endowed with\nmeasurability tests and show that it forms a cpo-enriched cartesian closed\ncategory. This category gives a denotational model of an extension of PCF\nsupporting the main primitives of probabilistic functional programming, like\ncontinuous and discrete probabilistic distributions, sampling, conditioning and\nfull recursion. We prove the soundness and adequacy of this model with respect\nto a call-by-name operational semantics and give some examples of its\ndenotations.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 12:08:37 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Ehrhard", "Thomas", "", "IRIF"], ["Pagani", "Michele", "", "IRIF"], ["Tasson", "Christine", "", "IRIF"]]}, {"id": "1711.09791", "submitter": "Ashkan Tousimojarad Mr", "authors": "Ashkan Tousimojarad, Wim Vanderbauwhede, W Paul Cockshott", "title": "2D Image Convolution using Three Parallel Programming Models on the Xeon\n  Phi", "comments": "This work has been included in the doctoral thesis of Ashkan\n  Tousimojarad, \"GPRM: a high performance programming framework for manycore\n  processors\", University of Glasgow, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image convolution is widely used for sharpening, blurring and edge detection.\nIn this paper, we review two common algorithms for convolving a 2D image by a\nseparable kernel (filter). After optimising the naive codes using loop\nunrolling and SIMD vectorisation, we choose the algorithm with better\nperformance as the baseline for parallelisation. We then compare the parallel\nperformance of the optimised code using OpenMP, OpenCL and GPRM implementations\non the Intel Xeon Phi. We also measure the effects of optimisation techniques\nand demonstrate how they affect both sequential and parallel performance. Apart\nfrom comparing the code complexity as well as the performance of the chosen\nparallel programming models, we investigate the impact of a parallelisation\ntechnique, task agglomeration in GPRM.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 15:59:53 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Tousimojarad", "Ashkan", ""], ["Vanderbauwhede", "Wim", ""], ["Cockshott", "W Paul", ""]]}, {"id": "1711.10201", "submitter": "Marco Peressotti", "authors": "Lu\\'is Cruz-Filipe, Fabrizio Montesi, Marco Peressotti", "title": "Communications in Choreographies, Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choreographic Programming is a paradigm for developing\ncorrect-by-construction concurrent programs, by writing high-level descriptions\nof the desired communications and then synthesising process implementations\nautomatically. So far, choreographic programming has been explored in the\nmonadic setting: interaction terms express point-to-point communications of a\nsingle value. However, real-world systems often rely on interactions of\npolyadic nature, where multiple values are communicated among two or more\nparties, like multicast, scatter-gather, and atomic exchanges. We introduce a\nnew model for choreographic programming equipped with a primitive for grouped\ninteractions that subsumes all the above scenarios. Intuitively, grouped\ninteractions can be thought of as being carried out as one single interaction.\nIn practice, they are implemented by processes that carry them out in a\nconcurrent fashion. After formalising the intuitive semantics of grouped\ninteractions, we prove that choreographic programs and their implementations\nare correct and deadlock-free by construction.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 09:37:02 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Cruz-Filipe", "Lu\u00eds", ""], ["Montesi", "Fabrizio", ""], ["Peressotti", "Marco", ""]]}, {"id": "1711.10301", "submitter": "Beniamino Accattoli", "authors": "Beniamino Accattoli", "title": "The Maximal MAM, a Reasonable Implementation of the Maximal Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note is about a reasonable abstract machine, called Maximal MAM,\nimplementing the maximal strategy of the lambda-calculus, that is, the strategy\nthat always produces a longest evaluation sequence.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 14:19:15 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Accattoli", "Beniamino", ""]]}, {"id": "1711.10413", "submitter": "Gheorghe-Teodor Bercea PhD", "authors": "Gheorghe-Teodor Bercea, Carlo Bertolli, Arpith C. Jacob, Alexandre\n  Eichenberger, Alexey Bataev, Georgios Rokos, Hyojin Sung, Tong Chen, Kevin\n  O'Brien", "title": "Implementing implicit OpenMP data sharing on GPUs", "comments": null, "journal-ref": null, "doi": "10.1145/3148173.3148189", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OpenMP is a shared memory programming model which supports the offloading of\ntarget regions to accelerators such as NVIDIA GPUs. The implementation in\nClang/LLVM aims to deliver a generic GPU compilation toolchain that supports\nboth the native CUDA C/C++ and the OpenMP device offloading models. There are\nsituations where the semantics of OpenMP and those of CUDA diverge. One such\nexample is the policy for implicitly handling local variables. In CUDA, local\nvariables are implicitly mapped to thread local memory and thus become private\nto a CUDA thread. In OpenMP, due to semantics that allow the nesting of regions\nexecuted by different numbers of threads, variables need to be implicitly\n\\emph{shared} among the threads of a contention group. In this paper we\nintroduce a re-design of the OpenMP device data sharing infrastructure that is\nresponsible for the implicit sharing of local variables in the Clang/LLVM\ntoolchain. We introduce a new data sharing infrastructure that lowers\nimplicitly shared variables to the shared memory of the GPU. We measure the\namount of shared memory used by our scheme in cases that involve scalar\nvariables and statically allocated arrays. The evaluation is carried out by\noffloading to K40 and P100 NVIDIA GPUs. For scalar variables the pressure on\nshared memory is relatively low, under 26\\% of shared memory utilization for\nthe K40, and does not negatively impact occupancy. The limiting occupancy\nfactor in that case is register pressure. The data sharing scheme offers the\nusers a simple memory model for controlling the implicit allocation of device\nshared memory.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 17:15:28 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Bercea", "Gheorghe-Teodor", ""], ["Bertolli", "Carlo", ""], ["Jacob", "Arpith C.", ""], ["Eichenberger", "Alexandre", ""], ["Bataev", "Alexey", ""], ["Rokos", "Georgios", ""], ["Sung", "Hyojin", ""], ["Chen", "Tong", ""], ["O'Brien", "Kevin", ""]]}, {"id": "1711.10604", "submitter": "Dustin Tran", "authors": "Joshua V. Dillon, Ian Langmore, Dustin Tran, Eugene Brevdo, Srinivas\n  Vasudevan, Dave Moore, Brian Patton, Alex Alemi, Matt Hoffman, Rif A. Saurous", "title": "TensorFlow Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The TensorFlow Distributions library implements a vision of probability\ntheory adapted to the modern deep-learning paradigm of end-to-end\ndifferentiable computation. Building on two basic abstractions, it offers\nflexible building blocks for probabilistic computation. Distributions provide\nfast, numerically stable methods for generating samples and computing\nstatistics, e.g., log density. Bijectors provide composable volume-tracking\ntransformations with automatic caching. Together these enable modular\nconstruction of high dimensional distributions and transformations not possible\nwith previous libraries (e.g., pixelCNNs, autoregressive flows, and reversible\nresidual networks). They are the workhorse behind deep probabilistic\nprogramming systems like Edward and empower fast black-box inference in\nprobabilistic models built on deep-network components. TensorFlow Distributions\nhas proven an important part of the TensorFlow toolkit within Google and in the\nbroader deep learning community.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 23:05:15 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Dillon", "Joshua V.", ""], ["Langmore", "Ian", ""], ["Tran", "Dustin", ""], ["Brevdo", "Eugene", ""], ["Vasudevan", "Srinivas", ""], ["Moore", "Dave", ""], ["Patton", "Brian", ""], ["Alemi", "Alex", ""], ["Hoffman", "Matt", ""], ["Saurous", "Rif A.", ""]]}, {"id": "1711.10708", "submitter": "EPTCS", "authors": "Massimo Bartoletti (University of Cagliari, IT), Laura Bocchi\n  (University of Kent, UK), Ludovic Henrio (CNRS, Sophia Antipolis, FR), Sophia\n  Knight (Uppsala University, SE)", "title": "Proceedings 10th Interaction and Concurrency Experience", "comments": null, "journal-ref": "EPTCS 261, 2017", "doi": "10.4204/EPTCS.261", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of ICE 2017, the 10th Interaction and\nConcurrency Experience, which was held in Neuch\\^atel, Switzerland on the 21st\nand 22nd of June 2017 as a satellite event of DisCoTec'17.\n  The ICE procedure for paper selection allows PC members to interact,\nanonymously, with authors. During the review phase, each submitted paper is\npublished on a discussion forum whose access is restricted to the authors and\nto all the PC members not declaring a conflict of interest. The PC members post\ncomments and questions that the authors reply to. For the first time, the 2017\nedition of ICE included a double blind reviewing of original research papers,\nin order to increase fairness and avoid bias in reviewing. The gender balance\nof accepted papers was also more even than in past years.\n  Each paper was reviewed by three PC members, and altogether five papers were\naccepted for publication (the workshop also featured five brief announcements\nwhich are not part of this volume). We were proud to host three invited talks,\nby Christian Cachin, Marieke Huismann, and Pawe{\\l} Sobocinski. The abstracts\nof these talks are included in this volume together with the regular papers.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 07:14:40 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Bartoletti", "Massimo", "", "University of Cagliari, IT"], ["Bocchi", "Laura", "", "University of Kent, UK"], ["Henrio", "Ludovic", "", "CNRS, Sophia Antipolis, FR"], ["Knight", "Sophia", "", "Uppsala University, SE"]]}, {"id": "1711.10713", "submitter": "Lse Lse", "authors": "Damien Pollet (1), St\\'ephane Ducasse (1) ((1) RMOD)", "title": "A critical analysis of string APIs: The case of Pharo", "comments": "Science of Computer Programming, Elsevier, 2017", "journal-ref": null, "doi": "10.1016/j.scico.2017.11.005", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most programming languages, besides C, provide a native abstraction for\ncharacter strings, but string APIs vary widely in size, expressiveness, and\nsubjective convenience across languages. In Pharo, while at first glance the\nAPI of the String class seems rich, it often feels cumbersome in practice; to\nimprove its usability, we faced the challenge of assessing its design. However,\nwe found hardly any guideline about design forces and how they structure the\ndesign space, and no comprehensive analysis of the expected string operations\nand their different variations. In this article, we first analyse the Pharo 4\nString library, then contrast it with its Haskell, Java, Python, Ruby, and Rust\ncounterparts. We harvest criteria to describe a string API, and reflect on\nfeatures and design tensions. This analysis should help language designers in\nunderstanding the design space of strings, and will serve as a basis for a\nfuture redesign of the string library in Pharo.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 07:48:12 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Pollet", "Damien", "", "RMOD"], ["Ducasse", "St\u00e9phane", "", "RMOD"]]}, {"id": "1711.10860", "submitter": "Tom Hirschowitz", "authors": "Clovis Eberhart and Tom Hirschowitz", "title": "What's in a game? A theory of game models", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game semantics is a rich and successful class of denotational models for\nprogramming languages. Most game models feature a rather intuitive setup, yet\nsurprisingly difficult proofs of such basic results as associativity of\ncomposition of strategies. We set out to unify these models into a basic\nabstract framework for game semantics, game settings. Our main contribution is\nthe generic construction, for any game setting, of a category of games and\nstrategies. Furthermore, we extend the framework to deal with innocence, and\nprove that innocent strategies form a subcategory. We finally show that our\nconstructions cover many concrete cases, mainly among the early models and the\nvery recent sheaf-based ones.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 14:07:56 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Eberhart", "Clovis", ""], ["Hirschowitz", "Tom", ""]]}, {"id": "1711.11211", "submitter": "EPTCS", "authors": "Lu\\'is Cruz-Filipe (University of Southern Denmark), Fabrizio Montesi\n  (University of Southern Denmark)", "title": "On Asynchrony and Choreographies", "comments": "In Proceedings ICE 2017, arXiv:1711.10708", "journal-ref": "EPTCS 261, 2017, pp. 76-90", "doi": "10.4204/EPTCS.261.8", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choreographic Programming is a paradigm for the development of concurrent\nsoftware, where deadlocks are prevented syntactically. However, choreography\nlanguages are typically synchronous, whereas many real-world systems have\nasynchronous communications. Previous attempts at enriching choreographies with\nasynchrony rely on ad-hoc constructions, whose adequacy is only argued\ninformally. In this work, we formalise the properties that an asynchronous\nsemantics for choreographies should have: messages can be sent without the\nintended receiver being ready, and all sent messages are eventually received.\nWe explore how out-of-order execution, used in choreographies for modelling\nconcurrency, can be exploited to endow choreographies with an asynchronous\nsemantics. Our approach satisfies the properties we identified. We show how our\ndevelopment yields a pleasant correspondence with FIFO-based asynchronous\nmessaging, modelled in a process calculus, and discuss how it can be adopted in\nmore complex choreography models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 03:46:38 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Cruz-Filipe", "Lu\u00eds", "", "University of Southern Denmark"], ["Montesi", "Fabrizio", "", "University of Southern Denmark"]]}, {"id": "1711.11396", "submitter": "Ajay Brahmakshatriya", "authors": "Ajay Brahmakshatriya, Piyus Kedia, Derrick Paul McKee, Pratik Bhatu,\n  Deepak Garg, Akash Lal, Aseem Rastogi", "title": "CONFLLVM: A Compiler for Enforcing Data Confidentiality in Low-Level\n  Code", "comments": "Technical report for CONFLLVM: A Compiler for Enforcing Data\n  Confidentiality in Low-Level Code, appearing at EuroSys 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an instrumenting compiler for enforcing data confidentiality in\nlow-level applications (e.g. those written in C) in the presence of an active\nadversary. In our approach, the programmer marks secret data by writing\nlightweight annotations on top-level definitions in the source code. The\ncompiler then uses a static flow analysis coupled with efficient runtime\ninstrumentation, a custom memory layout, and custom control-flow integrity\nchecks to prevent data leaks even in the presence of low-level attacks. We have\nimplemented our scheme as part of the LLVM compiler. We evaluate it on the SPEC\nmicro-benchmarks for performance, and on larger, real-world applications\n(including OpenLDAP, which is around 300KLoC) for programmer overhead required\nto restructure the application when protecting the sensitive data such as\npasswords. We find that performance overheads introduced by our instrumentation\nare moderate (average 12% on SPEC), and the programmer effort to port OpenLDAP\nis only about 160 LoC.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 13:53:38 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 05:42:53 GMT"}, {"version": "v3", "created": "Wed, 13 Mar 2019 18:20:32 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Brahmakshatriya", "Ajay", ""], ["Kedia", "Piyus", ""], ["McKee", "Derrick Paul", ""], ["Bhatu", "Pratik", ""], ["Garg", "Deepak", ""], ["Lal", "Akash", ""], ["Rastogi", "Aseem", ""]]}, {"id": "1711.11438", "submitter": "EPTCS", "authors": "Rajeev Alur (1), Dana Fisman (2), Rishabh Singh (3), Armando\n  Solar-Lezama (4) ((1) University of Pennsylvania, (2) Ben-Gurion University,\n  (3) Microsoft Research, Redmond, (4) Massachusetts Institute of Technology)", "title": "SyGuS-Comp 2017: Results and Analysis", "comments": "In Proceedings SYNT 2017, arXiv:1711.10224. arXiv admin note: text\n  overlap with arXiv:1611.07627, arXiv:1602.01170", "journal-ref": "EPTCS 260, 2017, pp. 97-115", "doi": "10.4204/EPTCS.260.9", "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Syntax-Guided Synthesis (SyGuS) is the computational problem of finding an\nimplementation f that meets both a semantic constraint given by a logical\nformula phi in a background theory T, and a syntactic constraint given by a\ngrammar G, which specifies the allowed set of candidate implementations. Such a\nsynthesis problem can be formally defined in SyGuS-IF, a language that is built\non top of SMT-LIB.\n  The Syntax-Guided Synthesis Competition (SyGuS-Comp) is an effort to\nfacilitate, bring together and accelerate research and development of efficient\nsolvers for SyGuS by providing a platform for evaluating different synthesis\ntechniques on a comprehensive set of benchmarks. In this year's competition six\nnew solvers competed on over 1500 benchmarks. This paper presents and analyses\nthe results of SyGuS-Comp'17.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 01:31:10 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Alur", "Rajeev", ""], ["Fisman", "Dana", ""], ["Singh", "Rishabh", ""], ["Solar-Lezama", "Armando", ""]]}]