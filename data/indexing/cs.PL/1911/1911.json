[{"id": "1911.00268", "submitter": "Kazutaka Matsuda", "authors": "Kazutaka Matsuda", "title": "Modular Inference of Linear Types for Multiplicity-Annotated Arrows", "comments": "The full version of our paper to appear in ESOP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bernardy et al. [2018] proposed a linear type system $\\lambda^q_\\to$ as a\ncore type system of Linear Haskell. In the system, linearity is represented by\nannotated arrow types $A \\to_m B$, where $m$ denotes the multiplicity of the\nargument. Thanks to this representation, existing non-linear code typechecks as\nit is, and newly written linear code can be used with existing non-linear code\nin many cases. However, little is known about the type inference of\n$\\lambda^q_\\to$. Although the Linear Haskell implementation is equipped with\ntype inference, its algorithm has not been formalized, and the implementation\noften fails to infer principal types, especially for higher-order functions. In\nthis paper, based on OutsideIn(X) [Vytiniotis et al., 2011], we propose an\ninference system for a rank 1 qualified-typed variant of $\\lambda^q_\\to$, which\ninfers principal types. A technical challenge in this new setting is to deal\nwith ambiguous types inferred by naive qualified typing. We address this\nambiguity issue through quantifier elimination and demonstrate the\neffectiveness of the approach with examples.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 09:06:46 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 05:26:18 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Matsuda", "Kazutaka", ""]]}, {"id": "1911.00406", "submitter": "Mauricio Ayala-Rincon", "authors": "Ariane Alves Almeida and Mauricio Ayala-Rincon", "title": "Formalizing the Dependency Pair Criterion for Innermost Termination", "comments": "Paper accepted for presentation at SBMF 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Rewriting is a framework for reasoning about functional programming. The\ndependency pair criterion is a well-known mechanism to analyze termination of\nterm rewriting systems. Functional specifications with an operational semantics\nbased on evaluation are related, in the rewriting framework, to the innermost\nreduction relation. This paper presents a PVS formalization of the dependency\npair criterion for the innermost reduction relation: a term rewriting system is\ninnermost terminating if and only if it is terminating by the dependency pair\ncriterion. The paper also discusses the application of this criterion to check\ntermination of functional specifications.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 03:28:16 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Almeida", "Ariane Alves", ""], ["Ayala-Rincon", "Mauricio", ""]]}, {"id": "1911.00561", "submitter": "Hongfa Xue", "authors": "Hongfa Xue, Yongsheng Mei, Kailash Gogineni, Guru Venkataramani, Tian\n  Lan", "title": "Twin-Finder: Integrated Reasoning Engine for Pointer-related Code Clone\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting code clones is crucial in various software engineering tasks. In\nparticular, code clone detection can have significant uses in the context of\nanalyzing and fixing bugs in large scale applications. However, prior works,\nsuch as machine learning-based clone detection, may cause a considerable amount\nof false positives. In this paper, we propose Twin-Finder, a novel, closed-loop\napproach for pointer-related code clone detection that integrates machine\nlearning and symbolic execution techniques to achieve precision. Twin-Finder\nintroduces a clone verification mechanism to formally verify if two clone\nsamples are indeed clones and a feedback loop to automatically generated formal\nrules to tune machine learning algorithm and further reduce the false\npositives. Our experimental results show that Twin-Finder can swiftly identify\nup 9X more code clones comparing to a tree-based clone detector, Deckard and\nremove an average 91.69% false positives.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 19:20:33 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 18:53:11 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2020 21:02:48 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Xue", "Hongfa", ""], ["Mei", "Yongsheng", ""], ["Gogineni", "Kailash", ""], ["Venkataramani", "Guru", ""], ["Lan", "Tian", ""]]}, {"id": "1911.00583", "submitter": "Ravi Chugh", "authors": "Justin Lubin and Nick Collins and Cyrus Omar and Ravi Chugh", "title": "Program Sketching with Live Bidirectional Evaluation", "comments": "ICFP 2020 Paper + Supplementary Appendices", "journal-ref": null, "doi": "10.1145/3408991", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system called Smyth for program sketching in a typed functional\nlanguage whereby the concrete evaluation of ordinary assertions gives rise to\ninput-output examples, which are then used to guide the search to complete the\nholes. The key innovation, called live bidirectional evaluation, propagates\nexamples \"backward\" through partially evaluated sketches. Live bidirectional\nevaluation enables Smyth to (a) synthesize recursive functions without\ntrace-complete sets of examples and (b) specify and solve interdependent\nsynthesis goals. Eliminating the trace-completeness requirement resolves a\nsignificant limitation faced by prior synthesis techniques when given partial\nspecifications in the form of input-output examples.\n  To assess the practical implications of our techniques, we ran several\nexperiments on benchmarks used to evaluate Myth, a state-of-the-art\nexample-based synthesis tool. First, given expert examples (and no partial\nimplementations), we find that Smyth requires on average 66% of the number of\nexpert examples required by Myth. Second, we find that Smyth is robust to\nrandomly-generated examples, synthesizing many tasks with relatively few more\nrandom examples than those provided by an expert. Third, we create a suite of\nsmall sketching tasks by systematically employing a simple sketching strategy\nto the Myth benchmarks; we find that user-provided sketches in Smyth often\nfurther reduce the total specification burden (i.e. the combination of partial\nimplementations and examples). Lastly, we find that Leon and Synquid, two\nstate-of-the-art logic-based synthesis tools, fail to complete several tasks on\nwhich Smyth succeeds.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 20:34:00 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 22:35:44 GMT"}, {"version": "v3", "created": "Thu, 7 May 2020 20:23:09 GMT"}, {"version": "v4", "created": "Mon, 13 Jul 2020 02:11:21 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Lubin", "Justin", ""], ["Collins", "Nick", ""], ["Omar", "Cyrus", ""], ["Chugh", "Ravi", ""]]}, {"id": "1911.00705", "submitter": "Peter Thiemann", "authors": "Peter Thiemann (University of Freiburg, Germany), Vasco T. Vasconcelos\n  (University of Lisbon, Portugal)", "title": "Label-Dependent Session Types", "comments": "POPL 2020", "journal-ref": "Proc. ACM Program. Lang. 4, POPL, Article 67 (January 2020)", "doi": "10.1145/3371135", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session types have emerged as a typing discipline for communication\nprotocols. Existing calculi with session types come equipped with many\ndifferent primitives that combine communication with the introduction or\nelimination of the transmitted value.\n  We present a foundational session type calculus with a lightweight\noperational semantics. It fully decouples communication from the introduction\nand elimination of data and thus features a single communication reduction,\nwhich acts as a rendezvous between senders and receivers. We achieve this\ndecoupling by introducing label-dependent session types, a minimalist\nvalue-dependent session type system with subtyping. The system is sufficiently\npowerful to simulate existing functional session type systems. Compared to such\nsystems, label-dependent session types place fewer restrictions on the code. We\nfurther introduce primitive recursion over natural numbers at the type level,\nthus allowing to describe protocols whose behaviour depends on numbers\nexchanged in messages. An algorithmic type checking system is introduced and\nproved equivalent to its declarative counterpart. The new calculus showcases a\nnovel lightweight integration of dependent types and linear typing, with has\nuses beyond session type systems.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 12:39:54 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 18:20:33 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Thiemann", "Peter", "", "University of Freiburg, Germany"], ["Vasconcelos", "Vasco T.", "", "University of Lisbon, Portugal"]]}, {"id": "1911.00815", "submitter": "Eric Goodman", "authors": "Eric L. Goodman, Dirk Grunwald", "title": "A Streaming Analytics Language for Processing Cyber Data", "comments": "Machine Learning and Data Mining 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a domain-specific language called SAL(the Streaming Analytics\nLanguage) for processing data in a semi-streaming model. In particular we\nexamine the use case of processing netflow data in order to identify malicious\nactors within a network. Because of the large volume of data generated from\nnetworks, it is often only feasible to process the data with a single pass,\nutilizing a streaming (O(polylog n) space requirements) or semi-streaming\ncomputing model ( O(n polylog n) space requirements). Despite these\nconstraints, we are able to achieve an average of 0.87 for the AUC of the ROC\ncurve for a set of situations dealing with botnet detection. The implementation\nof an interpreter for SAL, which we call SAM (Streaming Analytics Machine),\nachieves scaling results that show improved throughput to 61 nodes (976 cores),\nwith an overall rate of 373,000 netflows per second or 32.2 billion per day.\nSAL provides a succinct way to describe common analyses that allow cyber\nanalysts to find data of interest, and SAM is a scalable interpreter of the\nlanguage.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 03:14:21 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Goodman", "Eric L.", ""], ["Grunwald", "Dirk", ""]]}, {"id": "1911.01077", "submitter": "J\\\"urgen Giesl", "authors": "Florian Frohn, Matthias Naaf, Marc Brockschmidt, J\\\"urgen Giesl", "title": "Inferring Lower Runtime Bounds for Integer Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique to infer lower bounds on the worst-case runtime\ncomplexity of integer programs, where in contrast to earlier work, our approach\nis not restricted to tail-recursion. Our technique constructs symbolic\nrepresentations of program executions using a framework for iterative,\nunder-approximating program simplification. The core of this simplification is\na method for (under-approximating) program acceleration based on recurrence\nsolving and a variation of ranking functions. Afterwards, we deduce asymptotic\nlower bounds from the resulting simplified programs using a special-purpose\ncalculus and an SMT encoding. We implemented our technique in our tool LoAT and\nshow that it infers non-trivial lower bounds for a large class of examples.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 09:03:49 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 13:51:49 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2020 16:19:28 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Frohn", "Florian", ""], ["Naaf", "Matthias", ""], ["Brockschmidt", "Marc", ""], ["Giesl", "J\u00fcrgen", ""]]}, {"id": "1911.02178", "submitter": "Emily Herbert", "authors": "Emily Herbert and Arjun Guha", "title": "A Language-based Serverless Function Accelerator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing is an approach to cloud computing that allows\nprogrammers to run serverless functions in response to external events.\nServerless functions are priced at sub-second granularity, support transparent\nelasticity, and relieve programmers from managing the operating system. Thus\nserverless functions allow programmers to focus on writing application code,\nand the cloud provider to manage computing resources globally. Unfortunately,\ntoday's serverless platforms exhibit high latency, because it is difficult to\nmaximize resource utilization while minimizing operating costs.\n  This paper presents serverless function acceleration, which is an approach\nthat transparently lowers the latency and resource utilization of a large class\nof serverless functions. We accomplish this using language-based sandboxing,\nwhereas existing serverless platforms employ more expensive operating system\nsandboxing technologies, such as containers and virtual machines. OS-based\nsandboxing is compatible with more programs than language-based techniques.\nHowever, instead of ruling out any programs, we use language-based sandboxing\nwhen possible, and OS-based sandboxing if necessary. Moreover, we seamlessly\ntransition between language-based and OS-based sandboxing by leveraging the\nfact that serverless functions must tolerate re-execution for fault tolerance.\nTherefore, when a serverless function attempts to perform an unsupported\noperation in the language-based sandbox, we can safely re-execute it in a\ncontainer. We use a new approach to trace compilation to build source-level,\ninterprocedural, execution trace trees for serverless functions written in\nJavaScript. We compile trace trees to a safe subset of Rust, validate the\ncompiler output, and link it to a runtime system. We evaluate these techniques\nin our implementation, which we call Containerless.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 03:20:16 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 19:33:16 GMT"}, {"version": "v3", "created": "Fri, 22 Nov 2019 23:07:16 GMT"}, {"version": "v4", "created": "Mon, 3 Aug 2020 21:44:54 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Herbert", "Emily", ""], ["Guha", "Arjun", ""]]}, {"id": "1911.02452", "submitter": "Alexander McCaskey", "authors": "Alexander J. McCaskey, Dmitry I. Lyakh, Eugene F. Dumitrescu, Sarah S.\n  Powers, Travis S. Humble", "title": "XACC: A System-Level Software Infrastructure for Heterogeneous\n  Quantum-Classical Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum programming techniques and software have advanced significantly over\nthe past five years, with a majority focusing on high-level language frameworks\ntargeting remote REST library APIs. As quantum computing architectures advance\nand become more widely available, lower-level, system software infrastructures\nwill be needed to enable tighter, co-processor programming and access models.\nHere we present XACC, a system-level software infrastructure for\nquantum-classical computing that promotes a service-oriented architecture to\nexpose interfaces for core quantum programming, compilation, and execution\ntasks. We detail XACC's interfaces, their interactions, and its implementation\nas a hardware-agnostic framework for both near-term and future\nquantum-classical architectures. We provide concrete examples demonstrating the\nutility of this framework with paradigmatic tasks. Our approach lays the\nfoundation for the development of compilers, associated runtimes, and low-level\nsystem tools tightly integrating quantum and classical workflows.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 16:02:35 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["McCaskey", "Alexander J.", ""], ["Lyakh", "Dmitry I.", ""], ["Dumitrescu", "Eugene F.", ""], ["Powers", "Sarah S.", ""], ["Humble", "Travis S.", ""]]}, {"id": "1911.02564", "submitter": "Bertrand Meyer", "authors": "Jean-Michel Bruel, Sophie Ebersold, Florian Galinier, Alexandr\n  Naumchev, Manuel Mazzara, Bertrand Meyer", "title": "The role of formalism in system requirements (full version)", "comments": "Fourth version (15 April 2020). This is the full version, including\n  some sections and 2 appendices not appearing in the short version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.FL cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A major determinant of the quality of software systems is the quality of\ntheir requirements, which should be both understandable and precise. Most\nrequirements are written in natural language, good for understandability but\nlacking in precision. To make requirements precise, researchers have for years\nadvocated the use of mathematics-based notations and methods, known as\n\"formal\". Many exist, differing in their style, scope and applicability. The\npresent survey discusses some of the main formal approaches and compares them\nto informal methods. The analysis uses a set of 9 complementary criteria, such\nas level of abstraction, tool availability, traceability support. It classifies\nthe approaches into five categories: general-purpose, natural-language,\ngraph/automata, other mathematical notations, seamless\n(programming-language-based). It presents approaches in all of these\ncategories, altogether 22 different ones, including for example SysML, Relax,\nEiffel, Event-B, Alloy. The review discusses a number of open questions,\nincluding seamlessness, the role of tools and education, and how to make\nindustrial applications benefit more from the contributions of formal\napproaches.\n  (This is the full version of the survey, including some sections and two\nappendices which, because of length restrictions, do not appear in the\nsubmitted version.)\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 14:40:16 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 14:03:04 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 07:35:39 GMT"}, {"version": "v4", "created": "Sun, 12 Apr 2020 09:26:44 GMT"}, {"version": "v5", "created": "Wed, 15 Apr 2020 17:02:44 GMT"}, {"version": "v6", "created": "Thu, 16 Apr 2020 12:42:18 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Bruel", "Jean-Michel", ""], ["Ebersold", "Sophie", ""], ["Galinier", "Florian", ""], ["Naumchev", "Alexandr", ""], ["Mazzara", "Manuel", ""], ["Meyer", "Bertrand", ""]]}, {"id": "1911.02624", "submitter": "Nathana\\\"el Fijalkow", "authors": "Judith Clymo, Haik Manukian, Nathana\\\"el Fijalkow, Adri\\`a Gasc\\'on,\n  Brooks Paige", "title": "Data Generation for Neural Programming by Example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming by example is the problem of synthesizing a program from a small\nset of input / output pairs. Recent works applying machine learning methods to\nthis task show promise, but are typically reliant on generating synthetic\nexamples for training. A particular challenge lies in generating meaningful\nsets of inputs and outputs, which well-characterize a given program and\naccurately demonstrate its behavior. Where examples used for testing are\ngenerated by the same method as training data then the performance of a model\nmay be partly reliant on this similarity. In this paper we introduce a novel\napproach using an SMT solver to synthesize inputs which cover a diverse set of\nbehaviors for a given program. We carry out a case study comparing this method\nto existing synthetic data generation procedures in the literature, and find\nthat data generated using our approach improves both the discriminatory power\nof example sets and the ability of trained machine learning models to\ngeneralize to unfamiliar data.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 20:57:03 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Clymo", "Judith", ""], ["Manukian", "Haik", ""], ["Fijalkow", "Nathana\u00ebl", ""], ["Gasc\u00f3n", "Adri\u00e0", ""], ["Paige", "Brooks", ""]]}, {"id": "1911.03262", "submitter": "Nuno Macedo", "authors": "Hugo Pacheco and Nuno Macedo", "title": "ROSY: An elegant language to teach the pure reactive nature of robot\n  programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotics is incredibly fun and is long recognized as a great way to teach\nprogramming, while drawing inspiring connections to other branches of\nengineering and science such as maths, physics or electronics. Although this\nsymbiotic relationship between robotics and programming is perceived as largely\nbeneficial, educational approaches often feel the need to hide the underlying\ncomplexity of the robotic system, but as a result fail to transmit the reactive\nessence of robot programming to the roboticists and programmers of the future.\nThis paper presents ROSY, a novel language for teaching novice programmers\nthrough robotics. Its functional style is both familiar with a high-school\nalgebra background and a materialization of the inherent reactive nature of\nrobotic programming. Working at a higher-level of abstraction also teaches\nvaluable design principles of decomposition of robotics software into\ncollections of interacting controllers. Despite its simplicity, ROSY is\ncompletely valid Haskell code compatible with the ROS ecosystem. We make a\nconvincing case for our language by demonstrating how non-trivial applications\ncan be expressed with ease and clarity, exposing its sound functional\nprogramming foundations, and developing a web-enabled robot programming\nenvironment.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 13:51:45 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Pacheco", "Hugo", ""], ["Macedo", "Nuno", ""]]}, {"id": "1911.03807", "submitter": "Suguman Bansal", "authors": "Suguman Bansal, Kedar S. Namjoshi, Yaniv Sa'ar", "title": "Synthesis of coordination programs from linear temporal logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.FL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a method for synthesizing a reactive program which\ncoordinates the actions of a group of other reactive programs, so that the\ncombined system satisfies a temporal specification of its desired long-term\nbehavior. Traditionally, reactive synthesis has been applied to the\nconstruction of a stateful hardware circuit. This work is motivated by\napplications to other domains, such as the IoT (the Internet of Things) and\nrobotics, where it is necessary to coordinate the actions of multiple sensors,\ndevices, and robots. The mathematical model represents such entities as\nindividual processes in Hoare's CSP model. Given a network of interacting\nentities, called an \\emph{environment}, and a temporal specification of\nlong-term behavior, the synthesis method constructs a \\emph{coordinator}\nprocess (if one exists) that guides the actions of the environment entities so\nthat the combined system is deadlock-free and satisfies the given\nspecification. The main technical challenge is that a coordinator may have only\n\\emph{partial knowledge} of the environment state, due to non-determinism\nwithin the environment, and environment actions that are hidden from the\ncoordinator. This is the first method to handle both sources of partial\nknowledge, and to do so for arbitrary linear temporal logic specifications. It\nis shown that the coordination synthesis problem is \\PSPACE-hard in the size of\nthe environment. A prototype implementation is able to synthesize compact\nsolutions for a number of coordination problems.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 00:07:54 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Bansal", "Suguman", ""], ["Namjoshi", "Kedar S.", ""], ["Sa'ar", "Yaniv", ""]]}, {"id": "1911.03926", "submitter": "Aditya Srinivasan", "authors": "Aditya Srinivasan, Andrew D. Hilton", "title": "Gemini: A Functional Programming Language for Hardware Description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Gemini, a functional programming language for hardware\ndescription that provides features such as parametric polymorphism, recursive\ndatatypes, higher-order functions, and type inference for higher expressivity\ncompared to modern hardware description languages. Gemini demonstrates the\ntheory and implementation of novel type-theoretical concepts through its unique\ntype system consisting of multiple atomic kinds and dependent types, which\nallows the language to model both software and hardware constructs safely and\nperform type inference through multi-staged compilation. The primary technical\nresults of this paper include formalizations of the Gemini grammar, typing\nrules, and evaluation rules, a proof of safety of Gemini's type system, and a\nprototype implementation of the compiler's semantic analysis phase.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 13:19:02 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Srinivasan", "Aditya", ""], ["Hilton", "Andrew D.", ""]]}, {"id": "1911.04026", "submitter": "Daniel Leivant", "authors": "Daniel Leivant", "title": "A generic imperative language for polynomial time", "comments": "18 pages, submitted to a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.PL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ramification method in Implicit Computational Complexity has been\nassociated with functional programming, but adapting it to generic imperative\nprogramming is highly desirable, given the wider algorithmic applicability of\nimperative programming. We introduce a new approach to ramification which,\namong other benefits, adapts readily to fully general imperative programming.\nThe novelty is in ramifying finite second-order objects, namely finite\nstructures, rather than ramifying elements of free algebras. In so doing we\nbridge between Implicit Complexity's type theoretic characterizations of\nfeasibility, and the data-flow approach of Static Analysis.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 01:15:07 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 02:10:18 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Leivant", "Daniel", ""]]}, {"id": "1911.04091", "submitter": "Zheng Guo", "authors": "Zheng Guo, Michael James, David Justo, Jiaxiao Zhou, Ziteng Wang,\n  Ranjit Jhala and Nadia Polikarpova", "title": "Program Synthesis by Type-Guided Abstraction Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of type-directed component based synthesis where,\ngiven a set of (typed) components and a query type, the goal is to synthesize a\nterm that inhabits the query. Classical approaches based on proof search in\nintuitionistic logics do not scale up to the standard libraries of modern\nlanguages, which span hundreds or thousands of components. Recent graph\nreachability based methods proposed for languages like Java do scale, but only\napply to components over monomorphic data and functions: polymorphic data and\nfunctions infinitely explode the size of the graph that must be searched,\nrendering synthesis intractable. We introduce type-guided abstraction\nrefinement (TYGAR), a new approach for scalable type-directed synthesis over\npolymorphic datatypes and components. Our key insight is that we can overcome\nthe explosion by building a graph over abstract types which represent a\npotentially unbounded set of concrete types. We show how to use graph\nreachability to search for candidate terms over abstract types, and introduce a\nnew algorithm that uses proofs of untypeability of ill-typed candidates to\niteratively refine the abstraction until a well-typed result is found.\n  We have implemented TYGAR in H+, a tool that takes as input a set of Haskell\nlibraries and a query type, and returns a Haskell term that uses functions from\nthe provided libraries to implement the query type. We have evaluated H+ on a\nset of 44 queries using a set of popular Haskell libraries with a total of 291\ncomponents. Our results demonstrate that H+ returns an interesting solution\nwithin the first five results for 33 out of 44 queries. Moreover, TYGAR allows\nH+ to rapidly return well-typed terms, with the median time to first solution\nof just 1.4 seconds.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 05:52:01 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Guo", "Zheng", ""], ["James", "Michael", ""], ["Justo", "David", ""], ["Zhou", "Jiaxiao", ""], ["Wang", "Ziteng", ""], ["Jhala", "Ranjit", ""], ["Polikarpova", "Nadia", ""]]}, {"id": "1911.04422", "submitter": "Mat\\'u\\v{s} Sul\\'ir", "authors": "Mat\\'u\\v{s} Sul\\'ir, J\\'an Juh\\'ar", "title": "Draw This Object: A Study of Debugging Representations", "comments": null, "journal-ref": "Companion of the 3rd International Conference on Art, Science, and\n  Engineering of Programming (Programming '19), ACM, 2019", "doi": "10.1145/3328433.3328454", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain-specific debugging visualizations try to provide a view of a runtime\nobject tailored to a specific domain and highlighting its important properties.\nThe research in this area has focused mainly on the technical aspects of the\ncreation of such views so far. However, we still lack answers to questions such\nas what properties of objects are considered important for these\nvisualizations, whether all objects have an appropriate domain-specific view,\nor what clues could help us to construct these views fully automatically. In\nthis paper, we describe an exploratory study where the participants were asked\nto inspect runtime states of objects displayed in a traditional debugger and\ndraw ideal domain-specific views of these objects on paper. We describe\ninteresting observations and findings obtained during this study and a\npreliminary taxonomy of these visualizations.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 17:55:36 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Sul\u00edr", "Mat\u00fa\u0161", ""], ["Juh\u00e1r", "J\u00e1n", ""]]}, {"id": "1911.04523", "submitter": "Gordon Plotkin", "authors": "Martin Abadi, Gordon D. Plotkin", "title": "A Simple Differentiable Programming Language", "comments": "In POPL2020", "journal-ref": null, "doi": "10.1145/3371106", "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic differentiation plays a prominent role in scientific computing and\nin modern machine learning, often in the context of powerful programming\nsystems. The relation of the various embodiments of automatic differentiation\nto the mathematical notion of derivative is not always entirely\nclear---discrepancies can arise, sometimes inadvertently. In order to study\nautomatic differentiation in such programming contexts, we define a small but\nexpressive programming language that includes a construct for reverse-mode\ndifferentiation. We give operational and denotational semantics for this\nlanguage. The operational semantics employs popular implementation techniques,\nwhile the denotational semantics employs notions of differentiation familiar\nfrom real analysis. We establish that these semantics coincide.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 19:14:15 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 23:02:37 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2019 15:58:58 GMT"}, {"version": "v4", "created": "Sat, 1 Feb 2020 19:42:56 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Abadi", "Martin", ""], ["Plotkin", "Gordon D.", ""]]}, {"id": "1911.04560", "submitter": "Raimil Cruz", "authors": "Raimil Cruz and \\'Eric Tanter", "title": "Existential Types for Relaxed Noninterference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Information-flow security type systems ensure confidentiality by enforcing\nnoninterference: a program cannot leak private data to public channels.\nHowever, in practice, programs need to selectively declassify information about\nprivate data. Several approaches have provided a notion of relaxed\nnoninterference supporting selective and expressive declassification while\nretaining a formal security property. The labels-as-functions approach provides\nrelaxed noninterference by means of declassification policies expressed as\nfunctions. The labels-as-types approach expresses declassification policies\nusing type abstraction and faceted types, a pair of types representing the\nsecret and public facets of values. The original proposal of labels-as-types is\nformulated in an object-oriented setting where type abstraction is realized by\nsubtyping. The object-oriented approach however suffers from limitations due to\nits receiver-centric paradigm.\n  In this work, we consider an alternative approach to labels-as-types,\napplicable in non-object-oriented languages, which allows us to express\nadvanced declassification policies, such as extrinsic policies, based on a\ndifferent form of type abstraction: existential types. An existential type\nexposes abstract types and operations on these; we leverage this abstraction\nmechanism to express secrets that can be declassified using the provided\noperations. We formalize the approach in a core functional calculus with\nexistential types, define existential relaxed noninterference, and prove that\nwell-typed programs satisfy this form of type-based relaxed noninterference.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 20:53:47 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Cruz", "Raimil", ""], ["Tanter", "\u00c9ric", ""]]}, {"id": "1911.04588", "submitter": "G. A. Kavvos", "authors": "G. A. Kavvos and Edward Morehouse and Daniel R. Licata and Norman\n  Danner", "title": "Recurrence Extraction for Functional Programs through Call-by-Push-Value\n  (Extended Version)", "comments": "POPL 2020", "journal-ref": "Proc. ACM Program. Lang. 4, POPL, Article 15 (January 2020)", "doi": "10.1145/3371083", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The main way of analyzing the complexity of a program is that of extracting\nand solving a recurrence that expresses its running time in terms of the size\nof its input. We develop a method that automatically extracts such recurrences\nfrom the syntax of higher-order recursive functional programs. The resulting\nrecurrences, which are programs in a call-by-name language with recursion,\nexplicitly compute the running time in terms of the size of the input. In order\nto achieve this in a uniform way that covers both call-by-name and\ncall-by-value evaluation strategies, we use Call-by-Push-Value (CBPV) as an\nintermediate language. Finally, we use domain theory to develop a denotational\ncost semantics for the resulting recurrences.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 22:36:07 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Kavvos", "G. A.", ""], ["Morehouse", "Edward", ""], ["Licata", "Daniel R.", ""], ["Danner", "Norman", ""]]}, {"id": "1911.04631", "submitter": "Satoshi Egi", "authors": "Satoshi Egi", "title": "Scheme Macros for Non-linear Pattern Matching with Backtracking for\n  Non-free Data Types", "comments": "18 pages, Scheme and Functional Programming Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern matching is an important feature of programming languages for data\nabstraction. Many pattern-matching extensions have been proposed and\nimplemented for extending the range of data types to which pattern matching is\napplicable. Among them, the pattern-matching system proposed by Egi and\nNishiwaki features practical pattern matching for non-free data types by\nproviding an extensible non-linear pattern-matching facility with backtracking.\nHowever, they implemented their proposal only in an interpreter of the Egison\nprogramming language, and a method for compiling pattern-matching expressions\nof Egison was not discussed. This paper proposes a method for translating a\nprogram that contains pattern-matching expressions of Egison to a program that\ncontains only the existing syntax constructs of functional programming\nlanguages. This method is based on the three key ideas: (i) transformation of a\nmatcher to a function that takes a pattern and a target, and returns lists of\ntriples of a pattern, a matcher, and a target; (ii) compilation of match-all to\napplication of the map function; (iii) transformation of a value pattern to a\nfunction that takes an intermediate pattern-matching result and returns a\nvalue. This paper shows the proposed method works by showing Scheme macros that\nprovide the users the pattern-matching facility of Egison. This paper also\npresents benchmark results that show Egison pattern-matching embedded in Gauche\nScheme is faster than the original Egison interpreter.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 01:30:02 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Egi", "Satoshi", ""]]}, {"id": "1911.04710", "submitter": "Wishnu Prasetya", "authors": "I. S. W. B. Prasetya", "title": "Aplib: Tactical Programming of Intelligent Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents aplib, a Java library for programming intelligent agents,\nfeaturing BDI and multi agency, but adding on top of it a novel layer of\ntactical programming inspired by the domain of theorem proving. Aplib is also\nimplemented in such a way to provide the fluency of a Domain Specific Language\n(DSL). Compared to dedicated BDI agent programming languages such as JASON,\n2APL, or GOAL,aplib's embedded DSL approach does mean that \\aplib\\ programmers\nwill still be limited by Java syntax, but on other hand they get all the\nadvantages that Java programmers get: rich language features (object\norientation, static type checking, $\\lambda$-expression, libraries, etc), a\nwhole array of development tools, integration with other technologies, large\ncommunity, etc.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 07:39:07 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Prasetya", "I. S. W. B.", ""]]}, {"id": "1911.04732", "submitter": "Bas Spitters", "authors": "Jakob Botsch Nielsen, Bas Spitters", "title": "Smart Contract Interactions in Coq", "comments": null, "journal-ref": "1st Workshop on Formal Methods for Blockchains, 3rd Formal Methods\n  World Congress on October 11, 2019 in Porto, Portugal", "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model/executable specification of smart contract execution in\nCoq. Our formalization allows for inter-contract communication and generalizes\nexisting work by allowing modelling of both depth-first execution blockchains\n(like Ethereum) and breadth-first execution blockchains (like Tezos). We\nrepresent smart contracts programs in Coq's functional language Gallina,\nenabling easier reasoning about functional correctness of concrete contracts\nthan other approaches. In particular we develop a Congress contract in this\nstyle. This contract -- a simplified version of the infamous DAO -- is\ninteresting because of its very dynamic communication pattern with other\ncontracts. We give a high-level partial specification of the Congress's\nbehavior, related to reentrancy, and prove that the Congress satisfies it for\nall possible smart contract execution orders.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 08:23:19 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Nielsen", "Jakob Botsch", ""], ["Spitters", "Bas", ""]]}, {"id": "1911.05660", "submitter": "Nandita Vijaykumar", "authors": "Nandita Vijaykumar", "title": "Enhancing Programmability, Portability, and Performance with Rich\n  Cross-Layer Abstractions", "comments": "PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programmability, performance portability, and resource efficiency have\nemerged as critical challenges in harnessing complex and diverse architectures\ntoday to obtain high performance and energy efficiency. While there is abundant\nresearch, and thus significant improvements, at different levels of the stack\nthat address these very challenges, in this thesis, we observe that we are\nfundamentally limited by the interfaces and abstractions between the\napplication and the underlying system/hardware--specifically, the\nhardware-software interface. The existing narrow interfaces pose two critical\nchallenges. First, significant effort and expertise are required to write\nhigh-performance code to harness the full potential of today's diverse and\nsophisticated hardware. Second, as a hardware/system designer, architecting\nfaster and more efficient systems is challenging as the vast majority of the\nprogram's semantic content gets lost in translation with today's\nhardware-software interface. Moving towards the future, these challenges in\nprogrammability and efficiency will be even more intractable as we architect\nincreasingly heterogeneous and sophisticated systems.\n  This thesis makes the case for rich low-overhead cross-layer abstractions as\na highly effective means to address the above challenges. These abstractions\nare designed to communicate higher-level program information from the\napplication to the underlying system and hardware in a highly efficient manner,\nrequiring only minor additions to the existing interfaces. In doing so, they\nenable a rich space of hardware-software cooperative mechanisms to optimize for\nperformance. We propose 4 different approaches to designing richer abstractions\nbetween the application, system software, and hardware architecture in\ndifferent contexts to significantly improve programmability, portability, and\nperformance in CPUs and GPUs.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 18:22:11 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Vijaykumar", "Nandita", ""]]}, {"id": "1911.05839", "submitter": "Akshay Bhosale", "authors": "Akshay Bhosale and Rudolf Eigenmann", "title": "Compile-time Parallelization of Subscripted Subscript Patterns", "comments": "15 pages , 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of scientific applications are making use of irregular\ndata access patterns. An important class of such patterns involve\nsubscripted-subscripts, wherein an array value appears in the index expression\nof another array. Even though the information required to parallelize loops\nwith such patterns is often available in the program, present compiler\ntechniques fall short of analyzing that information. In this paper we present a\nstudy of subscripted-subscripts, the properties that define the subscript\narrays, and an algorithm based on symbolic range aggregation, that will help\nprove the presence of some of the properties of the subscript array in the\nprogram. We show that, in an important class of programs, the algorithm can\nboost the performance from essentially sequential execution to close to fully\nparallel.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 22:19:33 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Bhosale", "Akshay", ""], ["Eigenmann", "Rudolf", ""]]}, {"id": "1911.05900", "submitter": "Yasunori Ishihara", "authors": "Soichiro Hidaka, Yasunori Ishihara, Zachary G. Ives", "title": "Proceedings of the Third Workshop on Software Foundations for Data\n  Interoperability (SFDI2019+), October 28, 2019, Fukuoka, Japan", "comments": "Proceedings of the Third Workshop on Software Foundations for Data\n  Interoperability (SFDI2019+), October 28, 2019, Fukuoka, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the papers presented at the Third Workshop on Software\nFoundations for Data Interoperability (SFDI2019+) held on October 28, 2019, in\nFukuoka, co-located with the 11th Asia-Pasific Symposium on Internetware\n(Internetware 2019). One regular paper and six short papers have been accepted\nfor presentation, each of which has its unique ideas and/or interesting results\non interoperability of autonomic distributed data. Moreover, SFDI2019+ featured\ntwo keynote talks by Hiroyuki Seki (Nagoya University) and Zinovy Diskin\n(McMaster University), which introduce concepts and directions novel to the\nseries of SFDI workshops so far.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 02:32:16 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Hidaka", "Soichiro", ""], ["Ishihara", "Yasunori", ""], ["Ives", "Zachary G.", ""]]}, {"id": "1911.06153", "submitter": "Ningning Xie", "authors": "Ningning Xie, Richard A. Eisenberg, Bruno C. d. S. Oliveira", "title": "Kind Inference for Datatypes: Technical Supplement", "comments": "Technical supplement for POPL2020 paper Kind Inference for Datatypes", "journal-ref": null, "doi": "10.1145/3371121", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, languages like Haskell have seen a dramatic surge of new\nfeatures that significantly extends the expressive power of their type systems.\nWith these features, the challenge of kind inference for datatype declarations\nhas presented itself and become a worthy research problem on its own.\n  This paper studies kind inference for datatypes. Inspired by previous\nresearch on type-inference, we offer declarative specifications for what\ndatatype declarations should be accepted, both for Haskell98 and for a more\nadvanced system we call PolyKinds, based on the extensions in modern Haskell,\nincluding a limited form of dependent types. We believe these formulations to\nbe novel and without precedent, even for Haskell98. These specifications are\ncomplemented with implementable algorithmic versions. We study soundness,\ncompleteness and the existence of principal kinds in these systems, proving the\nproperties where they hold. This work can serve as a guide both to language\ndesigners who wish to formalize their datatype declarations and also to\nimplementors keen to have principled inference of principal types.\n  This technical supplement to Kind Inference for Datatypes serves to expand\nupon the text in the main paper. It contains detailed typing rules, proofs, and\nconnections to the Glasgow Haskell Compiler (GHC).\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 02:26:22 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Xie", "Ningning", ""], ["Eisenberg", "Richard A.", ""], ["Oliveira", "Bruno C. d. S.", ""]]}, {"id": "1911.06391", "submitter": "Benedikt Ahrens", "authors": "Benedikt Ahrens and Andr\\'e Hirschowitz and Ambroise Lafont and Marco\n  Maggesi", "title": "Reduction Monads and Their Signatures", "comments": "POPL 2020", "journal-ref": "Proc. ACM Program. Lang. 4, POPL, Article 31 (January 2020)", "doi": "10.1145/3371099", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study 'reduction monads', which are essentially the same as\nmonads relative to the free functor from sets into multigraphs. Reduction\nmonads account for two aspects of the lambda calculus: on the one hand, in the\nmonadic viewpoint, the lambda calculus is an object equipped with a\nwell-behaved substitution; on the other hand, in the graphical viewpoint, it is\nan oriented multigraph whose vertices are terms and whose edges witness the\nreductions between two terms.\n  We study presentations of reduction monads. To this end, we propose a notion\nof 'reduction signature'. As usual, such a signature plays the role of a\nvirtual presentation, and specifies arities for generating\noperations---possibly subject to equations---together with arities for\ngenerating reduction rules. For each such signature, we define a category of\nmodels; any model is, in particular, a reduction monad. If the initial object\nof this category of models exists, we call it the 'reduction monad presented\n(or specified) by the given reduction signature'.\n  Our main result identifies a class of reduction signatures which specify a\nreduction monad in the above sense. We show in the examples that our approach\ncovers several standard variants of the lambda calculus.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 21:38:27 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Ahrens", "Benedikt", ""], ["Hirschowitz", "Andr\u00e9", ""], ["Lafont", "Ambroise", ""], ["Maggesi", "Marco", ""]]}, {"id": "1911.06567", "submitter": "Evgenii Moiseenko", "authors": "Evgenii Moiseenko, Anton Podkopaev, Ori Lahav, Orestis Melkonian and\n  Viktor Vafeiadis", "title": "Reconciling Event Structures with Modern Multiprocessors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Weakestmo is a recently proposed memory consistency model that uses event\nstructures to resolve the infamous \"out-of-thin-air\" problem. Although it has\nbeen shown to have important benefits over other memory models, its established\ncompilation schemes are suboptimal in that they add more fences than necessary.\nIn this paper, we prove the correctness in Coq of the intended compilation\nschemes for Weakestmo to a range of hardware memory models (x86, POWER, ARMv7,\nARMv8, RISC-V). Our proof is the first that establishes correctness of\ncompilation of an event-structure-based model that forbids \"thin-air\"\nbehaviors, as well as the first mechanized compilation proof of a weak memory\nmodel supporting sequentially consistent accesses to such a range of hardware\nplatforms. Our compilation proof goes via the recent Intermediate Memory Model\n(IMM), which we suitably extend with sequentially consistent accesses.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 11:09:19 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 16:35:34 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Moiseenko", "Evgenii", ""], ["Podkopaev", "Anton", ""], ["Lahav", "Ori", ""], ["Melkonian", "Orestis", ""], ["Vafeiadis", "Viktor", ""]]}, {"id": "1911.07260", "submitter": "Yunming Zhang", "authors": "Yunming Zhang, Ajay Brahmakshatriya, Xinyi Chen, Laxman Dhulipala,\n  Shoaib Kamil, Saman Amarasinghe, Julian Shun", "title": "Optimizing Ordered Graph Algorithms with GraphIt", "comments": null, "journal-ref": "CGO 2020", "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many graph problems can be solved using ordered parallel graph algorithms\nthat achieve significant speedup over their unordered counterparts by reducing\nredundant work. This paper introduces a new priority-based extension to\nGraphIt, a domain-specific language for writing graph applications, to simplify\nwriting high-performance parallel ordered graph algorithms. The extension\nenables vertices to be processed in a dynamic order while hiding low-level\nimplementation details from the user. We extend the compiler with new program\nanalyses, transformations, and code generation to produce fast implementations\nof ordered parallel graph algorithms. We also introduce bucket fusion, a new\nperformance optimization that fuses together different rounds of ordered\nalgorithms to reduce synchronization overhead, resulting in\n$1.2\\times$--3$\\times$ speedup over the fastest existing ordered algorithm\nimplementations on road networks with large diameters. With the extension,\nGraphIt achieves up to 3$\\times$ speedup on six ordered graph algorithms over\nstate-of-the-art frameworks and hand-optimized implementations (Julienne,\nGalois, and GAPBS) that support ordered algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 15:51:02 GMT"}, {"version": "v2", "created": "Sun, 26 Jan 2020 23:37:14 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Zhang", "Yunming", ""], ["Brahmakshatriya", "Ajay", ""], ["Chen", "Xinyi", ""], ["Dhulipala", "Laxman", ""], ["Kamil", "Shoaib", ""], ["Amarasinghe", "Saman", ""], ["Shun", "Julian", ""]]}, {"id": "1911.07393", "submitter": "Vladimir Filkov", "authors": "Baishakhi Ray, Prem Devanbu, Vladimir Filkov", "title": "Rebuttal to Berger et al., TOPLAS 2019", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Berger et al., published in TOPLAS 2019, is a critique of our 2014 FSE\nconference abstract and its archival version, the 2017 CACM paper: A\nLarge-Scale Study of Programming Languages and Code Quality in Github. In their\npaper Berger et al. make academic claims about the veracity of our work. Here,\nwe respond to their technical and scientific critiques aimed at our work,\nattempting to stick with scientific discourse. We find that Berger et al.\nlargely replicated our results, and agree with us in their conclusion: that the\neffects (in a statistical sense) found in the data are small, and should be\ntaken with caution, and that it is possible that an absence of effect is the\ncorrect interpretation. Thus, our CACM paper's conclusions still hold, even\nmore so now that they have been reproduced, and our paper is eminently citable.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 01:22:58 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Ray", "Baishakhi", ""], ["Devanbu", "Prem", ""], ["Filkov", "Vladimir", ""]]}, {"id": "1911.07567", "submitter": "Gustavo Grieco Dr.", "authors": "Alex Groce and Josselin Feist and Gustavo Grieco and Michael Colburn", "title": "What are the Actual Flaws in Important Smart Contracts (and How Can We\n  Find Them)?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem in smart contract security is understanding the\nlikelihood and criticality of discovered, or potential, weaknesses in\ncontracts. In this paper we provide a summary of Ethereum smart contract audits\nperformed for 23 professional stakeholders, avoiding the common problem of\nreporting issues mostly prevalent in low-quality contracts. These audits were\nperformed at a leading company in blockchain security, using both open-source\nand proprietary tools, as well as human code analysis performed by professional\nsecurity engineers. We categorize 246 individual defects, making it possible to\ncompare the severity and frequency of different vulnerability types, compare\nsmart contract and non-smart contract flaws, and to estimate the efficacy of\nautomated vulnerability detection approaches.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 11:57:22 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 16:53:38 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Groce", "Alex", ""], ["Feist", "Josselin", ""], ["Grieco", "Gustavo", ""], ["Colburn", "Michael", ""]]}, {"id": "1911.07707", "submitter": "Rahul Gopinath", "authors": "Rahul Gopinath, Andreas Zeller", "title": "Building Fast Fuzzers", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzing is one of the key techniques for evaluating the robustness of\nprograms against attacks. Fuzzing has to be effective in producing inputs that\ncover functionality and find vulnerabilities. But it also has to be efficient\nin producing such inputs quickly. Random fuzzers are very efficient, as they\ncan quickly generate random inputs; but they are not very effective, as the\nlarge majority of inputs generated is syntactically invalid. Grammar-based\nfuzzers make use of a grammar (or another model for the input language) to\nproduce syntactically correct inputs, and thus can quickly cover input space\nand associated functionality. Existing grammar-based fuzzers are surprisingly\ninefficient, though: Even the fastest grammar fuzzer Dharma still produces\ninputs about a thousand times slower than the fastest random fuzzer. So far,\none can have an effective or an efficient fuzzer, but not both.\n  In this paper, we describe how to build fast grammar fuzzers from the ground\nup, treating the problem of fuzzing from a programming language implementation\nperspective. Starting with a Python textbook approach, we adopt and adapt\noptimization techniques from functional programming and virtual machine\nimplementation techniques together with other novel domain-specific\noptimizations in a step-by-step fashion. In our F1 prototype fuzzer, these\nimprove production speed by a factor of 100--300 over the fastest grammar\nfuzzer Dharma. As F1 is even 5--8 times faster than a lexical random fuzzer, we\ncan find bugs faster and test with much larger valid inputs than previously\npossible.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 15:35:16 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Gopinath", "Rahul", ""], ["Zeller", "Andreas", ""]]}, {"id": "1911.08033", "submitter": "Wolfgang Jeltsch", "authors": "Wolfgang Jeltsch", "title": "A Process Calculus for Formally Verifying Blockchain Consensus Protocols", "comments": "Part of DECLARE 19 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchains are becoming increasingly relevant in a variety of fields, such\nas finance, logistics, and real estate. The fundamental task of a blockchain\nsystem is to establish data consistency among distributed agents in an open\nnetwork. Blockchain consensus protocols are central for performing this task.\n  Since consensus protocols play such a crucial role in blockchain technology,\nseveral projects are underway that apply formal methods to these protocols. One\nsuch project is carried out by a team of the Formal Methods Group at IOHK. This\nproject, in which the author is involved, aims at a formally verified\nimplementation of the Ouroboros family of consensus protocols, the backbone of\nthe Cardano blockchain. The first outcome of our project is the\n$\\natural$-calculus (pronounced \"natural calculus\"), a general-purpose process\ncalculus that serves as our implementation language. The $\\natural$-calculus is\na domain-specific language embedded in a functional host language using\nhigher-order abstract syntax.\n  This paper will be a ramble through the $\\natural$-calculus. First we will\nlook at its language and its operational semantics. The latter is unique in\nthat it uses a stack of two labeled transition systems to treat phenomena like\ndata transfer and the opening and closing of channel scope in a modular\nfashion. The presence of multiple transition systems calls for a generic\ntreatment of derived concurrency concepts. We will see how such a treatment can\nbe achieved by capturing notions like scope opening and silent transitions\nabstractly using axiomatically defined algebraic structures based on functors\nand monads.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 01:34:03 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Jeltsch", "Wolfgang", ""]]}, {"id": "1911.08174", "submitter": "Thorsten Wissmann", "authors": "Andreas Abel and Thierry Coquand", "title": "Failure of Normalization in Impredicative Type Theory with\n  Proof-Irrelevant Propositional Equality", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 2 (June 30,\n  2020) lmcs:6606", "doi": "10.23638/LMCS-16(2:14)2020", "report-no": null, "categories": "cs.LO cs.PL math.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Normalization fails in type theory with an impredicative universe of\npropositions and a proof-irrelevant propositional equality. The counterexample\nto normalization is adapted from Girard's counterexample against normalization\nof System F equipped with a decider for type equality. It refutes Werner's\nnormalization conjecture [LMCS 2008].\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 09:38:00 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2020 16:56:28 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2020 12:42:16 GMT"}, {"version": "v4", "created": "Sun, 28 Jun 2020 19:04:27 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Abel", "Andreas", ""], ["Coquand", "Thierry", ""]]}, {"id": "1911.08286", "submitter": "Sarah McDaid PhD", "authors": "Edward McDaid, Sarah McDaid", "title": "Zoea -- Composable Inductive Programming Without Limits", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic generation of software from some form of specification has been a\nlong standing goal of computer science research. To date successful results\nhave been reported for the production of relatively small programs. This paper\npresents Zoea which is a simple programming language that allows software to be\ngenerated from a specification format that closely resembles a set of automated\nfunctional tests. Zoea incorporates a number of advances that enable it to\ngenerate software that is large enough to have commercial value. Zoea also\nallows programs to be composed to form still larger programs. As a result Zoea\ncan be used to produce software of any size and complexity. An overview of the\ncore Zoea language is provided together with a high level description of the\nsymbolic AI based Zoea compiler.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 12:28:17 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["McDaid", "Edward", ""], ["McDaid", "Sarah", ""]]}, {"id": "1911.09421", "submitter": "Christos Psarras M.Sc.", "authors": "Christos Psarras, Henrik Barthels and Paolo Bientinesi", "title": "The Linear Algebra Mapping Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observe a disconnect between the developers and the end users of linear\nalgebra libraries. On the one hand, the numerical linear algebra and the\nhigh-performance communities invest significant effort in the development and\noptimization of highly sophisticated numerical kernels and libraries, aiming at\nthe maximum exploitation of both the properties of the input matrices, and the\narchitectural features of the target computing platform. On the other hand, end\nusers are progressively less likely to go through the error-prone and time\nconsuming process of directly using said libraries by writing their code in C\nor Fortran; instead, languages and libraries such as Matlab, Julia, Eigen and\nArmadillo, which offer a higher level of abstraction, are becoming more and\nmore popular. Users are given the opportunity to code matrix computations with\na syntax that closely resembles the mathematical description; it is then a\ncompiler or an interpreter that internally maps the input program to lower\nlevel kernels, as provided by libraries such as BLAS and LAPACK. Unfortunately,\nour experience suggests that in terms of performance, this translation is\ntypically vastly suboptimal.\n  In this paper, we first introduce the Linear Algebra Mapping Problem, and\nthen investigate how effectively a benchmark of test problems is solved by\npopular high-level programming languages. Specifically, we consider Matlab,\nOctave, Julia, R, Armadillo (C++), Eigen (C++), and NumPy (Python); the\nbenchmark is meant to test both standard compiler optimizations such as common\nsubexpression elimination and loop-invariant code motion, as well as linear\nalgebra specific optimizations such as optimal parenthesization of a matrix\nproduct and kernel selection for matrices with properties. The aim of this\nstudy is to give concrete guidelines for the development of languages and\nlibraries that support linear algebra computations.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 11:42:14 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Psarras", "Christos", ""], ["Barthels", "Henrik", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1911.09668", "submitter": "Chenglong Wang", "authors": "Chenglong Wang, Yu Feng, Rastislav Bodik, Alvin Cheung, Isil Dillig", "title": "Visualization by Example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While visualizations play a crucial role in gaining insights from data,\ngenerating useful visualizations from a complex dataset is far from an easy\ntask. Besides understanding the functionality provided by existing\nvisualization libraries, generating the desired visualization also requires\nreshaping and aggregating the underlying data as well as composing different\nvisual elements to achieve the intended visual narrative. This paper aims to\nsimplify visualization tasks by automatically synthesizing the required program\nfrom simple visual sketches provided by the user. Specifically, given an input\ndata set and a visual sketch that demonstrates how to visualize a very small\nsubset of this data, our technique automatically generates a program that can\nbe used to visualize the entire data set.\n  Automating visualization poses several challenges. First, because many\nvisualization tasks require data wrangling in addition to generating plots, we\nneed to decompose the end-to-end synthesis task into two separate sub-problems.\nSecond, because the intermediate specification that results from the\ndecomposition is necessarily imprecise, this makes the data wrangling task\nparticularly challenging in our context. In this paper, we address these\nproblems by developing a new compositional visualization-by-example technique\nthat (a) decomposes the end-to-end task into two different synthesis problems\nover different DSLs and (b) leverages bi-directional program analysis to deal\nwith the complexity that arises from having an imprecise intermediate\nspecification.\n  We implemented our visualization-by-example algorithm and evaluate it on 83\nvisualization tasks collected from on-line forums and tutorials. Viser can\nsolve 84% of these benchmarks within a 600 second time limit, and, for those\ntasks that can be solved, the desired visualization is among the top-5\ngenerated by Viser in 70% of the cases.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 18:55:35 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Wang", "Chenglong", ""], ["Feng", "Yu", ""], ["Bodik", "Rastislav", ""], ["Cheung", "Alvin", ""], ["Dillig", "Isil", ""]]}, {"id": "1911.09755", "submitter": "David Monniaux", "authors": "Hang Yu, David Monniaux (VERIMAG - IMAG)", "title": "An Efficient Parametric Linear Programming Solver and Application to\n  Polyhedral Projection", "comments": null, "journal-ref": "Static Analysis (SAS 2019), Oct 2019, Porto, Portugal. pp.203-224", "doi": "10.1007/978-3-030-32304-2_11", "report-no": null, "categories": "math.OC cs.CG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polyhedral projection is a main operation of the polyhedron abstract\ndomain.It can be computed via parametric linear programming (PLP), which is\nmore efficient than the classic Fourier-Motzkin elimination method.In prior\nwork, PLP was done in arbitrary precision rational arithmetic.In this paper, we\npresent an approach where most of the computation is performed in\nfloating-point arithmetic, then exact rational results are reconstructed.We\nalso propose a workaround for a difficulty that plagued previous attempts at\nusing PLP for computations on polyhedra: in general the linear programming\nproblems are degenerate, resulting in redundant computations and geometric\ndescriptions.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 16:17:14 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Yu", "Hang", "", "VERIMAG - IMAG"], ["Monniaux", "David", "", "VERIMAG - IMAG"]]}, {"id": "1911.10081", "submitter": "Taha Ceritli", "authors": "Taha Ceritli, Christopher K. I. Williams, James Geddes", "title": "ptype: Probabilistic Type Inference", "comments": null, "journal-ref": "Data Mining and Knowledge Discovery (2020)", "doi": "10.1007/s10618-020-00680-1", "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Type inference refers to the task of inferring the data type of a given\ncolumn of data. Current approaches often fail when data contains missing data\nand anomalies, which are found commonly in real-world data sets. In this paper,\nwe propose ptype, a probabilistic robust type inference method that allows us\nto detect such entries, and infer data types. We further show that the proposed\nmethod outperforms the existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 15:21:15 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 12:25:40 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Ceritli", "Taha", ""], ["Williams", "Christopher K. I.", ""], ["Geddes", "James", ""]]}, {"id": "1911.10353", "submitter": "Alexandr Naumchev", "authors": "Alexandr Naumchev", "title": "Seamless Object-Oriented Requirements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design by Contract enables seamless software development by unifying software\nrequirements with their implementations. In its pure form, however, Design by\nContract leaves some problems with contracts' expressiveness, verifiability,\nand reusability open. These problems significantly reduce practical\napplicability of seamless development. The present article introduces seamless\nobject-oriented requirements - a novel approach to seamless development that\nbuilds upon Design by Contract and now-available advanced program proving\ntools. The article explains and illustrates the new approach, concluding with a\nquantitative evaluation of the extent to which the approach fixes the problems\nof traditional contracts.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 12:10:34 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Naumchev", "Alexandr", ""]]}, {"id": "1911.10982", "submitter": "Philip Dexter", "authors": "Philip Dexter, Yu David Liu, Kenneth Chiu", "title": "Formal Foundations of Continuous Graph Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing need for online and iterative graph processing, software\nsystems that continuously process large-scale graphs become widely deployed.\nWith optimizations inherent as part of their design, these systems are complex,\nand have unique features beyond conventional graph processing. This paper\ndescribes CG Calculus, the first semantic foundation for continuous graph\nprocessing. The calculus captures the essential behavior of both the backend\ngraph processing engine and the frontend application, with a focus on two\nessential features: temporal locality optimization (TLO) and incremental\noperation processing (IOP). A key design insight is that the operations\ncontinuously applied to the graph can be captured by a semantics defined over\nthe operation stream flowing through the graph nodes. CG Calculus is a\nsystematic study on the correctness of building continuous graph processing\nsystems and applications. The most important result is result determinism:\ndespite significant non-deterministic executions introduced by TLO and IOP, the\nresults produced by CG Calculus are the same as conventional graph processing\nwithout TLO or IOP. The metatheory of CG Calculus is mechanized in Coq.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 15:30:37 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 15:02:14 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Dexter", "Philip", ""], ["Liu", "Yu David", ""], ["Chiu", "Kenneth", ""]]}, {"id": "1911.11184", "submitter": "Parisa Ataei", "authors": "Parisa Ataei, Qiaoran Li, Eric Walkingshaw, Arash Termehchy", "title": "Managing Variability in Relational Databases by VDBMS", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Variability inherently exists in databases in various contexts which creates\ndatabase variants. For example, variants of a database could have different\nschemas/content (database evolution problem), variants of a database could root\nfrom different sources (data integration problem), variants of a database could\nbe deployed differently for specific application domain (deploying a database\nfor different configurations of a software system), etc. Unfortunately, while\nthere are specific solutions to each of the problems arising in these contexts,\nthere is no general solution that accounts for variability in databases and\naddresses managing variability within a database. In this paper, we formally\ndefine variational databases (VDBs) and statically-typed variational relational\nalgebra (VRA) to query VDBs---both database and queries explicitly account for\nvariation. We also design and implement variational database management system\n(VDBMS) to run variational queries over a VDB effectively and efficiently. To\nassess this, we generate two VDBs from real-world databases in the context of\nsoftware development and database evolution with a set of experimental queries\nfor each.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 19:33:25 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Ataei", "Parisa", ""], ["Li", "Qiaoran", ""], ["Walkingshaw", "Eric", ""], ["Termehchy", "Arash", ""]]}, {"id": "1911.11376", "submitter": "Markus Knecht", "authors": "Markus Knecht", "title": "Mandala: A Smart Contract Programming Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart contracts on a blockchain behave precisely as specified by their code.\nA vulnerability in this code can lead to unexpected behaviour, which is hard to\nfix because a blockchain does not allow to change smart contract code after its\ndeployment. Such vulnerabilities have led to several incidents. In the\naftermath of such an event, a hard-fork between Ethereum and Ethereum classic\nwas the result. This thesis proposes to develop a new smart contract\nprogramming language with the primary focus on safety, auditability, and the\nintention to prevent as many of the known categories of vulnerabilities by\ndesign as possible. The programming language's code is validated during\ndeployment and afterwards isolated from other smart contracts running on the\nsame blockchain to enforce compile-time guarantees during runtime. The designed\nprogramming language does evaluate new concepts and paradigms rarely used in\nnon-smart contract environments for their potential benefit in a smart contract\nenvironment.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 07:33:22 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Knecht", "Markus", ""]]}, {"id": "1911.11728", "submitter": "Sahil Bhatia", "authors": "Sahil Bhatia, Saswat Padhi, Nagarajan Natarajan, Rahul Sharma, Prateek\n  Jain", "title": "On Scaling Data-Driven Loop Invariant Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated synthesis of inductive invariants is an important problem in\nsoftware verification. Once all the invariants have been specified, software\nverification reduces to checking of verification conditions. Although static\nanalyses to infer invariants have been studied for over forty years, recent\nyears have seen a flurry of data-driven invariant inference techniques which\nguess invariants from examples instead of analyzing program text. However,\nthese techniques have been demonstrated to scale only to programs with a small\nnumber of variables. In this paper, we study these scalability issues and\naddress them in our tool oasis that improves the scale of data-driven invariant\ninference and outperforms state-of-the-art systems on benchmarks from the\ninvariant inference track of the Syntax Guided Synthesis competition.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 18:05:46 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 17:34:27 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Bhatia", "Sahil", ""], ["Padhi", "Saswat", ""], ["Natarajan", "Nagarajan", ""], ["Sharma", "Rahul", ""], ["Jain", "Prateek", ""]]}, {"id": "1911.11824", "submitter": "Jacques Carette", "authors": "Jacques Carette and Brooks MacLachlan and W. Spencer Smith", "title": "GOOL: A Generic Object-Oriented Language (extended version)", "comments": "Long version of paper at PEPM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GOOL, a Generic Object-Oriented Language. It demonstrates that a\nlanguage, with the right abstractions, can capture the essence of\nobject-oriented programs. We show how GOOL programs can be used to generate\nhuman-readable, documented and idiomatic source code in multiple languages.\nMoreover, in GOOL, it is possible to express common programming idioms and\npatterns, from simple library-level functions, to simple tasks (command-line\narguments, list processing, printing), to more complex patterns, such as\nmethods with a mixture of input, output and in-out parameters, and finally\nDesign Patterns (such as Observer, State and Strategy). GOOL is an embedded DSL\nin Haskell that can generate code in Python, Java, C# and C++.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 20:40:30 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Carette", "Jacques", ""], ["MacLachlan", "Brooks", ""], ["Smith", "W. Spencer", ""]]}, {"id": "1911.11894", "submitter": "Emery Berger", "authors": "Emery D. Berger, Petr Maj, Olga Vitek, Jan Vitek", "title": "FSE/CACM Rebuttal$^2$: Correcting A Large-Scale Study of Programming\n  Languages and Code Quality in GitHub", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ray, Devanbu and Filkov issued a rebuttal of our TOPLAS paper \"On the Impact\nof Programming Languages on Code Quality: A Reproduction Study\". Our paper\nreproduced \"A Large-Scale Study of Programming Languages and Code Quality in\nGitHub\", which appeared at FSE 2014 and was subsequently republished as a CACM\nresearch highlight in 2017. This article is a rebuttal to that rebuttal.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 00:36:03 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Berger", "Emery D.", ""], ["Maj", "Petr", ""], ["Vitek", "Olga", ""], ["Vitek", "Jan", ""]]}, {"id": "1911.12555", "submitter": "Ao Li", "authors": "Ao Li, Jemin Andrew Choi, Fan Long", "title": "Securing Smart Contract On The Fly", "comments": null, "journal-ref": null, "doi": "10.1145/3385412.3385982", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Solythesis, a source to source Solidity compiler which takes a\nsmart contract code and a user specified invariant as the input and produces an\ninstrumented contract that rejects all transactions that violate the invariant.\nThe design of Solythesis is driven by our observation that the consensus\nprotocol and the storage layer are the primary and the secondary performance\nbottlenecks of Ethereum, respectively. Solythesis operates with our novel delta\nupdate and delta check techniques to minimize the overhead caused by the\ninstrumented storage access statements. Our experimental results validate our\nhypothesis that the overhead of runtime validation, which is often too\nexpensive for other domains, is in fact negligible for smart contracts. The CPU\noverhead of Solythesis is only 0.12% on average for our 23 benchmark contracts.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 06:52:31 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Li", "Ao", ""], ["Choi", "Jemin Andrew", ""], ["Long", "Fan", ""]]}, {"id": "1911.12557", "submitter": "Junyi Liu", "authors": "Junyi Liu, Li Zhou and Mingsheng Ying", "title": "Expected Runtime of Quantum Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building upon recent work on probabilistic programs, we formally define the\nnotion of expected runtime for quantum programs. A representation of the\nexpected runtimes of quantum programs is introduced with an interpretation as\nan observable in physics. A method for computing the expected runtimes of\nquantum programs in finite-dimensional state spaces is developed. Several\nexamples are provided as applications of this method; in particular, an open\nproblem of computing the expected runtime of quantum random walks is solved\nusing our method.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 06:59:26 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Liu", "Junyi", ""], ["Zhou", "Li", ""], ["Ying", "Mingsheng", ""]]}, {"id": "1911.12651", "submitter": "Andrew Habib", "authors": "Andrew Habib, Avraham Shinnar, Martin Hirzel, Michael Pradel", "title": "Type Safety with JSON Subschema", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  JSON is a popular data format used pervasively in web APIs, cloud computing,\nNoSQL databases, and increasingly also machine learning. JSON Schema is a\nlanguage for declaring the structure of valid JSON data. There are validators\nthat can decide whether a JSON document is valid with respect to a schema.\nUnfortunately, like all instance-based testing, these validators can only show\nthe presence and never the absence of a bug. This paper presents a\ncomplementary technique: JSON subschema checking, which can be used for static\ntype checking with JSON Schema. Deciding whether one schema is a subschema of\nanother is non-trivial because of the richness of the JSON Schema specification\nlanguage. Given a pair of schemas, our approach first canonicalizes and\nsimplifies both schemas, then decides the subschema question on the canonical\nforms, dispatching simpler subschema queries to type-specific checkers. We\napply an implementation of our subschema checking algorithm to 8,548 pairs of\nreal-world JSON schemas from different domains, demonstrating that it can\ndecide the subschema question for most schema pairs and is always correct for\nschema pairs that it can decide. We hope that our work will bring more static\nguarantees to hard-to-debug domains, such as cloud computing and artificial\nintelligence.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 11:33:07 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 12:38:53 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Habib", "Andrew", ""], ["Shinnar", "Avraham", ""], ["Hirzel", "Martin", ""], ["Pradel", "Michael", ""]]}, {"id": "1911.12855", "submitter": "Gushu Li", "authors": "Gushu Li, Li Zhou, Nengkun Yu, Yufei Ding, Mingsheng Ying, Yuan Xie", "title": "Proq: Projection-based Runtime Assertions for Debugging on a Quantum\n  Computer", "comments": "A major revision, in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL cs.ET quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose Proq, a runtime assertion scheme for testing and\ndebugging quantum programs on a quantum computer. The predicates in Proq are\nrepresented by projections (or equivalently, closed subspaces of the state\nspace), following Birkhoff-von Neumann quantum logic. The satisfaction of a\nprojection by a quantum state can be directly checked upon a small number of\nprojective measurements rather than a large number of repeated executions. On\nthe theory side, we rigorously prove that checking projection-based assertions\ncan help locate bugs or statistically assure that the semantic function of the\ntested program is close to what we expect, for both exact and approximate\nquantum programs. On the practice side, we consider hardware constraints and\nintroduce several techniques to transform the assertions, making them directly\nexecutable on the measurement-restricted quantum computers. We also propose to\nachieve simplified assertion implementation using local projection technique\nwith soundness guaranteed. We compare Proq with existing quantum program\nassertions and demonstrate the effectiveness and efficiency of Proq by its\napplications to assert two ingenious quantum algorithms, the\nHarrow-Hassidim-Lloyd algorithm and Shor's algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 20:24:11 GMT"}, {"version": "v2", "created": "Fri, 29 May 2020 20:56:55 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Li", "Gushu", ""], ["Zhou", "Li", ""], ["Yu", "Nengkun", ""], ["Ding", "Yufei", ""], ["Ying", "Mingsheng", ""], ["Xie", "Yuan", ""]]}, {"id": "1911.12932", "submitter": "Caleb Helbling", "authors": "Caleb Helbling and Samuel Z Guyer", "title": "Juniper: A Functional Reactive Programming Language for the Arduino", "comments": "9 pages, ICFP FARM 2016", "journal-ref": "Proceedings of the 4th International Workshop on Functional Art,\n  Music, Modelling, and Design, 2016", "doi": "10.1145/2975980.2975982", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the design and implementation of Juniper: a functional\nreactive programming language (FRP) targeting the Arduino and related\nmicrocontroller systems. Juniper provides a number of high level features,\nincluding parametric polymorphic functions, anonymous functions, automatic\nmemory management, and immutable data structures. Also included is a standard\nlibrary which offers many useful FRP signal processing functions. Juniper is\ntranslated to standard C++ and compiled with the existing Arduino development\ntools, allowing Juniper programs to fit on resource-constrained devices, and\nenabling seamless interoperability with existing C++ libraries for these\ndevices.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 03:00:51 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Helbling", "Caleb", ""], ["Guyer", "Samuel Z", ""]]}]