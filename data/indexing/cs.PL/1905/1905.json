[{"id": "1905.00402", "submitter": "Yannis Smaragdakis", "authors": "Yannis Smaragdakis", "title": "Next-Paradigm Programming Languages: What Will They Look Like and What\n  Changes Will They Bring?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dream of programming language design is to bring about\norders-of-magnitude productivity improvements in software development tasks.\nDesigners can endlessly debate on how this dream can be realized and on how\nclose we are to its realization. Instead, I would like to focus on a question\nwith an answer that can be, surprisingly, clearer: what will be the common\nprinciples behind next-paradigm, high-productivity programming languages, and\nhow will they change everyday program development? Based on my decade-plus\nexperience of heavy-duty development in declarative languages, I speculate that\ncertain tenets of high-productivity languages are inevitable. These include,\nfor instance, enormous variations in performance (including automatic\ntransformations that change the asymptotic complexity of algorithms); a radical\nchange in a programmer's workflow, elevating testing from a near-menial task to\nan act of deep understanding; a change in the need for formal proofs; and more.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 17:37:08 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Smaragdakis", "Yannis", ""]]}, {"id": "1905.00922", "submitter": "Minh Ngo", "authors": "Minh Ngo and David A. Naumann and Tamara Rezk", "title": "Type-based Declassification for Free", "comments": "The short version of this paper is accepted in ICFEM 2020. 100 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides a study to demonstrate the potential of using\noff-the-shelf programming languages and their theories to build sound\nlanguage-based-security tools. Our study focuses on information flow security\nencompassing declassification policies that allow us to express flexible\nsecurity policies needed for practical requirements. We translate security\npolicies, with declassification, into an interface for which an unmodified\nstandard typechecker can be applied to a source program---if the program\ntypechecks, it provably satisfies the policy. Our proof reduces security\nsoundness---with declassification---to the mathematical foundation of data\nabstraction, Reynolds' abstraction theorem.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 18:23:08 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 14:17:05 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 17:07:07 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Ngo", "Minh", ""], ["Naumann", "David A.", ""], ["Rezk", "Tamara", ""]]}, {"id": "1905.01453", "submitter": "Atsushi Igarashi", "authors": "Hiroaki Inoue and Atsushi Igarashi", "title": "A Type System for First-Class Layers with Inheritance, Subtyping, and\n  Swapping", "comments": null, "journal-ref": "Sci. Comput. Program. 179: 54-86 (2019)", "doi": "10.1016/j.scico.2019.03.008", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context-Oriented Programming (COP) is a programming paradigm to encourage\nmodularization of context-dependent software. Key features of COP are\nlayers---modules to describe context-dependent behavioral variations of a\nsoftware system---and their dynamic activation, which can modify the behavior\nof multiple objects that have already been instantiated. Typechecking programs\nwritten in a COP language is difficult because the activation of a layer can\neven change objects' interfaces. Inoue et al. have informally discussed how to\nmake JCop, an extension of Java for COP by Appeltauer et al., type-safe.\n  In this article, we formalize a small COP language called ContextFJ$_{<:}$\nwith its operational semantics and type system and show its type soundness. The\nlanguage models main features of the type-safe version of JCop, including\ndynamically activated first-class layers, inheritance of layer definitions,\nlayer subtyping, and layer swapping.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 08:12:29 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Inoue", "Hiroaki", ""], ["Igarashi", "Atsushi", ""]]}, {"id": "1905.01469", "submitter": "Andrzej Blikle", "authors": "Blikle Andrzej", "title": "An experiment with denotational semantics", "comments": "A preprint of 44 pages. All comments will be highly appreciated. You\n  may contact me on andrzej.blikle@moznainaczej.com.pl", "journal-ref": null, "doi": "10.13140/RG.2.2.31272.42249", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper is devoted to showing how to systematically design a programming\nlanguage in 'reverse order', i.e. from denotations to syntax. This construction\nis developed in an algebraic framework consisting of three many-sorted\nalgebras: of denotations, of an abstract syntax and of a concrete syntax. These\nalgebras are constructed in such a way that there is a unique homomorphism from\nconcrete syntax to denotations, which constitutes the denotational semantics of\nthe language.\n  Besides its algebraic framework, the model is set-theoretic, i.e. the\ndenotational domains are just sets, rather than Scott's reflexive domains.\n  The method is illustrated by a layer-by-layer development of a virtual\nlanguage Lingua: an applicative layer, an imperative layer (with recursive\nprocedures) and an SQL layer where Lingua is regarded as an API (Application\nProgramming Interface) for an SQL engine. The latter is given a denotational\nsemantics as well.\n  The langue is equipped with a strong typing mechanism which covers basic\ntypes (numbers, Booleans, etc.), lists, arrays, record and their arbitrary\ncombinations plus SQL-like types: rows, tables and databases. The model of\ntypes includes SQL integrity constraints.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 10:02:02 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Andrzej", "Blikle", ""]]}, {"id": "1905.01473", "submitter": "Andrzej Blikle", "authors": "Blikle Andrzej", "title": "A Denotational Engineering of Programming Languages", "comments": "A preprint of a book; 340 pages. All comments will be highly\n  appreciated. You may write to me on andrzej.blikle@moznainaczej.com.pl", "journal-ref": null, "doi": "10.13140/RG.2.2.27499.39201/3", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The book is devoted to two research areas: (1) Designing programming\nlanguages along with their denotational models. A denotational model of a\nlanguage consists of two many-sorted algebras - an algebra of syntax and an\nalgebra of denotations - and a (unique) homomorphism from syntax to denotations\ncalled the semantics of the language. (2) Designing sound program-constructors\nfor languages with denotational models. In our approach programs syntactically\ncontain their total-correctness specifications. A program is said to be correct\nif it is correct wrt its specification. A program-constructor is sound if given\ncorrect component-programs yields a correct resulting program. Both methods are\nillustrated on an example-language Lingua.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 10:39:57 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Andrzej", "Blikle", ""]]}, {"id": "1905.01659", "submitter": "Chao Peng", "authors": "Chao Peng, Sefa Akca and Ajitha Rajan", "title": "SIF: A Framework for Solidity Code Instrumentation and Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solidity is an object-oriented and high-level language for writing smart\ncontracts that are used to execute, verify and enforce credible transactions on\npermissionless blockchains. In the last few years, analysis of smart contracts\nhas raised considerable interest and numerous techniques have been proposed to\ncheck the presence of vulnerabilities in them. Current techniques lack\ntraceability in source code and have widely differing work flows. There is no\nsingle unifying framework for analysis, instrumentation, optimisation and code\ngeneration of Solidity contracts.\n  In this paper, we present SIF, a comprehensive framework for Solidity\ncontract analysis, query, instrumentation, and code generation. SIF provides\nsupport for Solidity contract developers and testers to build source level\ntechniques for analysis, understanding, diagnostics, optimisations and code\ngeneration. We show feasibility and applicability of the framework by building\npractical tools on top of it and running them on 1838 real smart contracts\ndeployed on the Ethereum network.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 11:31:23 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 09:34:43 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Peng", "Chao", ""], ["Akca", "Sefa", ""], ["Rajan", "Ajitha", ""]]}, {"id": "1905.02033", "submitter": "James Cheney", "authors": "Junao Wu and Arek Mikolajczak and James Cheney", "title": "TryLinks: An interactive tutorial system for a cross-tier Web\n  programming language", "comments": "ProWeb 2019 workshop paper", "journal-ref": null, "doi": "10.1145/3328433.3328450", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Links is a web programming language under development in Edinburgh aimed at\nsimplifying web development. Conventional multi-tier applications involve\nprogramming in several languages for different layers, and the mismatches\nbetween these layers and abstractions need to be handled by the programmer,\nwhich can lead to costly errors or security vulnerabilities. In Links, programs\ncombine all of the code of a web application in a single program, and the\nimplementation generates appropriate JavaScript and HTML for the client, and\nSQL queries for the database.\n  However, installing and using Links is non-trivial, making it difficult for\nnew programmers to get started and learn about Links. This paper reports on a\nWeb-based \"TryLinks\" system, allowing anyone to experiment with Links without\ngoing through the installation process. TryLinks was designed with two major\nfunctionalities: an interactive Links shell that teaches the basic syntax of\nLinks and acts as a playground, as well as a short tutorial series on how Links\nis used in practical web development. Tutorials can also be created or modified\nby administrators. We present the design and implementation of TryLinks, and\nconclude with discussion of lessons learned from this project and remaining\nchallenges for Web-based tutorials for Web programming languages.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 13:25:13 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Wu", "Junao", ""], ["Mikolajczak", "Arek", ""], ["Cheney", "James", ""]]}, {"id": "1905.02051", "submitter": "James Cheney", "authors": "Stefan Fehrenbach and James Cheney", "title": "Language-integrated provenance by trace analysis", "comments": "DBPL 2019", "journal-ref": null, "doi": "10.1145/3315507.3330198", "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language-integrated provenance builds on language-integrated query techniques\nto make provenance information explaining query results readily available to\nprogrammers. In previous work we have explored language-integrated approaches\nto provenance in Links and Haskell. However, implementing a new form of\nprovenance in a language-integrated way is still a major challenge. We propose\na self-tracing transformation and trace analysis features that, together with\nexisting techniques for type-directed generic programming, make it possible to\ndefine different forms of provenance as user code. We present our design as an\nextension to a core language for Links called LinksT, give examples showing its\ncapabilities, and outline its metatheory and key correctness properties.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 13:56:46 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Fehrenbach", "Stefan", ""], ["Cheney", "James", ""]]}, {"id": "1905.02088", "submitter": "George Fourtounis", "authors": "Neville Grech, George Fourtounis, Adrian Francalanza, Yannis\n  Smaragdakis", "title": "Heaps Don't Lie: Countering Unsoundness with Heap Snapshots", "comments": "OOPSLA 2017", "journal-ref": "Proceedings of the ACM on Programming Languages archive Volume 1\n  Issue OOPSLA, October 2017 Article No. 68", "doi": "10.1145/3133892", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static analyses aspire to explore all possible executions in order to achieve\nsoundness. Yet, in practice, they fail to capture common dynamic behavior.\nEnhancing static analyses with dynamic information is a common pattern, with\ntools such as Tamiflex. Past approaches, however, miss significant portions of\ndynamic behavior, due to native code, unsupported features (e.g., invokedynamic\nor lambdas in Java), and more. We present techniques that substantially\ncounteract the unsoundness of a static analysis, with virtually no intrusion to\nthe analysis logic. Our approach is reified in the HeapDL toolchain and\nconsists in taking whole-heap snapshots during program execution, that are\nfurther enriched to capture significant aspects of dynamic behavior, regardless\nof the causes of such behavior. The snapshots are then used as extra inputs to\nthe static analysis. The approach exhibits both portability and significantly\nincreased coverage. Heap information under one set of dynamic inputs allows a\nstatic analysis to cover many more behaviors under other inputs. A\nHeapDL-enhanced static analysis of the DaCapo benchmarks computes 99.5%\n(median) of the call-graph edges of unseen dynamic executions (vs. 76.9% for\nthe Tamiflex tool).\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 15:11:12 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Grech", "Neville", ""], ["Fourtounis", "George", ""], ["Francalanza", "Adrian", ""], ["Smaragdakis", "Yannis", ""]]}, {"id": "1905.02145", "submitter": "S\\'ergio Medeiros", "authors": "S\\'ergio Queiroz de Medeiros, Gilney de Azevedo Alvez Junior, Fabio\n  Mascarenhas", "title": "Automatic Syntax Error Reporting and Recovery in Parsing Expression\n  Grammars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error recovery is an essential feature for a parser that should be plugged in\nIntegrated Development Environments (IDEs), which must build Abstract Syntax\nTrees (ASTs) even for syntactically invalid programs in order to offer features\nsuch as automated refactoring and code completion.\n  Parsing Expressions Grammars (PEGs) are a formalism that naturally describes\nrecursive top-down parsers using a restricted form of backtracking. Labeled\nfailures are a conservative extension of PEGs that adds an error reporting\nmechanism for PEG parsers, and these labels can also be associated with\nrecovery expressions to provide an error recovery mechanism. These expressions\ncan use the full expressivity of PEGs to recover from syntactic errors.\n  Manually annotating a large grammar with labels and recovery expressions can\nbe difficult. In this work, we present two approaches, Standard and Unique, to\nautomatically annotate a PEG with labels, and to build their corresponding\nrecovery expressions. The Standard approach annotates a grammar in a way\nsimilar to manual annotation, but it may insert labels incorrectly, while the\nUnique approach is more conservative to annotate a grammar and does not insert\nlabels incorrectly.\n  We evaluate both approaches by using them to generate error recovering\nparsers for four programming languages: Titan, C, Pascal and Java. In our\nevaluation, the parsers produced using the Standard approach, after a manual\nintervention to remove the labels incorrectly added, gave an acceptable\nrecovery for at least 70% of the files in each language. By it turn, the\nacceptable recovery rate of the parsers produced via the Unique approach,\nwithout the need of manual intervention, ranged from 41% to 76%.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 16:56:35 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 16:39:16 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["de Medeiros", "S\u00e9rgio Queiroz", ""], ["Junior", "Gilney de Azevedo Alvez", ""], ["Mascarenhas", "Fabio", ""]]}, {"id": "1905.02158", "submitter": "Zhuozhao Li", "authors": "Yadu Babuji, Anna Woodard, Zhuozhao Li, Daniel S. Katz, Ben Clifford,\n  Rohan Kumar, Lukasz Lacinski, Ryan Chard, Justin M. Wozniak, Ian Foster,\n  Michael Wilde, Kyle Chard", "title": "Parsl: Pervasive Parallel Programming in Python", "comments": null, "journal-ref": null, "doi": "10.1145/3307681.3325400", "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-level programming languages such as Python are increasingly used to\nprovide intuitive interfaces to libraries written in lower-level languages and\nfor assembling applications from various components. This migration towards\norchestration rather than implementation, coupled with the growing need for\nparallel computing (e.g., due to big data and the end of Moore's law),\nnecessitates rethinking how parallelism is expressed in programs. Here, we\npresent Parsl, a parallel scripting library that augments Python with simple,\nscalable, and flexible constructs for encoding parallelism. These constructs\nallow Parsl to construct a dynamic dependency graph of components that it can\nthen execute efficiently on one or many processors. Parsl is designed for\nscalability, with an extensible set of executors tailored to different use\ncases, such as low-latency, high-throughput, or extreme-scale execution. We\nshow, via experiments on the Blue Waters supercomputer, that Parsl executors\ncan allow Python scripts to execute components with as little as 5 ms of\noverhead, scale to more than 250 000 workers across more than 8000 nodes, and\nprocess upward of 1200 tasks per second. Other Parsl features simplify the\nconstruction and execution of composite programs by supporting elastic\nprovisioning and scaling of infrastructure, fault-tolerant execution, and\nintegrated wide-area data management. We show that these capabilities satisfy\nthe needs of many-task, interactive, online, and machine learning applications\nin fields such as biology, cosmology, and materials science.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 17:19:10 GMT"}, {"version": "v2", "created": "Sat, 18 May 2019 02:56:23 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Babuji", "Yadu", ""], ["Woodard", "Anna", ""], ["Li", "Zhuozhao", ""], ["Katz", "Daniel S.", ""], ["Clifford", "Ben", ""], ["Kumar", "Rohan", ""], ["Lacinski", "Lukasz", ""], ["Chard", "Ryan", ""], ["Wozniak", "Justin M.", ""], ["Foster", "Ian", ""], ["Wilde", "Michael", ""], ["Chard", "Kyle", ""]]}, {"id": "1905.02529", "submitter": "Gabriel Radanne", "authors": "Gabriel Radanne, Thomas Gazagnaire, Anil Madhavapeddy, Jeremy Yallop,\n  Richard Mortier, Hannes Mehnert, Mindy Preston, David Scott", "title": "Programming Unikernels in the Large via Functor Driven Development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compiling applications as unikernels allows them to be tailored to diverse\nexecution environments. Dependency on a monolithic operating system is replaced\nwith linkage against libraries that provide specific services. Doing so in\npractice has revealed a major barrier: managing the configuration matrix across\nheterogenous execution targets. A realistic unikernel application depends on\nhundreds of libraries, each of which may place different demands on the\ndifferent target execution platforms (e.g.,~cryptographic acceleration).\n  We propose a modular approach to structuring large scale codebases that\ncleanly separates configuration, application and operating system logic. Our\nimplementation is built on the \\mirage unikernel framework, using the \\ocaml\nlanguage's powerful abstraction and metaprogramming facilities. Leveraging\nmodules allows us to build many components independently, with only loose\ncoupling through a set of standardised signatures. Components can be\nparameterized by other components and composed. Our approach accounts for\nstate, dependency ordering, and error management, and our usage over the years\nhas demonstrated significant efficiency benefits by leveraging compiler\nfeatures such as global link-time optimisation during the configuration\nprocess. We describe our application architecture and experiences via some\npractical applications of our approach, and discuss how library development in\n\\mirage can facilitate adoption in other unikernel frameworks and programming\nlanguages.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 13:09:32 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Radanne", "Gabriel", ""], ["Gazagnaire", "Thomas", ""], ["Madhavapeddy", "Anil", ""], ["Yallop", "Jeremy", ""], ["Mortier", "Richard", ""], ["Mehnert", "Hannes", ""], ["Preston", "Mindy", ""], ["Scott", "David", ""]]}, {"id": "1905.02617", "submitter": "Brigitte Pientka", "authors": "Brigitte Pientka, David Thibodeau, Andreas Abel, Francisco Ferreira,\n  and Rebecca Zucchini", "title": "A Type Theory for Defining Logics and Proofs", "comments": "arXiv admin note: substantial text overlap with arXiv:1901.03378", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a Martin-L\\\"of-style dependent type theory, called Cocon, that\nallows us to mix the intensional function space that is used to represent\nhigher-order abstract syntax (HOAS) trees with the extensional function space\nthat describes (recursive) computations. We mediate between HOAS\nrepresentations and computations using contextual modal types. Our type theory\nalso supports an infinite hierarchy of universes and hence supports type-level\ncomputation thereby providing metaprogramming and (small-scale) reflection. Our\nmain contribution is the development of a Kripke-style model for Cocon that\nallows us to prove normalization. From the normalization proof, we derive\nsubject reduction and consistency. Our work lays the foundation to incorporate\nthe methodology of logical frameworks into systems such as Agda and bridges the\nlongstanding gap between these two worlds.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 14:43:01 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Pientka", "Brigitte", ""], ["Thibodeau", "David", ""], ["Abel", "Andreas", ""], ["Ferreira", "Francisco", ""], ["Zucchini", "Rebecca", ""]]}, {"id": "1905.03746", "submitter": "David Greaves", "authors": "David J. Greaves", "title": "Research Note: An Open Source Bluespec Compiler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this Research Note we report on an open-source compiler for the Bluespec\nhardware description language.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 11:13:49 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Greaves", "David J.", ""]]}, {"id": "1905.03957", "submitter": "Sanjana Singh", "authors": "Sanjana Singh, Divyanjali Sharma and Subodh Sharma", "title": "Dynamic Verification with Observational Equivalence of C/C++ Concurrency", "comments": "Paper has some issues with certain claims that needs to be rectified", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Program executions under relaxed memory model (rmm) semantics are\nsignificantly more difficult to analyze; the rmm semantics result in out of\norder execution of program events leading to an explosion of state-space.\nDynamic partial order reduction (DPOR) is a powerful technique to address such\na state-space explosion and has been used to verify programs under rmm such as\nTSO, PSO, and POWER. Central to such DPOR techniques is the notion of\ntrace-equivalence, which is computed based on the independence relation among\nprogram events. We propose a coarser notion of rmm-aware trace equivalence\ncalled observational equivalence (OE). Two program behaviors are\nobservationally equivalent if every read event reads the same value in both the\nbehaviors. We propose a notion of observational independence (OI) and provide\nan algorithmic construction to compute trace equivalence (modulo OI)\nefficiently. We also demonstrate the effectiveness of DPOR with OE on threaded\nC/C++ programs by first providing an elaborate happensbefore (hb) relation for\ncapturing the C/C++ concurrency semantics. We implement the presented technique\nin a runtime model checker called Drista. Our experiments reflect that (i) when\ncompared to existing nonOE techniques, we achieve significant savings in the\nnumber of traces explored under OE, and (ii) our treatment of C/C++ concurrency\nis more extensive than the existing state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 06:08:47 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 13:43:53 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Singh", "Sanjana", ""], ["Sharma", "Divyanjali", ""], ["Sharma", "Subodh", ""]]}, {"id": "1905.04232", "submitter": "Patrik Christen", "authors": "Patrik Christen and Olivier Del Fabbro", "title": "Automatic Programming of Cellular Automata and Artificial Neural\n  Networks Guided by Philosophy", "comments": "12 pages, 1 figure", "journal-ref": "Rolf Dornberger, editor, New Trends in Business Information\n  Systems and Technology: Digital Innovation and Digital Business\n  Transformation, pages 131-146. Springer, Cham, 2020", "doi": "10.1007/978-3-030-48332-6", "report-no": null, "categories": "cs.AI cs.LG cs.MA cs.NE cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many computer models such as cellular automata and artificial neural networks\nhave been developed and successfully applied. However, in some cases, these\nmodels might be restrictive on the possible solutions or their solutions might\nbe difficult to interpret. To overcome this problem, we outline a new approach,\nthe so-called allagmatic method, that automatically programs and executes\nmodels with as little limitations as possible while maintaining human\ninterpretability. Earlier we described a metamodel and its building blocks\naccording to the philosophical concepts of structure (spatial dimension) and\noperation (temporal dimension). They are entity, milieu, and update function\nthat together abstractly describe cellular automata, artificial neural\nnetworks, and possibly any kind of computer model. By automatically combining\nthese building blocks in an evolutionary computation, interpretability might be\nincreased by the relationship to the metamodel, and models might be translated\ninto more interpretable models via the metamodel. We propose generic and\nobject-oriented programming to implement the entities and their milieus as\ndynamic and generic arrays and the update function as a method. We show two\nexperiments where a simple cellular automaton and an artificial neural network\nare automatically programmed, compiled, and executed. A target state is\nsuccessfully evolved and learned in the cellular automaton and artificial\nneural network, respectively. We conclude that the allagmatic method can create\nand execute cellular automaton and artificial neural network models in an\nautomated manner with the guidance of philosophy.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 16:00:09 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 14:03:51 GMT"}, {"version": "v3", "created": "Wed, 24 Jul 2019 10:06:41 GMT"}, {"version": "v4", "created": "Wed, 20 Nov 2019 08:47:05 GMT"}, {"version": "v5", "created": "Sun, 3 May 2020 21:05:20 GMT"}, {"version": "v6", "created": "Mon, 31 Aug 2020 21:45:47 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Christen", "Patrik", ""], ["Del Fabbro", "Olivier", ""]]}, {"id": "1905.04366", "submitter": "Massimo Bartoletti", "authors": "Massimo Bartoletti, Letterio Galletta, Maurizio Murgia", "title": "A true concurrent model of smart contracts executions", "comments": "Full version of the paper presented at COORDINATION 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of blockchain technologies has enabled the trustless\nexecution of so-called smart contracts, i.e. programs that regulate the\nexchange of assets (e.g., cryptocurrency) between users. In a decentralized\nblockchain, the state of smart contracts is collaboratively maintained by a\npeer-to-peer network of mutually untrusted nodes, which collect from users a\nset of transactions (representing the required actions on contracts), and\nexecute them in some order. Once this sequence of transactions is appended to\nthe blockchain, the other nodes validate it, re-executing the transactions in\nthe same order. The serial execution of transactions does not take advantage of\nthe multi-core architecture of modern processors, so contributing to limit the\nthroughput. In this paper we propose a true concurrent model of smart contract\nexecution. Based on this, we show how static analysis of smart contracts can be\nexploited to parallelize the execution of transactions.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 20:06:15 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 17:56:21 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 15:36:02 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Bartoletti", "Massimo", ""], ["Galletta", "Letterio", ""], ["Murgia", "Maurizio", ""]]}, {"id": "1905.04642", "submitter": "Julio C\\'esar P\\'erez Sansalvador", "authors": "Ricardo Serrato Barrera and Gustavo Rodr\\'iguez G\\'omez and Julio\n  C\\'esar P\\'erez Sansalvador and Saul E. Pomares Hern\\'andez and Leticia\n  Flores Pulido and Antonio Mu\\~noz", "title": "Software System Design based on Patterns for Newton-Type Methods", "comments": "19 pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of engineering applications uses optimisation techniques as part\nof their solution process. The researcher uses specialized software that\nimplements well-known optimisation techniques to solve his problem. However,\nwhen it comes to develop original optimisation techniques that fit a particular\nproblem the researcher has no option but to implement his own new method from\nscratch. This leads to large development times and error prone code that, in\ngeneral, will not be reused for any other application. In this work, we present\na novel methodology that simplifies, fasten and improves the development\nprocess of scientific software. This methodology guide us on the identification\nof design patterns. The application of this methodology generates reusable,\nflexible and high quality scientific software. Furthermore, the produced\nsoftware becomes a documented tool to transfer the knowledge on the development\nprocess of scientific software. We apply this methodology for the design of an\noptimisation framework implementing Newton's type methods which can be used as\na fast prototyping tool of new optimisation techniques based on Newton's type\nmethods. The abstraction, reusability and flexibility of the developed\nframework is measured by means of Martin's metric. The results indicate that\nthe developed software is highly reusable.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 04:04:22 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Barrera", "Ricardo Serrato", ""], ["G\u00f3mez", "Gustavo Rodr\u00edguez", ""], ["Sansalvador", "Julio C\u00e9sar P\u00e9rez", ""], ["Hern\u00e1ndez", "Saul E. Pomares", ""], ["Pulido", "Leticia Flores", ""], ["Mu\u00f1oz", "Antonio", ""]]}, {"id": "1905.05251", "submitter": "Ke Wang", "authors": "Ke Wang", "title": "Learning Scalable and Precise Representation of Program Semantics", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural program embedding has shown potential in aiding the analysis of\nlarge-scale, complicated software. Newly proposed deep neural architectures\npride themselves on learning program semantics rather than superficial\nsyntactic features. However, by considering the source code only, the vast\nmajority of neural networks do not capture a deep, precise representation of\nprogram semantics. In this paper, we present \\dypro, a novel deep neural\nnetwork that learns from program execution traces. Compared to the prior\ndynamic models, not only is \\dypro capable of generalizing across multiple\nexecutions for learning a program's dynamic semantics in its entirety, but\n\\dypro is also more efficient when dealing with programs yielding long\nexecution traces. For evaluation, we task \\dypro with semantic classification\n(i.e. categorizing programs based on their semantics) and compared it against\ntwo prominent static models: Gated Graph Neural Network and TreeLSTM. We find\nthat \\dypro achieves the highest prediction accuracy among all models. To\nfurther reveal the capacity of all aforementioned deep neural architectures, we\nexamine if the models can learn to detect deeper semantic properties of a\nprogram. In particular given a task of recognizing loop invariants, we show\n\\dypro beats all static models by a wide margin.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 19:16:22 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 17:16:46 GMT"}, {"version": "v3", "created": "Sun, 26 May 2019 23:57:07 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Wang", "Ke", ""]]}, {"id": "1905.05684", "submitter": "Kartik Nagar", "authors": "Kartik Nagar and Suresh Jagannathan", "title": "Automated Parameterized Verification of CRDTs", "comments": "Extended Version of CAV 2019 Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintaining multiple replicas of data is crucial to achieving scalability,\navailability and low latency in distributed applications. Conflict-free\nReplicated Data Types (CRDTs) are important building blocks in this domain\nbecause they are designed to operate correctly under the myriad behaviors\npossible in a weakly-consistent distributed setting. Because of the possibility\nof concurrent updates to the same object at different replicas, and the absence\nof any ordering guarantees on these updates, convergence is an important\ncorrectness criterion for CRDTs. This property asserts that two replicas which\nreceive the same set of updates (in any order) must nonetheless converge to the\nsame state. One way to prove that operations on a CRDT converge is to show that\nthey commute since commutative actions by definition behave the same regardless\nof the order in which they execute. In this paper, we present a framework for\nautomatically verifying convergence of CRDTs under different weak-consistency\npolicies. Surprisingly, depending upon the consistency policy supported by the\nunderlying system, we show that not all operations of a CRDT need to commute to\nachieve convergence. We develop a proof rule parameterized by a consistency\nspecification based on the concepts of commutativity modulo consistency policy\nand non-interference to commutativity. We describe the design and\nimplementation of a verification engine equipped with this rule and show how it\ncan be used to provide the first automated convergence proofs for a number of\nchallenging CRDTs, including sets, lists, and graphs.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 15:59:15 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Nagar", "Kartik", ""], ["Jagannathan", "Suresh", ""]]}, {"id": "1905.05800", "submitter": "Qinheping Hu", "authors": "Qinheping Hu, Jason Breck, John Cyphert, Loris D'Antoni, Thomas Reps", "title": "Proving Unrealizability for Syntax-Guided Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proving Unrealizability for Syntax-Guided Synthesis\n  We consider the problem of automatically establishing that a given\nsyntax-guided-synthesis (SyGuS) problem is unrealizable (i.e., has no\nsolution). Existing techniques have quite limited ability to establish\nunrealizability for general SyGuS instances in which the grammar describing the\nsearch space contains infinitely many programs. By encoding the synthesis\nproblem's grammar G as a nondeterministic program P_G, we reduce the\nunrealizability problem to a reachability problem such that, if a standard\nprogram-analysis tool can establish that a certain assertion in P_G always\nholds, then the synthesis problem is unrealizable.\n  Our method can be used to augment any existing SyGus tool so that it can\nestablish that a successfully synthesized program q is optimal with respect to\nsome syntactic cost -- e.g., q has the fewest possible if-then-else operators.\nUsing known techniques, grammar G can be automatically transformed to generate\nexactly all programs with lower cost than q -- e.g., fewer conditional\nexpressions. Our algorithm can then be applied to show that the resulting\nsynthesis problem is unrealizable. We implemented the proposed technique in a\ntool called NOPE. NOPE can prove unrealizability for 59/134 variants of\nexisting linear-integer-arithmetic SyGus benchmarks, whereas all existing SyGus\nsolvers lack the ability to prove that these benchmarks are unrealizable, and\ntime out on them.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 19:18:54 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 20:11:32 GMT"}, {"version": "v3", "created": "Tue, 23 Jul 2019 19:19:54 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Hu", "Qinheping", ""], ["Breck", "Jason", ""], ["Cyphert", "John", ""], ["D'Antoni", "Loris", ""], ["Reps", "Thomas", ""]]}, {"id": "1905.05909", "submitter": "Sam Lindley", "authors": "Sam Lindley, Gabriel Scherer", "title": "Proceedings ML Family / OCaml Users and Developers workshops", "comments": null, "journal-ref": "EPTCS 294, 2019", "doi": "10.4204/EPTCS.294", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the joint post-proceedings of the 2017 editions of the\nML Family Workshop and the OCaml Users and Developers Workshop which took place\nin Oxford, UK, September 2017, and which were colocated with the ICFP 2017\nconference.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 01:30:27 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Lindley", "Sam", ""], ["Scherer", "Gabriel", ""]]}, {"id": "1905.06233", "submitter": "Horatiu Cirstea", "authors": "Horatiu Cirstea and Pierre-Etienne Moreau", "title": "Generic Encodings of Constructor Rewriting Systems", "comments": "Added appendix with proofs and extended examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rewriting is a formalism widely used in computer science and mathematical\nlogic. The classical formalism has been extended, in the context of functional\nlanguages, with an order over the rules and, in the context of rewrite based\nlanguages, with the negation over patterns. We propose in this paper a concise\nand clear algorithm computing the difference over patterns which can be used to\ndefine generic encodings of constructor term rewriting systems with negation\nand order into classical term rewriting systems. As a direct consequence,\nestablished methods used for term rewriting systems can be applied to analyze\nproperties of the extended systems. The approach can also be seen as a generic\ncompiler which targets any language providing basic pattern matching\nprimitives. The formalism provides also a new method for deciding if a set of\npatterns subsumes a given pattern and thus, for checking the presence of\nuseless patterns or the completeness of a set of patterns.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 14:06:09 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 11:07:38 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Cirstea", "Horatiu", ""], ["Moreau", "Pierre-Etienne", ""]]}, {"id": "1905.06495", "submitter": "Jake Silverman", "authors": "Jake Silverman and Zachary Kincaid", "title": "Loop Summarization with Rational Vector Addition Systems (extended\n  version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a technique for computing numerical loop summaries. The\nmethod synthesizes a rational vector addition system with resets (Q-VASR) that\nsimulates the action of an input loop, and then uses the reachability relation\nof that Q-VASR to over-approximate the behavior of the loop. The key technical\nproblem solved in this paper is to automatically synthesize a Q-VASR that is a\nbest abstraction of a given loop in the sense that (1) it simulates the loop\nand (2) it is simulated by any other Q-VASR that simulates the loop. Since our\nloop summarization scheme is based on computing the exact reachability relation\nof a best abstraction of a loop, we can make theoretical guarantees about its\nbehavior. Moreover, we show experimentally that the technique is precise and\nperformant in practice.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 02:00:42 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 20:07:36 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Silverman", "Jake", ""], ["Kincaid", "Zachary", ""]]}, {"id": "1905.06543", "submitter": "EPTCS", "authors": "Runhang Li (Twitter, Inc), Jeremy Yallop (University of Cambridge)", "title": "Extending OCaml's 'open'", "comments": "In Proceedings ML 2017, arXiv:1905.05909", "journal-ref": "EPTCS 294, 2019, pp. 1-14", "doi": "10.4204/EPTCS.294.1", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a harmonious extension of OCaml's 'open' construct.\n  OCaml's existing construct 'open M' imports the names exported by the module\n'M' into the current scope. At present 'M' is required to be the path to a\nmodule. We propose extending 'open' to instead accept an arbitrary module\nexpression, making it possible to succinctly address a number of existing\nscope-related difficulties that arise when writing OCaml programs.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 06:11:17 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Li", "Runhang", "", "Twitter, Inc"], ["Yallop", "Jeremy", "", "University of Cambridge"]]}, {"id": "1905.06544", "submitter": "EPTCS", "authors": "Oleg Kiselyov (Tohoku University, Japan)", "title": "Effects Without Monads: Non-determinism -- Back to the Meta Language", "comments": "In Proceedings ML 2017, arXiv:1905.05909", "journal-ref": "EPTCS 294, 2019, pp. 15-40", "doi": "10.4204/EPTCS.294.2", "report-no": null, "categories": "cs.PL cs.LO cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reflect on programming with complicated effects, recalling an\nundeservingly forgotten alternative to monadic programming and checking to see\nhow well it can actually work in modern functional languages. We adopt and\nargue the position of factoring an effectful program into a first-order\neffectful DSL with a rich, higher-order 'macro' system. Not all programs can be\nthus factored. Although the approach is not general-purpose, it does admit\ninteresting programs. The effectful DSL is likewise rather problem-specific and\nlacks general-purpose monadic composition, or even functions. On the upside, it\nexpresses the problem elegantly, is simple to implement and reason about, and\nlends itself to non-standard interpretations such as code generation\n(compilation) and abstract interpretation. A specialized DSL is liable to be\nfrequently extended; the experience with the tagless-final style of DSL\nembedding shown that the DSL evolution can be made painless, with the maximum\ncode reuse. We illustrate the argument on a simple but representative example\nof a rather complicated effect -- non-determinism, including committed choice.\nUnexpectedly, it turns out we can write interesting non-deterministic programs\nin an ML-like language just as naturally and elegantly as in the\nfunctional-logic language Curry -- and not only run them but also statically\nanalyze, optimize and compile. The richness of the Meta Language does, in\nreality, compensate for the simplicity of the effectful DSL. The key idea goes\nback to the origins of ML as the Meta Language for the Edinburgh LCF theorem\nprover. Instead of using ML to build theorems, we now build (DSL) programs.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 06:11:37 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Kiselyov", "Oleg", "", "Tohoku University, Japan"]]}, {"id": "1905.06545", "submitter": "EPTCS", "authors": "John Whitington, Tom Ridge", "title": "Direct Interpretation of Functional Programs for Debugging", "comments": "In Proceedings ML 2017, arXiv:1905.05909", "journal-ref": "EPTCS 294, 2019, pp. 41-73", "doi": "10.4204/EPTCS.294.3", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We make another assault on the longstanding problem of debugging. After\nexploring why debuggers are not used as widely as one might expect, especially\nin functional programming environments, we define the characteristics of a\ndebugger which make it usable and thus widely used. We present initial work\ntowards a new debugger for OCaml which operates by direct interpretation of the\nprogram source, allowing the printing out of individual steps of the program's\nevaluation. We present OCamli, a standalone interpreter, and propose a\nmechanism by which the interpreter could be integrated into compiled\nexecutables, allowing part of a program to be interpreted in the same fashion\nas OCamli whilst the rest of the program runs natively. We show how such a\nmechanism might create a source-level debugging system that has the\ncharacteristics of a usable debugger (such as being independent of its\nenvironment) and so may eventually be expected to be suitable for widespread\nadoption.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 06:11:56 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Whitington", "John", ""], ["Ridge", "Tom", ""]]}, {"id": "1905.06546", "submitter": "EPTCS", "authors": "Jeremy Yallop (University of Cambridge), Stephen Dolan (University of\n  Cambridge)", "title": "First-Class Subtypes", "comments": "In Proceedings ML 2017, arXiv:1905.05909", "journal-ref": "EPTCS 294, 2019, pp. 74-85", "doi": "10.4204/EPTCS.294.4", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First class type equalities, in the form of generalized algebraic data types\n(GADTs), are commonly found in functional programs. However, first-class\nrepresentations of other relations between types, such as subtyping, are not\nyet directly supported in most functional programming languages.\n  We present several encodings of first-class subtypes using existing features\nof the OCaml language (made more convenient by the proposed modular implicits\nextension), show that any such encodings are interconvertible, and illustrate\nthe utility of the encodings with several examples.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 06:12:14 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Yallop", "Jeremy", "", "University of Cambridge"], ["Dolan", "Stephen", "", "University of\n  Cambridge"]]}, {"id": "1905.06707", "submitter": "Jessica Schrouff", "authors": "Jessica Schrouff, Kai Wohlfahrt, Bruno Marnette, Liam Atkinson", "title": "Inferring Javascript types using Graph Neural Networks", "comments": "Published at the Representation Learning on Graphs and Manifolds ICLR\n  2019 workshop (https://rlgm.github.io/papers/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent use of `Big Code' with state-of-the-art deep learning methods\noffers promising avenues to ease program source code writing and correction. As\na first step towards automatic code repair, we implemented a graph neural\nnetwork model that predicts token types for Javascript programs. The\npredictions achieve an accuracy above $90\\%$, which improves on previous\nsimilar work.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 12:58:50 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Schrouff", "Jessica", ""], ["Wohlfahrt", "Kai", ""], ["Marnette", "Bruno", ""], ["Atkinson", "Liam", ""]]}, {"id": "1905.06777", "submitter": "Igor Ivkic", "authors": "Igor Ivkic, Alexander W\\\"ohrer, Markus Tauber", "title": "Towards Comparing Programming Paradigms", "comments": null, "journal-ref": "2017 12th International Conference for Internet Technology and\n  Secured Transactions (ICITST), Cambridge, UK", "doi": "10.23919/ICITST.2017.8356440", "report-no": null, "categories": "cs.HC cs.CL cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid technological progress in computer sciences finds solutions and at the\nsame time creates ever more complex requirements. Due to an evolving complexity\ntodays programming languages provide powerful frameworks which offer standard\nsolutions for recurring tasks to assist the programmer and to avoid the\nre-invention of the wheel with so-called out-of-the-box-features. In this\npaper, we propose a way of comparing different programming paradigms on a\ntheoretical, technical and practical level. Furthermore, the paper presents the\nresults of an initial comparison of two representative programming approaches,\nboth in the closed SAP environment.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 09:55:10 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Ivkic", "Igor", ""], ["W\u00f6hrer", "Alexander", ""], ["Tauber", "Markus", ""]]}, {"id": "1905.07212", "submitter": "Finn Teegen", "authors": "Sandra Dylus, Jan Christiansen, Finn Teegen", "title": "Implementing a Library for Probabilistic Programming using Non-strict\n  Non-determinism", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents PFLP, a library for probabilistic programming in the\nfunctional logic programming language Curry. It demonstrates how the concepts\nof a functional logic programming language support the implementation of a\nlibrary for probabilistic programming. In fact, the paradigms of functional\nlogic and probabilistic programming are closely connected. That is, language\ncharacteristics from one area exist in the other and vice versa. For example,\nthe concepts of non-deterministic choice and call-time choice as known from\nfunctional logic programming are related to and coincide with stochastic\nmemoization and probabilistic choice in probabilistic programming,\nrespectively. We will further see that an implementation based on the concepts\nof functional logic programming can have benefits with respect to performance\ncompared to a standard list-based implementation and can even compete with\nfull-blown probabilistic programming languages, which we illustrate by several\nbenchmarks. Under consideration in Theory and Practice of Logic Programming\n(TPLP).\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 11:38:33 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Dylus", "Sandra", ""], ["Christiansen", "Jan", ""], ["Teegen", "Finn", ""]]}, {"id": "1905.07362", "submitter": "Markus Lepper", "authors": "Baltasar Tranc\\'on y Widemann and Markus Lepper", "title": "Simple and Effective Relation-Based Approaches To XPath and XSLT Type\n  Checking (Technical Report, Bad Honnef 2015)", "comments": "10 pages, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  XPath is a language for addressing parts of an XML document. We give an\nabstract interpretation of XPath expressions in terms of relations on document\nnode types. Node-set-related XPath language constructs are mapped\nstraightforwardly onto basic, well-understood and easily computable relational\noperations. Hence our interpretation gives both extremely concise type-level\ndenotational semantics and a practical analysis tool for the node-set fragment\nof the XPath 1.0 language. This method is part of the TPath implementation of\nXPath.\n  XSL-T is a pure functional language for transforming XML documents. For the\nmost common case, the transformation into an XML document, type checking of the\ntransformation code is unfeasible in general, but strongly required in\npractice. It turned out that the relational approach of TPath can be carried\nover to check all fragments of the result language, which are contained\nverbatim in the transformation code. This leads to a technique called\n\"Fragmented Validation\" and is part of the txsl implementation of XSL-T.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 16:32:33 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Widemann", "Baltasar Tranc\u00f3n y", ""], ["Lepper", "Markus", ""]]}, {"id": "1905.07457", "submitter": "Saswat Padhi", "authors": "Saswat Padhi, Todd Millstein, Aditya Nori, Rahul Sharma", "title": "Overfitting in Synthesis: Theory and Practice (Extended Version)", "comments": "24 pages (5 pages of appendices), 7 figures, includes proofs of\n  theorems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In syntax-guided synthesis (SyGuS), a synthesizer's goal is to automatically\ngenerate a program belonging to a grammar of possible implementations that\nmeets a logical specification. We investigate a common limitation across\nstate-of-the-art SyGuS tools that perform counterexample-guided inductive\nsynthesis (CEGIS). We empirically observe that as the expressiveness of the\nprovided grammar increases, the performance of these tools degrades\nsignificantly.\n  We claim that this degradation is not only due to a larger search space, but\nalso due to overfitting. We formally define this phenomenon and prove\nno-free-lunch theorems for SyGuS, which reveal a fundamental tradeoff between\nsynthesizer performance and grammar expressiveness.\n  A standard approach to mitigate overfitting in machine learning is to run\nmultiple learners with varying expressiveness in parallel. We demonstrate that\nthis insight can immediately benefit existing SyGuS tools. We also propose a\nnovel single-threaded technique called hybrid enumeration that interleaves\ndifferent grammars and outperforms the winner of the 2018 SyGuS competition\n(Inv track), solving more problems and achieving a $5\\times$ mean speedup.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 20:07:16 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 15:17:06 GMT"}, {"version": "v3", "created": "Sat, 8 Jun 2019 00:31:22 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Padhi", "Saswat", ""], ["Millstein", "Todd", ""], ["Nori", "Aditya", ""], ["Sharma", "Rahul", ""]]}, {"id": "1905.07639", "submitter": "Stefano Lande", "authors": "Nicola Atzei and Massimo Bartoletti and Stefano Lande and Nobuko\n  Yoshida and Roberto Zunino", "title": "Developing secure Bitcoin contracts with BitML", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a toolchain for developing and verifying smart contracts that can\nbe executed on Bitcoin. The toolchain is based on BitML, a recent\ndomain-specific language for smart contracts with a computationally sound\nembedding into Bitcoin. Our toolchain automatically verifies relevant\nproperties of contracts, among which liquidity, ensuring that funds do not\nremain frozen within a contract forever. A compiler is provided to translate\nBitML contracts into sets of standard Bitcoin transactions: executing a\ncontract corresponds to appending these transactions to the blockchain. We\nassess our toolchain through a benchmark of representative contracts.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 20:14:10 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 21:19:47 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Atzei", "Nicola", ""], ["Bartoletti", "Massimo", ""], ["Lande", "Stefano", ""], ["Yoshida", "Nobuko", ""], ["Zunino", "Roberto", ""]]}, {"id": "1905.07653", "submitter": "Yonghae Kim", "authors": "Yonghae Kim, Hyesoon Kim", "title": "A Case Study: Exploiting Neural Machine Translation to Translate CUDA to\n  OpenCL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sequence-to-sequence (seq2seq) model for neural machine translation has\nsignificantly improved the accuracy of language translation. There have been\nnew efforts to use this seq2seq model for program language translation or\nprogram comparisons. In this work, we present the detailed steps of using a\nseq2seq model to translate CUDA programs to OpenCL programs, which both have\nvery similar programming styles. Our work shows (i) a training input set\ngeneration method, (ii) pre/post processing, and (iii) a case study using\nPolybench-gpu-1.0, NVIDIA SDK, and Rodinia benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 22:12:42 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Kim", "Yonghae", ""], ["Kim", "Hyesoon", ""]]}, {"id": "1905.07705", "submitter": "Ron Shemer", "authors": "Ron Shemer, Arie Gurfinkel, Sharon Shoham and Yakir Vizel", "title": "Property Directed Self Composition", "comments": "28 pages, accepted to CAV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of verifying k-safety properties: properties that\nrefer to k-interacting executions of a program. A prominent way to verify\nk-safety properties is by self composition. In this approach, the problem of\nchecking k-safety over the original program is reduced to checking an\n\"ordinary\" safety property over a program that executes k copies of the\noriginal program in some order. The way in which the copies are composed\ndetermines how complicated it is to verify the composed program. We view this\ncomposition as provided by a semantic self composition function that maps each\nstate of the composed program to the copies that make a move. Since the\n\"quality\" of a self composition function is measured by the ability to verify\nthe safety of the composed program, we formulate the problem of inferring a\nself composition function together with the inductive invariant needed to\nverify safety of the composed program, where both are restricted to a given\nlanguage. We develop a property-directed inference algorithm that, given a set\nof predicates, infers composition-invariant pairs expressed by Boolean\ncombinations of the given predicates, or determines that no such pair exists.\nWe implemented our algorithm and demonstrate that it is able to find self\ncompositions that are beyond reach of existing tools.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 08:17:36 GMT"}, {"version": "v2", "created": "Sun, 26 May 2019 21:20:47 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Shemer", "Ron", ""], ["Gurfinkel", "Arie", ""], ["Shoham", "Sharon", ""], ["Vizel", "Yakir", ""]]}, {"id": "1905.07739", "submitter": "Yotam Feldman", "authors": "Yotam M. Y. Feldman, James R. Wilcox, Sharon Shoham, Mooly Sagiv", "title": "Inferring Inductive Invariants from Phase Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infinite-state systems such as distributed protocols are challenging to\nverify using interactive theorem provers or automatic verification tools. Of\nthese techniques, deductive verification is highly expressive but requires the\nuser to annotate the system with inductive invariants. To relieve the user from\nthis labor-intensive and challenging task, invariant inference aims to find\ninductive invariants automatically. Unfortunately, when applied to\ninfinite-state systems such as distributed protocols, existing inference\ntechniques often diverge, which limits their applicability.\n  This paper proposes user-guided invariant inference based on phase\ninvariants, which capture the different logical phases of the protocol. Users\nconveys their intuition by specifying a phase structure, an automaton with\nedges labeled by program transitions; the tool automatically infers assertions\nthat hold in the automaton's states, resulting in a full safety proof.The\nadditional structure from phases guides the inference procedure towards finding\nan invariant.\n  Our results show that user guidance by phase structures facilitates\nsuccessful inference beyond the state of the art. We find that phase structures\nare pleasantly well matched to the intuitive reasoning routinely used by domain\nexperts to understand why distributed protocols are correct, so that providing\na phase structure reuses this existing intuition.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 13:31:41 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Feldman", "Yotam M. Y.", ""], ["Wilcox", "James R.", ""], ["Shoham", "Sharon", ""], ["Sagiv", "Mooly", ""]]}, {"id": "1905.07805", "submitter": "Idan Berkovits", "authors": "Idan Berkovits, Marijana Lazic, Giuliano Losa, Oded Padon, Sharon\n  Shoham", "title": "Verification of Threshold-Based Distributed Algorithms by Decomposition\n  to Decidable Logics", "comments": "23 pages, extended version of the paper with the same title presented\n  in CAV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification of fault-tolerant distributed protocols is an immensely\ndifficult task. Often, in these protocols, thresholds on set cardinalities are\nused both in the process code and in its correctness proof, e.g., a process can\nperform an action only if it has received an acknowledgment from at least half\nof its peers. Verification of threshold-based protocols is extremely\nchallenging as it involves two kinds of reasoning: first-order reasoning about\nthe unbounded state of the protocol, together with reasoning about sets and\ncardinalities. In this work, we develop a new methodology for decomposing the\nverification task of such protocols into two decidable logics: EPR and BAPA.\nOur key insight is that such protocols use thresholds in a restricted way as a\nmeans to obtain certain properties of \"intersection\" between sets. We define a\nlanguage for expressing such properties, and present two translations: to EPR\nand BAPA. The EPR translation allows verifying the protocol while assuming\nthese properties, and the BAPA translation allows verifying the correctness of\nthe properties. We further develop an algorithm for automatically generating\nthe properties needed for verifying a given protocol, facilitating fully\nautomated deductive verification. Using this technique we have verified several\nchallenging protocols, including Byzantine one-step consensus, hybrid reliable\nbroadcast and fast Byzantine Paxos.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 20:40:05 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Berkovits", "Idan", ""], ["Lazic", "Marijana", ""], ["Losa", "Giuliano", ""], ["Padon", "Oded", ""], ["Shoham", "Sharon", ""]]}, {"id": "1905.08178", "submitter": "Sandeep Dasgupta", "authors": "Sandeep Dasgupta and Tanmay Gangwani", "title": "Partial Redundancy Elimination using Lazy Code Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial Redundancy Elimination (PRE) is a compiler optimization that\neliminates expressions that are redundant on some but not necessarily all paths\nthrough a program. In this project, we implemented a PRE optimization pass in\nLLVM and measured results on a variety of applications. We chose PRE because it\nis a powerful technique that subsumes Common Subexpression Elimination (CSE)\nand Loop Invariant Code Motion (LICM), and hence has the potential to greatly\nimprove performance.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 15:46:34 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Dasgupta", "Sandeep", ""], ["Gangwani", "Tanmay", ""]]}, {"id": "1905.08240", "submitter": "Peter Breuer", "authors": "Peter T. Breuer", "title": "Safe and Chaotic Compilation for Hidden Deterministic Hardware Aliasing", "comments": "11 pages. arXiv admin note: text overlap with arXiv:1901.10926", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CR cs.IT cs.PL math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware aliasing occurs when the same logical address can access different\nphysical memory locations. This is a problem for software on some embedded\nsystems and more generally when hardware becomes faulty in irretrievable\nlocations, such as on a Mars Lander. We show how to work around the hardware\nproblem with software logic, compiling code so it works on any platform with\nhardware aliasing with hidden determinism. That is: (i) a copy of an address\naccesses the same location, and (ii) repeating an address calculation exactly\nwill repeat the same access again. Stuck bits can mean that even adding zero to\nan address can make a difference in that environment so nothing but a\nsystematic approach has a chance of working. The technique is extended to\ngenerate aliasing as well as compensate for it, in so-called chaotic\ncompilation, and a sketch proof is included to show it may produce object code\nthat is secure against discovery of the programmer's intention. A prototype\ncompiler implementing the technology covers all of ANSI C except\nlongjmp/setjmp.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 00:42:38 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Breuer", "Peter T.", ""]]}, {"id": "1905.08325", "submitter": "Omer Katz", "authors": "Omer Katz, Yuval Olshaker, Yoav Goldberg, Eran Yahav", "title": "Towards Neural Decompilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of automatic decompilation, converting a program in\nlow-level representation back to a higher-level human-readable programming\nlanguage. The problem of decompilation is extremely important for security\nresearchers. Finding vulnerabilities and understanding how malware operates is\nmuch easier when done over source code.\n  The importance of decompilation has motivated the construction of\nhand-crafted rule-based decompilers. Such decompilers have been designed by\nexperts to detect specific control-flow structures and idioms in low-level code\nand lift them to source level. The cost of supporting additional languages or\nnew language features in these models is very high.\n  We present a novel approach to decompilation based on neural machine\ntranslation. The main idea is to automatically learn a decompiler from a given\ncompiler. Given a compiler from a source language S to a target language T ,\nour approach automatically trains a decompiler that can translate (decompile) T\nback to S . We used our framework to decompile both LLVM IR and x86 assembly to\nC code with high success rates. Using our LLVM and x86 instantiations, we were\nable to successfully decompile over 97% and 88% of our benchmarks respectively.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 20:02:53 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Katz", "Omer", ""], ["Olshaker", "Yuval", ""], ["Goldberg", "Yoav", ""], ["Yahav", "Eran", ""]]}, {"id": "1905.08364", "submitter": "Samuel Drews", "authors": "Samuel Drews, Aws Albarghouthi, and Loris D'Antoni", "title": "Efficient Synthesis with Probabilistic Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of synthesizing a program given a probabilistic\nspecification of its desired behavior. Specifically, we study the recent\nparadigm of distribution-guided inductive synthesis (DIGITS), which iteratively\ncalls a synthesizer on finite sample sets from a given distribution. We make\ntheoretical and algorithmic contributions: (i) We prove the surprising result\nthat DIGITS only requires a polynomial number of synthesizer calls in the size\nof the sample set, despite its ostensibly exponential behavior. (ii) We present\na property-directed version of DIGITS that further reduces the number of\nsynthesizer calls, drastically improving synthesis performance on a range of\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 22:10:47 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Drews", "Samuel", ""], ["Albarghouthi", "Aws", ""], ["D'Antoni", "Loris", ""]]}, {"id": "1905.08505", "submitter": "Dirk Beyer", "authors": "Dirk Beyer and Heike Wehrheim", "title": "Verification Artifacts in Cooperative Verification: Survey and Unifying\n  Component Framework", "comments": "22 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of cooperative verification is to combine verification approaches in\nsuch a way that they work together to verify a system model. In particular,\ncooperative verifiers provide exchangeable information (verification artifacts)\nto other verifiers or consume such information from other verifiers with the\ngoal of increasing the overall effectiveness and efficiency of the verification\nprocess. This paper first gives an overview over approaches for leveraging\nstrengths of different techniques, algorithms, and tools in order to increase\nthe power and abilities of the state of the art in software verification.\nSecond, we specifically outline cooperative verification approaches and discuss\ntheir employed verification artifacts. We formalize all artifacts in a uniform\nway, thereby fixing their semantics and providing verifiers with a precise\nmeaning of the exchanged information.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 08:57:10 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Beyer", "Dirk", ""], ["Wehrheim", "Heike", ""]]}, {"id": "1905.08921", "submitter": "Markus Lepper", "authors": "Markus Lepper and Baltasar Tranc\\'on y Widemann", "title": "D2d -- XML for Authors", "comments": "10 pages, 4 tables (Technical Report -- Bad Honnef 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  D2d is an input format which allows experienced authors to create type\ncorrect xml text objects with minimal disturbance of the creative flow of\nwriting. This paper contains the complete specification of the parsing process,\nincluding the generation of error messages.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 16:12:06 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Lepper", "Markus", ""], ["Widemann", "Baltasar Tranc\u00f3n y", ""]]}, {"id": "1905.09137", "submitter": "Kunal Banerjee", "authors": "Kunal Banerjee, Chandan Karfa", "title": "A Quick Introduction to Functional Verification of Array-Intensive\n  Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Array-intensive programs are often amenable to parallelization across many\ncores on a single machine as well as scaling across multiple machines and hence\nare well explored, especially in the domain of high-performance computing.\nThese programs typically undergo loop transformations and arithmetic\ntransformations in addition to parallelizing transformations. Although a lot of\neffort has been invested in improving parallelizing compilers, experienced\nprogrammers still resort to hand-optimized transformations which is typically\nfollowed by careful tuning of the transformed program to finally obtain the\noptimized program. Therefore, it is critical to verify that the functional\ncorrectness of an original sequential program is not sacrificed during the\nprocess of optimization. In this paper, we cover important literature on\nfunctional verification of array-intensive programs which we believe can be a\ngood starting point for one interested in this field.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 13:43:28 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Banerjee", "Kunal", ""], ["Karfa", "Chandan", ""]]}, {"id": "1905.09242", "submitter": "Anthony Vandikas", "authors": "Azadeh Farzan, Anthony Vandikas", "title": "Reductions for Automated Hypersafety Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an automated verification technique for hypersafety properties,\nwhich express sets of valid interrelations between multiple finite runs of a\nprogram. The key observation is that constructing a proof for a small\nrepresentative set of the runs of the product program (i.e. the product of the\nseveral copies of the program by itself), called a reduction, is sufficient to\nformally prove the hypersafety property about the program. We propose an\nalgorithm based on a counterexample-guided refinement loop that simultaneously\nsearches for a reduction and a proof of the correctness for the reduction. We\ndemonstrate that our tool Weaver is very effective in verifying a diverse array\nof hypersafety properties for a diverse class of input programs.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 17:01:28 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Farzan", "Azadeh", ""], ["Vandikas", "Anthony", ""]]}, {"id": "1905.09423", "submitter": "Joseph Eremondi", "authors": "Joseph Eremondi", "title": "Set Constraints, Pattern Match Analysis, and SMT", "comments": "Accepted in Post-Conference Proceedings of TFP 2019. Recipient of the\n  John McCarthy Best Paper award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set constraints provide a highly general way to formulate program analyses.\nHowever, solving arbitrary boolean combinations of set constraints is\nNEXPTIME-hard. Moreover, while theoretical algorithms to solve arbitrary set\nconstraints exist, they are either too complex to realistically implement or\ntoo slow to ever run.\n  We present a translation that converts a set constraint formula into an SMT\nproblem. Our technique allows for arbitrary boolean combinations of set\nconstraints, and leverages the performance of modern SMT solvers. To show the\nusefulness of unrestricted set constraints, we use them to devise a pattern\nmatch analysis for functional languages, which ensures that missing cases of\npattern matches are always unreachable. We implement our analysis in the Elm\ncompiler and show that our translation is fast enough to be used in practical\nverification.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 01:37:32 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 14:13:08 GMT"}, {"version": "v3", "created": "Sun, 1 Mar 2020 18:52:23 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Eremondi", "Joseph", ""]]}, {"id": "1905.09610", "submitter": "Lu\\'is Cruz-Filipe", "authors": "Lu\\'is Cruz-Filipe, Gra\\c{c}a Gaspar, Isabel Nunes", "title": "Hypothetical answers to continuous queries over data streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous queries over data streams may suffer from blocking operations\nand/or unbound wait, which may delay answers until some relevant input arrives\nthrough the data stream. These delays may turn answers, when they arrive,\nobsolete to users who sometimes have to make decisions with no help whatsoever.\nTherefore, it can be useful to provide hypothetical answers - \"given the\ncurrent information, it is possible that X will become true at time t\" -\ninstead of no information at all.\n  In this paper we present a semantics for queries and corresponding answers\nthat covers such hypothetical answers, together with an online algorithm for\nupdating the set of facts that are consistent with the currently available\ninformation.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 12:11:51 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 11:26:13 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Cruz-Filipe", "Lu\u00eds", ""], ["Gaspar", "Gra\u00e7a", ""], ["Nunes", "Isabel", ""]]}, {"id": "1905.09721", "submitter": "Yipeng Huang", "authors": "Yipeng Huang, Margaret Martonosi", "title": "Statistical Assertions for Validating Patterns and Finding Bugs in\n  Quantum Programs", "comments": "In The 46th Annual International Symposium on Computer Architecture\n  (ISCA '19). arXiv admin note: text overlap with arXiv:1811.05447", "journal-ref": null, "doi": "10.1145/3307650.3322213", "report-no": null, "categories": "quant-ph cs.ET cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In support of the growing interest in quantum computing experimentation,\nprogrammers need new tools to write quantum algorithms as program code.\nCompared to debugging classical programs, debugging quantum programs is\ndifficult because programmers have limited ability to probe the internal states\nof quantum programs; those states are difficult to interpret even when\nobservations exist; and programmers do not yet have guidelines for what to\ncheck for when building quantum programs. In this work, we present quantum\nprogram assertions based on statistical tests on classical observations. These\nallow programmers to decide if a quantum program state matches its expected\nvalue in one of classical, superposition, or entangled types of states. We\nextend an existing quantum programming language with the ability to specify\nquantum assertions, which our tool then checks in a quantum program simulator.\nWe use these assertions to debug three benchmark quantum programs in factoring,\nsearch, and chemistry. We share what types of bugs are possible, and lay out a\nstrategy for using quantum programming patterns to place assertions and prevent\nbugs.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 20:24:50 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Huang", "Yipeng", ""], ["Martonosi", "Margaret", ""]]}, {"id": "1905.09825", "submitter": "Felix Klein", "authors": "Bernd Finkbeiner, Felix Klein, Ruzica Piskac, Mark Santolucito", "title": "Synthesizing Functional Reactive Programs", "comments": "arXiv admin note: text overlap with arXiv:1712.00246", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional Reactive Programming (FRP) is a paradigm that has simplified the\nconstruction of reactive programs. There are many libraries that implement\nincarnations of FRP, using abstractions such as Applicative, Monads, and\nArrows. However, finding a good control flow, that correctly manages state and\nswitches behaviors at the right times, still poses a major challenge to\ndevelopers. An attractive alternative is specifying the behavior instead of\nprogramming it, as made possible by the recently developed logic: Temporal\nStream Logic (TSL). However, it has not been explored so far how Control Flow\nModels (CFMs), as synthesized from TSL specifications, can be turned into\nexecutable code that is compatible with libraries building on FRP. We bridge\nthis gap, by showing that CFMs are indeed a suitable formalism to be turned\ninto Applicative, Monadic, and Arrowized FRP. We demonstrate the effectiveness\nof our translations on a real-world kitchen timer application, which we\ntranslate to a desktop application using the Arrowized FRP library Yampa, a web\napplication using the Monadic threepenny-gui library, and to hardware using the\nApplicative hardware description language ClaSH.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 10:44:07 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Finkbeiner", "Bernd", ""], ["Klein", "Felix", ""], ["Piskac", "Ruzica", ""], ["Santolucito", "Mark", ""]]}, {"id": "1905.09996", "submitter": "Peizun Liu", "authors": "Peizun Liu and Thomas Wahl and Akash LaL", "title": "Verifying Asynchronous Event-Driven Programs Using Partial Abstract\n  Transformers (Extended Manuscript)", "comments": "This is an extended technical report of a paper published in CAV 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of analyzing asynchronous event-driven programs, in\nwhich concurrent agents communicate via unbounded message queues. The safety\nverification problem for such programs is undecidable. We present in this paper\na technique that combines queue-bounded exploration with a convergence test: if\nthe sequence of certain abstractions of the reachable states, for increasing\nqueue bounds k, converges, we can prove any property of the program that is\npreserved by the abstraction. If the abstract state space is finite,\nconvergence is guaranteed; the challenge is to catch the point k_max where it\nhappens. We further demonstrate how simple invariants formulated over the\nconcrete domain can be used to eliminate spurious abstract states, which\notherwise prevent the sequence from converging. We have implemented our\ntechnique for the P programming language for event-driven programs. We show\nexperimentally that the sequence of abstractions often converges fully\nautomatically, in hard cases with minimal designer support in the form of\nsequentially provable invariants, and that this happens for a value of k_max\nsmall enough to allow the method to succeed in practice.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 01:48:26 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Liu", "Peizun", ""], ["Wahl", "Thomas", ""], ["LaL", "Akash", ""]]}, {"id": "1905.10065", "submitter": "Bernadette Spieler", "authors": "Bernadette Spieler and Wolfgang Slany", "title": "A Customised App to Attract Female Teenagers to Coding", "comments": "10 pages, 6 figures, Proceedings of the 2nd International Conference\n  on Gender Research ICGR 2019 Rom, Italy, 11-12 April 2019 isbn:\n  978-1-912764-16-7,issn: 2516-2810, Academic Conferences and Publishing\n  International Limited", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CY cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The number of women in IT-related disciplines is far below the number of men,\nespecially in developed countries. Middle-school girls appear to be engaged in\ncoding courses, but when they choose academic majors relevant to their future\ncareers, only few pursue computer science as a major. In order to show students\na new way of learning and to engage them with coding activities, we used the\nlearning app Pocket Code. In the \"No One Left Behind\" H2020 European project,\nthe app was evaluated in several school subjects. An evaluation of the\nattractiveness of the app shows that students were motivated by Pocket Code's\nease of use and its appealing design; however, girls rated the app less\nenthusiastically. To appeal to female teenagers in particular, a tailored\nversion of the app \"Luna&Cat\" has been developed. This customised version\nstands in contrast to the \"one size fits all\" solution Pocket Code, which may\ndiscourage certain user groups. For apps to have a higher chance to appeal to a\nspecific target group, it is, among many other points, necessary to optimise\ntheir store listing on app stores, especially as we found that app stores are\nthe most effective way to reach teenagers. Thus, this paper covers the\nfollowing research question. What customizations are necessary in Pocket Code\nto reinforce female teenagers in their coding activities? To answer this\nquestion, a focus group discussion was performed. This discussion first brought\ninsights about our target group and suggested names and designs for the new\napp; and second, allowed each student to make proposals for their desired\ngames. Later, these game ideas were analysed, graphically designed, and further\ndeveloped together with university design students. By showing female teenagers\ngames designed by other young women in their age group, we help them to get\nideas and inspiration to code their own programs.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 07:21:07 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Spieler", "Bernadette", ""], ["Slany", "Wolfgang", ""]]}, {"id": "1905.10728", "submitter": "Jan Malakhovski", "authors": "Jan Malakhovski and Sergei Soloviev", "title": "Programming with Applicative-like expressions", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fact that Applicative type class allows one to express simple parsers in\na variable-less combinatorial style is well appreciated among Haskell\nprogrammers for its conceptual simplicity, ease of use, and usefulness for\nsemi-automated code generation (metaprogramming).\n  We notice that such Applicative computations can be interpreted as providing\na mechanism to construct a data type with \"ports\" \"pluggable\" by\nsubcomputations. We observe that it is this property that makes them so much\nmore convenient in practice than the usual way of building the same\ncomputations using conventional composition. We distill this observation into a\nmore general algebraic structure of (and/or technique for expressing)\n\"Applicative-like\" computations and demonstrate several other instances of this\nstructure.\n  Our interest in all of this comes from the fact that the aforementioned\ninstances allow us to express arbitrary transformations between simple data\ntypes of a single constructor (similarly to how Applicative parsing allows to\ntransform from streams of Chars to such data types) using a style that closely\nfollows conventional Applicative computations, thus greatly simplifying (if not\ncompletely automating away) a lot of boiler-plate code present in many\nfunctional programs.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 04:56:24 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Malakhovski", "Jan", ""], ["Soloviev", "Sergei", ""]]}, {"id": "1905.10855", "submitter": "Martin Sulzmann", "authors": "Martin Sulzmann and Kai Stadtm\\\"uller", "title": "Data Race Prediction for Inaccurate Traces", "comments": "26 pages with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Happens-before based data race prediction methods infer from a trace of\nevents a partial order to check if one event happens before another event. If\ntwo two write events are unordered, they are in a race. We observe that common\ntracing methods provide no guarantee that the trace order corresponds to an\nactual program run. The consequence of inaccurate tracing is that results\n(races) reported are inaccurate. We introduce diagnostic methods to examine if\n(1) a race is guaranteed to be correct regardless of any potential\ninaccuracies, (2) maybe is incorrect due to inaccurate tracing. We have fully\nimplemented the approach and provide for an empirical comparison with state of\nthe art happens-before based race predictors such as FastTrack and SHB.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 18:51:24 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 07:57:50 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Sulzmann", "Martin", ""], ["Stadtm\u00fcller", "Kai", ""]]}, {"id": "1905.11445", "submitter": "Ke Wang", "authors": "Ke Wang, Mihai Christodorescu", "title": "COSET: A Benchmark for Evaluating Neural Program Embeddings", "comments": "8 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural program embedding can be helpful in analyzing large software, a task\nthat is challenging for traditional logic-based program analyses due to their\nlimited scalability. A key focus of recent machine-learning advances in this\narea is on modeling program semantics instead of just syntax. Unfortunately\nevaluating such advances is not obvious, as program semantics does not lend\nitself to straightforward metrics. In this paper, we introduce a benchmarking\nframework called COSET for standardizing the evaluation of neural program\nembeddings. COSET consists of a diverse dataset of programs in source-code\nformat, labeled by human experts according to a number of program properties of\ninterest. A point of novelty is a suite of program transformations included in\nCOSET. These transformations when applied to the base dataset can simulate\nnatural changes to program code due to optimization and refactoring and can\nserve as a \"debugging\" tool for classification mistakes. We conducted a pilot\nstudy on four prominent models: TreeLSTM, gated graph neural network (GGNN),\nAST-Path neural network (APNN), and DYPRO. We found that COSET is useful in\nidentifying the strengths and limitations of each model and in pinpointing\nspecific syntactic and semantic characteristics of programs that pose\nchallenges.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 18:44:54 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Wang", "Ke", ""], ["Christodorescu", "Mihai", ""]]}, {"id": "1905.12292", "submitter": "Sanket Tavarageri", "authors": "Sanket Tavarageri", "title": "Categorization of Program Regions for Agile Compilation using Machine\n  Learning and Hardware Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A compiler processes the code written in a high level language and produces\nmachine executable code. The compiler writers often face the challenge of\nkeeping the compilation times reasonable. That is because aggressive\noptimization passes which potentially will give rise to high performance are\noften expensive in terms of running time and memory footprint. Consequently the\ncompiler designers arrive at a compromise where they either simplify the\noptimization algorithm which may decrease the performance of the produced code,\nor they will restrict the optimization to the subset of the overall input\nprogram in which case large parts of the input application will go\nun-optimized.\n  The problem we address in this paper is that of keeping the compilation times\nreasonable, and at the same time optimizing the input program to the fullest\nextent possible. Consequently, the performance of the produced code will match\nthe performance when all the aggressive optimization passes are applied over\nthe entire input program.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 09:46:49 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Tavarageri", "Sanket", ""]]}, {"id": "1905.12444", "submitter": "Andrzej Blikle", "authors": "Andrzej Blikle", "title": "An Experiment with a User Manual of a Programming Language Based on a\n  Denotational Semantics", "comments": "47 pages. This paper is based on my preprinted book 'A Denotational\n  Engineering of Programming Languages' (available on arXiv:1905.01473), which\n  describes a virtual programming language whose manual is discussed in the\n  paper. To make the paper possibly self-contained large parts of the book are\n  quoted in the paper. arXiv admin note: substantial text overlap with\n  arXiv:1905.01469", "journal-ref": null, "doi": "10.13140/RG.2.2.23355.67366", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Denotational models should provide an opportunity for the revision of current\npractices seen in the manuals of programming languages. New styles should on\none hand base on denotational models but on the other - do not assume that\ntoday readers are acquainted in this field. A manual should, therefore, provide\nsome basic knowledge and notation needed to understand the definition of a\nprogramming language written in a new style. At the same time - I strongly\nbelieve that - it should be written for professional programmers rather than\nfor amateurs. The role of a manual is not to teach the skills of programming.\nSuch textbooks are, of course, necessary, but they should tell the readers what\nthe programming is about rather than the technicalities of a concrete language.\nThe paper contains an example of a manual for a virtual programming language\nLingua developed in our project.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 10:25:58 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Blikle", "Andrzej", ""]]}, {"id": "1905.12594", "submitter": "Hengchu Zhang", "authors": "Hengchu Zhang, Edo Roth, Andreas Haeberlen, Benjamin C. Pierce, Aaron\n  Roth", "title": "Fuzzi: A Three-Level Logic for Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Curators of sensitive datasets sometimes need to know whether queries against\nthe data are differentially private [Dwork et al. 2006]. Two sorts of logics\nhave been proposed for checking this property: (1) type systems and other\nstatic analyses, which fully automate straightforward reasoning with concepts\nlike \"program sensitivity\" and \"privacy loss,\" and (2) full-blown program\nlogics such as apRHL (an approximate, probabilistic, relational Hoare logic)\n[Barthe et al. 2016], which support more flexible reasoning about subtle\nprivacy-preserving algorithmic techniques but offer only minimal automation.\n  We propose a three-level logic for differential privacy in an imperative\nsetting and present a prototype implementation called Fuzzi. Fuzzi's lowest\nlevel is a general-purpose logic; its middle level is apRHL; and its top level\nis a novel sensitivity logic adapted from the linear-logic-inspired type system\nof Fuzz, a differentially private functional language [Reed and Pierce 2010].\nThe key novelty is a high degree of integration between the sensitivity logic\nand the two lower-level logics: the judgments and proofs of the sensitivity\nlogic can be easily translated into apRHL; conversely, privacy properties of\nkey algorithmic building blocks can be proved manually in apRHL and the base\nlogic, then packaged up as typing rules that can be applied by a checker for\nthe sensitivity logic to automatically construct privacy proofs for composite\nprograms of arbitrary size.\n  We demonstrate Fuzzi's utility by implementing four different private\nmachine-learning algorithms and showing that Fuzzi's checker is able to derive\ntight sensitivity bounds.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 17:13:41 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Zhang", "Hengchu", ""], ["Roth", "Edo", ""], ["Haeberlen", "Andreas", ""], ["Pierce", "Benjamin C.", ""], ["Roth", "Aaron", ""]]}, {"id": "1905.12734", "submitter": "Nassim Seghir", "authors": "Earl T. Barr and David W. Binkley and Mark Harman and Mohamed Nassim\n  Seghir", "title": "Sub-Turing Islands in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been growing debate as to whether or not static analysis\ncan be truly sound. In spite of this concern, research on techniques seeking to\nat least partially answer undecidable questions has a long history. However,\nlittle attention has been given to the more empirical question of how often an\nexact solution might be given to a question despite the question being, at\nleast in theory, undecidable. This paper investigates this issue by exploring\nsub-Turing islands -- regions of code for which a question of interest is\ndecidable. We define such islands and then consider how to identify them. We\nimplemented Cook, a prototype for finding sub-Turing islands and applied it to\na corpus of 1100 Android applications, containing over 2 million methods.\nResults reveal that 55\\% of the all methods are sub-Turing. Our results also\nprovide empirical, scientific evidence for the scalability of sub-Turing island\nidentification. Sub-Turing identification has many downstream applications,\nbecause islands are so amenable to static analysis. We illustrate two\ndownstream uses of the analysis. In the first, we found that over 37\\% of the\nverification conditions associated with runtime exceptions fell within\nsub-Turing islands and thus are statically decidable. A second use of our\nanalysis is during code review where it provides guidance to developers. The\nsub-Turing islands from our study turns out to contain significantly fewer bugs\nthan `theswamp' (non sub-Turing methods). The greater bug density in the swamp\nis unsurprising; the fact that bugs remain prevalent in islands is, however,\nsurprising: these are bugs whose repair can be fully automated.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 21:16:36 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Barr", "Earl T.", ""], ["Binkley", "David W.", ""], ["Harman", "Mark", ""], ["Seghir", "Mohamed Nassim", ""]]}, {"id": "1905.13334", "submitter": "Rafael-Michael Karampatsis", "authors": "Rafael-Michael Karampatsis and Charles Sutton", "title": "How Often Do Single-Statement Bugs Occur? The ManySStuBs4J Dataset", "comments": "5 pages; to appear in Proceedings of MSR 2020", "journal-ref": null, "doi": "10.1145/3379597.3387491", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program repair is an important but difficult software engineering problem.\nOne way to achieve acceptable performance is to focus on classes of simple\nbugs, such as bugs with single statement fixes, or that match a small set of\nbug templates. However, it is very difficult to estimate the recall of repair\ntechniques for simple bugs, as there are no datasets about how often the\nassociated bugs occur in code. To fill this gap, we provide a dataset of\n153,652 single statement bug-fix changes mined from 1,000 popular open-source\nJava projects, annotated by whether they match any of a set of 16 bug\ntemplates, inspired by state-of-the-art program repair techniques. In an\ninitial analysis, we find that about 33% of the simple bug fixes match the\ntemplates, indicating that a remarkable number of single-statement bugs can be\nrepaired with a relatively small set of templates. Further, we find that\ntemplate fitting bugs appear with a frequency of about one bug per 1,600-2,500\nlines of code (as measured by the size of the project's latest version). We\nhope that the dataset will prove a resource for both future work in program\nrepair and studies in empirical software engineering.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 21:58:19 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 18:33:44 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Karampatsis", "Rafael-Michael", ""], ["Sutton", "Charles", ""]]}, {"id": "1905.13411", "submitter": "Markus N Rabe", "authors": "Markus N. Rabe, Leander Tentrup, Cameron Rasmussen, and Sanjit A.\n  Seshia", "title": "Understanding and Extending Incremental Determinization for 2QBF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental determinization is a recently proposed algorithm for solving\nquantified Boolean formulas with one quantifier alternation. In this paper, we\nformalize incremental determinization as a set of inference rules to help\nunderstand the design space of similar algorithms. We then present additional\ninference rules that extend incremental determinization in two ways. The first\nextension integrates the popular CEGAR principle and the second extension\nallows us to analyze different cases in isolation. The experimental evaluation\ndemonstrates that the extensions significantly improve the performance.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 04:35:07 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Rabe", "Markus N.", ""], ["Tentrup", "Leander", ""], ["Rasmussen", "Cameron", ""], ["Seshia", "Sanjit A.", ""]]}, {"id": "1905.13429", "submitter": "Andr\\'e Platzer", "authors": "Andr\\'e Platzer and Yong Kiam Tan", "title": "Differential Equation Invariance Axiomatization", "comments": "Significantly extended version of arXiv:1802.01226", "journal-ref": "J. ACM 67(1), Article 6, 2020, 66 pages", "doi": "10.1145/3380825", "report-no": null, "categories": "cs.LO cs.PL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proves the completeness of an axiomatization for differential\nequation invariants described by Noetherian functions. First, the differential\nequation axioms of differential dynamic logic are shown to be complete for\nreasoning about analytic invariants. Completeness crucially exploits\ndifferential ghosts, which introduce additional variables that can be chosen to\nevolve freely along new differential equations. Cleverly chosen differential\nghosts are the proof-theoretical counterpart of dark matter. They create new\nhypothetical state, whose relationship to the original state variables\nsatisfies invariants that did not exist before. The reflection of these new\ninvariants in the original system then enables its analysis.\n  An extended axiomatization with existence and uniqueness axioms is complete\nfor all local progress properties, and, with a real induction axiom, is\ncomplete for all semianalytic invariants. This parsimonious axiomatization\nserves as the logical foundation for reasoning about invariants of differential\nequations. Indeed, it is precisely this logical treatment that enables the\ngeneralization of completeness to the Noetherian case.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 06:04:32 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 02:26:02 GMT"}, {"version": "v3", "created": "Sat, 8 Feb 2020 01:55:19 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Platzer", "Andr\u00e9", ""], ["Tan", "Yong Kiam", ""]]}, {"id": "1905.13674", "submitter": "Konstantin L\\\"aufer", "authors": "Gerald Baumgartner, Konstantin L\\\"aufer, Vincent F. Russo", "title": "On the Interaction of Object-Oriented Design Patterns and Programming\n  Languages", "comments": "Purdue University Department of Computer Science Technical Report", "journal-ref": null, "doi": null, "report-no": "96-020", "categories": "cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Design patterns are distilled from many real systems to catalog common\nprogramming practice. However, some object-oriented design patterns are\ndistorted or overly complicated because of the lack of supporting programming\nlanguage constructs or mechanisms. For this paper, we have analyzed several\npublished design patterns looking for idiomatic ways of working around\nconstraints of the implementation language. From this analysis, we lay a\ngroundwork of general-purpose language constructs and mechanisms that, if\nprovided by a statically typed, object-oriented language, would better support\nthe implementation of design patterns and, transitively, benefit the\nconstruction of many real systems. In particular, our catalog of language\nconstructs includes subtyping separate from inheritance, lexically scoped\nclosure objects independent of classes, and multimethod dispatch. The proposed\nconstructs and mechanisms are not radically new, but rather are adopted from a\nvariety of languages and programming language research and combined in a new,\northogonal manner. We argue that by describing design patterns in terms of the\nproposed constructs and mechanisms, pattern descriptions become simpler and,\ntherefore, accessible to a larger number of language communities. Constructs\nand mechanisms lacking in a particular language can be implemented using\nparadigmatic idioms.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 15:27:42 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Baumgartner", "Gerald", ""], ["L\u00e4ufer", "Konstantin", ""], ["Russo", "Vincent F.", ""]]}, {"id": "1905.13706", "submitter": "Richard Eisenberg", "authors": "Stephanie Weirich, Pritam Choudhury, Antoine Voizard, Richard A.\n  Eisenberg", "title": "A Role for Dependent Types in Haskell (Extended version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Haskell supports zero-cost coercions, a mechanism where types that\nshare the same run-time representation may be freely converted between. To make\nsure such conversions are safe and desirable, this feature relies on a\nmechanism of roles to prohibit invalid coercions. In this work, we show how to\nintegrate roles with dependent type systems and prove, using the Coq proof\nassistant, that the resulting system is sound. We have designed this work as a\nfoundation for the addition of dependent types to the Glasgow Haskell Compiler,\nbut we also expect that it will be of use to designers of other\ndependently-typed languages who might want to adopt Haskell's safe coercions\nfeature.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 16:35:23 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Weirich", "Stephanie", ""], ["Choudhury", "Pritam", ""], ["Voizard", "Antoine", ""], ["Eisenberg", "Richard A.", ""]]}, {"id": "1905.13716", "submitter": "Beatrice {\\AA}kerblom", "authors": "Beatrice \\~A kerblom (Stockholm University, Sweden), Elias Castegren\n  (KTH Royal Institute of Technology, Sweden), Tobias Wrigstad (Uppsala\n  University, Sweden)", "title": "Reference Capabilities for Safe Parallel Array Programming", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2020, Vol. 4,\n  Issue 1, Article 1", "doi": "10.22152/programming-journal.org/2020/4/1", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The array is a fundamental data structure that provides an efficient way to\nstore and retrieve non-sparse data contiguous in memory. Arrays are important\nfor the performance of many memory-intensive applications due to the design of\nmodern memory hierarchies: contiguous storage facilitates spatial locality and\npredictive access patterns which enables prefetching. Operations on large\narrays often lend themselves well to parallelisation, such as a fork-join style\ndivide-and-conquer algorithm for sorting. For parallel operations on arrays to\nbe deterministic, data-race freedom must be guaranteed. For operations on\narrays of primitive data, data-race freedom is obtained by coordinating\naccesses so that no two threads operate on the same array indices. This is\nhowever not enough for arrays of non-primitives due to aliasing: accesses of\nseparate array elements may return pointers to the same object, or overlapping\nstructures. Reference capabilities have been used successfully in the past to\nstatically guarantee the absence of data-races in object-oriented programs.\nThis paper presents the first extension of reference capabilities -- called\narray capabilities -- that support concurrent and parallel operations on arrays\nof both primitive and non-primitive values. In addition to element access,\narray capabilities support the abstract manipulation of arrays, logical\nsplitting of arrays into subarrays, and merging subarrays. These operations\nallow expressing a wide range of array use cases. This paper presents the array\ncapability design space and show how it applies to a number of array use cases.\nThe core ideas are formalised and proven sound in a simple calculus, along with\na proof that shows that well-typed programs with array capabilities are free\nfrom data-races.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 16:53:22 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 09:41:12 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["kerblom", "Beatrice \u00c3", "", "Stockholm University, Sweden"], ["Castegren", "Elias", "", "KTH Royal Institute of Technology, Sweden"], ["Wrigstad", "Tobias", "", "Uppsala\n  University, Sweden"]]}]