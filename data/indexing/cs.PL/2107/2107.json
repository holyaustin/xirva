[{"id": "2107.00063", "submitter": "HeuiChan Lim", "authors": "HeuiChan Lim and Stephen Kobourov", "title": "Visualizing The Intermediate Representation of Just-in-Time Compilers", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Just-in-Time (JIT) compilers are used by many modern programming systems in\norder to improve performance. Bugs in JIT compilers provide exploitable\nsecurity vulnerabilities and debugging them is difficult as they are large,\ncomplex, and dynamic. Current debugging and visualization tools deal with\nstatic code and are not suitable in this domain. We describe a new approach for\nsimplifying the large and complex intermediate representation, generated by a\nJIT compiler and visualize it with a metro map metaphor to aid developers in\ndebugging. Experiments using our prototype implementation on Google's V8\nJavaScript interpreter and TurboFan JIT compiler demonstrate that it can help\nidentify and localize buggy code.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 04:20:46 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Lim", "HeuiChan", ""], ["Kobourov", "Stephen", ""]]}, {"id": "2107.00064", "submitter": "Yu Chen", "authors": "Jialiang Tan, Yu Chen, Zhenming Liu, Bin Ren, Shuaiwen Leon Song,\n  Xipeng Shen and Xu Liu", "title": "Toward Efficient Interactions between Python and Native Libraries", "comments": "In Proceedings of the 29th ACM Joint European Software Engineering\n  Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE\n  2021), August 23-27, 2021, Athens, Greece. ACM, New York,NY, USA, 12 pages", "journal-ref": null, "doi": "10.1145/3468264.3468541", "report-no": null, "categories": "cs.PL cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Python has become a popular programming language because of its excellent\nprogrammability. Many modern software packages utilize Python for high-level\nalgorithm design and depend on native libraries written in C/C++/Fortran for\nefficient computation kernels. Interaction between Python code and native\nlibraries introduces performance losses because of the abstraction lying on the\nboundary of Python and native libraries. On the one side, Python code,\ntypically run with interpretation, is disjoint from its execution behavior. On\nthe other side, native libraries do not include program semantics to understand\nalgorithm defects.\n  To understand the interaction inefficiencies, we extensively study a large\ncollection of Python software packages and categorize them according to the\nroot causes of inefficiencies. We extract two inefficiency patterns that are\ncommon in interaction inefficiencies. Based on these patterns, we develop\nPieProf, a lightweight profiler, to pinpoint interaction inefficiencies in\nPython applications. The principle of PieProf is to measure the inefficiencies\nin the native execution and associate inefficiencies with high-level Python\ncode to provide a holistic view. Guided by PieProf, we optimize 17 real-world\napplications, yielding speedups up to 6.3$\\times$ on application level.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 00:48:02 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Tan", "Jialiang", ""], ["Chen", "Yu", ""], ["Liu", "Zhenming", ""], ["Ren", "Bin", ""], ["Song", "Shuaiwen Leon", ""], ["Shen", "Xipeng", ""], ["Liu", "Xu", ""]]}, {"id": "2107.00086", "submitter": "Clement Aubert", "authors": "Cl\\'ement Aubert, Thomas Rubiano (LIPN), Neea Rusch, Thomas Seiller\n  (LIPN, CNRS)", "title": "An extended and more practical mwp flow analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.PL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We improve and refine a method for certifying that the values' sizes computed\nby an imperative program will be bounded by polynomials in the program's\ninputs' sizes. Our work ''tames'' the non-determinism of the original analysis,\nand offers an innovative way of completing the analysis when a non-polynomial\ngrowth is found. We furthermore enrich the analyzed language by adding function\ndefinitions and calls, allowing to compose the analysis of different libraries\nand offering generally more modularity. The implementation of our improved\nmethod, discussed in a tool paper\n(https://hal.archives-ouvertes.fr/hal-03269121), also required to reason about\nthe efficiency of some of the needed operations on the matrices produced by the\nanalysis. It is our hope that this work will enable and facilitate static\nanalysis of source code to guarantee its correctness with respect to resource\nusages.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 08:18:00 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 09:45:47 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Aubert", "Cl\u00e9ment", "", "LIPN"], ["Rubiano", "Thomas", "", "LIPN"], ["Rusch", "Neea", "", "LIPN, CNRS"], ["Seiller", "Thomas", "", "LIPN, CNRS"]]}, {"id": "2107.00093", "submitter": "Craig Innes", "authors": "Craig Innes, Yordan Hristov, Georgios Kamaras, Subramanian Ramamoorthy", "title": "Automatic Synthesis of Experiment Designs from Probabilistic Environment\n  Specifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an extension to the probabilistic programming language\nProbRobScene, allowing users to automatically synthesize uniform experiment\ndesigns directly from environment specifications. We demonstrate its\neffectiveness on a number of environment specification snippets from tabletop\nmanipulation, and show that our method generates reliably low-discrepancy\ndesigns.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 14:58:31 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Innes", "Craig", ""], ["Hristov", "Yordan", ""], ["Kamaras", "Georgios", ""], ["Ramamoorthy", "Subramanian", ""]]}, {"id": "2107.00097", "submitter": "Clement Aubert", "authors": "Cl\\'ement Aubert, Thomas Rubiano (LIPN), Neea Rusch, Thomas Seiller\n  (LIPN, CNRS)", "title": "An implementation of flow calculus for complexity analysis (tool paper)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CC cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract. We present a tool to automatically perform the data-size analysis\nof imperative programs written in C. This tool, called pymwp, is inspired by a\nclassical work on complexity analysis [10], and allows to certify that the size\nof the values computed by a program will be bounded by a polynomial in the\nprogram's inputs. Strategies to provide meaningful feedback on non-polynomial\nprograms and to ``tame'' the non-determinism of the original analysis were\nimplemented following recent progresses [3], but required particular care to\naccommodate the growing complexity of the analysis. The Python source code is\nintensively documented, and our numerous example files encompass the original\nexamples as well as multiple test cases. A pip package should make it easy to\ninstall pymwp on any plat-form, but an on-line demo is also available for\nconvenience.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 08:19:32 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 09:44:05 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Aubert", "Cl\u00e9ment", "", "LIPN"], ["Rubiano", "Thomas", "", "LIPN"], ["Rusch", "Neea", "", "LIPN, CNRS"], ["Seiller", "Thomas", "", "LIPN, CNRS"]]}, {"id": "2107.00101", "submitter": "Xinyun Chen", "authors": "Xinyun Chen, Dawn Song, Yuandong Tian", "title": "Latent Execution for Neural Program Synthesis Beyond Domain-Specific\n  Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program synthesis from input-output examples has been a long-standing\nchallenge, and recent works have demonstrated some success in designing deep\nneural networks for program synthesis. However, existing efforts in\ninput-output neural program synthesis have been focusing on domain-specific\nlanguages, thus the applicability of previous approaches to synthesize code in\nfull-fledged popular programming languages, such as C, remains a question. The\nmain challenges lie in two folds. On the one hand, the program search space\ngrows exponentially when the syntax and semantics of the programming language\nbecome more complex, which poses higher requirements on the synthesis\nalgorithm. On the other hand, increasing the complexity of the programming\nlanguage also imposes more difficulties on data collection, since building a\nlarge-scale training set for input-output program synthesis require random\nprogram generators to sample programs and input-output examples. In this work,\nwe take the first step to synthesize C programs from input-output examples. In\nparticular, we propose LaSynth, which learns the latent representation to\napproximate the execution of partially generated programs, even if their\nsemantics are not well-defined. We demonstrate the possibility of synthesizing\nelementary C code from input-output examples, and leveraging learned execution\nsignificantly improves the prediction performance over existing approaches.\nMeanwhile, compared to the randomly generated ground-truth programs, LaSynth\nsynthesizes more concise programs that resemble human-written code. We show\nthat training on these synthesized programs further improves the prediction\nperformance for both Karel and C program synthesis, indicating the promise of\nleveraging the learned program synthesizer to improve the dataset quality for\ninput-output program synthesis.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 02:21:32 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Chen", "Xinyun", ""], ["Song", "Dawn", ""], ["Tian", "Yuandong", ""]]}, {"id": "2107.00111", "submitter": "Mary Southern", "authors": "Gopalan Nadathur and Mary Southern", "title": "A Logic for Reasoning About LF Specifications", "comments": "arXiv admin note: substantial text overlap with arXiv:2105.04110", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a logic named L_{LF} whose intended use is to formalize properties\nof specifications developed in the dependently typed lambda calculus LF. The\nlogic is parameterized by the LF signature that constitutes the specification.\nAtomic formulas correspond to typing derivations relative to this signature.\nThe logic includes a collection of propositional connectives and quantifiers.\nQuantification ranges over expressions that denote LF terms and LF contexts.\nQuantifiers of the first variety are qualified by simple types that describe\nthe functional structure associated with the variables they bind; deeper,\ndependency related properties are expressed by the body of the formula.\nContext-level quantifiers are qualified by context schemas that identify\npatterns of declarations out of which actual contexts may be constructed. The\nsemantics of variable-free atomic formulas is articulated via the derivability\nin LF of the judgements they encode. Propositional constants and connectives\nare understood in the usual manner and the meaning of quantifiers is explicated\nthrough substitutions of expressions that adhere to the type qualifications.\nThe logic is complemented by a proof system that enables reasoning that is\nsound with respect to the described semantics. The main novelties of the proof\nsystem are the provision for case-analysis style reasoning about LF judgements,\nsupport for inductive reasoning over the heights of LF derivations and the\nencoding of LF meta-theorems. The logic is motivated by the paradigmatic\nexample of type assignment in the simply-typed lambda calculus and the proof\nsystem is illustrated through the formalization of a proof of type uniqueness\nfor this calculus.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 21:33:24 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Nadathur", "Gopalan", ""], ["Southern", "Mary", ""]]}, {"id": "2107.00522", "submitter": "Chaitanya Koparkar", "authors": "Chaitanya Koparkar, Mike Rainey, Michael Vollmer, Milind Kulkarni,\n  Ryan R. Newton", "title": "Efficient Tree-Traversals: Reconciling Parallelism and Dense Data\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work showed that compiling functional programs to use dense,\nserialized memory representations for recursive algebraic datatypes can yield\nsignificant constant-factor speedups for sequential programs. But serializing\ndata in a maximally dense format consequently serializes the processing of that\ndata, yielding a tension between density and parallelism. This paper shows that\na disciplined, practical compromise is possible. We present Parallel Gibbon, a\ncompiler that obtains the benefits of dense data formats and parallelism. We\nformalize the semantics of the parallel location calculus underpinning this\nnovel implementation strategy, and show that it is type-safe. Parallel Gibbon\nexceeds the parallel performance of existing compilers for purely functional\nprograms that use recursive algebraic datatypes, including, notably,\nabstract-syntax-tree traversals as in compilers.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 15:08:53 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Koparkar", "Chaitanya", ""], ["Rainey", "Mike", ""], ["Vollmer", "Michael", ""], ["Kulkarni", "Milind", ""], ["Newton", "Ryan R.", ""]]}, {"id": "2107.00555", "submitter": "Alexandros Nikolaos Ziogas", "authors": "Alexandros Nikolaos Ziogas, Timo Schneider, Tal Ben-Nun, Alexandru\n  Calotoiu, Tiziano De Matteis, Johannes de Fine Licht, Luca Lavarini, and\n  Torsten Hoefler", "title": "Productivity, Portability, Performance: Data-Centric Python", "comments": null, "journal-ref": null, "doi": "10.1145/1122445.1122456", "report-no": null, "categories": "cs.PL cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Python has become the de facto language for scientific computing. Programming\nin Python is highly productive, mainly due to its rich science-oriented\nsoftware ecosystem built around the NumPy module. As a result, the demand for\nPython support in High Performance Computing (HPC) has skyrocketed. However,\nthe Python language itself does not necessarily offer high performance. In this\nwork, we present a workflow that retains Python's high productivity while\nachieving portable performance across different architectures. The workflow's\nkey features are HPC-oriented language extensions and a set of automatic\noptimizations powered by a data-centric intermediate representation. We show\nperformance results and scaling across CPU, GPU, FPGA, and the Piz Daint\nsupercomputer (up to 23,328 cores), with 2.47x and 3.75x speedups over\nprevious-best solutions, first-ever Xilinx and Intel FPGA results of annotated\nPython, and up to 93.16% scaling efficiency on 512 nodes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 15:51:18 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ziogas", "Alexandros Nikolaos", ""], ["Schneider", "Timo", ""], ["Ben-Nun", "Tal", ""], ["Calotoiu", "Alexandru", ""], ["De Matteis", "Tiziano", ""], ["Licht", "Johannes de Fine", ""], ["Lavarini", "Luca", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2107.00564", "submitter": "Aart Bik", "authors": "Aart J.C. Bik", "title": "A Note on Exhaustive State Space Search for Efficient Code Generation", "comments": "white paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This note explores state space search to find efficient instruction sequences\nthat perform particular data manipulations. Once found, the instruction\nsequences are hard-wired in the code generator that needs these data\nmanipulations. Since state space is only searched while developing the\ncompiler, search time is not at a premium, which allows exhaustively searching\nfor the best possible instruction sequences.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 04:23:13 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Bik", "Aart J. C.", ""]]}, {"id": "2107.00613", "submitter": "Fengmin Zhu", "authors": "Fengmin Zhu and Fei He", "title": "EqFix: Fixing LaTeX Equation Errors by Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LaTeX is a widely-used document preparation system. Its powerful ability in\nmathematical equation editing is perhaps the main reason for its popularity in\nacademia. Sometimes, however, even an expert user may spend much time on fixing\nan erroneous equation. In this paper, we present EqFix, a synthesis-based\nrepairing system for LaTeX equations. It employs a set of fixing rules, and can\nsuggest possible repairs for common errors in LaTeX equations. A domain\nspecific language is proposed for formally expressing the fixing rules. The\nfixing rules can be automatically synthesized from a set of input-output\nexamples. An extension of relaxer is also introduced to enhance the\npracticality of EqFix. We evaluate EqFix on real-world examples and find that\nit can synthesize rules with high generalization ability. Compared with a\nstate-of-the-art string transformation synthesizer, EqFix solved 37% more cases\nand spent only one third of their synthesis time.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 17:04:56 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Zhu", "Fengmin", ""], ["He", "Fei", ""]]}, {"id": "2107.01155", "submitter": "Alejandro Aguirre", "authors": "Alejandro Aguirre, Gilles Barthe, Marco Gaboardi, Deepak Garg, Shin-ya\n  Katsumata, and Tetsuya Sato", "title": "Higher-order probabilistic adversarial computations: Categorical\n  semantics and program logics", "comments": "Full version of ICFP 21 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial computations are a widely studied class of computations where\nresource-bounded probabilistic adversaries have access to oracles, i.e.,\nprobabilistic procedures with private state. These computations arise routinely\nin several domains, including security, privacy and machine learning. In this\npaper, we develop program logics for reasoning about adversarial computations\nin a higher-order setting. Our logics are built on top of a simply typed\n$\\lambda$-calculus extended with a graded monad for probabilities and state.\nThe grading is used to model and restrict the memory footprint and the cost (in\nterms of oracle calls) of computations. Under this view, an adversary is a\nhigher-order expression that expects as arguments the code of its oracles. We\ndevelop unary program logics for reasoning about error probabilities and\nexpected values, and a relational logic for reasoning about coupling-based\nproperties. All logics feature rules for adversarial computations, and yield\nguarantees that are valid for all adversaries that satisfy a fixed resource\npolicy. We prove the soundness of the logics in the category of quasi-Borel\nspaces, using a general notion of graded predicate liftings, and we use logical\nrelations over graded predicate liftings to establish the soundness of proof\nrules for adversaries. We illustrate the working of our logics with simple but\nillustrative examples.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 15:54:42 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Aguirre", "Alejandro", ""], ["Barthe", "Gilles", ""], ["Gaboardi", "Marco", ""], ["Garg", "Deepak", ""], ["Katsumata", "Shin-ya", ""], ["Sato", "Tetsuya", ""]]}, {"id": "2107.01295", "submitter": "William J. Bowman", "authors": "Stephen Chang, Michael Ballantyne, Milo Turner, William J. Bowman", "title": "Dependent Type Systems as Macros", "comments": null, "journal-ref": "Proceedings of the ACM on Programming Languages, Volume 4, Issue\n  POPL, Article 3. January 2020", "doi": "10.1145/3371071", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Turnstile+, a high-level, macros-based metaDSL for building\ndependently typed languages. With it, programmers may rapidly prototype and\niterate on the design of new dependently typed features and extensions. Or they\nmay create entirely new DSLs whose dependent type \"power\" is tailored to a\nspecific domain. Our framework's support of language-oriented programming also\nmakes it suitable for experimenting with systems of interacting components,\ne.g., a proof assistant and its companion DSLs. This paper explains the\nimplementation details of Turnstile+, as well as how it may be used to create a\nwide-variety of dependently typed languages, from a lightweight one with\nindexed types, to a full spectrum proof assistant, complete with a tactic\nsystem and extensions for features like sized types and SMT interaction.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 22:43:51 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Chang", "Stephen", ""], ["Ballantyne", "Michael", ""], ["Turner", "Milo", ""], ["Bowman", "William J.", ""]]}, {"id": "2107.01542", "submitter": "Gershom Bazerman", "authors": "Gershom Bazerman", "title": "The Semantics of Package Management via Event Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an approach to the semantics of package management which relates\nit to general event structures, well-known mathematical objects used in the\nsemantics of concurrent, nondeterministic systems. In this approach, the data\nof a package repository is treated as a declarative specification of a\nnondeterministic, concurrent program. We introduce a process calculus\ncorresponding to this data, and investigate its operational and categorical\nsemantics. Our hope is this lays the basis for further formal study of package\nmanagement in which the weight of existing tools can be brought to bear.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 05:14:05 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Bazerman", "Gershom", ""]]}, {"id": "2107.01621", "submitter": "Sarah McDaid PhD", "authors": "Edward McDaid, Sarah McDaid", "title": "The Composability of Intermediate Values in Composable Inductive\n  Programming", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is believed that mechanisms including intermediate values enable\ncomposable inductive programming (CIP) to be used to produce software of any\nsize. We present the results of a study that investigated the relationships\nbetween program size, the number of intermediate values and the number of test\ncases used to specify programs using CIP. In the study 96,000 programs of\nvarious sizes were randomly generated, decomposed into fragments and\ntransformed into test cases. The test cases were then used to regenerate new\nversions of the original programs using Zoea. The results show linear\nrelationships between the number of intermediate values and regenerated program\nsize, and between the number of test cases and regenerated program size within\nthe size range studied. In addition, as program size increases there is\nincreasing scope for trading off the number of test cases against the number of\nintermediate values and vice versa.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 13:17:52 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["McDaid", "Edward", ""], ["McDaid", "Sarah", ""]]}, {"id": "2107.01815", "submitter": "Brae Webb", "authors": "Brae J. Webb, Mark Utting, Ian J. Hayes", "title": "A Formal Semantics of the GraalVM Intermediate Representation", "comments": "16 pages, 8 figures, to be published to ATVA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The optimization phase of a compiler is responsible for transforming an\nintermediate representation (IR) of a program into a more efficient form.\nModern optimizers, such as that used in the GraalVM compiler, use an IR\nconsisting of a sophisticated graph data structure that combines data flow and\ncontrol flow into the one structure. As part of a wider project on the\nverification of optimization passes of GraalVM, this paper describes a\nsemantics for its IR within Isabelle/HOL. The semantics consists of a big-step\noperational semantics for data nodes (which are represented in a graph-based\nstatic single assignment (SSA) form) and a small-step operational semantics for\nhandling control flow including heap-based reads and writes, exceptions, and\nmethod calls. We have proved a suite of canonicalization optimizations and\nconditional elimination optimizations with respect to the semantics.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 06:48:18 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Webb", "Brae J.", ""], ["Utting", "Mark", ""], ["Hayes", "Ian J.", ""]]}, {"id": "2107.01883", "submitter": "Sandro Stucki", "authors": "Sandro Stucki and Paolo G. Giarrusso", "title": "A Theory of Higher-Order Subtyping with Type Intervals (Extended\n  Version)", "comments": "73 pages; to be presented at the 26th ACM SIGPLAN International\n  Conference on Functional Programming (ICFP 2021), 22-27 August 2021", "journal-ref": "Proc. ACM Program. Lang. 5 (2021) 69:1-69:30 (ICFP)", "doi": "10.1145/3473574", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The calculus of Dependent Object Types (DOT) has enabled a more principled\nand robust implementation of Scala, but its support for type-level computation\nhas proven insufficient. As a remedy, we propose $F^\\omega_{..}$, a rigorous\ntheoretical foundation for Scala's higher-kinded types. $F^\\omega_{..}$ extends\n$F^\\omega_{<:}$ with interval kinds, which afford a unified treatment of\nimportant type- and kind-level abstraction mechanisms found in Scala, such as\nbounded quantification, bounded operator abstractions, translucent type\ndefinitions and first-class subtyping constraints. The result is a flexible and\ngeneral theory of higher-order subtyping. We prove type and kind safety of\n$F^\\omega_{..}$, as well as weak normalization of types and undecidability of\nsubtyping. All our proofs are mechanized in Agda using a fully syntactic\napproach based on hereditary substitution.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 09:14:53 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Stucki", "Sandro", ""], ["Giarrusso", "Paolo G.", ""]]}, {"id": "2107.02244", "submitter": "Devon Loehr", "authors": "John Sonchack, Devon Loehr, Jennifer Rexford, David Walker", "title": "Lucid: A Language for Control in the Data Plane", "comments": "12 pages plus 5 pages references/appendix. 17 figures. To appear in\n  SIGCOMM 2021", "journal-ref": null, "doi": "10.1145/3452296.3472903", "report-no": null, "categories": "cs.NI cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Programmable switch hardware makes it possible to move fine-grained control\nlogic inside the network data plane, improving performance for a wide range of\napplications. However, applications with integrated control are inherently hard\nto write in existing data-plane programming languages such as P4. This paper\npresents Lucid, a language that raises the level of abstraction for putting\ncontrol functionality in the data plane. Lucid introduces abstractions that\nmake it easy to write sophisticated data-plane applications with interleaved\npacket-handling and control logic, specialized type and syntax systems that\nprevent programmer bugs related to data-plane state, and an open-sourced\ncompiler that translates Lucid programs into P4 optimized for the Intel Tofino.\nThese features make Lucid general and easy to use, as we demonstrate by writing\na suite of ten different data-plane applications in Lucid. Working prototypes\ntake well under an hour to write, even for a programmer without prior Tofino\nexperience, have around 10x fewer lines of code compared to P4, and compile\nefficiently to real hardware. In a stateful firewall written in Lucid, we find\nthat moving control from a switch's CPU to its data-plane processor using Lucid\nreduces the latency of performance-sensitive operations by over 300X.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 19:33:10 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Sonchack", "John", ""], ["Loehr", "Devon", ""], ["Rexford", "Jennifer", ""], ["Walker", "David", ""]]}, {"id": "2107.02346", "submitter": "Divyanjali Sharma", "authors": "Divyanjali Sharma, Subodh Sharma", "title": "Thread-modular Analysis of Release-Acquire Concurrency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a thread-modular abstract interpretation(TMAI) technique to verify\nprograms under the release-acquire (RA) memory model for safety property\nviolations. The main contributions of our work are: we capture the execution\norder of program statements as an abstract domain, and propose a sound upper\napproximation over this domain to efficiently reason over RA concurrency. The\nproposed domain is general in its application and captures the ordering\nrelations as a first-class feature in the abstract interpretation theory. In\nparticular, the domain represents a set of sequences of modifications of a\nglobal variable in concurrent programs as a partially ordered set. Under this\napproximation, older sequenced-before stores of a global variable are forgotten\nand only the latest stores per variable are preserved. We establish the\nsoundness of our proposed abstractions and implement them in a prototype\nabstract interpreter called PRIORI. The evaluations of PRIORI on existing and\nchallenging RA benchmarks demonstrate that the proposed technique is not only\ncompetitive in refutation, but also in verification. PRIORI shows significantly\nfast analysis runtimes with higher precision compared to recent\nstate-of-the-art tools for RA concurrency.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 02:08:09 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 07:34:30 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Sharma", "Divyanjali", ""], ["Sharma", "Subodh", ""]]}, {"id": "2107.02438", "submitter": "Dmitrijs Trizna", "authors": "Dmitrijs Trizna", "title": "Shell Language Processing: Unix command parsing for Machine Learning", "comments": "3 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we present a Shell Language Preprocessing (SLP) library,\nwhich implements tokenization and encoding directed on the parsing of Unix and\nLinux shell commands. We describe the rationale behind the need for a new\napproach with specific examples when conventional Natural Language Processing\n(NLP) pipelines fail. Furthermore, we evaluate our methodology on a security\nclassification task against widely accepted information and communications\ntechnology (ICT) tokenization techniques and achieve significant improvement of\nan F1-score from 0.392 to 0.874.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 07:34:16 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Trizna", "Dmitrijs", ""]]}, {"id": "2107.03155", "submitter": "Pierre Clairambault", "authors": "Pierre Clairambault (LIP, PLUME), Hugo Paquet", "title": "The Quantitative Collapse of Concurrent Games with Symmetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore links between the thin concurrent games of Castellan, Clairambault\nand Winskel, and the weighted relational models of linear logic studied by\nLaird, Manzonetto, McCusker and Pagani. More precisely, we show that there is\nan interpretationpreserving \"collapse\" functor from the former to the latter.\nOn objects, the functor defines for each game a set of possible execution\nstates. Defining the action on morphisms is more subtle, and this is the main\ncontribution of the paper. Given a strategy and an execution state, our functor\nneeds to count the witnesses for this state within the strategy. Strategies in\nthin concurrent games describe non-linear behaviour explicitly, so in general\neach witness exists in countably many symmetric copies. The challenge is to\ndefine the right notion of witnesses, factoring out this infinity while\nmatching the weighted relational model. Understanding how witnesses compose is\nparticularly subtle and requires a delve into the combinatorics of witnesses\nand their symmetries. In its basic form, this functor connects thin concurrent\ngames and a relational model weighted by N $\\cup$ {+$\\infty$}. We will\nadditionally consider a generalised setting where both models are weighted by\nelements of an arbitrary continuous semiring; this covers the probabilistic\ncase, among others. Witnesses now additionally carry a value from the semiring,\nand our interpretation-preserving collapse functor extends to this setting.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 11:32:08 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Clairambault", "Pierre", "", "LIP, PLUME"], ["Paquet", "Hugo", ""]]}, {"id": "2107.03569", "submitter": "Umang Mathur", "authors": "Rucha Kulkarni and Umang Mathur and Andreas Pavlogiannis", "title": "Dynamic Data-Race Detection through the Fine-Grained Lens", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data races are among the most common bugs in concurrency. The standard\napproach to data-race detection is via dynamic analyses, which work over\nexecutions of concurrent programs, instead of the program source code. The rich\nliterature on the topic has created various notions of dynamic data races,\nwhich are known to be detected efficiently when certain parameters (e.g.,\nnumber of threads) are small. However, the \\emph{fine-grained} complexity of\nall these notions of races has remained elusive, making it impossible to\ncharacterize their trade-offs between precision and efficiency.\n  In this work we establish several fine-grained separations between many\npopular notions of dynamic data races. The input is an execution trace with $N$\nevents, $T$ threads and $L$ locks. Our main results are as follows. First, we\nshow that happens-before (HB) races can be detected in $O(N\\cdot \\min(T, L))$\ntime, improving over the standard $O(N\\cdot T)$ bound when $L=o(T)$. Moreover,\nwe show that even reporting an HB race that involves a read access is hard for\n2-orthogonal vectors (2-OV). This is the first rigorous proof of the\nconjectured quadratic lower-bound in detecting HB races. Second, we show that\nthe recently introduced synchronization-preserving races are hard to detect for\nOV-3 and thus have a cubic lower bound, when $T=\\Omega(N)$. This establishes a\ncomplexity separation from HB races which are known to be less expressive.\nThird, we show that lock-cover races are hard for 2-OV, and thus have a\nquadratic lower-bound, even when $T=2$ and $L = \\omega(\\log N)$. The similar\nnotion of lock-set races is known to be detectable in $O(N\\cdot L)$ time, and\nthus we achieve a complexity separation between the two. Moreover, we show that\nlock-set races become hitting-set (HS)-hard when $L=\\Theta(N)$, and thus also\nhave a quadratic lower bound, when the input is sufficiently complex.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 02:21:31 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Kulkarni", "Rucha", ""], ["Mathur", "Umang", ""], ["Pavlogiannis", "Andreas", ""]]}, {"id": "2107.03653", "submitter": "Nikhil Pratap Ghanathe", "authors": "Nikhil Pratap Ghanathe, Vivek Seshadri, Rahul Sharma, Steve Wilton,\n  Aayan Kumar", "title": "MAFIA: Machine Learning Acceleration on FPGAs for IoT Applications", "comments": "Accepted at The International Conference on Field-Programmable Logic\n  and Applications (FPL), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.DC cs.LG cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recent breakthroughs in ML have produced new classes of models that allow ML\ninference to run directly on milliwatt-powered IoT devices. On one hand,\nexisting ML-to-FPGA compilers are designed for deep neural-networks on large\nFPGAs. On the other hand, general-purpose HLS tools fail to exploit properties\nspecific to ML inference, thereby resulting in suboptimal performance. We\npropose MAFIA, a tool to compile ML inference on small form-factor FPGAs for\nIoT applications. MAFIA provides native support for linear algebra operations\nand can express a variety of ML algorithms, including state-of-the-art models.\nWe show that MAFIA-generated programs outperform best-performing variant of a\ncommercial HLS compiler by 2.5x on average.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 07:38:23 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Ghanathe", "Nikhil Pratap", ""], ["Seshadri", "Vivek", ""], ["Sharma", "Rahul", ""], ["Wilton", "Steve", ""], ["Kumar", "Aayan", ""]]}, {"id": "2107.03660", "submitter": "Yushan Zhang", "authors": "Yushan Zhang, Peisen Yao, Rongxin Wu, Charles Zhang", "title": "Duplicate-sensitivity Guided Transformation Synthesis for DBMS\n  Correctness Bug Detection", "comments": "11 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Database Management System (DBMS) plays a core role in modern software from\nmobile apps to online banking. It is critical that DBMS should provide correct\ndata to all applications. When the DBMS returns incorrect data, a correctness\nbug is triggered. Current production-level DBMSs still suffer from insufficient\ntesting due to the limited hand-written test cases. Recently several works\nproposed to automatically generate many test cases with query transformation, a\nprocess of generating an equivalent query pair and testing a DBMS by checking\nwhether the system returns the same result set for both queries. However, all\nof them still heavily rely on manual work to provide a transformation which\nlargely confines their exploration of the valid input query space.\n  This paper introduces duplicate-sensitivity guided transformation synthesis\nwhich automatically finds new transformations by first synthesizing many\ncandidates then filtering the nonequivalent ones. Our automated synthesis is\nachieved by mutating a query while keeping its duplicate sensitivity, which is\na necessary condition for query equivalence. After candidate synthesis, we keep\nthe mutant query which is equivalent to the given one by using a query\nequivalent checker. Furthermore, we have implemented our idea in a tool Eqsql\nand used it to test the production-level DBMSs. In two months, we detected in\ntotal 30 newly confirmed and unique bugs in MySQL, TiDB and CynosDB.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 07:47:35 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Zhang", "Yushan", ""], ["Yao", "Peisen", ""], ["Wu", "Rongxin", ""], ["Zhang", "Charles", ""]]}, {"id": "2107.03984", "submitter": "Felix Stutz", "authors": "Rupak Majumdar, Madhavan Mukund, Felix Stutz, Damien Zufferey", "title": "Generalising Projection in Asynchronous Multiparty Session Types", "comments": "16 pages, 36 pages including appendix; to appear in CONCUR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiparty session types (MSTs) provide an efficient methodology for\nspecifying and verifying message passing software systems. In the theory of\nMSTs, a global type specifies the interaction among the roles at the global\nlevel. A local specification for each role is generated by projecting from the\nglobal type on to the message exchanges it participates in. Whenever a global\ntype can be projected on to each role, the composition of the projections is\ndeadlock free and has exactly the behaviours specified by the global type. The\nkey to the usability of MSTs is the projection operation: a more expressive\nprojection allows more systems to be type-checked but requires a more difficult\nsoundness argument. In this paper, we generalise the standard projection\noperation in MSTs. This allows us to model and type-check many design patterns\nin distributed systems, such as load balancing, that are rejected by the\nstandard projection. The key to the new projection is an analysis that tracks\ncausality between messages. Our soundness proof uses novel graph-theoretic\ntechniques from the theory of message-sequence charts. We demonstrate the\nefficacy of the new projection operation by showing many global types for\ncommon patterns that can be projected under our projection but not under the\nstandard projection operation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 17:24:18 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Majumdar", "Rupak", ""], ["Mukund", "Madhavan", ""], ["Stutz", "Felix", ""], ["Zufferey", "Damien", ""]]}, {"id": "2107.04521", "submitter": "Pooja Rani", "authors": "Pooja Rani, Sebastiano Panichella, Manuel Leuenberger, Andrea Di\n  Sorbo, and Oscar Nierstrasz", "title": "How to Identify Class Comment Types? A Multi-language Approach for Class\n  Comment Classification", "comments": "25 pages, 10 figures, 8 tables", "journal-ref": null, "doi": "10.1016/j.jss.2021.111047", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Most software maintenance and evolution tasks require developers to\nunderstand the source code of their software systems. Software developers\nusually inspect class comments to gain knowledge about program behavior,\nregardless of the programming language they are using. Unfortunately, (i)\ndifferent programming languages present language-specific code commenting\nnotations/guidelines; and (ii) the source code of software projects often lacks\ncomments that adequately describe the class behavior, which complicates program\ncomprehension and evolution activities.\n  To handle these challenges, this paper investigates the different\nlanguage-specific class commenting practices of three programming languages:\nPython, Java, and Smalltalk. In particular, we systematically analyze the\nsimilarities and differences of the information types found in class comments\nof projects developed in these languages.\n  We propose an approach that leverages two techniques, namely Natural Language\nProcessing and Text Analysis, to automatically identify various types of\ninformation from class comments i.e., the specific types of semantic\ninformation found in class comments. To the best of our knowledge, no previous\nwork has provided a comprehensive taxonomy of class comment types for these\nthree programming languages with the help of a common automated approach. Our\nresults confirm that our approach can classify frequent class comment\ninformation types with high accuracy for Python, Java, and Smalltalk\nprogramming languages. We believe this work can help to monitor and assess the\nquality and evolution of code comments in different program languages, and thus\nsupport maintenance and evolution tasks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 16:12:39 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 11:56:10 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Rani", "Pooja", ""], ["Panichella", "Sebastiano", ""], ["Leuenberger", "Manuel", ""], ["Di Sorbo", "Andrea", ""], ["Nierstrasz", "Oscar", ""]]}, {"id": "2107.04663", "submitter": "Yue Niu", "authors": "Yue Niu (1), Jonathan Sterling (1), Harrison Grodin (1), Robert Harper\n  (1) ((1) Carnegie Mellon University)", "title": "A cost-aware logical framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present \\textbf{calf}, a \\textbf{c}ost-\\textbf{a}ware \\textbf{l}ogical\n\\textbf{f}ramework for studying quantitative aspects of functional programs.\nTaking inspiration from recent work that reconstructs traditional aspects of\nprogramming languages in terms of a modal account of \\emph{phase distinctions},\nwe argue that the cost structure of programs motivates a phase distinction\nbetween \\emph{intension} and \\emph{extension}. Armed with this technology, we\ncontribute a synthetic account of cost structure as a computational effect in\nwhich cost-aware programs enjoy an internal noninterference property:\ninput/output behavior cannot depend on cost.\n  As a full-spectrum dependent type theory, \\textbf{calf} presents a unified\nlanguage for programming and specification of both cost and behavior that can\nbe integrated smoothly with existing mathematical libraries available in type\ntheoretic proof assistants. We evaluate \\textbf{calf} as a general framework\nfor cost analysis by implementing two fundamental techniques for algorithm\nanalysis: the \\emph{method of recurrence relations} and \\emph{physicist's\nmethod for amortized analysis}. We deploy these techniques on a variety of case\nstudies: we prove a tight, closed bound for Euclid's algorithm, verify the\namortized complexity of batched queues, and derive tight, closed bounds for the\nsequential and \\emph{parallel} complexity of merge sort, all fully mechanized\nin the Agda proof assistant. Lastly we substantiate the soundness of\nquantitative reasoning in \\textbf{calf} by means of a model construction.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 20:30:16 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Niu", "Yue", "", "Carnegie Mellon University"], ["Sterling", "Jonathan", "", "Carnegie Mellon University"], ["Grodin", "Harrison", "", "Carnegie Mellon University"], ["Harper", "Robert", "", "Carnegie Mellon University"]]}, {"id": "2107.04859", "submitter": "Joseph Eremondi", "authors": "Joseph Eremondi, Ronald Garcia, \\'Eric Tanter", "title": "Approximate Normalization and Eager Equality Checking for Gradual\n  Inductive Families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Harnessing the power of dependently typed languages can be difficult.\nProgrammers must manually construct proofs to produce well-typed programs,\nwhich is not an easy task. In particular, migrating code to these languages is\nchallenging. Gradual typing can make dependently-typed languages easier to use\nby mixing static and dynamic checking in a principled way. With gradual types,\nprogrammers can incrementally migrate code to a dependently typed language.\n  However, adding gradual types to dependent types creates a new challenge:\nmixing decidable type-checking and incremental migration in a full-featured\nlanguage is a precarious balance. Programmers expect type-checking to\nterminate, but dependent type-checkers evaluate terms at compile time, which is\nproblematic because gradual types can introduce non-termination into an\notherwise terminating language. Steps taken to mitigate this non-termination\nmust not jeopardize the smooth transitions between dynamic and static.\n  We present a gradual dependently-typed language that supports inductive type\nfamilies, has decidable type-checking, and provably supports smooth migration\nbetween static and dynamic, as codified by the refined criteria for gradual\ntyping proposed by Siek et al. (2015). Like Eremondi et al. (2019), we use\napproximate normalization for terminating compile-time evaluation. Unlike\nEremondi et al., our normalization does not require comparison of variables,\nallowing us to show termination with a syntactic model that accommodates\ninductive types. Moreover, we design a novel a technique for tracking\nconstraints on type indices, so that dynamic constraint violations signal\nrun-time errors eagerly. To facilitate these checks, we define an algebraic\nnotion of gradual precision, axiomatizing certain semantic properties of\ngradual terms.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 15:21:32 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Eremondi", "Joseph", ""], ["Garcia", "Ronald", ""], ["Tanter", "\u00c9ric", ""]]}, {"id": "2107.05225", "submitter": "Toby Murray", "authors": "Toby Murray, Pengbo Yan, Gidon Ernst", "title": "Incremental Vulnerability Detection via Back-Propagating Symbolic\n  Execution of Insecurity Separation Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first compositional, incremental static analysis for detecting\nmemory-safety and information leakage vulnerabilities in C-like programs. To do\nso, we develop the first under-approximate relational program logics, including\nInsecurity Separation Logic (InsecSL). We show how InsecSL can be automated via\nback-propagating symbolic execution (BPSE) to build a bottom-up,\ninter-procedural and incremental analysis for detecting vulnerabilities. We\nprove our approach sound in Isabelle/HOL and implement it in a proof-of-concept\ntool, Underflow, for analysing C programs, which we apply to various case\nstudies.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 07:11:58 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Murray", "Toby", ""], ["Yan", "Pengbo", ""], ["Ernst", "Gidon", ""]]}, {"id": "2107.05566", "submitter": "Philipp Seifer", "authors": "Philipp Seifer, Ralf L\\\"ammel, Steffen Staab", "title": "ProGS: Property Graph Shapes Language (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Property graphs constitute data models for representing knowledge graphs.\nThey allow for the convenient representation of facts, including facts about\nfacts, represented by triples in subject or object position of other triples.\nKnowledge graphs such as Wikidata are created by a diversity of contributors\nand a range of sources leaving them prone to two types of errors. The first\ntype of error, falsity of facts, is addressed by property graphs through the\nrepresentation of provenance and validity, making triples occur as first-order\nobjects in subject position of metadata triples. The second type of error,\nviolation of domain constraints, has not been addressed with regard to property\ngraphs so far. In RDF representations, this error can be addressed by shape\nlanguages such as SHACL or ShEx, which allow for checking whether graphs are\nvalid with respect to a set of domain constraints. Borrowing ideas from the\nsyntax and semantics definitions of SHACL, we design a shape language for\nproperty graphs, ProGS, which allows for formulating shape constraints on\nproperty graphs including their specific constructs, such as edges with\nidentities and key-value annotations to both nodes and edges. We define a\nformal semantics of ProGS, investigate the resulting complexity of validating\nproperty graphs against sets of ProGS shapes, compare with corresponding\nresults for SHACL, and implement a prototypical validator that utilizes answer\nset programming.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 16:44:21 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Seifer", "Philipp", ""], ["L\u00e4mmel", "Ralf", ""], ["Staab", "Steffen", ""]]}, {"id": "2107.05679", "submitter": "Ra\\'ul E. Monti", "authors": "Marieke Huisman and Ra\\'ul E. Monti", "title": "Teaching Design by Contract using Snap!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the progress in deductive program verification research, new tools and\ntechniques have become available to support design-by-contract reasoning about\nnon-trivial programs written in widely-used programming languages. However,\ndeductive program verification remains an activity for experts, with ample\nexperience in programming, specification and verification. We would like to\nchange this situation, by developing program verification techniques that are\navailable to a larger audience. In this paper, we present how we developed\nprototypal program verification support for Snap!. Snap! is a visual\nprogramming language, aiming in particular at high school students. We added\nspecification language constructs in a similar visual style, designed to make\nthe intended semantics clear from the look and feel of the specification\nconstructs. We provide support both for static and dynamic verification of\nSnap! programs. Special attention is given to the error messaging, to make this\nas intuitive as possible.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 18:32:15 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Huisman", "Marieke", ""], ["Monti", "Ra\u00fal E.", ""]]}, {"id": "2107.05681", "submitter": "Charitha Saumya Gusthinna Waduge", "authors": "Charitha Saumya, Kirshanthan Sundararajah and Milind Kulkarni", "title": "CFM: SIMT Thread Divergence Reduction by Melding Similar Control-Flow\n  Regions in GPGPU Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPGPUs use the Single-Instruction-Multiple-Thread (SIMT) execution model\nwhere a group of threads--wavefront or war--execute instructions in lockstep.\nWhen threads in a group encounter a branching instruction, not all threads in\nthe group take the same path, a phenomenon known as control-flow divergence.\nThe control-flow divergence causes performance degradation because both paths\nof the branch must be executed one after the other. Prior research has\nprimarily addressed this issue through architectural modifications. We observe\nthat certain GPGPU kernels with control-flow divergence have similar\ncontrol-flow structures with similar instructions on both sides of a branch.\nThis structure can be exploited to reduce control-flow divergence by melding\nthe two sides of the branch allowing threads to reconverge early, reducing\ndivergence. In this work, we present CFM, a compiler analysis and\ntransformation framework that can meld divergent control-flow structures with\nsimilar instruction sequences. We show that CFM can reduce the performance\ndegradation from control-flow divergence.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 18:34:04 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Saumya", "Charitha", ""], ["Sundararajah", "Kirshanthan", ""], ["Kulkarni", "Milind", ""]]}, {"id": "2107.06253", "submitter": "Anders Miltner", "authors": "Anders Miltner and Adrian Trejo Nu\\~nez and Ana Brendel and Swarat\n  Chaudhuri and Isil Dillig", "title": "Bottom-up Synthesis of Recursive Functional Programs using Angelic\n  Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel bottom-up method for the synthesis of functional recursive\nprograms. While bottom-up synthesis techniques can work better than top-down\nmethods in certain settings, there is no prior technique for synthesizing\nrecursive programs from logical specifications in a purely bottom-up fashion.\nThe main challenge is that effective bottom-up methods need to execute\nsub-expressions of the code being synthesized, but it is impossible to execute\na recursive subexpression of a program that has not been fully constructed yet.\nIn this paper, we address this challenge using the concept of angelic\nsemantics. Specifically, our method finds a program that satisfies the\nspecification under angelic semantics (we refer to this as angelic synthesis),\nanalyzes the assumptions made during its angelic execution, uses this analysis\nto strengthen the specification, and finally reattempts synthesis with the\nstrengthened specification. Our proposed angelic synthesis algorithm is based\non version space learning and therefore deals effectively with many incremental\nsynthesis calls made during the overall algorithm. We have implemented this\napproach in a prototype called Burst and evaluate it on synthesis problems from\nprior work. Our experiments show that Burst is able to synthesize a solution to\n95% of the benchmarks in our benchmark suite, outperforming prior work.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 17:25:26 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Miltner", "Anders", ""], ["Nu\u00f1ez", "Adrian Trejo", ""], ["Brendel", "Ana", ""], ["Chaudhuri", "Swarat", ""], ["Dillig", "Isil", ""]]}, {"id": "2107.06591", "submitter": "Beniamino Accattoli", "authors": "Beniamino Accattoli, Maico Leberle", "title": "Useful Open Call-by-Need", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies useful sharing, which is a sophisticated optimization for\nlambda-calculi, in the context of call-by-need evaluation in presence of open\nterms. Useful sharing turns out to be harder to manipulate in call-by-need than\nin call-by-name or call-by-value, because call-by-need evaluates inside\nenvironments, making it harder to specify when a substitution step is useful.\nWe isolate the key involved concepts and prove the correctness of useful\nsharing in this setting.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 10:29:58 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Accattoli", "Beniamino", ""], ["Leberle", "Maico", ""]]}, {"id": "2107.06639", "submitter": "Kacper Sokol", "authors": "Kacper Sokol and Peter Flach", "title": "You Only Write Thrice: Creating Documents, Computational Notebooks and\n  Presentations From a Single Source", "comments": "Published at Rethinking ML Papers -- ICLR 2021 Workshop. OpenReview:\n  https://openreview.net/forum?id=i4zpuNRiU4G Exhibit:\n  https://so-cool.github.io/you-only-write-thrice/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Academic trade requires juggling multiple variants of the same content\npublished in different formats: manuscripts, presentations, posters and\ncomputational notebooks. The need to track versions to accommodate for the\nwrite--review--rebut--revise life-cycle adds another layer of complexity. We\npropose to significantly reduce this burden by maintaining a single source\ndocument in a version-controlled environment (such as git), adding\nfunctionality to generate a collection of output formats popular in academia.\nTo this end, we utilise various open-source tools from the Jupyter scientific\ncomputing ecosystem and operationalise selected software engineering concepts.\nWe offer a proof-of-concept workflow that composes Jupyter Book (an online\ndocument), Jupyter Notebook (a computational narrative) and reveal.js slides\nfrom a single markdown source file. Hosted on GitHub, our approach supports\nchange tracking and versioning, as well as a transparent review process based\non the underlying code issue management infrastructure. An exhibit of our\nworkflow can be previewed at https://so-cool.github.io/you-only-write-thrice/.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 21:02:09 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Sokol", "Kacper", ""], ["Flach", "Peter", ""]]}, {"id": "2107.07296", "submitter": "Christophe De Troyer", "authors": "Christophe De Troyer (Vrije Universiteit Brussel, Belgium), Jens\n  Nicolay (Vrije Universiteit Brussel, Belgium), Wolfgang De Meuter (Vrije\n  Universiteit Brussel, Belgium)", "title": "The Art of the Meta Stream Protocol: Torrents of Streams", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2022, Vol. 6,\n  Issue 1, Article 2", "doi": "10.22152/programming-journal.org/2022/6/2", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of streaming libraries such as Akka Stream, Reactive Extensions, and\nLINQ popularized the declarative functional style of data processing. The\nstream paradigm offers concise syntax to write down processing pipelines to\nconsume the vast amounts of real-time data available today. These libraries\noffer the programmer a domain specific language (DSL) embedded in the host\nlanguage to describe data streams. These libraries however, all suffer from\nextensibility issues. The semantics of a stream is hard-coded into the DSL\nlanguage and cannot be changed by the user of the library. We introduce an\napproach to modify the semantics of a streaming library by means of\nmeta-programming at both run-time and compile-time, and showcase its\ngenerality. We show that the expressiveness of the meta-facilities is strong\nenough to enable push and pull semantics, error handling, parallelism, and\noperator fusion. We evaluate our work by implementing the identified\nshortcomings in terms of a novel stream meta-architecture and show that its\ndesign and architecture adhere to the design principles of a meta-level\narchitecture. The state of the art offers plenty of choice to programmers\nregarding reactive stream processing libraries. Expressing reactive systems is\notherwise difficult to do in general purpose languages. Extensibility and\nfine-tuning should be possible in these libraries to ensure a broad variety of\napplications can be expressed within this single DSL.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 13:03:25 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["De Troyer", "Christophe", "", "Vrije Universiteit Brussel, Belgium"], ["Nicolay", "Jens", "", "Vrije Universiteit Brussel, Belgium"], ["De Meuter", "Wolfgang", "", "Vrije\n  Universiteit Brussel, Belgium"]]}, {"id": "2107.07298", "submitter": "Nicolas Chappe", "authors": "Nicolas Chappe (University of Lyon, France / EnsL, France / Claude\n  Bernard University Lyon 1, France / CNRS, France / Inria, France / LIP,\n  France), Ludovic Henrio (University of Lyon, France / EnsL, France / Claude\n  Bernard University Lyon 1, France / CNRS, France / Inria, France / LIP,\n  France), Amaury Maill\\'e (University of Lyon, France / EnsL, France / Claude\n  Bernard University Lyon 1, France / CNRS, France / Inria, France / LIP,\n  France), Matthieu Moy (University of Lyon, France / EnsL, France / Claude\n  Bernard University Lyon 1, France / CNRS, France / Inria, France / LIP,\n  France), Hadrien Renaud (University of Lyon, France / EnsL, France / Claude\n  Bernard University Lyon 1, France / CNRS, France / Inria, France / LIP,\n  France / \\'Ecole Polytechnique, France / Institut Polytechnique de Paris,\n  France)", "title": "An Optimised Flow for Futures: From Theory to Practice", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2022, Vol. 6,\n  Issue 1, Article 3", "doi": "10.22152/programming-journal.org/2022/6/3", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A future is an entity representing the result of an ongoing computation. A\nsynchronisation with a \"get\" operation blocks the caller until the computation\nis over, to return the corresponding value. When a computation in charge of\nfulfilling a future delegates part of its processing to another task,\nmainstream languages return nested futures, and several \"get\" operations are\nneeded to retrieve the computed value (we call such futures \"control-flow\nfutures\"). Several approaches were proposed to tackle this issues: the\n\"forward\" construct, that allows the programmer to make delegation explicit and\navoid nested futures, and \"data-flow explicit futures\" which natively collapse\nnested futures into plain futures. This paper supports the claim that data-flow\nexplicit futures form a powerful set of language primitives, on top of which\nother approaches can be built. We prove the equivalence, in the context of\ndata-flow explicit futures, between the \"forward\" construct and classical\n\"return\" from functions. The proof relies on a branching bisimulation between a\nprogram using \"forward\" and its \"return\" counterpart. This result allows\nlanguage designers to consider \"forward\" as an optimisation directive rather\nthan as a language primitive. Following the principles of the Godot system, we\nprovide a library implementation of control-flow futures, based on data-flow\nexplicit futures implemented in the compiler. This small library supports the\nclaim that the implementation of classical futures based on data-flow ones is\neasier than the opposite. Our benchmarks show the viability of the approach\nfrom a performance point of view.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 13:03:38 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Chappe", "Nicolas", "", "University of Lyon, France / EnsL, France / Claude\n  Bernard University Lyon 1, France / CNRS, France / Inria, France / LIP,\n  France"], ["Henrio", "Ludovic", "", "University of Lyon, France / EnsL, France / Claude\n  Bernard University Lyon 1, France / CNRS, France / Inria, France / LIP,\n  France"], ["Maill\u00e9", "Amaury", "", "University of Lyon, France / EnsL, France / Claude\n  Bernard University Lyon 1, France / CNRS, France / Inria, France / LIP,\n  France"], ["Moy", "Matthieu", "", "University of Lyon, France / EnsL, France / Claude\n  Bernard University Lyon 1, France / CNRS, France / Inria, France / LIP,\n  France"], ["Renaud", "Hadrien", "", "University of Lyon, France / EnsL, France / Claude\n  Bernard University Lyon 1, France / CNRS, France / Inria, France / LIP,\n  France / \u00c9cole Polytechnique, France / Institut Polytechnique de Paris,\n  France"]]}, {"id": "2107.07300", "submitter": "Angel Luis Scull Pupo", "authors": "Angel Luis Scull Pupo (Vrije Universiteit Brussel, Belgium), Jens\n  Nicolay (Vrije Universiteit Brussel, Belgium), Elisa Gonzalez Boix (Vrije\n  Universiteit Brussel, Belgium)", "title": "Deriving Static Security Testing from Runtime Security Protection for\n  Web Applications", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2022, Vol. 6,\n  Issue 1, Article 1", "doi": "10.22152/programming-journal.org/2022/6/1", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Static Application Security Testing (SAST) and Runtime Application\nSecurity Protection (RASP) are important and complementary techniques used for\ndetecting and enforcing application-level security policies in web\napplications.\n  Inquiry: The current state of the art, however, does not allow a safe and\nefficient combination of SAST and RASP based on a shared set of security\npolicies, forcing developers to reimplement and maintain the same policies and\ntheir enforcement code in both tools.\n  Approach: In this work, we present a novel technique for deriving SAST from\nan existing RASP mechanism by using a two-phase abstract interpretation\napproach in the SAST component that avoids duplicating the effort of specifying\nsecurity policies and implementing their semantics. The RASP mechanism enforces\nsecurity policies by instrumenting a base program to trap security-relevant\noperations and execute the required policy enforcement code. The static\nanalysis of security policies is then obtained from the RASP mechanism by first\nstatically analyzing the base program without any traps. The results of this\nfirst phase are used in a second phase to detect trapped operations and\nabstractly execute the associated and unaltered RASP policy enforcement code.\n  Knowledge: Splitting the analysis into two phases enables running each phase\nwith a specific analysis configuration, rendering the static analysis approach\ntractable while maintaining sufficient precision.\n  Grounding: We validate the applicability of our two-phase analysis approach\nby using it to both dynamically enforce and statically detect a range of\nsecurity policies found in related work. Our experiments suggest that our\ntwo-phase analysis can enable faster and more precise policy violation\ndetection compared to analyzing the full instrumented application under a\nsingle analysis configuration.\n  Importance: Deriving a SAST component from a RASP mechanism enables\nequivalent semantics for the security policies across the static and dynamic\ncontexts in which policies are verified during the software development\nlifecycle. Moreover, our two-phase abstract interpretation approach does not\nrequire RASP developers to reimplement the enforcement code for static\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 13:05:37 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Pupo", "Angel Luis Scull", "", "Vrije Universiteit Brussel, Belgium"], ["Nicolay", "Jens", "", "Vrije Universiteit Brussel, Belgium"], ["Boix", "Elisa Gonzalez", "", "Vrije\n  Universiteit Brussel, Belgium"]]}, {"id": "2107.07301", "submitter": "Yudai Tanabe", "authors": "Yudai Tanabe (Tokyo Institute of Technology, Japan), Luthfan Anshar\n  Lubis (Tokyo Institute of Technology, Japan), Tomoyuki Aotani (Mamezou,\n  Japan), Hidehiko Masuhara (Tokyo Institute of Technology, Japan)", "title": "A Functional Programming Language with Versions", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2022, Vol. 6,\n  Issue 1, Article 5", "doi": "10.22152/programming-journal.org/2022/6/5", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While modern software development heavily uses versioned packages,\nprogramming languages rarely support the concept of versions in their\nsemantics, which makes software updates more bulky and unsafe. This paper\nproposes a programming language that intrinsically supports versions. The main\ngoals are to design core language features to support multiple versions in one\nprogram and establish a proper notion of type safety with those features. The\nproposed core calculus, called Lambda VL, has versioned values, each containing\ndifferent values under different versions. We show the construction of the type\nsystem as an extension of coeffect calculus by mapping versions to\ncomputational resources. The type system guarantees the existence of a valid\ncombination of versions for a program. The calculus enables programming\nlanguages to use multiple versions of a package within a program. It will serve\nas a basis for designing advanced language features like module systems and\nsemantic versioning.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 13:05:54 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Tanabe", "Yudai", "", "Tokyo Institute of Technology, Japan"], ["Lubis", "Luthfan Anshar", "", "Tokyo Institute of Technology, Japan"], ["Aotani", "Tomoyuki", "", "Mamezou,\n  Japan"], ["Masuhara", "Hidehiko", "", "Tokyo Institute of Technology, Japan"]]}, {"id": "2107.07376", "submitter": "EPTCS", "authors": "Elaine Pimentel (UFRN), Enrico Tassi (Inria)", "title": "Proceedings of the Sixteenth Workshop on Logical Frameworks and\n  Meta-Languages: Theory and Practice", "comments": null, "journal-ref": "EPTCS 337, 2021", "doi": "10.4204/EPTCS.337", "report-no": null, "categories": "cs.LO cs.AI cs.LG cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Logical frameworks and meta-languages form a common substrate for\nrepresenting, implementing and reasoning about a wide variety of deductive\nsystems of interest in logic and computer science. Their design, implementation\nand their use in reasoning tasks, ranging from the correctness of software to\nthe properties of formal systems, have been the focus of considerable research\nover the last two decades. This workshop brings together designers,\nimplementors and practitioners to discuss various aspects impinging on the\nstructure and utility of logical frameworks, including the treatment of\nvariable binding, inductive and co-inductive reasoning techniques and the\nexpressiveness and lucidity of the reasoning process.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 05:19:09 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Pimentel", "Elaine", "", "UFRN"], ["Tassi", "Enrico", "", "Inria"]]}, {"id": "2107.07664", "submitter": "EPTCS", "authors": "Laila El-Beheiry (Carnegie Mellon University), Giselle Reis (Carnegie\n  Mellon University), Ammar Karkour (Carnegie Mellon University)", "title": "SMLtoCoq: Automated Generation of Coq Specifications and Proof\n  Obligations from SML Programs with Contracts", "comments": "In Proceedings LFMTP 2021, arXiv:2107.07376", "journal-ref": "EPTCS 337, 2021, pp. 71-87", "doi": "10.4204/EPTCS.337.6", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Formally reasoning about functional programs is supposed to be\nstraightforward and elegant, however, it is not typically done as a matter of\ncourse. Reasoning in a proof assistant requires \"reimplementing\" the code in\nthose tools, which is far from trivial. SMLtoCoq provides an automatic\ntranslation of SML programs and function contracts into Coq. Programs are\ntranslated into Coq specifications, and function contracts into theorems, which\ncan then be formally proved. Using the Equations plugin and other well\nestablished Coq libraries, SMLtoCoq is able to translate SML programs without\nside-effects containing partial functions, structures, functors, records, among\nothers. Additionally, we provide a Coq version of many parts of SML's basis\nlibrary, so that calls to these libraries are kept almost as is.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 01:44:37 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["El-Beheiry", "Laila", "", "Carnegie Mellon University"], ["Reis", "Giselle", "", "Carnegie\n  Mellon University"], ["Karkour", "Ammar", "", "Carnegie Mellon University"]]}, {"id": "2107.07666", "submitter": "EPTCS", "authors": "Mary Southern (University of Minnesota), Gopalan Nadathur (University\n  of Minnesota)", "title": "Adelfa: A System for Reasoning about LF Specifications", "comments": "In Proceedings LFMTP 2021, arXiv:2107.07376", "journal-ref": "EPTCS 337, 2021, pp. 104-120", "doi": "10.4204/EPTCS.337.8", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a system called Adelfa that provides mechanized support for\nreasoning about specifications developed in the Edinburgh Logical Framework or\nLF. Underlying Adelfa is a new logic named L_LF. Typing judgements in LF are\nrepresented by atomic formulas in L_LF and quantification is permitted over\ncontexts and terms that appear in such formulas. Contexts, which constitute\ntype assignments to uniquely named variables that are modelled using the\ntechnical device of nominal constants, are characterized in L_LF by context\nschemas that describe their inductive structure. We present these formulas and\nan associated semantics before sketching a proof system for constructing\narguments that are sound with respect to the semantics. We then outline the\nrealization of this proof system in Adelfa and illustrate its use through a few\nexample proof developments. We conclude the paper by relating Adelfa to\nexisting systems for reasoning about LF specifications.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 01:45:07 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Southern", "Mary", "", "University of Minnesota"], ["Nadathur", "Gopalan", "", "University\n  of Minnesota"]]}, {"id": "2107.07670", "submitter": "EPTCS", "authors": "Matthieu Sozeau (Inria & LS2N, Universit\\'e de Nantes)", "title": "Touring the MetaCoq Project (Invited Paper)", "comments": "In Proceedings LFMTP 2021, arXiv:2107.07376", "journal-ref": "EPTCS 337, 2021, pp. 13-29", "doi": "10.4204/EPTCS.337.2", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Proof assistants are getting more widespread use in research and industry to\nprovide certified and independently checkable guarantees about theories,\ndesigns, systems and implementations. However, proof assistant implementations\nthemselves are seldom verified, although they take a major share of the trusted\ncode base in any such certification effort. In this area, proof assistants\nbased on Higher-Order Logic enjoy stronger guarantees, as self-certified\nimplementations have been available for some years. One cause of this\ndifference is the inherent complexity of dependent type theories together with\ntheir extensions with inductive types, universe polymorphism and complex sort\nsystems, and the gap between theory on paper and practical implementations in\nefficient programming languages. MetaCoq is a collaborative project that aims\nto tackle these difficulties to provide the first fully-certified realistic\nimplementation of a type checker for the full calculus underlying the Coq proof\nassistant. To achieve this, we refined the sometimes blurry, if not incorrect,\nspecification and implementation of the system. We show how theoretical tools\nfrom this community such as bidirectional type-checking,\nTait-Martin-L\\\"of/Takahashi's confluence proof technique and monadic and\ndependently-typed programming can help construct the following artefacts: a\nspecification of Coq's syntax and type theory, the Polymorphic Cumulative\nCalculus of (Co)-Inductive Constructions (PCUIC); a monad for the manipulation\nof raw syntax and interaction with the Coq system; a verification of PCUIC's\nmetatheory, whose main results are the confluence of reduction, type\npreservation and principality of typing; a realistic, correct and complete\ntype-checker for PCUIC; a sound type and proof erasure procedure from PCUIC to\nuntyped lambda-calculus, i.e., the core of the extraction mechanism of Coq.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 02:13:47 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Sozeau", "Matthieu", "", "Inria & LS2N, Universit\u00e9 de Nantes"]]}, {"id": "2107.07809", "submitter": "Michael Lukin", "authors": "K. I. Mihajlenko, M. A. Lukin, A. S. Stankevich", "title": "A method for decompilation of AMD GCN kernels to OpenCL", "comments": "10 pages, 5 figures", "journal-ref": "Information and Control Systems, 2021, no. 2, pp. 33-42", "doi": "10.31799/1684-8853-2021-2-33-42", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction: Decompilers are useful tools for software analysis and support\nin the absence of source code. They are available for many hardware\narchitectures and programming languages. However, none of the existing\ndecompilers support modern AMD GPU architectures such as AMD GCN and RDNA.\nPurpose: We aim at developing the first assembly decompiler tool for a modern\nAMD GPU architecture that generates code in the OpenCL language, which is\nwidely used for programming GPGPUs. Results: We developed the algorithms for\nthe following operations: preprocessing assembly code, searching data accesses,\nextracting system values, decompiling arithmetic operations and recovering data\ntypes. We also developed templates for decompilation of branching operations.\nPractical relevance: We implemented the presented algorithms in Python as a\ntool called OpenCLDecompiler, which supports a large subset of AMD GCN\ninstructions. This tool automatically converts disassembled GPGPU code into the\nequivalent OpenCL code, which reduces the effort required to analyze assembly\ncode.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 10:32:54 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Mihajlenko", "K. I.", ""], ["Lukin", "M. A.", ""], ["Stankevich", "A. S.", ""]]}, {"id": "2107.08038", "submitter": "Dirk Beyer", "authors": "Dirk Beyer, Lars Grunske, Thomas Lemberger, Minxing Tang", "title": "Towards a Benchmark Set for Program Repair Based on Partial Fixes", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Software bugs significantly contribute to software cost and increase the risk\nof system malfunctioning. In recent years, many automated program-repair\napproaches have been proposed to automatically fix undesired program behavior.\nDespite of their great success, specific problems such as fixing bugs with\npartial fixes still remain unresolved. A partial fix to a known software issue\nis a programmer's failed attempt to fix the issue the first time. Even though\nit fails, this fix attempt still conveys important information such as the\nsuspicious software region and the bug type. In this work we do not propose an\napproach for program repair with partial fixes, but instead answer a\npreliminary question: Do partial fixes occur often enough, in general, to be\nrelevant for the research area of automated program repair? We crawled 1500\nopen-source C repositories on GitHub for partial fixes. The result is a\nbenchmark set of 2204 benchmark tasks for automated program repair based on\npartial fixes. The benchmark set is available open source and open to further\ncontributions and improvement.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 17:58:08 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Beyer", "Dirk", ""], ["Grunske", "Lars", ""], ["Lemberger", "Thomas", ""], ["Tang", "Minxing", ""]]}, {"id": "2107.08132", "submitter": "Michael Kruse", "authors": "Michael Kruse", "title": "Loop Transformations using Clang's Abstract Syntax Tree", "comments": "LLPP'21 ( The First Workshop on LLVM in Parallel Processing) preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OpenMP 5.1 introduced the first loop nest transformation directives unroll\nand tile, and more are expected to be included in OpenMP 6.0. We discuss the\ntwo Abstract Syntax Tree (AST) representations used by Clang's implementation\nthat is currently under development. The first representation is designed for\ncompatibility with the existing implementation and stores the transformed loop\nnest in a shadow AST next to the syntactical AST. The second representation\nintroduces a new meta AST-node OMPCanonicalLoop that guarantees that the\nsemantic requirements of an OpenMP loop are met, and a CanonicalLoopInfo type\nthat the OpenMPIRBuilder uses to represent literal and transformed loops. This\nsecond approach provides a better abstraction of loop semantics, removes the\nneed for shadow AST nodes that are only relevant for code generation, allows\nsharing the implementation with other front-ends such as flang, but depends on\nthe OpenMPIRBuilder which is currently under development.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 21:55:06 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Kruse", "Michael", ""]]}, {"id": "2107.08729", "submitter": "Alceste Scalas", "authors": "Christian Bartolo Burl\\`o, Adrian Francalanza, Alceste Scalas, Catia\n  Trubiani, Emilio Tuosto", "title": "Towards Probabilistic Session-Type Monitoring", "comments": null, "journal-ref": "Proceedings 23rd IFIP WG 6.1 International Conference,\n  COORDINATION 2021. Springer International Publishing", "doi": "10.1007/978-3-030-78142-2_7", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a tool-based approach for the runtime analysis of communicating\nprocesses grounded on probabilistic binary session types. We synthesise a\nmonitor out of a probabilistic session type where each choice point is\ndecorated by a probability distribution. The monitor observes the execution of\na process, infers its probabilistic behaviour and issues warnings when the\nobserved behaviour deviates from the one specified by the probabilistic session\ntype.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 09:56:30 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Burl\u00f2", "Christian Bartolo", ""], ["Francalanza", "Adrian", ""], ["Scalas", "Alceste", ""], ["Trubiani", "Catia", ""], ["Tuosto", "Emilio", ""]]}, {"id": "2107.08824", "submitter": "Samuel Chassot", "authors": "Samuel Chassot, Viktor Kun\\v{c}ak", "title": "Verified Mutable Data Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Malfunctions in software like airplane control systems or nuclear plant\ncontrol systems can have catastrophic consequences. Formal verification is the\nonly form of sofware testing that can guarantee the absence of bugs. Formally\nverified software gives a mathematical proof that the specification is\ncorrectly implemented and that no bugs would induce unwanted behaviour. This\nhas a high development cost and having an entirely verified program takes time\nand effort. However, having verified components already has great benefits. We\nimplement in Scala and formally verify with Stainless a hash map that can then\nbe reused and act as a basis on which to rely. The implementation we propose is\nbased on the LongMap of the Scala standard library with some minor adaptations.\nThis map is implemented with mutable arrays. We give the specification with\nrespect to an implementation of a map based on a list of tuples, that we\nimplement and formally verify as well.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 11:56:07 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 11:23:52 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Chassot", "Samuel", ""], ["Kun\u010dak", "Viktor", ""]]}, {"id": "2107.08852", "submitter": "Brandon Bohrer", "authors": "Brandon Bohrer, Andr\\'e Platzer", "title": "Structured Proofs for Adversarial Cyber-Physical Systems", "comments": "Preprint of paper appearing in ESWEEK-TECS (EMSOFT 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many cyber-physical systems (CPS) are safety-critical, so it is important to\nformally verify them, e.g. in formal logics that show a model's correctness\nspecification always holds. Constructive Differential Game Logic (CdGL) is such\na logic for (constructive) hybrid games, including hybrid systems. To overcome\nundecidability, the user first writes a proof, for which we present a\nproof-checking tool.\n  We introduce Kaisar, the first language and tool for CdGL proofs, which until\nnow could only be written by hand with a low-level proof calculus. Kaisar's\nstructured proofs simplify challenging CPS proof tasks, especially by using\nprogramming language principles and high-level stateful reasoning. Kaisar\nexploits CdGL's constructivity and refinement relations to build proofs around\nmodels of game strategies. The evaluation reproduces and extends existing case\nstudies on 1D and 2D driving. Proof metrics are compared and reported\nexperiences are discussed for the original studies and their reproductions.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 13:10:22 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Bohrer", "Brandon", ""], ["Platzer", "Andr\u00e9", ""]]}, {"id": "2107.09472", "submitter": "Lucas Franceschino", "authors": "Lucas Franceschino, David Pichardie, Jean-Pierre Talpin", "title": "Verified Functional Programming of an Abstract Interpreter", "comments": "To be published in SAS21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Abstract interpreters are complex pieces of software: even if the abstract\ninterpretation theory and companion algorithms are well understood, their\nimplementations are subject to bugs, that might question the soundness of their\ncomputations.\n  While some formally verified abstract interpreters have been written in the\npast, writing and understanding them requires expertise in the use of proof\nassistants, and requires a non-trivial amount of interactive proofs.\n  This paper presents a formally verified abstract interpreter fully programmed\nand proved correct in the F* verified programming environment. Thanks to F*\nrefinement types and SMT prover capabilities we demonstrate a substantial\nsaving in proof effort compared to previous works based on interactive proof\nassistants.\n  Almost all the code of our implementation, proofs included, written in a\nfunctional style, are presented directly in the paper.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 13:24:43 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Franceschino", "Lucas", ""], ["Pichardie", "David", ""], ["Talpin", "Jean-Pierre", ""]]}, {"id": "2107.09766", "submitter": "Taro Sekiyama", "authors": "Takeshi Tsukada and Hiroshi Unno and Taro Sekiyama and Kohei Suenaga", "title": "Enhancing Loop-Invariant Synthesis via Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loop-invariant synthesis is the basis of every program verification\nprocedure. Due to its undecidability in general, a tool for invariant synthesis\nnecessarily uses heuristics. Despite the common belief that the design of\nheuristics is vital for the effective performance of a verifier, little work\nhas been performed toward obtaining the optimal heuristics for each\ninvariant-synthesis tool. Instead, developers have hand-tuned the heuristics of\ntools. This study demonstrates that we can effectively and automatically learn\na good heuristic via reinforcement learning for an invariant synthesizer PCSat.\nOur experiment shows that PCSat combined with the heuristic learned by\nreinforcement learning outperforms the state-of-the-art solvers for this task.\nTo the best of our knowledge, this is the first work that investigates learning\nthe heuristics of an invariant synthesis tool.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 11:17:05 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Tsukada", "Takeshi", ""], ["Unno", "Hiroshi", ""], ["Sekiyama", "Taro", ""], ["Suenaga", "Kohei", ""]]}, {"id": "2107.10160", "submitter": "Emanuele De Angelis", "authors": "Emanuele De Angelis and Wim Vanhoof", "title": "Pre-proceedings of the 31st International Symposium on Logic-Based\n  Program Synthesis and Transformation (LOPSTR 2021)", "comments": "Papers selected for presentation at LOPSTR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume constitutes the pre-proceedings of the 31st International\nSymposium on Logic-Based Program Synthesis and Transformation (LOPSTR 2021),\nheld on 7-8th September 2021 as a hybrid (blended) meeting, both in-person (at\nthe Teachers' House in Tallinn, Estonia) and virtual, and co-located with the\n23rd International Symposium on Principles and Practice of Declarative\nProgramming (PPDP 2021). After discussion at the symposium papers will go\nthrough a second round of refereeing and selection for the formal proceedings.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 15:43:23 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 07:40:00 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["De Angelis", "Emanuele", ""], ["Vanhoof", "Wim", ""]]}, {"id": "2107.10533", "submitter": "Piyus Kedia", "authors": "Piyus Kedia, Rahul Purandare, Udit Kumar Agarwal, Rishabh", "title": "CGuard: Efficient Spatial Safety for C", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial safety violations are the root cause of many security attacks and\nunexpected behavior of applications. Existing techniques to enforce spatial\nsafety work broadly at either object or pointer granularity. Object-based\napproaches tend to incur high CPU overheads, whereas pointer-based approaches\nincur both high CPU and memory overheads. SGXBounds, an object-based approach,\nis so far the most efficient technique that provides complete out-of-bounds\nprotection for objects. However, a major drawback of this approach is that it\nrestricts the application address space to 4GB.\n  In this paper, we present CGuard, a tool that provides object-bounds\nprotection for C applications with comparable overheads to SGXBounds without\nrestricting the application address space. CGuard stores the bounds information\njust before the base address of an object and encodes the relative offset of\nthe base address in the spare bits of the virtual address available in x86_64\narchitecture. For an object that can't fit in the spare bits, CGuard uses a\ncustom memory layout that enables it to find the base address of the object in\njust one memory access. Our study revealed spatial safety violations in the gcc\nand x264 benchmarks from the SPEC CPU2017 benchmark suite and the string_match\nbenchmark from the Phoenix benchmark suite. The execution time overheads for\nthe SPEC CPU2017 and Phoenix benchmark suites were 44% and 25% respectively,\nwhereas the reduction in the throughput for the Apache webserver when the CPUs\nwere fully saturated was 30%. These results indicate that CGuard can be highly\neffective while maintaining a reasonable degree of efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 09:09:37 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Kedia", "Piyus", ""], ["Purandare", "Rahul", ""], ["Agarwal", "Udit Kumar", ""], ["Rishabh", "", ""]]}, {"id": "2107.10545", "submitter": "Peter Mosses", "authors": "Peter D. Mosses", "title": "Fundamental Constructs in Programming Languages", "comments": "20 pages plus appendix, submitted to ISoLA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Specifying the semantics of a programming language formally can have many\nbenefits. However, it can also require a huge effort. The effort can be\nsignificantly reduced by translating language syntax to so-called fundamental\nconstructs (funcons). A translation to funcons is easy to update when the\nlanguage evolves, and it exposes relationships between individual language\nconstructs.\n  The PLanCompS project has developed an initial collection of funcons\n(primarily for translation of functional and imperative languages). The\nbehaviour of each funcon is defined, once and for all, using a modular variant\nof structural operational semantics. The definitions are available online.\n  This paper introduces and motivates funcons. It illustrates translation of\nlanguage constructs to funcons, and how funcons are defined. It also relates\nfuncons to notation used in previous frameworks, including monadic semantics\nand action semantics.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 09:53:04 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Mosses", "Peter D.", ""]]}, {"id": "2107.10566", "submitter": "Martin Ruefenacht", "authors": "Martin Ruefenacht, Derek Schafer, Anthony Skjellum, Purushotham V.\n  Bangalore", "title": "MPIs Language Bindings are Holding MPI Back", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Over the past two decades, C++ has been adopted as a major HPC language\n(displacing C to a large extent, andFortran to some degree as well). Idiomatic\nC++ is clearly how C++ is being used nowadays. But, MPIs syntax and semantics\ndefined and extended with C and Fortran interfaces that align with the\ncapabilities and limitations of C89 and Fortran-77.Unfortunately, the\nlanguage-independent specification also clearly reflects the intersection of\nwhat these languages could syntactically and semantically manage at the outset\nin 1993, rather than being truly language neutral.In this paper, we propose a\nmodern C++ language interface to replace the C language binding for C++\nprogrammers with an upward-compatible architecture that leverages all the\nbenefits of C++11-20 for performance, productivity, and interoperability with\nother popular C++ libraries and interfaces for HPC. Demand is demonstrably\nstrong for this second attempt at language support for C++ in MPI after the\noriginal interface, which was added in MPI-2, then was found to lack specific\nbenefits over theC binding, and so was subsequently removed in MPI-3. Since C++\nand its idiomatic usage have evolved since the original C++ language binding\nwas removed from the standard, this new effort is both timely and important for\nMPI applications. Also, many C++ application programmers create their own, ad\nhoc shim libraries over MPI to provide some degree of abstraction unique to\ntheir particular project, which means many such abstraction libraries are being\ndevised without any specific commonality other than the demand for such.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 10:37:29 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Ruefenacht", "Martin", ""], ["Schafer", "Derek", ""], ["Skjellum", "Anthony", ""], ["Bangalore", "Purushotham V.", ""]]}, {"id": "2107.10793", "submitter": "Kwanghoon Choi", "authors": "Kwanghoon Choi, James Cheney, Sam Lindley, Bob Reynders", "title": "A Typed Slicing Compilation of the Polymorphic RPC Calculus", "comments": "A long version of PPDP 2021 (23rd International Symposium on\n  Principles and Practice of Declarative Programming)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The polymorphic RPC calculus allows programmers to write succinct multitier\nprograms using polymorphic location constructs. However, until now it lacked an\nimplementation. We develop an experimental programming language based on the\npolymorphic RPC calculus. We introduce a polymorphic Client-Server (CS)\ncalculus with the client and server parts separated. In contrast to existing\nuntyped CS calculi, our calculus is not only able to resolve polymorphic\nlocations statically, but it is also able to do so dynamically. We design a\ntype-based slicing compilation of the polymorphic RPC calculus into this CS\ncalculus, proving type and semantic correctness. We propose a method to erase\ntypes unnecessary for execution but retaining locations at runtime by\ntranslating the polymorphic CS calculus into an untyped CS calculus, proving\nsemantic correctness.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 16:48:28 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 13:57:35 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Choi", "Kwanghoon", ""], ["Cheney", "James", ""], ["Lindley", "Sam", ""], ["Reynders", "Bob", ""]]}, {"id": "2107.10936", "submitter": "Alen Arslanagi\\'c", "authors": "Alen Arslanagic, Anda-Amelia Palamariuc, Jorge A. P\\'erez", "title": "Minimal Session Types for the \\pi-calculus (Extended Version)", "comments": "Extended version of a PPDP 2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Session types enable the static verification of message-passing programs. A\nsession type specifies a channel's protocol as sequences of messages. Prior\nwork established a minimality result: every process typable with standard\nsession types can be compiled down to a process typable using minimal session\ntypes: session types without the sequencing construct. This result justifies\nsession types in terms of themselves; it holds for a higher-order session\n\\pi-calculus, where values are abstractions (functions from names to\nprocesses).\n  This paper establishes a new minimality result but now for the session\n\\pi-calculus, the language in which values are names and for which session\ntypes have been more widely studied. Remarkably, this new minimality result can\nbe obtained by composing known results. We develop optimizations of our new\nminimality result, and establish its static and dynamic correctness.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 21:33:51 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 20:40:40 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Arslanagic", "Alen", ""], ["Palamariuc", "Anda-Amelia", ""], ["P\u00e9rez", "Jorge A.", ""]]}, {"id": "2107.11280", "submitter": "Chuangjie Xu", "authors": "Serdar Erbatur, Ulrich Sch\\\"opp, Chuangjie Xu", "title": "Type-based Enforcement of Infinitary Trace Properties for Java", "comments": "main part (14 pages) published at PPDP'21; arXiv version contains an\n  appendix on the FJ operational semantics and the extension to support\n  exception handling (15 pages total)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common approach to improve software quality is to use programming\nguidelines to avoid common kinds of errors. In this paper, we consider the\nproblem of enforcing guidelines for Featherweight Java (FJ). We formalize\nguidelines as sets of finite or infinite execution traces and develop a\nregion-based type and effect system for FJ that can enforce such guidelines. We\nbuild on the work by Erbatur, Hofmann and Z\\u{a}linescu, who presented a type\nsystem for verifying the finite event traces of terminating FJ programs. We\nrefine this type system, separating region typing from FJ typing, and use ideas\nof Hofmann and Chen to extend it to capture also infinite traces produced by\nnon-terminating programs. Our type and effect system can express properties of\nboth finite and infinite traces and can compute information about the possible\ninfinite traces of FJ programs. Specifically, the set of infinite traces of a\nmethod is constructed as the greatest fixed point of the operator which\ncalculates the possible traces of method bodies. Our type inference algorithm\nis realized by working with the finitary abstraction of the system based on\nB\\\"uchi automata.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 14:45:46 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Erbatur", "Serdar", ""], ["Sch\u00f6pp", "Ulrich", ""], ["Xu", "Chuangjie", ""]]}, {"id": "2107.11347", "submitter": "James Cheney", "authors": "James Cheney and Wilmer Ricciotti", "title": "Comprehending nulls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Nested Relational Calculus (NRC) has been an influential high-level query\nlanguage, providing power and flexibility while still allowing translation to\nstandard SQL queries. It has also been used as a basis for language-integrated\nquery in programming languages such as F#, Scala, and Links. However, SQL's\ntreatment of incomplete information, using nulls and three-valued logic, is not\ncompatible with `standard' NRC based on two-valued logic. Nulls are widely used\nin practice for incomplete data, but the question of how to accommodate\nSQL-style nulls and incomplete information in NRC, or integrate such queries\ninto a typed programming language, appears not to have been studied thoroughly.\nIn this paper we consider two approaches: an explicit approach in which option\ntypes are used to represent (possibly) nullable primitive types, and an\nimplicit approach in which types are treated as possibly-null by default. We\ngive translations relating the implicit and explicit approaches, discuss\nhandling nulls in language integration, and sketch extensions of normalization\nand conservativity results.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 16:49:21 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Cheney", "James", ""], ["Ricciotti", "Wilmer", ""]]}, {"id": "2107.11598", "submitter": "Peng Qian", "authors": "Zhenguang Liu, Peng Qian, Xiaoyang Wang, Yuan Zhuang, Lin Qiu, Xun\n  Wang", "title": "Combining Graph Neural Networks with Expert Knowledge for Smart Contract\n  Vulnerability Detection", "comments": "This paper has been accepted by TKDE 2021", "journal-ref": null, "doi": "10.1109/TKDE.2021.3095196", "report-no": null, "categories": "cs.CR cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart contract vulnerability detection draws extensive attention in recent\nyears due to the substantial losses caused by hacker attacks. Existing efforts\nfor contract security analysis heavily rely on rigid rules defined by experts,\nwhich are labor-intensive and non-scalable. More importantly, expert-defined\nrules tend to be error-prone and suffer the inherent risk of being cheated by\ncrafty attackers. Recent researches focus on the symbolic execution and formal\nanalysis of smart contracts for vulnerability detection, yet to achieve a\nprecise and scalable solution. Although several methods have been proposed to\ndetect vulnerabilities in smart contracts, there is still a lack of effort that\nconsiders combining expert-defined security patterns with deep neural networks.\nIn this paper, we explore using graph neural networks and expert knowledge for\nsmart contract vulnerability detection. Specifically, we cast the rich control-\nand data- flow semantics of the source code into a contract graph. To highlight\nthe critical nodes in the graph, we further design a node elimination phase to\nnormalize the graph. Then, we propose a novel temporal message propagation\nnetwork to extract the graph feature from the normalized graph, and combine the\ngraph feature with designed expert patterns to yield a final detection system.\nExtensive experiments are conducted on all the smart contracts that have source\ncode in Ethereum and VNT Chain platforms. Empirical results show significant\naccuracy improvements over the state-of-the-art methods on three types of\nvulnerabilities, where the detection accuracy of our method reaches 89.15%,\n89.02%, and 83.21% for reentrancy, timestamp dependence, and infinite loop\nvulnerabilities, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 13:16:30 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Liu", "Zhenguang", ""], ["Qian", "Peng", ""], ["Wang", "Xiaoyang", ""], ["Zhuang", "Yuan", ""], ["Qiu", "Lin", ""], ["Wang", "Xun", ""]]}, {"id": "2107.11673", "submitter": "Hanchen Ye", "authors": "Hanchen Ye, Cong Hao, Jianyi Cheng, Hyunmin Jeong, Jack Huang, Stephen\n  Neuendorffer, Deming Chen", "title": "ScaleHLS: Scalable High-Level Synthesis through MLIR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High-level Synthesis (HLS) has been widely adopted as it significantly\nimproves the hardware design productivity and enables efficient design space\nexploration (DSE). HLS tools can be used to deliver solutions for many\ndifferent kinds of design problems, which are often better solved with\ndifferent levels of abstraction. While existing HLS tools are built using\ncompiler infrastructures largely based on a single-level abstraction (e.g.,\nLLVM), we propose ScaleHLS, a next-generation HLS compilation flow, on top of a\nmulti-level compiler infrastructure called MLIR, for the first time. By using\nan intermediate representation (IR) that can be better tuned to particular\nalgorithms at different representation levels, we are able to build this new\nHLS tool that is more scalable and customizable towards various applications\ncoming with intrinsic structural or functional hierarchies. ScaleHLS is able to\nrepresent and optimize HLS designs at multiple levels of abstraction and\nprovides an HLS-dedicated transform and analysis library to solve the\noptimization problems at the suitable representation levels. On top of the\nlibrary, we also build an automated DSE engine to explore the multi-dimensional\ndesign space efficiently. In addition, we develop an HLS C front-end and a\nC/C++ emission back-end to translate HLS designs into/from MLIR for enabling\nthe end-to-end ScaleHLS flow. Experimental results show that, comparing to the\nbaseline designs only optimized by Xilinx Vivado HLS, ScaleHLS improves the\nperformances with amazing quality-of-results -- up to 768.1x better on\ncomputation kernel level programs and up to 3825.0x better on neural network\nmodels.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 19:20:23 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Ye", "Hanchen", ""], ["Hao", "Cong", ""], ["Cheng", "Jianyi", ""], ["Jeong", "Hyunmin", ""], ["Huang", "Jack", ""], ["Neuendorffer", "Stephen", ""], ["Chen", "Deming", ""]]}, {"id": "2107.11674", "submitter": "Andrei Popescu", "authors": "Lorenzo Gheri and Andrei Popescu", "title": "Case Studies in Formal Reasoning About Lambda-Calculus: Semantics,\n  Church-Rosser, Standardization and HOAS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We have previously published the Isabelle/HOL formalization of a general\ntheory of syntax with bindings. In this companion paper, we instantiate the\ngeneral theory to the syntax of lambda-calculus and formalize the development\nleading to several fundamental constructions and results: sound semantic\ninterpretation, the Church-Rosser and standardization theorems, and\nhigher-order abstract syntax (HOAS) encoding. For Church-Rosser and\nstandardization, our work covers both the call-by-name and call-by-value\nversions of the calculus, following classic papers by Takahashi and Plotkin.\nDuring the formalization, we were able to stay focused on the high-level ideas\nof the development -- thanks to the arsenal provided by our general theory: a\nwealth of basic facts about the substitution, swapping and freshness operators,\nas well as recursive-definition and reasoning principles, including a\nspecialization to semantic interpretation of syntax.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 19:25:48 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Gheri", "Lorenzo", ""], ["Popescu", "Andrei", ""]]}, {"id": "2107.11679", "submitter": "Zhaowei Xu", "authors": "Zhaowei Xu, Mingsheng Ying and Beno\\^it Valiron", "title": "Reasoning about Recursive Quantum Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Most modern (classical) programming languages support recursion. Recursion\nhas also been successfully applied to the design of several quantum algorithms\nand introduced in a couple of quantum programming languages. So, it can be\nexpected that recursion will become one of the fundamental paradigms of quantum\nprogramming. Several program logics have been developed for verification of\nquantum while-programs. However, there are as yet no general methods for\nreasoning about (mutual) recursive procedures and ancilla quantum data\nstructure in quantum computing (with measurement). We fill the gap in this\npaper by proposing a parameterized quantum assertion logic and, based on which,\ndesigning a quantum Hoare logic for verifying parameterized recursive quantum\nprograms with ancilla data and probabilistic control. The quantum Hoare logic\ncan be used to prove partial, total, and even probabilistic correctness (by\nreducing to total correctness) of those quantum programs. In particular, two\ncounterexamples for illustrating incompleteness of non-parameterized assertions\nin verifying recursive procedures, and, one counterexample for showing the\nfailure of reasoning with exact probabilities based on partial correctness, are\nconstructed. The effectiveness of our logic is shown by three main examples --\nrecursive quantum Markov chain (with probabilistic control), fixed-point\nGrover's search, and recursive quantum Fourier sampling.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 20:11:46 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Xu", "Zhaowei", ""], ["Ying", "Mingsheng", ""], ["Valiron", "Beno\u00eet", ""]]}, {"id": "2107.11912", "submitter": "Enzo Rucci", "authors": "Manuel Costanzo and Enzo Rucci and Marcelo Naiouf and Armando De\n  Giusti", "title": "Performance vs Programming Effort between Rust and C on Multicore\n  Architectures: Case Study in N-Body", "comments": "This article was submitted to 2021 XLVI Latin American Computing\n  Conference (CLEI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Historically, Fortran and C have been the default programming languages in\nHigh-Performance Computing (HPC). In both, programmers have primitives and\nfunctions available that allow manipulating system memory and interacting\ndirectly with the underlying hardware, resulting in efficient code in both\nresponse times and resource use. On the other hand, it is a real challenge to\ngenerate code that is maintainable and scalable over time in these types of\nlanguages. In 2010, Rust emerged as a new programming language designed for\nconcurrent and secure applications, which adopts features of procedural,\nobject-oriented and functional languages. Among its design principles, Rust is\naimed at matching C in terms of efficiency, but with increased code security\nand productivity. This paper presents a comparative study between C and Rust in\nterms of performance and programming effort, selecting as a case study the\nsimulation of N computational bodies (N-Body), a popular problem in the HPC\ncommunity. Based on the experimental work, it was possible to establish that\nRust is a language that reduces programming effort while maintaining acceptable\nperformance levels, meaning that it is a possible alternative to C for HPC.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 00:09:35 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Costanzo", "Manuel", ""], ["Rucci", "Enzo", ""], ["Naiouf", "Marcelo", ""], ["De Giusti", "Armando", ""]]}, {"id": "2107.12136", "submitter": "Tihana Galinac Grbac", "authors": "Tihana Galinac Grbac", "title": "The Role of Functional Programming in Management and Orchestration of\n  Virtualized Network Resources Part I. System structure for Complex Systems\n  and Design Principles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This is part I of the follow-up lecture notes of the lectures given by the\nauthors at the Three \\CO\" (Composability, Comprehensibility, Correctness)\nWinter School held in Ko\\v{s}ice, Slovakia, in January 2018, and Summer School\nheld in Budapest, Hungary, in June 2019. In this part we explain the role of\nfunctional programming paradigm in the management of complex software systems,\nand how the functional programming concepts play important role in the\ndesigning such systems. Key prerequisite for implementing functional\nprogramming concepts is properly designed system structure following well\ndefined design principles and rules. That is the main goal of this lecture to\nintroduce students with proper system modeling. Furthermore, we also explain\nhow new emerging technologies are designed in such a way that they enforce the\ndevelopment of systems that comply to the design rules inspired by the\nfunctional programming. This is extremely important in view of the current\nnetwork evolution and virtualization concepts, which will require many\nfunctional programming concepts in the network services and functions, as will\nbe discussed in part II of these lecture notes. These notes provide an\nintroduction to the subject, with the goal of explaining the problems and the\nprinciples, methods and techniques used for their solution. The worked examples\nand exercises serve students as the teaching material, from which they can\nlearn how to use design principles to model effective system structures. Here\nwe focus on students understanding of importance of effective system structures\nfor coordination of development and management processes that are driven by\nbusiness goals and further evolution.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 12:14:50 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Grbac", "Tihana Galinac", ""]]}, {"id": "2107.12144", "submitter": "Robin Kaarsgaard", "authors": "Chris Heunen and Robin Kaarsgaard", "title": "Quantum Information Effects", "comments": "32 pages, including 10 page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the two dual quantum information effects to manipulate the amount of\ninformation in quantum computation: hiding and allocation. The resulting\ntype-and-effect system is fully expressive for irreversible quantum computing,\nincluding measurement. We provide universal categorical constructions that\nsemantically interpret this arrow metalanguage with choice, starting with any\nrig groupoid interpreting the reversible base language. Several properties of\nquantum measurement follow in general, and we translate quantum flow charts\ninto our language. The semantic constructions turn the category of unitaries\nbetween Hilbert spaces into the category of completely positive\ntrace-preserving maps, and they turn the category of bijections between finite\nsets into the category of functions with chosen garbage. Thus they capture the\nfundamental theorems of classical and quantum reversible computing of Toffoli\nand Stinespring.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 12:21:42 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Heunen", "Chris", ""], ["Kaarsgaard", "Robin", ""]]}, {"id": "2107.12567", "submitter": "Yuka Ikarashi", "authors": "Yuka Ikarashi, Jonathan Ragan-Kelley, Tsukasa Fukusato, Jun Kato,\n  Takeo Igarashi", "title": "Guided Optimization for Image Processing Pipelines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Writing high-performance image processing code is challenging and\nlabor-intensive. The Halide programming language simplifies this task by\ndecoupling high-level algorithms from \"schedules\" which optimize their\nimplementation. However, even with this abstraction, it is still challenging\nfor Halide programmers to understand complicated scheduling strategies and\nproductively write valid, optimized schedules. To address this, we propose a\nprogramming support method called \"guided optimization.\" Guided optimization\nprovides programmers a set of valid optimization options and interactive\nfeedback about their current choices, which enables them to comprehend and\nefficiently optimize image processing code without the time-consuming\ntrial-and-error process of traditional text editors. We implemented a\nproof-of-concept system, Roly-poly, which integrates guided optimization,\nprogram visualization, and schedule cost estimation to support the\ncomprehension and development of efficient Halide image processing code. We\nconducted a user study with novice Halide programmers and confirmed that\nRoly-poly and its guided optimization was informative, increased productivity,\nand resulted in higher-performing schedules in less time.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 03:02:17 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 03:00:11 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Ikarashi", "Yuka", ""], ["Ragan-Kelley", "Jonathan", ""], ["Fukusato", "Tsukasa", ""], ["Kato", "Jun", ""], ["Igarashi", "Takeo", ""]]}, {"id": "2107.12568", "submitter": "James Koppel", "authors": "James Koppel", "title": "Version Space Algebras are Acyclic Tree Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Version space algebras are ways of representing spaces of programs which can\nbe combined using union, intersection, and cross-product/``join\" operators. In\ntheir reified form as ASTs with explicit union and join nodes, they have the\nability to compactly represent exponentially-large spaces of programs, owing to\nwhich they have become become the most popular approach to enumerative program\nsynthesis since the introduction of FlashFill in 2010. We present a linear-time\nsemantics-preserving constructive embedding from version space algebras into\nnondeterministic finite tree automata, showing that the former are but a\nspecial case of the latter. Combined with recent results finding a\ncorrespondence between e-graphs and minimal deterministic tree automata, this\nshows that tree automata are strict generalizations of all recent major\napproaches to efficiently representing large spaces of programs by sharing.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 03:02:56 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Koppel", "James", ""]]}, {"id": "2107.12850", "submitter": "Paul Black", "authors": "Paul E. Black, Barbara Guttman, and Vadim Okun (National Institute of\n  Standards and Technology)", "title": "Guidelines on Minimum Standards for Developer Verification of Software", "comments": "30 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Executive Order (EO) 14028, \"Improving the Nation's Cybersecurity,\" 12 May\n2021, directs the National Institute of Standards and Technology (NIST) to\nrecommend minimum standards for software testing within 60 days. This document\ndescribes eleven recommendations for software verification techniques as well\nas providing supplemental information about the techniques and references for\nfurther information. It recommends the following techniques:\n  Threat modeling to look for design-level security issues\n  Automated testing for consistency and to minimize human effort\n  Static code scanning to look for top bugs\n  Heuristic tools to look for possible hardcoded secrets\n  Use of built-in checks and protections\n  \"Black box\" test cases\n  Code-based structural test cases\n  Historical test cases\n  Fuzzing\n  Web app scanners, if applicable\n  Address included code (libraries, packages, services)\n  The document does not address the totality of software verification, but\ninstead recommends techniques that are broadly applicable and form the minimum\nstandards.\n  The document was developed by NIST in consultation with the National Security\nAgency. Additionally, we received input from numerous outside organizations\nthrough papers submitted to a NIST workshop on the Executive Order held in\nearly June, 2021 and discussion at the workshop as well as follow up with\nseveral of the submitters.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 14:33:17 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Black", "Paul E.", "", "National Institute of\n  Standards and Technology"], ["Guttman", "Barbara", "", "National Institute of\n  Standards and Technology"], ["Okun", "Vadim", "", "National Institute of\n  Standards and Technology"]]}, {"id": "2107.12867", "submitter": "Wenqiang Li", "authors": "Wenqiang Li, Le Guan, Jingqiang Lin, Jiameng Shi, Fengjun Li", "title": "From Library Portability to Para-rehosting: Natively Executing\n  Microcontroller Software on Commodity Hardware", "comments": "18 pages, 4 figures, Network and Distributed Systems Security (NDSS)\n  Symposium 2021", "journal-ref": null, "doi": "10.14722/ndss.2021.24308", "report-no": null, "categories": "cs.PL cs.CR cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding bugs in microcontroller (MCU) firmware is challenging, even for\ndevice manufacturers who own the source code. The MCU runs different\ninstruction sets than x86 and exposes a very different development environment.\nThis invalidates many existing sophisticated software testing tools on x86. To\nmaintain a unified developing and testing environment, a straightforward way is\nto re-compile the source code into the native executable for a commodity\nmachine (called rehosting). However, ad-hoc re-hosting is a daunting and\ntedious task and subject to many issues (library-dependence, kernel-dependence\nand hardware-dependence). In this work, we systematically explore the\nportability problem of MCU software and propose pararehosting to ease the\nporting process. Specifically, we abstract and implement a portable MCU (PMCU)\nusing the POSIX interface. It models common functions of the MCU cores. For\nperipheral specific logic, we propose HAL-based peripheral function\nreplacement, in which high-level hardware functions are replaced with an\nequivalent backend driver on the host. These backend drivers are invoked by\nwell-designed para-APIs and can be reused across many MCU OSs. We categorize\ncommon HAL functions into four types and implement templates for quick backend\ndevelopment. Using the proposed approach, we have successfully rehosted nine\nMCU OSs including the widely deployed Amazon FreeRTOS, ARM Mbed OS, Zephyr and\nLiteOS. To demonstrate the superiority of our approach in terms of security\ntesting, we used off-the-shelf dynamic analysis tools (AFL and ASAN) against\nthe rehosted programs and discovered 28 previously-unknown bugs, among which 5\nwere confirmed by CVE and the other 19 were confirmed by vendors at the time of\nwriting.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 16:54:46 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Li", "Wenqiang", ""], ["Guan", "Le", ""], ["Lin", "Jingqiang", ""], ["Shi", "Jiameng", ""], ["Li", "Fengjun", ""]]}, {"id": "2107.12909", "submitter": "Yihao Sun", "authors": "Davis Ross Silverman, Yihao Sun, Kristopher Micinski, Thomas Gilray", "title": "So You Want to Analyze Scheme Programs With Datalog?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Static analysis approximates the results of a program by examining only its\nsyntax. For example, control-flow analysis (CFA) determines which syntactic\nlambdas (for functional languages) or (for object-oriented) methods may be\ninvoked at each call site within a program. Rich theoretical results exist\nstudying control flow analysis for Scheme-like languages, but implementations\nare often complex and specialized. By contrast, object-oriented languages (Java\nin particular) enjoy high-precision control-flow analyses that scale to\nthousands (or more) of lines of code. State-of-the-art implementations (such as\nDOOP on Souffl\\'e) structure the analysis using Horn-SAT (Datalog) to enable\ncompilation of the analysis to efficient implementations such as\nhigh-performance relational algebra kernels. In this paper, we present an\nimplementation of control-flow analysis for a significant subset of Scheme\n(including set!, call/cc, and primitive operations) using the Souffl\\'e Datalog\nengine. We present an evaluation on a worst-case term demonstrating the\npolynomial complexity of our m-CFA and remark upon scalability results using\nSouffl\\'e.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 16:07:11 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Silverman", "Davis Ross", ""], ["Sun", "Yihao", ""], ["Micinski", "Kristopher", ""], ["Gilray", "Thomas", ""]]}, {"id": "2107.13072", "submitter": "Marcel Moosbrugger", "authors": "Marcel Moosbrugger, Ezio Bartocci, Joost-Pieter Katoen, Laura Kov\\'acs", "title": "The Probabilistic Termination Tool Amber", "comments": "Accepted to FM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe the Amber tool for proving and refuting the termination of a\nclass of probabilistic while-programs with polynomial arithmetic, in a fully\nautomated manner. Amber combines martingale theory with properties of\nasymptotic bounding functions and implements relaxed versions of existing\nprobabilistic termination proof rules to prove/disprove (positive) almost sure\ntermination of probabilistic loops. Amber supports programs parameterized by\nsymbolic constants and drawing from common probability distributions. Our\nexperimental comparisons give practical evidence of Amber outperforming\nexisting state-of-the-art tools.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 20:17:50 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Moosbrugger", "Marcel", ""], ["Bartocci", "Ezio", ""], ["Katoen", "Joost-Pieter", ""], ["Kov\u00e1cs", "Laura", ""]]}, {"id": "2107.13101", "submitter": "Mathias Jakobsen", "authors": "Mathias Jakobsen and Alice Ravier and Ornela Dardha", "title": "Papaya: Global Typestate Analysis of Aliased Objects Extended Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typestates are state machines used in object-oriented programming to specify\nand verify correct order of method calls on an object. To avoid inconsistent\nobject states, typestates enforce linear typing, which eliminates - or at best\nlimits - aliasing. However, aliasing is an important feature in programming,\nand the state-of-the-art on typestates is too restrictive if we want typestates\nto be adopted in real-world software systems.\n  In this paper, we present a type system for an object-oriented language with\ntypestate annotations, which allows for unrestricted aliasing, and as opposed\nto previous approaches it does not require linearity constraints. The typestate\nanalysis is global and tracks objects throughout the entire program graph,\nwhich ensures that well-typed programs conform and complete the declared\nprotocols. We implement our framework in the Scala programming language and\nillustrate our approach using a running example that shows the interplay\nbetween typestates and aliases.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 23:04:55 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Jakobsen", "Mathias", ""], ["Ravier", "Alice", ""], ["Dardha", "Ornela", ""]]}, {"id": "2107.13242", "submitter": "Tesla Zhang", "authors": "Tesla Zhang", "title": "Type theories in category theory", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We introduce basic notions in category theory to type theorists, including\ncomprehension categories, categories with attributes, contextual categories,\ntype categories, and categories with families along with additional discussions\nthat are not very closely related to type theories by listing definitions,\nlemmata, and remarks. By doing so, this introduction becomes more friendly as a\nreferential material to be read in random order (instead of from the beginning\nto the end). In the end, we list some mistakes made in the early versions of\nthis introduction.\n  The interpretation of common type formers in dependent type theories are\ndiscussed based on existing categorical constructions instead of mechanically\nderived from their type theoretical definition. Non-dependent type formers\ninclude unit, products (as fiber products), and functions (as fiber exponents),\nand dependent ones include extensional equalities (as equalizers), dependent\nproducts, and the universe of (all) propositions (as the subobject classifier).\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 09:54:39 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Zhang", "Tesla", ""]]}, {"id": "2107.13347", "submitter": "Vladimir Zamdzhiev", "authors": "Xiaodong Jia, Andre Kornell, Bert Lindenhovius, Michael Mislove,\n  Vladimir Zamdzhiev", "title": "Semantics for Variational Quantum Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL math.CT math.OA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a programming language that can manipulate both classical and\nquantum information. Our language is type-safe and designed for variational\nquantum programming, which is a hybrid classical-quantum computational\nparadigm. The classical subsystem of the language is the Probabilistic FixPoint\nCalculus (PFPC), which is a lambda calculus with mixed-variance recursive\ntypes, term recursion and probabilistic choice. The quantum subsystem is a\nfirst-order linear type system that can manipulate quantum information. The two\nsubsystems are related by mixed classical/quantum terms that specify how\nclassical probabilistic effects are induced by quantum measurements, and\nconversely, how classical (probabilistic) programs can influence the quantum\ndynamics. We also describe a sound and computationally adequate denotational\nsemantics for the language. Classical probabilistic effects are interpreted\nusing a recently-described commutative probabilistic monad on DCPO. Quantum\neffects and resources are interpreted in a category of von Neumann algebras\nthat we show is enriched over (continuous) domains. This strong sense of\nenrichment allows us to develop novel semantic methods that we use to interpret\nthe relationship between the quantum and classical probabilistic effects. By\ndoing so we provide the first denotational analysis that relates models of\nclassical probabilistic programming to models of quantum programming.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 13:22:24 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Jia", "Xiaodong", ""], ["Kornell", "Andre", ""], ["Lindenhovius", "Bert", ""], ["Mislove", "Michael", ""], ["Zamdzhiev", "Vladimir", ""]]}, {"id": "2107.13433", "submitter": "David Sprunger", "authors": "Mario Alvarez-Picallo, Dan R. Ghica, David Sprunger, Fabio Zanasi", "title": "Functorial String Diagrams for Reverse-Mode Automatic Differentiation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We enhance the calculus of string diagrams for monoidal categories with\nhierarchical features in order to capture closed monoidal (and cartesian\nclosed) structure. Using this new syntax we formulate an automatic\ndifferentiation algorithm for (applied) simply typed lambda calculus in the\nstyle of [Pearlmutter and Siskind 2008] and we prove for the first time its\nsoundness. To give an efficient yet principled implementation of the AD\nalgorithm we define a sound and complete representation of hierarchical string\ndiagrams as a class of hierarchical hypergraphs we call hypernets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 15:25:32 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Alvarez-Picallo", "Mario", ""], ["Ghica", "Dan R.", ""], ["Sprunger", "David", ""], ["Zanasi", "Fabio", ""]]}, {"id": "2107.13477", "submitter": "Elizabeth Polgreen", "authors": "Elizabeth Polgreen, Andrew Reynolds and Sanjit A. Seshia", "title": "Satisfiability and Synthesis Modulo Oracles", "comments": "12 pages, 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In classic program synthesis algorithms, such as counterexample-guided\ninductive synthesis (CEGIS), the algorithms alternate between a synthesis phase\nand an oracle (verification) phase. Many synthesis algorithms use a white-box\noracle based on satisfiability modulo theory (SMT) solvers to provide\ncounterexamples. But what if a white-box oracle is either not available or not\neasy to work with? We present a framework for solving a general class of\noracle-guided synthesis problems which we term synthesis modulo oracles. In\nthis setting, oracles may be black boxes with a query-response interface\ndefined by the synthesis problem. As a necessary component of this framework,\nwe also formalize the problem of satisfiability modulo theories and oracles,\nand present an algorithm for solving this problem. We implement a prototype\nsolver for satisfiability and synthesis modulo oracles and demonstrate that, by\nusing oracles that execute functions not easily modeled in SMT-constraints,\nsuch as recursive functions or oracles that incorporate compilation and\nexecution of code, SMTO and SyMO are able to solve problems beyond the\nabilities of standard SMT and synthesis solvers.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 16:36:26 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Polgreen", "Elizabeth", ""], ["Reynolds", "Andrew", ""], ["Seshia", "Sanjit A.", ""]]}]