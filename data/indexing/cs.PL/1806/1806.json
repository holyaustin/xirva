[{"id": "1806.00441", "submitter": "Miguel Areias", "authors": "Miguel Areias and Ricardo Rocha", "title": "Table Space Designs For Implicit and Explicit Concurrent Tabled\n  Evaluation", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main advantages of Prolog is its potential for the implicit\nexploitation of parallelism and, as a high-level language, Prolog is also often\nused as a means to explicitly control concurrent tasks. Tabling is a powerful\nimplementation technique that overcomes some limitations of traditional Prolog\nsystems in dealing with recursion and redundant sub-computations. Given these\nadvantages, the question that arises is if tabling has also the potential for\nthe exploitation of concurrency/parallelism. On one hand, tabling still\nexploits a search space as traditional Prolog but, on the other hand, the\nconcurrent model of tabling is necessarily far more complex since it also\nintroduces concurrency on the access to the tables. In this paper, we summarize\nYap's main contributions to concurrent tabled evaluation and we describe the\ndesign and implementation challenges of several alternative table space designs\nfor implicit and explicit concurrent tabled evaluation which represent\ndifferent trade-offs between concurrency and memory usage. We also motivate for\nthe advantages of using fixed-size and lock-free data structures, elaborate on\nthe key role that the engine's memory allocator plays on such environments, and\ndiscuss how Yap's mode-directed tabling support can be extended to concurrent\nevaluation. Finally, we present our future perspectives towards an efficient\nand novel concurrent framework which integrates both implicit and explicit\nconcurrent tabled evaluation in a single Prolog engine. Under consideration in\nTheory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 16:57:20 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Areias", "Miguel", ""], ["Rocha", "Ricardo", ""]]}, {"id": "1806.00842", "submitter": "Michael Bar-Sinai", "authors": "Michael Bar-Sinai, Gera Weiss, Reut Shmuel", "title": "BPjs --- a framework for modeling reactive systems using a scripting\n  language and BP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.FL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe some progress towards a new common framework for model driven\nengineering, based on behavioral programming. The tool we have developed\nunifies almost all of the work done in behavioral programming so far, under a\ncommon set of interfaces. Its architecture supports pluggable event selection\nstrategies, which can make models more intuitive and compact. Program state\nspace can be traversed using various algorithms, such as DFS and A*.\nFurthermore, program state is represented in a way that enables scanning a\nstate space using parallel and distributed algorithms. Executable models\ncreated with this tool can be directly embedded in Java applications, enabling\na model-first approach to system engineering, where initially a model is\ncreated and verified, and then a working application is gradually built around\nthe model. The model itself consists of a collection of small scripts written\nin JavaScript (hence \"BPjs\"). Using a variety of case-studies, this paper shows\nhow the combination of a lenient programming language with formal model\nanalysis tools creates an efficient way of developing robust complex systems.\nAdditionally, as we learned from an experimental course we ran, the usage of\nJavaScript make practitioners more amenable to using this system and, thus,\nmodel checking and model driven engineering. In addition to providing\ninfrastructure for development and case-studies in behavioral programming, the\ntool is designed to serve as a common platform for research and innovation in\nbehavioral programming and in model driven engineering in general.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 17:48:22 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Bar-Sinai", "Michael", ""], ["Weiss", "Gera", ""], ["Shmuel", "Reut", ""]]}, {"id": "1806.00938", "submitter": "Ara Vartanian", "authors": "Evan Hernandez, Ara Vartanian, and Xiaojin Zhu", "title": "Program Synthesis from Visual Specification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program synthesis is the process of automatically translating a specification\ninto computer code. Traditional synthesis settings require a formal, precise\nspecification. Motivated by computer education applications where a student\nlearns to code simple turtle-style drawing programs, we study a novel synthesis\nsetting where only a noisy user-intention drawing is specified. This allows\nstudents to sketch their intended output, optionally together with their own\nincomplete program, to automatically produce a completed program. We formulate\nthis synthesis problem as search in the space of programs, with the score of a\nstate being the Hausdorff distance between the program output and the user\ndrawing. We compare several search algorithms on a corpus consisting of real\nuser drawings and the corresponding programs, and demonstrate that our\nalgorithms can synthesize programs optimally satisfying the specification.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 03:24:34 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Hernandez", "Evan", ""], ["Vartanian", "Ara", ""], ["Zhu", "Xiaojin", ""]]}, {"id": "1806.01405", "submitter": "Aleksandar Prokopec", "authors": "Aleksandar Prokopec, Fengyun Liu", "title": "On the Soundness of Coroutines with Snapshots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coroutines are a general control flow construct that can eliminate control\nflow fragmentation inherent in event-driven programs, and are still missing in\nmany popular languages. Coroutines with snapshots are a first-class, type-safe,\nstackful coroutine model, which unifies many variants of suspendable computing,\nand is sufficiently general to express iterators, single-assignment variables,\nasync-await, actors, event streams, backtracking, symmetric coroutines and\ncontinuations. In this paper, we develop a formal model called\n$\\lambda_{\\rightsquigarrow}$ (lambda-squiggly) that captures the essence of\ntype-safe, stackful, delimited coroutines with snapshots. We prove the standard\nprogress and preservation safety properties. Finally, we show a formal\ntransformation from the $\\lambda_{\\rightsquigarrow}$ calculus to the\nsimply-typed lambda calculus with references.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 21:49:33 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Prokopec", "Aleksandar", ""], ["Liu", "Fengyun", ""]]}, {"id": "1806.02027", "submitter": "Yi Wu", "authors": "Yi Wu, Siddharth Srivastava, Nicholas Hay, Simon Du, Stuart Russell", "title": "Discrete-Continuous Mixtures in Probabilistic Programming: Generalized\n  Semantics and Inference Algorithms", "comments": "Accepted by ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent successes of probabilistic programming languages (PPLs) in\nAI applications, PPLs offer only limited support for random variables whose\ndistributions combine discrete and continuous elements. We develop the notion\nof measure-theoretic Bayesian networks (MTBNs) and use it to provide more\ngeneral semantics for PPLs with arbitrarily many random variables defined over\narbitrary measure spaces. We develop two new general sampling algorithms that\nare provably correct under the MTBN framework: the lexicographic likelihood\nweighting (LLW) for general MTBNs and the lexicographic particle filter (LPF),\na specialized algorithm for state-space models. We further integrate MTBNs into\na widely used PPL system, BLOG, and verify the effectiveness of the new\ninference algorithms through representative examples.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 06:37:46 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 10:32:36 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 05:33:34 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Wu", "Yi", ""], ["Srivastava", "Siddharth", ""], ["Hay", "Nicholas", ""], ["Du", "Simon", ""], ["Russell", "Stuart", ""]]}, {"id": "1806.02136", "submitter": "Amir Shaikhha", "authors": "Amir Shaikhha, Andrew Fitzgibbon, Dimitrios Vytiniotis, Simon Peyton\n  Jones, Christoph Koch", "title": "Efficient Differentiable Programming in a Functional Array-Processing\n  Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG cs.PL cs.SC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for the automatic differentiation of a higher-order\nfunctional array-processing language. The core functional language underlying\nthis system simultaneously supports both source-to-source automatic\ndifferentiation and global optimizations such as loop transformations. Thanks\nto this feature, we demonstrate how for some real-world machine learning and\ncomputer vision benchmarks, the system outperforms the state-of-the-art\nautomatic differentiation tools.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 11:54:34 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Shaikhha", "Amir", ""], ["Fitzgibbon", "Andrew", ""], ["Vytiniotis", "Dimitrios", ""], ["Jones", "Simon Peyton", ""], ["Koch", "Christoph", ""]]}, {"id": "1806.02444", "submitter": "Chao Wang", "authors": "Meng Wu, Shengjian Guo, Patrick Schaumont, Chao Wang", "title": "Eliminating Timing Side-Channel Leaks using Program Repair", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method, based on program analysis and transformation, for\neliminating timing side channels in software code that implements\nsecurity-critical applications. Our method takes as input the original program\ntogether with a list of secret variables (e.g., cryptographic keys, security\ntokens, or passwords) and returns the transformed program as output. The\ntransformed program is guaranteed to be functionally equivalent to the original\nprogram and free of both instruction- and cache-timing side channels.\nSpecifically, we ensure that the number of CPU cycles taken to execute any path\nis independent of the secret data, and the cache behavior of memory accesses,\nin terms of hits and misses, is independent of the secret data. We have\nimplemented our method in LLVM and validated its effectiveness on a large set\nof applications, which are cryptographic libraries with 19,708 lines of C/C++\ncode in total. Our experiments show the method is both scalable for real\napplications and effective in eliminating timing side channels.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 22:27:31 GMT"}, {"version": "v2", "created": "Sat, 21 Jul 2018 16:39:57 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Wu", "Meng", ""], ["Guo", "Shengjian", ""], ["Schaumont", "Patrick", ""], ["Wang", "Chao", ""]]}, {"id": "1806.02577", "submitter": "Roberta Calegari", "authors": "Roberta Calegari, Enrico Denti, Stefano Mariani, Andrea Omicini", "title": "Logic Programming as a Service", "comments": null, "journal-ref": "CALEGARI, R., DENTI, E., MARIANI, S., & OMICINI, A. (2018). Logic\n  programming as a service. Theory and Practice of Logic Programming, 18(5-6),\n  846-873", "doi": "10.1017/S1471068418000364", "report-no": null, "categories": "cs.AI cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  New generations of distributed systems are opening novel perspectives for\nlogic programming (LP): on the one hand, service-oriented architectures\nrepresent nowadays the standard approach for distributed systems engineering;\non the other hand, pervasive systems mandate for situated intelligence. In this\npaper we introduce the notion of Logic Programming as a Service (LPaaS) as a\nmeans to address the needs of pervasive intelligent systems through logic\nengines exploited as a distributed service. First we define the abstract\narchitectural model by re-interpreting classical LP notions in the new context;\nthen we elaborate on the nature of LP interpreted as a service by describing\nthe basic LPaaS interface. Finally, we show how LPaaS works in practice by\ndiscussing its implementation in terms of distributed tuProlog engines,\naccounting for basic issues such as interoperability and configurability.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 09:24:01 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 12:27:36 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Calegari", "Roberta", ""], ["Denti", "Enrico", ""], ["Mariani", "Stefano", ""], ["Omicini", "Andrea", ""]]}, {"id": "1806.02693", "submitter": "Aaron Weiss", "authors": "Aaron Weiss, Daniel Patterson, and Amal Ahmed", "title": "Rust Distilled: An Expressive Tower of Languages", "comments": "ML '18 Final", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rust represents a major advancement in production programming languages\nbecause of its success in bridging the gap between high-level application\nprogramming and low-level systems programming. At the heart of its design lies\na novel approach to ownership that remains highly programmable.\n  In this talk, we will describe our ongoing work on designing a formal\nsemantics for Rust that captures ownership and borrowing without the details of\nlifetime analysis. This semantics models a high-level understanding of\nownership and as a result is close to source-level Rust (but with full type\nannotations) which differs from the recent RustBelt effort that essentially\nmodels MIR, a CPS-style IR used in the Rust compiler. Further, while RustBelt\naims to verify the safety of unsafe code in Rust's standard library, we model\nstandard library APIs as primitives, which is sufficient to reason about their\nbehavior. This yields a simpler model of Rust and its type system that we think\nresearchers will find easier to use as a starting point for investigating Rust\nextensions. Unlike RustBelt, we aim to prove type soundness using progress and\npreservation instead of a Kripke logical relation. Finally, our semantics is a\nfamily of languages of increasing expressive power, where subsequent levels\nhave features that are impossible to define in previous levels. Following\nFelleisen, expressive power is defined in terms of observational equivalence.\nSeparating the language into different levels of expressive power should\nprovide a framework for future work on Rust verification and compiler\noptimization.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 14:13:04 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 18:34:19 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Weiss", "Aaron", ""], ["Patterson", "Daniel", ""], ["Ahmed", "Amal", ""]]}, {"id": "1806.02932", "submitter": "Riley Simmons-Edler", "authors": "Riley Simmons-Edler, Anders Miltner, Sebastian Seung", "title": "Program Synthesis Through Reinforcement Learning Guided Tree Search", "comments": "9 pages, 5 figures, Submitted to NIPS 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program Synthesis is the task of generating a program from a provided\nspecification. Traditionally, this has been treated as a search problem by the\nprogramming languages (PL) community and more recently as a supervised learning\nproblem by the machine learning community. Here, we propose a third approach,\nrepresenting the task of synthesizing a given program as a Markov decision\nprocess solvable via reinforcement learning(RL). From observations about the\nstates of partial programs, we attempt to find a program that is optimal over a\nprovided reward metric on pairs of programs and states. We instantiate this\napproach on a subset of the RISC-V assembly language operating on floating\npoint numbers, and as an optimization inspired by search-based techniques from\nthe PL community, we combine RL with a priority search tree. We evaluate this\ninstantiation and demonstrate the effectiveness of our combined method compared\nto a variety of baselines, including a pure RL ablation and a state of the art\nMarkov chain Monte Carlo search method on this task.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 00:53:43 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Simmons-Edler", "Riley", ""], ["Miltner", "Anders", ""], ["Seung", "Sebastian", ""]]}, {"id": "1806.03108", "submitter": "Amir Kafshdar Goharshady", "authors": "Krishnendu Chatterjee, Amir Kafshdar Goharshady, Rasmus Ibsen-Jensen,\n  Yaron Velner", "title": "Ergodic Mean-Payoff Games for the Analysis of Attacks in\n  Crypto-Currencies", "comments": "Accepted to CONCUR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.GT cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crypto-currencies are digital assets designed to work as a medium of\nexchange, e.g., Bitcoin, but they are susceptible to attacks (dishonest\nbehavior of participants). A framework for the analysis of attacks in\ncrypto-currencies requires (a) modeling of game-theoretic aspects to analyze\nincentives for deviation from honest behavior; (b) concurrent interactions\nbetween participants; and (c) analysis of long-term monetary gains. Traditional\ngame-theoretic approaches for the analysis of security protocols consider\neither qualitative temporal properties such as safety and termination, or the\nvery special class of one-shot (stateless) games. However, to analyze general\nattacks on protocols for crypto-currencies, both stateful analysis and\nquantitative objectives are necessary. In this work our main contributions are\nas follows: (a) we show how a class of concurrent mean-payoff games, namely\nergodic games, can model various attacks that arise naturally in\ncrypto-currencies; (b) we present the first practical implementation of\nalgorithms for ergodic games that scales to model realistic problems for\ncrypto-currencies; and (c) we present experimental results showing that our\nframework can handle games with thousands of states and millions of\ntransitions.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 12:12:27 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Goharshady", "Amir Kafshdar", ""], ["Ibsen-Jensen", "Rasmus", ""], ["Velner", "Yaron", ""]]}, {"id": "1806.03205", "submitter": "Fabian Kunze", "authors": "Fabian Kunze, Gert Smolka, Yannick Forster", "title": "Formal Small-step Verification of a Call-by-value Lambda Calculus\n  Machine", "comments": null, "journal-ref": "APLAS 2018, LNCS 11275, pp. 264-283", "doi": "10.1007/978-3-030-02768-1_15", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formally verify an abstract machine for a call-by-value lambda-calculus\nwith de Bruijn terms, simple substitution, and small-step semantics. We follow\na stepwise refinement approach starting with a naive stack machine with\nsubstitution. We then refine to a machine with closures, and finally to a\nmachine with a heap providing structure sharing for closures. We prove the\ncorrectness of the three refinement steps with compositional small-step\nbottom-up simulations. There is an accompanying Coq development verifying all\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 14:59:07 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 13:33:29 GMT"}, {"version": "v3", "created": "Wed, 2 Jan 2019 15:04:41 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Kunze", "Fabian", ""], ["Smolka", "Gert", ""], ["Forster", "Yannick", ""]]}, {"id": "1806.03476", "submitter": "Joachim Breitner", "authors": "Richard A. Eisenberg and Joachim Breitner and Simon Peyton Jones", "title": "Type variables in patterns", "comments": "Submitted to Haskell'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many years, GHC has implemented an extension to Haskell that allows type\nvariables to be bound in type signatures and patterns, and to scope over terms.\nThis extension was never properly specified. We rectify that oversight here.\nWith the formal specification in hand, the otherwise-labyrinthine path toward a\ndesign for binding type variables in patterns becomes blindingly clear. We thus\nextend ScopedTypeVariables to bind type variables explicitly, obviating the\nProxy workaround to the dustbin of history.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 13:43:01 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Eisenberg", "Richard A.", ""], ["Breitner", "Joachim", ""], ["Jones", "Simon Peyton", ""]]}, {"id": "1806.03541", "submitter": "Joachim Breitner", "authors": "Niki Vazou and Joachim Breitner and Will Kunkel and David Van Horn and\n  Graham Hutton", "title": "Functional Pearl: Theorem Proving for All (Equational Reasoning in\n  Liquid Haskell)", "comments": "Submitted to Haskell'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Equational reasoning is one of the key features of pure functional languages\nsuch as Haskell. To date, however, such reasoning always took place externally\nto Haskell, either manually on paper, or mechanised in a theorem prover. This\narticle shows how equational reasoning can be performed directly and seamlessly\nwithin Haskell itself, and be checked using Liquid Haskell. In particular,\nlanguage learners --- to whom external theorem provers are out of reach --- can\nbenefit from having their proofs mechanically checked. Concretely, we show how\nthe equational proofs and derivations from Graham's textbook can be recast as\nproofs in Haskell (spoiler: they look essentially the same).\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 20:57:09 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Vazou", "Niki", ""], ["Breitner", "Joachim", ""], ["Kunkel", "Will", ""], ["Van Horn", "David", ""], ["Hutton", "Graham", ""]]}, {"id": "1806.03806", "submitter": "Ketan Patil", "authors": "Ketan Patil, Aditya Kanade", "title": "Greybox fuzzing as a contextual bandits problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Greybox fuzzing is one of the most useful and effective techniques for the\nbug detection in large scale application programs. It uses minimal amount of\ninstrumentation. American Fuzzy Lop (AFL) is a popular coverage based\nevolutionary greybox fuzzing tool. AFL performs extremely well in fuzz testing\nlarge applications and finding critical vulnerabilities, but AFL involves a lot\nof heuristics while deciding the favored test case(s), skipping test cases\nduring fuzzing, assigning fuzzing iterations to test case(s). In this work, we\naim at replacing the heuristics the AFL uses while assigning the fuzzing\niterations to a test case during the random fuzzing. We formalize this problem\nas a `contextual bandit problem' and we propose an algorithm to solve this\nproblem. We have implemented our approach on top of the AFL. We modify the\nAFL's heuristics with our learned model through the policy gradient method. Our\nlearning algorithm selects the multiplier of the number of fuzzing iterations\nto be assigned to a test case during random fuzzing, given a fixed length\nsubstring of the test case to be fuzzed. We fuzz the substring with this new\nenergy value and continuously updates the policy based upon the interesting\ntest cases it produces on fuzzing.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 04:49:00 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Patil", "Ketan", ""], ["Kanade", "Aditya", ""]]}, {"id": "1806.03959", "submitter": "Petr Ro\\v{c}kai", "authors": "Henrich Lauko, Petr Ro\\v{c}kai, Ji\\v{r}\\'i Barnat", "title": "Symbolic Computation via Program Transformation", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-02508-3_17", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic computation is an important approach in automated program analysis.\nMost state-of-the-art tools perform symbolic computation as interpreters and\ndirectly maintain symbolic data. In this paper, we show that it is feasible,\nand in fact practical, to use a compiler-based strategy instead. Using compiler\ntooling, we propose and implement a transformation which takes a standard\nprogram and outputs a program that performs semantically equivalent, but\npartially symbolic, computation. The transformed program maintains symbolic\nvalues internally and operates directly on them hence the program can be\nprocessed by a tool without support for symbolic manipulation.\n  The main motivation for the transformation is in symbolic verification, but\nthere are many other possible use-cases, including test generation and concolic\ntesting. Moreover using the transformation simplifies tools, since the symbolic\ncomputation is handled by the program directly. We have implemented the\ntransformation at the level of LLVM bitcode. The paper includes an experimental\nevaluation, based on an explicit-state software model checker as a verification\nbackend.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 11:35:52 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Lauko", "Henrich", ""], ["Ro\u010dkai", "Petr", ""], ["Barnat", "Ji\u0159\u00ed", ""]]}, {"id": "1806.04355", "submitter": "Stijn Volckaert", "authors": "Dokyung Song, Julian Lettner, Prabhu Rajasekaran, Yeoul Na, Stijn\n  Volckaert, Per Larsen, Michael Franz", "title": "SoK: Sanitizing for Security", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The C and C++ programming languages are notoriously insecure yet remain\nindispensable. Developers therefore resort to a multi-pronged approach to find\nsecurity issues before adversaries. These include manual, static, and dynamic\nprogram analysis. Dynamic bug finding tools --- henceforth \"sanitizers\" --- can\nfind bugs that elude other types of analysis because they observe the actual\nexecution of a program, and can therefore directly observe incorrect program\nbehavior as it happens.\n  A vast number of sanitizers have been prototyped by academics and refined by\npractitioners. We provide a systematic overview of sanitizers with an emphasis\non their role in finding security issues. Specifically, we taxonomize the\navailable tools and the security vulnerabilities they cover, describe their\nperformance and compatibility properties, and highlight various trade-offs.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 06:36:30 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Song", "Dokyung", ""], ["Lettner", "Julian", ""], ["Rajasekaran", "Prabhu", ""], ["Na", "Yeoul", ""], ["Volckaert", "Stijn", ""], ["Larsen", "Per", ""], ["Franz", "Michael", ""]]}, {"id": "1806.04556", "submitter": "Stefan Wagner", "authors": "Tobias Roehm and Daniel Veihelmann and Stefan Wagner and Elmar\n  Juergens", "title": "Evaluating Maintainability Prejudices with a Large-Scale Study of\n  Open-Source Projects", "comments": "20 pages", "journal-ref": "In D. Winkler, S. Biffl, and J. Bergsmann, editors, Software\n  Quality: The Complexity and Challenges of Software Engineering and Software\n  Quality in the Cloud, Springer International Publishing, 2019", "doi": "10.1007/978-3-030-05767-1_10", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exaggeration or context changes can render maintainability experience into\nprejudice. For example, JavaScript is often seen as least elegant language and\nhence of lowest maintainability. Such prejudice should not guide decisions\nwithout prior empirical validation. We formulated 10 hypotheses about\nmaintainability based on prejudices and test them in a large set of open-source\nprojects (6,897 GitHub repositories, 402 million lines, 5 programming\nlanguages). We operationalize maintainability with five static analysis\nmetrics. We found that JavaScript code is not worse than other code, Java code\nshows higher maintainability than C# code and C code has longer methods than\nother code. The quality of interface documentation is better in Java code than\nin other code. Code developed by teams is not of higher and large code bases\nnot of lower maintainability. Projects with high maintainability are not more\npopular or more often forked. Overall, most hypotheses are not supported by\nopen-source data.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 14:29:01 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Roehm", "Tobias", ""], ["Veihelmann", "Daniel", ""], ["Wagner", "Stefan", ""], ["Juergens", "Elmar", ""]]}, {"id": "1806.04709", "submitter": "Christopher Jenkins", "authors": "Aaron Stump and Christopher Jenkins", "title": "Syntax and Semantics of Cedille", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document presents the syntax, classification rules, realizability\nsemantics, and soundness theorem for Cedille, an extrinsic (i.e., Curry-style)\ntype theory extending the Calculus of Constructions, and designed for deriving\nof inductive datatypes, with their induction principles.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 18:45:55 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 04:20:21 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 20:56:16 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Stump", "Aaron", ""], ["Jenkins", "Christopher", ""]]}, {"id": "1806.04929", "submitter": "Kai Mindermann M.Sc.", "authors": "Kai Mindermann, Philipp Keck, Stefan Wagner", "title": "How Usable are Rust Cryptography APIs?", "comments": null, "journal-ref": null, "doi": "10.1109/QRS.2018.00028", "report-no": null, "categories": "cs.CR cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Poor usability of cryptographic APIs is a severe source of\nvulnerabilities. Aim: We wanted to find out what kind of cryptographic\nlibraries are present in Rust and how usable they are. Method: We explored\nRust's cryptographic libraries through a systematic search, conducted an\nexploratory study on the major libraries and a controlled experiment on two of\nthese libraries with 28 student participants. Results: Only half of the major\nlibraries explicitly focus on usability and misuse resistance, which is\nreflected in their current APIs. We found that participants were more\nsuccessful using rust-crypto which we considered less usable than ring before\nthe experiment. Conclusion: We discuss API design insights and make\nrecommendations for the design of crypto libraries in Rust regarding the detail\nand structure of the documentation, higher-level APIs as wrappers for the\nexisting low-level libraries, and selected, good-quality example code to\nimprove the emerging cryptographic libraries of Rust.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 10:21:48 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 09:08:49 GMT"}, {"version": "v3", "created": "Tue, 26 Jun 2018 07:21:25 GMT"}, {"version": "v4", "created": "Wed, 18 Jul 2018 13:58:39 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Mindermann", "Kai", ""], ["Keck", "Philipp", ""], ["Wagner", "Stefan", ""]]}, {"id": "1806.05230", "submitter": "Peng Fu", "authors": "Peng Fu, Peter Selinger", "title": "Dependently Typed Folds for Nested Data Types", "comments": "source code for each section is at:\n  https://github.com/Fermat/dependent-fold", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to develop folds for nested data types using dependent\ntypes. We call such folds $\\textit{dependently typed folds}$, they have the\nfollowing properties. (1) Dependently typed folds are defined by well-founded\nrecursion and they can be defined in a total dependently typed language. (2)\nDependently typed folds do not depend on maps, map functions and many\nterminating functions can be defined using dependently typed folds. (3) The\ninduction principles for nested data types follow from the definitions of\ndependently typed folds and the programs defined by dependently typed folds can\nbe formally verified. (4) Dependently typed folds exist for any nested data\ntypes and they can be specialized to the traditional $\\textit{higher-order\nfolds}$. Using various of examples, we show how to program and reason about\ndependently typed folds. We also show how to obtain dependently typed folds in\ngeneral and how to specialize them to the corresponding higher-order folds.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 19:12:56 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Fu", "Peng", ""], ["Selinger", "Peter", ""]]}, {"id": "1806.07041", "submitter": "Taro Sekiyama", "authors": "Taro Sekiyama, Atsushi Igarashi", "title": "Reasoning about Polymorphic Manifest Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifest contract calculi, which integrate cast-based dynamic contract\nchecking and refinement type systems, have been studied as foundations for\nhybrid contract checking. In this article, we study techniques to reasoning\nabout a polymorphic manifest contract calculus, including a few program\ntransformations related to static contract verification. We first define a\npolymorphic manifest contract calculus $\\mathrm{F}_{H}$, which is much simpler\nthan a previously studied one with delayed substitution, and a logical relation\nfor it and prove that the logical relation is sound with respect to contextual\nequivalence. Next, we show that the upcast elimination property, which has been\nstudied as correctness of subtyping-based static cast verification, holds for\n$\\mathrm{F}_{H}$. More specifically, we give a subtyping relation (which is not\npart of the calculus) for $\\mathrm{F}_{H}$ types and prove that a term obtained\nby eliminating upcasts---casts from one type to a supertype of it---is\nlogically related and so contextually equivalent to the original one. We also\njustify two other program transformations for casts: selfification and static\ncast decomposition, which help upcast elimination. A challenge is that, due to\nthe subsumption-free approach to manifest contracts, these program\ntransformations do not always preserve well-typedness of terms. To address it,\nthe logical relation and contextual equivalence in this work are defined as\nsemityped relations: only one side of the relations is required to be well\ntyped and the other side may be ill typed.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 05:08:27 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Sekiyama", "Taro", ""], ["Igarashi", "Atsushi", ""]]}, {"id": "1806.07100", "submitter": "Germ\\'an Vidal", "authors": "Ivan Lanese, Naoki Nishida, Adri\\'an Palacios, and Germ\\'an Vidal", "title": "A Theory of Reversibility for Erlang", "comments": "To appear in the Journal of Logical and Algebraic Methods in\n  Programming (Elsevier)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a reversible language, any forward computation can be undone by a finite\nsequence of backward steps. Reversible computing has been studied in the\ncontext of different programming languages and formalisms, where it has been\nused for testing and verification, among others. In this paper, we consider a\nsubset of Erlang, a functional and concurrent programming language based on the\nactor model. We present a formal semantics for reversible computation in this\nlanguage and prove its main properties, including its causal consistency. We\nalso build on top of it a rollback operator that can be used to undo the\nactions of a process up to a given checkpoint.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 08:35:51 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Lanese", "Ivan", ""], ["Nishida", "Naoki", ""], ["Palacios", "Adri\u00e1n", ""], ["Vidal", "Germ\u00e1n", ""]]}, {"id": "1806.07336", "submitter": "Tal Ben-Nun", "authors": "Tal Ben-Nun, Alice Shoshana Jakobovits, Torsten Hoefler", "title": "Neural Code Comprehension: A Learnable Representation of Code Semantics", "comments": "Published at NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent success of embeddings in natural language processing,\nresearch has been conducted into applying similar methods to code analysis.\nMost works attempt to process the code directly or use a syntactic tree\nrepresentation, treating it like sentences written in a natural language.\nHowever, none of the existing methods are sufficient to comprehend program\nsemantics robustly, due to structural features such as function calls,\nbranching, and interchangeable order of statements. In this paper, we propose a\nnovel processing technique to learn code semantics, and apply it to a variety\nof program analysis tasks. In particular, we stipulate that a robust\ndistributional hypothesis of code applies to both human- and machine-generated\nprograms. Following this hypothesis, we define an embedding space, inst2vec,\nbased on an Intermediate Representation (IR) of the code that is independent of\nthe source programming language. We provide a novel definition of contextual\nflow for this IR, leveraging both the underlying data- and control-flow of the\nprogram. We then analyze the embeddings qualitatively using analogies and\nclustering, and evaluate the learned representation on three different\nhigh-level tasks. We show that even without fine-tuning, a single RNN\narchitecture and fixed inst2vec embeddings outperform specialized approaches\nfor performance prediction (compute device mapping, optimal thread coarsening);\nand algorithm classification from raw code (104 classes), where we set a new\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 16:43:44 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 16:10:52 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 08:15:00 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Ben-Nun", "Tal", ""], ["Jakobovits", "Alice Shoshana", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1806.07449", "submitter": "Mat\\'u\\v{s} Sul\\'ir", "authors": "Mat\\'u\\v{s} Sul\\'ir and Jaroslav Porub\\\"an", "title": "Augmenting Source Code Lines with Sample Variable Values", "comments": null, "journal-ref": "ICPC '18: 26th IEEE/ACM International Conference on Program\n  Comprehension, ACM, 2018", "doi": "10.1145/3196321.3196364", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source code is inherently abstract, which makes it difficult to understand.\nActivities such as debugging can reveal concrete runtime details, including the\nvalues of variables. However, they require that a developer explicitly requests\nthese data for a specific execution moment. We present a simple approach,\nRuntimeSamp, which collects sample variable values during normal executions of\na program by a programmer. These values are then displayed in an ambient way at\nthe end of each line in the source code editor. We discuss questions which\nshould be answered for this approach to be usable in practice, such as how to\nefficiently record the values and when to display them. We provide partial\nanswers to these questions and suggest future research directions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 20:05:43 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Sul\u00edr", "Mat\u00fa\u0161", ""], ["Porub\u00e4n", "Jaroslav", ""]]}, {"id": "1806.07523", "submitter": "Yuting Wang", "authors": "Gopalan Nadathur, Yuting Wang", "title": "Schematic Polymorphism in the Abella Proof Assistant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Abella interactive theorem prover has proven to be an effective vehicle\nfor reasoning about relational specifications. However, the system has a\nlimitation that arises from the fact that it is based on a simply typed logic:\nformalizations that are identical except in the respect that they apply to\ndifferent types have to be repeated at each type. We develop an approach that\novercomes this limitation while preserving the logical underpinnings of the\nsystem. In this approach object constructors, formulas and other relevant\nlogical notions are allowed to be parameterized by types, with the\ninterpretation that they stand for the (infinite) collection of corresponding\nconstructs that are obtained by instantiating the type parameters. The proof\nstructures that we consider for formulas that are schematized in this fashion\nare limited to ones whose type instances are valid proofs in the simply typed\nlogic. We develop schematic proof rules that ensure this property, a task that\nis complicated by the fact that type information influences the notion of\nunification that plays a key role in the logic. Our ideas, which have been\nimplemented in an updated version of the system, accommodate schematic\npolymorphism both in the core logic of Abella and in the executable\nspecification logic that it embeds.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 02:04:41 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Nadathur", "Gopalan", ""], ["Wang", "Yuting", ""]]}, {"id": "1806.07728", "submitter": "Shigeyuki Sato", "authors": "Shigeyuki Sato, Wei Hao, and Kiminori Matsuzaki", "title": "Parallelization of XPath Queries using Modern XQuery Processors", "comments": "This is the full version of our publication to appear at ADBIS 2018\n  as a short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A practical and promising approach to parallelizing XPath queries was\nproposed by Bordawekar et al. in 2009, which enables parallelization on top of\nexisting XML database engines. Although they experimentally demonstrated the\nspeedup by their approach, their practice has already been out of date because\nthe software environment has largely changed with the capability of XQuery\nprocessing. In this work, we implement their approach in two ways on top of a\nstate-of-the-art XML database engine and experimentally demonstrate that our\nimplementations can bring significant speedup on a commodity server.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 13:45:24 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Sato", "Shigeyuki", ""], ["Hao", "Wei", ""], ["Matsuzaki", "Kiminori", ""]]}, {"id": "1806.07966", "submitter": "Daniel Huang", "authors": "Daniel Huang, Greg Morrisett, Bas Spitters", "title": "An Application of Computable Distributions to the Semantics of\n  Probabilistic Programs", "comments": "Accepted as contribution to \"Foundations of Probabilistic\n  Programming\"", "journal-ref": "Foundations of Probabilistic Programming, ed. Gilles Barthe,\n  Joost-Pieter Katoen & Alexandra Silva, Cambridge University Press, 2020", "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter, we explore how (Type-2) computable distributions can be used\nto give both (algorithmic) sampling and distributional semantics to\nprobabilistic programs with continuous distributions. Towards this end, we\nsketch an encoding of computable distributions in a fragment of Haskell and\nshow how topological domains can be used to model the resulting PCF-like\nlanguage. We also examine the implications that a (Type-2) computable semantics\nhas for implementing conditioning. We hope to draw out the connection between\nan approach based on (Type-2) computability and ordinary programming throughout\nthe chapter as well as highlight the relation with constructive mathematics\n(via realizability).\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 20:11:18 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 07:17:46 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Huang", "Daniel", ""], ["Morrisett", "Greg", ""], ["Spitters", "Bas", ""]]}, {"id": "1806.08128", "submitter": "Tangliu Wen", "authors": "Tangliu Wen", "title": "Strict Linearizability and Abstract Atomicity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linearizability is a commonly accepted consistency condition for concurrent\nobjects. Filipovi\\'{c} et al. show that linearizability is equivalent to\nobservational refinement. However, linearizability does not permit concurrent\nobjects to share memory spaces with their client programs. We show that\nlinearizability (or observational refinement) can be broken even though a\nclient program of an object accesses the shared memory spaces without\ninterference from the methods of the object. In this paper, we present strict\nlinearizability which lifts this limitation and can ensure client-side traces\nand final-states equivalence even in a relaxed program model allowing clients\nto directly access the states of concurrent objects. We also investigate\nseveral important properties of strict linearizability.\n  At a high level of abstraction, a concurrent object can be viewed as a\nconcurrent implementation of an abstract data type (ADT). We also present a\ncorrectness criterion for relating an ADT and its concurrent implementation,\nwhich is the combination of linearizability and data abstraction and can ensure\nobservational equivalence. We also investigate its relationship with strict\nlinearizability.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 09:20:26 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Wen", "Tangliu", ""]]}, {"id": "1806.08206", "submitter": "Tangliu Wen", "authors": "Tangliu Wen", "title": "Proving Linearizability Using Reduction", "comments": "the co-authors of the paper require me to withdraw it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lipton's reduction theory provides an intuitive and simple way for deducing\nthe non-interference properties of concurrent programs, but it is difficult to\ndirectly apply the technique to verify linearizability of sophisticated\nfine-grained concurrent data structures. In this paper, we propose three\nreduction-based proof methods that can handle such data structures. The key\nidea behind our reduction methods is that an irreducible operation can be\nviewed as an atomic operation at a higher level of abstraction. This allows us\nto focus on the reduction properties of an operation related to its abstract\nsemantics. We have successfully applied the methods to verify 11 concurrent\ndata structures including the most challenging ones: the Herlihy and Wing\nqueue, the HSY elimination-based stack, and the time-stamped queue, and the\nlazy list. Our methods inherit intuition and simplicity of Lipton's reduction,\nand concurrent data structures designers can easily and quickly learn to use\nthe methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 12:43:22 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 08:59:18 GMT"}, {"version": "v3", "created": "Thu, 30 Aug 2018 01:02:50 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Wen", "Tangliu", ""]]}, {"id": "1806.08416", "submitter": "Kartik Nagar", "authors": "Kartik Nagar and Suresh Jagannathan", "title": "Automated Detection of Serializability Violations under Weak Consistency", "comments": "Extended version of CONCUR 18 paper, 28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While a number of weak consistency mechanisms have been developed in recent\nyears to improve performance and ensure availability in distributed, replicated\nsystems, ensuring correctness of transactional applications running on top of\nsuch systems remains a difficult and important problem. Serializability is a\nwell-understood correctness criterion for transactional programs; understanding\nwhether applications are serializable when executed in a weakly-consistent\nenvironment, however remains a challenging exercise. In this work, we combine\nthe dependency graph-based characterization of serializability and the\nframework of abstract executions to develop a fully automated approach for\nstatically finding bounded serializability violations under \\emph{any} weak\nconsistency model. We reduce the problem of serializability to satisfiability\nof a formula in First-Order Logic, which allows us to harness the power of\nexisting SMT solvers. We provide rules to automatically construct the FOL\nencoding from programs written in SQL (allowing loops and conditionals) and the\nconsistency specification written as a formula in FOL. In addition to detecting\nbounded serializability violations, we also provide two orthogonal schemes to\nreason about unbounded executions by providing sufficient conditions (in the\nform of FOL formulae) whose satisfiability would imply the absence of anomalies\nin any arbitrary execution. We have applied the proposed technique on TPC-C, a\nreal world database program with complex application logic, and were able to\ndiscover anomalies under Parallel Snapshot Isolation, and verify\nserializability for unbounded executions under Snapshot Isolation, two\nconsistency mechanisms substantially weaker than serializability.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 20:21:24 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Nagar", "Kartik", ""], ["Jagannathan", "Suresh", ""]]}, {"id": "1806.08771", "submitter": "David Feller", "authors": "David Feller, Joe B. Wells, Fairouz Kamareddine (ULTRA), Sebastien\n  Carlier", "title": "What Does This Notation Mean Anyway?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the introduction of BNF notation by Backus for the Algol 60 report\nand subsequent notational variants, a metalanguage involving formal \"grammars\"\nhas developed for discussing structured objects in Computer Science and\nMathematical Logic. We refer to this offspring of BNF as Math-BNF or MBNF, to\nthe original BNF and its notational variants just as BNF, and to aspects common\nto both as BNF-style. What all BNF-style notations share is the use of\nproduction rules roughly of this form: $$\\bullet \\mathrel{::=} \\circ_1 \\mid\n\\cdots \\mid \\circ_n $$ Normally, such a rule says \"every instance of $\\circ_i$\nfor $i \\in \\{1, \\ldots, n\\}$ is also an in stance of $\\bullet$\". MBNF is\ndistinct from BNF in the entities and operations it allows. Instead of strings,\nMBNF builds arrangements of symbols that we call math-text. Sometimes \"syntax\"\nis defined by interleaving MBNF production rules and other mathematical\ndefinitions that can contain chunks of math-text. There is no clear definition\nof MBNF. Readers do not have a document which tells them how MBNF is to be read\nand must learn MBNF through a process of cultural initiation. To the extent\nthat MBNF is defined, it is largely through examples scattered throughout the\nliterature and which require readers to guess the mathematical structures\nunderpinning them. This paper gives MBNF examples illustrating some of the\ndifferences between MBNF and BNF. We propose a definition of syntactic math\ntext (SMT) which handles many (but far from all) uses of math-text and MBNF in\nthe wild. We aim to balance the goal of being accessible and not requiring too\nmuch prerequisite knowledge with the conflicting goal of providing a rich\nmathematical structure that already supports many uses and has possibilities to\nbe extended to support more challenging cases.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 13:07:15 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Feller", "David", "", "ULTRA"], ["Wells", "Joe B.", "", "ULTRA"], ["Kamareddine", "Fairouz", "", "ULTRA"], ["Carlier", "Sebastien", ""]]}, {"id": "1806.09392", "submitter": "Sebastiaan Joosten", "authors": "Sebastiaan J. C. Joosten", "title": "Finding models through graph saturation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DM cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give a procedure that can be used to automatically satisfy invariants of a\ncertain shape. These invariants may be written with the operations\nintersection, composition and converse over binary relations, and equality over\nthese operations. We call these invariants \\tr{}s that we interpret over\ngraphs. For questions stated through sets of these sentences, this paper gives\na semi-decision procedure we call graph saturation. It decides entailment over\nthese \\tr{}s, inspired on graph rewriting. We prove correctness of the\nprocedure. Moreover, we show the corresponding decision problem to be\nundecidable. This confirms a conjecture previously stated by the author.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 11:34:38 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Joosten", "Sebastiaan J. C.", ""]]}, {"id": "1806.09851", "submitter": "EPTCS", "authors": "Afshin Amighi (Hogeschool Rotterdam), Marieke Huisman (University of\n  Twente), Stefan Blom (Better Be)", "title": "Verification of Shared-Reading Synchronisers", "comments": "In Proceedings MeTRiD 2018, arXiv:1806.09330", "journal-ref": "EPTCS 272, 2018, pp. 107-120", "doi": "10.4204/EPTCS.272.9", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronisation classes are an important building block for shared memory\nconcurrent programs. Thus to reason about such programs, it is important to be\nable to verify the implementation of these synchronisation classes, considering\natomic operations as the synchronisation primitives on which the\nimplementations are built. For synchronisation classes controlling exclusive\naccess to a shared resource, such as locks, a technique has been proposed to\nreason about their behaviour. This paper proposes a technique to verify\nimplementations of both exclusive access and shared-reading synchronisers. We\nuse permission-based Separation Logic to describe the behaviour of the main\natomic operations, and the basis for our technique is formed by a specification\nfor class AtomicInteger, which is commonly used to implement synchronisation\nclasses in java.util.concurrent. To demonstrate the applicability of our\napproach, we mechanically verify the implementation of various synchronisation\nclasses like Semaphore, CountDownLatch and Lock.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 08:54:50 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Amighi", "Afshin", "", "Hogeschool Rotterdam"], ["Huisman", "Marieke", "", "University of\n  Twente"], ["Blom", "Stefan", "", "Better Be"]]}, {"id": "1806.09852", "submitter": "EPTCS", "authors": "Kasper Dokter (CWI), Farhad Arbab (CWI)", "title": "Treo: Textual Syntax for Reo Connectors", "comments": "In Proceedings MeTRiD 2018, arXiv:1806.09330", "journal-ref": "EPTCS 272, 2018, pp. 121-135", "doi": "10.4204/EPTCS.272.10", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reo is an interaction-centric model of concurrency for compositional\nspecification of communication and coordination protocols. Formal verification\ntools exist to ensure correctness and compliance of protocols specified in Reo,\nwhich can readily be (re)used in different applications, or composed into more\ncomplex protocols. Recent benchmarks show that compiling such high-level Reo\nspecifications produces executable code that can compete with or even beat the\nperformance of hand-crafted programs written in languages such as C or Java\nusing conventional concurrency constructs.\n  The original declarative graphical syntax of Reo does not support intuitive\nconstructs for parameter passing, iteration, recursion, or conditional\nspecification. This shortcoming hinders Reo's uptake in large-scale practical\napplications. Although a number of Reo-inspired syntax alternatives have\nappeared in the past, none of them follows the primary design principles of\nReo: a) declarative specification; b) all channel types and their sorts are\nuser-defined; and c) channels compose via shared nodes. In this paper, we offer\na textual syntax for Reo that respects these principles and supports flexible\nparameter passing, iteration, recursion, and conditional specification. In\non-going work, we use this textual syntax to compile Reo into target languages\nsuch as Java, Promela, and Maude.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 08:55:13 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Dokter", "Kasper", "", "CWI"], ["Arbab", "Farhad", "", "CWI"]]}, {"id": "1806.10899", "submitter": "Mandy Neumann", "authors": "Ruslan R. Fayzrakhmanov, Christopher Michels, Mandy Neumann", "title": "Introduction to OXPath", "comments": "63 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary web pages with increasingly sophisticated interfaces rival\ntraditional desktop applications for interface complexity and are often called\nweb applications or RIA (Rich Internet Applications). They often require the\nexecution of JavaScript in a web browser and can call AJAX requests to\ndynamically generate the content, reacting to user interaction. From the\nautomatic data acquisition point of view, thus, it is essential to be able to\ncorrectly render web pages and mimic user actions to obtain relevant data from\nthe web page content. Briefly, to obtain data through existing Web interfaces\nand transform it into structured form, contemporary wrappers should be able to:\n1) interact with sophisticated interfaces of web applications; 2) precisely\nacquire relevant data; 3) scale with the number of crawled web pages or states\nof web application; 4) have an embeddable programming API for integration with\nexisting web technologies. OXPath is a state-of-the-art technology, which is\ncompliant with these requirements and demonstrated its efficiency in\ncomprehensive experiments. OXPath integrates Firefox for correct rendering of\nweb pages and extends XPath 1.0 for the DOM node selection, interaction, and\nextraction. It provides means for converting extracted data into different\nformats, such as XML, JSON, CSV, and saving data into relational databases.\n  This tutorial explains main features of the OXPath language and the setup of\na suitable working environment. The guidelines for using OXPath are provided in\nthe form of prototypical examples.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 11:58:05 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Fayzrakhmanov", "Ruslan R.", ""], ["Michels", "Christopher", ""], ["Neumann", "Mandy", ""]]}, {"id": "1806.11150", "submitter": "S\\'ergio Medeiros", "authors": "S\\'ergio Medeiros and Fabio Mascarenhas", "title": "Syntax Error Recovery in Parsing Expression Grammars", "comments": "Published on ACM Symposium On Applied Computing 2018", "journal-ref": null, "doi": "10.1145/3167132.3167261", "report-no": null, "categories": "cs.PL cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parsing Expression Grammars (PEGs) are a formalism used to describe top-down\nparsers with backtracking. As PEGs do not provide a good error recovery\nmechanism, PEG-based parsers usually do not recover from syntax errors in the\ninput, or recover from syntax errors using ad-hoc, implementation-specific\nfeatures. The lack of proper error recovery makes PEG parsers unsuitable for\nusing with Integrated Development Environments (IDEs), which need to build\nsyntactic trees even for incomplete, syntactically invalid programs.\n  We propose a conservative extension, based on PEGs with labeled failures,\nthat adds a syntax error recovery mechanism for PEGs. This extension associates\nrecovery expressions to labels, where a label now not only reports a syntax\nerror but also uses this recovery expression to reach a synchronization point\nin the input and resume parsing. We give an operational semantics of PEGs with\nthis recovery mechanism, and use an implementation based on such semantics to\nbuild a robust parser for the Lua language. We evaluate the effectiveness of\nthis parser, alone and in comparison with a Lua parser with automatic error\nrecovery generated by ANTLR, a popular parser generator.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 19:20:06 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Medeiros", "S\u00e9rgio", ""], ["Mascarenhas", "Fabio", ""]]}]