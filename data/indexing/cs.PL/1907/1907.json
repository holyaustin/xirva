[{"id": "1907.00298", "submitter": "Adithya Murali", "authors": "Umang Mathur, Adithya Murali, Paul Krogmeier, P. Madhusudan, Mahesh\n  Viswanathan", "title": "Deciding Memory Safety for Single-Pass Heap-Manipulating Programs", "comments": "StreamVerif tool for automata-based verification of uninterpreted\n  programs can be found at https://github.com/umangm/streamverif", "journal-ref": "Proceedings of the ACM on Programming Languages Vol. 4, Issue\n  POPL, Article 35 (December 2019)", "doi": "10.1145/3371103", "report-no": null, "categories": "cs.PL cs.FL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the decidability of automatic program verification for\nprograms that manipulate heaps, and in particular, decision procedures for\nproving memory safety for them. We extend recent work that identified a\ndecidable subclass of uninterpreted programs to a class of alias-aware programs\nthat can update maps. We apply this theory to develop verification algorithms\nfor memory safety--- determining if a heap-manipulating program that allocates\nand frees memory locations and manipulates heap pointers does not dereference\nan unallocated memory location. We show that this problem is decidable when the\ninitial allocated heap forms a forest data-structure and when programs are\nstreaming-coherent, which intuitively restricts programs to make a single pass\nover a data-structure. Our experimental evaluation on a set of library routines\nthat manipulate forest data-structures shows that common single-pass algorithms\non data-structures often fall in the decidable class, and that our decision\nprocedure is efficient in verifying them.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 23:53:22 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 12:34:38 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Mathur", "Umang", ""], ["Murali", "Adithya", ""], ["Krogmeier", "Paul", ""], ["Madhusudan", "P.", ""], ["Viswanathan", "Mahesh", ""]]}, {"id": "1907.00421", "submitter": "Thorsten Wissmann", "authors": "Mario Bravetti, Marco Carbone, Julien Lange, Nobuko Yoshida, Gianluigi\n  Zavattaro", "title": "A Sound Algorithm for Asynchronous Session Subtyping and its\n  Implementation", "comments": "This is an extended version of the CONCUR'19 version", "journal-ref": "Logical Methods in Computer Science, Volume 17, Issue 1 (March 4,\n  2021) lmcs:7238", "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Session types, types for structuring communication between endpoints in\ndistributed systems, are recently being integrated into mainstream programming\nlanguages. In practice, a very important notion for dealing with such types is\nthat of subtyping, since it allows for typing larger classes of system, where a\nprogram has not precisely the expected behaviour but a similar one.\nUnfortunately, recent work has shown that subtyping for session types in an\nasynchronous setting is undecidable. To cope with this negative result, the\nonly approaches we are aware of either restrict the syntax of session types or\nlimit communication (by considering forms of bounded asynchrony). Both\napproaches are too restrictive in practice, hence we proceed differently by\npresenting an algorithm for checking subtyping which is sound, but not complete\n(in some cases it terminates without returning a decisive verdict). The\nalgorithm is based on a tree representation of the coinductive definition of\nasynchronous subtyping; this tree could be infinite, and the algorithm checks\nfor the presence of finite witnesses of infinite successful subtrees.\nFurthermore, we provide a tool that implements our algorithm. We use this tool\nto test our algorithm on many examples that cannot be managed with the previous\napproaches, and to provide an empirical evaluation of the time and space cost\nof the algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 17:14:36 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 05:25:11 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 09:22:58 GMT"}, {"version": "v4", "created": "Fri, 12 Feb 2021 09:50:15 GMT"}, {"version": "v5", "created": "Wed, 3 Mar 2021 08:49:51 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Bravetti", "Mario", ""], ["Carbone", "Marco", ""], ["Lange", "Julien", ""], ["Yoshida", "Nobuko", ""], ["Zavattaro", "Gianluigi", ""]]}, {"id": "1907.00509", "submitter": "Justin Slepak", "authors": "Justin Slepak, Olin Shivers, Panagiotis Manolios", "title": "The Semantics of Rank Polymorphism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iverson's APL and its descendants (such as J, K and FISh) are examples of the\nfamily of \"rank-polymorphic\" programming languages. The principal control\nmechanism of such languages is the general lifting of functions that operate on\narrays of rank (or dimension) $r$ to operate on arrays of any higher rank $r' >\nr$. We present a core, functional language, Remora, that captures this\nmechanism, and develop both a formal, dynamic semantics for the language, and\nan accompanying static, rank-polymorphic type system for the language.\nCritically, the static semantics captures the shape-based lifting mechanism of\nthe language. We establish the usual progress and preservation properties for\nthe type system, showing that it is sound, which means that \"array shape\"\nerrors cannot occur at run time in a well-typed program. Our type system uses\ndependent types, including an existential type abstraction which permits\nprograms to operate on arrays whose shape or rank is computed dynamically;\nhowever, it is restricted enough to permit static type checking.\n  The rank-polymorphic computational paradigm is unusual in that the types of\narguments affect the dynamic execution of the program -- they are what drive\nthe rank-polymorphic distribution of a function across arrays of higher rank.\nTo highlight this property, we additionally present a dynamic semantics for a\npartially erased variant of the fully-typed language and show that a\ncomputation performed with a fully-typed term stays in lock step with the\ncomputation performed with its partially erased term. The residual types thus\nprecisely characterise the type information that is needed by the dynamic\nsemantics, a property useful for the (eventual) construction of efficient\ncompilers for rank-polymorphic languages.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 01:42:53 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Slepak", "Justin", ""], ["Shivers", "Olin", ""], ["Manolios", "Panagiotis", ""]]}, {"id": "1907.00713", "submitter": "Robert Sison", "authors": "Robert Sison (Data61, CSIRO and UNSW Sydney), Toby Murray (University\n  of Melbourne)", "title": "Verifying that a compiler preserves concurrent value-dependent\n  information-flow security", "comments": "To appear in the 10th International Conference on Interactive Theorem\n  Proving (ITP 2019). Extended version with appendix. For supplement material,\n  see https://covern.org/itp19.html", "journal-ref": null, "doi": "10.4230/LIPIcs.ITP.2019.27", "report-no": null, "categories": "cs.LO cs.CR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is common to prove by reasoning over source code that programs do not leak\nsensitive data. But doing so leaves a gap between reasoning and reality that\ncan only be filled by accounting for the behaviour of the compiler. This task\nis complicated when programs enforce value-dependent information-flow security\nproperties (in which classification of locations can vary depending on values\nin other locations) and complicated further when programs exploit\nshared-variable concurrency.\n  Prior work has formally defined a notion of concurrency-aware refinement for\npreserving value-dependent security properties. However, that notion is\nconsiderably more complex than standard refinement definitions typically\napplied in the verification of semantics preservation by compilers. To date it\nremains unclear whether it can be applied to a realistic compiler, because\nthere exist no general decomposition principles for separating it into smaller,\nmore familiar, proof obligations.\n  In this work, we provide such a decomposition principle, which we show can\nalmost halve the complexity of proving secure refinement. Further, we\ndemonstrate its applicability to secure compilation, by proving in Isabelle/HOL\nthe preservation of value-dependent security by a proof-of-concept compiler\nfrom an imperative While language to a generic RISC-style assembly language,\nfor programs with shared-memory concurrency mediated by locking primitives.\nFinally, we execute our compiler in Isabelle on a While language model of the\nCross Domain Desktop Compositor, demonstrating to our knowledge the first use\nof a compiler verification result to carry an information-flow security\nproperty down to the assembly-level model of a non-trivial concurrent program.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 12:34:15 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Sison", "Robert", "", "Data61, CSIRO and UNSW Sydney"], ["Murray", "Toby", "", "University\n  of Melbourne"]]}, {"id": "1907.00822", "submitter": "Xin Zhao", "authors": "Xin Zhao, Philipp Haller", "title": "Consistency types for replicated data in a higher-order distributed\n  programming language", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2021, Vol. 5,\n  Issue 2, Article 6", "doi": "10.22152/programming-journal.org/2021/5/6", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed systems address the increasing demand for fast access to\nresources and fault tolerance for data. However, due to scalability\nrequirements, software developers need to trade consistency for performance.\nFor certain data, consistency guarantees may be weakened if application\ncorrectness is unaffected. In contrast, data flow from data with weak\nconsistency to data with strong consistency requirements is problematic, since\napplication correctness may be broken. In this paper, we propose lattice-based\nconsistency types for replicated data (CTRD), a higher-order static\nconsistency-typed language with replicated data types. The type system of CTRD\nsupports shared data among multiple clients, and statically enforces\nnoninterference between data types with weaker consistency and data types with\nstronger consistency. The language can be applied to many distributed\napplications and guarantees that updates of weakly-consistent data can never\naffect strongly-consistent data. We also extend the basic CTRD with an\noptimization that reduces synchronization for generating reference graphs.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 14:29:11 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 12:43:42 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 12:59:29 GMT"}, {"version": "v4", "created": "Fri, 30 Oct 2020 15:18:44 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Zhao", "Xin", ""], ["Haller", "Philipp", ""]]}, {"id": "1907.00844", "submitter": "Gert-Jan Bottu", "authors": "Gert-Jan Bottu, Ningning Xie, Koar Marntirosian, Tom Schrijvers", "title": "Coherence of Type Class Resolution", "comments": "Accepted to ICFP 2019", "journal-ref": null, "doi": "10.1145/3341695", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Elaboration-based type class resolution, as found in languages like Haskell,\nMercury and PureScript, is generally nondeterministic: there can be multiple\nways to satisfy a wanted constraint in terms of global instances and locally\ngiven constraints. Coherence is the key property that keeps this sane; it\nguarantees that, despite the nondeterminism, programs still behave predictably.\nEven though elaboration-based resolution is generally assumed coherent, as far\nas we know, there is no formal proof of this property in the presence of\nsources of nondeterminism, like superclasses and flexible contexts.\n  This paper provides a formal proof to remedy the situation. The proof is\nnon-trivial because the semantics elaborates resolution into a target language\nwhere different elaborations can be distinguished by contexts that do not have\na source language counterpart. Inspired by the notion of full abstraction, we\npresent a two-step strategy that first elaborates nondeterministically into an\nintermediate language that preserves contextual equivalence, and then\ndeterministically elaborates from there into the target language. We use an\napproach based on logical relations to establish contextual equivalence and\nthus coherence for the first step of elaboration, while the second step's\ndeterminism straightforwardly preserves this coherence property.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 15:10:44 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 12:00:29 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Bottu", "Gert-Jan", ""], ["Xie", "Ningning", ""], ["Marntirosian", "Koar", ""], ["Schrijvers", "Tom", ""]]}, {"id": "1907.00855", "submitter": "Martin Leinberger", "authors": "Martin Leinberger and Philipp Seifer and Claudia Schon and Ralf\n  L\\\"ammel and Steffen Staab", "title": "Type Checking Program Code using SHACL (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a strength of graph-based data formats, like RDF, that they are very\nflexible with representing data. To avoid run-time errors, program code that\nprocesses highly-flexible data representations exhibits the difficulty that it\nmust always include the most general case, in which attributes might be\nset-valued or possibly not available. The Shapes Constraint Language (SHACL)\nhas been devised to enforce constraints on otherwise random data structures. We\npresent our approach, Type checking using SHACL (TyCuS), for type checking code\nthat queries RDF data graphs validated by a SHACL shape graph. To this end, we\nderive SHACL shapes from queries and integrate data shapes and query shapes as\ntypes into a $\\lambda$-calculus. We provide the formal underpinnings and a\nproof of type safety for TyCuS. A programmer can use our method in order to\nprocess RDF data with simplified, type checked code that will not encounter\nrun-time errors (with usual exceptions as type checking cannot prevent\naccessing empty lists).\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 15:20:42 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Leinberger", "Martin", ""], ["Seifer", "Philipp", ""], ["Schon", "Claudia", ""], ["L\u00e4mmel", "Ralf", ""], ["Staab", "Steffen", ""]]}, {"id": "1907.00863", "submitter": "Manuel Rigger", "authors": "Manuel Rigger, Stefan Marr, Bram Adams, Hanspeter M\\\"ossenb\\\"ock", "title": "Understanding GCC Builtins to Develop Better Tools", "comments": "Accepted at ESEC/FSE 2019 (see\n  https://esec-fse19.ut.ee/program/research-papers/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  C programs can use compiler builtins to provide functionality that the C\nlanguage lacks. On Linux, GCC provides several thousands of builtins that are\nalso supported by other mature compilers, such as Clang and ICC. Maintainers of\nother tools lack guidance on whether and which builtins should be implemented\nto support popular projects. To assist tool developers who want to support GCC\nbuiltins, we analyzed builtin use in 4,913 C projects from GitHub. We found\nthat 37% of these projects relied on at least one builtin. Supporting an\nincreasing proportion of projects requires support of an exponentially\nincreasing number of builtins; however, implementing only 10 builtins already\ncovers over 30% of the projects. Since we found that many builtins in our\ncorpus remained unused, the effort needed to support 90% of the projects is\nmoderate, requiring about 110 builtins to be implemented. For each project, we\nanalyzed the evolution of builtin use over time and found that the majority of\nprojects mostly added builtins. This suggests that builtins are not a legacy\nfeature and must be supported in future tools. Systematic testing of builtin\nsupport in existing tools revealed that many lacked support for builtins either\npartially or completely; we also discovered incorrect implementations in\nvarious tools, including the formally verified CompCert compiler.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 15:25:03 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Rigger", "Manuel", ""], ["Marr", "Stefan", ""], ["Adams", "Bram", ""], ["M\u00f6ssenb\u00f6ck", "Hanspeter", ""]]}, {"id": "1907.01257", "submitter": "Dan Ghica", "authors": "Dan R. Ghica and Koko Muroya and Todd Waugh Ambridge", "title": "Local Reasoning for Robust Observational Equivalence", "comments": "73 pages, 57 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new core calculus for programming languages with effects,\ninterpreted using a hypergraph-rewriting abstract machine inspired by the\nGeometry of Interaction. The intrinsic calculus syntax and semantics only deal\nwith the basic structural aspects of programming languages: variable binding,\nname binding, and thunking. Everything else, including features which are\ncommonly thought of as intrinsic, such as arithmetic or function abstraction\nand application, must be provided as extrinsic operations, with associated\nrewrite rules. The graph representation yields natural concepts of locality and\nrobustness for equational properties and reduction rules, which enable a novel\nflexible and powerful reasoning methodology about (type-free) languages with\neffects. We illustrate and motivate the technique with challenging examples\nfrom the literature.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 09:34:59 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Ghica", "Dan R.", ""], ["Muroya", "Koko", ""], ["Ambridge", "Todd Waugh", ""]]}, {"id": "1907.01297", "submitter": "Ekaterina Komendantskaya Dr", "authors": "Ekaterina Komendantskaya and Rob Stewart and Kirsy Duncan and Daniel\n  Kienitz and Pierre Le Hen and Pascal Bacchus", "title": "Neural Network Verification for the Masses (of AI graduates)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid development of AI applications has stimulated demand for, and has given\nrise to, the rapidly growing number and diversity of AI MSc degrees. AI and\nRobotics research communities, industries and students are becoming\nincreasingly aware of the problems caused by unsafe or insecure AI\napplications. Among them, perhaps the most famous example is vulnerability of\ndeep neural networks to ``adversarial attacks''. Owing to wide-spread use of\nneural networks in all areas of AI, this problem is seen as particularly acute\nand pervasive.\n  Despite of the growing number of research papers about safety and security\nvulnerabilities of AI applications, there is a noticeable shortage of\naccessible tools, methods and teaching materials for incorporating verification\ninto AI programs. LAIV -- the Lab for AI and Verification -- is a newly opened\nresearch lab at Heriot-Watt university that engages AI and Robotics MSc\nstudents in verification projects, as part of their MSc dissertation work. In\nthis paper, we will report on successes and unexpected difficulties LAIV faces,\nmany of which arise from limitations of existing programming languages used for\nverification. We will discuss future directions for incorporating verification\ninto AI degrees.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 11:09:04 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Komendantskaya", "Ekaterina", ""], ["Stewart", "Rob", ""], ["Duncan", "Kirsy", ""], ["Kienitz", "Daniel", ""], ["Hen", "Pierre Le", ""], ["Bacchus", "Pascal", ""]]}, {"id": "1907.01727", "submitter": "Darion Cassel", "authors": "Darion Cassel, Yan Huang, Limin Jia", "title": "Uncovering Information Flow Policy Violations in C Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programmers of cryptographic applications written in C need to avoid common\nmistakes such as sending private data over public channels, modifying trusted\ndata with untrusted functions, or improperly ordering protocol steps. These\nsecrecy, integrity, and sequencing policies can be cumbersome to check with\nexisting general-purpose tools. We have developed a novel means of specifying\nand uncovering violations of these policies that allows for a much\nlighter-weight approach than previous tools. We embed the policy annotations in\nC's type system via a source-to-source translation and leverage existing C\ncompilers to check for policy violations, achieving high performance and\nscalability. We show through case studies of recent cryptographic libraries and\napplications that our work is able to express detailed policies for large\nbodies of C code and can find subtle policy violations. To gain formal\nunderstanding of our policy annotations, we show formal connections between the\npolicy annotations and an information flow type system and prove a\nnoninterference guarantee.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 04:12:11 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Cassel", "Darion", ""], ["Huang", "Yan", ""], ["Jia", "Limin", ""]]}, {"id": "1907.02064", "submitter": "Mark Hill", "authors": "Mark D. Hill and Vijay Janapa Reddi", "title": "Accelerator-level Parallelism", "comments": "6 pages, 3 figures, & 7 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.OS cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future applications demand more performance, but technology advances have\nbeen faltering. A promising approach to further improve computer system\nperformance under energy constraints is to employ hardware accelerators.\nAlready today, mobile systems concurrently employ multiple accelerators in what\nwe call accelerator-level parallelism (ALP). To spread the benefits of ALP more\nbroadly, we charge computer scientists to develop the science needed to best\nachieve the performance and cost goals of ALP hardware and software.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 21:04:47 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 14:53:53 GMT"}, {"version": "v3", "created": "Sat, 24 Aug 2019 20:11:05 GMT"}, {"version": "v4", "created": "Wed, 19 Aug 2020 14:58:25 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Hill", "Mark D.", ""], ["Reddi", "Vijay Janapa", ""]]}, {"id": "1907.02192", "submitter": "Ramy Shahin", "authors": "Ramy Shahin, Marsha Chechik, Rick Salay", "title": "Lifting Datalog-Based Analyses to Software Product Lines", "comments": "FSE'19 paper", "journal-ref": null, "doi": "10.1145/3338906.3338928", "report-no": null, "categories": "cs.SE cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying program analyses to Software Product Lines (SPLs) has been a\nfundamental research problem at the intersection of Product Line Engineering\nand software analysis. Different attempts have been made to \"lift\" particular\nproduct-level analyses to run on the entire product line. In this paper, we\ntackle the class of Datalog-based analyses (e.g., pointer and taint analyses),\nstudy the theoretical aspects of lifting Datalog inference, and implement a\nlifted inference algorithm inside the Souffl\\'e Datalog engine. We evaluate our\nimplementation on a set of benchmark product lines. We show significant savings\nin processing time and fact database size (billions of times faster on one of\nthe benchmarks) compared to brute-force analysis of each product individually.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 02:31:13 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 00:01:30 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Shahin", "Ramy", ""], ["Chechik", "Marsha", ""], ["Salay", "Rick", ""]]}, {"id": "1907.02558", "submitter": "Goran Piskachev", "authors": "Sriteja Kummita and Goran Piskachev", "title": "Integration of the Static Analysis Results Interchange Format in\n  CogniCrypt", "comments": null, "journal-ref": null, "doi": null, "report-no": "tr-ri-19-359", "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background - Software companies increasingly rely on static analysis tools to\ndetect potential bugs and security vulnerabilities in their software products.\nIn the past decade, more and more commercial and open-source static analysis\ntools have been developed and are maintained. Each tool comes with its own\nreporting format, preventing an easy integration of multiple analysis tools in\na single interface, such as the Static Analysis Server Protocol (SASP). In\n2017, a collaborative effort in industry, including Microsoft and GrammaTech,\nhas proposed the Static Analysis Results Interchange Format (SARIF) to address\nthis issue. SARIF is a standardized format in which static analysis warnings\ncan be encoded, to allow the import and export of analysis reports between\ndifferent tools.\n  Purpose - This paper explains the SARIF format through examples and presents\na proof of concept of the connector that allows the static analysis tool\nCogniCrypt to generate and export its results in SARIF format.\n  Design/Approach - We conduct a cross-sectional study between the SARIF format\nand CogniCrypt's output format before detailing the implementation of the\nconnector. The study aims to find the components of interest in CogniCrypt that\nthe SARIF export module can complete.\n  Originality/Value - The integration of SARIF into CogniCrypt described in\nthis paper can be reused to integrate SARIF into other static analysis tools.\n  Conclusion - After detailing the SARIF format, we present an initial\nimplementation to integrate SARIF into CogniCrypt. After taking advantage of\nall the features provided by SARIF, CogniCrypt will be able to support SASP.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 18:31:09 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Kummita", "Sriteja", ""], ["Piskachev", "Goran", ""]]}, {"id": "1907.02597", "submitter": "Maarten de Jong", "authors": "Maarten de Jong", "title": "Multi-dimensional interpolations in C++", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A C++ software design is presented that can be used to interpolate data in\nany number of dimensions. The design is based on a combination of templates of\nfunctional collections of elements and so-called type lists. The design allows\nfor different search methodologies and interpolation techniques in each\ndimension. It is also possible to expand and reduce the number of dimensions,\nto interpolate composite data types and to produce on-the-fly additional values\nsuch as derivatives of the interpolating function.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 08:16:49 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["de Jong", "Maarten", ""]]}, {"id": "1907.02859", "submitter": "Eric Schulte", "authors": "Eric Schulte and Jonathan Dorn and Antonio Flores-Montoya and Aaron\n  Ballman and Tom Johnson", "title": "GTIRB: Intermediate Representation for Binaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GTIRB is an intermediate representation for binary analysis and rewriting\ntools including disassemblers, lifters, analyzers, rewriters, and\npretty-printers. GTIRB is designed to enable communication between tools in a\nformat that provides the basic information necessary for analysis and rewriting\nwhile making no further assumptions about domain (e.g., malware vs. cleanware,\nor PE vs. ELF) or semantic interpretation (functional vs. operational\nsemantics). This design supports the goals of (1) encouraging tool\nmodularization and re-use allowing researchers and developers to focus on a\nsingle aspect of binary analysis and rewriting without committing to any single\ntool chain and (2) facilitating communication and comparison between tools.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 13:59:49 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 22:57:41 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Schulte", "Eric", ""], ["Dorn", "Jonathan", ""], ["Flores-Montoya", "Antonio", ""], ["Ballman", "Aaron", ""], ["Johnson", "Tom", ""]]}, {"id": "1907.02952", "submitter": "Silvia Crafa", "authors": "Silvia Crafa and Matteo Di Pirro", "title": "Solidity 0.5: when typed does not mean type safe", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent release of Solidity 0.5 introduced a new type to prevent Ether\ntransfers to smart contracts that are not supposed to receive money.\nUnfortunately, the compiler fails in enforcing the guarantees this type\nintended to convey, hence the type soundness of Solidity 0.5 is no better than\nthat of Solidity 0.4. In this paper we discuss a paradigmatic example showing\nthat vulnerable Solidity patterns based on potentially unsafe callback\nexpressions are still unchecked. We also point out a solution that strongly\nrelies on formal methods to support a type-safer smart contracts programming\ndiscipline, while being retro-compatible with legacy Solidity code.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 17:38:17 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Crafa", "Silvia", ""], ["Di Pirro", "Matteo", ""]]}, {"id": "1907.02990", "submitter": "Oliver Bra\\v{c}evac", "authors": "Oliver Bra\\v{c}evac, Guido Salvaneschi, Sebastian Erdweg, Mira Mezini", "title": "Type-safe, Polyvariadic Event Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The pivotal role that event correlation technology plays in todays\napplications has lead to the emergence of different families of event\ncorrelation approaches with a multitude of specialized correlation semantics,\nincluding computation models that support the composition and extension of\ndifferent semantics. However, type-safe embeddings of extensible and composable\nevent patterns into statically-typed general-purpose programming languages have\nnot been systematically explored so far. Event correlation technology has often\nadopted well-known and intuitive notations from database queries, for which\napproaches to type-safe embedding do exist. However, we argue in the paper that\nthese approaches, which are essentially descendants of the work on monadic\ncomprehensions, are not well-suited for event correlations and, thus, cannot\nwithout further ado be reused/re-purposed for embedding event patterns. To\nclose this gap we propose PolyJoin, a novel approach to type-safe embedding for\nfully polyvariadic event patterns with polymorphic correlation semantics. Our\napproach is based on a tagless final encoding with uncurried higher-order\nabstract syntax (HOAS) representation of event patterns with n variables, for\narbitrary $n \\in \\mathbb{N}$. Thus, our embedding is defined in terms of the\nhost language without code generation and exploits the host language type\nsystem to model and type check the type system of the pattern language. Hence,\nby construction it impossible to define ill-typed patterns. We show that it is\npossible to have a purely library-level embedding of event patterns, in the\nfamiliar join query notation, which is not restricted to monads. PolyJoin is\npractical, type-safe and extensible. An implementation of it in pure multicore\nOCaml is readily usable.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 18:27:36 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Bra\u010devac", "Oliver", ""], ["Salvaneschi", "Guido", ""], ["Erdweg", "Sebastian", ""], ["Mezini", "Mira", ""]]}, {"id": "1907.03105", "submitter": "Peter-Michael Osera", "authors": "Peter-Michael Osera", "title": "Constraint-Based Type-Directed Program Synthesis", "comments": "Extended version of report that appeared in TyDe 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore an approach to type-directed program synthesis rooted in\nconstraint-based type inference techniques. By doing this, we aim to more\nefficiently synthesize polymorphic code while also tackling advanced typing\nfeatures such as GADTs that build upon polymorphism. Along the way, we also\npresent an implementation of these techniques in Scythe, a prototype live,\ntype-directed programming tool for the Haskell programming language and reflect\non our initial experience with the tool.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 09:58:55 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Osera", "Peter-Michael", ""]]}, {"id": "1907.03436", "submitter": "Alexander Myltsev", "authors": "Alexander A. Myltsev", "title": "parboiled2: a macro-based approach for effective generators of parsing\n  expressions grammars in Scala", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's computerized world, parsing is ubiquitous. Developers parse logs,\nqueries to databases and websites, programming and natural languages. When Java\necosystem maturity, concise syntax, and runtime speed matters, developers\nchoose parboiled2 that generates grammars for parsing expression grammars\n(PEG). The following open source libraries have chosen parboiled2 for parsing\nfacilities: - akka-http is the Streaming-first HTTP server/module of Lightbend\nAkka - Sangria is a Scala GraphQL implementation - http4s is a minimal,\nidiomatic Scala interface for HTTP - cornichon is Scala DSL for testing HTTP\nJSON API - scala-uri is a simple Scala library for building and parsing URIs\nThe library uses a wide range of Scala facilities to provide required\nfunctionality. We also discuss the extensions to PEGs. In particular, we show\nthe implementation of an internal Scala DSL that features intuitive syntax and\nsemantics. We demonstrate how parboiled2 extensively uses Scala typing to\nverify DSL integrity. We also show the connections to inner structures of\nparboiled2, which can give the developer a better understanding of how to\ncompose more effective grammars. Finally, we expose how a grammar is expanded\nwith Scala Macros to an effective runtime code.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 07:38:57 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Myltsev", "Alexander A.", ""]]}, {"id": "1907.03536", "submitter": "EPTCS", "authors": "Micah Halter (Georgia Tech Research Institute), Christine Herlihy\n  (Georgia Tech Research Institute), James Fairbanks (Georgia Tech Research\n  Institute)", "title": "A Compositional Framework for Scientific Model Augmentation", "comments": "In Proceedings ACT 2019, arXiv:2009.06334", "journal-ref": "EPTCS 323, 2020, pp. 172-182", "doi": "10.4204/EPTCS.323.12", "report-no": null, "categories": "cs.PL cs.SE math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists construct and analyze computational models to understand the\nworld. That understanding comes from efforts to augment, combine, and compare\nmodels of related phenomena. We propose SemanticModels.jl, a system that\nleverages techniques from static and dynamic program analysis to process\nexecutable versions of scientific models to perform such metamodeling tasks. By\nframing these metamodeling tasks as metaprogramming problems, SemanticModels.jl\nenables writing programs that generate and expand models. To this end, we\npresent a category theory-based framework for defining metamodeling tasks, and\nextracting semantic information from model implementations, and show how this\nframework can be used to enhance scientific workflows in a working case study.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 13:10:10 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 02:16:08 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Halter", "Micah", "", "Georgia Tech Research Institute"], ["Herlihy", "Christine", "", "Georgia Tech Research Institute"], ["Fairbanks", "James", "", "Georgia Tech Research\n  Institute"]]}, {"id": "1907.03996", "submitter": "EPTCS", "authors": "Mihai Herda (Karlsruhe Institute of Technology (KIT)), Michael Kirsten\n  (Karlsruhe Institute of Technology (KIT)), Etienne Brunner (Karlsruhe\n  Institute of Technology (KIT)), Joana Plewnia (Karlsruhe Institute of\n  Technology (KIT)), Ulla Scheler (Karlsruhe Institute of Technology (KIT)),\n  Chiara Staudenmaier (Karlsruhe Institute of Technology (KIT)), Benedikt\n  Wagner (Karlsruhe Institute of Technology (KIT)), Pascal Zwick (Karlsruhe\n  Institute of Technology (KIT)), Bernhard Beckert (Karlsruhe Institute of\n  Technology (KIT))", "title": "Understanding Counterexamples for Relational Properties with DIbugger", "comments": "In Proceedings HCVS/PERR 2019, arXiv:1907.03523", "journal-ref": "EPTCS 296, 2019, pp. 6-13", "doi": "10.4204/EPTCS.296.4", "report-no": null, "categories": "cs.SE cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software verification is a tedious process that involves the analysis of\nmultiple failed verification attempts, and adjustments of the program or\nspecification. This is especially the case for complex requirements, e.g.,\nregarding security or fairness, when one needs to compare multiple related runs\nof the same software. Verification tools often provide counterexamples\nconsisting of program inputs when a proof attempt fails, however it is often\nnot clear why the reported counterexample leads to a violation of the checked\nproperty. In this paper, we enhance this aspect of the software verification\nprocess by providing DIbugger, a tool for analyzing counterexamples of\nrelational properties, allowing the user to debug multiple related programs\nsimultaneously.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 06:00:51 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Herda", "Mihai", "", "Karlsruhe Institute of Technology"], ["Kirsten", "Michael", "", "Karlsruhe Institute of Technology"], ["Brunner", "Etienne", "", "Karlsruhe\n  Institute of Technology"], ["Plewnia", "Joana", "", "Karlsruhe Institute of\n  Technology"], ["Scheler", "Ulla", "", "Karlsruhe Institute of Technology"], ["Staudenmaier", "Chiara", "", "Karlsruhe Institute of Technology"], ["Wagner", "Benedikt", "", "Karlsruhe Institute of Technology"], ["Zwick", "Pascal", "", "Karlsruhe\n  Institute of Technology"], ["Beckert", "Bernhard", "", "Karlsruhe Institute of\n  Technology"]]}, {"id": "1907.03997", "submitter": "EPTCS", "authors": "Qi Zhou (Georgia Institute of Technology), David Heath (Georgia\n  Institute of Technology), William Harris (Galois Inc.)", "title": "Relational Verification via Invariant-Guided Synchronization", "comments": "In Proceedings HCVS/PERR 2019, arXiv:1907.03523", "journal-ref": "EPTCS 296, 2019, pp. 28-41", "doi": "10.4204/EPTCS.296.6", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational properties describe relationships that hold over multiple\nexecutions of one or more programs, such as functional equivalence.\nConventional approaches for automatically verifying such properties typically\nrely on syntax-based, heuristic strategies for finding synchronization points\namong the input programs. These synchronization points are then annotated with\nappropriate relational invariants to complete the proof. However, when\nsuboptimal synchronization points are chosen the required invariants can be\ncomplicated or even inexpressible in the target theory.\n  In this work, we propose a novel approach to verifying relational properties.\nThis approach searches for synchronization points and synthesizes relational\ninvariants simultaneously. Specifically, the approach uses synthesized\ninvariants as a guide for finding proper synchronization points that lead to a\ncomplete proof. We implemented our approach as a tool named PEQUOD, which\ntargets Java Virtual Machine (JVM) bytecode. We evaluated PEQUOD by using it to\nsolve verification challenges drawn from the from the research literature and\nby verifying properties of student-submitted solutions to online challenge\nproblems. The results show that PEQUOD solve verification problems that cannot\nbe addressed by current techniques.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 06:02:04 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Zhou", "Qi", "", "Georgia Institute of Technology"], ["Heath", "David", "", "Georgia\n  Institute of Technology"], ["Harris", "William", "", "Galois Inc."]]}, {"id": "1907.03999", "submitter": "EPTCS", "authors": "Emanuele De Angelis (DEC, University \"G. d'Annunzio\" of\n  Chieti-Pescara, Italy), Fabio Fioravanti (DEC, University \"G. d'Annunzio\" of\n  Chieti-Pescara, Italy), Alberto Pettorossi (DICII, University of Roma Tor\n  Vergata, Italy), Maurizio Proietti (CNR-IASI, Rome, Italy)", "title": "Proving Properties of Sorting Programs: A Case Study in Horn Clause\n  Verification", "comments": "In Proceedings HCVS/PERR 2019, arXiv:1907.03523", "journal-ref": "EPTCS 296, 2019, pp. 48-75", "doi": "10.4204/EPTCS.296.8", "report-no": null, "categories": "cs.LO cs.PL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proof of a program property can be reduced to the proof of satisfiability\nof a set of constrained Horn clauses (CHCs) which can be automatically\ngenerated from the program and the property. In this paper we have conducted a\ncase study in Horn clause verification by considering several sorting programs\nwith the aim of exploring the effectiveness of a transformation technique which\nallows us to eliminate inductive data structures such as lists or trees. If\nthis technique is successful, we derive a set of CHCs with constraints over the\nintegers and booleans only, and the satisfiability check can often be performed\nin an effective way by using state-of-the-art CHC solvers, such as Eldarica or\nZ3. In this case study we have also illustrated the usefulness of a companion\ntechnique based on the introduction of the so-called difference predicates,\nwhose definitions correspond to lemmata required during the verification. We\nhave considered functional programs which implement the following kinds of\nsorting algorithms acting on lists of integers: (i) linearly recursive sorting\nalgorithms, such as insertion sort and selection sort, and (ii) non-linearly\nrecursive sorting algorithms, such as quicksort and mergesort, and we have\nconsidered the following properties: (i) the partial correctness properties,\nthat is, the orderedness of the output lists, and the equality of the input and\noutput lists when viewed as multisets, and (ii) some arithmetic properties,\nsuch as the equality of the sum of the elements before and after sorting.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 06:02:42 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["De Angelis", "Emanuele", "", "DEC, University \"G. d'Annunzio\" of\n  Chieti-Pescara, Italy"], ["Fioravanti", "Fabio", "", "DEC, University \"G. d'Annunzio\" of\n  Chieti-Pescara, Italy"], ["Pettorossi", "Alberto", "", "DICII, University of Roma Tor\n  Vergata, Italy"], ["Proietti", "Maurizio", "", "CNR-IASI, Rome, Italy"]]}, {"id": "1907.04134", "submitter": "David Wonnacott", "authors": "David G. Wonnacott and Peter-Michael Osera", "title": "A Bridge Anchored on Both Sides: Formal Deduction in Introductory CS,\n  and Code Proofs in Discrete Math", "comments": "36 pages, including references; \"experiments\" section to be discussed\n  at ICER 2019 work-in-progress session; prior material currently under review", "journal-ref": null, "doi": null, "report-no": "HC-CS-TR 2019-01", "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a sharp disconnect between the programming and mathematical portions\nof the standard undergraduate computer science curriculum, leading to student\nmisunderstanding about how the two are related. We propose connecting the\nsubjects early in the curriculum---specifically, in CS1 and the introductory\ndiscrete mathematics course---by using formal reasoning about programs as a\nbridge between them.\n  This article reports on Haverford and Grinnell College's experience in\nconstructing the end points of this bridge between CS1 and discrete\nmathematics. Haverford's long-standing \"3-2-1\" curriculum introduces code\nreasoning in conjunction with introductory programming concepts, and Grinnell's\ndiscrete mathematics introduces code reasoning as a motivation for logic and\nformal deduction. Both courses present code reasoning in a style based on\nsymbolic code execution techniques from the programming language community, but\ntuned to address the particulars of each course.\n  These courses rely primarily on traditional means of proof authoring with\npen-and-paper. This is unsatisfactory for students who receive no feedback\nuntil grading on their work and instructors who must shoulder the burden of\ninterpreting students' proofs and giving useful feedback. To this end, we also\ndescribe the current state of Orca, an in-development proof assistant for\nundergraduate education that we are developing to address these issues in our\ncourses.\n  Finally, in teaching our courses, we have discovered a number of educational\nresearch questions about the effectiveness of code reasoning in bridging the\ngap between programming and mathematics, and the ability of tools like \\orca to\nsupport this pedagogy. We pose these research questions as next steps to\nformalize our initial experiences in our courses with the hope of eventually\ngeneralizing our approaches for wider adoption.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 15:33:35 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Wonnacott", "David G.", ""], ["Osera", "Peter-Michael", ""]]}, {"id": "1907.04241", "submitter": "Yurong Chen", "authors": "Yurong Chen, Hongfa Xue, Tian Lan, Guru Venkataramani", "title": "CHOP: Bypassing Runtime Bounds Checking Through Convex Hull OPtimization", "comments": "14 pages, 9 figures, 6 tables;", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.PF", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Unsafe memory accesses in programs written using popular programming\nlanguages like C/C++ have been among the leading causes for software\nvulnerability. Prior memory safety checkers such as SoftBound enforce memory\nspatial safety by checking if every access to array elements are within the\ncorresponding array bounds. However, it often results in high execution time\noverhead due to the cost of executing the instructions associated with bounds\nchecking. To mitigate this problem, redundant bounds check elimination\ntechniques are needed. In this paper, we propose CHOP, a Convex Hull\nOPtimization based framework, for bypassing redundant memory bounds checking\nvia profile-guided inferences. In contrast to existing check elimination\ntechniques that are limited by static code analysis, our solution leverages a\nmodel-based inference to identify redundant bounds checking based on runtime\ndata from past program executions. For a given function, it rapidly derives and\nupdates a knowledge base containing sufficient conditions for identifying\nredundant array bounds checking. We evaluate CHOP on real-world applications\nand benchmark (such as SPEC) and the experimental results show that on average\n80.12% of dynamic bounds check instructions can be avoided, resulting in\nimproved performance up to 95.80% over SoftBound.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 15:09:03 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Chen", "Yurong", ""], ["Xue", "Hongfa", ""], ["Lan", "Tian", ""], ["Venkataramani", "Guru", ""]]}, {"id": "1907.04243", "submitter": "Antoine Genitrini", "authors": "Olivier Bodini, Matthieu Dien, Antoine Genitrini and Fr\\'ed\\'eric\n  Peschanski", "title": "The Combinatorics of Barrier Synchronization", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-21571-2_21", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the notion of synchronization from the point of view\nof combinatorics. As a first step, we address the quantitative problem of\ncounting the number of executions of simple processes interacting with\nsynchronization barriers. We elaborate a systematic decomposition of processes\nthat produces a symbolic integral formula to solve the problem. Based on this\nprocedure, we develop a generic algorithm to generate process executions\nuniformly at random. For some interesting sub-classes of processes we propose\nvery efficient counting and random sampling algorithms. All these algorithms\nhave one important characteristic in common: they work on the control graph of\nprocesses and thus do not require the explicit construction of the state-space.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 12:27:46 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Bodini", "Olivier", ""], ["Dien", "Matthieu", ""], ["Genitrini", "Antoine", ""], ["Peschanski", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1907.04262", "submitter": "\\'Akos Hajdu", "authors": "\\'Akos Hajdu and Dejan Jovanovi\\'c", "title": "solc-verify: A Modular Verifier for Solidity Smart Contracts", "comments": "Authors' manuscript. Published in S. Chakraborty and J. A. Navas\n  (Eds.): VSTTE 2019, LNCS 12031, 2020. The final publication is available at\n  Springer via https://doi.org/10.1007/978-3-030-41600-3_11", "journal-ref": null, "doi": "10.1007/978-3-030-41600-3_11", "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present solc-verify, a source-level verification tool for Ethereum smart\ncontracts. Solc-verify takes smart contracts written in Solidity and discharges\nverification conditions using modular program analysis and SMT solvers. Built\non top of the Solidity compiler, solc-verify reasons at the level of the\ncontract source code, as opposed to the more common approaches that operate at\nthe level of Ethereum bytecode. This enables solc-verify to effectively reason\nabout high-level contract properties while modeling low-level language\nsemantics precisely. The contract properties, such as contract invariants, loop\ninvariants, and function pre- and post-conditions, can be provided as\nannotations in the code by the developer. This enables automated, yet\nuser-friendly formal verification for smart contracts. We demonstrate\nsolc-verify by examining real-world examples where our tool can effectively\nfind bugs and prove correctness of non-trivial properties with minimal user\neffort.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 15:58:08 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 13:43:41 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Hajdu", "\u00c1kos", ""], ["Jovanovi\u0107", "Dejan", ""]]}, {"id": "1907.04934", "submitter": "Isaac Oscar Gariano", "authors": "Isaac Oscar Gariano, James Noble and Marco Servetto", "title": "CallE: An Effect System for Method Calls", "comments": null, "journal-ref": null, "doi": "10.1145/3359591.3359731", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effect systems are used to statically reason about the effects an expression\nmay have when evaluated. In the literature, such effects include various\nbehaviours as diverse as memory accesses and exception throwing. Here we\npresent CallE, an object-oriented language that takes a flexible approach where\neffects are just method calls: this works well because ordinary methods often\nmodel things like I/O operations, access to global state, or primitive language\noperations such as thread creation. CallE supports both flexible and\nfine-grained control over such behaviour, in a way designed to minimise the\ncomplexity of annotations.\n  CallE's effect system can be used to prevent OO code from performing\nprivileged operations, such as querying a database, modifying GUI widgets,\nexiting the program, or performing network communication. It can also be used\nto ensure determinism, by preventing methods from (indirectly) calling\nnon-deterministic primitives like random number generation or file reading.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 21:26:49 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 01:28:03 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 23:48:07 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Gariano", "Isaac Oscar", ""], ["Noble", "James", ""], ["Servetto", "Marco", ""]]}, {"id": "1907.05045", "submitter": "Bernhard Scholz", "authors": "David Zhao, Pavle Subotic, Bernhard Scholz", "title": "Provenance for Large-scale Datalog", "comments": "28 pages, 18 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logic programming languages such as Datalog have become popular as Domain\nSpecific Languages (DSLs) for solving large-scale, real-world problems, in\nparticular, static program analysis and network analysis. The logic\nspecifications which model analysis problems, process millions of tuples of\ndata and contain hundreds of highly recursive rules. As a result, they are\nnotoriously difficult to debug. While the database community has proposed\nseveral data-provenance techniques that address the Declarative Debugging\nChallenge for Databases, in the cases of analysis problems, these\nstate-of-the-art techniques do not scale.\n  In this paper, we introduce a novel bottom-up Datalog evaluation strategy for\ndebugging: our provenance evaluation strategy relies on a new provenance\nlattice that includes proof annotations, and a new fixed-point semantics for\nsemi-naive evaluation. A debugging query mechanism allows arbitrary provenance\nqueries, constructing partial proof trees of tuples with minimal height. We\nintegrate our technique into Souffle, a Datalog engine that synthesizes C++\ncode, and achieve high performance by using specialized parallel data\nstructures. Experiments are conducted with DOOP/DaCapo, producing proof\nannotations for tens of millions of output tuples. We show that our method has\na runtime overhead of 1.27x on average while being more flexible than existing\nstate-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 08:33:13 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Zhao", "David", ""], ["Subotic", "Pavle", ""], ["Scholz", "Bernhard", ""]]}, {"id": "1907.05118", "submitter": "Olivier Fl\\\"uckiger", "authors": "Olivier Fl\\\"uckiger, Guido Chari, Jan Je\\v{c}men, Ming-Ho Yee, Jakob\n  Hain, Jan Vitek", "title": "R Melts Brains -- An IR for First-Class Environments and Lazy Effectful\n  Arguments", "comments": null, "journal-ref": "Proceedings of the 15th ACM SIGPLAN International Symposium on\n  Dynamic Languages (DLS 2019)", "doi": "10.1145/3359619.3359744", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The R programming language combines a number of features considered hard to\nanalyze and implement efficiently: dynamic typing, reflection, lazy evaluation,\nvectorized primitive types, first-class closures, and extensive use of native\ncode. Additionally, variable scopes are reified at runtime as first-class\nenvironments. The combination of these features renders most static program\nanalysis techniques impractical, and thus, compiler optimizations based on them\nineffective. We present our work on PIR, an intermediate representation with\nexplicit support for first-class environments and effectful lazy evaluation. We\ndescribe two dataflow analyses on PIR: the first enables reasoning about\nvariables and their environments, and the second infers where arguments are\nevaluated. Leveraging their results, we show how to elide environment creation\nand inline functions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 11:32:07 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 21:32:34 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Fl\u00fcckiger", "Olivier", ""], ["Chari", "Guido", ""], ["Je\u010dmen", "Jan", ""], ["Yee", "Ming-Ho", ""], ["Hain", "Jakob", ""], ["Vitek", "Jan", ""]]}, {"id": "1907.05244", "submitter": "Catalin Hritcu", "authors": "Kenji Maillard, Catalin Hritcu, Exequiel Rivas, Antoine Van Muylder", "title": "The Next 700 Relational Program Logics", "comments": "POPL 2020 camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose the first framework for defining relational program logics for\narbitrary monadic effects. The framework is embedded within a relational\ndependent type theory and is highly expressive. At the semantic level, we\nprovide an algebraic presentation of relational specifications as a class of\nrelative monads, and link computations and specifications by introducing\nrelational effect observations, which map pairs of monadic computations to\nrelational specifications in a way that respects the algebraic structure. For\nan arbitrary relational effect observation, we generically define the core of a\nsound relational program logic, and explain how to complete it to a\nfull-fledged logic for the monadic effect at hand. We show that this generic\nframework can be used to define relational program logics for effects as\ndiverse as state, input-output, nondeterminism, and discrete probabilities. We,\nmoreover, show that by instantiating our framework with state and unbounded\niteration we can embed a variant of Benton's Relational Hoare Logic, and also\nsketch how to reconstruct Relational Hoare Type Theory. Finally, we identify\nand overcome conceptual challenges that prevented previous relational program\nlogics from properly dealing with control effects, and are the first to provide\na relational program logic for exceptions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 14:39:27 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 14:20:45 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 10:13:13 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Maillard", "Kenji", ""], ["Hritcu", "Catalin", ""], ["Rivas", "Exequiel", ""], ["Van Muylder", "Antoine", ""]]}, {"id": "1907.05308", "submitter": "Michael Greenberg", "authors": "Michael Greenberg and Austin J. Blatt", "title": "Executable formal semantics for the POSIX shell", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The POSIX shell is a widely deployed, powerful tool for managing computer\nsystems. The shell is the expert's control panel, a necessary tool for\nconfiguring, compiling, installing, maintaining, and deploying systems. Even\nthough it is powerful, critical infrastructure, the POSIX shell is maligned and\nmisunderstood. Its power and its subtlety are a dangerous combination.\n  We define a formal, mechanized, executable small-step semantics for the POSIX\nshell, which we call Smoosh. We compared Smoosh against seven other shells that\naim for some measure of POSIX compliance (bash, dash, zsh, OSH, mksh, ksh93,\nand yash). Using three test suites---the POSIX test suite, the Modernish test\nsuite and shell diagnosis, and a test suite of our own device---we found\nSmoosh's semantics to be the most conformant to the POSIX standard. Modernish\njudges Smoosh to have the fewest bugs (just one, from using dash's parser) and\nno quirks. To show that our semantics is useful beyond yielding a conformant,\nexecutable shell, we also implemented a symbolic stepper to illuminate the\nsubtle behavior of the shell.\n  Smoosh will serve as a foundation for formal study of the POSIX shell,\nsupporting research on and development of new shells, new tooling for shells,\nand new shell designs.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 15:33:34 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Greenberg", "Michael", ""], ["Blatt", "Austin J.", ""]]}, {"id": "1907.05320", "submitter": "Catalin Hritcu", "authors": "Carmine Abate, Roberto Blanco, Stefan Ciobaca, Adrien Durier, Deepak\n  Garg, Catalin Hritcu, Marco Patrignani, \\'Eric Tanter, J\\'er\\'emy Thibault", "title": "Trace-Relating Compiler Correctness and Secure Compilation", "comments": "ESOP'20 camera ready version together with online appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compiler correctness is, in its simplest form, defined as the inclusion of\nthe set of traces of the compiled program into the set of traces of the\noriginal program, which is equivalent to the preservation of all trace\nproperties. Here traces collect, for instance, the externally observable events\nof each execution. This definition requires, however, the set of traces of the\nsource and target languages to be exactly the same, which is not the case when\nthe languages are far apart or when observations are fine-grained. To overcome\nthis issue, we study a generalized compiler correctness definition, which uses\nsource and target traces drawn from potentially different sets and connected by\nan arbitrary relation. We set out to understand what guarantees this\ngeneralized compiler correctness definition gives us when instantiated with a\nnon-trivial relation on traces. When this trace relation is not equality, it is\nno longer possible to preserve the trace properties of the source program\nunchanged. Instead, we provide a generic characterization of the target trace\nproperty ensured by correctly compiling a program that satisfies a given source\nproperty, and dually, of the source trace property one is required to show in\norder to obtain a certain target property for the compiled code. We show that\nthis view on compiler correctness can naturally account for undefined behavior,\nresource exhaustion, different source and target values, side-channels, and\nvarious abstraction mismatches. Finally, we show that the same generalization\nalso applies to many secure compilation definitions, which characterize the\nprotection of a compiled program against linked adversarial code.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 15:45:58 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 08:50:55 GMT"}, {"version": "v3", "created": "Fri, 25 Oct 2019 13:34:38 GMT"}, {"version": "v4", "created": "Sun, 23 Feb 2020 12:40:39 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Abate", "Carmine", ""], ["Blanco", "Roberto", ""], ["Ciobaca", "Stefan", ""], ["Durier", "Adrien", ""], ["Garg", "Deepak", ""], ["Hritcu", "Catalin", ""], ["Patrignani", "Marco", ""], ["Tanter", "\u00c9ric", ""], ["Thibault", "J\u00e9r\u00e9my", ""]]}, {"id": "1907.05431", "submitter": "Abhinav Verma", "authors": "Abhinav Verma, Hoang M. Le, Yisong Yue, Swarat Chaudhuri", "title": "Imitation-Projected Programmatic Reinforcement Learning", "comments": "Published in Advances in Neural Information Processing Systems\n  (NeurIPS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of programmatic reinforcement learning, in which\npolicies are represented as short programs in a symbolic language. Programmatic\npolicies can be more interpretable, generalizable, and amenable to formal\nverification than neural policies; however, designing rigorous learning\napproaches for such policies remains a challenge. Our approach to this\nchallenge -- a meta-algorithm called PROPEL -- is based on three insights.\nFirst, we view our learning task as optimization in policy space, modulo the\nconstraint that the desired policy has a programmatic representation, and solve\nthis optimization problem using a form of mirror descent that takes a gradient\nstep into the unconstrained policy space and then projects back onto the\nconstrained space. Second, we view the unconstrained policy space as mixing\nneural and programmatic representations, which enables employing\nstate-of-the-art deep policy gradient approaches. Third, we cast the projection\nstep as program synthesis via imitation learning, and exploit contemporary\ncombinatorial methods for this task. We present theoretical convergence results\nfor PROPEL and empirically evaluate the approach in three continuous control\ndomains. The experiments show that PROPEL can significantly outperform\nstate-of-the-art approaches for learning programmatic policies.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 18:00:56 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 05:14:08 GMT"}, {"version": "v3", "created": "Mon, 4 Nov 2019 12:32:34 GMT"}, {"version": "v4", "created": "Tue, 19 Jan 2021 20:52:42 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Verma", "Abhinav", ""], ["Le", "Hoang M.", ""], ["Yue", "Yisong", ""], ["Chaudhuri", "Swarat", ""]]}, {"id": "1907.05451", "submitter": "Shivam Handa", "authors": "Shivam Handa, Vikash Mansinghka and Martin Rinard", "title": "Compositional Inference Metaprogramming with Convergence Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference metaprogramming enables effective probabilistic programming by\nsupporting the decomposition of executions of probabilistic programs into\nsubproblems and the deployment of hybrid probabilistic inference algorithms\nthat apply different probabilistic inference algorithms to different\nsubproblems. We introduce the concept of independent subproblem inference (as\nopposed to entangled subproblem inference in which the subproblem inference\nalgorithm operates over the full program trace) and present a mathematical\nframework for studying convergence properties of hybrid inference algorithms\nthat apply different Markov-Chain Monte Carlo algorithms to different parts of\nthe inference problem. We then use this formalism to prove asymptotic\nconvergence results for probablistic programs with inference metaprogramming.\nTo the best of our knowledge this is the first asymptotic convergence result\nfor hybrid probabilistic inference algorithms defined by (subproblem-based)\ninference metaprogramming.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 19:07:49 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 14:57:54 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Handa", "Shivam", ""], ["Mansinghka", "Vikash", ""], ["Rinard", "Martin", ""]]}, {"id": "1907.05590", "submitter": "Giuseppe Castagna", "authors": "Giuseppe Castagna (CNRS, UParis), Victor Lanvin (IRIF), Micka\\\"el\n  Laurent (ENS Paris Saclay), Kim Nguyen (LRI)", "title": "Revisiting Occurrence Typing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit occurrence typing, a technique to refine the type of variables\noccurring in type-cases and, thus, capture some programming patterns used in\nuntyped languages. Although occurrence typing was tied from its inception to\nset-theoretic types -- union types, in particular -- it never fully exploited\nthe capabilities of these types. Here we show how, by using set-theoretic\ntypes, it is possible to develop a general typing framework that encompasses\nand generalizes several aspects of current occurrence typing proposals and that\ncan be applied to tackle other problems such as the reconstruction of\nintersection types for unannotated or partially annotated functions and the\noptimization of the compilation of gradually typed languages.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 06:43:14 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 08:57:24 GMT"}, {"version": "v3", "created": "Wed, 17 Jul 2019 08:23:59 GMT"}, {"version": "v4", "created": "Tue, 5 Nov 2019 15:49:39 GMT"}, {"version": "v5", "created": "Wed, 4 Mar 2020 14:59:08 GMT"}, {"version": "v6", "created": "Wed, 12 May 2021 08:40:29 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Castagna", "Giuseppe", "", "CNRS, UParis"], ["Lanvin", "Victor", "", "IRIF"], ["Laurent", "Micka\u00ebl", "", "ENS Paris Saclay"], ["Nguyen", "Kim", "", "LRI"]]}, {"id": "1907.05637", "submitter": "Hong Long Pham", "authors": "Long H. Pham, Quang Loc Le, Quoc-Sang Phan, Jun Sun", "title": "Concolic Testing Heap-Manipulating Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concolic testing is a test generation technique which works effectively by\nintegrating random testing generation and symbolic execution. Existing concolic\ntesting engines focus on numeric programs. Heap-manipulating programs make\nextensive use of complex heap objects like trees and lists. Testing such\nprograms is challenging due to multiple reasons. Firstly, test inputs for such\nprogram are required to satisfy non-trivial constraints which must be specified\nprecisely. Secondly, precisely encoding and solving path conditions in such\nprograms are challenging and often expensive. In this work, we propose the\nfirst concolic testing engine called CSF for heap-manipulating programs based\non separation logic. CSF effectively combines specification-based testing and\nconcolic execution for test input generation. It is evaluated on a set of\nchallenging heap-manipulating programs. The results show that CSF generates\nvalid test inputs with high coverage efficiently. Furthermore, we show that CSF\ncan be potentially used in combination with precondition inference tools to\nreduce the user effort.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 09:17:35 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Pham", "Long H.", ""], ["Le", "Quang Loc", ""], ["Phan", "Quoc-Sang", ""], ["Sun", "Jun", ""]]}, {"id": "1907.05649", "submitter": "Bruce Collie", "authors": "Bruce Collie, Michael O'Boyle", "title": "Augmenting Type Signatures for Program Synthesis", "comments": "TyDe 2019 Extended Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective program synthesis requires a way to minimise the number of\ncandidate programs being searched. A type signature, for example, places some\nsmall restrictions on the structure of potential candidates. We introduce and\nmotivate a distilled program synthesis problem where a type signature is the\nonly machine-readable information available, but does not sufficiently minimise\nthe search space. To address this, we develop a system of property relations\nthat can be used to flexibly encode and query information that was not\npreviously available to the synthesiser. Our experience using these tools has\nbeen positive: by encoding simple properties and by using a minimal set of\nsynthesis primitives, we have been able to synthesise complex programs in novel\ncontexts\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 09:53:03 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Collie", "Bruce", ""], ["O'Boyle", "Michael", ""]]}, {"id": "1907.05690", "submitter": "Hiroshi Yonai", "authors": "Hiroshi Yonai, Yasuhiro Hayase, Hiroyuki Kitagawa", "title": "Mercem: Method Name Recommendation Based on Call Graph Embedding", "comments": "9 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comprehensibility of source code is strongly affected by identifier names,\ntherefore software developers need to give good (e.g. meaningful but short)\nnames to identifiers. On the other hand, giving a good name is sometimes a\ndifficult and time-consuming task even for experienced developers. To support\nnaming identifiers, several techniques for recommending identifier name\ncandidates have been proposed. These techniques, however, still have challenges\non the goodness of suggested candidates and limitations on applicable\nsituations. This paper proposes a new approach to recommending method names by\napplying graph embedding techniques to the method call graph. The evaluation\nexperiment confirms that the proposed technique can suggest more appropriate\nmethod name candidates in difficult situations than the state of the art\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 12:06:50 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Yonai", "Hiroshi", ""], ["Hayase", "Yasuhiro", ""], ["Kitagawa", "Hiroyuki", ""]]}, {"id": "1907.05818", "submitter": "James Cheney", "authors": "Jan Stolarek and James Cheney", "title": "Verified Self-Explaining Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common programming tools, like compilers, debuggers, and IDEs, crucially rely\non the ability to analyse program code to reason about its behaviour and\nproperties. There has been a great deal of work on verifying compilers and\nstatic analyses, but far less on verifying dynamic analyses such as program\nslicing. Recently, a new mathematical framework for slicing was introduced in\nwhich forward and backward slicing are dual in the sense that they constitute a\nGalois connection. This paper formalises forward and backward dynamic slicing\nalgorithms for a simple imperative programming language, and formally verifies\ntheir duality using the Coq proof assistant.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 16:13:03 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Stolarek", "Jan", ""], ["Cheney", "James", ""]]}, {"id": "1907.05871", "submitter": "Youssef Bassil", "authors": "Youssef Bassil", "title": "Phoenix -- The Arabic Object-Oriented Programming Language", "comments": "LACSC Lebanese Association for Computational Sciences,\n  http://www.lacsc.org", "journal-ref": "International Journal of Computer Trends and Technology, vol. 67,\n  no. 2, pp. 7-11, 2019", "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A computer program is a set of electronic instructions executed from within\nthe computer memory by the computer central processing unit. Its purpose is to\ncontrol the functionalities of the computer allowing it to perform various\ntasks. Basically, a computer program is written by humans using a programming\nlanguage. A programming language is the set of grammatical rules and vocabulary\nthat governs the correct writing of a computer program. In practice, the\nmajority of the existing programming languages are written in English-speaking\ncountries and thus they all use the English language to express their syntax\nand vocabulary. However, many other programming languages were written in\nnon-English languages, for instance, the Chinese BASIC, the Chinese Python, the\nRussian Rapira, and the Arabic Loughaty. This paper discusses the design and\nimplementation of a new programming language, called Phoenix. It is a\nGeneral-Purpose, High-Level, Imperative, Object-Oriented, and Compiled Arabic\nprogramming language that uses the Arabic language as syntax and vocabulary.\nThe core of Phoenix is a compiler system made up of six components, they are\nthe Preprocessor, the scanner, the parser, the semantic analyzer, the code\ngenerator, and the linker. The experiments conducted have illustrated the\nseveral powerful features of the Phoenix language including functions,\nwhile-loop, and arithmetic operations. As future work, more advanced features\nare to be developed including inheritance, polymorphism, file processing,\ngraphical user interface, and networking.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 16:54:49 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Bassil", "Youssef", ""]]}, {"id": "1907.05873", "submitter": "Youssef Bassil", "authors": "Youssef Bassil", "title": "Compiler Design for Legal Document Translation in Digital Government", "comments": "LACSC Lebanese Association for Computational Sciences,\n  http://www.lacsc.org", "journal-ref": "International Journal of Engineering Trends and Technology, vol.\n  67, no. 3, pp. 100-104, 2019", "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main purposes of a computer is automation. In fact, automation is\nthe technology by which a manual task is performed with minimum or zero human\nassistance. Over the years, automation has proved to reduce operation cost and\nmaintenance time in addition to increase system productivity, reliability, and\nperformance. Today, most computerized automation are done by a computer program\nwhich is a set of instructions executed from within the computer memory by the\ncomputer central processing unit to control the computers various operations.\nThis paper proposes a compiler program that automates the validation and\ntranslation of input documents written in the Arabic language into XML output\nfiles that can be read by a computer. The input document is by nature\nunstructured and in plain-text as it is written by people manually; while, the\ngenerated output is a structured machine-readable XML file. The proposed\ncompiler program is actually a part of a bigger project related to digital\ngovernment and is meant to automate the processing and archiving of juridical\ndata and documents. In essence, the proposed compiler program is composed of a\nscanner, a parser, and a code generator. Experiments showed that such\nautomation practices could prove to be a starting point for a future digital\ngovernment platform for the Lebanese government. As further research, other\ntypes of juridical documents are to be investigated, mainly those that require\nerror detection and correction.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 16:29:23 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Bassil", "Youssef", ""]]}, {"id": "1907.05920", "submitter": "Steffen Smolka", "authors": "Steffen Smolka, Nate Foster, Justin Hsu, Tobias Kapp\\'e, Dexter Kozen,\n  and Alexandra Silva", "title": "Guarded Kleene Algebra with Tests: Verification of Uninterpreted\n  Programs in Nearly Linear Time", "comments": "Extended version with appendix", "journal-ref": "POPL 2020", "doi": "10.1145/3371129", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guarded Kleene Algebra with Tests (GKAT) is a variation on Kleene Algebra\nwith Tests (KAT) that arises by restricting the union ($+$) and iteration ($*$)\noperations from KAT to predicate-guarded versions. We develop the (co)algebraic\ntheory of GKAT and show how it can be efficiently used to reason about\nimperative programs. In contrast to KAT, whose equational theory is\nPSPACE-complete, we show that the equational theory of GKAT is (almost) linear\ntime. We also provide a full Kleene theorem and prove completeness for an\nanalogue of Salomaa's axiomatization of Kleene Algebra.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 18:54:04 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 09:21:41 GMT"}, {"version": "v3", "created": "Wed, 20 Nov 2019 08:06:44 GMT"}, {"version": "v4", "created": "Fri, 13 Dec 2019 17:45:10 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Smolka", "Steffen", ""], ["Foster", "Nate", ""], ["Hsu", "Justin", ""], ["Kapp\u00e9", "Tobias", ""], ["Kozen", "Dexter", ""], ["Silva", "Alexandra", ""]]}, {"id": "1907.06057", "submitter": "Giulio Guerrieri", "authors": "Beniamino Accattoli, Andrea Condoluci, Giulio Guerrieri, Claudio\n  Sacerdoti Coen", "title": "Crumbling Abstract Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending the lambda-calculus with a construct for sharing, such as let\nexpressions, enables a special representation of terms: iterated applications\nare decomposed by introducing sharing points in between any two of them,\nreducing to the case where applications have only values as immediate subterms.\n  This work studies how such a crumbled representation of terms impacts on the\ndesign and the efficiency of abstract machines for call-by-value evaluation.\nAbout the design, it removes the need for data structures encoding the\nevaluation context, such as the applicative stack and the dump, that get\nencoded in the environment. About efficiency, we show that there is no\nslowdown, clarifying in particular a point raised by Kennedy, about the\npotential inefficiency of such a representation.\n  Moreover, we prove that everything smoothly scales up to the delicate case of\nopen terms, needed to implement proof assistants. Along the way, we also point\nout that continuation-passing style transformations--that may be alternatives\nto our representation--do not scale up to the open case.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 11:46:02 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Accattoli", "Beniamino", ""], ["Condoluci", "Andrea", ""], ["Guerrieri", "Giulio", ""], ["Coen", "Claudio Sacerdoti", ""]]}, {"id": "1907.06205", "submitter": "Venkatesh Theru Mohan", "authors": "Venkatesh Theru Mohan and Ali Jannesari", "title": "Automatic Repair and Type Binding of Undeclared Variables using Neural\n  Networks", "comments": "16 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.FL cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning had been used in program analysis for the prediction of hidden\nsoftware defects using software defect datasets, security vulnerabilities using\ngenerative adversarial networks as well as identifying syntax errors by\nlearning a trained neural machine translation on program codes. However, all\nthese approaches either require defect datasets or bug-free source codes that\nare executable for training the deep learning model. Our neural network model\nis neither trained with any defect datasets nor bug-free programming source\ncodes, instead it is trained using structural semantic details of Abstract\nSyntax Tree (AST) where each node represents a construct appearing in the\nsource code. This model is implemented to fix one of the most common semantic\nerrors, such as undeclared variable errors as well as infer their type\ninformation before program compilation. By this approach, the model has\nachieved in correctly locating and identifying 81% of the programs on prutor\ndataset of 1059 programs with only undeclared variable errors and also\ninferring their types correctly in 80% of the programs.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 11:14:14 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Mohan", "Venkatesh Theru", ""], ["Jannesari", "Ali", ""]]}, {"id": "1907.06249", "submitter": "Feras Saad", "authors": "Feras A. Saad, Marco F. Cusumano-Towner, Ulrich Schaechtle, Martin C.\n  Rinard, Vikash K. Mansinghka", "title": "Bayesian Synthesis of Probabilistic Programs for Automatic Data Modeling", "comments": null, "journal-ref": "Proc. ACM Program. Lang. 3, POPL, Article 37 (January 2019)", "doi": "10.1145/3290350", "report-no": null, "categories": "cs.PL cs.AI cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new techniques for automatically constructing probabilistic\nprograms for data analysis, interpretation, and prediction. These techniques\nwork with probabilistic domain-specific data modeling languages that capture\nkey properties of a broad class of data generating processes, using Bayesian\ninference to synthesize probabilistic programs in these modeling languages\ngiven observed data. We provide a precise formulation of Bayesian synthesis for\nautomatic data modeling that identifies sufficient conditions for the resulting\nsynthesis procedure to be sound. We also derive a general class of synthesis\nalgorithms for domain-specific languages specified by probabilistic\ncontext-free grammars and establish the soundness of our approach for these\nlanguages. We apply the techniques to automatically synthesize probabilistic\nprograms for time series data and multivariate tabular data. We show how to\nanalyze the structure of the synthesized programs to compute, for key\nqualitative properties of interest, the probability that the underlying data\ngenerating process exhibits each of these properties. Second, we translate\nprobabilistic programs in the domain-specific language into probabilistic\nprograms in Venture, a general-purpose probabilistic programming system. The\ntranslated Venture programs are then executed to obtain predictions of new time\nseries data and new multivariate data records. Experimental results show that\nour techniques can accurately infer qualitative structure in multiple\nreal-world data sets and outperform standard data analysis methods in\nforecasting and predicting new data.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 17:12:55 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Saad", "Feras A.", ""], ["Cusumano-Towner", "Marco F.", ""], ["Schaechtle", "Ulrich", ""], ["Rinard", "Martin C.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1907.06522", "submitter": "Chenyi  Zhang", "authors": "Xilong Zhuo and Chenyi Zhang", "title": "A Relational Static Semantics for Call Graph Construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of resolving virtual method and interface calls in\nobject-oriented languages has been a long standing challenge to the program\nanalysis community. The complexities are due to various reasons, such as\nincreased levels of class inheritance and polymorphism in large programs. In\nthis paper, we propose a new approach called type flow analysis that represent\npropagation of type information between program variables by a group of\nrelations without the help of a heap abstraction. We prove that regarding the\nprecision on reachability of class information to a variable, our method\nproduces results equivalent to that one can derive from a points-to analysis.\nMoreover, in practice, our method consumes lower time and space usage, as\nsupported by the experimental results.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 14:32:52 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Zhuo", "Xilong", ""], ["Zhang", "Chenyi", ""]]}, {"id": "1907.07154", "submitter": "J\\\"orn Koepe", "authors": "J\\\"orn Koepe", "title": "Object-Capability as a Means of Permission and Authority in Software\n  Systems", "comments": "6 pages, Technical report", "journal-ref": null, "doi": null, "report-no": "tr-ri-19-360", "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The object-capability model is a security measure that consists in encoding\naccess rights in individual objects to restrict its interactions with other\nobjects. Since its introduction in 2013, different approaches to\nobject-capability have been formalized and implemented. In this paper, we\npresent the object-capability model, and present and discuss the\nstate-of-the-art research in the area. In the end, we conclude, that object\ncapabilities can help in increasing the security of software, although this\nconcept is not widely spread.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 17:26:50 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Koepe", "J\u00f6rn", ""]]}, {"id": "1907.07283", "submitter": "Vikraman Choudhury", "authors": "Vikraman Choudhury and Neel Krishnaswami", "title": "Recovering Purity with Comonads and Capabilities", "comments": "Extended version of the ICFP 2020 paper", "journal-ref": null, "doi": "10.1145/3408993", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we take a pervasively effectful (in the style of ML) typed\nlambda calculus, and show how to extend it to permit capturing pure expressions\nwith types. Our key observation is that, just as the pure simply-typed lambda\ncalculus can be extended to support effects with a monadic type discipline, an\nimpure typed lambda calculus can be extended to support purity with a comonadic\ntype discipline. We establish the correctness of our type system via a simple\ndenotational model, which we call the capability space model. Our model\nformalizes the intuition common to systems programmers that the ability to\nperform effects should be controlled via access to a permission or capability,\nand that a program is capability-safe if it performs no effects that it does\nnot have a runtime capability for. We then identify the axiomatic categorical\nstructure that the capability space model validates, and use these axioms to\ngive a categorical semantics for our comonadic type system. We then give an\nequational theory (substitution and the call-by-value $\\beta$ and $\\eta$ laws)\nfor the imperative lambda calculus, and show its soundness relative to this\nsemantics. Finally, we give a translation of the pure simply-typed lambda\ncalculus into our comonadic imperative calculus, and show that any two terms\nwhich are $\\beta\\eta$-equal in the STLC are equal in the equational theory of\nthe comonadic calculus, establishing that pure programs can be mapped in an\nequation-preserving way into our imperative calculus.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 23:06:31 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 18:22:23 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 23:33:54 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Choudhury", "Vikraman", ""], ["Krishnaswami", "Neel", ""]]}, {"id": "1907.07587", "submitter": "Michael Innes", "authors": "Mike Innes, Alan Edelman, Keno Fischer, Chris Rackauckas, Elliot Saba,\n  Viral B Shah, Will Tebbutt", "title": "A Differentiable Programming System to Bridge Machine Learning and\n  Scientific Computing", "comments": "Submitted to NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific computing is increasingly incorporating the advancements in\nmachine learning and the ability to work with large amounts of data. At the\nsame time, machine learning models are becoming increasingly sophisticated and\nexhibit many features often seen in scientific computing, stressing the\ncapabilities of machine learning frameworks. Just as the disciplines of\nscientific computing and machine learning have shared common underlying\ninfrastructure in the form of numerical linear algebra, we now have the\nopportunity to further share new computational infrastructure, and thus ideas,\nin the form of Differentiable Programming. We describe Zygote, a Differentiable\nProgramming system that is able to take gradients of general program\nstructures. We implement this system in the Julia programming language. Our\nsystem supports almost all language constructs (control flow, recursion,\nmutation, etc.) and compiles high-performance code without requiring any user\nintervention or refactoring to stage computations. This enables an expressive\nprogramming model for deep learning, but more importantly, it enables us to\nincorporate a large ecosystem of libraries in our models in a straightforward\nway. We discuss our approach to automatic differentiation, including its\nsupport for advanced techniques such as mixed-mode, complex and checkpointed\ndifferentiation, and present several examples of differentiating programs.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 15:35:04 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 12:56:11 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Innes", "Mike", ""], ["Edelman", "Alan", ""], ["Fischer", "Keno", ""], ["Rackauckas", "Chris", ""], ["Saba", "Elliot", ""], ["Shah", "Viral B", ""], ["Tebbutt", "Will", ""]]}, {"id": "1907.07761", "submitter": "Malte Schmitz", "authors": "Martin Leucker, C\\'esar S\\'anchez, Torben Scheffel, Malte Schmitz,\n  Daniel Thoma", "title": "Runtime Verification For Timed Event Streams With Partial Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Runtime Verification (RV) studies how to analyze execution traces of a system\nunder observation. Stream Runtime Verification (SRV) applies stream\ntransformations to obtain information from observed traces. Incomplete traces\nwith information missing in gaps pose a common challenge when applying RV and\nSRV techniques to real-world systems as RV approaches typically require the\ncomplete trace without missing parts. This paper presents a solution to perform\nSRV on incomplete traces based on abstraction. We use TeSSLa as specification\nlanguage for non-synchronized timed event streams and define abstract event\nstreams representing the set of all possible traces that could have occurred\nduring gaps in the input trace. We show how to translate a TeSSLa specification\nto its abstract counterpart that can propagate gaps through the transformation\nof the input streams and thus generate sound outputs even if the input streams\ncontain gaps and events with imprecise values. The solution has been\nimplemented as a set of macros for the original TeSSLa and an empirical\nevaluation shows the feasibility of the approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 15:27:44 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Leucker", "Martin", ""], ["S\u00e1nchez", "C\u00e9sar", ""], ["Scheffel", "Torben", ""], ["Schmitz", "Malte", ""], ["Thoma", "Daniel", ""]]}, {"id": "1907.07764", "submitter": "Issam Damaj", "authors": "Ahmed Ablak and Issam Damaj (American University of Kuwait)", "title": "HTCC: Haskell to Handel-C Compiler", "comments": "8 pages, 8 figures, 2 tables", "journal-ref": "The 19th EUROMICRO Conference on Digital System Design, IEEE,\n  Limassol, Cyprus, August 31-September 2 (2016) 192-199", "doi": "10.1109/DSD.2016.24", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional programming languages, such as Haskell, enable simple, concise,\nand correct-by-construction hardware development. HTCC compiles a subset of\nHaskell to Handel-C language with hardware output. Moreover, HTCC generates\nVHDL, Verilog, EDIF, and SystemC programs. The design of HTCC compiler includes\nlexical, syntax and semantic analyzers. HTCC automates a transformational\nderivation methodology to rapidly produce hardware that maps onto Field\nProgrammable Gate Arrays (FPGAs) . HTCC is generated using ANTLR\ncompiler-compiler tool and supports an effective integrated development\nenvironment. This paper presents the design rationale and the implementation of\nHTCC. Several sample generations of first-class and higher-order functions are\npresented. In-addition, a compilation case-study is presented for the XTEA\ncipher. The investigation comprises a thorough evaluation and performance\nanalysis. The targeted FPGAs include Cyclone II, Stratix IV, and Virtex-6 from\nAltera and Xilinx.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 19:45:57 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Ablak", "Ahmed", "", "American University of Kuwait"], ["Damaj", "Issam", "", "American University of Kuwait"]]}, {"id": "1907.07794", "submitter": "Alex Sanchez-Stern", "authors": "Alex Sanchez-Stern and Yousef Alhessi and Lawrence Saul and Sorin\n  Lerner", "title": "Generating Correctness Proofs with Neural Networks", "comments": "Condensed version to be published at MAPL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Foundational verification allows programmers to build software which has been\nempirically shown to have high levels of assurance in a variety of important\ndomains. However, the cost of producing foundationally verified software\nremains prohibitively high for most projects,as it requires significant manual\neffort by highly trained experts. In this paper we present Proverbot9001,a\nproof search system using machine learning techniques to produce proofs of\nsoftware correctness in interactive theorem provers. We demonstrate\nProverbot9001 on the proof obligations from a large practical proof project,the\nCompCert verified C compiler,and show that it can effectively automate what\nwere previously manual proofs,automatically producing proofs for 27.5% of\ntheorem statements in our test dataset, when combined with solver-based\ntooling. Without any additional solvers,we exhibit a proof completion rate that\nis a 4X improvement over prior state-of-the-art machine learning models for\ngenerating proofs in Coq.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 22:11:13 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 18:32:16 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 17:39:35 GMT"}, {"version": "v4", "created": "Thu, 28 May 2020 19:28:53 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Sanchez-Stern", "Alex", ""], ["Alhessi", "Yousef", ""], ["Saul", "Lawrence", ""], ["Lerner", "Sorin", ""]]}, {"id": "1907.08003", "submitter": "Dominik Aumayr", "authors": "Dominik Aumayr, Stefan Marr, Elisa Gonzalez Boix, Hanspeter\n  M\\\"ossenb\\\"ock", "title": "Asynchronous Snapshots of Actor Systems for Latency-Sensitive\n  Applications", "comments": "This is the author's version of the work. It is posted here for your\n  personal use. Not for redistribution. The definitive Version of Record was\n  published in Proceedings of the 16th ACM SIGPLAN International Conference on\n  Managed Programming Languages and Runtimes (MPLR '19), October 21-22, 2019,\n  Athens, Greece, https://doi.org/10.1145/3357390.3361019", "journal-ref": null, "doi": "10.1145/3357390.3361019", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The actor model is popular for many types of server applications. Efficient\nsnapshotting of applications is crucial in the deployment of pre-initialized\napplications or moving running applications to different machines, e.g for\ndebugging purposes. A key issue is that snapshotting blocks all other\noperations. In modern latency-sensitive applications, stopping the application\nto persist its state needs to be avoided, because users may not tolerate the\nincreased request latency. In order to minimize the impact of snapshotting on\nrequest latency, our approach persists the application's state asynchronously\nby capturing partial heaps, completing snapshots step by step. Additionally,\nour solution is transparent and supports arbitrary object graphs. We prototyped\nour snapshotting approach on top of the Truffle/Graal platform and evaluated it\nwith the Savina benchmarks and the Acme Air microservice application. When\nperforming a snapshot every thousand Acme Air requests, the number of slow\nrequests ( 0.007% of all requests) with latency above 100ms increases by 5.43%.\nOur Savina microbenchmark results detail how different utilization patterns\nimpact snapshotting cost. To the best of our knowledge, this is the first\nsystem that enables asynchronous snapshotting of actor applications, i.e.\nwithout stop-the-world synchronization, and thereby minimizes the impact on\nlatency. We thus believe it enables new deployment and debugging options for\nactor systems.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 11:49:57 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 08:40:30 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Aumayr", "Dominik", ""], ["Marr", "Stefan", ""], ["Boix", "Elisa Gonzalez", ""], ["M\u00f6ssenb\u00f6ck", "Hanspeter", ""]]}, {"id": "1907.08251", "submitter": "Chaoqiang Deng", "authors": "Chaoqiang Deng, Patrick Cousot", "title": "Responsibility Analysis by Abstract Interpretation", "comments": "This is the extended version (33 pages) of a paper to be appeared in\n  the Static Analysis Symposium (SAS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a behavior of interest in the program, statically determining the\ncorresponding responsible entity is a task of critical importance, especially\nin program security. Classical static analysis techniques (e.g. dependency\nanalysis, taint analysis, slicing, etc.) assist programmers in narrowing down\nthe scope of responsibility, but none of them can explicitly identify the\nresponsible entity. Meanwhile, the causality analysis is generally not\npertinent for analyzing programs, and the structural equations model (SEM) of\nactual causality misses some information inherent in programs, making its\nanalysis on programs imprecise. In this paper, a novel definition of\nresponsibility based on the abstraction of event trace semantics is proposed,\nwhich can be applied in program security and other scientific fields. Briefly\nspeaking, an entity ER is responsible for behavior B, if and only if ER is free\nto choose its input value, and such a choice is the first one that ensures the\noccurrence of B in the forthcoming execution. Compared to current analysis\nmethods, the responsibility analysis is more precise. In addition, our\ndefinition of responsibility takes into account the cognizance of the observer,\nwhich, to the best of our knowledge, is a new innovative idea in program\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 19:03:38 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Deng", "Chaoqiang", ""], ["Cousot", "Patrick", ""]]}, {"id": "1907.08337", "submitter": "Jake Roemer", "authors": "Jake Roemer and Michael D. Bond", "title": "Online Set-Based Dynamic Analysis for Sound Predictive Race Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive data race detectors find data races that exist in executions other\nthan the observed execution. Smaragdakis et al. introduced the\ncausally-precedes (CP) relation and a polynomial-time analysis for sound (no\nfalse races) predictive data race detection. However, their analysis cannot\nscale beyond analyzing bounded windows of execution traces. This work\nintroduces a novel dynamic analysis called Raptor that computes CP soundly and\ncompletely. Raptor is inherently an online analysis that analyzes and finds all\nCP-races of an execution trace in its entirety. An evaluation of a prototype\nimplementation of Raptor shows that it scales to program executions that the\nprior CP analysis cannot handle, finding data races that the prior CP analysis\ncannot find.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 01:45:37 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Roemer", "Jake", ""], ["Bond", "Michael D.", ""]]}, {"id": "1907.08695", "submitter": "Adam Duracz", "authors": "Yao-Hsiang Yang, Adam Duracz, Ferenc A. Bartha, Ryuichi Sai, Ahsan\n  Pervaiz, Saeid Barati, Dung Nguyen, Robert Cartwright, Henry Hoffmann,\n  Krishna V. Palem", "title": "Language Support for Adaptation: Intent-Driven Programming in FAST", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historically, programming language semantics has focused on assigning a\nprecise mathematical meaning to programs. That meaning is a function from the\nprogram's input domain to its output domain determined solely by its syntactic\nstructure. Such a semantics, fosters the development of portable applications\nwhich are oblivious to the performance characteristics and limitations (such as\na maximum memory footprint) of particular hardware and software platforms. This\npaper introduces the idea of intent-driven programming where the meaning of a\nprogram additionally depends on an accompanying intent specification expressing\nhow the ordinary program meaning is dynamically modified during execution to\nsatisfy additional properties expressed by the intent. These include both\nintensional properties---e.g., resource usage---and extensional\nproperties---e.g., accuracy of the computed answer. To demonstrate the\nintent-driven programming model's value, this paper presents a general-purpose\nintent-driven programming language---called FAST---implemented as an extension\nof Swift. FAST consists of an intent compiler, a profiler, a general controller\ninterface and a runtime module which supports interoperation with legacy C/C++\ncodes. Compared to existing frameworks for adaptive computing, \\FAST{} supports\ndynamic adaptation to changes both in the operating environment and in the\nintent itself, and enables the mixing of procedural control and control based\non feedback and optimization.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 23:11:03 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Yang", "Yao-Hsiang", ""], ["Duracz", "Adam", ""], ["Bartha", "Ferenc A.", ""], ["Sai", "Ryuichi", ""], ["Pervaiz", "Ahsan", ""], ["Barati", "Saeid", ""], ["Nguyen", "Dung", ""], ["Cartwright", "Robert", ""], ["Hoffmann", "Henry", ""], ["Palem", "Krishna V.", ""]]}, {"id": "1907.08827", "submitter": "Wonyeol Lee", "authors": "Wonyeol Lee, Hangyeol Yu, Xavier Rival, Hongseok Yang", "title": "Towards Verified Stochastic Variational Inference for Probabilistic\n  Programs", "comments": "To appear at Principles of Programming Languages (POPL) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Probabilistic programming is the idea of writing models from statistics and\nmachine learning using program notations and reasoning about these models using\ngeneric inference engines. Recently its combination with deep learning has been\nexplored intensely, leading to the development of deep probabilistic\nprogramming languages such as Pyro. At the core of this development lie\ninference engines based on stochastic variational inference algorithms. When\nasked to find information about the posterior distribution of a model written\nin such a language, these algorithms convert this posterior-inference query\ninto an optimisation problem and solve it approximately by gradient ascent. In\nthis paper, we analyse one of the most fundamental and versatile variational\ninference algorithms, called score estimator, using tools from denotational\nsemantics and program analysis. We formally express what this algorithm does on\nmodels denoted by programs, and expose implicit assumptions made by the\nalgorithm. The violation of these assumptions may lead to an undefined\noptimisation objective or the loss of convergence guarantee of the optimisation\nprocess. We then describe rules for proving these assumptions, which can be\nautomated by static program analyses. Some of our rules use nontrivial facts\nfrom continuous mathematics, and let us replace requirements about integrals in\nthe assumptions, by conditions involving differentiation or boundedness, which\nare much easier to prove automatically. Following our general methodology, we\nhave developed a static program analysis for Pyro that aims at discharging the\nassumption about what we call model-guide support match. Applied to the eight\nrepresentative model-guide pairs from the Pyro webpage, our analysis finds a\nbug in one of these cases, reveals a non-standard use of an inference engine in\nanother, and shows the assumptions are met in the remaining cases.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 15:33:40 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 19:04:47 GMT"}, {"version": "v3", "created": "Sat, 26 Oct 2019 11:36:57 GMT"}, {"version": "v4", "created": "Mon, 18 Nov 2019 05:37:42 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Lee", "Wonyeol", ""], ["Yu", "Hangyeol", ""], ["Rival", "Xavier", ""], ["Yang", "Hongseok", ""]]}, {"id": "1907.08834", "submitter": "James Cheney", "authors": "S\\'andor Bartha and James Cheney", "title": "Towards meta-interpretive learning of programming language semantics", "comments": "ILP 2019, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new application for inductive logic programming: learning the\nsemantics of programming languages from example evaluations. In this short\npaper, we explored a simplified task in this domain using the Metagol\nmeta-interpretive learning system. We highlighted the challenging aspects of\nthis scenario, including abstracting over function symbols, nonterminating\nexamples, and learning non-observed predicates, and proposed extensions to\nMetagol helpful for overcoming these challenges, which may prove useful in\nother domains.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 16:39:06 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Bartha", "S\u00e1ndor", ""], ["Cheney", "James", ""]]}, {"id": "1907.09282", "submitter": "Martin Monperrus", "authors": "Zhongxing Yu, Matias Martinez, Tegawend\\'e F. Bissyand\\'e, Martin\n  Monperrus", "title": "Learning the Relation between Code Features and Code Transforms with\n  Structured Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper the first approach for structurally predicting code\ntransforms at the level of AST nodes using conditional random fields. Our\napproach first learns offline a probabilistic model that captures how certain\ncode transforms are applied to certain AST nodes, and then uses the learned\nmodel to predict transforms for new, unseen code snippets. We implement our\napproach in the context of repair transform prediction for Java programs. Our\nimplementation contains a set of carefully designed code features, deals with\nthe training data imbalance issue, and comprises transform constraints that are\nspecific to code. We conduct a large-scale experimental evaluation based on a\ndataset of 4,590,679 bug fixing commits from real-world Java projects. The\nexperimental results show that our approach predicts the code transforms with a\nsuccess rate varying from 37.1% to 61.1% depending on the transforms.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 12:42:32 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Yu", "Zhongxing", ""], ["Martinez", "Matias", ""], ["Bissyand\u00e9", "Tegawend\u00e9 F.", ""], ["Monperrus", "Martin", ""]]}, {"id": "1907.09820", "submitter": "Angelos Charalambidis", "authors": "Angelos Charalambidis, Christos Nomikos, Panos Rondogiannis", "title": "The Expressive Power of Higher-Order Datalog", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  24 pages, LaTeX", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 925-940", "doi": "10.1017/S1471068419000279", "report-no": null, "categories": "cs.PL cs.CC cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical result in descriptive complexity theory states that Datalog\nexpresses exactly the class of polynomially computable queries on ordered\ndatabases. In this paper we extend this result to the case of higher-order\nDatalog. In particular, we demonstrate that on ordered databases, for all\n$k\\geq2$, $k$-order Datalog captures $(k-1)$-EXPTIME. This result suggests that\nhigher-order extensions of Datalog possess superior expressive power and they\nare worthwhile of further investigation both in theory and in practice. This\npaper is under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 11:21:49 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 14:03:23 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Charalambidis", "Angelos", ""], ["Nomikos", "Christos", ""], ["Rondogiannis", "Panos", ""]]}, {"id": "1907.10096", "submitter": "Enrique Martin-Martin", "authors": "Elvira Albert, Miquel Bofill, Cristina Borralleras, Enrique\n  Martin-Martin, Albert Rubio", "title": "Resource Analysis driven by (Conditional) Termination Proofs", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 722-739", "doi": "10.1017/S1471068419000152", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When programs feature a complex control flow, existing techniques for\nresource analysis produce cost relation systems (CRS) whose cost functions\nretain the complex flow of the program and, consequently, might not be solvable\ninto closed-form upper bounds. This paper presents a novel approach to resource\nanalysis that is driven by the result of a termination analysis. The\nfundamental idea is that the termination proof encapsulates the flows of the\nprogram which are relevant for the cost computation so that, by driving the\ngeneration of the CRS using the termination proof, we produce a\nlinearly-bounded CRS (LB-CRS). A LB-CRS is composed of cost functions that are\nguaranteed to be locally bounded by linear ranking functions and thus greatly\nsimplify the process of CRS solving. We have built a new resource analysis\ntool, named MaxCore, that is guided by the VeryMax termination analyzer and\nuses CoFloCo and PUBS as CRS solvers. Our experimental results on the set of\nbenchmarks from the Complexity and Termination Competition 2019 for C Integer\nprograms show that MaxCore outperforms all other resource analysis tools. Under\nconsideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 18:34:36 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Albert", "Elvira", ""], ["Bofill", "Miquel", ""], ["Borralleras", "Cristina", ""], ["Martin-Martin", "Enrique", ""], ["Rubio", "Albert", ""]]}, {"id": "1907.10278", "submitter": "Ariyam Das", "authors": "Ariyam Das and Carlo Zaniolo", "title": "A Case for Stale Synchronous Distributed Model for Declarative Recursive\n  Computation", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large class of traditional graph and data mining algorithms can be\nconcisely expressed in Datalog, and other Logic-based languages, once\naggregates are allowed in recursion. In fact, for most BigData algorithms, the\ndifficult semantic issues raised by the use of non-monotonic aggregates in\nrecursion are solved by Pre-Mappability (PreM), a property that assures that\nfor a program with aggregates in recursion there is an equivalent\naggregate-stratified program. In this paper we show that, by bringing together\nthe formal abstract semantics of stratified programs with the efficient\noperational one of unstratified programs, PreM can also facilitate and improve\ntheir parallel execution. We prove that PreM-optimized lock-free and\ndecomposable parallel semi-naive evaluations produce the same results as the\nsingle executor programs. Therefore, PreM can be assimilated into the\ndata-parallel computation plans of different distributed systems, irrespective\nof whether these follow bulk synchronous parallel (BSP) or asynchronous\ncomputing models. In addition, we show that non-linear recursive queries can be\nevaluated using a hybrid stale synchronous parallel (SSP) model on distributed\nenvironments. After providing a formal correctness proof for the recursive\nquery evaluation with PreM under this relaxed synchronization model, we present\nexperimental evidence of its benefits. This paper is under consideration for\nacceptance in Theory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 07:35:18 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Das", "Ariyam", ""], ["Zaniolo", "Carlo", ""]]}, {"id": "1907.10662", "submitter": "Xuankang Lin", "authors": "Xuankang Lin, He Zhu, Roopsha Samanta, Suresh Jagannathan", "title": "ART: Abstraction Refinement-Guided Training for Provably Correct Neural\n  Networks", "comments": "FMCAD'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL cs.SC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Artificial Neural Networks (ANNs) have demonstrated remarkable utility in\nvarious challenging machine learning applications. While formally verified\nproperties of their behaviors are highly desired, they have proven notoriously\ndifficult to derive and enforce. Existing approaches typically formulate this\nproblem as a post facto analysis process. In this paper, we present a novel\nlearning framework that ensures such formal guarantees are enforced by\nconstruction. Our technique enables training provably correct networks with\nrespect to a broad class of safety properties, a capability that goes\nwell-beyond existing approaches, without compromising much accuracy. Our key\ninsight is that we can integrate an optimization-based abstraction refinement\nloop into the learning process and operate over dynamically constructed\npartitions of the input space that considers accuracy and safety objectives\nsynergistically. The refinement procedure iteratively splits the input space\nfrom which training data is drawn, guided by the efficacy with which such\npartitions enable safety verification. We have implemented our approach in a\ntool (ART) and applied it to enforce general safety properties on unmanned\naviator collision avoidance system ACAS Xu dataset and the Collision Detection\ndataset. Importantly, we empirically demonstrate that realizing safety does not\ncome at the price of much accuracy. Our methodology demonstrates that an\nabstraction refinement methodology provides a meaningful pathway for building\nboth accurate and correct machine learning networks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 16:58:33 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 02:42:17 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 21:49:51 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Lin", "Xuankang", ""], ["Zhu", "He", ""], ["Samanta", "Roopsha", ""], ["Jagannathan", "Suresh", ""]]}, {"id": "1907.10674", "submitter": "Danil Annenkov", "authors": "Danil Annenkov, Jakob Botsch Nielsen, Bas Spitters", "title": "ConCert: A Smart Contract Certification Framework in Coq", "comments": "Extended the related work section. Significantly extended sections on\n  translation and semantics. Added more examples and details about the\n  formalisation. Commented of unquote and the trusted computing base. Commented\n  on adequacy", "journal-ref": "CPP 2020: Proceedings of the 9th ACM SIGPLAN International\n  Conference on Certified Programs and Proofs, January 2020, Pages 215-228", "doi": "10.1145/3372885.3373829", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new way of embedding functional languages into the Coq proof\nassistant by using meta-programming. This allows us to develop the meta-theory\nof the language using the deep embedding and provides a convenient way for\nreasoning about concrete programs using the shallow embedding. We connect the\ndeep and the shallow embeddings by a soundness theorem. As an instance of our\napproach, we develop an embedding of a core smart contract language into Coq\nand verify several important properties of a crowdfunding contract based on a\nprevious formalisation of smart contract execution in blockchains.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 19:19:29 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 07:19:01 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2019 10:00:02 GMT"}, {"version": "v4", "created": "Fri, 20 Dec 2019 17:59:41 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Annenkov", "Danil", ""], ["Nielsen", "Jakob Botsch", ""], ["Spitters", "Bas", ""]]}, {"id": "1907.10699", "submitter": "Ravi Chugh", "authors": "Brian Hempel and Justin Lubin and Ravi Chugh", "title": "Sketch-n-Sketch: Output-Directed Programming for SVG", "comments": "UIST 2019 Paper + Appendix", "journal-ref": null, "doi": "10.1145/3332165.3347925", "report-no": null, "categories": "cs.HC cs.GR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For creative tasks, programmers face a choice: Use a GUI and sacrifice\nflexibility, or write code and sacrifice ergonomics?\n  To obtain both flexibility and ease of use, a number of systems have explored\na workflow that we call output-directed programming. In this paradigm, direct\nmanipulation of the program's graphical output corresponds to writing code in a\ngeneral-purpose programming language, and edits not possible with the mouse can\nstill be enacted through ordinary text edits to the program. Such capabilities\nprovide hope for integrating graphical user interfaces into what are currently\ntext-centric programming environments.\n  To further advance this vision, we present a variety of new output-directed\ntechniques that extend the expressive power of Sketch-n-Sketch, an\noutput-directed programming system for creating programs that generate vector\ngraphics. To enable output-directed interaction at more stages of program\nconstruction, we expose intermediate execution products for manipulation and we\npresent a mechanism for contextual drawing. Looking forward to output-directed\nprogramming beyond vector graphics, we also offer generic refactorings through\nthe GUI, and our techniques employ a domain-agnostic provenance tracing scheme.\n  To demonstrate the improved expressiveness, we implement a dozen new\nparametric designs in Sketch-n-Sketch without text-based edits. Among these is\nthe first demonstration of building a recursive function in an output-directed\nprogramming setting.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 20:16:48 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 02:10:58 GMT"}, {"version": "v3", "created": "Tue, 30 Jul 2019 02:20:28 GMT"}, {"version": "v4", "created": "Sat, 10 Aug 2019 21:07:31 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Hempel", "Brian", ""], ["Lubin", "Justin", ""], ["Chugh", "Ravi", ""]]}, {"id": "1907.10708", "submitter": "Justin Hsu", "authors": "Gilles Barthe, Justin Hsu, Kevin Liao", "title": "A Probabilistic Separation Logic", "comments": null, "journal-ref": null, "doi": "10.1145/3371123", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic independence is a useful concept for describing the result of\nrandom sampling---a basic operation in all probabilistic languages---and for\nreasoning about groups of random variables. Nevertheless, existing verification\nmethods handle independence poorly, if at all. We propose a probabilistic\nseparation logic PSL, where separation models probabilistic independence. We\nfirst give a new, probabilistic model of the logic of bunched implications\n(BI). We then build a program logic based on these assertions, and prove\nsoundness of the proof system. We demonstrate our logic by verifying\ninformation-theoretic security of cryptographic constructions for several\nwell-known tasks, including private information retrieval, oblivious transfer,\nsecure multi-party addition, and simple oblivious RAM. Our proofs reason purely\nin terms of high-level properties, like independence and uniformity.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 20:39:30 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 19:38:23 GMT"}, {"version": "v3", "created": "Fri, 22 Nov 2019 19:23:36 GMT"}, {"version": "v4", "created": "Fri, 17 Jul 2020 21:40:51 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Barthe", "Gilles", ""], ["Hsu", "Justin", ""], ["Liao", "Kevin", ""]]}, {"id": "1907.10914", "submitter": "Fernando S\\'aenz-P\\'erez", "authors": "Fernando S\\'aenz-P\\'erez", "title": "Applying Constraint Logic Programming to SQL Semantic Analysis", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": "Theory and Practice of Logic Programming 19 (2019) 808-825", "doi": "10.1017/S1471068419000206", "report-no": null, "categories": "cs.DB cs.AI cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the use of Constraint Logic Programming (CLP) to model\nSQL queries in a data-independent abstract layer by focusing on some semantic\nproperties for signalling possible errors in such queries. First, we define a\ntranslation from SQL to Datalog, and from Datalog to CLP, so that solving this\nCLP program will give information about inconsistency, tautology, and possible\nsimplifications. We use different constraint domains which are mapped to SQL\ntypes, and propose them to cooperate for improving accuracy. Our approach\nleverages a deductive system that includes SQL and Datalog, and we present an\nimplementation in this system which is currently being tested in classroom,\nshowing its advantages and differences with respect to other approaches, as\nwell as some performance data. This paper is under consideration for acceptance\nin TPLP.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 09:19:30 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["S\u00e1enz-P\u00e9rez", "Fernando", ""]]}, {"id": "1907.10919", "submitter": "Julia Sapi\\~na", "authors": "Mar\\'ia Alpuente, Demis Ballis, Santiago Escobar, Julia Sapi\\~na", "title": "Symbolic Analysis of Maude Theories with Narval", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concurrent functional languages that are endowed with symbolic reasoning\ncapabilities such as Maude offer a high-level, elegant, and efficient approach\nto programming and analyzing complex, highly nondeterministic software systems.\nMaude's symbolic capabilities are based on equational unification and narrowing\nin rewrite theories, and provide Maude with advanced logic programming\ncapabilities such as unification modulo user-definable equational theories and\nsymbolic reachability analysis in rewrite theories. Intricate computing\nproblems may be effectively and naturally solved in Maude thanks to the synergy\nof these recently developed symbolic capabilities and classical Maude features,\nsuch as: (i) rich type structures with sorts (types), subsorts, and\noverloading; (ii) equational rewriting modulo various combinations of axioms\nsuch as associativity, commutativity, and identity; and (iii) classical\nreachability analysis in rewrite theories. However, the combination of all of\nthese features may hinder the understanding of Maude symbolic computations for\nnon-experienced developers. The purpose of this article is to describe how\nprogramming and analysis of Maude rewrite theories can be made easier by\nproviding a sophisticated graphical tool called Narval that supports the\nfine-grained inspection of Maude symbolic computations. This paper is under\nconsideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 09:27:07 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Alpuente", "Mar\u00eda", ""], ["Ballis", "Demis", ""], ["Escobar", "Santiago", ""], ["Sapi\u00f1a", "Julia", ""]]}, {"id": "1907.10922", "submitter": "Pierre Talbot", "authors": "Pierre Talbot", "title": "Spacetime Programming: A Synchronous Language for Composable Search\n  Strategies", "comments": "Extended version (with appendices), with minor corrections, of a\n  paper presented at the 21st ACM International Symposium on Principles and\n  Practice of Declarative Programming (PPDP 2019), Porto, Portugal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search strategies are crucial to efficiently solve constraint satisfaction\nproblems. However, programming search strategies in the existing constraint\nsolvers is a daunting task and constraint-based languages usually have\ncompositionality issues. We propose spacetime programming, a paradigm extending\nthe synchronous language Esterel and timed concurrent constraint programming\nwith backtracking, for creating and composing search strategies. In this\nformalism, the search strategies are composed in the same way as we compose\nconcurrent processes. Our contributions include the design and behavioral\nsemantics of spacetime programming, and the proofs that spacetime programs are\ndeterministic, reactive and extensive functions. Moreover, spacetime\nprogramming provides a bridge between the theoretical foundations of\nconstraint-based concurrency and the practical aspects of constraint solving.\nWe developed a prototype of the compiler that produces search strategies with a\nsmall overhead compared to the hard-coded ones.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 09:30:03 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 15:12:14 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Talbot", "Pierre", ""]]}, {"id": "1907.11133", "submitter": "Lau Skorstengaard", "authors": "Lau Skorstengaard", "title": "An Introduction to Logical Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logical relations (LR) have been around for many years, and today they are\nused in many formal results. However, it can be difficult to LR beginners to\nfind a good place to start to learn. Papers often use highly specialized LRs\nthat use the latest advances of the technique which makes it impossible to make\na proper presentation within the page limit.\n  This note is a good starting point for beginners that want to learn about\nLRs. Almost no prerequisite knowledge is assumed, and the note starts from the\nvery basics. The note covers the following: LRs for proving normalization and\ntype safety of simply typed lambda calculus, relational substitutions for\nreasoning about universal and existential types, step-indexing for reasoning\nabout recursive types, and worlds for reasoning about references.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 15:17:27 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Skorstengaard", "Lau", ""]]}, {"id": "1907.11317", "submitter": "Dimitri Racordon", "authors": "Dimitri Racordon, Didier Buchs", "title": "Explicit and Controllable Assignment Semantics", "comments": "13 pages, 2 figures, MPLR'2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the plethora of powerful software to spot bugs, identify performance\nbottlenecks or simply improve the overall quality of code, programming\nlanguages remain the first and most important tool of a developer. Therefore,\nappropriate abstractions, unambiguous syntaxes and intuitive semantics are\nparamount to convey intent concisely and efficiently. The continuing growth in\ncomputing power has allowed modern compilers and runtime system to handle once\nthought unrealistic features, such as type inference and reflection, making\ncode simpler to write and clearer to read. Unfortunately, relics of the\nunderlying memory model still transpire in most programming languages, leading\nto confusing assignment semantics. This paper introduces Anzen, a programming\nlanguage that aims to make assignments easier to understand and manipulate. The\nlanguage offers three assignment operators, with unequivocal semantics, that\ncan reproduce all common imperative assignment strategies. It is accompanied by\na type system based on type capabilities to enforce uniqueness and\nimmutability. We present Anzen's features informally and formalize it by the\nmeans of a minimal calculus.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 21:56:17 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Racordon", "Dimitri", ""], ["Buchs", "Didier", ""]]}, {"id": "1907.11354", "submitter": "EPTCS", "authors": "Paul Tarau, Jan Wielemaker, Tom Schrijvers", "title": "Lazy Stream Programming in Prolog", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 224-237", "doi": "10.4204/EPTCS.306.26", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, stream processing has become a prominent approach for\nincrementally handling large amounts of data, with special support and\nlibraries in many programming languages. Unfortunately, support in Prolog has\nso far been lacking and most existing approaches are ad-hoc. To remedy this\nsituation, we present lazy stream generators as a unified Prolog interface for\nstateful computations on both finite and infinite sequences of data that are\nproduced incrementally through I/O and/or algorithmically.\n  We expose stream generators to the application programmer in two ways: 1)\nthrough an abstract sequence manipulation API, convenient for defining custom\ngenerators, and 2) as idiomatic lazy lists, compatible with many existing list\npredicates. We define an algebra of stream generator operations that extends\nProlog via an embedded language interpreter, provides a compact notation for\ncomposing generators and supports moving between the two isomorphic\nrepresentations.\n  As a special instance, we introduce answer stream generators that encapsulate\nthe work of coroutining first-class logic engines and support interoperation\nbetween forward recursive AND-streams and backtracking-generated OR-streams.\n  Keywords: lazy stream generators, lazy lists, first-class logic engines,\nstream combinators, AND-stream / OR-stream interoperation, Prolog extensions\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 01:24:45 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 13:31:37 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Tarau", "Paul", ""], ["Wielemaker", "Jan", ""], ["Schrijvers", "Tom", ""]]}, {"id": "1907.11817", "submitter": "Firas Alomari", "authors": "F Alomari, M Harbi", "title": "Scalable Source Code Similarity Detection in Large Code Repositories", "comments": "11 pages, 5 figures, Journal", "journal-ref": "EAI Endorsed Transactions on Scalable Information Systems: Online\n  first, 2019", "doi": "10.4108/eai.13-7-2018.159353", "report-no": null, "categories": "cs.SE cs.IR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Source code similarity are increasingly used in application development to\nidentify clones, isolate bugs, and find copy-rights violations. Similar code\nfragments can be very problematic due to the fact that errors in the original\ncode must be fixed in every copy. Other maintenance changes, such as extensions\nor patches, must be applied multiple times. Furthermore, the diversity of\ncoding styles and flexibility of modern languages makes it difficult and cost\nineffective to manually inspect large code repositories. Therefore, detection\nis only feasible by automatic techniques. We present an efficient and scalable\napproach for similar code fragment identification based on source code control\nflow graphs fingerprinting. The source code is processed to generate control\nflow graphs that are then hashed to create a unique fingerprint of the code\ncapturing semantics as well as syntax similarity. The fingerprints can then be\nefficiently stored and retrieved to perform similarity search between code\nfragments. Experimental results from our prototype implementation supports the\nvalidity of our approach and show its effectiveness and efficiency in\ncomparison with other solutions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 23:28:30 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Alomari", "F", ""], ["Harbi", "M", ""]]}, {"id": "1907.12345", "submitter": "Samir Genaim", "authors": "Jes\\'us J. Dom\\'enech and John P. Gallagher and Samir Genaim", "title": "Control-Flow Refinement by Partial Evaluation, and its Application to\n  Termination and Cost Analysis", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Control-flow refinement refers to program transformations whose purpose is to\nmake implicit control-flow explicit, and is used in the context of program\nanalysis to increase precision. Several techniques have been suggested for\ndifferent programming models, typically tailored to improving precision for a\nparticular analysis. In this paper we explore the use of partial evaluation of\nHorn clauses as a general-purpose technique for control-flow refinement for\ninteger transitions systems. These are control-flow graphs where edges are\nannotated with linear constraints describing transitions between corresponding\nnodes, and they are used in many program analysis tools. Using partial\nevaluation for control-flow refinement has the clear advantage over other\napproaches in that soundness follows from the general properties of partial\nevaluation; in particular, properties such as termination and complexity are\npreserved. We use a partial evaluation algorithm incorporating property-based\nabstraction, and show how the right choice of properties allows us to prove\ntermination and to infer complexity of challenging programs that cannot be\nhandled by state-of-the-art tools. We report on the integration of the\ntechnique in a termination analyzer, and its use as a preprocessing step for\nseveral cost analyzers. Under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 11:42:56 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 09:12:24 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Dom\u00e9nech", "Jes\u00fas J.", ""], ["Gallagher", "John P.", ""], ["Genaim", "Samir", ""]]}, {"id": "1907.13227", "submitter": "Paul Downen", "authors": "Paul Downen, Zena M. Ariola", "title": "Compiling With Classical Connectives", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 3 (August\n  28, 2020) lmcs:6740", "doi": "10.23638/LMCS-16(3:13)2020", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The study of polarity in computation has revealed that an \"ideal\" programming\nlanguage combines both call-by-value and call-by-name evaluation; the two\ncalling conventions are each ideal for half the types in a programming\nlanguage. But this binary choice leaves out call-by-need which is used in\npractice to implement lazy-by-default languages like Haskell. We show how the\nnotion of polarity can be extended beyond the value/name dichotomy to include\ncall-by-need by adding a mechanism for sharing which is enough to compile a\nHaskell-like functional language with user-defined types. The key to capturing\nsharing in this mixed-evaluation setting is to generalize the usual notion of\npolarity \"shifts:\" rather than just two shifts (between positive and negative)\nwe have a family of four dual shifts.\n  We expand on this idea of logical duality -- \"and\" is dual to \"or;\" proof is\ndual to refutation -- for the purpose of compiling a variety of types. Based on\na general notion of data and codata, we show how classical connectives can be\nused to encode a wide range of built-in and user-defined types. In contrast\nwith an intuitionistic logic corresponding to pure functional programming,\nthese classical connectives bring more of the pleasant symmetries of classical\nlogic to the computationally-relevant, constructive setting. In particular, an\ninvolutive pair of negations bridges the gulf between the wide-spread notions\nof parametric polymorphism and abstract data types in programming languages. To\ncomplete the study of duality in compilation, we also consider the dual to\ncall-by-need evaluation, which shares the computation within the control flow\nof a program instead of computation within the information flow.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 21:31:06 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 22:36:52 GMT"}, {"version": "v3", "created": "Wed, 26 Aug 2020 20:06:47 GMT"}, {"version": "v4", "created": "Wed, 9 Jun 2021 17:58:06 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Downen", "Paul", ""], ["Ariola", "Zena M.", ""]]}, {"id": "1907.13263", "submitter": "Pedro Lopez-Garcia", "authors": "Ignacio Casso, Jose F. Morales, Pedro Lopez-Garcia, Manuel V.\n  Hermenegildo", "title": "Computing Abstract Distances in Logic Programs", "comments": "21 pages, 8 figures; submitted to ICLP'19, accepted as technical\n  communication", "journal-ref": null, "doi": null, "report-no": "CLIP-2/2019.0", "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract interpretation is a well-established technique for performing static\nanalyses of logic programs. However, choosing the abstract domain, widening,\nfixpoint, etc. that provides the best precision-cost trade-off remains an open\nproblem. This is in a good part because of the challenges involved in measuring\nand comparing the precision of different analyses. We propose a new approach\nfor measuring such precision, based on defining distances in abstract domains\nand extending them to distances between whole analyses of a given program, thus\nallowing comparing precision across different analyses. We survey and extend\nexisting proposals for distances and metrics in lattices or abstract domains,\nand we propose metrics for some common domains used in logic program analysis,\nas well as extensions of those metrics to the space of whole program analysis.\nWe implement those metrics within the CiaoPP framework and apply them to\nmeasure the precision of different analyses over both benchmarks and a\nrealistic program.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 23:58:04 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Casso", "Ignacio", ""], ["Morales", "Jose F.", ""], ["Lopez-Garcia", "Pedro", ""], ["Hermenegildo", "Manuel V.", ""]]}, {"id": "1907.13272", "submitter": "Pedro Lopez-Garcia", "authors": "Maximiliano Klemen, Pedro Lopez-Garcia, John P. Gallagher, Jose F.\n  Morales, Manuel V. Hermenegildo", "title": "Towards a General Framework for Static Cost Analysis of Parallel Logic\n  Programs", "comments": "19 pages, 3 tables; submitted to ICLP'19, accepted as technical\n  communication", "journal-ref": null, "doi": null, "report-no": "CLIP-1/2019.0", "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation and control of resource usage is now an important challenge in\nan increasing number of computing systems. In particular, requirements on\ntiming and energy arise in a wide variety of applications such as internet of\nthings, cloud computing, health, transportation, and robots. At the same time,\nparallel computing, with (heterogeneous) multi-core platforms in particular,\nhas become the dominant paradigm in computer architecture. Predicting resource\nusage on such platforms poses a difficult challenge. Most work on static\nresource analysis has focused on sequential programs, and relatively little\nprogress has been made on the analysis of parallel programs, or more\nspecifically on parallel logic programs. We propose a novel, general, and\nflexible framework for setting up cost equations/relations which can be\ninstantiated for performing resource usage analysis of parallel logic programs\nfor a wide range of resources, platforms and execution models. The analysis\nestimates both lower and upper bounds on the resource usage of a parallel\nprogram (without executing it) as functions on input data sizes. In addition,\nit also infers other meaningful information to better exploit and assess the\npotential and actual parallelism of a system. We develop a method for solving\ncost relations involving the max function that arise in the analysis of\nparallel programs. Finally, we instantiate our general framework for the\nanalysis of logic programs with Independent And-Parallelism, report on an\nimplementation within the CiaoPP system, and provide some experimental results.\nTo our knowledge, this is the first approach to the cost analysis of parallel\nlogic programs.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 01:20:00 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Klemen", "Maximiliano", ""], ["Lopez-Garcia", "Pedro", ""], ["Gallagher", "John P.", ""], ["Morales", "Jose F.", ""], ["Hermenegildo", "Manuel V.", ""]]}, {"id": "1907.13288", "submitter": "Vasudevan Nagendra", "authors": "Vasudevan Nagendra, Arani Bhattacharya, Vinod Yegneswaran, Amir\n  Rahmati, Samir R Das", "title": "VISCR: Intuitive & Conflict-free Automation for Securing the Dynamic\n  Consumer IoT Infrastructures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.FL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumer IoT is characterized by heterogeneous devices with diverse\nfunctionality and programming interfaces. This lack of homogeneity makes the\nintegration and security management of IoT infrastructures a daunting task for\nusers and administrators. In this paper, we introduce VISCR, a\nVendor-Independent policy Specification and Conflict Resolution engine that\nenables conflict-free policy specification and enforcement in IoT environments.\nVISCR converts the topology of the IoT infrastructure into a tree-based\nabstraction and translates existing policies from heterogeneous vendor-specific\nprogramming languages such as Groovy-based SmartThings, OpenHAB, IFTTT-based\ntemplates, and MUD-based profiles into a vendor-independent graph-based\nspecification. Using the two, VISCR can automatically detect rouge policies,\nconflicts, and bugs for coherent automation. Upon detection, VISCR infers new\npolicies and proposes them to users as alternatives to existing policies for\nfine-tuning and conflict-free enforcement. We evaluated VISCR using a dataset\nof 907 IoT apps, programmed using heterogeneous automation specifications in a\nsimulated smart-building IoT infrastructure. In our experiments, among 907 IoT\napps, VISCR exposed 342 of IoT apps as exhibiting one or more violations. VISCR\ndetected 100% of violations reported by existing state-of-the-art tool, while\ndetecting new types of violations in an additional 266 apps. In terms of\nperformance, VISCR can generate 400 abstraction trees (used in specifying\npolicies) with 100K leaf nodes in <1.2sec. In our experiments, VISCR took 80.7\nseconds to analyze our infrastructure of 907 apps; a 14.2X reduction compared\nto the state-of-the-art. After the initial analysis, VISCR is capable of\nadopting new policies in sub-second latency to handle changes.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 02:31:06 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Nagendra", "Vasudevan", ""], ["Bhattacharya", "Arani", ""], ["Yegneswaran", "Vinod", ""], ["Rahmati", "Amir", ""], ["Das", "Samir R", ""]]}]