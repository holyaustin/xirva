[{"id": "2103.00031", "submitter": "Dominik Aumayr", "authors": "Dominik Aumayr, Stefan Marr, Sophie Kaleba, Elisa Gonzalez Boix,\n  Hanspeter M\\\"ossenb\\\"ock", "title": "Capturing High-level Nondeterminism in Concurrent Programs for Practical\n  Concurrency Model Agnostic Record & Replay", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2021, Vol. 5,\n  Issue 3, Article 14", "doi": "10.22152/programming-journal.org/2021/5/14", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With concurrency being integral to most software systems, developers combine\nhigh-level concurrency models in the same application to tackle each problem\nwith appropriate abstractions. While languages and libraries offer a wide range\nof concurrency models, debugging support for applications that combine them has\nnot yet gained much attention. Record & replay aids debugging by\ndeterministically reproducing recorded bugs, but is typically designed for a\nsingle concurrency model only. This paper proposes a practical\nconcurrency-model-agnostic record & replay approach for multi-paradigm\nconcurrent programs, i.e. applications that combine concurrency models. Our\napproach traces high-level nondeterministic events by using a uniform\nmodel-agnostic trace format and infrastructure. This enables orderingbased\nrecord & replay support for a wide range of concurrency models, and thereby\nenables debugging of applications that combine them. In addition, it allows\nlanguage implementors to add new concurrency models and reuse the\nmodel-agnostic record & replay support. We argue that a\nconcurrency-model-agnostic record & replay is practical and enables advanced\ndebugging support for a wide range of concurrency models. The evaluation shows\nthat our approach is expressive and flexible enough to support record & replay\nof applications using threads & locks, communicating event loops, communicating\nsequential processes, software transactional memory and combinations of those\nconcurrency models. For the actor model, we reach recording performance\ncompetitive with an optimized special-purpose record & replay solution. The\naverage recording overhead on the Savina actor benchmark suite is 10% (min. 0%,\nmax. 23%). The performance for other concurrency models and combinations\nthereof is at a similar level. We believe our concurrency-model-agnostic\napproach helps developers of applications that mix and match concurrency\nmodels. We hope that this substrate inspires new tools and languages making\nbuilding and maintaining of multi-paradigm concurrent applications simpler and\nsafer.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 19:48:15 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Aumayr", "Dominik", ""], ["Marr", "Stefan", ""], ["Kaleba", "Sophie", ""], ["Boix", "Elisa Gonzalez", ""], ["M\u00f6ssenb\u00f6ck", "Hanspeter", ""]]}, {"id": "2103.00032", "submitter": "Janice Chin", "authors": "Janice Chin, David Pearce", "title": "Finding Bugs with Specification-Based Testing is Easy!", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2021, Vol. 5,\n  Issue 3, Article 13", "doi": "10.22152/programming-journal.org/2021/5/13", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated specification-based testing has a long history with several notable\ntools having emerged. For example, QuickCheck for Haskell focuses on testing\nagainst user-provided properties. Others, such as JMLUnit, use specifications\nin the form of pre- and post-conditions to drive testing. An interesting (and\nunder-explored) question is how effective this approach is at finding bugs in\npractice. In general, one would assume automated testing is less effective at\nbug finding than static verification. But, how much less effective? To shed\nlight on this question, we consider automated testing of programs written in\nWhiley -- a language with first-class support for specifications. Whilst\noriginally designed with static verification in mind, we have anecdotally found\nautomated testing for Whiley surprisingly useful and cost-effective. For\nexample, when an error is detected with automated testing, a counterexample is\nalways provided. This has motivated the more rigorous empirical examination\npresented in this paper. To that end, we provide a technical discussion of the\nimplementation behind an automated testing tool for Whiley. Here, a key\nusability concern is the ability to parameterise the input space, and we\npresent novel approaches for references and lambdas. We then report on several\nlarge experiments investigating the tool's effectiveness at bug finding using a\nrange of benchmarks, including a suite of 1800+ mutants. The results indicate\nthe automated testing is effective in many cases, and that sampling offers\nuseful performance benefits with only modest reductions in bug-finding\ncapability. Finally, we report on some real-world uses of the tool where it has\nproved effective at finding bugs (such as in the standard library).\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 19:49:48 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Chin", "Janice", ""], ["Pearce", "David", ""]]}, {"id": "2103.00033", "submitter": "Sebastian Burckhardt", "authors": "Sebastian Burckhardt, Chris Gillum, David Justo, Konstantinos Kallas,\n  Connor McMahon, Christopher S. Meiklejohn", "title": "Serverless Workflows with Durable Functions and Netherite", "comments": "This paper was written in September 2020, and the content has not\n  been edited after October 10, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Serverless is an increasingly popular choice for service architects because\nit can provide elasticity and load-based billing with minimal developer effort.\nA common and important use case is to compose serverless functions and cloud\nstorage into reliable workflows. However, existing solutions for authoring\nworkflows provide a rudimentary experience compared to writing standard code in\na modern programming language. Furthermore, executing workflows reliably in an\nelastic serverless environment poses significant performance challenges.\n  To address these, we propose Durable Functions, a programming model for\nserverless workflows, and Netherite, a distributed execution engine to execute\nthem efficiently. Workflows in Durable Functions are expressed as task-parallel\ncode in a host language of choice. Internally, the workflows are translated to\nfine-grained stateful communicating processes, which are load-balanced over an\nelastic cluster. The main challenge is to minimize the cost of reliably\npersisting progress to storage while supporting elastic scale. Netherite solves\nthis by introducing partitioning, recovery logs, asynchronous snapshots, and\nspeculative communication.\n  Our results show that Durable Functions simplifies the expression of complex\nworkflows, and that Netherite achieves lower latency and higher throughput than\nthe prevailing approaches for serverless workflows in Azure and AWS, by orders\nof magnitude in some cases.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 19:51:58 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Burckhardt", "Sebastian", ""], ["Gillum", "Chris", ""], ["Justo", "David", ""], ["Kallas", "Konstantinos", ""], ["McMahon", "Connor", ""], ["Meiklejohn", "Christopher S.", ""]]}, {"id": "2103.00194", "submitter": "Kingshuk Majumder", "authors": "Kingshuk Majumder, Uday Bondhugula", "title": "HIR: An MLIR-based Intermediate Representation for Hardware Accelerator\n  Description", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The emergence of machine learning, image and audio processing on edge devices\nhas motivated research towards power efficient custom hardware accelerators.\nThough FPGAs are an ideal target for energy efficient custom accelerators, the\ndifficulty of hardware design and the lack of vendor agnostic, standardized\nhardware compilation infrastructure has hindered their adoption.\n  This paper introduces HIR, an MLIR-based intermediate representation (IR) to\ndescribe hardware accelerator designs. HIR combines high level language\nfeatures, such as loops and multi-dimensional tensors, with programmer defined\nexplicit scheduling, to provide a high-level IR suitable for DSL compiler\npipelines without compromising control over the micro-architecture of the\naccelerator. HIR's explicit schedules allow it to express fine-grained,\nsynchronization-free parallelism and optimizations such as retiming and\npipelining. Built as a dialect in MLIR, it draws from best IR practices learnt\nfrom communities like those of LLVM. While offering rich optimization\nopportunities and a high level abstraction, HIR enables sharing of\noptimizations, utilities and passes with software compiler infrastructure.\n  Our implementation shows that the code generation time of the HIR code\ngenerator is on average 1112x lower than that of Xilinx Vivado HLS on a range\nof kernels without a compromise on the quality of the generated hardware. We\nbelieve that these are significant steps forward in the design of IRs for\nhardware synthesis and in equipping domain-specific languages with a productive\nand performing compilation path to custom hardware acceleration.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 11:34:30 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Majumder", "Kingshuk", ""], ["Bondhugula", "Uday", ""]]}, {"id": "2103.00201", "submitter": "Giambattista Gruosso", "authors": "Giulia Crocioni, Giambattista Gruosso, Danilo Pau, Davide Denaro,\n  Luigi Zambrano, Giuseppe di Giore", "title": "Characterization of Neural Networks Automatically Mapped on\n  Automotive-grade Microcontrollers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.PL cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Nowadays, Neural Networks represent a major expectation for the realization\nof powerful Deep Learning algorithms, which can determine several physical\nsystems' behaviors and operations. Computational resources required for model,\ntraining, and running are large, especially when related to the amount of data\nthat Neural Networks typically need to generalize. The latest TinyML\ntechnologies allow integrating pre-trained models on embedded systems, allowing\nmaking computing at the edge faster, cheaper, and safer. Although these\ntechnologies originated in the consumer and industrial worlds, many sectors can\ngreatly benefit from them, such as the automotive industry. In this paper, we\npresent a framework for implementing Neural Network-based models on a family of\nautomotive Microcontrollers, showing their efficiency in two case studies\napplied to vehicles: intrusion detection on the Controller Area Network bus and\nresidual capacity estimation in Lithium-Ion batteries, widely used in Electric\nVehicles.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 12:16:50 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Crocioni", "Giulia", ""], ["Gruosso", "Giambattista", ""], ["Pau", "Danilo", ""], ["Denaro", "Davide", ""], ["Zambrano", "Luigi", ""], ["di Giore", "Giuseppe", ""]]}, {"id": "2103.00587", "submitter": "Vitalis Salis", "authors": "Vitalis Salis, Thodoris Sotiropoulos, Panos Louridas, Diomidis\n  Spinellis and Dimitris Mitropoulos", "title": "PyCG: Practical Call Graph Generation in Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Call graphs play an important role in different contexts, such as profiling\nand vulnerability propagation analysis. Generating call graphs in an efficient\nmanner can be a challenging task when it comes to high-level languages that are\nmodular and incorporate dynamic features and higher-order functions.\n  Despite the language's popularity, there have been very few tools aiming to\ngenerate call graphs for Python programs. Worse, these tools suffer from\nseveral effectiveness issues that limit their practicality in realistic\nprograms. We propose a pragmatic, static approach for call graph generation in\nPython. We compute all assignment relations between program identifiers of\nfunctions, variables, classes, and modules through an inter-procedural\nanalysis. Based on these assignment relations, we produce the resulting call\ngraph by resolving all calls to potentially invoked functions. Notably, the\nunderlying analysis is designed to be efficient and scalable, handling several\nPython features, such as modules, generators, function closures, and multiple\ninheritance.\n  We have evaluated our prototype implementation, which we call PyCG, using two\nbenchmarks: a micro-benchmark suite containing small Python programs and a set\nof macro-benchmarks with several popular real-world Python packages. Our\nresults indicate that PyCG can efficiently handle thousands of lines of code in\nless than a second (0.38 seconds for 1k LoC on average). Further, it\noutperforms the state-of-the-art for Python in both precision and recall: PyCG\nachieves high rates of precision ~99.2%, and adequate recall ~69.9%. Finally,\nwe demonstrate how PyCG can aid dependency impact analysis by showcasing a\npotential enhancement to GitHub's \"security advisory\" notification service\nusing a real-world example.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 18:49:25 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Salis", "Vitalis", ""], ["Sotiropoulos", "Thodoris", ""], ["Louridas", "Panos", ""], ["Spinellis", "Diomidis", ""], ["Mitropoulos", "Dimitris", ""]]}, {"id": "2103.00668", "submitter": "Sam Stites", "authors": "Sam Stites, Heiko Zimmermann, Hao Wu, Eli Sennesh, Jan-Willem van de\n  Meent", "title": "Learning Proposals for Probabilistic Programs with Inference Combinators", "comments": "Accepted to UAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop operators for construction of proposals in probabilistic programs,\nwhich we refer to as inference combinators. Inference combinators define a\ngrammar over importance samplers that compose primitive operations such as\napplication of a transition kernel and importance resampling. Proposals in\nthese samplers can be parameterized using neural networks, which in turn can be\ntrained by optimizing variational objectives. The result is a framework for\nuser-programmable variational methods that are correct by construction and can\nbe tailored to specific models. We demonstrate the flexibility of this\nframework by implementing advanced variational methods based on amortized Gibbs\nsampling and annealing.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 00:17:53 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 18:47:15 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 01:11:40 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Stites", "Sam", ""], ["Zimmermann", "Heiko", ""], ["Wu", "Hao", ""], ["Sennesh", "Eli", ""], ["van de Meent", "Jan-Willem", ""]]}, {"id": "2103.01046", "submitter": "Anil Shukla", "authors": "Anish Mallick, Anil Shukla", "title": "Extending Prolog for Quantified Boolean Horn Formulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prolog is a well known declarative programming language based on\npropositional Horn formulas. It is useful in various areas, including\nartificial intelligence, automated theorem proving, mathematical logic and so\non. An active research area for many years is to extend Prolog to larger\nclasses of logic. Some important extensions of it includes the constraint logic\nprogramming, and the object oriented logic programming. However, it cannot\nsolve problems having arbitrary quantified Horn formulas.\n  To be precise, the facts, rules and queries in Prolog are not allowed to have\narbitrary quantified variables. The paper overcomes this major limitations of\nProlog by extending it for the quantified Boolean Horn formulas. We achieved\nthis by extending the SLD-resolution proof system for quantified Boolean Horn\nformulas, followed by proposing an efficient model for implementation. The\npaper shows that the proposed implementation also supports the first-order\npredicate Horn logic with arbitrary quantified variables.\n  The paper also introduces for the first time, a declarative programming for\nthe quantified Boolean Horn formulas.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 14:39:56 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Mallick", "Anish", ""], ["Shukla", "Anil", ""]]}, {"id": "2103.01346", "submitter": "Pengyu Nie", "authors": "Pengyu Nie, Karl Palmskog, Junyi Jessy Li, Milos Gligoric", "title": "Roosterize: Suggesting Lemma Names for Coq Verification Projects Using\n  Deep Learning", "comments": "Accepted in International Conference on Software Engineering,\n  Demonstrations Track (ICSE-DEMO 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naming conventions are an important concern in large verification projects\nusing proof assistants, such as Coq. In particular, lemma names are used by\nproof engineers to effectively understand and modify Coq code. However,\nproviding accurate and informative lemma names is a complex task, which is\ncurrently often carried out manually. Even when lemma naming is automated using\nrule-based tools, generated names may fail to adhere to important conventions\nnot specified explicitly. We demonstrate a toolchain, dubbed Roosterize, which\nautomatically suggests lemma names in Coq projects. Roosterize leverages a\nneural network model trained on existing Coq code, thus avoiding manual\nspecification of naming conventions. To allow proof engineers to conveniently\naccess suggestions from Roosterize during Coq project development, we\nintegrated the toolchain into the popular Visual Studio Code editor. Our\nevaluation shows that Roosterize substantially outperforms strong baselines for\nsuggesting lemma names and is useful in practice. The demo video for Roosterize\ncan be viewed at: https://youtu.be/HZ5ac7Q14rc.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 23:07:44 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 04:26:53 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Nie", "Pengyu", ""], ["Palmskog", "Karl", ""], ["Li", "Junyi Jessy", ""], ["Gligoric", "Milos", ""]]}, {"id": "2103.01553", "submitter": "Sanjana Singh", "authors": "Sanjana Singh, Divyanjali Sharma and Subodh Sharma", "title": "Dynamic Verification of C/C++11 Concurrency over Multi Copy Atomics", "comments": "12 pages of main paper + 2 5 pages of appendix and 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the problem of runtime analysis of C11 programs under\nMulti-Copy-Atomic semantics (MCA). Under MCA, one can analyze program outcomes\nsolely through interleaving and reordering of thread events. As a result,\nobtaining intuitive explanations of program outcomes becomes straightforward.\nNewer versions of ARM (ARMv8 and later), Alpha, and Intel's x-86 support MCA.\nOur tests reveal that state-of-the-art dynamic verification techniques that\nanalyze program executions under the C11 memory model generate safety property\nviolations that can be interpreted as false alarms under MCA semantics. Sorting\nthe true from false violations puts an undesirable burden on the user. In this\nwork, we provide a dynamic verification technique (MoCA) to analyze C11 program\nexecutions which are permitted under the MCA model. We design a happens-before\nrelation and introduce coherence rules to capture precisely those C11 program\nexecutions which are allowed under the MCA model. MoCA's exploration of the\nstate-space is based on the state-of-the-art dynamic verification algorithm,\nsource-DPOR. Our experiments validate that MoCA captures all coherent C11\nprogram executions, and is precise for the MCA model.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 08:12:10 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 16:47:08 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Singh", "Sanjana", ""], ["Sharma", "Divyanjali", ""], ["Sharma", "Subodh", ""]]}, {"id": "2103.01911", "submitter": "W{\\l}odzimierz Drabent", "authors": "W{\\l}odzimierz Drabent", "title": "SLD-resolution without occur-check, an example", "comments": "9 pages. This version: small corrections, mainly p.7", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the occur-check is not needed for a certain definite clause\nlogic program, independently from the selection rule. First we prove that the\nprogram is occur-check free. Then we consider a more general class of queries,\nunder which the program is not occur-check free; however we show that it will\nbe correctly executed under Prolog without occur-check.\n  The main result of this report states that the occur-check may be skipped for\nthe cases in which a single run of a standard nondeterministic unification\nalgorithm does not fail due to the occur-check. The usual approaches are based\non the notion of NSTO (not subject to occur-check), which considers all the\nruns. To formulate the result, it was necessary to introduce an abstraction of\na \"unification\" algorithm without the occur-check.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:09:09 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 17:27:10 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Drabent", "W\u0142odzimierz", ""]]}, {"id": "2103.02171", "submitter": "Sandip Ghosal", "authors": "Sandip Ghosal and R. K. Shyamasundar", "title": "An Axiomatic Approach to Detect Information Leaks in Concurrent Programs", "comments": "5 pages, 2 figures; accepted paper for the 43rd International\n  Conference on Software Engineering (ICSE 2021), Track: New Ideas and Emerging\n  Results (NIER)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Realizing flow security in a concurrent environment is extremely challenging,\nprimarily due to non-deterministic nature of execution. The difficulty is\nfurther exacerbated from a security angle if sequential threads disclose\ncontrol locations through publicly observable statements like print, sleep,\ndelay, etc. Such observations lead to internal and external timing attacks.\nInspired by previous works that use classical Hoare style proof systems for\nestablishing correctness of distributed (real-time) programs, in this paper, we\ndescribe a method for finding information leaks in concurrent programs through\nthe introduction of leaky assertions at observable program points. Specifying\nleaky assertions akin to classic assertions, we demonstrate how information\nleaks can be detected in a concurrent context. To our knowledge, this is the\nfirst such work that enables integration of different notions of\nnon-interference used in functional and security context. While the approach is\nsound and relatively complete in the classic sense, it enables the use of\nalgorithmic techniques that enable programmers to come up with leaky assertions\nthat enable checking for information leaks in sensitive applications.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 04:34:23 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Ghosal", "Sandip", ""], ["Shyamasundar", "R. K.", ""]]}, {"id": "2103.02177", "submitter": "Michael Greenberg", "authors": "Niki Vazou and Michael Greenberg", "title": "Functional Extensionality for Refinement Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Refinement type checkers are a powerful way to reason about functional\nprograms. For example, one can prove properties of a slow, specification\nimplementation, porting the proofs to an optimized implementation that behaves\nthe same. Without functional extensionality, proofs must relate functions that\nare fully applied. When data itself has a higher-order representation, fully\napplied proofs face serious impediments! When working with first-order data,\nfully applied proofs lead to noisome duplication when using higher-order\nfunctions.\n  While dependent type theories are typically consistent with functional\nextensionality axioms, refinement type systems with semantic subtyping treat\nnaive phrasings of functional extensionality inconsistently, leading to\nunsoundness. We demonstrate this unsoundness and develop a new approach to\nequality in Liquid Haskell: we define a propositional equality in a library we\ncall PEq. Using PEq avoids the unsoundness while still proving useful\nequalities at higher types; we demonstrate its use in several case studies. We\nvalidate PEq by building a small model and developing its metatheory.\nAdditionally, we prove metaproperties of PEq inside Liquid Haskell itself using\nan unnamed folklore technique, which we dub `classy induction'.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 04:50:53 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Vazou", "Niki", ""], ["Greenberg", "Michael", ""]]}, {"id": "2103.02209", "submitter": "Xinyuan Sun", "authors": "Shaokai Lin, Xinyuan Sun, Jianan Yao, Ronghui Gu", "title": "SciviK: A Versatile Framework for Specifying and Verifying Smart\n  Contracts", "comments": "22 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing adoption of smart contracts on blockchains poses new security\nrisks that can lead to significant monetary loss, while existing approaches\neither provide no (or partial) security guarantees for smart contracts or\nrequire huge proof effort. To address this challenge, we present SciviK, a\nversatile framework for specifying and verifying industrial-grade smart\ncontracts. SciviK's versatile approach extends previous efforts with three key\ncontributions: (i) an expressive annotation system enabling built-in directives\nfor vulnerability pattern checking, neural-based loop invariant inference, and\nthe verification of rich properties of real-world smart contracts (ii) a\nfine-grained model for the Ethereum Virtual Machine (EVM) that provides\nlow-level execution semantics, (iii) an IR-level verification framework\nintegrating both SMT solvers and the Coq proof assistant.\n  We use SciviK to specify and verify security properties for 12 benchmark\ncontracts and a real-world Decentralized Finance (DeFi) smart contract. Among\nall 158 specified security properties (in six types), 151 properties can be\nautomatically verified within 2 seconds, five properties can be automatically\nverified after moderate modifications, and two properties are manually proved\nwith around 200 lines of Coq code.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 06:45:05 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Lin", "Shaokai", ""], ["Sun", "Xinyuan", ""], ["Yao", "Jianan", ""], ["Gu", "Ronghui", ""]]}, {"id": "2103.02830", "submitter": "Akash Lal", "authors": "Ranadeep Biswas, Diptanshu Kakwani, Jyothi Vedurada, Constantin Enea,\n  Akash Lal", "title": "MonkeyDB: Effectively Testing Correctness against Weak Isolation Levels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern applications, such as social networking systems and e-commerce\nplatforms are centered around using large-scale storage systems for storing and\nretrieving data. In the presence of concurrent accesses, these storage systems\ntrade off isolation for performance. The weaker the isolation level, the more\nbehaviors a storage system is allowed to exhibit and it is up to the developer\nto ensure that their application can tolerate those behaviors. However, these\nweak behaviors only occur rarely in practice, that too outside the control of\nthe application, making it difficult for developers to test the robustness of\ntheir code against weak isolation levels.\n  This paper presents MonkeyDB, a mock storage system for testing\nstorage-backed applications. MonkeyDB supports a Key-Value interface as well as\nSQL queries under multiple isolation levels. It uses a logical specification of\nthe isolation level to compute, on a read operation, the set of all possible\nreturn values. MonkeyDB then returns a value randomly from this set. We show\nthat MonkeyDB provides good coverage of weak behaviors, which is complete in\nthe limit. We test a variety of applications for assertions that fail only\nunder weak isolation. MonkeyDB is able to break each of those assertions in a\nsmall number of attempts.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 04:35:35 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Biswas", "Ranadeep", ""], ["Kakwani", "Diptanshu", ""], ["Vedurada", "Jyothi", ""], ["Enea", "Constantin", ""], ["Lal", "Akash", ""]]}, {"id": "2103.02976", "submitter": "Nikita Zyuzin", "authors": "Nikita Zyuzin, Aleksandar Nanevski", "title": "Contextual Modal Types for Algebraic Effects and Handlers", "comments": "34 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Programming languages with algebraic effects often rely on effect annotations\nto track algebraic operations in a computation. This is particularly important\nin the presence of effect handlers, where handling of an effect may remove it\nfrom the annotation in the computation's type.\n  In this paper we view the algebraic effect theory of a computation as a\nvariable context, and track the effects directly in computation's modal type\nwith ECMTT, a novel calculus for algebraic effects and handlers. Our calculus\nsupports computations with effects and derives from Contextual Modal Type\nTheory (CMTT), from which it inherits a number of important logical properties.\n  In contrast to type-and-effect systems, where effects are attached as\nannotations to a prior computational language, ECMMT tracks effects by a\ncontextualized variant of the modal $\\Box$ (necessity) operator of the\nintuitionistic modal logic of CMTT. In programming, the effect annotations are\nexplicitly managed by the dedicated term constructors corresponding to the\nlogical introduction and elimination forms for the modality. The modal\nfoundation leads to customary logical properties of local soundness and\ncompleteness, and determines the operational semantics of ECMTT directly by\n$\\beta$-reduction. In addition, effect handlers become a generalization of\nexplicit substitutions, which in CMTT serve to reach one context from another.\n  To the best of our knowledge, ECMTT is the first system to relate algebraic\neffects to modal types. We also see it as a step towards providing a\ncorrespondence in the style of Curry and Howard that may transfer a number of\nresults from the fields of modal logic and modal type theory to that of\nalgebraic effects.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 11:50:25 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Zyuzin", "Nikita", ""], ["Nanevski", "Aleksandar", ""]]}, {"id": "2103.03116", "submitter": "Linfeng Liu", "authors": "Linfeng Liu, Hoan Nguyen, George Karypis, Srinivasan Sengamedu", "title": "Universal Representation for Code", "comments": "PAKDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning from source code usually requires a large amount of labeled data.\nDespite the possible scarcity of labeled data, the trained model is highly\ntask-specific and lacks transferability to different tasks. In this work, we\npresent effective pre-training strategies on top of a novel graph-based code\nrepresentation, to produce universal representations for code. Specifically,\nour graph-based representation captures important semantics between code\nelements (e.g., control flow and data flow). We pre-train graph neural networks\non the representation to extract universal code properties. The pre-trained\nmodel then enables the possibility of fine-tuning to support various downstream\napplications. We evaluate our model on two real-world datasets -- spanning over\n30M Java methods and 770K Python methods. Through visualization, we reveal\ndiscriminative properties in our universal code representation. By comparing\nmultiple benchmarks, we demonstrate that the proposed framework achieves\nstate-of-the-art results on method name prediction and code graph link\nprediction.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 15:39:25 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Liu", "Linfeng", ""], ["Nguyen", "Hoan", ""], ["Karypis", "George", ""], ["Sengamedu", "Srinivasan", ""]]}, {"id": "2103.03198", "submitter": "Denis Merigoux", "authors": "Denis Merigoux, Nicolas Chataing, Jonathan Protzenko", "title": "Catala: A Programming Language for the Law", "comments": null, "journal-ref": null, "doi": "10.1145/3473582", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Law at large underpins modern society, codifying and governing many aspects\nof citizens' daily lives. Oftentimes, law is subject to interpretation, debate\nand challenges throughout various courts and jurisdictions. But in some other\nareas, law leaves little room for interpretation, and essentially aims to\nrigorously describe a computation, a decision procedure or, simply said, an\nalgorithm. Unfortunately, prose remains a woefully inadequate tool for the job.\nThe lack of formalism leaves room for ambiguities; the structure of legal\nstatutes, with many paragraphs and sub-sections spread across multiple pages,\nmakes it hard to compute the intended outcome of the algorithm underlying a\ngiven text; and, as with any other piece of poorly-specified critical software,\nthe use of informal language leaves corner cases unaddressed. We introduce\nCatala, a new programming language that we specifically designed to allow a\nstraightforward and systematic translation of statutory law into an executable\nimplementation. Catala aims to bring together lawyers and programmers through a\nshared medium, which together they can understand, edit and evolve, bridging a\ngap that often results in dramatically incorrect implementations of the law. We\nhave implemented a compiler for Catala, and have proven the correctness of its\ncore compilation steps using the F* proof assistant. We evaluate Catala on\nseveral legal texts that are algorithms in disguise, notably section 121 of the\nUS federal income tax and the byzantine French family benefits; in doing so, we\nuncover a bug in the official implementation. We observe as a consequence of\nthe formalization process that using Catala enables rich interactions between\nlawyers and programmers, leading to a greater understanding of the original\nlegislative intent, while producing a correct-by-construction executable\nspecification reusable by the greater software ecosystem.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 18:03:15 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 22:06:50 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Merigoux", "Denis", ""], ["Chataing", "Nicolas", ""], ["Protzenko", "Jonathan", ""]]}, {"id": "2103.03309", "submitter": "Anthony Savidis", "authors": "Anthony Savidis", "title": "Translating declarative control elements to imperative using 'l-value\n  redefinition graphs'", "comments": "15 pages, 13 figures, includes code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We focus on control constructs that allow programmers define actions to be\nperformed when respective conditions are met without requiring the explicit\nevaluation and testing of conditions as part of an imperative algorithm. Such\nelements are commonly referred as declarative, not theoretically related to\ndeclarative languages. We introduce declarative constructs in the C++ language,\npresenting the translation method to standard C++. The innovative feature of\nour method is the accommodation of l-values involving arbitrary pointer / array\nexpressions and objects, supporting immediate runtime evaluation upon content\nupdate even if such l-values bind to variant storage locations at runtime. To\naccomplish this we define 'l-value redefinition graphs', capturing storage\nbinding dependencies among variables, being the floor-plan of our code\ngeneration and runtime management approach.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 20:25:54 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Savidis", "Anthony", ""]]}, {"id": "2103.03809", "submitter": "Xuezixiang Li", "authors": "Xuezixiang Li, Qu Yu, Heng Yin", "title": "PalmTree: Learning an Assembly Language Model for Instruction Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has demonstrated its strengths in numerous binary analysis\ntasks, including function boundary detection, binary code search, function\nprototype inference, value set analysis, etc. When applying deep learning to\nbinary analysis tasks, we need to decide what input should be fed into the\nneural network model. More specifically, we need to answer how to represent an\ninstruction in a fixed-length vector. The idea of automatically learning\ninstruction representations is intriguing, however the existing schemes fail to\ncapture the unique characteristics of disassembly. These schemes ignore the\ncomplex intra-instruction structures and mainly rely on control flow in which\nthe contextual information is noisy and can be influenced by compiler\noptimizations.\n  In this paper, we propose to pre-train an assembly language model called\nPalmTree for generating general-purpose instruction embeddings by conducting\nself-supervised training on large-scale unlabeled binary corpora. PalmTree\nutilizes three pre-training tasks to capture various characteristics of\nassembly language. These training tasks overcome the problems in existing\nschemes, thus can help to generate high-quality representations. We conduct\nboth intrinsic and extrinsic evaluations, and compare PalmTree with other\ninstruction embedding schemes. PalmTree has the best performance for intrinsic\nmetrics, and outperforms the other instruction embedding schemes for all\ndownstream tasks.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 22:30:01 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 19:48:48 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Li", "Xuezixiang", ""], ["Yu", "Qu", ""], ["Yin", "Heng", ""]]}, {"id": "2103.03908", "submitter": "Miroslav Stankovic", "authors": "Ezio Bartocci, Laura Kovacs, Miroslav Stankovic", "title": "MORA -- Automatic Generation of Moment-Based Invariants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce MORA, an automated tool for generating invariants of\nprobabilistic programs. Inputs to MORA are so-called Prob-solvable loops, that\nis probabilistic programs with polynomial assignments over random variables and\nparametrized distributions. Combining methods from symbolic computation and\nstatistics, MORA computes invariant properties over higher-order moments of\nloop variables, expressing, for example, statistical properties, such as\nexpected values and variances, over the value distribution of loop variables.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 19:36:10 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Bartocci", "Ezio", ""], ["Kovacs", "Laura", ""], ["Stankovic", "Miroslav", ""]]}, {"id": "2103.04188", "submitter": "Qinheping Hu", "authors": "Qinheping Hu, John Cyphert, Loris D'Antoni, Thomas Reps", "title": "Synthesis with Asymptotic Resource Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for synthesizing recursive functions that satisfy both a\nfunctional specification and an asymptotic resource bound. Prior methods for\nsynthesis with a resource metric require the user to specify a concrete\nexpression exactly describing resource usage, whereas our method uses big-O\nnotation to specify the asymptotic resource usage. Our method can synthesize\nprograms with complex resource bounds, such as a sort function that has\ncomplexity O(nlog(n)). Our synthesis procedure uses a type system that is able\nto assign an asymptotic complexity to terms, and can track recurrence relations\nof functions. These typing rules are justified by theorems used in analysis of\nalgorithms, such as the Master Theorem and the Akra-Bazzi method. We\nimplemented our method as an extension of prior type-based synthesis work. Our\ntool, SynPlexity, was able to synthesize complex divide-and-conquer programs\nthat cannot be synthesized by prior solvers.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 20:03:50 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 05:16:44 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Hu", "Qinheping", ""], ["Cyphert", "John", ""], ["D'Antoni", "Loris", ""], ["Reps", "Thomas", ""]]}, {"id": "2103.04388", "submitter": "Rohan Padhye", "authors": "Vasudev Vikram, Rohan Padhye, Koushik Sen", "title": "Growing a Test Corpus with Bonsai Fuzzing", "comments": "Accepted at the 43rd International Conference on Software Engineering\n  (ICSE 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a coverage-guided grammar-based fuzzing technique for\nautomatically generating a corpus of concise test inputs for programs such as\ncompilers. We walk-through a case study of a compiler designed for education\nand the corresponding problem of generating meaningful test cases to provide to\nstudents. The prior state-of-the-art solution is a combination of fuzzing and\ntest-case reduction techniques such as variants of delta-debugging. Our key\ninsight is that instead of attempting to minimize convoluted fuzzer-generated\ntest inputs, we can instead grow concise test inputs by construction using a\nform of iterative deepening. We call this approach Bonsai Fuzzing. Experimental\nresults show that Bonsai Fuzzing can generate test corpora having inputs that\nare 16--45% smaller in size on average as compared to a fuzz-then-reduce\napproach, while achieving approximately the same code coverage and\nfault-detection capability.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 16:13:17 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Vikram", "Vasudev", ""], ["Padhye", "Rohan", ""], ["Sen", "Koushik", ""]]}, {"id": "2103.04648", "submitter": "Mohammad Reza Besharati", "authors": "Mohammad Reza Besharati, Mohammad Izadi", "title": "Langar: An Approach to Evaluate Reo Programming Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reo is a formal coordination language. In order to assess and evaluate its\ncapabilities, we need a multi-perspective Language Evaluation Framework. Langar\n(Language Analysis for Reo) is a framework aimed to provide such an evaluation\nmethod. In this paper, we introduce Langar. Based on a review on various\nlanguage evaluation methods, a tool-kit for useful evaluation techniques are\nprovided. After Reo Evaluation, this method and tool-kit also could be used for\nanother programming, computational and even natural languages. Furthermore, two\nsuggestions for some future efforts and directions are provided for software\nengineering and software methodology communities.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 10:16:00 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Besharati", "Mohammad Reza", ""], ["Izadi", "Mohammad", ""]]}, {"id": "2103.04880", "submitter": "Jarrett Holtz", "authors": "Jarrett Holtz, Simon Andrews, Arjun Guha, Joydeep Biswas", "title": "Iterative Program Synthesis for Adaptable Social Navigation", "comments": "In submission to IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot social navigation is influenced by human preferences and\nenvironment-specific scenarios such as elevators and doors, thus necessitating\nend-user adaptability. State-of-the-art approaches to social navigation fall\ninto two categories: model-based social constraints and learning-based\napproaches. While effective, these approaches have fundamental limitations --\nmodel-based approaches require constraint and parameter tuning to adapt to\npreferences and new scenarios, while learning-based approaches require reward\nfunctions, significant training data, and are hard to adapt to new social\nscenarios or new domains with limited demonstrations. In this work, we propose\nIterative Dimension Informed Program Synthesis (IDIPS) to address these\nlimitations by learning and adapting social navigation in the form of\nhuman-readable symbolic programs. IDIPS works by combining program synthesis,\nparameter optimization, predicate repair, and iterative human demonstration to\nlearn and adapt model-free action selection policies from orders of magnitude\nless data than learning-based approaches. We introduce a novel predicate repair\ntechnique that can accommodate previously unseen social scenarios or\npreferences by growing existing policies. We present experimental results\nshowing that IDIPS: 1) synthesizes effective policies that model user\npreference, 2) can adapt existing policies to changing preferences, 3) can\nextend policies to handle novel social scenarios such as locked doors, and 4)\ngenerates policies that can be transferred from simulation to real-world robots\nwith minimal effort.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 16:35:15 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Holtz", "Jarrett", ""], ["Andrews", "Simon", ""], ["Guha", "Arjun", ""], ["Biswas", "Joydeep", ""]]}, {"id": "2103.05030", "submitter": "Shivam Handa", "authors": "Shivam Handa and Martin Rinard", "title": "Program Synthesis Over Noisy Data with Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore and formalize the task of synthesizing programs over noisy data,\ni.e., data that may contain corrupted input-output examples. By formalizing the\nconcept of a Noise Source, an Input Source, and a prior distribution over\nprograms, we formalize the probabilistic process which constructs a noisy\ndataset. This formalism allows us to define the correctness of a synthesis\nalgorithm, in terms of its ability to synthesize the hidden underlying program.\nThe probability of a synthesis algorithm being correct depends upon the match\nbetween the Noise Source and the Loss Function used in the synthesis\nalgorithm's optimization process. We formalize the concept of an optimal Loss\nFunction given prior information about the Noise Source. We provide a technique\nto design optimal Loss Functions given perfect and imperfect information about\nthe Noise Sources. We also formalize the concept and conditions required for\nconvergence, i.e., conditions under which the probability that the synthesis\nalgorithm produces a correct program increases as the size of the noisy data\nset increases. This paper presents the first formalization of the concept of\noptimal Loss Functions, the first closed form definition of optimal Loss\nFunctions, and the first conditions that ensure that a noisy synthesis\nalgorithm will have convergence guarantees.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 19:39:13 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 23:06:22 GMT"}, {"version": "v3", "created": "Sat, 20 Mar 2021 15:25:51 GMT"}, {"version": "v4", "created": "Tue, 27 Apr 2021 18:17:19 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Handa", "Shivam", ""], ["Rinard", "Martin", ""]]}, {"id": "2103.05241", "submitter": "Dorra Ben Khalifa", "authors": "Assal\\'e Adj\\'e, Dorra Ben Khalifa and Matthieu Martel", "title": "Fast and Efficient Bit-Level Precision Tuning", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we introduce a new technique for precision tuning. This\nproblem consists of finding the least data types for numerical values such that\nthe result of the computation satisfies some accuracy requirement. State of the\nart techniques for precision tuning use a try and fail approach. They change\nthe data types of some variables of the program and evaluate the accuracy of\nthe result. Depending on what is obtained, they change more or less data types\nand repeat the process. Our technique is radically different. Based on semantic\nequations, we generate an Integer Linear Problem (ILP) from the program source\ncode. Basically, this is done by reasoning on the most significant bit and the\nnumber of significant bits of the values which are integer quantities. The\ninteger solution to this problem, computed in polynomial time by a (real)\nlinear programming solver, gives the optimal data types at the bit level. A\nfiner set of semantic equations is also proposed which does not reduce directly\nto an ILP problem. So we use policy iteration to find the solution. Both\ntechniques have been implemented and we show that our results encompass the\nresults of state of the art tools.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 06:12:41 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Adj\u00e9", "Assal\u00e9", ""], ["Khalifa", "Dorra Ben", ""], ["Martel", "Matthieu", ""]]}, {"id": "2103.05573", "submitter": "Kiarash Rahmani", "authors": "Kia Rahmani, Kartik Nagar, Benjamin Delaware, Suresh Jagannathan", "title": "Repairing Serializability Bugs in Distributed Database Programs via\n  Automated Schema Refactoring", "comments": "20 pages, to appear in 42nd ACM SIGPLAN Conference on Programming\n  Language Design and Implementation (PLDI 2021)", "journal-ref": null, "doi": "10.1145/3453483.3454028", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Serializability is a well-understood concurrency control mechanism that eases\nreasoning about highly-concurrent database programs. Unfortunately, enforcing\nserializability has a high-performance cost, especially on geographically\ndistributed database clusters. Consequently, many databases allow programmers\nto choose when a transaction must be executed under serializability, with the\nexpectation that transactions would only be so marked when necessary to avoid\nserious concurrency bugs. However, this is a significant burden to impose on\ndevelopers, requiring them to (a) reason about subtle concurrent interactions\namong potentially interfering transactions, (b) determine when such\ninteractions would violate desired invariants, and (c) then identify the\nminimum number of transactions whose executions should be serialized to prevent\nthese violations. To mitigate this burden, in this paper we present a sound and\nfully automated schema refactoring procedure that transforms a program's data\nlayout -- rather than its concurrency control logic -- to eliminate statically\nidentified concurrency bugs, allowing more transactions to be safely executed\nunder weaker and more performant database guarantees. Experimental results over\na range of database benchmarks indicate that our approach is highly effective\nin eliminating concurrency bugs, with safe refactored programs showing an\naverage of 120% higher throughput and 45% lower latency compared to the\nbaselines.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 17:25:58 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Rahmani", "Kia", ""], ["Nagar", "Kartik", ""], ["Delaware", "Benjamin", ""], ["Jagannathan", "Suresh", ""]]}, {"id": "2103.05779", "submitter": "Jayaraj Poroor", "authors": "Jayaraj Poroor", "title": "Natural Hoare Logic: Towards formal verification of programs from\n  logical forms of natural language specifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Formal verification provides strong guarantees of correctness of software,\nwhich are especially important in safety or security critical systems. Hoare\nlogic is a widely used formalism for rigorous verification of software against\nspecifications in the form of pre-condition/post-condition assertions. The\nadvancement of semantic parsing techniques and higher computational\ncapabilities enable us to extract semantic content from natural language text\nas formal logical forms, with increasing accuracy and coverage.\n  This paper proposes a formal framework for Hoare logic-based formal\nverification of imperative programs using logical forms generated from\ncompositional semantic parsing of natural language assertions. We call our\nreasoning approach Natural Hoare Logic. This enables formal verification of\nsoftware directly against safety requirements specified by a domain expert in\nnatural language.\n  We consider both declarative assertions of program invariants and state\nchange as well as imperative assertions that specify commands which alter the\nprogram state. We discuss how the reasoning approach can be extended using\ndomain knowledge and a practical approach for guarding against semantic parser\nerrors.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 17:36:55 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Poroor", "Jayaraj", ""]]}, {"id": "2103.06039", "submitter": "Sandip Ghosal", "authors": "Sandip Ghosal and R. K. Shyamasundar", "title": "Pifthon: A Compile-Time Information Flow Analyzer For An Imperative\n  Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compile-time information flow analysis has been a promising technique for\nprotecting confidentiality and integrity of private data. In the last couple of\ndecades, a large number of information flow security tools in the form of\nrun-time execution-monitors or static type systems have been developed for\nprogramming languages to analyze information flow security policies. However,\nexisting flow analysis tools lack in precision and usability, which is the\nprimary reason behind not being widely adopted in real application development.\nIn this paper, we propose a compile-time information flow analysis for an\nimperative program based on a hybrid (mutable + immutable) labelling approach\nthat enables a user to detect information flow-policy breaches and modify the\nprogram to overcome violations. We have developed an information flow security\nanalyzer for a dialect of Python language, PyX, called Pifthon using the said\napproach. The flow-analyzer aids in identifying possible misuse of the\ninformation in sequential PyX programs corresponding to a given information\nflow policy (IFP). Pifthon has distinct advantages like reduced labelling\noverhead that ameliorates usability, covers a wide range of PyX programs that\ninclude termination-and progress-sensitive channels, in contrast to other\napproaches in the literature. The proposed flow analysis is proved to be sound\nunder the classical non-interference property. Further, case study and\nexperience in the usage of Pifthon are provided.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 13:19:27 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Ghosal", "Sandip", ""], ["Shyamasundar", "R. K.", ""]]}, {"id": "2103.06127", "submitter": "Arnaud Spiwack", "authors": "Jean-Philippe Bernardy, Richard Eisenberg, Csongor Kiss, Arnaud\n  Spiwack, Nicolas Wu", "title": "Linear Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A linear argument must be consumed exactly once in the body of its function.\nA linear type system can verify the correct usage of resources such as file\nhandles and manually managed memory. But this verification requires\nbureaucracy. This paper presents linear constraints, a front-end feature for\nlinear typing that decreases the bureaucracy of working with linear types.\nLinear constraints are implicit linear arguments that are to be filled in\nautomatically by the compiler. Linear constraints are presented as a qualified\ntype system, together with an inference algorithm which extends OutsideIn,\nGHC's existing constraint solver algorithm. Soundness of linear constraints is\nensured by the fact that they desugar into Linear Haskell.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 15:28:14 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Bernardy", "Jean-Philippe", ""], ["Eisenberg", "Richard", ""], ["Kiss", "Csongor", ""], ["Spiwack", "Arnaud", ""], ["Wu", "Nicolas", ""]]}, {"id": "2103.06195", "submitter": "Arnaud Spiwack", "authors": "Jean-Philippe Bernardy, Arnaud Spiwack", "title": "Evaluating Linear Functions to Symmetric Monoidal Categories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A number of domain specific languages, such as circuits or data-science\nworkflows, are best expressed as diagrams of boxes connected by wires.\nUnfortunately, functional languages have traditionally been ill-equipped to\nembed this sort of languages. The Arrow abstraction is an approximation, but we\nargue that it does not capture the right properties.\n  A faithful abstraction is Symmetric Monoidal Categories (SMCs), but,so far,it\nhasn't been convenient to use. We show how the advent of linear typing in\nHaskell lets us bridge this gap. We provide a library which lets us program in\nSMCs with linear functions instead of SMC combinators. This considerably lowers\nthe syntactic overhead of the EDSL to be on par with that of monadic DSLs. A\nremarkable feature of our library is that, contrary to previously known methods\nfor categories, it does not use any metaprogramming.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 17:16:32 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 13:58:49 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Bernardy", "Jean-Philippe", ""], ["Spiwack", "Arnaud", ""]]}, {"id": "2103.06333", "submitter": "Wasi Ahmad", "authors": "Wasi Uddin Ahmad and Saikat Chakraborty and Baishakhi Ray and Kai-Wei\n  Chang", "title": "Unified Pre-training for Program Understanding and Generation", "comments": "NAACL 2021 (camera ready)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code summarization and generation empower conversion between programming\nlanguage (PL) and natural language (NL), while code translation avails the\nmigration of legacy code from one PL to another. This paper introduces PLBART,\na sequence-to-sequence model capable of performing a broad spectrum of program\nand language understanding and generation tasks. PLBART is pre-trained on an\nextensive collection of Java and Python functions and associated NL text via\ndenoising autoencoding. Experiments on code summarization in the English\nlanguage, code generation, and code translation in seven programming languages\nshow that PLBART outperforms or rivals state-of-the-art models. Moreover,\nexperiments on discriminative tasks, e.g., program repair, clone detection, and\nvulnerable code detection, demonstrate PLBART's effectiveness in program\nunderstanding. Furthermore, analysis reveals that PLBART learns program syntax,\nstyle (e.g., identifier naming convention), logical flow (e.g., if block inside\nan else block is equivalent to else if block) that are crucial to program\nsemantics and thus excels even with limited annotations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 20:32:59 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 19:48:33 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ahmad", "Wasi Uddin", ""], ["Chakraborty", "Saikat", ""], ["Ray", "Baishakhi", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2103.06376", "submitter": "Amir Shaikhha", "authors": "Amir Shaikhha, Mathieu Huot, Jaclyn Smith, Dan Olteanu", "title": "Functional Collection Programming with Semi-Ring Dictionaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces semi-ring dictionaries, a powerful class of\ncompositional and purely functional collections that subsume other collection\ntypes such as sets, multisets, arrays, vectors, and matrices. We develop SDQL,\na statically typed language centered around semi-ring dictionaries, that can\nencode expressions in relational algebra with aggregations, functional\ncollections, and linear algebra. Furthermore, thanks to the semi-ring algebraic\nstructures behind these dictionaries, SDQL unifies a wide range of\noptimizations commonly used in databases and linear algebra. As a result, SDQL\nenables efficient processing of hybrid database and linear algebra workloads,\nby putting together optimizations that are otherwise confined to either\ndatabase systems or linear algebra frameworks. Through experimental results, we\nshow that a handful of relational and linear algebra workloads can take\nadvantage of the SDQL language and optimizations. Overall, we observe that SDQL\nachieves competitive performance to Typer and Tectorwise, which are\nstate-of-the-art in-memory systems for (flat, not nested) relational data, and\nachieves an average 2x speedup over SciPy for linear algebra workloads.\nFinally, for hybrid workloads involving linear algebra processing over nested\nbiomedical data, SDQL can give up to one order of magnitude speedup over\nTrance, a state-of-the-art nested relational engine.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 22:54:13 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Shaikhha", "Amir", ""], ["Huot", "Mathieu", ""], ["Smith", "Jaclyn", ""], ["Olteanu", "Dan", ""]]}, {"id": "2103.06757", "submitter": "Nicol\\'as Cardozo", "authors": "Nicol\\'as Cardozo and Ivana Dusparic", "title": "Auto-COP: Adaptation Generation in Context-Oriented Programming using\n  Reinforcement Learning Options", "comments": "Submitted to The Art, Science, and Engineering of Programming\n  Journal. 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.SE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Self-adaptive software systems continuously adapt in response to internal and\nexternal changes in their execution environment, captured as contexts. The COP\nparadigm posits a technique for the development of self-adaptive systems,\ncapturing their main characteristics with specialized programming language\nconstructs. COP adaptations are specified as independent modules composed in\nand out of the base system as contexts are activated and deactivated in\nresponse to sensed circumstances from the surrounding environment. However, the\ndefinition of adaptations, their contexts and associated specialized behavior,\nneed to be specified at design time. In complex CPS this is intractable due to\nnew unpredicted operating conditions. We propose Auto-COP, a new technique to\nenable generation of adaptations at run time. Auto-COP uses RL options to build\naction sequences, based on the previous instances of the system execution.\nOptions are explored in interaction with the environment, and the most suitable\noptions for each context are used to generate adaptations exploiting COP. To\nvalidate Auto-COP, we present two case studies exhibiting different system\ncharacteristics and application domains: a driving assistant and a robot\ndelivery system. We present examples of Auto-COP code generated at run time, to\nillustrate the types of circumstances (contexts) requiring adaptation, and the\ncorresponding generated adaptations for each context. We confirm that the\ngenerated adaptations exhibit correct system behavior measured by\ndomain-specific performance metrics, while reducing the number of required\nexecution/actuation steps by a factor of two showing that the adaptations are\nregularly selected by the running system as adaptive behavior is more\nappropriate than the execution of primitive actions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 16:14:56 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Cardozo", "Nicol\u00e1s", ""], ["Dusparic", "Ivana", ""]]}, {"id": "2103.06913", "submitter": "Paul Downen", "authors": "Paul Downen and Zena M. Ariola", "title": "Classical (Co)Recursion: Programming", "comments": "34 pages, 7 figures. Submitted to Journal of Functional Programming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our aim here is to illustrate how the benefits of structural corecursion can\nbe found in a broader swath of the programming landscape than previously\nthought. Beginning from a tutorial on structural corecursion in the total, pure\nfunctional language Agda, we show how these same ideas are mapped to familiar\nconcepts in a variety of different languages. We show how corecursion can be\ndone in strict functional languages like Scheme, and even escapes the\nfunctional paradigm entirely, showing up in the natural expression of common\nobject-oriented features found in languages like Python and Java. Opening up\nstructural corecursion to a much wider selection of languages and paradigms --\nand therefore, also to a much larger audience of programmers -- lets us also\nask how corecursion interacts with computational effects. Of note, we\ndemonstrate that combining structural corecursion with effects can increase its\nexpressive power. We show a classical version of corecursion -- using\nfirst-class control made possible by Scheme's classical call/cc -- that enables\nus to write some new stream-processing algorithms that aren't possible in\neffect-free languages.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 19:13:42 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Downen", "Paul", ""], ["Ariola", "Zena M.", ""]]}, {"id": "2103.07543", "submitter": "Yao Li", "authors": "Yao Li, Li-yao Xia, Stephanie Weirich", "title": "Reasoning about the garden of forking paths", "comments": "28 pages, accepted by ICFP'21", "journal-ref": null, "doi": "10.1145/3473585", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lazy evaluation is a powerful tool for functional programmers. It enables the\nconcise expression of on-demand computation and a form of compositionality not\navailable under other evaluation strategies. However, the stateful nature of\nlazy evaluation makes it hard to analyze a program's computational cost, either\ninformally or formally. In this work, we present a novel and simple framework\nfor formally reasoning about lazy computation costs based on a recent model of\nlazy evaluation: clairvoyant call-by-value. The key feature of our framework is\nits simplicity, as expressed by our definition of the clairvoyance monad. This\nmonad is both simple to define (around 20 lines of Coq) and simple to reason\nabout. We show that this monad can be effectively used to mechanically reason\nabout the computational cost of lazy functional programs written in Coq.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 21:42:36 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 01:06:44 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Li", "Yao", ""], ["Xia", "Li-yao", ""], ["Weirich", "Stephanie", ""]]}, {"id": "2103.08117", "submitter": "Hammad Ahmad", "authors": "Hammad Ahmad and Jean-Baptiste Jeannin", "title": "A Program Logic to Verify Signal Temporal Logic Specifications of Hybrid\n  Systems: Extended Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Signal temporal logic (STL) was introduced for monitoring temporal properties\nof continuous-time signals for continuous and hybrid systems. Differential\ndynamic logic (dL) was introduced to reason about the end states of a hybrid\nprogram. Over the past decade, STL and its variants have significantly gained\nin popularity in the industry for monitoring purposes, while dL has gained in\npopularity for verification of hybrid systems. In this paper, we bridge the gap\nbetween the two different logics by introducing signal temporal dynamic logic\n(STdL) -- a dynamic logic that reasons about a subset of STL specifications\nover executions of hybrid systems. Our work demonstrates that STL can be used\nfor deductive verification of hybrid systems. STdL significantly augments the\nexpressiveness of dL by allowing reasoning about temporal properties in given\ntime intervals. We provide a semantics and a proof calculus for STdL, along\nwith a proof of soundness and relative completeness.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 03:27:46 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 00:34:56 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Ahmad", "Hammad", ""], ["Jeannin", "Jean-Baptiste", ""]]}, {"id": "2103.08521", "submitter": "Paul Downen", "authors": "Paul Downen and Zena M. Ariola", "title": "Classical (Co)Recursion: Mechanics", "comments": "54 pages, 9 figures. Submitted to Journal of Functional Programming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Primitive recursion is a mature, well-understood topic in the theory and\npractice of programming. Yet its dual, primitive corecursion, is\nunderappreciated and still seen as exotic. We aim to put them both on equal\nfooting by giving a foundation for primitive corecursion based on computation,\ngiving a terminating calculus analogous to the original computational\nfoundation of recursion. We show how the implementation details in an abstract\nmachine strengthens their connection, syntactically deriving corecursion from\nrecursion via logical duality. We also observe the impact of evaluation\nstrategy on the computational complexity of primitive (co)recursive\ncombinators: call-by-name allows for more efficient recursion, but\ncall-by-value allows for more efficient corecursion.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 16:46:09 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Downen", "Paul", ""], ["Ariola", "Zena M.", ""]]}, {"id": "2103.08577", "submitter": "Ethan Cecchetti", "authors": "Ethan Cecchetti, Siqiu Yao, Haobin Ni, Andrew C. Myers", "title": "Compositional Security for Reentrant Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The disastrous vulnerabilities in smart contracts sharply remind us of our\nignorance: we do not know how to write code that is secure in composition with\nmalicious code. Information flow control has long been proposed as a way to\nachieve compositional security, offering strong guarantees even when combining\nsoftware from different trust domains. Unfortunately, this appealing story\nbreaks down in the presence of reentrancy attacks. We formalize a general\ndefinition of reentrancy and introduce a security condition that allows\nsoftware modules like smart contracts to protect their key invariants while\nretaining the expressive power of safe forms of reentrancy. We present a\nsecurity type system that provably enforces secure information flow; in\nconjunction with run-time mechanisms, it enforces secure reentrancy even in the\npresence of unknown code; and it helps locate and correct recent high-profile\nvulnerabilities.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 17:45:04 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 21:06:41 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Cecchetti", "Ethan", ""], ["Yao", "Siqiu", ""], ["Ni", "Haobin", ""], ["Myers", "Andrew C.", ""]]}, {"id": "2103.08805", "submitter": "Chike Abuah", "authors": "Chike Abuah, Alex Silence, David Darais, Joe Near", "title": "DDUO: General-Purpose Dynamic Analysis for Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Differential privacy enables general statistical analysis of data with formal\nguarantees of privacy protection at the individual level. Tools that assist\ndata analysts with utilizing differential privacy have frequently taken the\nform of programming languages and libraries. However, many existing programming\nlanguages designed for compositional verification of differential privacy\nimpose significant burden on the programmer (in the form of complex type\nannotations). Supplementary library support for privacy analysis built on top\nof existing general-purpose languages has been more usable, but incapable of\npervasive end-to-end enforcement of sensitivity analysis and privacy\ncomposition. We introduce DDUO, a dynamic analysis for enforcing differential\nprivacy. DDUO is usable by non-experts: its analysis is automatic and it\nrequires no additional type annotations. DDUO can be implemented as a library\nfor existing programming languages; we present a reference implementation in\nPython which features moderate runtime overheads on realistic workloads. We\ninclude support for several data types, distance metrics and operations which\nare commonly used in modern machine learning programs. We also provide initial\nsupport for tracking the sensitivity of data transformations in popular Python\nlibraries for data analysis. We formalize the novel core of the DDUO system and\nprove it sound for sensitivity analysis via a logical relation for metric\npreservation. We also illustrate DDUO's usability and flexibility through\nvarious case studies which implement state-of-the-art machine learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 02:11:43 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Abuah", "Chike", ""], ["Silence", "Alex", ""], ["Darais", "David", ""], ["Near", "Joe", ""]]}, {"id": "2103.09069", "submitter": "Jianjun Zhao", "authors": "Pengzhan Zhao, Jianjun Zhao and Lei Ma", "title": "Identifying Bug Patterns in Quantum Programs", "comments": "The short version of this paper will appear in the Second\n  International Workshop on Quantum Software Engineering (Q-SE 2021) co-located\n  with ICSE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bug patterns are erroneous code idioms or bad coding practices that have been\nproved to fail time and time again, which are usually caused by the\nmisunderstanding of a programming language's features, the use of erroneous\ndesign patterns, or simple mistakes sharing common behaviors. This paper\nidentifies and categorizes some bug patterns in the quantum programming\nlanguage Qiskit and briefly discusses how to eliminate or prevent those bug\npatterns. We take this research as the first step to provide an underlying\nbasis for debugging and testing quantum programs.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 13:43:45 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Zhao", "Pengzhan", ""], ["Zhao", "Jianjun", ""], ["Ma", "Lei", ""]]}, {"id": "2103.09414", "submitter": "Naoki Kobayashi", "authors": "Naoki Kobayashi, Taro Sekiyama, Issei Sato and Hiroshi Unno", "title": "Toward Neural-Network-Guided Program Synthesis and Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework of program and invariant synthesis called neural\nnetwork-guided synthesis. We first show that, by suitably designing and\ntraining neural networks, we can extract logical formulas over integers from\nthe weights and biases of the trained neural networks. Based on the idea, we\nhave implemented a tool to synthesize formulas from positive/negative examples\nand implication constraints, and obtained promising experimental results. We\nalso discuss two applications of our synthesis method. One is the use of our\ntool for qualifier discovery in the framework of ICE-learning-based CHC\nsolving, which can in turn be applied to program verification and inductive\ninvariant synthesis. Another application is to a new program development\nframework called oracle-based programming, which is a neural-network-guided\nvariation of Solar-Lezama's program synthesis by sketching.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 03:09:05 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Kobayashi", "Naoki", ""], ["Sekiyama", "Taro", ""], ["Sato", "Issei", ""], ["Unno", "Hiroshi", ""]]}, {"id": "2103.09518", "submitter": "Marco Peressotti", "authors": "Fabrizio Montesi, Marco Peressotti, Valentino Picotti", "title": "Sliceable Monolith: Monolith First, Microservices Later", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Sliceable Monolith, a new methodology for developing microservice\narchitectures and perform their integration testing by leveraging most of the\nsimplicity of a monolith: a single codebase and a local execution environment\nthat simulates distribution. Then, a tool compiles a codebase for each\nmicroservice and a cloud deployment configuration. The key enabler of our\napproach is the technology-agnostic service definition language offered by\nJolie.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 09:03:18 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 15:22:04 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Montesi", "Fabrizio", ""], ["Peressotti", "Marco", ""], ["Picotti", "Valentino", ""]]}, {"id": "2103.10164", "submitter": "Mark Leake", "authors": "Jack W Shepherd, Ed J Higgins, Adam J M Wollman, Mark C Leake", "title": "PySTACHIO: Python Single-molecule TrAcking stoiCHiometry Intensity and\n  simulatiOn, a flexible, extensible, beginner-friendly and optimized program\n  for analysis of single-molecule microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.PL physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As camera pixel arrays have grown larger and faster, and optical microscopy\ntechniques ever more refined, there has been an explosion in the quantity of\ndata acquired during routine light microcopy. At the single-molecule level,\nanalysis involves multiple steps and can rapidly become computationally\nexpensive, in some cases intractable on office workstations. Complex bespoke\nsoftware can present high activation barriers to entry for new users. Here, we\nredevelop our quantitative single-molecule analysis routines into an optimized\nand extensible Python program, with GUI and command-line implementations to\nfacilitate use on local machines and remote clusters, by beginners and advanced\nusers alike. We demonstrate that its performance is on par with previous MATLAB\nimplementations but runs an order of magnitude faster. We tested it against\nchallenge data and demonstrate its performance is comparable to\nstate-of-the-art analysis platforms. We show the code can extract fluorescence\nintensity values for single reporter dye molecules and, using these, estimate\nmolecular stoichiometries and cellular copy numbers of fluorescently-labeled\nbiomolecules. It can evaluate 2D diffusion coefficients for the\ncharacteristically short single-particle tracking data. To facilitate\nbenchmarking we include data simulation routines to compare different analysis\nprograms. Finally, we show that it works with 2-color data and enables\ncolocalization analysis based on overlap integration, to infer interactions\nbetween differently labelled biomolecules. By making this freely available we\naim to make complex light microscopy single-molecule analysis more\ndemocratized.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 10:59:55 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 09:35:27 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 20:52:02 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Shepherd", "Jack W", ""], ["Higgins", "Ed J", ""], ["Wollman", "Adam J M", ""], ["Leake", "Mark C", ""]]}, {"id": "2103.10269", "submitter": "Francisco Ferreira Ruiz", "authors": "David Castro-Perez, Francisco Ferreira, Lorenzo Gheri, and Nobuko\n  Yoshida", "title": "Zooid: a DSL for Certified Multiparty Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We design and implement Zooid, a domain specific language for certified\nmultiparty communication, embedded in Coq and implemented atop our\nmechanisation framework of asynchronous multiparty session types (the first of\nits kind). Zooid provides a fully mechanised metatheory for the semantics of\nglobal and local types, and a fully verified end-point process language that\nfaithfully reflects the type-level behaviours and thus inherits the global\ntypes properties such as deadlock freedom, protocol compliance, and liveness\nguarantees.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 14:05:47 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Castro-Perez", "David", ""], ["Ferreira", "Francisco", ""], ["Gheri", "Lorenzo", ""], ["Yoshida", "Nobuko", ""]]}, {"id": "2103.11751", "submitter": "Hiromi Ishii", "authors": "Hiromi Ishii", "title": "Functional Pearl: Witness Me -- Constructive Arguments Must Be Guided\n  with Concrete Witness", "comments": "Submitted to Haskell'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Beloved Curry--Howard correspondence tells that types are intuitionistic\npropositions, and in constructive math, a proof of proposition can be seen as\nsome kind of a construction, or witness, conveying the information of the\nproposition. We demonstrate how useful this point of view is as the guiding\nprinciple for developing dependently-typed programs.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 12:08:45 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Ishii", "Hiromi", ""]]}, {"id": "2103.11882", "submitter": "Shashank Srikant", "authors": "Shashank Srikant, Sijia Liu, Tamara Mitrovska, Shiyu Chang, Quanfu\n  Fan, Gaoyuan Zhang, Una-May O'Reilly", "title": "Generating Adversarial Computer Programs using Optimized Obfuscations", "comments": "This work will be published at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning (ML) models that learn and predict properties of computer\nprograms are increasingly being adopted and deployed. These models have\ndemonstrated success in applications such as auto-completing code, summarizing\nlarge programs, and detecting bugs and malware in programs. In this work, we\ninvestigate principled ways to adversarially perturb a computer program to fool\nsuch learned models, and thus determine their adversarial robustness. We use\nprogram obfuscations, which have conventionally been used to avoid attempts at\nreverse engineering programs, as adversarial perturbations. These perturbations\nmodify programs in ways that do not alter their functionality but can be\ncrafted to deceive an ML model when making a decision. We provide a general\nformulation for an adversarial program that allows applying multiple\nobfuscation transformations to a program in any language. We develop\nfirst-order optimization algorithms to efficiently determine two key aspects --\nwhich parts of the program to transform, and what transformations to use. We\nshow that it is important to optimize both these aspects to generate the best\nadversarially perturbed program. Due to the discrete nature of this problem, we\nalso propose using randomized smoothing to improve the attack loss landscape to\nease optimization. We evaluate our work on Python and Java programs on the\nproblem of program summarization. We show that our best attack proposal\nachieves a $52\\%$ improvement over a state-of-the-art attack generation\napproach for programs trained on a seq2seq model. We further show that our\nformulation is better at training models that are robust to adversarial\nattacks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 10:47:15 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Srikant", "Shashank", ""], ["Liu", "Sijia", ""], ["Mitrovska", "Tamara", ""], ["Chang", "Shiyu", ""], ["Fan", "Quanfu", ""], ["Zhang", "Gaoyuan", ""], ["O'Reilly", "Una-May", ""]]}, {"id": "2103.11930", "submitter": "Andrew Willis", "authors": "Andrew Willis, Prashant Ganesh, Kyle Volle, Jincheng Zhang, Kevin\n  Brink", "title": "Volumetric Procedural Models for Shape Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a volumetric approach for procedural shape modeling\nand a new Procedural Shape Modeling Language (PSML) that facilitates the\nspecification of these models. PSML provides programmers the ability to\ndescribe shapes in terms of their 3D elements where each element may be a\nsemantic group of 3D objects, e.g., a brick wall, or an indivisible object,\ne.g., an individual brick. Modeling shapes in this manner facilitates the\ncreation of models that more closely approximate the organization and structure\nof their real-world counterparts. As such, users may query these models for\nvolumetric information such as the number, position, orientation and volume of\n3D elements which cannot be provided using surface based model-building\ntechniques. PSML also provides a number of new language-specific capabilities\nthat allow for a rich variety of context-sensitive behaviors and\npost-processing functions. These capabilities include an object-oriented\napproach for model design, methods for querying the model for component-based\ninformation and the ability to access model elements and components to perform\nBoolean operations on the model parts. PSML is open-source and includes freely\navailable tutorial videos, demonstration code and an integrated development\nenvironment to support writing PSML programs.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 15:17:33 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Willis", "Andrew", ""], ["Ganesh", "Prashant", ""], ["Volle", "Kyle", ""], ["Zhang", "Jincheng", ""], ["Brink", "Kevin", ""]]}, {"id": "2103.12357", "submitter": "Xiaolei Ren", "authors": "Xiaolei Ren, Michael Ho, Jiang Ming, Yu Lei, Li Li", "title": "Unleashing the Hidden Power of Compiler Optimization on Binary Code\n  Difference: An Empirical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since compiler optimization is the most common source contributing to binary\ncode differences in syntax, testing the resilience against the changes caused\nby different compiler optimization settings has become a standard evaluation\nstep for most binary diffing approaches. For example, 47 top-venue papers in\nthe last 12 years compared different program versions compiled by default\noptimization levels (e.g., -Ox in GCC and LLVM). Although many of them claim\nthey are immune to compiler transformations, it is yet unclear about their\nresistance to non-default optimization settings. Especially, we have observed\nthat adversaries explored non-default compiler settings to amplify malware\ndifferences.\n  This paper takes the first step to systematically studying the effectiveness\nof compiler optimization on binary code differences. We tailor search-based\niterative compilation for the auto-tuning of binary code differences. We\ndevelop BinTuner to search near-optimal optimization sequences that can\nmaximize the amount of binary code differences. We run BinTuner with GCC 10.2\nand LLVM 11.0 on SPEC benchmarks (CPU2006 & CPU2017), Coreutils, and OpenSSL.\nOur experiments show that at the cost of 279 to 1,881 compilation iterations,\nBinTuner can find custom optimization sequences that are substantially better\nthan the general -Ox settings. BinTuner's outputs seriously undermine prominent\nbinary diffing tools' comparisons. In addition, the detection rate of the IoT\nmalware variants tuned by BinTuner falls by more than 50%. Our findings paint a\ncautionary tale for security analysts that attackers have a new way to mutate\nmalware code cost-effectively, and the research community needs to step back to\nreassess optimization-resistance evaluations.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 07:34:30 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 15:59:38 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Ren", "Xiaolei", ""], ["Ho", "Michael", ""], ["Ming", "Jiang", ""], ["Lei", "Yu", ""], ["Li", "Li", ""]]}, {"id": "2103.12862", "submitter": "Melissa Antonelli", "authors": "Melissa Antonelli, Ugo Dal Lago, Paolo Pistone", "title": "On Counting Propositional Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study counting propositional logic as an extension of propositional logic\nwith counting quantifiers. We prove that the complexity of the underlying\ndecision problem perfectly matches the appropriate level of Wagner's counting\nhierarchy, but also that the resulting logic admits a satisfactory\nproof-theoretical treatment. From the latter, a type system for a probabilistic\nlambda-calculus is derived in the spirit of the Curry-Howard correspondence,\nshowing the potential of counting propositional logic as a useful tool in\nseveral fields of theoretical computer science.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 21:49:16 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 13:15:50 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Antonelli", "Melissa", ""], ["Lago", "Ugo Dal", ""], ["Pistone", "Paolo", ""]]}, {"id": "2103.13390", "submitter": "Rongxiao Fu", "authors": "Rongxiao Fu, Xueying Qin, Ornela Dardha, Michel Steuwer", "title": "Row-Polymorphic Types for Strategic Rewriting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a type system for strategy languages that express program\ntransformations as compositions of rewrite rules. Our row-polymorphic type\nsystem assists compiler engineers to write correct strategies by statically\nrejecting non meaningful compositions of rewrites that otherwise would fail\nduring rewriting at runtime. Furthermore, our type system enables reasoning\nabout how rewriting transforms the shape of the computational program. We\npresent a formalization of our language at its type system and demonstrate its\npractical use for expressing compiler optimization strategies. Our type system\nbuilds the foundation for many interesting future applications, including\nverifying the correctness of program transformations and synthesizing program\ntransformations from specifications encoded as types.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 18:00:47 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Fu", "Rongxiao", ""], ["Qin", "Xueying", ""], ["Dardha", "Ornela", ""], ["Steuwer", "Michel", ""]]}, {"id": "2103.13921", "submitter": "Itai Segall", "authors": "Martin Carroll, Kedar S. Namjoshi, Itai Segall", "title": "The Resh Programming Language for Multirobot Orchestration", "comments": "Accepted for publication at ICRA'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.RO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes Resh, a new, statically typed, interpreted programming\nlanguage and associated runtime for orchestrating multirobot systems. The main\nfeatures of Resh are: (1) It offloads much of the tedious work of programming\nsuch systems away from the programmer and into the language runtime; (2) It is\nbased on a small set of temporal and locational operators; and (3) It is not\nrestricted to specific robot types or tasks. The Resh runtime consists of three\nengines that collaborate to run a Resh program using the available robots in\ntheir current environment. This paper describes both Resh and its runtime and\ngives examples of its use.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 15:32:16 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Carroll", "Martin", ""], ["Namjoshi", "Kedar S.", ""], ["Segall", "Itai", ""]]}, {"id": "2103.14466", "submitter": "Wen Kokke", "authors": "Wen Kokke and Ornela Dardha", "title": "Prioritise the Best Variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Binary session types guarantee communication safety and session fidelity, but\nalone they cannot rule out deadlocks arising from the interleaving of different\nsessions. In Classical Processes (CP)$-$a process calculus based on classical\nlinear logic$-$deadlock freedom is guaranteed by combining channel creation and\nparallel composition under the same logical cut rule. Similarly, in Good\nVariation (GV)$-$a linear concurrent $\\lambda$-calculus$-$deadlock freedom is\nguaranteed by combining channel creation and thread spawning under the same\noperation, called fork.\n  In both CP and GV, deadlock freedom is achieved at the expense of\nexpressivity, as the only processes allowed are tree-structured. Dardha and Gay\ndefine Priority CP (PCP), which allows cyclic-structured processes and restores\ndeadlock freedom by using priorities, in line with Kobayashi and Padovani.\n  Following PCP, we present Priority GV (PGV), a variant of GV which decouples\nchannel creation from thread spawning. Consequently, we type cyclic-structured\nprocesses and restore deadlock freedom by using priorities. We show that our\ntype system is sound by proving subject reduction and progress. We define an\nencoding from PCP to PGV and prove that the encoding preserves typing and is\nsound and complete with respect to the operational semantics.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 13:36:41 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 10:52:08 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Kokke", "Wen", ""], ["Dardha", "Ornela", ""]]}, {"id": "2103.14481", "submitter": "Wen Kokke", "authors": "Wen Kokke and Ornela Dardha", "title": "Deadlock-Free Session Types in Linear Haskell", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Priority Sesh is a library for session-typed communication in Linear Haskell\nwhich offers strong compile-time correctness guarantees. Priority Sesh offers\ntwo deadlock-free APIs for session-typed communication. The first guarantees\ndeadlock freedom by restricting the process structure to trees and forests. It\nis simple and composeable, but rules out cyclic structures. The second\nguarantees deadlock freedom via priorities, which allows the programmer to\nsafely use cyclic structures as well.\n  Our library relies on Linear Haskell to guarantee linearity, which leads to\neasy-to-write session types and highly idiomatic code, and lets us avoid the\ncomplex encodings of linearity in the Haskell type system that made previous\nlibraries difficult to use.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 14:08:01 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Kokke", "Wen", ""], ["Dardha", "Ornela", ""]]}, {"id": "2103.14688", "submitter": "Semyon Grigorev", "authors": "Ekaterina Shemetova, Rustam Azimov, Egor Orachev, Ilya Epelbaum,\n  Semyon Grigorev", "title": "One Algorithm to Evaluate Them All: Unified Linear Algebra Based\n  Approach to Evaluate Both Regular and Context-Free Path Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Kronecker product-based algorithm for context-free path querying (CFPQ)\nwas proposed by Orachev et al. (2020). We reduce this algorithm to operations\nover Boolean matrices and extend it with the mechanism to extract all paths of\ninterest. We also prove $O(n^3/\\log{n})$ time complexity of the proposed\nalgorithm, where n is a number of vertices of the input graph. Thus, we provide\nthe alternative way to construct a slightly subcubic algorithm for CFPQ which\nis based on linear algebra and incremental transitive closure (a classic\ngraph-theoretic problem), as opposed to the algorithm with the same complexity\nproposed by Chaudhuri (2008). Our evaluation shows that our algorithm is a good\ncandidate to be the universal algorithm for both regular and context-free path\nquerying.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 18:54:22 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Shemetova", "Ekaterina", ""], ["Azimov", "Rustam", ""], ["Orachev", "Egor", ""], ["Epelbaum", "Ilya", ""], ["Grigorev", "Semyon", ""]]}, {"id": "2103.14698", "submitter": "Jin Sano", "authors": "Jin Sano", "title": "Implementing G-Machine in HyperLMNtal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since language processing systems generally allocate/discard memory with\ncomplex reference relationships, including circular and indirect references,\ntheir implementation is often not trivial. Here, the allocated memory and the\nreferences can be abstracted to the labeled vertices and edges of a graph. And\nthere exists a graph rewriting language, a programming language or a\ncalculation model that can handle graph intuitively, safely and efficiently.\nTherefore, the implementation of a language processing system can be highly\nexpected as an application field of graph rewriting language. To show this, in\nthis research, we implemented G-machine, the virtual machine for lazy\nevaluation, in hypergraph rewriting language, HyperLMNtal.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 03:36:58 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Sano", "Jin", ""]]}, {"id": "2103.14988", "submitter": "Zhiwei Chen", "authors": "Zao Liu, Kan Song, Zhiwei Chen", "title": "NMRPy: a novel NMR scripting system to implement artificial intelligence\n  and advanced applications", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Background: Software is an important windows to offer a variety of complex\ninstrument control and data processing for nuclear magnetic resonance (NMR)\nspectrometer. NMR software should allow researchers to flexibly implement\nvarious functionality according to the requirement of applications. Scripting\nsystem can offer an open environment for NMR users to write custom programs\nwith basic libraries. Emerging technologies, especially multivariate\nstatistical analysis and artificial intelligence, have been successfully\napplied to NMR applications such as metabolomics and biomacromolecules.\nScripting system should support more complex NMR libraries, which will enable\nthe emerging technologies to be easily implemented in the scripting\nenvironment. Result: Here, a novel NMR scripting system named \"NMRPy\" is\nintroduced. In the scripting system, both Java based NMR methods and original\nCPython based libraries are supported. A module was built as a bridge to\nintegrate the runtime environment of Java and CPython. It works as an extension\nin CPython environment, as well as interacts with Java part by Java Native\nInterface. Leveraging the bridge, Java based instrument control and data\nprocessing methods can be called as a CPython style. Compared with traditional\nscripting system, NMRPy is easier for NMR researchers to develop complex\nfunctionality with fast numerical computation, multivariate statistical\nanalysis, deep learning etc. Non-uniform sampling and protein structure\nprediction methods based on deep learning can be conveniently integrated into\nNMRPy. Conclusion: NMRPy offers a user-friendly environment to implement custom\nfunctionality leveraging its powerful basic NMR and rich CPython libraries. NMR\napplications with emerging technologies can be easily integrated. The scripting\nsystem is free of charge and can be downloaded by visiting\nhttp://www.spinstudioj.net/nmrpy.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 20:28:18 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Zao", ""], ["Song", "Kan", ""], ["Chen", "Zhiwei", ""]]}, {"id": "2103.15103", "submitter": "Ruizhe Zhao", "authors": "Ruizhe Zhao, Jianyi Cheng", "title": "Phism: Polyhedral High-Level Synthesis in MLIR", "comments": "Will be presented at LATTE'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Polyhedral optimisation, a methodology that views nested loops as polyhedra\nand searches for their optimal transformation regarding specific objectives\n(parallelism, locality, etc.), sounds promising for mitigating difficulties in\nautomatically optimising hardware designs described by high-level synthesis\n(HLS), which are typically software programs with nested loops. Nevertheless,\nexisting polyhedral tools cannot meet the requirements from HLS developers for\nplatform-specific customisation and software/hardware co-optimisation. This\npaper proposes $\\phi_{sm}$ (phism), a polyhedral HLS framework built on MLIR,\nto address these challenges through progressive lowering multi-level\nintermediate representations (IRs) from polyhedra to HLS designs.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 10:21:23 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhao", "Ruizhe", ""], ["Cheng", "Jianyi", ""]]}, {"id": "2103.15193", "submitter": "Henry DeYoung", "authors": "Ankush Das, Henry DeYoung, Andreia Mordido, Frank Pfenning", "title": "Subtyping on Nested Polymorphic Session Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of subtyping to enable a wider range of well-typed programs is\nundeniable. However, the interaction between subtyping, recursion, and\npolymorphism is not completely understood yet. In this work, we explore\nsubtyping in a system of nested, recursive, and polymorphic types with a\ncoinductive interpretation, and we prove that this problem is undecidable. Our\nresults will be broadly applicable, but to keep our study grounded in a\nconcrete setting, we work with an extension of session types with explicit\npolymorphism, parametric type constructors, and nested types. We prove that\nsubtyping is undecidable even for the fragment with only internal choices and\nnested unary recursive type constructors. Despite this negative result, we\npresent a subtyping algorithm for our system and prove its soundness. We\nminimize the impact of the inescapable incompleteness by enabling the\nprogrammer to seed the algorithm with subtyping declarations (that are\nvalidated by the algorithm). We have implemented the proposed algorithm in Rast\nand it showed to be efficient in various example programs.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 18:19:07 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Das", "Ankush", ""], ["DeYoung", "Henry", ""], ["Mordido", "Andreia", ""], ["Pfenning", "Frank", ""]]}, {"id": "2103.15408", "submitter": "Tesla Zhang", "authors": "Tesla Zhang", "title": "A simpler encoding of indexed types", "comments": "14 pages, 8 figures, ICFP 2021 TyDe workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In functional programming languages, generalized algebraic data types (GADTs)\nare very useful as the unnecessary pattern matching over them can be ruled out\nby the failure of unification of type arguments. In dependent type systems,\nthis is usually called indexed types and it's particularly useful as the\nidentity type is a special case of it. However, pattern matching over indexed\ntypes is very complicated as it requires term unification in general. We study\na simplified version of indexed types (called simpler indexed types) where we\nexplicitly specify the selection process of constructors, and we discuss its\nexpressiveness, limitations, and properties.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 08:13:30 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 20:37:46 GMT"}, {"version": "v3", "created": "Sat, 19 Jun 2021 10:29:33 GMT"}, {"version": "v4", "created": "Mon, 5 Jul 2021 18:51:24 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Zhang", "Tesla", ""]]}, {"id": "2103.15420", "submitter": "Chengjun Chen", "authors": "Mohan Cui, Chengjun Chen, Hui Xu, Yangfan Zhou", "title": "SafeDrop: Detecting Memory Deallocation Bugs of Rust Programs via Static\n  Data-Flow Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rust is an emerging programming language that aims to prevent memory-safety\nbugs. However, the current design of Rust also brings side effects which may\nincrease the risk of memory-safety issues. In particular, it employs OBRM\n(ownership-based resource management) and enforces automatic deallocation of\nunused resources without the garbage collector. It may therefore falsely\ndeallocate reclaimed memory and lead to use-after-free or double-free issues.\nIn this paper, we study the problem of invalid memory deallocation and propose\nSafeDrop, a static path-sensitive data-flow analysis approach to detect such\nbugs. Our approach analyzes each API of a Rust crate iteratively by traversing\nthe control-flow graph and extracting all aliases of each data-flow. To\nguarantee precision and scalability, we leverage a modified Tarjan algorithm to\nachieve scalable path-sensitive analysis, and a cache-based strategy to achieve\nefficient inter-procedural analysis. Our experiment results show that our\napproach can successfully detect all existing CVEs of such issues with a\nlimited number of false positives. The analysis overhead ranges from 1.0% to\n110.7% in comparison with the original compilation time. We further apply our\ntool to several real-world Rust crates and find 8 Rust crates involved with\ninvalid memory deallocation issues.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 08:33:42 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 07:37:52 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Cui", "Mohan", ""], ["Chen", "Chengjun", ""], ["Xu", "Hui", ""], ["Zhou", "Yangfan", ""]]}, {"id": "2103.15453", "submitter": "Pierre Clairambault", "authors": "Simon Castellan (IRISA, CELTIQUE), Pierre Clairambault (LIP, PLUME)", "title": "Disentangling Parallelism and Interference in Game Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game semantics is a denotational semantics presenting compositionally the\ncomputational behaviour of various kinds of effectful programs. One of its\ncelebrated achievement is to have obtained full abstraction results for\nprogramming languages with a variety of computational effects, in a single\nframework. This is known as the semantic cube or Abramsky's cube, which for\nsequential deterministic programs establishes a correspondence between certain\nconditions on strategies (''innocence'', ''well-bracketing'', ''visibility'')\nand the absence of matching computational effects. Outside of the sequential\ndeterministic realm, there are still a wealth of game semantics-based full\nabstraction results; but they no longer fit in a unified canvas. In particular,\nGhica and Murawski's fully abstract model for shared state concurrency (IA)\ndoes not have a matching notion of pure parallel program-we say that\nparallelism and interference (i.e. state plus semaphores) are entangled. In\nthis paper we construct a causal version of Ghica and Murawski's model, also\nfully abstract for IA. We provide compositional conditions parallel innocence\nand sequentiality, respectively banning interference and parallelism, and\nleading to four full abstraction results. To our knowledge, this is the first\nextension of Abramsky's semantic cube programme beyond the sequential\ndeterministic world.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 09:37:30 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Castellan", "Simon", "", "IRISA, CELTIQUE"], ["Clairambault", "Pierre", "", "LIP, PLUME"]]}, {"id": "2103.15776", "submitter": "Matthijs V\\'ak\\'ar", "authors": "Matthijs V\\'ak\\'ar", "title": "CHAD: Combinatory Homomorphic Automatic Differentiation", "comments": "arXiv admin note: substantial text overlap with arXiv:2007.05283", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Combinatory Homomorphic Automatic Differentiation (CHAD), a\nprincipled, pure, provably correct method for performing forward- and\nreverse-mode automatic differentiation (AD) on programming languages with\nexpressive features. It implements AD as a compositional, type-respecting\nsource-code transformation that generates purely functional code. This code\ntransformation is principled in the sense that it is the unique homomorphic\n(structure preserving) extension to expressive languages of the well-known and\nunambiguous definitions of automatic differentiation for a first-order\nfunctional language. Correctness of the method follows by a (compositional)\nlogical relations argument that shows that the semantics of the syntactic\nderivative is the usual calculus derivative of the semantics of the original\nprogram. In their most elegant formulation, the transformations generate code\nwith linear types. However, the transformations can be implemented in a\nstandard functional language without sacrificing correctness. This\nimplementation can be achieved by making use of abstract data types to\nrepresent the required linear types, e.g. through the use of a basic module\nsystem.\n  In this paper, we detail the method when applied to a simple higher-order\nlanguage for manipulating statically sized arrays. However, we explain how the\nmethodology applies, more generally, to functional languages with other\nexpressive features.\n  Finally, we discuss how the scope of CHAD extends beyond applications in\nautomatic differentiation to other dynamic program analyses that accumulate\ndata in a commutative monoid.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 17:10:22 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 07:23:45 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["V\u00e1k\u00e1r", "Matthijs", ""]]}, {"id": "2103.16080", "submitter": "James Wallbridge", "authors": "James Clift, Daniel Murfet, James Wallbridge", "title": "Geometry of Program Synthesis", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We re-evaluate universal computation based on the synthesis of Turing\nmachines. This leads to a view of programs as singularities of analytic\nvarieties or, equivalently, as phases of the Bayesian posterior of a synthesis\nproblem. This new point of view reveals unexplored directions of research in\nprogram synthesis, of which neural networks are a subset, for example in\nrelation to phase transitions, complexity and generalisation. We also lay the\nempirical foundations for these new directions by reporting on our\nimplementation in code of some simple experiments.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 05:14:15 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Clift", "James", ""], ["Murfet", "Daniel", ""], ["Wallbridge", "James", ""]]}, {"id": "2103.16105", "submitter": "Di Wang", "authors": "Di Wang, Jan Hoffmann, Thomas Reps", "title": "Expected-Cost Analysis for Probabilistic Programs and Semantics-Level\n  Adaption of Optional Stopping Theorems", "comments": "arXiv admin note: substantial text overlap with arXiv:2001.10150", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we present a semantics-level adaption of the Optional\nStopping Theorem, sketch an expected-cost analysis as its application, and\nsurvey different variants of the Optional Stopping Theorem that have been used\nin static analysis of probabilistic programs.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 06:30:40 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Wang", "Di", ""], ["Hoffmann", "Jan", ""], ["Reps", "Thomas", ""]]}, {"id": "2103.16240", "submitter": "Fran\\c{c}ois Gauthier", "authors": "Nicholas Allen, Fran\\c{c}ois Gauthier and Alexander Jordan", "title": "IFDS Taint Analysis with Access Paths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, static taint analysis emerged as the analysis of choice to\ndetect some of the most common web application vulnerabilities, such as SQL\ninjection (SQLi) and cross-site scripting (XSS)~\\cite{OWASP}. Furthermore, from\nan implementation perspective, the IFDS dataflow framework stood out as one of\nthe most successful vehicles to implement static taint analysis for real-world\nJava applications. While existing approaches scale reasonably to medium-size\napplications (e.g. up to one hour analysis time for less than 100K lines of\ncode), our experience suggests that no existing solution can scale to very\nlarge industrial code bases (e.g. more than 1M lines of code). In this paper,\nwe present our novel IFDS-based solution to perform fast and precise static\ntaint analysis of very large industrial Java web applications. Similar to\nstate-of-the-art approaches to taint analysis, our IFDS-based taint analysis\nuses \\textit{access paths} to abstract objects and fields in a program.\nHowever, contrary to existing approaches, our analysis is demand-driven, which\nrestricts the amount of code to be analyzed, and does not rely on a\ncomputationally expensive alias analysis, thereby significantly improving\nscalability.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 10:44:26 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Allen", "Nicholas", ""], ["Gauthier", "Fran\u00e7ois", ""], ["Jordan", "Alexander", ""]]}]