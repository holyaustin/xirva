[{"id": "1201.0023", "submitter": "Jeremy Siek", "authors": "Jeremy G. Siek and Michael M. Vitousek and Jonathan D. Turner", "title": "Effects for Funargs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stack allocation and first-class functions don't naturally mix together. In\nthis paper we show that a type and effect system can be the detergent that\nhelps these features form a nice emulsion. Our interest in this problem comes\nfrom our work on the Chapel language, but this problem is also relevant to\nlambda expressions in C++ and blocks in Objective C. The difficulty in mixing\nfirst-class functions and stack allocation is a tension between safety,\nefficiency, and simplicity. To preserve safety, one must worry about functions\noutliving the variables they reference: the classic upward funarg problem.\nThere are systems which regain safety but lose programmer-predictable\nefficiency, and ones that provide both safety and efficiency, but give up\nsimplicity by exposing regions to the programmer. In this paper we present a\nsimple design that combines a type and effect system, for safety, with\nfunction-local storage, for control over efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 21:22:31 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Siek", "Jeremy G.", ""], ["Vitousek", "Michael M.", ""], ["Turner", "Jonathan D.", ""]]}, {"id": "1201.0024", "submitter": "Jeremy Siek", "authors": "Erik Silkensen and Jeremy G. Siek", "title": "Well-typed Islands Parse Faster", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of specifying and parsing the syntax of\ndomain-specific languages (DSLs) in a modular, user-friendly way. That is, we\nwant to enable the design of composable DSLs that combine the natural syntax of\nexternal DSLs with the easy implementation of internal DSLs. The challenge in\nparsing composable DSLs is that the composition of several (individually\nunambiguous) languages is likely to contain ambiguities. In this paper, we\npresent the design of a system that uses a type-oriented variant of island\nparsing to efficiently parse the syntax of composable DSLs. In particular, we\nshow how type-oriented island parsing is constant time with respect to the\nnumber of DSLs imported. We also show how to use our tool to implement DSLs on\ntop of a host language such as Typed Racket.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 21:29:59 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Silkensen", "Erik", ""], ["Siek", "Jeremy G.", ""]]}, {"id": "1201.0027", "submitter": "Jeremy Siek", "authors": "Jeremy G. Siek", "title": "The C++0x \"Concepts\" Effort", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  C++0x is the working title for the revision of the ISO standard of the C++\nprogramming language that was originally planned for release in 2009 but that\nwas delayed to 2011. The largest language extension in C++0x was \"concepts\",\nthat is, a collection of features for constraining template parameters. In\nSeptember of 2008, the C++ standards committee voted the concepts extension\ninto C++0x, but then in July of 2009, the committee voted the concepts\nextension back out of C++0x.\n  This article is my account of the technical challenges and debates within the\n\"concepts\" effort in the years 2003 to 2009. To provide some background, the\narticle also describes the design space for constrained parametric\npolymorphism, or what is colloquially know as constrained generics. While this\narticle is meant to be generally accessible, the writing is aimed toward\nreaders with background in functional programming and programming language\ntheory. This article grew out of a lecture at the Spring School on Generic and\nIndexed Programming at the University of Oxford, March 2010.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 21:38:55 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Siek", "Jeremy G.", ""]]}, {"id": "1201.0345", "submitter": "EPTCS", "authors": "Jean-Yves Marion (LORIA)", "title": "Proceedings Second Workshop on Developments in Implicit Computational\n  Complexity", "comments": "EPTCS 75, 2012", "journal-ref": null, "doi": "10.4204/EPTCS.75", "report-no": null, "categories": "cs.LO cs.CC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of the Second International Workshop on\nDevelopments in Implicit Computational complExity (DICE 2011), which took place\non April 2-3 2011 in Saarbruecken, Germany, as a satellite event of the Joint\nEuropean Conference on Theory and Practice of Software, ETAPS 2011. Implicit\nComputational Complexity aims at studying computational complexity without\nreferring to external measuring conditions or particular machine models, but\ninstead by considering restrictions on programming languages or logical\nprinciples implying complexity properties. The aim of this workshop was to\nbring together researchers working on implicit computational complexity, from\nits logical and semantics aspects to those related to the static analysis of\nprograms, so as to foster their interaction and to give newcomers an overview\nof the current trends in this area.\n  The first DICE workshop was held in 2010 at ETAPS and published in EPTCS,\nvolume 23 (http://eptcs.org/content.cgi?DICE2010).\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2012 11:55:08 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Marion", "Jean-Yves", "", "LORIA"]]}, {"id": "1201.0874", "submitter": "Dariusz Biernacki", "authors": "Dariusz Biernacki and Serguei Lenglet", "title": "Applicative Bisimulations for Delimited-Control Operators", "comments": "A long version of an article accepted at FoSSaCS 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a behavioral theory for the untyped call-by-value lambda calculus\nextended with the delimited-control operators shift and reset. For this\ncalculus, we discuss the possible observable behaviors and we define an\napplicative bisimilarity that characterizes contextual equivalence. We then\ncompare the applicative bisimilarity and the CPS equivalence, a relation on\nterms often used in studies of control operators. In the process, we illustrate\nhow bisimilarity can be used to prove equivalence of terms with\ndelimited-control effects.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2012 11:22:08 GMT"}], "update_date": "2012-01-05", "authors_parsed": [["Biernacki", "Dariusz", ""], ["Lenglet", "Serguei", ""]]}, {"id": "1201.0979", "submitter": "Sanjit Seshia", "authors": "Sanjit A. Seshia", "title": "Sciduction: Combining Induction, Deduction, and Structure for\n  Verification and Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even with impressive advances in automated formal methods, certain problems\nin system verification and synthesis remain challenging. Examples include the\nverification of quantitative properties of software involving constraints on\ntiming and energy consumption, and the automatic synthesis of systems from\nspecifications. The major challenges include environment modeling,\nincompleteness in specifications, and the complexity of underlying decision\nproblems.\n  This position paper proposes sciduction, an approach to tackle these\nchallenges by integrating inductive inference, deductive reasoning, and\nstructure hypotheses. Deductive reasoning, which leads from general rules or\nconcepts to conclusions about specific problem instances, includes techniques\nsuch as logical inference and constraint solving. Inductive inference, which\ngeneralizes from specific instances to yield a concept, includes algorithmic\nlearning from examples. Structure hypotheses are used to define the class of\nartifacts, such as invariants or program fragments, generated during\nverification or synthesis. Sciduction constrains inductive and deductive\nreasoning using structure hypotheses, and actively combines inductive and\ndeductive reasoning: for instance, deductive techniques generate examples for\nlearning, and inductive reasoning is used to guide the deductive engines.\n  We illustrate this approach with three applications: (i) timing analysis of\nsoftware; (ii) synthesis of loop-free programs, and (iii) controller synthesis\nfor hybrid systems. Some future applications are also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2012 19:58:41 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Seshia", "Sanjit A.", ""]]}, {"id": "1201.1101", "submitter": "Sergue\\\"i Lenglet", "authors": "Sergue\\\"i Lenglet and J. B. Wells", "title": "Expansion for Universal Quantifiers", "comments": "Long version of the corresponding ESOP 2012 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expansion is an operation on typings (i.e., pairs of typing environments and\nresult types) defined originally in type systems for the lambda-calculus with\nintersection types in order to obtain principal (i.e., most informative,\nstrongest) typings. In a type inference scenario, expansion allows postponing\nchoices for whether and how to use non-syntax-driven typing rules (e.g.,\nintersection introduction) until enough information has been gathered to make\nthe right decision. Furthermore, these choices can be equivalent to inserting\nuses of such typing rules at deeply nested positions in a typing derivation,\nwithout needing to actually inspect or modify (or even have) the typing\nderivation. Expansion has in recent years become simpler due to the use of\nexpansion variables (e.g., in System E).\n  This paper extends expansion and expansion variables to systems with\nforall-quantifiers. We present System Fs, an extension of System F with\nexpansion, and prove its main properties. This system turns type inference into\na constraint solving problem; this could be helpful to design a modular type\ninference algorithm for System F types in the future.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2012 10:11:52 GMT"}], "update_date": "2012-01-06", "authors_parsed": [["Lenglet", "Sergue\u00ef", ""], ["Wells", "J. B.", ""]]}, {"id": "1201.1277", "submitter": "Mark Marron", "authors": "Mark Marron", "title": "Structural Analysis: Shape Information via Points-To Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new hybrid memory analysis, Structural Analysis,\nwhich combines an expressive shape analysis style abstract domain with\nefficient and simple points-to style transfer functions. Using data from\nempirical studies on the runtime heap structures and the programmatic idioms\nused in modern object-oriented languages we construct a heap analysis with the\nfollowing characteristics: (1) it can express a rich set of structural, shape,\nand sharing properties which are not provided by a classic points-to analysis\nand that are useful for optimization and error detection applications (2) it\nuses efficient, weakly-updating, set-based transfer functions which enable the\nanalysis to be more robust and scalable than a shape analysis and (3) it can be\nused as the basis for a scalable interprocedural analysis that produces precise\nresults in practice.\n  The analysis has been implemented for .Net bytecode and using this\nimplementation we evaluate both the runtime cost and the precision of the\nresults on a number of well known benchmarks and real world programs. Our\nexperimental evaluations show that the domain defined in this paper is capable\nof precisely expressing the majority of the connectivity, shape, and sharing\nproperties that occur in practice and, despite the use of weak updates, the\nstatic analysis is able to precisely approximate the ideal results. The\nanalysis is capable of analyzing large real-world programs (over 30K bytecodes)\nin less than 65 seconds and using less than 130MB of memory. In summary this\nwork presents a new type of memory analysis that advances the state of the art\nwith respect to expressive power, precision, and scalability and represents a\nnew area of study on the relationships between and combination of concepts from\nshape and points-to analyses.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2012 20:23:02 GMT"}], "update_date": "2012-01-06", "authors_parsed": [["Marron", "Mark", ""]]}, {"id": "1201.1327", "submitter": "Mark Marron", "authors": "Mark Marron and Cesar Sanchez and Zhendong Su and Manuel Fahndrich", "title": "Abstracting Runtime Heaps for Program Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern programming environments provide extensive support for inspecting,\nanalyzing, and testing programs based on the algorithmic structure of a\nprogram. Unfortunately, support for inspecting and understanding runtime data\nstructures during execution is typically much more limited. This paper provides\na general purpose technique for abstracting and summarizing entire runtime\nheaps. We describe the abstract heap model and the associated algorithms for\ntransforming a concrete heap dump into the corresponding abstract model as well\nas algorithms for merging, comparing, and computing changes between abstract\nmodels. The abstract model is designed to emphasize high-level concepts about\nheap-based data structures, such as shape and size, as well as relationships\nbetween heap structures, such as sharing and connectivity. We demonstrate the\nutility and computational tractability of the abstract heap model by building a\nmemory profiler. We then use this tool to check for, pinpoint, and correct\nsources of memory bloat from a suite of programs from DaCapo.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2012 00:19:57 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Marron", "Mark", ""], ["Sanchez", "Cesar", ""], ["Su", "Zhendong", ""], ["Fahndrich", "Manuel", ""]]}, {"id": "1201.2312", "submitter": "Seetha Lakshmi Lakshmi Narayanan", "authors": "B. Seetha Lakshmi, C. D. Balapriya, R. Soniya", "title": "Actor Garbage Collection in Distributed Systems using Graph\n  Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of research work has been done in the area of Garbage collection for\nboth uniprocessor and distributed systems. Actors are associated with activity\n(thread) and hence usual garbage collection algorithms cannot be applied for\nthem. Hence a separate algorithm should be used to collect them. If we\ntransform the active reference graph into a graph which captures all the\nfeatures of actors and looks like passive reference graph then any passive\nreference graph algorithm can be applied for it. But the cost of transformation\nand optimization are the core issues. An attempt has been made to walk through\nthese issues.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2012 08:24:07 GMT"}], "update_date": "2012-01-12", "authors_parsed": [["Lakshmi", "B. Seetha", ""], ["Balapriya", "C. D.", ""], ["Soniya", "R.", ""]]}, {"id": "1201.2430", "submitter": "Li Tan", "authors": "Li Tan", "title": "A Well-typed Lightweight Situation Calculus", "comments": "In Proceedings of the 21st Workshop on Logic-based methods in\n  Programming Environments (WLPE'11), ICLP 2011 Workshop, pp. 62-73, Lexington,\n  Kentucky, USA, July 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Situation calculus has been widely applied in Artificial Intelligence related\nfields. This formalism is considered as a dialect of logic programming language\nand mostly used in dynamic domain modeling. However, type systems are hardly\ndeployed in situation calculus in the literature. To achieve a correct and\nsound typed program written in situation calculus, adding typing elements into\nthe current situation calculus will be quite helpful. In this paper, we propose\nto add more typing mechanisms to the current version of situation calculus,\nespecially for three basic elements in situation calculus: situations, actions\nand objects, and then perform rigid type checking for existing situation\ncalculus programs to find out the well-typed and ill-typed ones. In this way,\ntype correctness and soundness in situation calculus programs can be guaranteed\nby type checking based on our type system. This modified version of a\nlightweight situation calculus is proved to be a robust and well-typed system.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2012 21:44:18 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2012 23:18:40 GMT"}], "update_date": "2012-06-19", "authors_parsed": [["Tan", "Li", ""]]}, {"id": "1201.3778", "submitter": "Francesco Versaci", "authors": "Francesco Versaci and Keshav Pingali", "title": "Processor Allocation for Optimistic Parallelization of Irregular\n  Programs", "comments": "12 pages, 3 figures, extended version of SPAA 2011 brief announcement", "journal-ref": "LNCS 7333/2012", "doi": "10.1007/978-3-642-31125-3_1", "report-no": null, "categories": "cs.PL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimistic parallelization is a promising approach for the parallelization of\nirregular algorithms: potentially interfering tasks are launched dynamically,\nand the runtime system detects conflicts between concurrent activities,\naborting and rolling back conflicting tasks. However, parallelism in irregular\nalgorithms is very complex. In a regular algorithm like dense matrix\nmultiplication, the amount of parallelism can usually be expressed as a\nfunction of the problem size, so it is reasonably straightforward to determine\nhow many processors should be allocated to execute a regular algorithm of a\ncertain size (this is called the processor allocation problem). In contrast,\nparallelism in irregular algorithms can be a function of input parameters, and\nthe amount of parallelism can vary dramatically during the execution of the\nirregular algorithm. Therefore, the processor allocation problem for irregular\nalgorithms is very difficult.\n  In this paper, we describe the first systematic strategy for addressing this\nproblem. Our approach is based on a construct called the conflict graph, which\n(i) provides insight into the amount of parallelism that can be extracted from\nan irregular algorithm, and (ii) can be used to address the processor\nallocation problem for irregular algorithms. We show that this problem is\nrelated to a generalization of the unfriendly seating problem and, by extending\nTur\\'an's theorem, we obtain a worst-case class of problems for optimistic\nparallelization, which we use to derive a lower bound on the exploitable\nparallelism. Finally, using some theoretically derived properties and some\nexperimental facts, we design a quick and stable control strategy for solving\nthe processor allocation problem heuristically.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2012 13:16:57 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2012 13:07:46 GMT"}], "update_date": "2012-06-28", "authors_parsed": [["Versaci", "Francesco", ""], ["Pingali", "Keshav", ""]]}, {"id": "1201.3907", "submitter": "Stephen Chang", "authors": "Stephen Chang and Matthias Felleisen", "title": "The Call-by-need Lambda Calculus, Revisited", "comments": "ESOP 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existing call-by-need lambda calculi describe lazy evaluation via\nequational logics. A programmer can use these logics to safely ascertain\nwhether one term is behaviorally equivalent to another or to determine the\nvalue of a lazy program. However, neither of the existing calculi models\nevaluation in a way that matches lazy implementations.\n  Both calculi suffer from the same two problems. First, the calculi never\ndiscard function calls, even after they are completely resolved. Second, the\ncalculi include re-association axioms even though these axioms are merely\nadministrative steps with no counterpart in any implementation.\n  In this paper, we present an alternative axiomatization of lazy evaluation\nusing a single axiom. It eliminates both the function call retention problem\nand the extraneous re-association axioms. Our axiom uses a grammar of contexts\nto describe the exact notion of a needed computation. Like its predecessors,\nour new calculus satisfies consistency and standardization properties and is\nthus suitable for reasoning about behavioral equivalence. In addition, we\nestablish a correspondence between our semantics and Launchbury's natural\nsemantics.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2012 20:44:35 GMT"}], "update_date": "2012-01-19", "authors_parsed": [["Chang", "Stephen", ""], ["Felleisen", "Matthias", ""]]}, {"id": "1201.4262", "submitter": "Chris Hankin", "authors": "Fan Yang, Chris Hankin, Flemming Nielson, Hanne Riis Nielson", "title": "Secondary use of data in EHR systems", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to use aspect-oriented programming to separate security and trust\nissues from the logical design of mobile, distributed systems. The main\nchallenge is how to enforce various types of security policies, in particular\npredictive access control policies - policies based on the future behavior of a\nprogram. A novel feature of our approach is that advice is able to analyze the\nfuture use of data. We consider a number of different security policies,\nconcerning both primary and secondary use of data, some of which can only be\nenforced by analysis of process continuations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2012 11:11:37 GMT"}], "update_date": "2012-01-23", "authors_parsed": [["Yang", "Fan", ""], ["Hankin", "Chris", ""], ["Nielson", "Flemming", ""], ["Nielson", "Hanne Riis", ""]]}, {"id": "1201.4567", "submitter": "James Royer", "authors": "Norman Danner and James S. Royer", "title": "Ramified Structural Recursion and Corecursion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate feasible computation over a fairly general notion of data and\ncodata. Specifically, we present a direct Bellantoni-Cook-style normal/safe\ntyped programming formalism, RS1, that expresses feasible structural recursions\nand corecursions over data and codata specified by polynomial functors. (Lists,\nstreams, finite trees, infinite trees, etc. are all directly definable.) A\nnovel aspect of RS1 is that it embraces structure-sharing as in standard\nfunctional-programming implementations. As our data representations use\nsharing, our implementation of structural recursions are memoized to avoid the\npossibly exponentially-many repeated subcomputations a naive implementation\nmight perform. We introduce notions of size for representations of data\n(accounting for sharing) and codata (using ideas from type-2 computational\ncomplexity) and establish that type-level 1 RS1-functions have\npolynomial-bounded runtimes and satisfy a polynomial-time completeness\ncondition. Also, restricting RS1 terms to particular types produces\ncharacterizations of some standard complexity classes (e.g., omega-regular\nlanguages, linear-space functions) and some less-standard classes (e.g.,\nlog-space streams).\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2012 16:06:01 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2012 22:18:42 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Danner", "Norman", ""], ["Royer", "James S.", ""]]}, {"id": "1201.4715", "submitter": "Marek Trt\\'ik", "authors": "Ji\\v{r}\\'i Slab\\'y, Jan Strej\\v{c}ek, Marek Trt\\'ik", "title": "Compact Symbolic Execution", "comments": "This is a full version of the paper accepted to ATVA 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a generalisation of King's symbolic execution technique called\ncompact symbolic execution. It proceeds in two steps. First, we analyse cyclic\npaths in the control flow graph of a given program, independently from the rest\nof the program. Our goal is to compute a so called template for each such a\ncyclic path. A template is a declarative parametric description of all possible\nprogram states, which may leave the analysed cyclic path after any number of\niterations along it. In the second step, we execute the program symbolically\nwith the templates in hand. The result is a compact symbolic execution tree. A\ncompact tree always carry the same information in all its leaves as the\ncorresponding classic symbolic execution tree. Nevertheless, a compact tree is\ntypically substantially smaller than the corresponding classic tree. There are\neven programs for which compact symbolic execution trees are finite while\nclassic symbolic execution trees are infinite.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2012 13:50:55 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2013 15:19:48 GMT"}], "update_date": "2013-09-18", "authors_parsed": [["Slab\u00fd", "Ji\u0159\u00ed", ""], ["Strej\u010dek", "Jan", ""], ["Trt\u00edk", "Marek", ""]]}, {"id": "1201.4719", "submitter": "Marek Trt\\'ik", "authors": "Ji\\v{r}\\'i Slab\\'y, Jan Strej\\v{c}ek, Marek Trt\\'ik", "title": "On Synergy of Metal, Slicing, and Symbolic Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel technique for finding real errors in programs. The\ntechnique is based on a synergy of three well-known methods: metacompilation,\nslicing, and symbolic execution. More precisely, we instrument a given program\nwith a code that tracks runs of state machines representing various kinds of\nerrors. Next we slice the program to reduce its size without affecting runs of\nstate machines. And then we symbolically execute the sliced program. Depending\non the kind of symbolic execution, the technique can be applied as a\nstand-alone bug finding technique, or to weed out some false positives from an\noutput of another bug-finding tool. We provide several examples demonstrating\nthe practical applicability of our technique.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2012 14:06:32 GMT"}], "update_date": "2012-01-24", "authors_parsed": [["Slab\u00fd", "Ji\u0159\u00ed", ""], ["Strej\u010dek", "Jan", ""], ["Trt\u00edk", "Marek", ""]]}, {"id": "1201.4801", "submitter": "Pierre-Evariste Dagand", "authors": "Dagand Pierre-Evariste and McBride Conor", "title": "Transporting Functions across Ornaments", "comments": null, "journal-ref": "J. Funct. Prog. 24 (2014) 316-383", "doi": "10.1017/S0956796814000069", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming with dependent types is a blessing and a curse. It is a blessing\nto be able to bake invariants into the definition of data-types: we can finally\nwrite correct-by-construction software. However, this extreme accuracy is also\na curse: a data-type is the combination of a structuring medium together with a\nspecial purpose logic. These domain-specific logics hamper any effort of code\nreuse among similarly structured data.\n  In this paper, we exorcise our data-types by adapting the notion of ornament\nto our universe of inductive families. We then show how code reuse can be\nachieved by ornamenting functions. Using these functional ornament, we capture\nthe relationship between functions such as the addition of natural numbers and\nthe concatenation of lists. With this knowledge, we demonstrate how the\nimplementation of the former informs the implementation of the latter: the user\ncan ask the definition of addition to be lifted to lists and she will only be\nasked the details necessary to carry on adding lists rather than numbers.\n  Our presentation is formalised in a type theory with a universe of data-types\nand all our constructions have been implemented as generic programs, requiring\nno extension to the type theory.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2012 18:27:52 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2012 20:58:57 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Pierre-Evariste", "Dagand", ""], ["Conor", "McBride", ""]]}, {"id": "1201.5240", "submitter": "James Cheney", "authors": "James Cheney (University of Edinburgh)", "title": "A dependent nominal type theory", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 8, Issue 1 (February\n  20, 2012) lmcs:1042", "doi": "10.2168/LMCS-8(1:8)2012", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nominal abstract syntax is an approach to representing names and binding\npioneered by Gabbay and Pitts. So far nominal techniques have mostly been\nstudied using classical logic or model theory, not type theory. Nominal\nextensions to simple, dependent and ML-like polymorphic languages have been\nstudied, but decidability and normalization results have only been established\nfor simple nominal type theories. We present a LF-style dependent type theory\nextended with name-abstraction types, prove soundness and decidability of\nbeta-eta-equivalence checking, discuss adequacy and canonical forms via an\nexample, and discuss extensions such as dependently-typed recursion and\ninduction principles.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2012 11:47:52 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2012 19:35:23 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Cheney", "James", "", "University of Edinburgh"]]}, {"id": "1201.5418", "submitter": "Rafael Caballero", "authors": "R. Caballero, M. Rodriguez-Artalejo and C. A. Romero-Diaz", "title": "A Transformation-based Implementation for CLP with Qualification and\n  Proximity", "comments": "To appear in Theory and Practice of Logic Programming (TPLP). arXiv\n  admin note: significant text overlap with arXiv:1009.1976", "journal-ref": null, "doi": null, "report-no": "12-R01-TPLP", "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Uncertainty in logic programming has been widely investigated in the last\ndecades, leading to multiple extensions of the classical LP paradigm. However,\nfew of these are designed as extensions of the well-established and powerful\nCLP scheme for Constraint Logic Programming. In a previous work we have\nproposed the SQCLP ({\\em proximity-based qualified constraint logic\nprogramming}) scheme as a quite expressive extension of CLP with support for\nqualification values and proximity relations as generalizations of uncertainty\nvalues and similarity relations, respectively. In this paper we provide a\ntransformation technique for transforming SQCLP programs and goals into\nsemantically equivalent CLP programs and goals, and a practical Prolog-based\nimplementation of some particularly useful instances of the SQCLP scheme. We\nalso illustrate, by showing some simple---and working---examples, how the\nprototype can be effectively used as a tool for solving problems where\nqualification values and proximity relations play a key role. Intended use of\nSQCLP includes flexible information retrieval applications.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2012 23:47:26 GMT"}], "update_date": "2012-01-27", "authors_parsed": [["Caballero", "R.", ""], ["Rodriguez-Artalejo", "M.", ""], ["Romero-Diaz", "C. A.", ""]]}, {"id": "1201.5728", "submitter": "Yusuf Motara", "authors": "Yusuf Moosa Motara", "title": "Functional Programming and Security", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper analyses the security contribution of typical functional-language\nfeatures by examining them in the light of accepted information security\nprinciples. Imperative and functional code are compared to illustrate various\ncases. In conclusion, there may be an excellent case for the use of functional\nlanguages on the grounds of better security; however, empirical research should\nbe done to validate this possibility.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2012 09:23:34 GMT"}], "update_date": "2012-01-30", "authors_parsed": [["Motara", "Yusuf Moosa", ""]]}, {"id": "1201.6057", "submitter": "Ralf L\\\"ammel", "authors": "Ralf Laemmel, Simon Thompson, and Markus Kaiser", "title": "Programming errors in traversal programs over structured data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traversal strategies \\'a la Stratego (also \\'a la Strafunski and 'Scrap Your\nBoilerplate') provide an exceptionally versatile and uniform means of querying\nand transforming deeply nested and heterogeneously structured data including\nterms in functional programming and rewriting, objects in OO programming, and\nXML documents in XML programming. However, the resulting traversal programs are\nprone to programming errors. We are specifically concerned with errors that go\nbeyond conservative type errors; examples we examine include divergent\ntraversals, prematurely terminated traversals, and traversals with dead code.\nBased on an inventory of possible programming errors we explore options of\nstatic typing and static analysis so that some categories of errors can be\navoided. This exploration generates suggestions for improvements to strategy\nlibraries as well as their underlying programming languages. Haskell is used\nfor illustrations and specifications with sufficient explanations to make the\npresentation comprehensible to the non-specialist. The overall ideas are\nlanguage-agnostic and they are summarized accordingly.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2012 16:50:15 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Laemmel", "Ralf", ""], ["Thompson", "Simon", ""], ["Kaiser", "Markus", ""]]}, {"id": "1201.6188", "submitter": "Massimo Bartoletti", "authors": "Massimo Bartoletti and Emilio Tuosto and Roberto Zunino", "title": "On the realizability of contracts in dishonest systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a theory of contracting systems, where behavioural contracts may\nbe violated by dishonest participants after they have been agreed upon - unlike\nin traditional approaches based on behavioural types. We consider the contracts\nof \\cite{CastagnaPadovaniGesbert09toplas}, and we embed them in a calculus that\nallows distributed participants to advertise contracts, reach agreements, query\nthe fulfilment of contracts, and realise them (or choose not to).\n  Our contract theory makes explicit who is culpable at each step of a\ncomputation. A participant is honest in a given context S when she is not\nculpable in each possible interaction with S. Our main result is a sufficient\ncriterion for classifying a participant as honest in all possible contexts.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2012 11:58:01 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Bartoletti", "Massimo", ""], ["Tuosto", "Emilio", ""], ["Zunino", "Roberto", ""]]}]