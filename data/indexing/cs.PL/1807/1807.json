[{"id": "1807.00067", "submitter": "Alan Jeffrey", "authors": "Alan Jeffrey", "title": "Josephine: Using JavaScript to safely manage the lifetimes of Rust data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper is about the interface between languages which use a garbage\ncollector and those which use fancy types for safe manual memory management.\nGarbage collection is the traditional memory management scheme for functional\nlanguages, whereas type systems are now used for memory safety in imperative\nlanguages. We use existing techniques for linear capabilities to provide safe\naccess to copyable references, but the application to languages with a tracing\ngarbage collector, and to data with explicit lifetimes is new. This work is\nrelated to mixed linear/non-linear programming, but the languages being mixed\nare Rust and JavaScript.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 20:59:55 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Jeffrey", "Alan", ""]]}, {"id": "1807.00137", "submitter": "Paola Giannini", "authors": "Paola Giannini and Marco Servetto and Elena Zucca and James Cone", "title": "Flexible recovery of uniqueness and immutability (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an imperative object calculus where types are annotated with\nqualifiers for aliasing and mutation control. There are two key novelties with\nrespect to similar proposals. First, the type system is very expressive.\nNotably, it adopts the \"recovery\" approach, that is, using the type context to\njustify strengthening types, greatly improving its power by permitting to\nrecover uniqueness and immutability properties even in presence of other\nreferences. This is achieved by rules which restrict the use of such other\nreferences in the portion of code which is recovered. Second, execution is\nmodeled by a non standard operational model, where properties of qualifiers can\nbe directly expressed on source terms, rather than as invariants on an\nauxiliary structure which mimics physical memory. Formally, this is achieved by\nthe block construct, introducing local variable declarations, which, when\nevaluated, play the role of store.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 08:00:49 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 21:21:43 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Giannini", "Paola", ""], ["Servetto", "Marco", ""], ["Zucca", "Elena", ""], ["Cone", "James", ""]]}, {"id": "1807.00515", "submitter": "Martin Monperrus", "authors": "Martin Monperrus", "title": "Automatic Software Repair: a Bibliography", "comments": null, "journal-ref": "ACM Computing Surveys, Association for Computing Machinery, 2017,\n  51, pp.1-24", "doi": "10.1145/3105906", "report-no": null, "categories": "cs.SE cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a survey on automatic software repair. Automatic\nsoftware repair consists of automatically finding a solution to software bugs\nwithout human intervention. This article considers all kinds of repairs. First,\nit discusses behavioral repair where test suites, contracts, models, and\ncrashing inputs are taken as oracle. Second, it discusses state repair, also\nknown as runtime repair or runtime recovery, with techniques such as checkpoint\nand restart, reconfiguration, and invariant restoration. The uniqueness of this\narticle is that it spans the research communities that contribute to this body\nof knowledge: software engineering, dependability, operating systems,\nprogramming languages, and security. It provides a novel and structured\noverview of the diversity of bug oracles and repair operators used in the\nliterature.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 08:09:28 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Monperrus", "Martin", ""]]}, {"id": "1807.00575", "submitter": "Shiqi Shen", "authors": "Shiqi Shen, Soundarya Ramesh, Shweta Shinde, Abhik Roychoudhury,\n  Prateek Saxena", "title": "Neuro-Symbolic Execution: The Feasibility of an Inductive Approach to\n  Symbolic Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic execution is a powerful technique for program analysis. However, it\nhas many limitations in practical applicability: the path explosion problem\nencumbers scalability, the need for language-specific implementation, the\ninability to handle complex dependencies, and the limited expressiveness of\ntheories supported by underlying satisfiability checkers. Often, relationships\nbetween variables of interest are not expressible directly as purely symbolic\nconstraints. To this end, we present a new approach -- neuro-symbolic execution\n-- which learns an approximation of the relationship as a neural net. It\nfeatures a constraint solver that can solve mixed constraints, involving both\nsymbolic expressions and neural network representation. To do so, we envision\nsuch constraint solving as procedure combining SMT solving and gradient-based\noptimization. We demonstrate the utility of neuro-symbolic execution in\nconstructing exploits for buffer overflows. We report success on 13/14 programs\nwhich have difficult constraints, known to require specialized extensions to\nsymbolic execution. In addition, our technique solves $100$\\% of the given\nneuro-symbolic constraints in $73$ programs from standard verification and\ninvariant synthesis benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 10:08:45 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Shen", "Shiqi", ""], ["Ramesh", "Soundarya", ""], ["Shinde", "Shweta", ""], ["Roychoudhury", "Abhik", ""], ["Saxena", "Prateek", ""]]}, {"id": "1807.00614", "submitter": "Pedro Zuidberg Dos Martires", "authors": "Pedro Zuidberg Dos Martires, Anton Dries, Luc De Raedt", "title": "Knowledge Compilation with Continuous Random Variables and its\n  Application in Hybrid Probabilistic Logic Programming", "comments": "8 pages, 2 figures, StarAI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In probabilistic reasoning, the traditionally discrete domain has been\nelevated to the hybrid domain encompassing additionally continuous random\nvariables. Inference in the hybrid domain, however, usually necessitates to\ncondone trade-offs on either the inference on discrete or continuous random\nvariables. We introduce a novel approach based on weighted model integration\nand algebraic model counting that circumvents these trade-offs. We then show\nhow it supports knowledge compilation and exact probabilistic inference.\nMoreover, we introduce the hybrid probabilistic logic programming language\nHAL-ProbLog, an extension of ProbLog, to which we apply our inference approach.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 12:04:59 GMT"}, {"version": "v2", "created": "Thu, 12 Jul 2018 13:34:35 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Martires", "Pedro Zuidberg Dos", ""], ["Dries", "Anton", ""], ["De Raedt", "Luc", ""]]}, {"id": "1807.00661", "submitter": "Stefan Marr", "authors": "Richard Roberts, Stefan Marr, Michael Homer, James Noble", "title": "Transient Typechecks are (Almost) Free", "comments": "Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transient gradual typing imposes run-time type tests that typically cause a\nlinear slowdown in programs' performance. This performance impact discourages\nthe use of type annotations because adding types to a program makes the program\nslower. A virtual machine can employ standard just-in-time optimizations to\nreduce the overhead of transient checks to near zero. These optimizations can\ngive gradually-typed languages performance comparable to state-of-the-art\ndynamic languages, so programmers can add types to their code without affecting\ntheir programs' performance.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 13:46:38 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 09:29:17 GMT"}, {"version": "v3", "created": "Mon, 18 Feb 2019 14:47:21 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Roberts", "Richard", ""], ["Marr", "Stefan", ""], ["Homer", "Michael", ""], ["Noble", "James", ""]]}, {"id": "1807.01053", "submitter": "Sergey Goncharov", "authors": "Sergey Goncharov, Julian Jakob, Renato Neves", "title": "A Semantics for Hybrid Iteration", "comments": "Corrected version of a CONCUR'18 paper; more proof details", "journal-ref": null, "doi": "10.4230/LIPIcs.CONCUR.2018.22", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced notions of guarded traced (monoidal) category and\nguarded (pre-)iterative monad aim at unifying different instances of partial\niteration whilst keeping in touch with the established theory of total\niteration and preserving its merits. In this paper we use these notions and the\ncorresponding stock of results to examine different types of iteration for\nhybrid computations. As a starting point we use an available notion of hybrid\nmonad restricted to the category of sets, and modify it in order to obtain a\nsuitable notion of guarded iteration with guardedness interpreted as\nprogressiveness in time - we motivate this modification by our intention to\ncapture Zeno behaviour in an arguably general and feasible way. We illustrate\nour results with a simple programming language for hybrid computations and\ninterpret it over the developed semantic foundations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 09:47:52 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 15:59:02 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Goncharov", "Sergey", ""], ["Jakob", "Julian", ""], ["Neves", "Renato", ""]]}, {"id": "1807.01352", "submitter": "EPTCS", "authors": "Fr\\'ed\\'eric Blanqui (INRIA), Giselle Reis (CMU)", "title": "Proceedings of the 13th International Workshop on Logical Frameworks and\n  Meta-Languages: Theory and Practice", "comments": null, "journal-ref": "EPTCS 274, 2018", "doi": "10.4204/EPTCS.274", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains a selection of papers presented at LFMTP 2018, the 13th\ninternational Workshop on Logical Frameworks and Meta-Languages: Theory and\nPractice (LFMTP), held on July 7, 2018, in Oxford, UK. The workshop was\naffiliated with the 3rd international conference on Formal Structures for\nComputation and Deduction (FSCD) within the 7th Federated Logic Conference\n(FLoC).\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 19:31:12 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Blanqui", "Fr\u00e9d\u00e9ric", "", "INRIA"], ["Reis", "Giselle", "", "CMU"]]}, {"id": "1807.01456", "submitter": "Hiromi Ishii", "authors": "Hiromi Ishii", "title": "A Purely Functional Computer Algebra System Embedded in Haskell", "comments": "16 pages, Accepted to CASC 2018", "journal-ref": "Computer Algebra in Scientific Computing, pp. 288-303. 20th\n  International Workshop, CASC 2018, Lille, France, September 17-21, 2018,\n  Proceedings", "doi": "10.1007/978-3-319-99639-4_20", "report-no": null, "categories": "cs.SC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate how methods in Functional Programming can be used to implement\na computer algebra system. As a proof-of-concept, we present the\ncomputational-algebra package. It is a computer algebra system implemented as\nan embedded domain-specific language in Haskell, a purely functional\nprogramming language. Utilising methods in functional programming and prominent\nfeatures of Haskell, this library achieves safety, composability, and\ncorrectness at the same time. To demonstrate the advantages of our approach, we\nhave implemented advanced Gr\\\"{o}bner basis algorithms, such as Faug\\`{e}re's\n$F_4$ and $F_5$, in a composable way.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 06:22:02 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 08:54:50 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Ishii", "Hiromi", ""]]}, {"id": "1807.01611", "submitter": "Anthony Savidis", "authors": "Anthony Savidis, Yannis Apostolidis, Yannis Lilis", "title": "Multi-Stage JavaScript", "comments": "Full source code for Spider Monkey extension (and Visual Studio\n  build) available via Git Hub", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-stage languages support generative metaprogramming via macros evaluated\nin a process preceding the actual interpretation or compilation of the program\nin which they are used. Macros update the source of their hosting program by\nemitting code that takes their place in the file, while their code may also be\nproduced, fully or partially, by nested macros. All macros at the same nesting\nbelong to the same stage, with the outer stage collecting the macros affect-ing\nonly the main program. We extended JavaScript with staging annotations and\nimplemented them in Spider Monkey, emitting pure JavaScript code as the final\noutcome of stage computation. We discuss how the original Spider Monkey system\nis minimally affected with extensions in the syntax, parser and internal AST\nstructures, and the addition of an unparser, a staging loop, some library\nfunctions and a debugger backend component for AST inspection. Since stages\nhave a generative metaprogramming role we do not foresee any interplay with the\nbrowser DOM, and thus there is no reason to repeat their evalua-tion on every\npage load. Hence, such JavaScript extensions are meant only for\ndevelopment-time, emitting pure JavaScript code that can be run in any browser.\nFinally, to enable debugging stages in any browser we implemented a pure\nJavaScript client, communicating with the extended Spider Monkey, and offering\nthe necessary AST display and unparsing that a browser debugger does not\nprovide.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 14:26:52 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Savidis", "Anthony", ""], ["Apostolidis", "Yannis", ""], ["Lilis", "Yannis", ""]]}, {"id": "1807.01624", "submitter": "Vladimir Kiriansky", "authors": "Vladimir Kiriansky, Haoran Xu, Martin Rinard, Saman Amarasinghe", "title": "Cimple: Instruction and Memory Level Parallelism", "comments": "To appear in PACT'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern out-of-order processors have increased capacity to exploit instruction\nlevel parallelism (ILP) and memory level parallelism (MLP), e.g., by using wide\nsuperscalar pipelines and vector execution units, as well as deep buffers for\nin-flight memory requests. These resources, however, often exhibit poor\nutilization rates on workloads with large working sets, e.g., in-memory\ndatabases, key-value stores, and graph analytics, as compilers and hardware\nstruggle to expose ILP and MLP from the instruction stream automatically.\n  In this paper, we introduce the IMLP (Instruction and Memory Level\nParallelism) task programming model. IMLP tasks execute as coroutines that\nyield execution at annotated long-latency operations, e.g., memory accesses,\ndivisions, or unpredictable branches. IMLP tasks are interleaved on a single\nthread, and integrate well with thread parallelism and vectorization. Our DSL\nembedded in C++, Cimple, allows exploration of task scheduling and\ntransformations, such as buffering, vectorization, pipelining, and prefetching.\n  We demonstrate state-of-the-art performance on core algorithms used in\nin-memory databases that operate on arrays, hash tables, trees, and skip lists.\nCimple applications reach 2.5x throughput gains over hardware multithreading on\na multi-core, and 6.4x single thread speedup.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 14:50:13 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Kiriansky", "Vladimir", ""], ["Xu", "Haoran", ""], ["Rinard", "Martin", ""], ["Amarasinghe", "Saman", ""]]}, {"id": "1807.01784", "submitter": "Mehdi Drissi", "authors": "Mehdi Drissi, Olivia Watkins, Aditya Khant, Vivaswat Ojha, Pedro\n  Sandoval, Rakia Segev, Eric Weiner, Robert Keller", "title": "Program Language Translation Using a Grammar-Driven Tree-to-Tree Model", "comments": "Accepted at the ICML workshop Neural Abstract Machines & Program\n  Induction v2. 4 pages excluding acknowledgements/references (6 pages total)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task of translating between programming languages differs from the\nchallenge of translating natural languages in that programming languages are\ndesigned with a far more rigid set of structural and grammatical rules.\nPrevious work has used a tree-to-tree encoder/decoder model to take advantage\nof the inherent tree structure of programs during translation. Neural decoders,\nhowever, by default do not exploit known grammar rules of the target language.\nIn this paper, we describe a tree decoder that leverages knowledge of a\nlanguage's grammar rules to exclusively generate syntactically correct\nprograms. We find that this grammar-based tree-to-tree model outperforms the\nstate of the art tree-to-tree model in translating between two programming\nlanguages on a previously used synthetic task.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 21:19:00 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Drissi", "Mehdi", ""], ["Watkins", "Olivia", ""], ["Khant", "Aditya", ""], ["Ojha", "Vivaswat", ""], ["Sandoval", "Pedro", ""], ["Segev", "Rakia", ""], ["Weiner", "Eric", ""], ["Keller", "Robert", ""]]}, {"id": "1807.01869", "submitter": "EPTCS", "authors": "Carlo Angiuli, Evan Cavallo, Kuen-Bang Hou (Favonia), Robert Harper,\n  Jonathan Sterling", "title": "The RedPRL Proof Assistant (Invited Paper)", "comments": "In Proceedings LFMTP 2018, arXiv:1807.01352", "journal-ref": "EPTCS 274, 2018, pp. 1-10", "doi": "10.4204/EPTCS.274.1", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RedPRL is an experimental proof assistant based on Cartesian cubical\ncomputational type theory, a new type theory for higher-dimensional\nconstructions inspired by homotopy type theory. In the style of Nuprl, RedPRL\nusers employ tactics to establish behavioral properties of cubical functional\nprograms embodying the constructive content of proofs. Notably, RedPRL\nimplements a two-level type theory, allowing an extensional, proof-irrelevant\nnotion of exact equality to coexist with a higher-dimensional proof-relevant\nnotion of paths.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 07:08:44 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Angiuli", "Carlo", "", "Favonia"], ["Cavallo", "Evan", "", "Favonia"], ["Hou", "Kuen-Bang", "", "Favonia"], ["Harper", "Robert", ""], ["Sterling", "Jonathan", ""]]}, {"id": "1807.01870", "submitter": "EPTCS", "authors": "Ernesto Copello (1), Nora Szasz (2), \\'Alvaro Tasistro (2) ((1)\n  University of Iowa, (2) Universidad ORT Uruguay)", "title": "Formalisation in Constructive Type Theory of Barendregt's Variable\n  Convention for Generic Structures with Binders", "comments": "In Proceedings LFMTP 2018, arXiv:1807.01352", "journal-ref": "EPTCS 274, 2018, pp. 11-26", "doi": "10.4204/EPTCS.274.2", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a universe of regular datatypes with variable binding\ninformation, for which we define generic formation and elimination (i.e.\ninduction /recursion) operators. We then define a generic alpha-equivalence\nrelation over the types of the universe based on name-swapping, and derive\niteration and induction principles which work modulo alpha-conversion capturing\nBarendregt's Variable Convention. We instantiate the resulting framework so as\nto obtain the Lambda Calculus and System F, for which we derive substitution\noperations and substitution lemmas for alpha-conversion and substitution\ncomposition. The whole work is carried out in Constructive Type Theory and\nmachine-checked by the system Agda.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 07:09:02 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Copello", "Ernesto", ""], ["Szasz", "Nora", ""], ["Tasistro", "\u00c1lvaro", ""]]}, {"id": "1807.01872", "submitter": "EPTCS", "authors": "Rodolphe Lepigre (Inria, LSV, CNRS, Universit\\'e Paris-Saclay, Cachan,\n  France), Christophe Raffalli (LAMA, CNRS, Universit\\'e Savoie Mont Blanc,\n  Chamb\\'ery, France)", "title": "Abstract Representation of Binders in OCaml using the Bindlib Library", "comments": "In Proceedings LFMTP 2018, arXiv:1807.01352", "journal-ref": "EPTCS 274, 2018, pp. 42-56", "doi": "10.4204/EPTCS.274.4", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bindlib library for OCaml provides a set of tools for the manipulation of\ndata structures with variable binding. It is very well suited for the\nrepresentation of abstract syntax trees, and has already been used for the\nimplementation of half a dozen languages and proof assistants (including a new\nversion of the logical framework Dedukti). Bindlib is optimised for fast\nsubstitution, and it supports variable renaming. Since the representation of\nbinders is based on higher-order abstract syntax, variable capture cannot arise\nduring substitution. As a consequence, variable names are not updated at\nsubstitution time. They can however be explicitly recomputed to avoid \"visual\ncapture\" (i.e., distinct variables with the same apparent name) when a data\nstructure is displayed.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 07:09:41 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Lepigre", "Rodolphe", "", "Inria, LSV, CNRS, Universit\u00e9 Paris-Saclay, Cachan,\n  France"], ["Raffalli", "Christophe", "", "LAMA, CNRS, Universit\u00e9 Savoie Mont Blanc,\n  Chamb\u00e9ry, France"]]}, {"id": "1807.01948", "submitter": "Roly Perera", "authors": "Rudi Horn and Roly Perera and James Cheney", "title": "Incremental Relational Lenses", "comments": "To appear, ICFP 2018", "journal-ref": null, "doi": "10.1145/3236769", "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lenses are a popular approach to bidirectional transformations, a\ngeneralisation of the view update problem in databases, in which we wish to\nmake changes to source tables to effect a desired change on a view. However,\nperhaps surprisingly, lenses have seldom actually been used to implement\nupdatable views in databases. Bohannon, Pierce and Vaughan proposed an approach\nto updatable views called relational lenses, but to the best of our knowledge\nthis proposal has not been implemented or evaluated to date. We propose\nincremental relational lenses, that equip relational lenses with\nchange-propagating semantics that map small changes to the view to\n(potentially) small changes to the source tables. We also present a\nlanguage-integrated implementation of relational lenses and a detailed\nexperimental evaluation, showing orders of magnitude improvement over the\nnon-incremental approach. Our work shows that relational lenses can be used to\nsupport expressive and efficient view updates at the language level, without\nrelying on updatable view support from the underlying database.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 11:42:04 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 12:16:05 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Horn", "Rudi", ""], ["Perera", "Roly", ""], ["Cheney", "James", ""]]}, {"id": "1807.02081", "submitter": "Ignacio F\\'abregas", "authors": "Luca Aceto, Ignacio F\\'abregas, \\'Alvaro Garc\\'ia-P\\'erez, Anna\n  Ing\\'olfsd\\'ottir and Yolanda Ortega-Mall\\'en", "title": "Rule Formats for Nominal Process Calculi", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 4 (October\n  14, 2019) lmcs:5834", "doi": "10.23638/LMCS-15(4:2)2019", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The nominal transition systems (NTSs) of Parrow et al. describe the\noperational semantics of nominal process calculi. We study NTSs in terms of the\nnominal residual transition systems (NRTSs) that we introduce. We provide rule\nformats for the specifications of NRTSs that ensure that the associated NRTS is\nan NTS and apply them to the operational specifications of the early and late\npi-calculus. We also explore alternative specifications of the NTSs in which we\nallow residuals of abstraction sort, and introduce translations between the\nsystems with and without residuals of abstraction sort. Our study stems from\nthe Nominal SOS of Cimini et al. and from earlier works in nominal sets and\nnominal logic by Gabbay, Pitts and their collaborators.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 16:41:40 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 19:36:07 GMT"}, {"version": "v3", "created": "Fri, 22 Mar 2019 16:05:58 GMT"}, {"version": "v4", "created": "Tue, 3 Sep 2019 14:52:08 GMT"}, {"version": "v5", "created": "Wed, 4 Sep 2019 10:21:35 GMT"}, {"version": "v6", "created": "Fri, 11 Oct 2019 08:36:41 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Aceto", "Luca", ""], ["F\u00e1bregas", "Ignacio", ""], ["Garc\u00eda-P\u00e9rez", "\u00c1lvaro", ""], ["Ing\u00f3lfsd\u00f3ttir", "Anna", ""], ["Ortega-Mall\u00e9n", "Yolanda", ""]]}, {"id": "1807.02132", "submitter": "David Van Horn", "authors": "Niki Vazou, \\'Eric Tanter, David Van Horn", "title": "Gradual Liquid Type Inference", "comments": "To appear at OOPSLA 2018", "journal-ref": null, "doi": "10.1145/3276502", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Liquid typing provides a decidable refinement inference mechanism that is\nconvenient but subject to two major issues: (1) inference is global and\nrequires top-level annotations, making it unsuitable for inference of modular\ncode components and prohibiting its applicability to library code, and (2)\ninference failure results in obscure error messages. These difficulties\nseriously hamper the migration of existing code to use refinements. This paper\nshows that gradual liquid type inference---a novel combination of liquid\ninference and gradual refinement types---addresses both issues. Gradual\nrefinement types, which support imprecise predicates that are optimistically\ninterpreted, can be used in argument positions to constrain liquid inference so\nthat the global inference process e effectively infers modular specifications\nusable for library components. Dually, when gradual refinements appear as the\nresult of inference, they signal an inconsistency in the use of static\nrefinements. Because liquid refinements are drawn from a nite set of\npredicates, in gradual liquid type inference we can enumerate the safe\nconcretizations of each imprecise refinement, i.e. the static refinements that\njustify why a program is gradually well-typed. This enumeration is useful for\nstatic liquid type error explanation, since the safe concretizations exhibit\nall the potential inconsistencies that lead to static type errors. We develop\nthe theory of gradual liquid type inference and explore its pragmatics in the\nsetting of Liquid Haskell.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 18:13:16 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 11:54:19 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Vazou", "Niki", ""], ["Tanter", "\u00c9ric", ""], ["Van Horn", "David", ""]]}, {"id": "1807.02358", "submitter": "Beniamino Accattoli", "authors": "Beniamino Accattoli, St\\'ephane Graham-Lengrand, Delia Kesner", "title": "Tight Typings and Split Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi types---aka non-idempotent intersection types---have been used to\nobtain quantitative bounds on higher-order programs, as pioneered by de\nCarvalho. Notably, they bound at the same time the number of evaluation steps\nand the size of the result. Recent results show that the number of steps can be\ntaken as a reasonable time complexity measure. At the same time, however, these\nresults suggest that multi types provide quite lax complexity bounds, because\nthe size of the result can be exponentially bigger than the number of steps.\n  Starting from this observation, we refine and generalise a technique\nintroduced by Bernadet & Graham-Lengrand to provide exact bounds for the\nmaximal strategy. Our typing judgements carry two counters, one measuring\nevaluation lengths and the other measuring result sizes. In order to emphasise\nthe modularity of the approach, we provide exact bounds for four evaluation\nstrategies, both in the lambda-calculus (head, leftmost-outermost, and maximal\nevaluation) and in the linear substitution calculus (linear head evaluation).\n  Our work aims at both capturing the results in the literature and extending\nthem with new outcomes. Concerning the literature, it unifies de Carvalho and\nBernadet & Graham-Lengrand via a uniform technique and a complexity-based\nperspective. The two main novelties are exact split bounds for the leftmost\nstrategy---the only known strategy that evaluates terms to full normal forms\nand provides a reasonable complexity measure---and the observation that the\ncomputing device hidden behind multi types is the notion of substitution at a\ndistance, as implemented by the linear substitution calculus.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 11:20:52 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Accattoli", "Beniamino", ""], ["Graham-Lengrand", "St\u00e9phane", ""], ["Kesner", "Delia", ""]]}, {"id": "1807.02554", "submitter": "Serhiy Semerikov", "authors": "S. O. Semerikov, O. P. Polishchuk", "title": "Methodic of joint using the tools of automation of lexical and parsing\n  analysis in the process of teaching the programming theory of future\n  informatics teachers", "comments": "27 pages, 2 tables, in Ukrainian", "journal-ref": "Theory and methods of learning mathematics, physics, informatics\n  13, 2 (2015) 174-200", "doi": null, "report-no": null, "categories": "cs.PL cs.CY cs.FL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The place and role of parsing analysis in formation of professional\ninformatics competences of future informatics teachers is determined. Separated\nautomation tools for lexical (lex) and syntax (yacc) analysis invariant to the\nprogramming language used. The expediency of using functional programming\nlanguages Scheme and SML is shown for learning how to develop compilers in the\ncourse of programming theory. The example of the MosML dialect illustrates the\nmain components of the methodic of joint using the tools of automation of\nlexical and parsing analysis in the process of teaching the programming theory\nof future informatics teachers. The main conclusions and recommendations: 1)\nthe considered example of the expanded calculator can be refined by changing\nthe grammar, in particular - for the introduction of conditional and cyclic\nconstructions; 2) the proposed scheme can be used to implement the interpreter\nof any formal language with an arbitrary typing method - the appropriate\nexamples of study will be subsets of procedural languages Basic and C and\nfunctional languages Scheme and SML: provided the addition of the machine code\ngeneration phase, this provides an opportunity to demonstrate the full\ndevelopment cycle for programming language compiler.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 18:15:43 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Semerikov", "S. O.", ""], ["Polishchuk", "O. P.", ""]]}, {"id": "1807.02786", "submitter": "Max New", "authors": "Max S. New and Amal Ahmed", "title": "Graduality from Embedding-projection Pairs (Extended Version)", "comments": "Extended version of paper accepted to ICFP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gradually typed languages allow statically typed and dynamically typed code\nto interact while maintaining benefits of both styles. The key to reasoning\nabout these mixed programs is Siek-Vitousek-Cimini-Boyland's (dynamic) gradual\nguarantee, which says that giving components of a program more precise types\nonly adds runtime type checking, and does not otherwise change behavior. In\nthis paper, we give a semantic reformulation of the gradual guarantee called\ngraduality. We change the name to promote the analogy that graduality is to\ngradual typing what parametricity is to polymorphism. Each gives a\nlocal-to-global, syntactic-to-semantic reasoning principle that is formulated\nin terms of a kind of observational approximation.\n  Utilizing the analogy, we develop a novel logical relation for proving\ngraduality. We show that embedding-projection pairs (ep pairs) are to\ngraduality what relations are to parametricity. We argue that casts between two\ntypes where one is \"more dynamic\" (less precise) than the other necessarily\nform an ep pair, and we use this to cleanly prove the graduality cases for\ncasts from the ep-pair property. To construct ep pairs, we give an analysis of\nthe type dynamism relation (also known as type precision or naive subtyping)\nthat interprets the rules for type dynamism as compositional constructions on\nep pairs, analogous to the coercion interpretation of subtyping.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 09:14:38 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["New", "Max S.", ""], ["Ahmed", "Amal", ""]]}, {"id": "1807.02809", "submitter": "Ryan Culpepper", "authors": "Mitchell Wand, Ryan Culpepper, Theophilos Giannakopoulos, Andrew Cobb", "title": "Contextual Equivalence for a Probabilistic Language with Continuous\n  Random Variables and Recursion", "comments": "Extended version of ICFP 2018 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a complete reasoning principle for contextual equivalence in an\nuntyped probabilistic language. The language includes continuous (real-valued)\nrandom variables, conditionals, and scoring. It also includes recursion, since\nthe standard call-by-value fixpoint combinator is expressible. We demonstrate\nthe usability of our characterization by proving several equivalence schemas,\nincluding familiar facts from lambda calculus as well as results specific to\nprobabilistic programming. In particular, we use it to prove that reordering\nthe random draws in a probabilistic program preserves contextual equivalence.\nThis allows us to show, for example, that (let x = $e_1$ in let y = $e_2$ in\n$e_0$) is equivalent to (let y = $e_2$ in let x = $e_1$ in $e_0$) (provided $x$\ndoes not occur free in $e_2$ and $y$ does not occur free in $e_1$) despite the\nfact that $e_1$ and $e_2$ may have sampling and scoring effects.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 12:55:02 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Wand", "Mitchell", ""], ["Culpepper", "Ryan", ""], ["Giannakopoulos", "Theophilos", ""], ["Cobb", "Andrew", ""]]}, {"id": "1807.02957", "submitter": "Ariyam Das", "authors": "Tyson Condie, Ariyam Das, Matteo Interlandi, Alexander Shkapsky, Mohan\n  Yang, Carlo Zaniolo", "title": "Scaling-Up Reasoning and Advanced Analytics on BigData", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BigDatalog is an extension of Datalog that achieves performance and\nscalability on both Apache Spark and multicore systems to the point that its\ngraph analytics outperform those written in GraphX. Looking back, we see how\nthis realizes the ambitious goal pursued by deductive database researchers\nbeginning forty years ago: this is the goal of combining the rigor and power of\nlogic in expressing queries and reasoning with the performance and scalability\nby which relational databases managed Big Data. This goal led to Datalog which\nis based on Horn Clauses like Prolog but employs implementation techniques,\nsuch as Semi-naive Fixpoint and Magic Sets, that extend the bottom-up\ncomputation model of relational systems, and thus obtain the performance and\nscalability that relational systems had achieved, as far back as the 80s, using\ndata-parallelization on shared-nothing architectures. But this goal proved\ndifficult to achieve because of major issues at (i) the language level and (ii)\nat the system level. The paper describes how (i) was addressed by simple rules\nunder which the fixpoint semantics extends to programs using count, sum and\nextrema in recursion, and (ii) was tamed by parallel compilation techniques\nthat achieve scalability on multicore systems and Apache Spark. This paper is\nunder consideration for acceptance in Theory and Practice of Logic Programming\n(TPLP).\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 06:40:08 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Condie", "Tyson", ""], ["Das", "Ariyam", ""], ["Interlandi", "Matteo", ""], ["Shkapsky", "Alexander", ""], ["Yang", "Mohan", ""], ["Zaniolo", "Carlo", ""]]}, {"id": "1807.03100", "submitter": "Oleksandr Polozov", "authors": "Chenglong Wang, Kedar Tatwawadi, Marc Brockschmidt, Po-Sen Huang, Yi\n  Mao, Oleksandr Polozov, Rishabh Singh", "title": "Robust Text-to-SQL Generation with Execution-Guided Decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of neural semantic parsing, which translates natural\nlanguage questions into executable SQL queries. We introduce a new mechanism,\nexecution guidance, to leverage the semantics of SQL. It detects and excludes\nfaulty programs during the decoding procedure by conditioning on the execution\nof partially generated program. The mechanism can be used with any\nautoregressive generative model, which we demonstrate on four state-of-the-art\nrecurrent or template-based semantic parsing models. We demonstrate that\nexecution guidance universally improves model performance on various\ntext-to-SQL datasets with different scales and query complexity: WikiSQL, ATIS,\nand GeoQuery. As a result, we achieve new state-of-the-art execution accuracy\nof 83.8% on WikiSQL.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 13:20:28 GMT"}, {"version": "v2", "created": "Sun, 9 Sep 2018 21:55:52 GMT"}, {"version": "v3", "created": "Thu, 13 Sep 2018 00:29:17 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Wang", "Chenglong", ""], ["Tatwawadi", "Kedar", ""], ["Brockschmidt", "Marc", ""], ["Huang", "Po-Sen", ""], ["Mao", "Yi", ""], ["Polozov", "Oleksandr", ""], ["Singh", "Rishabh", ""]]}, {"id": "1807.03168", "submitter": "Maksym Zavershynskyi", "authors": "Maksym Zavershynskyi, Alex Skidanov, Illia Polosukhin", "title": "NAPS: Natural Program Synthesis Dataset", "comments": "4 pages, 5 tables in 2nd Workshop on Neural Abstract Machines &\n  Program Induction (NAMPI), @ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a program synthesis-oriented dataset consisting of human written\nproblem statements and solutions for these problems. The problem statements\nwere collected via crowdsourcing and the program solutions were extracted from\nhuman-written solutions in programming competitions, accompanied by\ninput/output examples. We propose using this dataset for the program synthesis\ntasks aimed for working with real user-generated data. As a baseline we present\nfew models, with the best model achieving 8.8% accuracy, showcasing both the\ncomplexity of the dataset and large room for future research.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 02:59:34 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Zavershynskyi", "Maksym", ""], ["Skidanov", "Alex", ""], ["Polosukhin", "Illia", ""]]}, {"id": "1807.03280", "submitter": "Chao Wang", "authors": "Shengjian Guo and Meng Wu and Chao Wang", "title": "Adversarial Symbolic Execution for Detecting Concurrency-Related Cache\n  Timing Leaks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DC cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The timing characteristics of cache, a high-speed storage between the fast\nCPU and the slowmemory, may reveal sensitive information of a program, thus\nallowing an adversary to conduct side-channel attacks. Existing methods for\ndetecting timing leaks either ignore cache all together or focus only on\npassive leaks generated by the program itself, without considering leaks that\nare made possible by concurrently running some other threads. In this work, we\nshow that timing-leak-freedom is not a compositional property: a program that\nis not leaky when running alone may become leaky when interleaved with other\nthreads. Thus, we develop a new method, named adversarial symbolic execution,\nto detect such leaks. It systematically explores both the feasible program\npaths and their interleavings while modeling the cache, and leverages an SMT\nsolver to decide if there are timing leaks. We have implemented our method in\nLLVM and evaluated it on a set of real-world ciphers with 14,455 lines of C\ncode in total. Our experiments demonstrate both the efficiency of our method\nand its effectiveness in detecting side-channel leaks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 17:32:09 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Guo", "Shengjian", ""], ["Wu", "Meng", ""], ["Wang", "Chao", ""]]}, {"id": "1807.03329", "submitter": "Chao Wang", "authors": "Chungha Sung and Brandon Paulsen and Chao Wang", "title": "CANAL: A Cache Timing Analysis Framework via LLVM Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A unified modeling framework for non-functional properties of a program is\nessential for research in software analysis and verification, since it reduces\nburdens on individual researchers to implement new approaches and compare\nexisting approaches. We present CANAL, a framework that models the cache\nbehaviors of a program by transforming its intermediate representation in the\nLLVM compiler. CANAL inserts auxiliary variables and instructions over these\nvariables, to allow standard verification tools to handle a new class of cache\nrelated properties, e.g., for computing the worst-case execution time and\ndetecting side-channel leaks. We demonstrate the effectiveness of CANAL using\nthree verification tools: KLEE, SMACK and Crab-llvm. We confirm the accuracy of\nour cache model by comparing with CPU cycle-accurate simulation results of\nGEM5. CANAL is available on GitHub and YouTube.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 18:27:22 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Sung", "Chungha", ""], ["Paulsen", "Brandon", ""], ["Wang", "Chao", ""]]}, {"id": "1807.03566", "submitter": "Irene C\\'ordoba", "authors": "Irene C\\'ordoba and Juan de Lara", "title": "A modelling language for the effective design of Java annotations", "comments": "6 pages, 6 figures, 2015 conference", "journal-ref": "Proceedings of the 30th Annual ACM Symposium on Applied Computing\n  (SAC 2015), 2087-2092, 2015", "doi": "10.1145/2695664.2695717", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new modelling language for the effective design of\nJava annotations. Since their inclusion in the 5th edition of Java, annotations\nhave grown from a useful tool for the addition of meta-data to play a central\nrole in many popular software projects. Usually they are conceived as sets with\ndependency and integrity constraints within them; however, the native support\nprovided by Java for expressing this design is very limited. To overcome its\ndeficiencies and make explicit the rich conceptual model which lies behind a\nset of annotations, we propose a domain-specific modelling language. The\nproposal has been implemented as an Eclipse plug-in, including an editor and an\nintegrated code generator that synthesises annotation processors. The language\nhas been tested using a real set of annotations from the Java Persistence API\n(JPA). It has proven to cover a greater scope with respect to other related\nwork in different shared areas of application.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 10:59:37 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["C\u00f3rdoba", "Irene", ""], ["de Lara", "Juan", ""]]}, {"id": "1807.03585", "submitter": "Martin Sulzmann", "authors": "Martin Sulzmann and Kai Stadtmueller", "title": "Two-Phase Dynamic Analysis of Message-Passing Go Programs based on\n  Vector Clocks", "comments": "further details compared to PPDP'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the run-time behavior of concurrent programs is a challenging\ntask. A popular approach is to establish a happens- before relation via vector\nclocks. Thus, we can identify bugs and per- formance bottlenecks, for example,\nby checking if two con icting events may happen concurrently. We employ a\ntwo-phase method to derive vector clock information for a wide range of\nconcurrency features that includes all of the message-passing features in Go.\nThe rst phase (instrumentation and tracing) yields a run-time trace that\nrecords all events related to message-passing concurrency that took place. The\nsecond phase (trace replay) is carried out o ine and replays the recorded\ntraces to infer vector clock information. Trace replay operates on thread-local\ntraces. Thus, we can observe behav- ior that might result from some alternative\nschedule. Our approach is not tied to any speci c language. We have built a\nprototype for the Go programming language and provide empirical evidence of the\nusefulness of our method.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 12:06:59 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 09:10:29 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Sulzmann", "Martin", ""], ["Stadtmueller", "Kai", ""]]}, {"id": "1807.03703", "submitter": "Stefan Muller", "authors": "Stefan K. Muller, Umut A. Acar, Robert Harper", "title": "Competitive Parallelism: Getting Your Priorities Right", "comments": "Extended version of a paper to appear at ICFP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-threaded programs have traditionally fallen into one of two domains:\ncooperative and competitive. These two domains have traditionally remained\nmostly disjoint, with cooperative threading used for increasing throughput in\ncompute-intensive applications such as scientific workloads and cooperative\nthreading used for increasing responsiveness in interactive applications such\nas GUIs and games. As multicore hardware becomes increasingly mainstream, there\nis a need for bridging these two disjoint worlds, because many applications mix\ninteraction and computation and would benefit from both cooperative and\ncompetitive threading.\n  In this paper, we present techniques for programming and reasoning about\nparallel interactive applications that can use both cooperative and competitive\nthreading. Our techniques enable the programmer to write rich parallel\ninteractive programs by creating and synchronizing with threads as needed, and\nby assigning threads user-defined and partially ordered priorities. To ensure\nimportant responsiveness properties, we present a modal type system analogous\nto S4 modal logic that precludes low-priority threads from delaying\nhigh-priority threads, thereby statically preventing a crucial set of\npriority-inversion bugs. We then present a cost model that allows reasoning\nabout responsiveness and completion time of well-typed programs. The cost model\nextends the traditional work-span model for cooperative threading to account\nfor competitive scheduling decisions needed to ensure responsiveness. Finally,\nwe show that our proposed techniques are realistic by implementing them as an\nextension to the Standard ML language.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 15:16:41 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Muller", "Stefan K.", ""], ["Acar", "Umut A.", ""], ["Harper", "Robert", ""]]}, {"id": "1807.03732", "submitter": "EPTCS", "authors": "Robert Atkey (University of Strathclyde), Sam Lindley (University of\n  Edinburgh)", "title": "Proceedings of the 7th Workshop on Mathematically Structured Functional\n  Programming", "comments": null, "journal-ref": "EPTCS 275, 2018", "doi": "10.4204/EPTCS.275", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seventh workshop on Mathematically Structured Functional Programming is\ndevoted to the derivation of functionality from structure. It is a celebration\nof the direct impact of Theoretical Computer Science on programs as we write\nthem today. Modern programming languages, and in particular functional\nlanguages, support the direct expression of mathematical structures, equipping\nprogrammers with tools of remarkable power and abstraction. Where would Haskell\nbe without monads? Functional reactive programming without arrows?\nCall-by-push-value without adjunctions? The list goes on. This workshop is a\nforum for researchers who seek to reflect mathematical phenomena in data and\ncontrol.\n  The seventh workshop on Mathematically Structured Functional Programming was\nheld on 8th July 2018 affiliated with FSCD 2018 as part of FLoC 2018 in Oxford,\nUK.\n  There were two invited talks. In addition four full papers and two extended\nabstracts were selected by the programme committee for presentation.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 16:07:51 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Atkey", "Robert", "", "University of Strathclyde"], ["Lindley", "Sam", "", "University of\n  Edinburgh"]]}, {"id": "1807.03777", "submitter": "Chao Wang", "authors": "Chungha Sung and Shuvendu Lahiri and Constantin Enea and Chao Wang", "title": "Datalog-based Scalable Semantic Diffing of Concurrent Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When an evolving program is modified to address issues related to thread\nsynchronization, there is a need to confirm the change is correct, i.e., it\ndoes not introduce unexpected behavior. However, manually comparing two\nprograms to identify the semantic difference is labor intensive and error\nprone, whereas techniques based on model checking are computationally\nexpensive. To fill the gap, we develop a fast and approximate static analysis\nfor computing synchronization differences of two programs. The method is fast\nbecause, instead of relying on heavy-weight model checking techniques, it\nleverages a polynomial-time Datalog-based program analysis framework to compute\ndifferentiating data-flow edges, i.e., edges allowed by one program but not the\nother. Although approximation is used our method is sufficiently accurate due\nto careful design of the Datalog inference rules and iterative increase of the\nrequired data-flow edges for representing a difference. We have implemented our\nmethod and evaluated it on a large number of multithreaded C programs to\nconfirm its ability to produce, often within seconds, the same differences\nobtained by human; in contrast, prior techniques based on model checking take\nminutes or even hours and thus can be 10x to 1000x slower.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 17:59:51 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 08:36:06 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Sung", "Chungha", ""], ["Lahiri", "Shuvendu", ""], ["Enea", "Constantin", ""], ["Wang", "Chao", ""]]}, {"id": "1807.04084", "submitter": "EPTCS", "authors": "Exequiel Rivas (INRIA)", "title": "Relating Idioms, Arrows and Monads from Monoidal Adjunctions", "comments": "In Proceedings MSFP 2018, arXiv:1807.03732", "journal-ref": "EPTCS 275, 2018, pp. 18-33", "doi": "10.4204/EPTCS.275.3", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit once again the connection between three notions of computation:\nmonads, arrows and idioms (also called applicative functors). We employ\nmonoidal categories of finitary functors and profunctors on finite sets as\nmodels of these notions of computation, and develop the connections between\nthem through adjunctions. As a result, we obtain a categorical version of\nLindley, Yallop and Wadler's characterisation of monads and idioms as arrows\nsatisfying an isomorphism.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 11:39:07 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Rivas", "Exequiel", "", "INRIA"]]}, {"id": "1807.04085", "submitter": "EPTCS", "authors": "Conor McBride (University of Strathclyde)", "title": "Everybody's Got To Be Somewhere", "comments": "In Proceedings MSFP 2018, arXiv:1807.03732", "journal-ref": "EPTCS 275, 2018, pp. 53-69", "doi": "10.4204/EPTCS.275.6", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key to any nameless representation of syntax is how it indicates the\nvariables we choose to use and thus, implicitly, those we discard. Standard de\nBruijn representations delay discarding maximally till the leaves of terms\nwhere one is chosen from the variables in scope at the expense of the rest.\nConsequently, introducing new but unused variables requires term traversal.\nThis paper introduces a nameless 'co-de-Bruijn' representation which makes the\nopposite canonical choice, delaying discarding minimally, as near as possible\nto the root. It is literate Agda: dependent types make it a practical joy to\nexpress and be driven by strong intrinsic invariants which ensure that scope is\naggressively whittled down to just the support of each subterm, in which every\nremaining variable occurs somewhere. The construction is generic, delivering a\nuniverse of syntaxes with higher-order metavariables, for which the appropriate\nnotion of substitution is hereditary. The implementation of simultaneous\nsubstitution exploits tight scope control to avoid busywork and shift terms\nwithout traversal. Surprisingly, it is also intrinsically terminating, by\nstructural recursion alone.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 11:39:29 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["McBride", "Conor", "", "University of Strathclyde"]]}, {"id": "1807.04596", "submitter": "\\'Eric Tanter", "authors": "Elizabeth Labrada, Mat\\'ias Toro, \\'Eric Tanter", "title": "Gradual System F", "comments": "Journal submission, extends and subsumes POPL'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bringing the benefits of gradual typing to a language with parametric\npolymorphism like System F, while preserving relational parametricity, has\nproven extremely challenging: first attempts were formulated a decade ago, and\nseveral designs have been recently proposed, with varying syntax, behavior, and\nproperties. Starting from a detailed review of the challenges and tensions that\naffect the design of gradual parametric languages, this work presents an\nextensive account of the semantics and metatheory of GSF, a gradual counterpart\nof System F. In doing so, we also report on the extent to which the Abstracting\nGradual Typing methodology can help us derive such a language. Among gradual\nparametric languages that follow the syntax of System F, GSF achieves a unique\ncombination of properties. We clearly establish the benefits and limitations of\nthe language, and discuss several extensions of GSF towards a practical\nprogramming language.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 13:27:16 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 06:22:14 GMT"}, {"version": "v3", "created": "Mon, 21 Jan 2019 18:01:21 GMT"}, {"version": "v4", "created": "Thu, 24 Jan 2019 10:55:36 GMT"}, {"version": "v5", "created": "Fri, 29 May 2020 15:05:57 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Labrada", "Elizabeth", ""], ["Toro", "Mat\u00edas", ""], ["Tanter", "\u00c9ric", ""]]}, {"id": "1807.04603", "submitter": "Catalin Hritcu", "authors": "Carmine Abate and Roberto Blanco and Deepak Garg and Catalin Hritcu\n  and Marco Patrignani and J\\'er\\'emy Thibault", "title": "Journey Beyond Full Abstraction: Exploring Robust Property Preservation\n  for Secure Compilation", "comments": "Long version of CSF'19 paper, including online appendix", "journal-ref": null, "doi": "10.1109/CSF.2019.00025", "report-no": null, "categories": "cs.PL cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  (CROPPED TO FIT IN ARXIV'S SILLY LIMIT. SEE PDF FOR COMPLETE ABSTRACT.)\n  We are the first to thoroughly explore a large space of formal secure\ncompilation criteria based on robust property preservation, i.e., the\npreservation of properties satisfied against arbitrary adversarial contexts. We\nstudy robustly preserving various classes of trace properties such as safety,\nof hyperproperties such as noninterference, and of relational hyperproperties\nsuch as trace equivalence. This leads to many new secure compilation criteria,\nsome of which are easier to practically achieve and prove than full\nabstraction, and some of which provide strictly stronger security guarantees.\nFor each of the studied criteria we propose an equivalent \"property-free\"\ncharacterization that clarifies which proof techniques apply. For relational\nproperties and hyperproperties, which relate the behaviors of multiple\nprograms, our formal definitions of the property classes themselves are novel.\nWe order our criteria by their relative strength and show several collapses and\nseparation results. Finally, we adapt existing proof techniques to show that\neven the strongest of our secure compilation criteria, the robust preservation\nof all relational hyperproperties, is achievable for a simple translation from\na statically typed to a dynamically typed language.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 13:38:32 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 14:38:58 GMT"}, {"version": "v3", "created": "Mon, 23 Jul 2018 20:49:16 GMT"}, {"version": "v4", "created": "Fri, 2 Nov 2018 07:00:58 GMT"}, {"version": "v5", "created": "Wed, 27 Feb 2019 12:22:42 GMT"}, {"version": "v6", "created": "Fri, 17 May 2019 15:34:40 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Abate", "Carmine", ""], ["Blanco", "Roberto", ""], ["Garg", "Deepak", ""], ["Hritcu", "Catalin", ""], ["Patrignani", "Marco", ""], ["Thibault", "J\u00e9r\u00e9my", ""]]}, {"id": "1807.05091", "submitter": "Justin Hsu", "authors": "Arthur Azevedo de Amorim, Marco Gaboardi, Justin Hsu, Shin-ya\n  Katsumata", "title": "Probabilistic Relational Reasoning via Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fuzz programming language [Reed and Pierce, 2010] uses an elegant linear\ntype system combined with a monad-like type to express and reason about\nprobabilistic sensitivity properties, most notably $\\epsilon$-differential\nprivacy. We show how to extend Fuzz to capture more general relational\nproperties of probabilistic programs, with approximate, or $(\\epsilon,\n\\delta)$-differential privacy serving as a leading example. Our technical\ncontributions are threefold. First, we introduce the categorical notion of\ncomonadic lifting of a monad to model composition properties of probabilistic\ndivergences. Then, we show how to express relational properties in terms of\nsensitivity properties via an adjunction we call the path construction.\nFinally, we instantiate our semantics to model the terminating fragment of Fuzz\nextended with types carrying information about other divergences between\ndistributions.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 14:01:05 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 00:34:03 GMT"}, {"version": "v3", "created": "Fri, 19 Apr 2019 03:53:29 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["de Amorim", "Arthur Azevedo", ""], ["Gaboardi", "Marco", ""], ["Hsu", "Justin", ""], ["Katsumata", "Shin-ya", ""]]}, {"id": "1807.05923", "submitter": "Andrej Bauer", "authors": "Andrej Bauer", "title": "What is algebraic about algebraic effects and handlers?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note recapitulates and expands the contents of a tutorial on the\nmathematical theory of algebraic effects and handlers which I gave at the\nDagstuhl seminar 18172 \"Algebraic effect handlers go mainstream\". It is\ntargeted roughly at the level of a doctoral student with some amount of\nmathematical training, or at anyone already familiar with algebraic effects and\nhandlers as programming concepts who would like to know what they have to do\nwith algebra. We draw an uninterrupted line of thought between algebra and\ncomputational effects. We begin on the mathematical side of things, by\nreviewing the classic notions of universal algebra: signatures, algebraic\ntheories, and their models. We then generalize and adapt the theory so that it\napplies to computational effects. In the last step we replace traditional\nmathematical notation with one that is closer to programming languages.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 15:34:07 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 09:25:25 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Bauer", "Andrej", ""]]}, {"id": "1807.06230", "submitter": "Sergei Khashin", "authors": "S. I. Khashin, and S. E. Vaganov", "title": "Genetic algorithms in Forth", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for automatically finding a program (bytecode) realizing the given\nalgorithm is developed. The algorithm is specified as a set of tests\n(input\\_data) $ \\rightarrow $ (output\\_data). Genetic methods made it possible\nto find the implementation of relatively complex algorithms: sorting, decimal\ndigits, GCD, LCM, factorial, prime divisors, binomial coefficients, and others.\nThe algorithms are implemented on a highly simplified version of Forth\nlanguage.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 05:34:07 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Khashin", "S. I.", ""], ["Vaganov", "S. E.", ""]]}, {"id": "1807.06702", "submitter": "Gabriel Scherer", "authors": "Fr\\'ed\\'eric Bour, Thomas Refis, Gabriel Scherer", "title": "Merlin: A Language Server for OCaml (Experience Report)", "comments": null, "journal-ref": null, "doi": "10.1145/3236798", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We report on the experience of developing Merlin, a language server for the\nOCaml programming language in development since 2013. Merlin is a daemon that\nconnects to your favourite text editor and provides services that require a\nfine-grained understanding of the programming language syntax and static\nsemantics: instant feedback on warnings and errors, autocompletion, `type of\nthe code under the cursor', `go to definition', etc.\n  Language servers need to handle incomplete and partially-incorrect programs,\nand try to be incremental to minimize recomputation after small editing\nactions. Merlin was built by carefully adapting the existing tools (the\nOCamllex lexer and Menhir parser generators) to better support incrementality,\nincompleteness and error handling. These extensions are elegant and general, as\ndemonstrated by the interesting, unplanned uses that the OCaml community found\nfor them. They could be adapted to other frontends -- in any language.\n  Besides incrementality, we discuss the way Merlin communicates with editors,\ndescribe the design decisions that went into some demanding features and report\non some of the non-apparent difficulties in building good editor support,\nemerging from expressive programming languages or frustrating tooling\necosystems.\n  We expect this experience report to be of interest to authors of interactive\nlanguage tooling for any programming language; many design choices may be\nreused, and some hard-won lessons can serve as warnings.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 23:06:37 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 10:06:23 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Bour", "Fr\u00e9d\u00e9ric", ""], ["Refis", "Thomas", ""], ["Scherer", "Gabriel", ""]]}, {"id": "1807.06735", "submitter": "Rafael Auler", "authors": "Maksim Panchenko, Rafael Auler, Bill Nell, Guilherme Ottoni", "title": "BOLT: A Practical Binary Optimizer for Data Centers and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance optimization for large-scale applications has recently become\nmore important as computation continues to move towards data centers.\nData-center applications are generally very large and complex, which makes code\nlayout an important optimization to improve their performance. This has\nmotivated recent investigation of practical techniques to improve code layout\nat both compile time and link time. Although post-link optimizers had some\nsuccess in the past, no recent work has explored their benefits in the context\nof modern data-center applications.\n  In this paper, we present BOLT, a post-link optimizer built on top of the\nLLVM framework. Utilizing sample-based profiling, BOLT boosts the performance\nof real-world applications even for highly optimized binaries built with both\nfeedback-driven optimizations (FDO) and link-time optimizations (LTO). We\ndemonstrate that post-link performance improvements are complementary to\nconventional compiler optimizations, even when the latter are done at a\nwhole-program level and in the presence of profile information. We evaluated\nBOLT on both Facebook data-center workloads and open-source compilers. For\ndata-center applications, BOLT achieves up to 8.0% performance speedups on top\nof profile-guided function reordering and LTO. For the GCC and Clang compilers,\nour evaluation shows that BOLT speeds up their binaries by up to 20.4% on top\nof FDO and LTO, and up to 52.1% if the binaries are built without FDO and LTO.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 01:54:11 GMT"}, {"version": "v2", "created": "Fri, 12 Oct 2018 19:31:03 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Panchenko", "Maksim", ""], ["Auler", "Rafael", ""], ["Nell", "Bill", ""], ["Ottoni", "Guilherme", ""]]}, {"id": "1807.07022", "submitter": "Ilya Sergey", "authors": "Nadia Polikarpova and Ilya Sergey", "title": "Structuring the Synthesis of Heap-Manipulating Programs - Extended\n  Version", "comments": null, "journal-ref": "Proc. ACM Program. Lang. 3, POPL, Article 72 (January 2019)", "doi": "10.1145/3290385", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a deductive approach to synthesizing imperative programs\nwith pointers from declarative specifications expressed in Separation Logic.\nOur synthesis algorithm takes as input a pair of assertions---a pre- and a\npostcondition---which describe two states of the symbolic heap, and derives a\nprogram that transforms one state into the other, guided by the shape of the\nheap. The program synthesis algorithm rests on the novel framework of Synthetic\nSeparation Logic (SSL), which generalises the classical notion of heap\nentailment $\\mathcal{P} \\vdash \\mathcal{Q}$ to incorporate a possibility of\ntransforming a heap satisfying an assertion $\\mathcal{P}$ into a heap\nsatisfying an assertion $\\mathcal{Q}$. A synthesized program represents a proof\nterm for a transforming entailment statement $\\mathcal{P} \\leadsto\n\\mathcal{Q}$, and the synthesis procedure corresponds to a proof search. The\nderived programs are, thus, correct by construction, in the sense that they\nsatisfy the ascribed pre/postconditions, and are accompanied by complete proof\nderivations, which can be checked independently.\n  We have implemented a proof search engine for SSL in a form the program\nsynthesizer called SuSLik. For efficiency, the engine exploits properties of\nSSL rules, such as invertibility and commutativity of rule applications on\nseparate heaps, to prune the space of derivations it has to consider. We\nexplain and showcase the use of SSL on characteristic examples, describe the\ndesign of SuSLik, and report on our experience of using it to synthesize a\nseries of benchmark programs manipulating heap-based linked data structures.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 16:21:32 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 02:39:35 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Polikarpova", "Nadia", ""], ["Sergey", "Ilya", ""]]}, {"id": "1807.07165", "submitter": "Kevin Moran P", "authors": "Kevin Moran, Carlos Bernal Cardenas, Mario Linares Vasquez, Denys\n  Poshyvanyk", "title": "Overcoming Language Dichotomies: Toward Effective Program Comprehension\n  for Mobile App Development", "comments": "Invited Keynote Paper for the 26th IEEE/ACM International Conference\n  on Program Comprehension (ICPC'18)", "journal-ref": null, "doi": "10.1145/3196321.3196322", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile devices and platforms have become an established target for modern\nsoftware developers due to performant hardware and a large and growing user\nbase numbering in the billions. Despite their popularity, the software\ndevelopment process for mobile apps comes with a set of unique, domain-specific\nchallenges rooted in program comprehension. Many of these challenges stem from\ndeveloper difficulties in reasoning about different representations of a\nprogram, a phenomenon we define as a \"language dichotomy\". In this paper, we\nreflect upon the various language dichotomies that contribute to open problems\nin program comprehension and development for mobile apps. Furthermore, to help\nguide the research community towards effective solutions for these problems, we\nprovide a roadmap of directions for future work.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 21:34:47 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Moran", "Kevin", ""], ["Cardenas", "Carlos Bernal", ""], ["Vasquez", "Mario Linares", ""], ["Poshyvanyk", "Denys", ""]]}, {"id": "1807.07822", "submitter": "Valentin W\\\"ustholz", "authors": "Florentin Guth, Valentin W\\\"ustholz, Maria Christakis, Peter M\\\"uller", "title": "Specification Mining for Smart Contracts with Automatic Abstraction\n  Tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart contracts are programs that manage digital assets according to a\ncertain protocol, expressing for instance the rules of an auction.\nUnderstanding the possible behaviors of a smart contract is difficult, which\ncomplicates development, auditing, and the post-mortem analysis of attacks.\n  This paper presents the first specification mining technique for smart\ncontracts. Our technique extracts the possible behaviors of smart contracts\nfrom contract executions recorded on a blockchain and expresses them as finite\nautomata. A novel dependency analysis allows us to separate independent\ninteractions with a contract. Our technique tunes the abstractions for the\nautomata construction automatically based on configurable metrics, for\ninstance, to maximize readability or precision. We implemented our technique\nfor the Ethereum blockchain and evaluated its usability on several real-world\ncontracts.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 13:01:56 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Guth", "Florentin", ""], ["W\u00fcstholz", "Valentin", ""], ["Christakis", "Maria", ""], ["M\u00fcller", "Peter", ""]]}, {"id": "1807.07892", "submitter": "Anton Podkopaev", "authors": "Anton Podkopaev, Ori Lahav, Viktor Vafeiadis", "title": "Bridging the Gap between Programming Languages and Hardware Weak Memory\n  Models", "comments": null, "journal-ref": null, "doi": "10.1145/3290382", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new intermediate weak memory model, IMM, as a way of\nmodularizing the proofs of correctness of compilation from concurrent\nprogramming languages with weak memory consistency semantics to mainstream\nmulti-core architectures, such as POWER and ARM. We use IMM to prove the\ncorrectness of compilation from the promising semantics of Kang et al. to POWER\n(thereby correcting and improving their result) and ARMv7, as well as to the\nrecently revised ARMv8 model. Our results are mechanized in Coq, and to the\nbest of our knowledge, these are the first machine-verified compilation\ncorrectness results for models that are weaker than x86-TSO.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 15:29:26 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 20:50:15 GMT"}, {"version": "v3", "created": "Fri, 9 Nov 2018 16:57:25 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Podkopaev", "Anton", ""], ["Lahav", "Ori", ""], ["Vafeiadis", "Viktor", ""]]}, {"id": "1807.08015", "submitter": "Ant\\'onio Ravara", "authors": "Patr\\'icia Monteiro, Jo\\~ao Louren\\c{c}o, and Ant\\'onio Ravara", "title": "Uma an\\'alise comparativa de ferramentas de an\\'alise est\\'atica para\n  dete\\c{c}\\~ao de erros de mem\\'oria", "comments": "Article in Portuguese, accepted in the national informatics\n  conference INForum (http://inforum.org.pt/INForum2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  --- Portuguese version\n  As falhas de software est\\~ao com frequ\\^encia associadas a acidentes com\ngraves consequ\\^encias econ\\'omicas e/ou humanas, pelo que se torna imperioso\ninvestir na valida\\c{c}\\~ao do software, nomeadamente daquele que \\'e\ncr\\'itico. Este artigo endere\\c{c}a a tem\\'atica da qualidade do software\natrav\\'es de uma an\\'alise comparativa da usabilidade e efic\\'acia de quatro\nferramentas de an\\'alise est\\'atica de programas em C/C++. Este estudo permitiu\ncompreender o grande potencial e o elevado impacto que as ferramentas de\nan\\'alise est\\'atica podem ter na valida\\c{c}\\~ao e verifica\\c{c}\\~ao de\nsoftware. Como resultado complementar, foram identificados novos erros em\nprogramas de c\\'odigo aberto e com elevada popularidade, que foram reportados.\n  --- English version\n  Software bugs are frequently associated with accidents with serious\neconomical and/or human consequences, being thus imperative the investment in\nthe validation of software, namely of the critical one. This article addresses\nthe topic of software quality by making a comparative analysis of the usability\nand efficiency of four static analysis tools for C/C++ programs. This study\nallow to understand the big potential and high impact that these tools may have\nin the validation and verification of software. As a complementary result, we\nidentified new errors in very popular open source projects, which have been\nreported.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 20:12:24 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Monteiro", "Patr\u00edcia", ""], ["Louren\u00e7o", "Jo\u00e3o", ""], ["Ravara", "Ant\u00f3nio", ""]]}, {"id": "1807.08117", "submitter": "L\\'eo Stefanesco", "authors": "Paul-Andr\\'e Melli\\`es and L\\'eo Stefanesco", "title": "An Asynchronous soundness theorem for concurrent separation logic", "comments": "Full version of an extended abstract published at LICS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Concurrent separation logic (CSL) is a specification logic for concurrent\nimperative programs with shared memory and locks. In this paper, we develop a\nconcurrent and interactive account of the logic inspired by asynchronous game\nsemantics. To every program $C$, we associate a pair of asynchronous transition\nsystems $[C]_S$ and $[C]_L$ which describe the operational behavior of the Code\nwhen confronted to its Environment or Frame --- both at the level of machine\nstates ($S$) and of machine instructions and locks ($L$). We then establish\nthat every derivation tree $\\pi$ of a judgment $\\Gamma\\vdash\\{P\\}C\\{Q\\}$\ndefines a winning and asynchronous strategy $[\\pi]_{Sep}$ with respect to both\nasynchronous semantics $[C]_S$ and $[C]_L$. From this, we deduce an\nasynchronous soundness theorem for CSL, which states that the canonical map\n$\\mathcal{L}:[C]_S\\to[C]_L$ from the stateful semantics $[C]_S$ to the\nstateless semantics $[C]_L$ satisfies a basic fibrational property. We advocate\nthat this provides a clean and conceptual explanation for the usual soundness\ntheorem of CSL, including the absence of data races.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 10:01:36 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Melli\u00e8s", "Paul-Andr\u00e9", ""], ["Stefanesco", "L\u00e9o", ""]]}, {"id": "1807.08242", "submitter": "Georg Moser", "authors": "Martin Hofmann, Georg Moser", "title": "Analysis of Logarithmic Amortised Complexity", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel amortised resource analysis based on a potential-based\ntype system. This type system gives rise to logarithmic and polynomial bounds\non the runtime complexity and is the first such system to exhibit logarithmic\namortised complexity. We relate the thus obtained automatable amortised\nresource analysis to manual amortised analyses of self-adjusting data\nstructures, like splay trees, that can be found in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 06:14:55 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Hofmann", "Martin", ""], ["Moser", "Georg", ""]]}, {"id": "1807.08427", "submitter": "Umang Mathur", "authors": "Dileep Kini, Umang Mathur, Mahesh Viswanathan", "title": "Data Race Detection on Compressed Traces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting data races in program traces that have\nbeen compressed using straight line programs (SLP), which are special\ncontext-free grammars that generate exactly one string, namely the trace that\nthey represent. We consider two classical approaches to race detection ---\nusing the happens-before relation and the lockset discipline. We present\nalgorithms for both these methods that run in time that is linear in the size\nof the compressed, SLP representation. Typical program executions almost always\nexhibit patterns that lead to significant compression. Thus, our algorithms are\nexpected to result in large speedups when compared with analyzing the\nuncompressed trace. Our experimental evaluation of these new algorithms on\nstandard benchmarks confirms this observation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 04:47:48 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Kini", "Dileep", ""], ["Mathur", "Umang", ""], ["Viswanathan", "Mahesh", ""]]}, {"id": "1807.08576", "submitter": "Paulius Juodisius", "authors": "Paulius Juodisius (IT University of Copenhagen, Denmark), Atrisha\n  Sarkar (University of Waterloo, Canada), Raghava Rao Mukkamala (Department of\n  Technology, Kristiania University College, Norway), Michal Antkiewicz\n  (University of Waterloo, Canada), Krzysztof Czarnecki (University of\n  Waterloo, Canada), Andrzej Wasowski (IT University of Copenhagen, Denmark)", "title": "Clafer: Lightweight Modeling of Structure, Behaviour, and Variability", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2019, Vol. 3,\n  Issue 1, Article 2", "doi": "10.22152/programming-journal.org/2019/3/2", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedded software is growing fast in size and complexity, leading to intimate\nmixture of complex architectures and complex control. Consequently, software\nspecification requires modeling both structures and behaviour of systems.\nUnfortunately, existing languages do not integrate these aspects well, usually\nprioritizing one of them. It is common to develop a separate language for each\nof these facets. In this paper, we contribute Clafer: a small language that\nattempts to tackle this challenge. It combines rich structural modeling with\nstate of the art behavioural formalisms. We are not aware of any other modeling\nlanguage that seamlessly combines these facets common to system and software\nmodeling. We show how Clafer, in a single unified syntax and semantics, allows\ncapturing feature models (variability), component models, discrete control\nmodels (automata) and variability encompassing all these aspects. The language\nis built on top of first order logic with quantifiers over basic entities (for\nmodeling structures) combined with linear temporal logic (for modeling\nbehaviour). On top of this semantic foundation we build a simple but expressive\nsyntax, enriched with carefully selected syntactic expansions that cover\nhierarchical modeling, associations, automata, scenarios, and Dwyer's property\npatterns. We evaluate Clafer using a power window case study, and comparing it\nagainst other notations that substantially overlap with its scope (SysML, AADL,\nTemporal OCL and Live Sequence Charts), discussing benefits and perils of using\na single notation for the purpose.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 13:01:09 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Juodisius", "Paulius", "", "IT University of Copenhagen, Denmark"], ["Sarkar", "Atrisha", "", "University of Waterloo, Canada"], ["Mukkamala", "Raghava Rao", "", "Department of\n  Technology, Kristiania University College, Norway"], ["Antkiewicz", "Michal", "", "University of Waterloo, Canada"], ["Czarnecki", "Krzysztof", "", "University of\n  Waterloo, Canada"], ["Wasowski", "Andrzej", "", "IT University of Copenhagen, Denmark"]]}, {"id": "1807.08578", "submitter": "Patrick Rein", "authors": "Patrick Rein (Hasso Plattner Institute, Germany), Stefan Ramson (Hasso\n  Plattner Institute, Germany), Jens Lincke (Hasso Plattner Institute,\n  University of Potsdam, Germany), Robert Hirschfeld (Hasso-Plattner-Institut\n  (HPI), Germany), Tobias Pape (Hasso Plattner Institute, Germany)", "title": "Exploratory and Live, Programming and Coding: A Literature Study\n  Comparing Perspectives on Liveness", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2019, Vol. 3,\n  Issue 1, Article 1", "doi": "10.22152/programming-journal.org/2019/3/1", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various programming tools, languages, and environments give programmers the\nimpression of changing a program while it is running. This experience of\nliveness has been discussed for over two decades and a broad spectrum of\nresearch on this topic exists. Amongst others, this work has been carried out\nin the communities around three major ideas which incorporate liveness as an\nimportant aspect: live programming, exploratory programming, and live coding.\nWhile there have been publications on the focus of each particular community,\nthe overall spectrum of liveness across these three communities has not been\ninvestigated yet. Thus, we want to delineate the variety of research on\nliveness. At the same time, we want to investigate overlaps and differences in\nthe values and contributions between the three communities.\n  Therefore, we conducted a literature study with a sample of 212 publications\non the terms retrieved from three major indexing services. On this sample, we\nconducted a thematic analysis regarding the following aspects: motivation for\nliveness, application domains, intended outcomes of running a system, and types\nof contributions. We also gathered bibliographic information such as related\nkeywords and prominent publications.\n  Besides other characteristics the results show that the field of exploratory\nprogramming is mostly about technical designs and empirical studies on tools\nfor general-purpose programming. In contrast, publications on live coding have\nthe most variety in their motivations and methodologies with a majority being\nempirical studies with users. As expected, most publications on live coding are\napplied to performance art. Finally, research on live programming is mostly\nmotivated by making programming more accessible and easier to understand,\nevaluating their tool designs through empirical studies with users.\n  In delineating the spectrum of work on liveness, we hope to make the\nindividual communities more aware of the work of the others. Further, by giving\nan overview of the values and methods of the individual communities, we hope to\nprovide researchers new to the field of liveness with an initial overview.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 13:02:32 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Rein", "Patrick", "", "Hasso Plattner Institute, Germany"], ["Ramson", "Stefan", "", "Hasso\n  Plattner Institute, Germany"], ["Lincke", "Jens", "", "Hasso Plattner Institute,\n  University of Potsdam, Germany"], ["Hirschfeld", "Robert", "", "Hasso-Plattner-Institut"], ["Pape", "Tobias", "", "Hasso Plattner Institute, Germany"]]}, {"id": "1807.08711", "submitter": "David Darais", "authors": "David Darais, David Van Horn", "title": "Constructive Galois Connections", "comments": null, "journal-ref": "J. Funct. Prog. 29 (2019) e11", "doi": "10.1017/S0956796819000066", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Galois connections are a foundational tool for structuring abstraction in\nsemantics and their use lies at the heart of the theory of abstract\ninterpretation. Yet, mechanization of Galois connections using proof assistants\nremains limited to restricted modes of use, preventing their general\napplication in mechanized metatheory and certified programming.\n  This paper presents constructive Galois connections, a variant of Galois\nconnections that is effective both on paper and in proof assistants; is\ncomplete with respect to a large subset of classical Galois connections; and\nenables more general reasoning principles, including the \"calculational\" style\nadvocated by Cousot.\n  To design constructive Galois connections we identify a restricted mode of\nuse of classical ones which is both general and amenable to mechanization in\ndependently-typed functional programming languages. Crucial to our metatheory\nis the addition of monadic structure to Galois connections to control a\n\"specification effect.\" Effectful calculations may reason classically, while\npure calculations have extractable computational content. Explicitly moving\nbetween the worlds of specification and implementation is enabled by our\nmetatheory.\n  To validate our approach, we provide two case studies in mechanizing existing\nproofs from the literature: the first uses calculational abstract\ninterpretation to design a static analyzer; the second forms a semantic basis\nfor gradual typing. Both mechanized proofs closely follow their original\npaper-and-pencil counterparts, employ reasoning principles not captured by\nprevious mechanization approaches, support the extraction of verified\nalgorithms, and are novel.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 16:39:40 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Darais", "David", ""], ["Van Horn", "David", ""]]}, {"id": "1807.09175", "submitter": "Antonina Nepeivoda", "authors": "Antonina Nepeivoda", "title": "Supercompiling String Programs Using Word Equations as Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a general parameterized scheme of program and constraint analyses\nallowing us to specify both the program specialization method known as\nTurchin's supercompilation and Hmelevskii's algorithm solving the quadratic\nword equations. The scheme is specified for both sorts of the analysis and\nworks in a joint algorithm in which these two sorts of the analysis are used\ntogether. The word equations and the inequalities on regular patterns are used\nas the string constraint language in the algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 13:50:53 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Nepeivoda", "Antonina", ""]]}, {"id": "1807.09377", "submitter": "Kristopher Micinski", "authors": "Kristopher Micinski and Zhanpeng Wang and Thomas Gilray", "title": "Racets: Faceted Execution in Racket", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faceted Execution is a linguistic paradigm for dynamic information-flow\ncontrol. Under faceted execution, secure program data is represented by faceted\nvalues: decision trees that encode how the data should appear to its owner\n(represented by a label) versus everyone else. When labels are allowed to be\nfirst-class (i.e., predicates that decide at runtime which data to reveal),\nfaceted execution enables policy-agnostic programming: a programming style that\nallows privacy policies for data to be enforced independently of code that\ncomputes on that data.\n  To date, implementations of faceted execution are relatively heavyweight:\nrequiring either changing the language runtime or the application code (e.g.,\nby using monads). Following Racket's languages-as-libraries approach, we\npresent Racets: an implementation of faceted execution as a library of macros.\nGiven Racket's highly-expressive macro system, our implementation follows\nrelatively directly from the semantics of faceted execution. To demonstrate how\nRacets can be used for policy-agnostic programming, we use it to build a\nweb-based game of Battleship. Our implementation sheds light on several\ninteresting issues in interacting with code written without faceted execution.\nOur Racets implementation is open source, under development, and available\nonline.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 22:27:14 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Micinski", "Kristopher", ""], ["Wang", "Zhanpeng", ""], ["Gilray", "Thomas", ""]]}, {"id": "1807.09466", "submitter": "Mingsheng Ying", "authors": "Mingsheng Ying and Yuan Feng", "title": "Model Checking Quantum Systems --- A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article discusses the essential difficulties in developing\nmodel-checking techniques for quantum systems that are never present in model\nchecking classical systems. It further reviews some early researches on\nchecking quantum communication protocols as well as a new line of researches\npursued by the authors and their collaborators on checking general quantum\nsystems, applicable to both physical systems and quantum programs.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 07:58:14 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Ying", "Mingsheng", ""], ["Feng", "Yuan", ""]]}, {"id": "1807.09679", "submitter": "Mat\\'u\\v{s} Sul\\'ir", "authors": "Mat\\'u\\v{s} Sul\\'ir and Jaroslav Porub\\\"an", "title": "RuntimeSearch: Ctrl+F for a Running Program", "comments": null, "journal-ref": "Proceedings of the 32nd IEEE/ACM International Conference on\n  Automated Software Engineering (ASE), IEEE, 2017, pp. 388-393", "doi": "10.1109/ASE.2017.8115651", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developers often try to find occurrences of a certain term in a software\nsystem. Traditionally, a text search is limited to static source code files. In\nthis paper, we introduce a simple approach, RuntimeSearch, where the given term\nis searched in the values of all string expressions in a running program. When\na match is found, the program is paused and its runtime properties can be\nexplored with a traditional debugger. The feasibility and usefulness of\nRuntimeSearch is demonstrated on a medium-sized Java project.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 15:57:00 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Sul\u00edr", "Mat\u00fa\u0161", ""], ["Porub\u00e4n", "Jaroslav", ""]]}, {"id": "1807.10130", "submitter": "Elias Castegren", "authors": "Elias Castegren and Joel Wallin and Tobias Wrigstad", "title": "Bestow and Atomic: Concurrent Programming using Isolation, Delegation\n  and Grouping", "comments": null, "journal-ref": "Journal of Logical and Algebraic Methods in Programming, Vol. 100\n  (2018), 130-151", "doi": "10.1016/j.jlamp.2018.06.007", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any non-trivial concurrent system warrants synchronisation, regardless of the\nconcurrency model. Actor-based concurrency serialises all computations in an\nactor through asynchronous message passing. In contrast, lock-based concurrency\nserialises some computations by following a lock--unlock protocol for accessing\ncertain data. Both systems require sound reasoning about pointers and aliasing\nto exclude data-races. If actor isolation is broken, so is the\nsingle-thread-of-control abstraction. Similarly for locks, if a datum is\naccessible outside of the scope of the lock, the datum is not governed by the\nlock.\n  In this paper we discuss how to balance aliasing and synchronisation. In\nprevious work, we defined a type system that guarantees data-race freedom of\nactor-based concurrency and lock-based concurrency. This paper extends this\nwork by the introduction of two programming constructs; one for decoupling\nisolation and synchronisation and one for constructing higher-level atomicity\nguarantees from lower-level synchronisation. We focus predominantly on actors,\nand in particular the Encore programming language, but our ultimate goal is to\ndefine our constructs in such a way that they can be used both with locks and\nactors, given that combinations of both models occur frequently in actual\nsystems. We discuss the design space, provide several formalisations of\ndifferent semantics and discuss their properties, and connect them to case\nstudies showing how our proposed constructs can be useful. We also report on an\non-going implementation of our proposed constructs in Encore.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 13:42:38 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Castegren", "Elias", ""], ["Wallin", "Joel", ""], ["Wrigstad", "Tobias", ""]]}, {"id": "1807.10664", "submitter": "Roberto Metere", "authors": "Roberto Metere, Andreas Lindner, Roberto Guanciale", "title": "Sound Transpilation from Binary to Machine-Independent Code", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-70848-5_13", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to handle the complexity and heterogeneity of mod- ern instruction\nset architectures, analysis platforms share a common design, the adoption of\nhardware-independent intermediate representa- tions. The usage of these\nplatforms to verify systems down to binary-level is appealing due to the high\ndegree of automation they provide. How- ever, it introduces the need for\ntrusting the correctness of the translation from binary code to intermediate\nlanguage. Achieving a high degree of trust is challenging since this\ntranspilation must handle (i) all the side effects of the instructions, (ii)\nmultiple instruction encoding (e.g. ARM Thumb), and (iii) variable instruction\nlength (e.g. Intel). We overcome these problems by formally modeling one of\nsuch intermediate languages in the interactive theorem prover HOL4 and by\nimplementing a proof- producing transpiler. This tool translates ARMv8 programs\nto the in- termediate language and generates a HOL4 proof that demonstrates the\ncorrectness of the translation in the form of a simulation theorem. We also\nshow how the transpiler theorems can be used to transfer properties verified on\nthe intermediate language to the binary code.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 14:49:33 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Metere", "Roberto", ""], ["Lindner", "Andreas", ""], ["Guanciale", "Roberto", ""]]}, {"id": "1807.10695", "submitter": "Ruolong Lian", "authors": "Jin Hee Kim, Brett Grady, Ruolong Lian, John Brothers, Jason H.\n  Anderson", "title": "FPGA-Based CNN Inference Accelerator Synthesized from Multi-Threaded C\n  Software", "comments": null, "journal-ref": "J. H. Kim, B. Grady, R. Lian, J. Brothers and J. H. Anderson,\n  \"FPGA-based CNN inference accelerator synthesized from multi-threaded C\n  software,\" 2017 30th IEEE International System-on-Chip Conference (SOCC),\n  Munich, 2017, pp. 268-273", "doi": "10.1109/SOCC.2017.8226056", "report-no": null, "categories": "cs.LG cs.AR cs.PF cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A deep-learning inference accelerator is synthesized from a C-language\nsoftware program parallelized with Pthreads. The software implementation uses\nthe well-known producer/consumer model with parallel threads interconnected by\nFIFO queues. The LegUp high-level synthesis (HLS) tool synthesizes threads into\nparallel FPGA hardware, translating software parallelism into spatial\nparallelism. A complete system is generated where convolution, pooling and\npadding are realized in the synthesized accelerator, with remaining tasks\nexecuting on an embedded ARM processor. The accelerator incorporates reduced\nprecision, and a novel approach for zero-weight-skipping in convolution. On a\nmid-sized Intel Arria 10 SoC FPGA, peak performance on VGG-16 is 138 effective\nGOPS.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 15:46:16 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Kim", "Jin Hee", ""], ["Grady", "Brett", ""], ["Lian", "Ruolong", ""], ["Brothers", "John", ""], ["Anderson", "Jason H.", ""]]}, {"id": "1807.10852", "submitter": "Mahdi Soltan Mohammadi", "authors": "Mahdi Soltan Mohammadi, Kazem Cheshmi, Ganesh Gopalakrishnan, Mary\n  Hall, Maryam Mehri Dehnavi, Anand Venkat, Tomofumi Yuki, Michelle Mills\n  Strout", "title": "Sparse Matrix Code Dependence Analysis Simplification at Compile Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing array-based computations to determine data dependences is useful\nfor many applications including automatic parallelization, race detection,\ncomputation and communication overlap, verification, and shape analysis. For\nsparse matrix codes, array data dependence analysis is made more difficult by\nthe use of index arrays that make it possible to store only the nonzero entries\nof the matrix (e.g., in A[B[i]], B is an index array). Here, dependence\nanalysis is often stymied by such indirect array accesses due to the values of\nthe index array not being available at compile time. Consequently, many\ndependences cannot be proven unsatisfiable or determined until runtime.\nNonetheless, index arrays in sparse matrix codes often have properties such as\nmonotonicity of index array elements that can be exploited to reduce the amount\nof runtime analysis needed. In this paper, we contribute a formulation of array\ndata dependence analysis that includes encoding index array properties as\nuniversally quantified constraints. This makes it possible to leverage existing\nSMT solvers to determine whether such dependences are unsatisfiable and\nsignificantly reduces the number of dependences that require runtime analysis\nin a set of eight sparse matrix kernels. Another contribution is an algorithm\nfor simplifying the remaining satisfiable data dependences by discovering\nequalities and/or subset relationships. These simplifications are essential to\nmake a runtime-inspection-based approach feasible.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 23:08:46 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Mohammadi", "Mahdi Soltan", ""], ["Cheshmi", "Kazem", ""], ["Gopalakrishnan", "Ganesh", ""], ["Hall", "Mary", ""], ["Dehnavi", "Maryam Mehri", ""], ["Venkat", "Anand", ""], ["Yuki", "Tomofumi", ""], ["Strout", "Michelle Mills", ""]]}, {"id": "1807.11267", "submitter": "Thomas Winant", "authors": "Thomas Winant and Dominique Devriese", "title": "Coherent Explicit Dictionary Application for Haskell: Formalisation and\n  Coherence Proof", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Type classes are one of Haskell's most popular features and extend its type\nsystem with ad-hoc polymorphism. Since their conception, there were useful\nfeatures that could not be offered because of the desire to offer two\ncorrectness properties: coherence and global uniqueness of instances. Coherence\nessentially guarantees that program semantics are independent from type-checker\ninternals. Global uniqueness of instances is relied upon by libraries for\nenforcing, for example, that a single order relation is used for all\nmanipulations of an ordered binary tree.\n  The features that could not be offered include explicit dictionary\napplication and local instances, which would be highly useful in practice. We\npropose a new design for offering explicit dictionary application, without\ncompromising coherence and global uniqueness. We introduce a novel criterion\nbased on GHC's type argument roles to decide when a dictionary application is\nsafe with respect to global uniqueness of instances. We preserve coherence by\ndetecting potential sources of incoherence, and prove it formally. Moreover,\nour solution makes it possible to use local dictionaries. In addition to\ndeveloping our ideas formally, we have implemented a working prototype in GHC.\n  This report contains the full formalisation and coherence proof.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 10:07:19 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Winant", "Thomas", ""], ["Devriese", "Dominique", ""]]}, {"id": "1807.11792", "submitter": "Ivano Salvo", "authors": "Ivano Salvo and Agnese Pacifico", "title": "Computing Integer Sequences: Filtering vs Generation (Functional Pearl)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a report of a teaching experience, we analyse Haskell programs computing\ntwo integer sequences: the Hamming sequence and the Ulam sequence. For both of\nthem, we investigate two strategies of computation: the first is based on\nfiltering out those natural numbers that do not belong to the sequence, whereas\nthe second is based on the direct generation of num- bers that belong to the\nsequence. Advocating cross-fertilisation among ideas emerging when programming\nin different programming paradigms, in the background, we sketch out some\nconsiderations about corresponding C programs solving the same two problems.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 12:46:13 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 08:45:43 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Salvo", "Ivano", ""], ["Pacifico", "Agnese", ""]]}]