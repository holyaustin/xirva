[{"id": "2011.02626", "submitter": "Richard Peschke", "authors": "R. Peschke, K. Nishimura, G. Varner", "title": "ARGG-HDL: A High Level Python Based Object-Oriented HDL Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a High-Level Python-based Hardware Description Language\n(ARGG-HDL), It uses Python as its source language and converts it to standard\nVHDL. Compared to other approaches of building converters from a high-level\nprogramming language into a hardware description language, this new approach\naims to maintain an object-oriented paradigm throughout the entire process.\nInstead of removing all the high-level features from Python to make it into an\nHDL, this approach goes the opposite way. It tries to show how certain features\nfrom a high-level language can be implemented in an HDL, providing the\ncorresponding benefits of high-level programming for the user.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 02:43:50 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Peschke", "R.", ""], ["Nishimura", "K.", ""], ["Varner", "G.", ""]]}, {"id": "2011.03070", "submitter": "Micha{\\l} Gajda", "authors": "Micha{\\l} J. Gajda, Vitor Vitali Barrozzi, and Gabriel Araujo", "title": "Multicloud API binding generation from documentation", "comments": "Presented on XP 2020: Agility in Microservices workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present industry experience from implementing retargetable cloud API\nbinding generator.\n  The analysis is implemented in Haskell, using type classes, types a la carte,\nand code generation monad.\n  It also targets Haskell, and allows us to bind cloud APIs on short notice,\nand unprecedented scale.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 19:35:09 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Gajda", "Micha\u0142 J.", ""], ["Barrozzi", "Vitor Vitali", ""], ["Araujo", "Gabriel", ""]]}, {"id": "2011.03076", "submitter": "Micha{\\l} Gajda", "authors": "Micha{\\l} J. Gajda", "title": "Towards a more perfect union type", "comments": null, "journal-ref": null, "doi": "10.5281/zenodo.3929474", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a principled theoretical framework for inferring and checking the\nunion types, and show its work in practice on JSON data structures. The\nframework poses a union type inference as a learning problem from multiple\nexamples. The categorical framework is generic and easily extensible.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 19:53:42 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 17:17:39 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Gajda", "Micha\u0142 J.", ""]]}, {"id": "2011.03319", "submitter": "Chandrika Bhardwaj", "authors": "Chandrika Bhardwaj and Sanjiva Prasad", "title": "Secure Information Flow Connections", "comments": "arXiv admin note: text overlap with arXiv:1903.02835", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Denning's lattice model provided secure information flow analyses with an\nintuitive mathematical foundation: the lattice ordering determines permitted\nflows. We examine how this framework may be extended to support the flow of\ninformation between autonomous organisations, each employing possibly quite\ndifferent security lattices and information flow policies. We propose a\nconnection framework that permits different organisations to exchange\ninformation while maintaining both security of information flow as well as\ntheir autonomy in formulating and maintaining security policies. Our\nprescriptive framework is based on the rigorous mathematical framework of\nLagois connections proposed by Melton, together with a simple operational model\nfor transferring object data between domains. The merit of this formulation is\nthat it is simple, minimal, adaptable and intuitive. We show that our framework\nis semantically sound, by proving that the connections proposed preserve\nstandard correctness notions such as non-interference. We then illustrate how\nLagois theory also provides a robust framework and methodology for negotiating\nand maintaining secure agreements on information flow between autonomous\norganisations, even when either or both organisations change their security\nlattices. Composition and decomposition properties indicate support for a\nmodular approach to secure flow frameworks in complex organisations. We next\nshow that this framework extends naturally and conservatively to the\nDecentralised Labels Model of Myers et al. - a Lagois connection between the\nhierarchies of principals in two organisations naturally induces a Lagois\nconnection between the corresponding security label lattices, thus extending\nthe security guarantees ensured by the decentralised model to encompass\nbidirectional inter-organisational flows.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 08:35:41 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Bhardwaj", "Chandrika", ""], ["Prasad", "Sanjiva", ""]]}, {"id": "2011.03516", "submitter": "Yusuke Izawa", "authors": "Yusuke Izawa and Hidehiko Masuhara", "title": "Amalgamating Different JIT Compilations in a Meta-tracing JIT Compiler\n  Framework", "comments": null, "journal-ref": "Proceedings of the 16th ACM SIGPLAN International Symposium on\n  Dynamic Languages (DLS 2020)", "doi": "10.1145/3426422.3426977", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern virtual machines, such as JVMs, .NET Framework, and V8, employ a\njust-in-time (JIT) compiler to achieve their high-performance. There are two\nmajor compilation strategies; trace-based compilation and method-based\ncompilation. They have their own advantages and disadvantages, so we presume\nthat applying suitable strategies for different program parts is essential for\nfaster execution. This paper proposes a new approach called the meta-hybrid JIT\ncompiler framework, which combined the two strategies in a single meta-JIT\ncompiler framework. We implemented the BacCaml framework for proof-of-concept.\nWe also report that some programs actually ran faster by the hybrid compilation\nin our experiments.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 18:47:42 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 10:03:21 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2020 08:43:23 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Izawa", "Yusuke", ""], ["Masuhara", "Hidehiko", ""]]}, {"id": "2011.03536", "submitter": "Micha{\\l} Gajda", "authors": "Micha{\\l} J. Gajda, Dmitry Krylov", "title": "Fast XML/HTML for Haskell: XML TypeLift", "comments": null, "journal-ref": null, "doi": "10.5281/zenodo.3929549", "report-no": null, "categories": "cs.PL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents and compares a range of parsers with and without data\nmapping for conversion between XML and Haskell. The best performing parser\ncompetes favorably with the fastest tools available in other languages and is,\nthus, suitable for use in large-scale data analysis. The best performing parser\nalso allows software developers of intermediate-level Haskell programming\nskills to start processing large numbers of XML documents soon after finding\nthe relevant XML Schema from a simple internet search, without the need for\nspecialist prior knowledge or skills. We hope that this unique combination of\nparser performance and usability will provide a new standard for XML mapping to\nhigh-level languages.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 20:26:36 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Gajda", "Micha\u0142 J.", ""], ["Krylov", "Dmitry", ""]]}, {"id": "2011.03537", "submitter": "Micha{\\l} Gajda", "authors": "Micha{\\l} J. Gajda", "title": "Less Arbitrary waiting time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Property testing is the cheapest and most precise way of building up a test\nsuite for your program. Especially if the datatypes enjoy nice mathematical\nlaws. But it is also the easiest way to make it run for an unreasonably long\ntime. We prove connection between deeply recursive data structures, and\nepidemic growth rate, and show how to fix the problem, and make Arbitrary\ninstances run in linear time with respect to assumed test size.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 20:31:13 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 17:06:04 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Gajda", "Micha\u0142 J.", ""]]}, {"id": "2011.03538", "submitter": "Micha{\\l} Gajda", "authors": "Micha{\\l} J. Gajda, Hai Nguyen Quang, Do Ngoc Khanh, and Vuong Hai\n  Thanh", "title": "Infer XPath", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose reformulation of discovery of data structure within a web page as\nrelations between sets of document nodes. We start by reformulating web page\nanalysis as finding expressions in extension of XPath. Then we propose to\nautomatically discover these XPath expressions with InferXPath meta-language.\nOur goal is to automate laborious process of conversion of manually created web\npages that serve as software documentations, wikis, and reference documents,\nand speed up their conversion into tabular data that can be directly fed into\ndata pipeline.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 20:38:57 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Gajda", "Micha\u0142 J.", ""], ["Quang", "Hai Nguyen", ""], ["Khanh", "Do Ngoc", ""], ["Thanh", "Vuong Hai", ""]]}, {"id": "2011.03616", "submitter": "Niranjan Hasabnis", "authors": "Niranjan Hasabnis and Justin Gottschlich", "title": "ControlFlag: A Self-Supervised Idiosyncratic Pattern Detection System\n  for Software Control Structures", "comments": "To appear in Proceedings of the 5th ACM SIGPLAN International\n  Symposium on Machine Programming (MAPS '21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software debugging has been shown to utilize upwards of half of developers'\ntime. Yet, machine programming (MP), the field concerned with the automation of\nsoftware (and hardware) development, has recently made strides in both research\nand production-quality automated debugging systems. In this paper we present\nControlFlag, a self-supervised MP system that aims to improve debugging by\nattempting to detect idiosyncratic pattern violations in software control\nstructures. ControlFlag also suggests possible corrections in the event an\nanomalous pattern is detected. We present ControlFlag's design and provide an\nexperimental evaluation and analysis of its efficacy in identifying potential\nprogramming errors in production-quality software. As a first concrete evidence\ntowards improving software quality, ControlFlag has already found an anomaly in\nCURL that has been acknowledged and fixed by its developers. We also discuss\nfuture extensions of ControlFlag.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 22:19:05 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 06:55:50 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 00:44:27 GMT"}, {"version": "v4", "created": "Thu, 13 May 2021 21:02:03 GMT"}, {"version": "v5", "created": "Mon, 17 May 2021 16:22:04 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Hasabnis", "Niranjan", ""], ["Gottschlich", "Justin", ""]]}, {"id": "2011.03660", "submitter": "Yue Niu", "authors": "Yue Niu (1) and Robert Harper (1) ((1) Carnegie Mellon University)", "title": "Cost-Aware Type Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although computational complexity is a fundamental aspect of program\nbehavior, it is often at odds with common type theoretic principles such as\nfunction extensionality, which identifies all functions with the same\n$\\textit{input-output}$ behavior. We present a computational type theory called\n$\\mathbf{CATT}$ that has a primitive notion of cost (the number of evaluation\nsteps). We introduce a new dependent function type \"funtime\" whose semantics\ncan be viewed as a cost-aware version of function extensionality. We prove a\ncollection of lemmas for $\\mathbf{CATT}$, including a novel introduction rule\nfor the new funtime type. $\\mathbf{CATT}$ can be simultaneously viewed as a\nframework for analyzing computational complexity of programs and as the\nbeginnings of a semantic foundation for characterizing feasible mathematical\nproofs.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2020 02:13:01 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 02:11:10 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Niu", "Yue", "", "Carnegie Mellon University"], ["Harper", "Robert", "", "Carnegie Mellon University"]]}, {"id": "2011.04070", "submitter": "Pritam Choudhury", "authors": "Pritam Choudhury and Harley Eades III and Richard A. Eisenberg and\n  Stephanie C Weirich", "title": "A graded dependent type system with a usage-aware semantics (extended\n  version)", "comments": "Extended version of paper with same title at POPL 2021, 39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graded Type Theory provides a mechanism to track and reason about resource\nusage in type systems. In this paper, we develop GraD, a novel version of such\na graded dependent type system that includes functions, tensor products,\nadditive sums, and a unit type. Since standard operational semantics is\nresource-agnostic, we develop a heap-based operational semantics and prove a\nsoundness theorem that shows correct accounting of resource usage. Several\nuseful properties, including the standard type soundness theorem,\nnon-interference of irrelevant resources in computation and single pointer\nproperty for linear resources, can be derived from this theorem. We hope that\nour work will provide a base for integrating linearity, irrelevance and\ndependent types in practical programming languages like Haskell.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2020 20:21:07 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 15:56:42 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Choudhury", "Pritam", ""], ["Eades", "Harley", "III"], ["Eisenberg", "Richard A.", ""], ["Weirich", "Stephanie C", ""]]}, {"id": "2011.04360", "submitter": "S\\'ergio Medeiros", "authors": "S\\'ergio Medeiros and Carlos Olarte", "title": "A Semantic Framework for PEGs", "comments": null, "journal-ref": null, "doi": "10.1145/3426425.3426944", "report-no": null, "categories": "cs.FL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parsing Expression Grammars (PEGs) are a recognition-based formalism which\nallows to describe the syntactical and the lexical elements of a language. The\nmain difference between Context-Free Grammars (CFGs) and PEGs relies on the\ninterpretation of the choice operator: while the CFGs' unordered choice e | e'\nis interpreted as the union of the languages recognized by e and e, the PEGs'\nprioritized choice e/e' discards e' if e succeeds. Such subtle, but important\ndifference, changes the language recognized and yields more efficient parsing\nalgorithms. This paper proposes a rewriting logic semantics for PEGs. We start\nwith a rewrite theory giving meaning to the usual constructs in PEGs. Later, we\nshow that cuts, a mechanism for controlling backtracks in PEGs, finds also a\nnatural representation in our framework. We generalize such mechanism, allowing\nfor both local and global cuts with a precise, unified and formal semantics.\nHence, our work strives at better understanding and controlling backtracks in\nparsers for PEGs. The semantics we propose is executable and, besides being a\nparser with modest efficiency, it can be used as a playground to test different\noptimization ideas. More importantly, it is a mathematical tool that can be\nused for different analyses.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 11:49:37 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Medeiros", "S\u00e9rgio", ""], ["Olarte", "Carlos", ""]]}, {"id": "2011.04581", "submitter": "Ramanathan Srinivasan Thinniyam", "authors": "Pascal Baumann, Rupak Majumdar, Ramanathan S. Thinniyam, Georg\n  Zetzsche", "title": "Context-Bounded Verification of Liveness Properties for Multithreaded\n  Shared-Memory Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study context-bounded verification of liveness properties of\nmulti-threaded, shared-memory programs, where each thread can spawn additional\nthreads. Our main result shows that context-bounded fair termination is\ndecidable for the model; context-bounded implies that each spawned thread can\nbe context switched a fixed constant number of times. Our proof is technical,\nsince fair termination requires reasoning about the composition of unboundedly\nmany threads each with unboundedly large stacks. In fact, techniques for\nrelated problems, which depend crucially on replacing the pushdown threads with\nfinite-state threads, are not applicable. Instead, we introduce an extension of\nvector addition systems with states (VASS), called VASS with balloons (VASSB),\nas an intermediate model; it is an infinite-state model of independent\ninterest. A VASSB allows tokens that are themselves markings (balloons). We\nshow that context bounded fair termination reduces to fair termination for\nVASSB. We show the latter problem is decidable by showing a series of\nreductions: from fair termination to configuration reachability for VASSB and\nthence to the reachability problem for VASS. For a lower bound, fair\ntermination is known to be non-elementary already in the special case where\nthreads run to completion (no context switches).\n  We also show that the simpler problem of context-bounded termination is\n2EXPSPACE-complete, matching the complexity bound---and indeed the\ntechniques---for safety verification. Additionally, we show the related problem\nof fair starvation, which checks if some thread can be starved along a fair\nrun, is also decidable in the context-bounded case. The decidability employs an\nintricate reduction from fair starvation to fair termination. Like fair\ntermination, this problem is also non-elementary.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 17:26:34 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 12:36:28 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Baumann", "Pascal", ""], ["Majumdar", "Rupak", ""], ["Thinniyam", "Ramanathan S.", ""], ["Zetzsche", "Georg", ""]]}, {"id": "2011.04655", "submitter": "St\\'ephane Ducasse", "authors": "Thomas Dupriez (CNRS, CRIStAL, RMOD), Steven Costiou (CNRS, CRIStAL,\n  RMOD), St\\'ephane Ducasse (CNRS, CRIStAL, RMOD)", "title": "First Infrastructure and Experimentation in Echo-debugging", "comments": null, "journal-ref": "IWST20: International Workshop on Smalltalk Technologies, Sep\n  2020, Online, France", "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As applications get developed, bugs inevitably get introduced. Often, it is\nunclear why a given code change introduced a given bug. To find this causal\nrelation and more effectively debug, developers can leverage the existence of a\nprevious version of the code, without the bug. But traditional debug-ging tools\nare not designed for this type of work, making this operation tedious. In this\narticle, we propose as exploratory work the echo-debugger, a tool to debug two\ndifferent executions in parallel, and the Convergence Divergence Mapping (CDM)\nalgorithm to locate all the control-flow divergences and convergences of these\nexecutions. In this exploratory work, we present the architecture of the tool\nand a scenario to solve a non trivial bug.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 15:18:00 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Dupriez", "Thomas", "", "CNRS, CRIStAL, RMOD"], ["Costiou", "Steven", "", "CNRS, CRIStAL,\n  RMOD"], ["Ducasse", "St\u00e9phane", "", "CNRS, CRIStAL, RMOD"]]}, {"id": "2011.04876", "submitter": "Thomas Wies", "authors": "Zvonimir Pavlinovic, Yusen Su, Thomas Wies", "title": "Data Flow Refinement Type Inference", "comments": "Extended version of an article to appear in POPL'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Refinement types enable lightweight verification of functional programs.\nAlgorithms for statically inferring refinement types typically work by\nreduction to solving systems of constrained Horn clauses extracted from typing\nderivations. An example is Liquid type inference, which solves the extracted\nconstraints using predicate abstraction. However, the reduction to constraint\nsolving in itself already signifies an abstraction of the program semantics\nthat affects the precision of the overall static analysis. To better understand\nthis issue, we study the type inference problem in its entirety through the\nlens of abstract interpretation. We propose a new refinement type system that\nis parametric with the choice of the abstract domain of type refinements as\nwell as the degree to which it tracks context-sensitive control flow\ninformation. We then derive an accompanying parametric inference algorithm as\nan abstract interpretation of a novel data flow semantics of functional\nprograms. We further show that the type system is sound and complete with\nrespect to the constructed abstract semantics. Our theoretical development\nreveals the key abstraction steps inherent in refinement type inference\nalgorithms. The trade-off between precision and efficiency of these abstraction\nsteps is controlled by the parameters of the type system. Existing refinement\ntype systems and their respective inference algorithms, such as Liquid types,\nare captured by concrete parameter instantiations. We have implemented our\nframework in a prototype tool and evaluated it for a range of new parameter\ninstantiations (e.g., using octagons and polyhedra for expressing type\nrefinements). The tool compares favorably against other existing tools. Our\nevaluation indicates that our approach can be used to systematically construct\nnew refinement type inference algorithms that are both robust and precise.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 03:52:37 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Pavlinovic", "Zvonimir", ""], ["Su", "Yusen", ""], ["Wies", "Thomas", ""]]}, {"id": "2011.05194", "submitter": "Jayaraj Poroor", "authors": "Jayaraj Poroor", "title": "MotePy: A domain specific language for low-overhead machine learning and\n  data processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A domain specific language (DSL), named MotePy is presented. The DSL offers a\nhigh level syntax with low overheads for ML/data processing in time constrained\nor memory constrained systems. The DSL-to-C compiler has a novel static memory\nallocator that tracks object lifetimes and reuses the static memory, which we\ncall the compiler-managed heap.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 15:49:45 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 04:31:29 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Poroor", "Jayaraj", ""]]}, {"id": "2011.05219", "submitter": "Micha{\\l} Gajda", "authors": "Micha{\\l} J. Gajda", "title": "Curious properties of latency distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network latency distributions, their algebra, and use examples. This paper\nconsiders modeling of capacity-insensitive processes and distributed systems.\nIt provides algebraic properties of the latency distribution algebra and\nHaskell code to implement the model.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 20:49:16 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Gajda", "Micha\u0142 J.", ""]]}, {"id": "2011.05248", "submitter": "L\\'eo Stefanesco", "authors": "Nobuko Yoshida, Simon Castellan, L\\'eo Stefanesco", "title": "Game Semantics: Easy as Pi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Game semantics has proven to be a robust method to give compositional\nsemantics for a variety of higher-order programming languages. However, due to\nthe complexity of most game models, game semantics has remained unapproachable\nfor non-experts.\n  In this paper, we aim at making game semantics more accessible by viewing it\nas a syntactic translation into a session typed pi-calculus, referred to as\nmetalanguage, followed by a semantics interpretation of the metalanguage into a\nparticular game model. The syntactic translation can be defined for a wide\nrange of programming languages without knowledge of the particular game model\nused. Simple reasoning on the model (soundness, and adequacy) can be done at\nthe level of the metalanguage, escaping tedious technical proofs usually found\nin game semantics. We call this methodology programming game semantics.\n  We design a metalanguage (PiDiLL) inspired from Differential Linear Logic\n(DiLL), which is concise but expressive enough to support features required by\nconcurrent game semantics. We then demonstrate our methodology by yielding the\nfirst causal, non-angelic and interactive game model of CML, a higher-order\ncall-by-value language with shared memory concurrency. We translate CML into\nPiDiLL and show that the translation is adequate. We give a causal and\nnon-angelic game semantics model using event structures, which supports a\nsimple semantics interpretation of PiDiLL. Combining both of these results, we\nobtain the first interactive model of a concurrent language of this\nexpressivity which is adequate with respect to the standard weak bisimulation,\nand fully abstract for the contextual equivalence on second-order terms.\n  We have implemented a prototype which can explore the generated causal object\nfrom a subset of OCaml.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 17:15:39 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Yoshida", "Nobuko", ""], ["Castellan", "Simon", ""], ["Stefanesco", "L\u00e9o", ""]]}, {"id": "2011.05608", "submitter": "Ian Rogers", "authors": "Ian Rogers", "title": "Efficient global register allocation", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In a compiler, an essential component is the register allocator. Two main\nalgorithms have dominated implementations, graph coloring and linear scan,\ndiffering in how live values are modeled. Graph coloring uses an edge in an\n`interference graph' to show that two values cannot reside in the same\nregister. Linear scan numbers all values, creates intervals between definition\nand uses, and then intervals that do not overlap may be allocated to the same\nregister. For both algorithms the liveness models are computed at considerable\nruntime and memory cost. Furthermore, these algorithms do little to improve\ncode quality, where the target architecture and register coalescing are\nimportant concerns.\n  We describe a new register allocation algorithm with lightweight\nimplementation characteristics. The algorithm introduces a `future-active' set\nfor values that will reside in a register later in the allocation. Registers\nare allocated and freed in the manner of linear scan, although other ordering\nheuristics could improve code quality or lower runtime cost. An advantageous\nproperty of the approach is an ability to make these trade-offs. A key result\nis the `future-active' set can remove any liveness model for over 90% of\ninstructions and 80% of methods. The major contribution is the allocation\nalgorithm that, for example, solves an inability of the similarly motivated\nTreescan register allocator to look ahead of the instruction being allocated -\nallowing an unconstrained allocation order, and an ability to better handle\nfixed registers and loop carried values. The approach also is not reliant on\nproperties of SSA form, similar to the original linear scan work. An analysis\nis presented in a production compiler for Java code compiled through SSA form\nto Android dex files.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 07:46:33 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Rogers", "Ian", ""]]}, {"id": "2011.05825", "submitter": "Pascal Baumann", "authors": "Pascal Baumann and Rupak Majumdar and Ramanathan S. Thinniyam and\n  Georg Zetzsche", "title": "The complexity of bounded context switching with dynamic thread creation", "comments": null, "journal-ref": "ICALP 2020, vol. 168, pages 111:1-111:16", "doi": "10.4230/LIPIcs.ICALP.2020.111", "report-no": null, "categories": "cs.FL cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dynamic networks of concurrent pushdown systems (DCPS) are a theoretical\nmodel for multi-threaded recursive programs with shared global state and\ndynamical creation of threads. The (global) state reachability problem for DCPS\nis undecidable in general, but Atig et al. (2009) showed that it becomes\ndecidable, and is in 2EXPSPACE, when each thread is restricted to a fixed\nnumber of context switches. The best known lower bound for the problem is\nEXPSPACE-hard and this lower bound follows already when each thread is a\nfinite-state machine and runs atomically to completion (i.e., does not switch\ncontexts). In this paper, we close the gap by showing that state reachability\nis 2EXPSPACE-hard already with only one context switch. Interestingly, state\nreachability analysis is in EXPSPACE both for pushdown threads without context\nswitches as well as for finite-state threads with arbitrary context switches.\nThus, recursive threads together with a single context switch provide an\nexponential advantage.\n  Our proof techniques are of independent interest for 2EXPSPACE-hardness\nresults. We introduce transducer-defined Petri nets, a succinct representation\nfor Petri nets, and show coverability is 2EXPSPACE-hard for this model. To show\n2EXPSPACE-hardness, we present a modified version of Lipton's simulation of\ncounter machines by Petri nets, where the net programs can make explicit\nrecursive procedure calls up to a bounded depth.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 14:42:08 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Baumann", "Pascal", ""], ["Majumdar", "Rupak", ""], ["Thinniyam", "Ramanathan S.", ""], ["Zetzsche", "Georg", ""]]}, {"id": "2011.05948", "submitter": "Ryan Doenges", "authors": "Ryan Doenges, Mina Tahmasbi Arashloo, Santiago Bautista, Alexander\n  Chang, Newton Ni, Samwise Parkinson, Rudy Peterson, Alaia Solko-Breslin,\n  Amanda Xu, Nate Foster", "title": "Petr4: Formal Foundations for P4 Data Planes", "comments": "54 pages. Extended version of POPL 2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  P4 is a domain-specific language for programming and specifying\npacket-processing systems. It is based on an elegant design with high-level\nabstractions like parsers and match-action pipelines that can be compiled to\nefficient implementations in software or hardware. Unfortunately, like many\nindustrial languages, P4 has developed without a formal foundation. The P4\nLanguage Specification is a 160-page document with a mixture of informal prose,\ngraphical diagrams, and pseudocode. The P4 reference implementation is a\ncomplex system, running to over 40KLoC of C++ code. Clearly neither of these\nartifacts is suitable for formal reasoning.\n  This paper presents a new framework, called Petr4, that puts P4 on a solid\nfoundation. Petr4 consists of a clean-slate definitional interpreter and a\ncalculus that models the semantics of a core fragment of P4. Throughout the\nspecification, some aspects of program behavior are left up to targets. Our\ninterpreter is parameterized over a target interface which collects all the\ntarget-specific behavior in the specification in a single interface.\n  The specification makes ad-hoc restrictions on the nesting of certain program\nconstructs in order to simplify compilation and avoid the possibility of\nnonterminating programs. We captured the latter intention in our core calculus\nby stratifying its type system, rather than imposing unnatural syntactic\nrestrictions, and we proved that all programs in this core calculus terminate.\n  We have validated the interpreter against a suite of over 750 tests from the\nP4 reference implementation, exercising our target interface with tests for\ndifferent targets. We established termination for the core calculus by\ninduction on the stratified type system. While developing Petr4, we reported\ndozens of bugs in the language specification and the reference implementation,\nmany of which have been fixed.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 18:09:52 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Doenges", "Ryan", ""], ["Arashloo", "Mina Tahmasbi", ""], ["Bautista", "Santiago", ""], ["Chang", "Alexander", ""], ["Ni", "Newton", ""], ["Parkinson", "Samwise", ""], ["Peterson", "Rudy", ""], ["Solko-Breslin", "Alaia", ""], ["Xu", "Amanda", ""], ["Foster", "Nate", ""]]}, {"id": "2011.06094", "submitter": "Dominic Orchard", "authors": "Dominic Orchard, Mistral Contrastin, Matthew Danish, Andrew Rice", "title": "Guiding user annotations for units-of-measure verification", "comments": "Presented at HATRA 2020 (Workshop on Human Aspects of Types and\n  Reasoning Assistants) colocated with SPLASH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This extended abstract reports on previous work of the CamFort project in\nwhich we developed an external units-of-measure type system for Fortran code,\ntargeted at scientists. Our approach can guide the programmer in adding\nspecifications (type annotations) to existing code, with the aim of easing\nadoption on legacy code. Pertinent to the topics of the HATRA workshop, we\ndiscuss the human-aspects of the tool here. CamFort is open-source and freely\navailable online.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 22:04:34 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Orchard", "Dominic", ""], ["Contrastin", "Mistral", ""], ["Danish", "Matthew", ""], ["Rice", "Andrew", ""]]}, {"id": "2011.06171", "submitter": "Will Crichton", "authors": "Will Crichton", "title": "The Usability of Ownership", "comments": "To appear at HATRA @ SPLASH '20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ownership is the concept of tracking aliases and mutations to data, useful\nfor both memory safety and system design. The Rust programming language\nimplements ownership via the borrow checker, a static analyzer that extends the\ncore type system. The borrow checker is a notorious learning barrier for new\nRust users. In this paper, I focus on the gap between understanding ownership\nin theory versus its implementation in the borrow checker. As a sound and\nincomplete analysis, compiler errors may arise from either ownership-unsound\nbehavior or limitations of the analyzer. Understanding this distinction is\nessential for fixing ownership errors. But how are users actually supposed to\nmake the correct inference? Drawing on my experience with using and teaching\nRust, I explore the many challenges in interpreting and responding to ownership\nerrors. I also suggest educational and automated interventions that could\nimprove the usability of ownership.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 02:39:03 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Crichton", "Will", ""]]}, {"id": "2011.06404", "submitter": "Rohit Chadha", "authors": "Gilles Barthe and Rohit Chadha and Paul Krogmeier and A. Prasad Sistla\n  and Mahesh Viswanathan", "title": "Deciding Accuracy of Differential Privacy Schemes", "comments": null, "journal-ref": null, "doi": "10.1145/3434289", "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a mathematical framework for developing statistical\ncomputations with provable guarantees of privacy and accuracy. In contrast to\nthe privacy component of differential privacy, which has a clear mathematical\nand intuitive meaning, the accuracy component of differential privacy does not\nhave a generally accepted definition; accuracy claims of differential privacy\nalgorithms vary from algorithm to algorithm and are not instantiations of a\ngeneral definition. We identify program discontinuity as a common theme in\nexisting \\emph{ad hoc} definitions and introduce an alternative notion of\naccuracy parametrized by, what we call, {\\distance} -- the {\\distance} of an\ninput $x$ w.r.t., a deterministic computation $f$ and a distance $d$, is the\nminimal distance $d(x,y)$ over all $y$ such that $f(y)\\neq f(x)$. We show that\nour notion of accuracy subsumes the definition used in theoretical computer\nscience, and captures known accuracy claims for differential privacy\nalgorithms. In fact, our general notion of accuracy helps us prove better\nclaims in some cases. Next, we study the decidability of accuracy. We first\nshow that accuracy is in general undecidable. Then, we define a non-trivial\nclass of probabilistic computations for which accuracy is decidable\n(unconditionally, or assuming Schanuel's conjecture). We implement our decision\nprocedure and experimentally evaluate the effectiveness of our approach for\ngenerating proofs or counterexamples of accuracy for common algorithms from the\nliterature.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 14:17:51 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Barthe", "Gilles", ""], ["Chadha", "Rohit", ""], ["Krogmeier", "Paul", ""], ["Sistla", "A. Prasad", ""], ["Viswanathan", "Mahesh", ""]]}, {"id": "2011.07565", "submitter": "Michael Coblenz", "authors": "Michael Coblenz, Ariel Davis, Megan Hofmann, Vivian Huang, Siyue Jin,\n  Max Krieger, Kyle Liang, Brian Wei, Mengchen Sam Yong, Jonathan Aldrich", "title": "User-Centered Programming Language Design: A Course-Based Case Study", "comments": "7 pages. Presented at HATRA 2020\n  (https://2020.splashcon.org/home/hatra-2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, user-centered methods have been proposed to improve the design of\nprogramming languages. In order to explore what benefits these methods might\nhave for novice programming language designers, we taught a collection of\nuser-centered programming language design methods to a group of eight students.\nWe observed that natural programming and usability studies helped the students\nrefine their language designs and identify opportunities for improvement, even\nin the short duration of a course project.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 15:58:46 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Coblenz", "Michael", ""], ["Davis", "Ariel", ""], ["Hofmann", "Megan", ""], ["Huang", "Vivian", ""], ["Jin", "Siyue", ""], ["Krieger", "Max", ""], ["Liang", "Kyle", ""], ["Wei", "Brian", ""], ["Yong", "Mengchen Sam", ""], ["Aldrich", "Jonathan", ""]]}, {"id": "2011.07653", "submitter": "Georg Schmid", "authors": "Georg Stefan Schmid (EPFL), Olivier Blanvillain (EPFL), Jad Hamza\n  (EPFL), Viktor Kun\\v{c}ak (EPFL)", "title": "Coming to Terms with Your Choices: An Existential Take on Dependent\n  Types", "comments": "25 pages, 11 figures. A version of this manuscript had been submitted\n  for review in the first half of 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Type-level programming is an increasingly popular way to obtain additional\ntype safety. Unfortunately, it remains a second-class citizen in the majority\nof industrially-used programming languages. We propose a new dependently-typed\nsystem with subtyping and singleton types whose goal is to enable type-level\nprogramming in an accessible style. At the heart of our system lies a\nnon-deterministic choice operator. We argue that embracing non-determinism is\ncrucial for bringing dependent types to a broader audience of programmers,\nsince real-world programs will inevitably interact with imprecisely-typed, or\neven impure code. Furthermore, we show that singleton types combined with the\nchoice operator can serve as a replacement for many type functions of interest\nin practice. We establish the soundness of our approach using the Coq proof\nassistant. Our soundness approach models non-determinism using additional\nfunction arguments to represent choices. We represent type-level computation\nusing singleton types and existential types that quantify over choice\narguments. To demonstrate the practicality of our type system, we present an\nimplementation as a modification of the Scala compiler. We provide a case study\nin which we develop a strongly-typed wrapper for Spark datasets.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 23:09:55 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Schmid", "Georg Stefan", "", "EPFL"], ["Blanvillain", "Olivier", "", "EPFL"], ["Hamza", "Jad", "", "EPFL"], ["Kun\u010dak", "Viktor", "", "EPFL"]]}, {"id": "2011.07966", "submitter": "Denis Merigoux", "authors": "Denis Merigoux (PROSECCO), Rapha\\\"el Monat (APR), Jonathan Protzenko\n  (MSR)", "title": "A Modern Compiler for the French Tax Code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In France, income tax is computed from taxpayers' individual returns, using\nan algorithm that is authored, designed and maintained by the French Public\nFinances Directorate (DGFiP). This algorithm relies on a legacy custom language\nand compiler originally designed in 1990, which unlike French wine, did not age\nwell with time. Owing to the shortcomings of the input language and the\ntechnical limitations of the compiler, the algorithm is proving harder and\nharder to maintain, relying on ad-hoc behaviors and workarounds to implement\nthe most recent changes in tax law. Competence loss and aging code also mean\nthat the system does not benefit from any modern compiler techniques that would\nincrease confidence in the implementation. We overhaul this infrastructure and\npresent Mlang, an open-source compiler toolchain whose goal is to replace the\nexisting infrastructure. Mlang is based on a reverse-engineered formalization\nof the DGFiP's system, and has been thoroughly validated against the private\nDGFiP test suite. As such, Mlang has a formal semantics; eliminates previous\nhandwritten workarounds in C; compiles to modern languages (Python); and\nenables a variety of instrumentations, providing deep insights about the\nessence of French income tax computation. The DGFiP is now officially\ntransitioning to Mlang for their production system.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 13:57:00 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 10:53:30 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2021 10:19:30 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Merigoux", "Denis", "", "PROSECCO"], ["Monat", "Rapha\u00ebl", "", "APR"], ["Protzenko", "Jonathan", "", "MSR"]]}, {"id": "2011.07986", "submitter": "Michael Pradel", "authors": "Michael Pradel and Satish Chandra", "title": "Neural Software Analysis", "comments": null, "journal-ref": "Communications of the ACM, 2021", "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many software development problems can be addressed by program analysis\ntools, which traditionally are based on precise, logical reasoning and\nheuristics to ensure that the tools are practical. Recent work has shown\ntremendous success through an alternative way of creating developer tools,\nwhich we call neural software analysis. The key idea is to train a neural\nmachine learning model on numerous code examples, which, once trained, makes\npredictions about previously unseen code. In contrast to traditional program\nanalysis, neural software analysis naturally handles fuzzy information, such as\ncoding conventions and natural language embedded in code, without relying on\nmanually encoded heuristics. This article gives an overview of neural software\nanalysis, discusses when to (not) use it, and presents three example analyses.\nThe analyses address challenging software development problems: bug detection,\ntype prediction, and code completion. The resulting tools complement and\noutperform traditional program analyses, and are used in industrial practice.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 14:32:09 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 10:04:41 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Pradel", "Michael", ""], ["Chandra", "Satish", ""]]}, {"id": "2011.08242", "submitter": "Richard Lin", "authors": "Richard Lin, Bj\\\"orn Hartmann", "title": "Opportunities and Challenges for Circuit Board Level Hardware\n  Description Languages", "comments": "HATRA 2020 workshop paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Board-level hardware description languages (HDLs) are one approach to\nincreasing automation and raising the level of abstraction for designing\nelectronics. These systems borrow programming languages concepts like\ngenerators and type systems, but also must be designed with human factors in\nmind to serve existing hardware engineers. In this work, we look at one recent\nprototype system, and discuss open questions spanning from fundamental models\nthrough usable interfaces.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 19:38:11 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Lin", "Richard", ""], ["Hartmann", "Bj\u00f6rn", ""]]}, {"id": "2011.08881", "submitter": "Andrei Diaconu", "authors": "Andrei Diaconu", "title": "Learning functional programs with function invention and reuse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inductive programming (IP) is a field whose main goal is synthesising\nprograms that respect a set of examples, given some form of background\nknowledge. This paper is concerned with a subfield of IP, inductive functional\nprogramming (IFP). We explore the idea of generating modular functional\nprograms, and how those allow for function reuse, with the aim to reduce the\nsize of the programs. We introduce two algorithms that attempt to solve the\nproblem and explore type based pruning techniques in the context of modular\nprograms. By experimenting with the implementation of one of those algorithms,\nwe show reuse is important (if not crucial) for a variety of problems and\ndistinguished two broad classes of programs that will generally benefit from\nfunction reuse.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 19:11:00 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Diaconu", "Andrei", ""]]}, {"id": "2011.09012", "submitter": "Vishnu Reddy", "authors": "Gongming (Gabriel) Luo, Vishnu Reddy, Marcelo Almeida, Yingying Zhu,\n  Ke Du, Cyrus Omar", "title": "RustViz: Interactively Visualizing Ownership and Borrowing", "comments": "9 pages, 3 figures. Presented at HATRA 2020 (Human Aspects of Types\n  and Reasoning Assistants)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rust is a systems programming language that guarantees memory safety without\nthe need for a garbage collector by statically tracking ownership and borrowing\nevents. The associated rules are subtle and unique among industry programming\nlanguages, which can make learning Rust more challenging. Motivated by the\nchallenges that Rust learners face, we are developing RustViz, a tool that\nallows teachers to generate an interactive timeline depicting ownership and\nborrowing events for each variable in a Rust code example. These visualizations\nare intended to help Rust learners develop an understanding of ownership and\nborrowing by example. This paper introduces RustViz by example, shows how\nteachers can use it to generate visualizations, describes learning goals, and\nproposes a study designed to evaluate RustViz based on these learning goals.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 00:19:27 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Gongming", "", "", "Gabriel"], ["Luo", "", ""], ["Reddy", "Vishnu", ""], ["Almeida", "Marcelo", ""], ["Zhu", "Yingying", ""], ["Du", "Ke", ""], ["Omar", "Cyrus", ""]]}, {"id": "2011.09037", "submitter": "Ankush Das", "authors": "Ankush Das, Di Wang, Jan Hoffmann", "title": "Probabilistic Resource-Aware Session Types", "comments": "technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session types guarantee that message-passing processes adhere to predefined\ncommunication protocols. Prior work on session types has focused on\ndeterministic languages but many message-passing systems, such as Markov chains\nand randomized distributed algorithms, are probabilistic. To model and analyze\nsuch systems, this article introduces probabilistic session types and explores\ntheir application in automatic expected resource analysis. Probabilistic\nsession types describe probability distributions over messages and are a\nconservative extension of intuitionistic (binary) session types. To send on a\nprobabilistic channel, processes have to utilize internal randomness from a\nprobabilistic branching expression or external randomness from receiving on a\nprobabilistic channel. The analysis for expected resource bounds is integrated\nwith the type system and is a variant of automatic amortized resource analysis.\nIt can automatically derive symbolic bounds for different cost metrics by\nreducing type inference to linear constraint solving. The technical\ncontributions include the meta theory that is based on a novel nested\nmultiverse semantics and a type-reconstruction algorithm that allows flexible\nmixing of different sources of randomness without burdening the programmer with\ntype annotations. The type system has been implemented in the language PRast.\nExperiments demonstrate that PRast is applicable in different domains such as\nresource analysis of randomized distributed algorithms, verification of\nlimiting distributions in Markov chains, and analysis of probabilistic digital\ncontracts.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 02:10:31 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Das", "Ankush", ""], ["Wang", "Di", ""], ["Hoffmann", "Jan", ""]]}, {"id": "2011.10373", "submitter": "P\\'eter Bereczky", "authors": "P\\'eter Bereczky, D\\'aniel Horp\\'acsi, Simon Thompson", "title": "A Comparison of Big-step Semantics Definition Styles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal semantics provides rigorous, mathematically precise definitions of\nprogramming languages, with which we can argue about program behaviour and\nprogram equivalence by formal means; in particular, we can describe and verify\nour arguments with a proof assistant. There are various approaches to giving\nformal semantics to programming languages, at different abstraction levels and\napplying different mathematical machinery: the reason for using the semantics\ndetermines which approach to choose.\n  In this paper we investigate some of the approaches that share their roots\nwith traditional relational big-step semantics, such as (a) functional big-step\nsemantics (or, equivalently, a definitional interpreter), (b) pretty-big-step\nsemantics and (c) traditional natural semantics. We compare these approaches\nwith respect to the following criteria: executability of the semantics\ndefinition, proof complexity for typical properties (e.g. determinism) and the\nconciseness of expression equivalence proofs in that approach. We also briefly\ndiscuss the complexity of these definitions and the coinductive big-step\nsemantics, which enables reasoning about divergence.\n  To enable the comparison in practice, we present an example language for\ncomparing the semantics: a sequential subset of Core Erlang, a functional\nprogramming language, which is used in the intermediate steps of the Erlang/OTP\ncompiler. We have already defined a relational big-step semantics for this\nlanguage that includes treatment of exceptions and side effects. The aim of\nthis current work is to compare our big-step definition for this language with\na variety of other equivalent semantics in different styles from the point of\nview of testing and verifying code refactorings.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 12:25:12 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Bereczky", "P\u00e9ter", ""], ["Horp\u00e1csi", "D\u00e1niel", ""], ["Thompson", "Simon", ""]]}, {"id": "2011.10618", "submitter": "Kenji Maillard", "authors": "Meven Lennon-Bertrand, Kenji Maillard, Nicolas Tabareau, \\'Eric Tanter", "title": "Gradualizing the Calculus of Inductive Constructions", "comments": "64 pages (50 + bibliography + appendix), journal submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Acknowledging the ordeal of a fully formal development in a proof assistant\nsuch as Coq, we investigate gradual variations on the Calculus of Inductive\nConstruction (CIC) for swifter prototyping with imprecise types and terms. We\nobserve, with a no-go theorem, a crucial tradeoff between graduality and the\nkey properties of normalization and closure of universes under dependent\nproduct that CIC enjoys. Beyond this Fire Triangle of Graduality, we explore\nthe gradualization of CIC with three different compromises, each relaxing one\nedge of the Fire Triangle. We develop a parametrized presentation of Gradual\nCIC that encompasses all three variations, and develop their metatheory. We\nfirst present a bidirectional elaboration of Gradual CIC to a dependently-typed\ncast calculus, which elucidates the interrelation between typing, conversion,\nand the gradual guarantees. We use a syntactic model into CIC to inform the\ndesign of a safe, confluent reduction, and establish, when applicable,\nnormalization. We also study the stronger notion of graduality as\nembedding-projection pairs formulated by New and Ahmed, using appropriate\nsemantic model constructions. This work informs and paves the way towards the\ndevelopment of malleable proof assistants and dependently-typed programming\nlanguages.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 20:18:35 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Lennon-Bertrand", "Meven", ""], ["Maillard", "Kenji", ""], ["Tabareau", "Nicolas", ""], ["Tanter", "\u00c9ric", ""]]}, {"id": "2011.10789", "submitter": "Philipp Krause", "authors": "Philipp Klaus Krause", "title": "lospre in linear time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Lifetime-optimal speculative partial redundancy elimination (lospre) is the\nmost advanced currently known redundancy elimination technique. It subsumes\nmany previously known approaches, such as common subexpression elimination,\nglobal common subexpression elimination, and loop-invariant code motion.\nHowever, previously known lospre algorithms have high time complexity; faster\nbut less powerful approaches have been used and developed further instead. We\npresent a simple linear-time algorithm for lospre for structured programs that\ncan also handle some more general scenarios compared to previous approaches. We\nprove that our approach is optimal and that the runtime is linear in the number\nof nodes in the control-flow graph. The condition on programs of being\nstructured is automatically true for many programming languages and for others,\nsuch as C, is equivalent to a bound on the number of goto labels per function.\nAn implementation in a mainstream C compiler demonstrates the practical\nfeasibility of our approach. Our approach is based on graph-structure theory\nand uses tree-decompositions. We also show that, for structured programs, the\nruntime of deterministic implementations of the previously known MC-PRE and\nMC-SSAPRE algorithms can be bounded by $O(n^{2.5})$, improving the previous\nbounds of $O(n^3)$.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 13:35:54 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Krause", "Philipp Klaus", ""]]}, {"id": "2011.11063", "submitter": "Eli Sennesh", "authors": "Eli Sennesh", "title": "Learning a Deep Generative Model like a Program: the Free Category Prior", "comments": null, "journal-ref": "AAAI Symposium on Conceptual Abstraction and Analogy in Natural\n  and Artificial Intelligence, Fall 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans surpass the cognitive abilities of most other animals in our ability\nto \"chunk\" concepts into words, and then combine the words to combine the\nconcepts. In this process, we make \"infinite use of finite means\", enabling us\nto learn new concepts quickly and nest concepts within each-other. While\nprogram induction and synthesis remain at the heart of foundational theories of\nartificial intelligence, only recently has the community moved forward in\nattempting to use program learning as a benchmark task itself. The cognitive\nscience community has thus often assumed that if the brain has simulation and\nreasoning capabilities equivalent to a universal computer, then it must employ\na serialized, symbolic representation. Here we confront that assumption, and\nprovide a counterexample in which compositionality is expressed via network\nstructure: the free category prior over programs. We show how our formalism\nallows neural networks to serve as primitives in probabilistic programs. We\nlearn both program structure and model parameters end-to-end.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 17:16:17 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Sennesh", "Eli", ""]]}, {"id": "2011.11763", "submitter": "Viktor Toman", "authors": "Truc Lam Bui, Krishnendu Chatterjee, Tushar Gautam, Andreas\n  Pavlogiannis, Viktor Toman", "title": "The Reads-From Equivalence for the TSO and PSO Memory Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The verification of concurrent programs remains an open challenge due to the\nnon-determinism in inter-process communication. One algorithmic problem in this\nchallenge is the consistency verification of concurrent executions. Consistency\nverification under a reads-from map allows to compute the reads-from (RF)\nequivalence between concurrent traces, with direct applications to areas such\nas Stateless Model Checking (SMC). The RF equivalence was recently shown to be\ncoarser than the standard Mazurkiewicz equivalence, leading to impressive\nscalability improvements for SMC under SC (sequential consistency). However,\nfor the relaxed memory models of TSO and PSO (total/partial store order), the\nalgorithmic problem of deciding the RF equivalence, as well as its impact on\nSMC, has been elusive. In this work we solve the problem of consistency\nverification for the TSO and PSO memory models given a reads-from map, denoted\nVTSO-rf and VPSO-rf, respectively. For an execution of $n$ events over $k$\nthreads and $d$ variables, we establish novel bounds that scale as $n^{k+1}$\nfor TSO and as $n^{k+1}\\cdot \\min(n^{k^2}, 2^{k\\cdot d})$ for PSO. Based on our\nsolution to these problems, we develop an SMC algorithm under TSO and PSO that\nuses the RF equivalence. The algorithm is exploration-optimal, in the sense\nthat it is guaranteed to explore each class of the RF partitioning exactly\nonce, and spends polynomial time per class when $k$ is bounded. We implement\nall our algorithms in the SMC tool Nidhugg, and perform a large number of\nexperiments over benchmarks from existing literature. Our experimental results\nshow that our algorithms for VTSO-rf and VPSO-rf provide significant\nscalability improvements over standard alternatives. When used for SMC, the RF\npartitioning is often much coarser than the standard Shasha-Snir partitioning\nfor TSO/PSO, which yields a significant speedup in the model checking task.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 22:10:08 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 20:25:37 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Bui", "Truc Lam", ""], ["Chatterjee", "Krishnendu", ""], ["Gautam", "Tushar", ""], ["Pavlogiannis", "Andreas", ""], ["Toman", "Viktor", ""]]}, {"id": "2011.12468", "submitter": "Chandra Maddila", "authors": "Chandra Maddila, Sai Surya Upadrasta, Chetan Bansal, Nachiappan\n  Nagappan, Georgios Gousios, Arie van Deursen", "title": "Nudge: Accelerating Overdue Pull Requests Towards Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pull requests are a key part of the collaborative software development and\ncode review process today. However, pull requests can also slow down the\nsoftware development process when the reviewer(s) or the author do not actively\nengage with the pull request. In this work, we design an end-to-end service,\nNudge, for accelerating overdue pull requests towards completion by reminding\nthe author or the reviewer(s) to engage with their overdue pull requests.\nFirst, we use models based on effort estimation and machine learning to predict\nthe completion time for a given pull request. Second, we use activity detection\nto reduce false positives. Lastly, we use dependency determination to\nunderstand the blocker of the pull request and nudge the appropriate\nactor(author or reviewer(s)). We also do a correlation analysis to understand\nthe statistical relationship between the pull request completion times and\nvarious pull request and developer related attributes. Nudge has been deployed\non 147 repositories at Microsoft since 2019. We do a large scale evaluation\nbased on the implicit and explicit feedback we received from sending the Nudge\nnotifications on 8,500 pull requests. We observe significant reduction in\ncompletion time, by over 60%, for pull requests which were nudged thus\nincreasing the efficiency of the code review process and accelerating the pull\nrequest progression.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 01:22:29 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 17:06:22 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Maddila", "Chandra", ""], ["Upadrasta", "Sai Surya", ""], ["Bansal", "Chetan", ""], ["Nagappan", "Nachiappan", ""], ["Gousios", "Georgios", ""], ["van Deursen", "Arie", ""]]}, {"id": "2011.13127", "submitter": "Fredrik Kjolstad", "authors": "Haoran Xu and Fredrik Kjolstad", "title": "Copy-and-Patch Binary Code Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Runtime compilation of runtime-constructed code is becoming standard practice\nin libraries, DSLs, and database management systems. Since compilation is\nexpensive, systems that are sensitive to compile times such as relational\ndatabase query compilers compile only hot code and interprets the rest with a\nmuch slower interpreter.\n  We present a code generation technique that lowers an AST to binary code by\nstitching together code from a large library of binary AST node\nimplementations. We call the implementations stencils because they have holes\nwhere values must be inserted during code generation. We show how to construct\nsuch a stencil library and describe the copy-and-patch technique that generates\noptimized binary code.\n  The result is a code generator with negligible cost: it produces code from an\nAST in less time than it takes to construct the AST. Compared to LLVM,\ncompilation is two orders of magnitude faster than -O0 and three orders of\nmagnitude faster than higher optimization levels. The generated code runs an\norder of magnitude faster than interpretation and runs even faster than LLVM\n-O0. Thus, copy-and-patch can effectively replace both interpreters and LLVM\n-O0, making code generation more effective in compile-time sensitive\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 05:03:16 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Xu", "Haoran", ""], ["Kjolstad", "Fredrik", ""]]}, {"id": "2011.13234", "submitter": "Morteza Mohaqeqi", "authors": "Wang Yi, Morteza Mohaqeqi, Susanne Graf", "title": "MIMOS: A Deterministic Model for the Design and Update of Real-Time\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the pioneering work of Gilles Kahn on concurrent systems, we\npropose to model timed systems as a network of software components (implemented\nas real-time processes or tasks), each of which is specified to compute a\ncollection of functions according to given timing constraints. We present a\nfixed-point semantics for this model which shows that each system function of\nsuch a network computes for a given set of (timed) input streams, a\ndeterministic (timed) output stream. As a desired feature, such a network model\ncan be modified by integrating new components for adding new system functions\nwithout changing the existing ones. Additionally, existing components may be\nreplaced also by new ones fulfilling given requirements. Thanks to the\ndeterministic semantics, a model-based approach is enabled for not only\nbuilding systems but also updating them after deployment, allowing for\nefficient analysis techniques such as model-in-the-loop simulation to verify\nthe complete behaviour of the updated system.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 11:05:52 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Yi", "Wang", ""], ["Mohaqeqi", "Morteza", ""], ["Graf", "Susanne", ""]]}, {"id": "2011.13396", "submitter": "Sahil Verma", "authors": "Sahil Verma and Subhajit Roy", "title": "Debug-Localize-Repair: A Symbiotic Construction for Heap Manipulations", "comments": "24 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Wolverine, an integrated Debug-Localize-Repair environment for\nheap manipulating programs. Wolverine packages a new bug localization algorithm\nthat reduces the search space of repairs and a novel, proof-directed repair\nalgorithm to synthesize the repair patches. While concretely executing a\nprogram, Wolverine displays the abstract program states (as box-and-arrow\ndiagrams) as a visual aid to the programmer. Wolverine supports \"hot-patching\"\nof the generated patches to provide a seamless debugging environment, and the\nbug localization enkindles tremendous speedups in repair timing. Wolverine also\nfacilitates new debug-localize-repair possibilities, specification refinement,\nand checkpoint-based hopping. We evaluate our framework on 6400 buggy programs\n(generated using automated fault injection) on a variety of data-structures\nlike singly, doubly, and circular linked lists, AVL trees, Red-Black trees,\nSplay Trees, and Binary Search Trees; Wolverine could repair all the buggy\ninstances within realistic programmer wait-time (less than 5 sec in most\ncases). Wolverine could also repair more than 80% of the 247 (buggy) student\nsubmissions (where a reasonable attempt was made).\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 17:23:39 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Verma", "Sahil", ""], ["Roy", "Subhajit", ""]]}, {"id": "2011.13451", "submitter": "Wilmer Ricciotti", "authors": "Wilmer Ricciotti and James Cheney", "title": "Strongly-Normalizing Higher-Order Relational Queries", "comments": "33 pages, 6 figures. Extended version of paper published in the\n  proceedings of FSCD 2020", "journal-ref": null, "doi": "10.4230/LIPIcs.FSCD.2020.28", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Language-integrated query is a popular and powerful programming construct\nallowing database queries and ordinary program code to interoperate seamlessly\nand safely. Language-integrated query techniques rely on classical results\nabout the nested relational calculus stating that its queries can be\nalgorithmically translated to SQL, as long as their result type is a flat\nrelation. Cooper and others advocated higher-order nested relational calculi as\na basis for language-integrated queries in functional languages such as Links\nand F#. However, the translation of higher-order relational queries to SQL\nrelies on a rewrite system for which no strong normalization proof has been\npublished: a previous proof attempt does not deal correctly with rewrite rules\nthat duplicate subterms. This paper fills the gap in the literature, explaining\nthe difficulty with the previous attempt, and showing how to extend the\n$\\top\\top$-lifting approach of Lindley and Stark to accommodate duplicating\nrewrites. We also show how to extend the proof to a recently-introduced\ncalculus for heterogeneous queries mixing set and multiset semantics.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 19:27:07 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 16:46:56 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 16:46:53 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Ricciotti", "Wilmer", ""], ["Cheney", "James", ""]]}, {"id": "2011.14044", "submitter": "Tiago Soares", "authors": "Tiago Lopes Soares", "title": "A Deductive Verification Framework For Higher Order Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this report, we present the preliminary work developed for our research\nproject for the APDC (\\'Area Pr\\'atica de Desenvolvimento Curricular) course.\nThe main goal of this project is to develop a framework, on top of the Why3\ntool, for the verification of effectful higher-order programs. We use\ndefunctionalization as an intermediate transformation from higher-order OCaml\nimplementations into first order ones. The target for our translation is WhyML,\nthe Why3's programming language. We believe defunctionalization can be an\ninteresting route for the automated verification of higher-order programs,\nsince one can employ off-the-shelf automated program verifiers to prove the\ncorrectness of the generated first-order program. This report also serves to\nintroduce the reader to the subject of deductive program verification and some\nof the tools and concepts used to prove higher order effectful programs.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 01:22:11 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Soares", "Tiago Lopes", ""]]}, {"id": "2011.14476", "submitter": "Mario Alvarez-Picallo", "authors": "Mario Alvarez-Picallo, C.-H. Luke Ong", "title": "The Difference Lambda-Calculus: A Language for Difference Categories", "comments": "40 pages, to be submitted to a special issue of LMCS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cartesian difference categories are a recent generalisation of Cartesian\ndifferential categories which introduce a notion of \"infinitesimal\" arrows\nsatisfying an analogue of the Kock-Lawvere axiom, with the axioms of a\nCartesian differential category being satisfied only \"up to an infinitesimal\nperturbation\". In this work, we construct a simply-typed calculus in the spirit\nof the differential lambda-calculus equipped with syntactic infinitesimals and\nshow how its models correspond to difference lambda-categories, a family of\nCartesian difference categories equipped with suitably well-behaved\nexponentials.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 00:24:51 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Alvarez-Picallo", "Mario", ""], ["Ong", "C. -H. Luke", ""]]}, {"id": "2011.14617", "submitter": "Yican Sun", "authors": "Jinyi Wang, Yican Sun, Hongfei Fu, Krishnendu Chatterjee and Amir\n  Kafshdar Goharshady", "title": "Quantitative Analysis of Assertion Violations in Probabilistic Programs", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the fundamental problem of deriving quantitative\nbounds on the probability that a given assertion is violated in a probabilistic\nprogram. We provide automated algorithms that obtain both lower and upper\nbounds on the assertion violation probability in exponential forms. The main\nnovelty of our approach is that we prove new and dedicated fixed-point theorems\nwhich serve as the theoretical basis of our algorithms and enable us to reason\nabout assertion violation bounds in terms of pre and post fixed-point\nfunctions. To synthesize such fixed-points, we devise algorithms that utilize a\nwide range of mathematical tools, including repulsing ranking\nsuper-martingales, Hoeffding's lemma, Minkowski decompositions, Jensen's\ninequality, and convex optimization. On the theoretical side, we provide (i)\nthe first automated algorithm for lower-bounds on assertion violation\nprobabilities, (ii) the first complete algorithm for upper-bounds of\nexponential form in affine programs, and (iii) provably and significantly\ntighter upper-bounds than the previous approach of stochastic invariants. On\nthe practical side, we show that our algorithms can handle a wide variety of\nprograms from the literature and synthesize bounds that are several orders of\nmagnitude tighter in comparison with previous approaches.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 08:39:47 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 15:10:25 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Wang", "Jinyi", ""], ["Sun", "Yican", ""], ["Fu", "Hongfei", ""], ["Chatterjee", "Krishnendu", ""], ["Goharshady", "Amir Kafshdar", ""]]}, {"id": "2011.14751", "submitter": "Mat\\'u\\v{s} Sul\\'ir", "authors": "Mat\\'u\\v{s} Sul\\'ir", "title": "Toward a Benchmark Repository for Software Maintenance Tool Evaluations\n  with Humans", "comments": null, "journal-ref": "Proceedings of the 2019 ACM SIGPLAN International Conference on\n  Systems, Programming, Languages, and Applications: Software for Humanity\n  (SPLASH Companion '19), ACM, 2019, pp. 7-8", "doi": "10.1145/3359061.3362776", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To evaluate software maintenance techniques and tools in controlled\nexperiments with human participants, researchers currently use projects and\ntasks selected on an ad-hoc basis. This can unrealistically favor their tool,\nand it makes the comparison of results difficult. We suggest a gradual creation\nof a benchmark repository with projects, tasks, and metadata relevant for\nhuman-based studies. In this paper, we discuss the requirements and challenges\nof such a repository, along with the steps which could lead to its\nconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 13:05:38 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Sul\u00edr", "Mat\u00fa\u0161", ""]]}, {"id": "2011.14989", "submitter": "William Earley", "authors": "William Earley", "title": "The $\\aleph$ Calculus", "comments": "51 pages, 18 figures/listings; update references and acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a need for a model of reversible computation appropriate for a\nBrownian molecular architecture, the $\\aleph$ calculus is introduced. This\nnovel model is declarative, concurrent, and term-based--encapsulating all\ninformation about the program data and state within a single structure in order\nto obviate the need for a von Neumann-style discrete computational 'machine', a\nchallenge in a molecular environment. The name is inspired by the Greek for\n'not forgotten', due to the emphasis on (reversibly) learning and un-learning\nknowledge of different variables. To demonstrate its utility for this purpose,\nas well as its elegance as a programming language, a number of examples are\npresented; two of these examples, addition/subtraction and\nsquaring/square-rooting, are furnished with designs for abstract molecular\nimplementations. A natural by-product of these examples and accompanying\nsyntactic sugar is the design of a fully-fledged programming language, alethe,\nwhich is also presented along with an interpreter. Efficiently simulating\n$\\aleph$ on a deterministic computer necessitates some static analysis of\nprograms within the alethe interpreter in order to render the declarative\nprograms sequential. Finally, work towards a type system appropriate for such a\nreversible, declarative model of computation is presented.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 16:57:46 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 08:29:01 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Earley", "William", ""]]}]