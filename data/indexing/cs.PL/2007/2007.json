[{"id": "2007.00605", "submitter": "Daniel Hillerstr\\\"om", "authors": "Daniel Hillerstr\\\"om, Sam Lindley, and John Longley", "title": "Effects for Efficiency: Asymptotic Speedup with First-Class Control", "comments": null, "journal-ref": "Proc. ACM Program. Lang., Vol. 4, No. ICFP, Article 100, August\n  2020", "doi": "10.1145/3408982", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the fundamental efficiency of delimited control. Specifically, we\nshow that effect handlers enable an asymptotic improvement in runtime\ncomplexity for a certain class of functions. We consider the generic count\nproblem using a pure PCF-like base language $\\lambda_b$ and its extension with\neffect handlers $\\lambda_h$. We show that $\\lambda_h$ admits an asymptotically\nmore efficient implementation of generic count than any $\\lambda_b$\nimplementation. We also show that this efficiency gap remains when $\\lambda_b$\nis extended with mutable state. To our knowledge this result is the first of\nits kind for control operators.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 16:47:58 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 22:05:14 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Hillerstr\u00f6m", "Daniel", ""], ["Lindley", "Sam", ""], ["Longley", "John", ""]]}, {"id": "2007.00616", "submitter": "H\\\"armel Nestra", "authors": "H\\\"armel Nestra", "title": "Equational Reasoning for MTL Type Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ability to use definitions occurring in the code directly in equational\nreasoning is one of the key strengths of functional programming. This is\nimpossible in the case of Haskell type class methods unless a particular\ninstance type is specified, since class methods can be defined differently for\ndifferent instances. To allow uniform reasoning for all instances, many type\nclasses in the Haskell library come along with laws (axioms), specified in\ncomments, that all instances are expected to follow (albeit Haskell is unable\nto force it). For the type classes introduced in the Monad Transformer Library\n(MTL), such laws have not been specified; nevertheless, some sets of axioms\nhave occurred in the literature and the Haskell mailing lists. This paper\ninvestigates sets of laws usable for equational reasoning about methods of the\ntype classes MonadReader and MonadWriter and also reviews analogous earlier\nproposals for the classes MonadError and MonadState. For both MonadReader and\nMonadWriter, an equivalence result of two alternative axiomatizations in terms\nof different sets of operations is established. As a sideline, patterns in the\nchoice of methods of different classes are noticed which may inspire new\ndevelopments in MTL.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 17:06:49 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Nestra", "H\u00e4rmel", ""]]}, {"id": "2007.00695", "submitter": "Dimitur Krustev", "authors": "Dimitur Nikolaev Krustev", "title": "Experience Report: Smuggling a Little Bit of Coq Inside a CAD\n  Development Context (Extended Abstract)", "comments": "Submitted to Coq Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While the use of formal verification techniques is well established in the\ndevelopment of mission-critical software, it is still rare in the production of\nmost other kinds of software. We share our experience that a formal\nverification tool such as Coq can be very useful and practical in the context\nof off-the-shelf software development -- CAD in particular -- at least in some\noccasions. The emphasis is on 3 main areas: factors that can enable the use of\nCoq in an industrial context; some typical examples of tasks, where Coq can\noffer an advantage; examples of issues to overcome - and some non-issues - when\nintegrating Coq in a standard development process.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 18:35:00 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Krustev", "Dimitur Nikolaev", ""]]}, {"id": "2007.01277", "submitter": "Ao Li", "authors": "Ao Li, Bojian Zheng, Gennady Pekhimenko, and Fan Long", "title": "Automatic Horizontal Fusion for GPU Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present automatic horizontal fusion, a novel optimization technique that\ncomplements the standard kernel fusion techniques for GPU programs. Unlike the\nstandard fusion, whose goal is to eliminate intermediate data round trips, our\nhorizontal fusion technique aims to increase the thread-level parallelism to\nhide instruction latencies. We also present HFuse, a new source to source CUDA\ncompiler that implements automatic horizontal fusion. Our experimental results\nshow that horizontal fusion can speed up the running time by 2.5%-60.8%. Our\nresults reveal that the horizontal fusion is especially beneficial for fusing\nkernels with instructions that require different kinds of GPU resources (e.g.,\na memory-intensive kernel and a compute-intensive kernel).\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 17:34:07 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Li", "Ao", ""], ["Zheng", "Bojian", ""], ["Pekhimenko", "Gennady", ""], ["Long", "Fan", ""]]}, {"id": "2007.02220", "submitter": "Roei Schuster", "authors": "Roei Schuster, Congzheng Song, Eran Tromer, Vitaly Shmatikov", "title": "You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion", "comments": "Accepted at USENIX Security '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CL cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code autocompletion is an integral feature of modern code editors and IDEs.\nThe latest generation of autocompleters uses neural language models, trained on\npublic open-source code repositories, to suggest likely (not just statically\nfeasible) completions given the current context.\n  We demonstrate that neural code autocompleters are vulnerable to poisoning\nattacks. By adding a few specially-crafted files to the autocompleter's\ntraining corpus (data poisoning), or else by directly fine-tuning the\nautocompleter on these files (model poisoning), the attacker can influence its\nsuggestions for attacker-chosen contexts. For example, the attacker can \"teach\"\nthe autocompleter to suggest the insecure ECB mode for AES encryption, SSLv3\nfor the SSL/TLS protocol version, or a low iteration count for password-based\nencryption. Moreover, we show that these attacks can be targeted: an\nautocompleter poisoned by a targeted attack is much more likely to suggest the\ninsecure completion for files from a specific repo or specific developer.\n  We quantify the efficacy of targeted and untargeted data- and model-poisoning\nattacks against state-of-the-art autocompleters based on Pythia and GPT-2. We\nthen evaluate existing defenses against poisoning attacks and show that they\nare largely ineffective.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 01:13:36 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 21:34:38 GMT"}, {"version": "v3", "created": "Thu, 8 Oct 2020 23:12:25 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Schuster", "Roei", ""], ["Song", "Congzheng", ""], ["Tromer", "Eran", ""], ["Shmatikov", "Vitaly", ""]]}, {"id": "2007.02366", "submitter": "Vlado Keselj", "authors": "Vlado Keselj", "title": "Starfish: A Prototype for Universal Preprocessing and Text-Embedded\n  Programming", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel concept of universal text preprocessing and text-embedded\nprogramming (PTEP). Preprocessing and text-embedded programming has been widely\nused in programming languages and frameworks in a fragmented and mutually\nisolated way. The PTEP ideas can be found in the implementation of the \\TeX\\\ntypesetting system; they are prominent in PHP and similar web languages, and\nfinally they are used in the Jupyter data science framework. This paper\npresents this area of research and related work in a more unified framework,\nand we describe the implemented system Starfish that satisfies the following\nnovel principles of PTEP: universality, update and replace modes, flexiblity,\nconfigurability, and transparency. We describe the operating model and design\nof Starfish, which is an open-source system implementing universal\npreprocessing and text-embedded programming in Perl. The system is transparent\nand its design allows direct implementation in other programming languages as\nwell.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 15:37:44 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Keselj", "Vlado", ""]]}, {"id": "2007.02699", "submitter": "K. R. Chowdhary", "authors": "K. R. Chowdhary", "title": "On the Evolution of Programming Languages", "comments": "UGC National Conference on \"New Advances in Programming languages and\n  their implementation\", March 15-16, 2013 (APL-2013), Dept. of Computer\n  Science and Engineering, MBM Engineering College, JNV Univ. jodhpur, India.\n  (6 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper attempts to connects the evolution of computer languages with the\nevolution of life, where the later has been dictated by \\emph{theory of\nevolution of species}, and tries to give supportive evidence that the new\nlanguages are more robust than the previous, carry-over the mixed features of\nolder languages, such that strong features gets added into them and weak\nfeatures of older languages gets removed. In addition, an analysis of most\nprominent programming languages is presented, emphasizing on how the features\nof existing languages have influenced the development of new programming\nlanguages. At the end, it suggests a set of experimental languages, which may\nrule the world of programming languages in the time of new multi-core\narchitectures.\n  Index terms- Programming languages' evolution, classifications of languages,\nfuture languages, scripting-languages.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 10:18:14 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Chowdhary", "K. R.", ""]]}, {"id": "2007.03075", "submitter": "David A. Plaisted", "authors": "David Plaisted and Lee Barnett", "title": "A Term-Rewriting Semantics for Imperative Style Programming", "comments": "This paper was submitted to FSCD 2020 on December 24, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Term rewriting systems have a simple syntax and semantics and facilitate\nproofs of correctness. However, they are not as popular in industry or academia\nas imperative languages. We define a term rewriting based abstract programming\nlanguage with an imperative style and a precise semantics allowing programs to\nbe translatable into efficient imperative languages, to obtain proofs of\ncorrectness together with efficient execution. This language is designed to\nfacilitate translations into correct programs in imperative languages with\nassignment statements, iteration, recursion, arrays, pointers, and side\neffects. It can also be used in place of a pseudo-programming language to\nspecify algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 21:26:15 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Plaisted", "David", ""], ["Barnett", "Lee", ""]]}, {"id": "2007.03656", "submitter": "Hiroshi Unno", "authors": "Hiroshi Unno and Yuki Satake and Tachio Terauchi and Eric Koskinen", "title": "Program Verification via Predicate Constraint Satisfiability Modulo\n  Theories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a verification framework based on a new class of\npredicate Constraint Satisfaction Problems called pCSP where constraints are\nrepresented as clauses modulo first-order theories over function variables and\npredicate variables that may represent well-founded predicates. The\nverification framework generalizes an existing one based on Constrained Horn\nClauses (CHCs) to arbitrary clauses, function variables, and well-foundedness\nconstraints. While it is known that the satisfiability of CHCs and the validity\nof queries for Constrained Logic Programs (CLP) are inter-reducible, we show\nthat, thanks to the added expressiveness, pCSP is expressive enough to express\nmuCLP queries. muCLP itself is a new extension of CLP that we propose in this\npaper. It extends CLP with arbitrarily nested inductive and co-inductive\npredicates and is equi-expressive as first-order fixpoint logic. We show that\nmuCLP can naturally encode a wide variety of verification problems including\nbut not limited to termination/non-termination verification and even full modal\nmu-calculus model checking of programs written in various languages. To\nestablish our verification framework, we present (1) a sound and complete\nreduction algorithm from muCLP to pCSP and (2) a constraint solving method for\npCSP based on stratified CounterExample-Guided Inductive Synthesis (CEGIS) of\n(co-)inductive invariants, ranking functions, and Skolem functions witnessing\nexistential quantifiers. Stratified CEGIS combines CEGIS with stratified\nfamilies of templates to achieve relative completeness and faster and stable\nconvergence of CEGIS by avoiding the overfitting problem. We have implemented\nthe proposed framework and obtained promising results on diverse verification\nproblems that are beyond the scope of the previous verification frameworks\nbased on CHCs.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 17:46:45 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Unno", "Hiroshi", ""], ["Satake", "Yuki", ""], ["Terauchi", "Tachio", ""], ["Koskinen", "Eric", ""]]}, {"id": "2007.03936", "submitter": "Ali Kassem", "authors": "Chukri Soueidi, Ali Kassem and Yli\\`es Falcone (Univ. Grenoble Alpes,\n  Inria, CNRS, Grenoble INP, LIG, Grenoble, France)", "title": "BISM: Bytecode-Level Instrumentation for Software Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BISM (Bytecode-Level Instrumentation for Software Monitoring) is a\nlightweight bytecode instrumentation tool that features an expressive\nhigh-level control-flow-aware instrumentation language. The language follows\nthe aspect-oriented programming paradigm by adopting the joinpoint model,\nadvice inlining, and separate instrumentation mechanisms. BISM provides\njoinpoints ranging from bytecode instruction to method execution, access to\ncomprehensive static and dynamic context information, and instrumentation\nmethods. BISM runs in two instrumentation modes: build-time and load-time. We\ndemonstrate BISM effectiveness using two experiments: a security scenario and a\ngeneral runtime verification case. The results show that BISM instrumentation\nincurs low runtime and memory overheads.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 07:41:40 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 08:33:18 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Soueidi", "Chukri", "", "Univ. Grenoble Alpes,\n  Inria, CNRS, Grenoble INP, LIG, Grenoble, France"], ["Kassem", "Ali", "", "Univ. Grenoble Alpes,\n  Inria, CNRS, Grenoble INP, LIG, Grenoble, France"], ["Falcone", "Yli\u00e8s", "", "Univ. Grenoble Alpes,\n  Inria, CNRS, Grenoble INP, LIG, Grenoble, France"]]}, {"id": "2007.04621", "submitter": "Yuichi Nishiwaki", "authors": "Yuichi Nishiwaki, Toshiya Asai", "title": "Logic of computational semi-effects and categorical gluing for\n  equivariant functors", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we revisit Moggi's celebrated calculus of computational\neffects from the perspective of logic of monoidal action (actegory). Our\ndevelopment takes the following steps. Firstly, we perform proof-theoretic\nreconstruction of Moggi's computational metalanguage and obtain a type theory\nwith a modal type $\\rhd$ as a refinement. Through the proposition-as-type\nparadigm, its logic can be seen as a decomposition of lax logic via Benton's\nadjoint calculus. This calculus models as a programming language a weaker\nversion of effects, which we call \\emph{semi-effects}. Secondly, we give its\nsemantics using actegories and equivariant functors. Compared to previous\nstudies of effects and actegories, our approach is more general in that models\nare directly given by equivariant functors, which include Freyd categories\n(hence strong monads) as a special case. Thirdly, we show that categorical\ngluing along equivariant functors is possible and derive logical predicates for\n$\\rhd$-modality. We also show that this gluing, under a natural assumption,\ngives rise to logical predicates that coincide with those derived by\nKatsumata's categorical $\\top\\top$-lifting for Moggi's metalanguage.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 08:13:07 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Nishiwaki", "Yuichi", ""], ["Asai", "Toshiya", ""]]}, {"id": "2007.04691", "submitter": "Marco Maggesi", "authors": "Marco Maggesi, Massimo Nocentini", "title": "Kanren Light: A Dynamically Semi-Certified Interactive Logic Programming\n  System", "comments": "Accepted for communication to miniKanren 2020 - miniKanren and\n  Relational Programming Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an experimental system strongly inspired by miniKanren,\nimplemented on top of the tactics mechanism of the HOL~Light theorem prover.\nOur tool is at the same time a mechanism for enabling the logic programming\nstyle for reasoning and computing in a theorem prover, and a framework for\nwriting logic programs that produce solutions endowed with a formal proof of\ncorrectness.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 10:34:39 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Maggesi", "Marco", ""], ["Nocentini", "Massimo", ""]]}, {"id": "2007.04973", "submitter": "Paras Jain", "authors": "Paras Jain, Ajay Jain, Tianjun Zhang, Pieter Abbeel, Joseph E.\n  Gonzalez, Ion Stoica", "title": "Contrastive Code Representation Learning", "comments": "Code available at https://github.com/parasj/contracode", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL cs.SE stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recent work learns contextual representations of source code by\nreconstructing tokens from their context. For downstream semantic understanding\ntasks like summarizing code in English, these representations should ideally\ncapture program functionality. However, we show that the popular\nreconstruction-based BERT model is sensitive to source code edits, even when\nthe edits preserve semantics. We propose ContraCode: a contrastive pre-training\ntask that learns code functionality, not form. ContraCode pre-trains a neural\nnetwork to identify functionally similar variants of a program among many\nnon-equivalent distractors. We scalably generate these variants using an\nautomated source-to-source compiler as a form of data augmentation. Contrastive\npre-training improves JavaScript summarization and TypeScript type inference\naccuracy by 2% to 13%. We also propose a new zero-shot JavaScript code clone\ndetection dataset, showing that ContraCode is both more robust and semantically\nmeaningful. On it, we outperform RoBERTa by 39% AUROC in an adversarial setting\nand up to 5% on natural code.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:59:06 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 05:30:35 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 17:58:44 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Jain", "Paras", ""], ["Jain", "Ajay", ""], ["Zhang", "Tianjun", ""], ["Abbeel", "Pieter", ""], ["Gonzalez", "Joseph E.", ""], ["Stoica", "Ion", ""]]}, {"id": "2007.05282", "submitter": "Matthijs V\\'ak\\'ar", "authors": "Matthijs V\\'ak\\'ar", "title": "Denotational Correctness of Foward-Mode Automatic Differentiation for\n  Iteration and Recursion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present semantic correctness proofs of forward-mode Automatic\nDifferentiation (AD) for languages with sources of partiality such as partial\noperations, lazy conditionals on real parameters, iteration, and term and type\nrecursion. We first define an AD macro on a standard call-by-value language\nwith some primitive operations for smooth partial functions and constructs for\nreal conditionals and iteration, as a unique structure preserving macro\ndetermined by its action on the primitive operations. We define a semantics for\nthe language in terms of diffeological spaces, where the key idea is to make\nuse of a suitable partiality monad. A semantic logical relations argument,\nconstructed through a subsconing construction over diffeological spaces, yields\na correctness proof of the defined AD macro. A key insight is that, to reason\nabout differentiation at sum types, we work with relations which form sheaves.\nNext, we extend our language with term and type recursion. To model this in our\nsemantics, we introduce a new notion of space, suitable for modeling both\nrecursion and differentiation, by equipping a diffeological space with a\ncompatible $\\omega$cpo-structure. We demonstrate that our whole development\nextends to this setting. By making use of a semantic, rather than syntactic,\nlogical relations argument, we circumvent the usual technicalities of logical\nrelations techniques for type recursion.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 10:05:32 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["V\u00e1k\u00e1r", "Matthijs", ""]]}, {"id": "2007.05283", "submitter": "Matthijs V\\'ak\\'ar", "authors": "Matthijs V\\'ak\\'ar", "title": "Reverse AD at Higher Types: Pure, Principled and Denotationally Correct", "comments": null, "journal-ref": "Proc. ESOP 2021", "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to define forward- and reverse-mode automatic differentiation\nsource-code transformations or on a standard higher-order functional language.\nThe transformations generate purely functional code, and they are principled in\nthe sense that their definition arises from a categorical universal property.\nWe give a semantic proof of correctness of the transformations. In their most\nelegant formulation, the transformations generate code with linear types.\nHowever, we demonstrate how the transformations can be implemented in a\nstandard functional language without sacrificing correctness. To do so, we make\nuse of abstract data types to represent the required linear types, e.g. through\nthe use of a basic module system.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 10:05:46 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 10:02:59 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["V\u00e1k\u00e1r", "Matthijs", ""]]}, {"id": "2007.05569", "submitter": "Qirun Zhang", "authors": "Qirun Zhang", "title": "Conditional Lower Bound for Inclusion-Based Points-to Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inclusion-based (i.e., Andersen-style) points-to analysis is a fundamental\nstatic analysis problem. The seminal work of Andersen gave a worst-case cubic\n$O(n^3)$ time points-to analysis algorithm for C, where $n$ is proportional to\nthe number of program variables. An algorithm is truly subcubic if it runs in\n$O(n^{3-\\delta})$ time for some $\\delta > 0$. Despite decades of extensive\neffort on improving points-to analysis, the cubic bound remains unbeaten. The\nbest combinatorial analysis algorithms have a \"slightly subcubic\" $O(n^3 /\n\\text{log } n)$ complexity. It is an interesting open problem whether points-to\nanalysis can be solved in truly subcubic time.\n  In this paper, we prove that a truly subcubic $O(n^{3-\\delta})$ time\ncombinatorial algorithm for inclusion-based points-to analysis is unlikely: a\ntruly subcubic combinatorial points-to analysis algorithm implies a truly\nsubcubic combinatorial algorithm for Boolean Matrix Multiplication (BMM). BMM\nis a well-studied problem, and no truly subcubic combinatorial BMM algorithm\nhas been known. The fastest combinatorial BMM algorithms run in time $O(n^3/\n\\text{log}^4 n)$.\n  Our result includes a simplified proof of the BMM-hardness of\nDyck-reachability. The reduction is interesting in its own right. First, it is\nslightly stronger than the existing BMM-hardness results because our reduction\nonly requires one type of parenthesis in Dyck-reachability\n($D_1$-reachability). Second, we formally attribute the \"cubic bottleneck\" to\nthe need to solve $D_1$-reachability, which captures the semantics of pointer\nreferences/dereferences. This new perspective enables a more general reduction\nthat applies to programs with arbitrary pointer statements types. Last, our\nreduction based on $D_1$-reachability shows that demand-driven points-to\nanalysis is as hard as the exhaustive counterpart.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 18:49:46 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 02:14:30 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhang", "Qirun", ""]]}, {"id": "2007.06093", "submitter": "Zi Wang", "authors": "Zi Wang, Aws Albarghouthi, Gautam Prakriya, Somesh Jha", "title": "Interval Universal Approximation for Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To verify safety and robustness of neural networks, researchers have\nsuccessfully applied abstract interpretation, primarily using the interval\nabstract domain. In this paper, we study the theoretical power and limits of\nthe interval domain for neural-network verification.\n  First, we introduce the interval universal approximation (IUA) theorem. IUA\nshows that neural networks not only can approximate any continuous function $f$\n(universal approximation) as we have known for decades, but we can find a\nneural network, using any well-behaved activation function, whose interval\nbounds are an arbitrarily close approximation of the set semantics of $f$ (the\nresult of applying $f$ to a set of inputs). We call this notion of\napproximation interval approximation. Our theorem generalizes the recent result\nof Baader et al. (2020) from ReLUs to a rich class of activation functions that\nwe call squashable functions. Additionally, the IUA theorem implies that we can\nalways construct provably robust neural networks under $\\ell_\\infty$-norm using\nalmost any practical activation function.\n  Second, we study the computational complexity of constructing neural networks\nthat are amenable to precise interval analysis. This is a crucial question, as\nour constructive proof of IUA is exponential in the size of the approximation\ndomain. We boil this question down to the problem of approximating the range of\na neural network with squashable activation functions. We show that the range\napproximation problem (RA) is a $\\Delta_2$-intermediate problem, which is\nstrictly harder than $\\mathsf{NP}$-complete problems, assuming\n$\\mathsf{coNP}\\not\\subset \\mathsf{NP}$. As a result, IUA is an inherently hard\nproblem: No matter what abstract domain or computational tools we consider to\nachieve interval approximation, there is no efficient construction of such a\nuniversal approximator.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 20:43:56 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 16:12:48 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 19:21:37 GMT"}, {"version": "v4", "created": "Mon, 8 Feb 2021 07:24:08 GMT"}, {"version": "v5", "created": "Wed, 14 Jul 2021 05:51:30 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Wang", "Zi", ""], ["Albarghouthi", "Aws", ""], ["Prakriya", "Gautam", ""], ["Jha", "Somesh", ""]]}, {"id": "2007.06327", "submitter": "Lutz Klinkenberg", "authors": "Lutz Klinkenberg, Kevin Batz, Benjamin Lucien Kaminski, Joost-Pieter\n  Katoen, Joshua Moerman, Tobias Winkler", "title": "Generating Functions for Probabilistic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the usage of generating functions (GFs) encoding\nmeasures over the program variables for reasoning about discrete probabilistic\nprograms. To that end, we define a denotational GF-transformer semantics for\nprobabilistic while-programs, and show that it instantiates Kozen's seminal\ndistribution transformer semantics. We then study the effective usage of GFs\nfor program analysis. We show that finitely expressible GFs enable checking\nsuper-invariants by means of computer algebra tools, and that they can be used\nto determine termination probabilities. The paper concludes by characterizing a\nclass of -- possibly infinite-state -- programs whose semantics is a rational\nGF encoding a discrete phase-type distribution.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 11:47:01 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Klinkenberg", "Lutz", ""], ["Batz", "Kevin", ""], ["Kaminski", "Benjamin Lucien", ""], ["Katoen", "Joost-Pieter", ""], ["Moerman", "Joshua", ""], ["Winkler", "Tobias", ""]]}, {"id": "2007.06421", "submitter": "David Naumann", "authors": "David A. Naumann", "title": "Thirty-seven years of relational Hoare logic: remarks on its principles\n  and history", "comments": "A version appears in proceedings of ISOLA 2020. Version2: fix typos,\n  minor clarifications, add a citation. Version3: copy edits, add citations on\n  completeness. Version 4: minor corrections. Version 5: restore missing\n  precond in loop rule", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational Hoare logics extend the applicability of modular, deductive\nverification to encompass important 2-run properties including dependency\nrequirements such as confidentiality and program relations such as equivalence\nor similarity between program versions. A considerable number of recent works\nintroduce different relational Hoare logics without yet converging on a core\nset of proof rules. This paper looks backwards to little known early work. This\nbrings to light some principles that clarify and organize the rules as well as\nsuggesting a new rule and a new notion of completeness.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 14:53:22 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 22:25:19 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 16:07:12 GMT"}, {"version": "v4", "created": "Fri, 29 Jan 2021 02:19:59 GMT"}, {"version": "v5", "created": "Thu, 3 Jun 2021 14:48:32 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Naumann", "David A.", ""]]}, {"id": "2007.06677", "submitter": "Elizabeth Polgreen", "authors": "Nicolas Chan, Elizabeth Polgreen and Sanjit A. Seshia", "title": "Gradient Descent over Metagrammars for Syntax-Guided Synthesis", "comments": "5 pages, SYNT 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.LG cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The performance of a syntax-guided synthesis algorithm is highly dependent on\nthe provision of a good syntactic template, or grammar. Provision of such a\ntemplate is often left to the user to do manually, though in the absence of\nsuch a grammar, state-of-the-art solvers will provide their own default\ngrammar, which is dependent on the signature of the target program to be\nsythesized. In this work, we speculate this default grammar could be improved\nupon substantially. We build sets of rules, or metagrammars, for constructing\ngrammars, and perform a gradient descent over these metagrammars aiming to find\na metagrammar which solves more benchmarks and on average faster. We show the\nresulting metagrammar enables CVC4 to solve 26% more benchmarks than the\ndefault grammar within a 300s time-out, and that metagrammars learnt from tens\nof benchmarks generalize to performance on 100s of benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 20:37:35 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 18:31:10 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Chan", "Nicolas", ""], ["Polgreen", "Elizabeth", ""], ["Seshia", "Sanjit A.", ""]]}, {"id": "2007.06760", "submitter": "Federico Mora", "authors": "Federico Mora, Kevin Cheang, Elizabeth Polgreen, Sanjit A. Seshia", "title": "Synthesis in Uclid5", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an integration of program synthesis into Uclid5, a formal\nmodelling and verification tool. To the best of our knowledge, the new version\nof Uclid5 is the only tool that supports program synthesis with bounded model\nchecking, k-induction, sequential program verification, and hyperproperty\nverification. We use the integration to generate 25 program synthesis\nbenchmarks with simple, known solutions that are out of reach of current\nsynthesis engines, and we release the benchmarks to the community.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 01:39:36 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 02:21:41 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Mora", "Federico", ""], ["Cheang", "Kevin", ""], ["Polgreen", "Elizabeth", ""], ["Seshia", "Sanjit A.", ""]]}, {"id": "2007.06835", "submitter": "Nagarajan Natarajan", "authors": "Nagarajan Natarajan, Ajaykrishna Karthikeyan, Prateek Jain, Ivan\n  Radicek, Sriram Rajamani, Sumit Gulwani, Johannes Gehrke", "title": "Programming by Rewards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalize and study ``programming by rewards'' (PBR), a new approach for\nspecifying and synthesizing subroutines for optimizing some quantitative metric\nsuch as performance, resource utilization, or correctness over a benchmark. A\nPBR specification consists of (1) input features $x$, and (2) a reward function\n$r$, modeled as a black-box component (which we can only run), that assigns a\nreward for each execution. The goal of the synthesizer is to synthesize a\n\"decision function\" $f$ which transforms the features to a decision value for\nthe black-box component so as to maximize the expected reward $E[r \\circ f\n(x)]$ for executing decisions $f(x)$ for various values of $x$. We consider a\nspace of decision functions in a DSL of loop-free if-then-else programs, which\ncan branch on linear functions of the input features in a tree-structure and\ncompute a linear function of the inputs in the leaves of the tree. We find that\nthis DSL captures decision functions that are manually written in practice by\nprogrammers. Our technical contribution is the use of continuous-optimization\ntechniques to perform synthesis of such decision functions as if-then-else\nprograms. We also show that the framework is theoretically-founded ---in cases\nwhen the rewards satisfy nice properties, the synthesized code is optimal in a\nprecise sense.\n  We have leveraged PBR to synthesize non-trivial decision functions related to\nsearch and ranking heuristics in the PROSE codebase (an industrial strength\nprogram synthesis framework) and achieve competitive results to manually\nwritten procedures over multiple man years of tuning. We present empirical\nevaluation against other baseline techniques over real-world case studies\n(including PROSE) as well on simple synthetic benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 05:49:14 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Natarajan", "Nagarajan", ""], ["Karthikeyan", "Ajaykrishna", ""], ["Jain", "Prateek", ""], ["Radicek", "Ivan", ""], ["Rajamani", "Sriram", ""], ["Gulwani", "Sumit", ""], ["Gehrke", "Johannes", ""]]}, {"id": "2007.07047", "submitter": "Jianjun Zhao", "authors": "Jianjun Zhao", "title": "Quantum Software Engineering: Landscapes and Horizons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum software plays a critical role in exploiting the full potential of\nquantum computing systems. As a result, it is drawing increasing attention\nrecently. This paper defines the term \"quantum software engineering\" and\nintroduces a quantum software life cycle. Based on these, the paper provides a\ncomprehensive survey of the current state of the art in the field and presents\nthe challenges and opportunities that we face. The survey summarizes the\ntechnology available in the various phases of the quantum software life cycle,\nincluding quantum software requirements analysis, design, implementation, test,\nand maintenance. It also covers the crucial issue of quantum software reuse.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 14:13:44 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Zhao", "Jianjun", ""]]}, {"id": "2007.07593", "submitter": "Jurriaan Rot", "authors": "Jana Wagemaker and Paul Brunet and Simon Docherty and Tobias Kapp\\'e\n  and Jurriaan Rot and Alexandra Silva", "title": "Partially Observable Concurrent Kleene Algebra", "comments": "Accepted for publication at CONCUR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.FL cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce partially observable concurrent Kleene algebra (POCKA), an\nalgebraic framework to reason about concurrent programs with control\nstructures, such as conditionals and loops. POCKA enables reasoning about\nprograms that can access variables and values, which we illustrate through\nconcrete examples. We prove that POCKA is a sound and complete axiomatisation\nof a model of partial observations, and show the semantics passes an important\ncheck for sequential consistency.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 10:16:36 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 16:27:27 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Wagemaker", "Jana", ""], ["Brunet", "Paul", ""], ["Docherty", "Simon", ""], ["Kapp\u00e9", "Tobias", ""], ["Rot", "Jurriaan", ""], ["Silva", "Alexandra", ""]]}, {"id": "2007.08017", "submitter": "Jesse Michel", "authors": "Benjamin Sherman, Jesse Michel, Michael Carbin", "title": "$\\lambda_S$: Computable Semantics for Differentiable Programming with\n  Higher-Order Functions and Datatypes", "comments": "31 pages, 10 figures", "journal-ref": null, "doi": "10.1145/3434284", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is moving towards increasingly sophisticated optimization\nobjectives that employ higher-order functions, such as integration, continuous\noptimization, and root-finding. Since differentiable programming frameworks\nsuch as PyTorch and TensorFlow do not have first-class representations of these\nfunctions, developers must reason about the semantics of such objectives and\nmanually translate them to differentiable code.\n  We present a differentiable programming language, $\\lambda_S$, that is the\nfirst to deliver a semantics for higher-order functions, higher-order\nderivatives, and Lipschitz but nondifferentiable functions. Together, these\nfeatures enable $\\lambda_S$ to expose differentiable, higher-order functions\nfor integration, optimization, and root-finding as first-class functions with\nautomatically computed derivatives. $\\lambda_S$'s semantics is computable,\nmeaning that values can be computed to arbitrary precision, and we implement\n$\\lambda_S$ as an embedded language in Haskell.\n  We use $\\lambda_S$ to construct novel differentiable libraries for\nrepresenting probability distributions, implicit surfaces, and generalized\nparametric surfaces -- all as instances of higher-order datatypes -- and\npresent case studies that rely on computing the derivatives of these\nhigher-order functions and datatypes. In addition to modeling existing\ndifferentiable algorithms, such as a differentiable ray tracer for implicit\nsurfaces, without requiring any user-level differentiation code, we demonstrate\nnew differentiable algorithms, such as the Hausdorff distance of generalized\nparametric surfaces.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 22:11:48 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 17:23:29 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Sherman", "Benjamin", ""], ["Michel", "Jesse", ""], ["Carbin", "Michael", ""]]}, {"id": "2007.08048", "submitter": "Antonia Lopes", "authors": "Nuno Burnay and Ant\\'onia Lopes and Vasco T. Vasconcelos", "title": "SafeRESTScript: Statically Checking REST API Consumers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumption of REST services has become a popular means of invoking code\nprovided by third parties, particularly in web applications. Nowadays\nprogrammers of web applications can choose TypeScript over JavaScript to\nbenefit from static type checking that enables validating calls to local\nfunctions or to those provided by libraries. Errors in calls to REST services,\nhowever, can only be found at run-time. In this paper, we present\nSafeRESTScript (SRS, for short) a language that extends the support of static\nanalysis to calls to REST services, with the ability to statically find common\nerrors such as missing or invalid data in REST calls and misuse of the results\nfrom such calls. SafeRESTScript features a syntax similar to JavaScript and is\nequipped with (i) a rich collection of types (including objects, arrays and\nrefinement types)and (ii) primitives to natively support REST calls that are\nstatically validated against specifications of the corresponding APIs.\nSpecifications are written in HeadREST, a language that also features\nrefinement types and supports the description of semantic aspects of REST APIs\nin a style reminiscent of Hoare triples. We present SafeRESTScript and its\nvalidation system, based on a general-purpose verification tool (Boogie). The\nevaluation of SafeRESTScript and of the prototype implementations for its\nvalidator, available in the form of an Eclipse plugin, is also discussed.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 00:38:27 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Burnay", "Nuno", ""], ["Lopes", "Ant\u00f3nia", ""], ["Vasconcelos", "Vasco T.", ""]]}, {"id": "2007.08222", "submitter": "Ashish Rajendra Sai", "authors": "Ashish Rajendra Sai, Conor Holmes, Jim Buckley and Andrew Le Gear", "title": "Inheritance software metrics on smart contracts", "comments": "Accepted by International Conference on Program Comprehension (ICPC\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain systems have gained substantial traction recently, partly due to\nthe potential of decentralized immutable mediation of economic activities.\nEthereum is a prominent example that has the provision for executing stateful\ncomputing scripts known as Smart Contracts. These smart contracts resemble\ntraditional programs, but with immutability being the core differentiating\nfactor. Given their immutability and potential high monetary value, it becomes\nimperative to develop high-quality smart contracts. Software metrics have\ntraditionally been an essential tool in determining programming quality. Given\nthe similarity between smart contracts (written in Solidity for Ethereum) and\nobject-oriented (OO) programming, OO metrics would appear applicable. In this\npaper, we empirically evaluate inheritance-based metrics as applied to smart\ncontracts. We adopt this focus because, traditionally, inheritance has been\nlinked to a more complex codebase which we posit is not the case with Solidity\nbased smart contracts. In this work, we evaluate the hypothesis that, due to\nthe differences in the context of smart contracts and OO programs, it may not\nbe appropriate to use the same interpretation of inheritance based metrics for\nassessment.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 09:49:40 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Sai", "Ashish Rajendra", ""], ["Holmes", "Conor", ""], ["Buckley", "Jim", ""], ["Gear", "Andrew Le", ""]]}, {"id": "2007.08638", "submitter": "Michael Wolman", "authors": "Marcin Sabok, Sam Staton, Dario Stein, Michael Wolman", "title": "Probabilistic Programming Semantics for Name Generation", "comments": "29 pages, 1 figure; to be published in POPL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We make a formal analogy between random sampling and fresh name generation.\nWe show that quasi-Borel spaces, a model for probabilistic programming, can\nsoundly interpret Stark's $\\nu$-calculus, a calculus for name generation.\nMoreover, we prove that this semantics is fully abstract up to first-order\ntypes. This is surprising for an 'off-the-shelf' model, and requires a novel\nanalysis of probability distributions on function spaces. Our tools are diverse\nand include descriptive set theory and normal forms for the $\\nu$-calculus.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 21:06:07 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 14:07:59 GMT"}, {"version": "v3", "created": "Sun, 20 Dec 2020 20:11:24 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Sabok", "Marcin", ""], ["Staton", "Sam", ""], ["Stein", "Dario", ""], ["Wolman", "Michael", ""]]}, {"id": "2007.08926", "submitter": "Gordon Plotkin", "authors": "Martin Abadi and Gordon Plotkin", "title": "Smart Choices and the Selection Monad", "comments": "Improved resulss and presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Describing systems in terms of choices and their resulting costs and rewards\noffers the promise of freeing algorithm designers and programmers from\nspecifying how those choices should be made; in implementations, the choices\ncan be realized by optimization techniques and,increasingly, by\nmachine-learning methods. We study this approach from a programming-language\nperspective. We define two small languages that support decision-making\nabstractions: one with choices and rewards, and the other additionally with\nprobabilities. We give both operational and denotational semantics.\n  In the case of the second language we consider three denotational semantics,\nwith varying degrees of correlation between possible program values and\nexpected rewards. The operational semantics combine the usual semantics of\nstandard constructs with optimization over spaces of possible execution\nstrategies. The denotational semantics, which are compositional rely on the\nselection monad, to handle choice, augmented with an auxiliary monad to handle\nother effects, such as rewards or probability.\n  We establish adequacy theorems that the two semantics coincide in all cases.\nWe also prove full abstraction at base types, with varying notions of\nobservation in the probabilistic case corresponding to the various degrees of\ncorrelation. We present axioms for choice combined with rewards and\nprobability, establishing completeness at base types for the case of rewards\nwithout probability.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 12:13:16 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 18:53:23 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2020 18:33:28 GMT"}, {"version": "v4", "created": "Thu, 11 Mar 2021 11:48:28 GMT"}, {"version": "v5", "created": "Mon, 28 Jun 2021 02:11:21 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Abadi", "Martin", ""], ["Plotkin", "Gordon", ""]]}, {"id": "2007.09436", "submitter": "Konstantinos Kallas", "authors": "Nikos Vasilakis (MIT), Konstantinos Kallas (University of\n  Pennsylvania), Konstantinos Mamouras (Rice University), Achilleas\n  Benetopoulos (Unaffiliated), Lazar Cvetkovi\\'c (University of Belgrade)", "title": "PaSh: Light-touch Data-Parallel Shell Processing", "comments": "18 pages, 12 figures", "journal-ref": null, "doi": "10.1145/3447786.3456228", "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents {\\scshape PaSh}, a system for parallelizing POSIX shell\nscripts. Given a script, {\\scshape PaSh} converts it to a dataflow graph,\nperforms a series of semantics-preserving program transformations that expose\nparallelism, and then converts the dataflow graph back into a script -- one\nthat adds POSIX constructs to explicitly guide parallelism coupled with\n{\\scshape PaSh}-provided {\\scshape Unix}-aware runtime primitives for\naddressing performance- and correctness-related issues. A lightweight\nannotation language allows command developers to express key parallelizability\nproperties about their commands. An accompanying parallelizability study of\nPOSIX and GNU commands -- two large and commonly used groups -- guides the\nannotation language and optimized aggregator library that {\\scshape PaSh} uses.\nFinally, {\\scshape PaSh}'s {\\scshape PaSh}'s extensive evaluation over 44\nunmodified {\\scshape Unix} scripts shows significant speedups\n($0.89$--$61.1\\times$, avg: $6.7\\times$) stemming from the combination of its\nprogram transformations and runtime primitives.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 14:14:11 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 20:24:41 GMT"}, {"version": "v3", "created": "Mon, 4 Jan 2021 02:04:55 GMT"}, {"version": "v4", "created": "Sat, 3 Apr 2021 16:02:11 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Vasilakis", "Nikos", "", "MIT"], ["Kallas", "Konstantinos", "", "University of\n  Pennsylvania"], ["Mamouras", "Konstantinos", "", "Rice University"], ["Benetopoulos", "Achilleas", "", "Unaffiliated"], ["Cvetkovi\u0107", "Lazar", "", "University of Belgrade"]]}, {"id": "2007.09909", "submitter": "Fran\\c{c}ois Bry", "authors": "Fran\\c{c}ois Bry", "title": "Coinduction Plain and Simple", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Coinduction refers to both a technique for the definition of infinite\nstreams, so-called codata, and a technique for proving the equality of\ncoinductively specified codata. This article first reviews coinduction in\ndeclarative programming. Second, it reviews and slightly extends the formalism\ncommonly used for specifying codata. Third, it generalizes the coinduction\nproof principle, which has been originally specified for the equality predicate\nonly, to other predicates. This generalization makes the coinduction proof\nprinciple more intuitive and stresses its closeness with structural induction.\nThe article finally suggests in its conclusion extensions of functional and\nlogic programming with limited and decidable forms of the generalized\ncoinduction proof principle.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 06:52:54 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 08:23:10 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Bry", "Fran\u00e7ois", ""]]}, {"id": "2007.09944", "submitter": "Shankara Narayanan Krishna", "authors": "Parosh Aziz Abdulla, Mohamed Faouzi Atig, Adwait Godbole,\n  Shankaranarayanan Krishna, Viktor Vafeiadis", "title": "The Decidability of Verification under Promising 2.0", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In PLDI'20, Lee et al. introduced the \\emph{promising } semantics PS 2.0 of\nthe C++ concurrency that captures most of the common program transformations\nwhile satisfying the DRF guarantee. The reachability problem for finite-state\nprograms under PS 2.0 with only release-acquire accesses is already known to be\nundecidable. Therefore, we address, in this paper, the reachability problem for\nprograms running under PS 2.0 with relaxed accesses together with promises. We\nshow that this problem is undecidable even in the case where the input program\nhas finite state. Given this undecidability result, we consider the fragment of\nPS 2.0 with only relaxed accesses allowing bounded number of promises. We show\nthat under this restriction, the reachability is decidable, albeit very\nexpensive: it is non-primitive recursive. Given this high complexity with\nbounded number of promises and the undecidability result for the RA fragment of\nPS 2.0, we consider a bounded version of the reachability problem. To this end,\nwe bound both the number of promises and the \"view-switches\", i.e, the number\nof times the processes may switch their local views of the global memory. We\nprovide a code-to-code translation from an input program under PS 2.0, with\nrelaxed and release-acquire memory accesses along with promises, to a program\nunder SC. This leads to a reduction of the bounded reachability problem under\nPS 2.0 to the bounded context-switching problem under SC. We have implemented a\nprototype tool and tested it on a set of benchmarks, demonstrating that many\nbugs in programs can be found using a small bound.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 08:46:40 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 05:19:49 GMT"}, {"version": "v3", "created": "Sat, 25 Jul 2020 14:25:48 GMT"}, {"version": "v4", "created": "Wed, 9 Sep 2020 16:14:29 GMT"}, {"version": "v5", "created": "Fri, 16 Oct 2020 12:29:51 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Abdulla", "Parosh Aziz", ""], ["Atig", "Mohamed Faouzi", ""], ["Godbole", "Adwait", ""], ["Krishna", "Shankaranarayanan", ""], ["Vafeiadis", "Viktor", ""]]}, {"id": "2007.09946", "submitter": "Kees Middelburg", "authors": "C. A. Middelburg", "title": "Program algebra for random access machine programs", "comments": "25 pages, Sect. 2--4 are largely shortened versions of Sect. 2--4 of\n  arXiv:1808.04264, which, in turn, draw from preliminary sections of several\n  other papers. arXiv admin note: substantial text overlap with\n  arXiv:1901.08840", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algebraic theory of instruction sequences with\ninstructions for a random access machine (RAM) as basic instructions, the\nbehaviours produced by the instruction sequences concerned under execution, and\nthe interaction between such behaviours and RAM memories. This theory provides\na setting for the development of theory in areas such as computational\ncomplexity and analysis of algorithm that distinguishes itself by offering the\npossibility of equational reasoning to establish whether an instruction\nsequence computes a given function and being more general than the setting\nprovided by any known version of the RAM model of computation. In this setting,\na semi-realistic version of the RAM model of computation and a bit-oriented\ntime complexity measure for this version are introduced.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 08:50:21 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Middelburg", "C. A.", ""]]}, {"id": "2007.10146", "submitter": "Malin K\\\"all\\'en", "authors": "Malin K\\\"all\\'en, Tobias Wrigstad", "title": "Jupyter Notebooks on GitHub: Characteristics and Code Clones", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2021, Vol. 5,\n  Issue 3, Article 15", "doi": "10.22152/programming-journal.org/2021/5/15", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jupyter notebooks has emerged as a standard tool for data science\nprogramming. Programs in Jupyter notebooks are different from typical programs\nas they are constructed by a collection of code snippets interleaved with text\nand visualisation. This allows interactive exploration and snippets may be\nexecuted in different order which may give rise to different results due to\nside-effects between snippets. Previous studies have shown the presence of\nconsiderable code duplication -- code clones -- in sources of traditional\nprograms, in both so-called systems programming languages and so-called\nscripting languages. In this paper we present the first large-scale study of\ncode cloning in Jupyter notebooks. We analyse a corpus of 2.7 million Jupyter\nnotebooks hosted on GitHJub, representing 37 million individual snippets and\n227 million lines of code. We study clones at the level of individual snippets,\nand study the extent to which snippets are recurring across multiple notebooks.\nWe study both identical clones and approximate clones and conduct a small-scale\nocular inspection of the most common clones. We find that code cloning is\ncommon in Jupyter notebooks -- more than 70% of all code snippets are exact\ncopies of other snippets (with possible differences in white spaces), and\naround 50% of all notebooks do not have any unique snippet, but consists solely\nof snippets that are also found elsewhere. In notebooks written in Python, at\nleast 80% of all snippets are approximate clones and the prevalence of code\ncloning is higher in Python than in other languages. We further find that\nclones between different repositories are far more common than clones within\nthe same repository. However, the most common individual repository from which\na Jupyter notebook contains clones is the repository in which itself resides.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 14:19:10 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 22:38:32 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["K\u00e4ll\u00e9n", "Malin", ""], ["Wrigstad", "Tobias", ""]]}, {"id": "2007.10215", "submitter": "Tobias Reinhard", "authors": "Tobias Reinhard, Amin Timany, Bart Jacobs", "title": "A Separation Logic to Verify Termination of Busy-Waiting for Abrupt\n  Program Exit: Technical Report", "comments": "22 pages, 14 figures, Technical report (replacement: corrected\n  citation in conclusion)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programs for multiprocessor machines commonly perform busy-waiting for\nsynchronisation. In this paper, we make a first step towards proving\ntermination of such programs. We approximate (i) arbitrary waitable events by\nabrupt program termination and (ii) busy-waiting for events by busy-waiting to\nbe abruptly terminated.\n  We propose a separation logic for modularly verifying termination (under fair\nscheduling) of programs where some threads eventually abruptly terminate the\nprogram, and other threads busy-wait for this to happen.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 15:57:25 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 12:55:06 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Reinhard", "Tobias", ""], ["Timany", "Amin", ""], ["Jacobs", "Bart", ""]]}, {"id": "2007.10688", "submitter": "Caterina Urban", "authors": "Caterina Urban", "title": "What Programs Want: Automatic Inference of Input Data Specifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, as machine-learned software quickly permeates our society, we are\nbecoming increasingly vulnerable to programming errors in the data\npre-processing or training software, as well as errors in the data itself. In\nthis paper, we propose a static shape analysis framework for input data of\ndata-processing programs. Our analysis automatically infers necessary\nconditions on the structure and values of the data read by a data-processing\nprogram. Our framework builds on a family of underlying abstract domains,\nextended to indirectly reason about the input data rather than simply reasoning\nabout the program variables. The choice of these abstract domain is a parameter\nof the analysis. We describe various instances built from existing abstract\ndomains. The proposed approach is implemented in an open-source static analyzer\nfor Python programs. We demonstrate its potential on a number of representative\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 09:56:55 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Urban", "Caterina", ""]]}, {"id": "2007.10809", "submitter": "Marco Peressotti", "authors": "Marino Miculan and Marco Peressotti", "title": "Software Transactional Memory with Interactions", "comments": "arXiv admin note: substantial text overlap with arXiv:1602.05365", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software Transactional memory (STM) is an emerging abstraction for concurrent\nprogramming alternative to lock-based synchronizations. Most STM models admit\nonly isolated transactions, which are not adequate in multithreaded programming\nwhere transactions need to interact via shared data before committing. To\novercome this limitation, in this paper we present Open Transactional Memory\n(OTM), a programming abstraction supporting safe, data-driven interactions\nbetween composable memory transactions. This is achieved by relaxing isolation\nbetween transactions, still ensuring atomicity. This model allows for\nloosely-coupled interactions since transaction merging is driven only by\naccesses to shared data, with no need to specify participants beforehand.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 23:48:23 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Miculan", "Marino", ""], ["Peressotti", "Marco", ""]]}, {"id": "2007.10899", "submitter": "Richard Jones", "authors": "Tomas Kalibera and Richard Jones", "title": "Quantifying Performance Changes with Effect Size Confidence Intervals", "comments": "A preliminary version of a portion of this work was presented at the\n  Third European Performance Engineering Workshop", "journal-ref": null, "doi": null, "report-no": "University of Kent TR 4-12", "categories": "stat.ME cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Measuring performance & quantifying a performance change are core evaluation\ntechniques in programming language and systems research. Of 122 recent\nscientific papers, as many as 65 included experimental evaluation that\nquantified a performance change using a ratio of execution times. Few of these\npapers evaluated their results with the level of rigour that has come to be\nexpected in other experimental sciences. The uncertainty of measured results\nwas largely ignored. Scarcely any of the papers mentioned uncertainty in the\nratio of the mean execution times, and most did not even mention uncertainty in\nthe two means themselves. Most of the papers failed to address the\nnon-deterministic execution of computer programs (caused by factors such as\nmemory placement, for example), and none addressed non-deterministic\ncompilation. It turns out that the statistical methods presented in the\ncomputer systems performance evaluation literature for the design and summary\nof experiments do not readily allow this either. This poses a hazard to the\nrepeatability, reproducibility and even validity of quantitative results.\n  Inspired by statistical methods used in other fields of science, and building\non results in statistics that did not make it to introductory textbooks, we\npresent a statistical model that allows us both to quantify uncertainty in the\nratio of (execution time) means and to design experiments with a rigorous\ntreatment of those multiple sources of non-determinism that might impact\nmeasured performance. Better still, under our framework summaries can be as\nsimple as \"system A is faster than system B by 5.5% $\\pm$ 2.5%, with 95%\nconfidence\", a more natural statement than those derived from typical current\npractice, which are often misinterpreted.\n  November 2013\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 15:44:34 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Kalibera", "Tomas", ""], ["Jones", "Richard", ""]]}, {"id": "2007.11070", "submitter": "EPTCS", "authors": "Pedro Figueir\\^edo (E\\\"otv\\\"os Lor\\'and University), Yuri Kim\n  (E\\\"otv\\\"os Lor\\'and University), Nghia Le Minh (E\\\"otv\\\"os Lor\\'and\n  University), Evan Sitt (E\\\"otv\\\"os Lor\\'and University), Xue Ying (E\\\"otv\\\"os\n  Lor\\'and University), Vikt\\'oria Zs\\'ok (E\\\"otv\\\"os Lor\\'and University)", "title": "How to Increase Interest in Studying Functional Programming via\n  Interdisciplinary Application", "comments": "In Proceedings TFPIE 2019 and 2020, arXiv:2008.08923", "journal-ref": "EPTCS 321, 2020, pp. 37-54", "doi": "10.4204/EPTCS.321.3", "report-no": null, "categories": "cs.CY cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional programming represents a modern tool for applying and implementing\nsoftware. The state of the art in functional programming reports an increasing\nnumber of methodologies in this paradigm. However, extensive interdisciplinary\napplications are missing. Our goal is to increase student interest in pursuing\nfurther studies in functional programming with the use of an application: the\nray tracer. We conducted a teaching experience, with positive results and\nstudent feedback, described here in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 20:08:54 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 09:19:09 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Figueir\u00eado", "Pedro", "", "E\u00f6tv\u00f6s Lor\u00e1nd University"], ["Kim", "Yuri", "", "E\u00f6tv\u00f6s Lor\u00e1nd University"], ["Minh", "Nghia Le", "", "E\u00f6tv\u00f6s Lor\u00e1nd\n  University"], ["Sitt", "Evan", "", "E\u00f6tv\u00f6s Lor\u00e1nd University"], ["Ying", "Xue", "", "E\u00f6tv\u00f6s\n  Lor\u00e1nd University"], ["Zs\u00f3k", "Vikt\u00f3ria", "", "E\u00f6tv\u00f6s Lor\u00e1nd University"]]}, {"id": "2007.11203", "submitter": "Cambridge Yang", "authors": "Cambridge Yang, Eric Atkinson, Michael Carbin", "title": "Simplifying Dependent Reductions in the Polyhedral Model", "comments": null, "journal-ref": "Proc. ACM Program. Lang. 5, POPL, Article 20 (2021)", "doi": "10.1145/3434301", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Reduction -- an accumulation over a set of values, using an associative and\ncommutative operator -- is a common computation in many numerical computations,\nincluding scientific computations, machine learning, computer vision, and\nfinancial analytics.\n  Contemporary polyhedral-based compilation techniques make it possible to\noptimize reductions, such as prefix sums, in which each component of the\nreduction's output potentially shares computation with another component in the\nreduction. Therefore an optimizing compiler can identify the computation shared\nbetween multiple components and generate code that computes the shared\ncomputation only once.\n  These techniques, however, do not support reductions that -- when phrased in\nthe language of the polyhedral model -- span multiple dependent statements. In\nsuch cases, existing approaches can generate incorrect code that violates the\ndata dependences of the original, unoptimized program.\n  In this work, we identify and formalize the optimization of dependent\nreductions as an integer bilinear program. We present a heuristic optimization\nalgorithm that uses an affine sequential schedule of the program to determine\nhow to simplfy reductions yet still preserve the program's dependences.\n  We demonstrate that the algorithm provides optimal complexity for a set of\nbenchmark programs from the literature on probabilistic inference algorithms,\nwhose performance critically relies on simplifying these reductions. The\ncomplexities for 10 of the 11 programs improve siginifcantly by factors at\nleast of the sizes of the input data, which are in the range of $10^4$ to\n$10^6$ for typical real application inputs. We also confirm the significance of\nthe improvement by showing speedups in wall-clock time that range from\n$1.1\\text{x}$ to over $10^6\\text{x}$.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 05:12:00 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 19:37:54 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Yang", "Cambridge", ""], ["Atkinson", "Eric", ""], ["Carbin", "Michael", ""]]}, {"id": "2007.11686", "submitter": "Alastair Donaldson", "authors": "Alastair F. Donaldson", "title": "A report on the first virtual PLDI conference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This is a report on the PLDI 2020 conference, for which I was General Chair,\nwhich was held virtually for the first time as a result of the COVID-19\npandemic. The report contains: my personal reflections on the positive and\nnegative aspects of the event; a description of the format of the event and\nassociated logistical details; and data (with some analysis) on attendees'\nviews on the conference format, the extent to which attendees engaged with the\nconference, attendees' views on virtual vs. physical conferences (with a focus\non PLDI specifically) and the diversity of conference registrants. I hope that\nthe report will be a useful resource for organizers of upcoming virtual\nconferences, and generally interesting to the Programming Languages community\nand beyond.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 09:19:58 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Donaldson", "Alastair F.", ""]]}, {"id": "2007.11741", "submitter": "EPTCS", "authors": "Eleonora Iotti, Giuseppe Petrosino, Stefania Monica, Federico Bergenti", "title": "Exploratory Experiments on Programming Autonomous Robots in Jadescript", "comments": "In Proceedings AREA 2020, arXiv:2007.11260", "journal-ref": "EPTCS 319, 2020, pp. 55-67", "doi": "10.4204/EPTCS.319.5", "report-no": null, "categories": "cs.MA cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes exploratory experiments to validate the possibility of\nprogramming autonomous robots using an agent-oriented programming language.\nProper perception of the environment, by means of various types of sensors, and\ntimely reaction to external events, by means of effective actuators, are\nessential to provide robots with a sufficient level of autonomy. The\nagent-oriented programming paradigm is relevant with this respect because it\noffers language-level abstractions to process events and to command actuators.\nA recent agent-oriented programming language called Jadescript is presented in\nthis paper together with its new features specifically designed to handle\nevents. Exploratory experiments on a simple case-study application are\npresented to show the validity of the proposed approach and to exemplify the\nuse of the language to program autonomous robots.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 01:31:46 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Iotti", "Eleonora", ""], ["Petrosino", "Giuseppe", ""], ["Monica", "Stefania", ""], ["Bergenti", "Federico", ""]]}, {"id": "2007.12015", "submitter": "Austin Gadient", "authors": "Martin Rinard and Austin Gadient", "title": "Dataflow Analysis With Prophecy and History Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging concepts from state machine refinement proofs, we use prophecy\nvariables, which predict information about the future program execution, to\nenable forward reasoning for backward dataflow analyses. Drawing prophecy and\nhistory variables (concepts from the dynamic execution of the program) from the\nsame lattice as the static program analysis results, we require the analysis\nresults to satisfy both the dataflow equations and the transition relations in\nthe operational semantics of underlying programming language. This approach\neliminates explicit abstraction and concretization functions and promotes a\nmore direct connection between the analysis and program executions, with the\nconnection taking the form of a bisimulation relation between concrete\nexecutions and an augmented operational semantics over the analysis results. We\npresent several classical dataflow analyses with this approach (live variables,\nvery busy expressions, defined variables, and reaching definitions) along with\nproofs that highlight how this approach can enable more streamlined reasoning.\nTo the best of our knowledge, we are the first to use prophecy variables for\ndataflow analysis.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 13:53:58 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Rinard", "Martin", ""], ["Gadient", "Austin", ""]]}, {"id": "2007.12101", "submitter": "Ameesh Shah", "authors": "Ameesh Shah, Eric Zhan, Jennifer J. Sun, Abhinav Verma, Yisong Yue,\n  Swarat Chaudhuri", "title": "Learning Differentiable Programs with Admissible Neural Heuristics", "comments": "9 pages, published in NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning differentiable functions expressed as\nprograms in a domain-specific language. Such programmatic models can offer\nbenefits such as composability and interpretability; however, learning them\nrequires optimizing over a combinatorial space of program \"architectures\". We\nframe this optimization problem as a search in a weighted graph whose paths\nencode top-down derivations of program syntax. Our key innovation is to view\nvarious classes of neural networks as continuous relaxations over the space of\nprograms, which can then be used to complete any partial program. This relaxed\nprogram is differentiable and can be trained end-to-end, and the resulting\ntraining loss is an approximately admissible heuristic that can guide the\ncombinatorial search. We instantiate our approach on top of the A-star\nalgorithm and an iteratively deepened branch-and-bound search, and use these\nalgorithms to learn programmatic classifiers in three sequence classification\ntasks. Our experiments show that the algorithms outperform state-of-the-art\nmethods for program learning, and that they discover programmatic classifiers\nthat yield natural interpretations and achieve competitive accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 16:07:39 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 03:41:24 GMT"}, {"version": "v3", "created": "Sat, 26 Sep 2020 00:24:44 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2020 02:11:06 GMT"}, {"version": "v5", "created": "Sun, 28 Mar 2021 01:15:33 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Shah", "Ameesh", ""], ["Zhan", "Eric", ""], ["Sun", "Jennifer J.", ""], ["Verma", "Abhinav", ""], ["Yue", "Yisong", ""], ["Chaudhuri", "Swarat", ""]]}, {"id": "2007.12247", "submitter": "Antonina Nepeivoda", "authors": "Antonina Nepeivoda", "title": "On Solving Word Equations via Program Transformation", "comments": "Another version of this work will be uploaded on arXiv by eptcs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents an experiment of solving word equations via specialization\nof a configuration WE(R,E), where the program WE can be considered as an\ninterpreter testing whether a composition of substitutions R produces a\nsolution of a word equation E. Several variants of such interpreters, when\nspecialized using a basic unfold/fold strategy, are able to decide solvability\nfor a number of sets of the word equations with the overlapping variables.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 20:39:29 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 20:04:16 GMT"}, {"version": "v3", "created": "Sat, 19 Jun 2021 10:41:02 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Nepeivoda", "Antonina", ""]]}, {"id": "2007.12477", "submitter": "Joel Colloc", "authors": "Jo\\\"el Colloc (IDEES)", "title": "An Object Oriented Approach For the Protection of Information Systems", "comments": "in French. INFORSID 1991 Paris Pantheon Sorbonne, 1991, Paris, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a protection system making use of encapsulation, messages\ncommunication, interface functions coming from an object oriented model\ndescribed in previous works. Each user represents himself to the system by the\nmean of his \"USER\" object type. The recognition procedure is suitable to every\none's needs. Any user's objects and types are labeled with a personal\nsignature, exclusively provided and known by the system. Administrator's rights\nare restricted to backup procedures. The system verify each messages access, it\nis robust because partitioned, flexible, suitable and psychologically\nacceptable.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 08:12:39 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Colloc", "Jo\u00ebl", "", "IDEES"]]}, {"id": "2007.12630", "submitter": "Sam Tobin-Hochstadt", "authors": "Cameron Moy and Ph\\'uc C. Nguy\\~en and Sam Tobin-Hochstadt and David\n  Van Horn", "title": "Corpse Reviver: Sound and Efficient Gradual Typing via Contract\n  Verification", "comments": "To appear in POPL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gradually-typed programming languages permit the incremental addition of\nstatic types to untyped programs. To remain sound, languages insert run-time\nchecks at the boundaries between typed and untyped code. Unfortunately,\nperformance studies have shown that the overhead of these checks can be\ndisastrously high, calling into question the viability of sound gradual typing.\nIn this paper, we show that by building on existing work on soft contract\nverification, we can reduce or eliminate this overhead.\n  Our key insight is that while untyped code cannot be trusted by a gradual\ntype system, there is no need to consider only the worst case when optimizing a\ngradually-typed program. Instead, we statically analyze the untyped portions of\na gradually-typed program to prove that almost all of the dynamic checks\nimplied by gradual type boundaries cannot fail, and can be eliminated at\ncompile time. Our analysis is modular, and can be applied to any portion of a\nprogram.\n  We evaluate this approach on a dozen existing gradually-typed programs\npreviously shown to have prohibitive performance overhead---with a median\noverhead of $3.5\\times$ and up to $73.6\\times$ in the worst case---and\neliminate all overhead in most cases, suffering only $1.6\\times$ overhead in\nthe worst case.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 16:30:42 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 19:07:14 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Moy", "Cameron", ""], ["Nguyen", "Ph\u00fac C.", ""], ["Tobin-Hochstadt", "Sam", ""], ["Van Horn", "David", ""]]}, {"id": "2007.12737", "submitter": "Sam Tobin-Hochstadt", "authors": "Sarah Spall and Neil Mitchell and Sam Tobin-Hochstadt", "title": "Build Scripts with Perfect Dependencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Build scripts for most build systems describe the actions to run, and the\ndependencies between those actions---but often build scripts get those\ndependencies wrong. Most build scripts have both too few dependencies (leading\nto incorrect build outputs) and too many dependencies (leading to excessive\nrebuilds and reduced parallelism). Any programmer who has wondered why a small\nchange led to excess compilation, or who resorted to a \"clean\" step, has\nsuffered the ill effects of incorrect dependency specification. We outline a\nbuild system where dependencies are not specified, but instead captured by\ntracing execution. The consequence is that dependencies are always correct by\nconstruction and build scripts are easier to write. The simplest implementation\nof our approach would lose parallelism, but we are able to recover parallelism\nusing speculation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 18:56:14 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Spall", "Sarah", ""], ["Mitchell", "Neil", ""], ["Tobin-Hochstadt", "Sam", ""]]}, {"id": "2007.12987", "submitter": "Gian Pietro Farina", "authors": "Gian Pietro Farina, Stephen Chong, Marco Gaboardi", "title": "Coupled Relational Symbolic Execution for Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a de facto standard in data privacy with applications\nin the private and public sectors. Most of the techniques that achieve\ndifferential privacy are based on a judicious use of randomness. However,\nreasoning about randomized programs is difficult and error prone. For this\nreason, several techniques have been recently proposed to support designer in\nproving programs differentially private or in finding violations to it. In this\nwork we propose a technique based on symbolic execution for reasoning about\ndifferential privacy. Symbolic execution is a classic technique used for\ntesting, counterexample generation and to prove absence of bugs. Here we use\nsymbolic execution to support these tasks specifically for differential\nprivacy. To achieve this goal, we leverage two ideas that have been already\nproven useful in formal reasoning about differential privacy: relational\nreasoning and probabilistic coupling. Our technique integrates these two ideas\nand shows how such a combination can be used to both verify and find violations\nto differential privacy.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 18:08:07 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Farina", "Gian Pietro", ""], ["Chong", "Stephen", ""], ["Gaboardi", "Marco", ""]]}, {"id": "2007.13113", "submitter": "Jos Craaijo", "authors": "Jos Craaijo", "title": "IdSan: An identity-based memory sanitizer for fuzzing binaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most memory sanitizers work by instrumenting the program at compile time.\nThere are only a handful of memory sanitizers that can sanitize a binary\nprogram without source code. Most are location-based, and are therefore unable\nto detect overflows of global variables or variables on the stack. In this\npaper we introduce an identity-based memory sanitizer for binary AArch64\nprograms which does not need access to the source code. It is able to detect\noverflows of stack- and global variables if the user provides some annotations\nor DWARF debugging information is available, as well as dynamically allocated\nmemory.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 12:14:47 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Craaijo", "Jos", ""]]}, {"id": "2007.13529", "submitter": "Simon Foster", "authors": "Simon Foster, Kangfeng Ye, Ana Cavalcanti, Jim Woodcock", "title": "Automated Verification of Reactive and Concurrent Programs by\n  Calculation", "comments": "39 pages, accepted for publication in Journal of Logic and Algebraic\n  Methods in Programming (JLAMP), submitted 30/04/2019", "journal-ref": null, "doi": "10.1016/j.jlamp.2021.100681", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reactive programs combine traditional sequential programming constructs with\nprimitives to allow communication with other concurrent agents. They are\nubiquitous in modern applications, ranging from components systems and web\nservices, to cyber-physical systems and autonomous robots. In this paper, we\npresent an algebraic verification strategy for concurrent reactive programs,\nwith a large or infinite state space. We define novel operators to characterise\ninteractions and state updates, and an associated equational theory. With this\nwe can calculate a reactive program's denotational semantics, and thereby\nfacilitate automated proof. Of note is our reasoning support for iterative\nprograms with reactive invariants, based on Kleene algebra, and for parallel\ncomposition. We illustrate our strategy by verifying a reactive buffer. Our\nlaws and strategy are mechanised in Isabelle/UTP, our implementation of Hoare\nand He's Unifying Theories of Programming (UTP) framework, to provide soundness\nguarantees and practical verification support.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 13:07:08 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 14:18:09 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Foster", "Simon", ""], ["Ye", "Kangfeng", ""], ["Cavalcanti", "Ana", ""], ["Woodcock", "Jim", ""]]}, {"id": "2007.14075", "submitter": "Jacques Basald\\'ua Dr.", "authors": "Jacques Basald\\'ua", "title": "Formal Fields: A Framework to Automate Code Generation Across Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code generation, defined as automatically writing a piece of code to solve a\ngiven problem for which an evaluation function exists, is a classic hard AI\nproblem. Its general form, writing code using a general language used by human\nprogrammers from scratch is thought to be impractical. Adding constraints to\nthe code grammar, implementing domain specific concepts as primitives and\nproviding examples for the algorithm to learn, makes it practical. Formal\nfields is a framework to do code generation across domains using the same\nalgorithms and language structure. Its ultimate goal is not just solving\ndifferent narrow problems, but providing necessary abstractions to integrate\nmany working solutions as a single lifelong reasoning system. It provides a\ncommon grammar to define: a domain language, a problem and its evaluation. The\nframework learns from examples of code snippets about the structure of the\ndomain language and searches completely new code snippets to solve unseen\nproblems in the same field. Formal fields abstract the search algorithm away\nfrom the problem. The search algorithm is taken from existing reinforcement\nlearning algorithms. In our implementation it is an apropos Monte-Carlo Tree\nSearch (MCTS). We have implemented formal fields as a fully documented open\nsource project applied to the Abstract Reasoning Challenge (ARC). The\nimplementation found code snippets solving twenty two previously unsolved ARC\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 09:06:01 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Basald\u00faa", "Jacques", ""]]}, {"id": "2007.14259", "submitter": "Amir Kafshdar Goharshady", "authors": "Ali Asadi and Krishnendu Chatterjee and Hongfei Fu and Amir Kafshdar\n  Goharshady and Mohammad Mahdavi", "title": "Inductive Reachability Witnesses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the fundamental problem of reachability analysis\nover imperative programs with real variables. The reachability property\nrequires that a program can reach certain target states during its execution.\nPrevious works that tackle reachability analysis are either unable to handle\nprograms consisting of general loops (e.g. symbolic execution), or lack\ncompleteness guarantees (e.g. abstract interpretation), or are not automated\n(e.g. incorrectness logic/reverse Hoare logic). In contrast, we propose a novel\napproach for reachability analysis that can handle general programs, is\n(semi-)complete, and can be entirely automated for a wide family of programs.\nOur approach extends techniques from both invariant generation and\nranking-function synthesis to reachability analysis through the notion of\n(Universal) Inductive Reachability Witnesses (IRWs/UIRWs). While traditional\ninvariant generation uses over-approximations of reachable states, we consider\nthe natural dual problem of under-approximating the set of program states that\ncan reach a target state. We then apply an argument similar to ranking\nfunctions to ensure that all states in our under-approximation can indeed reach\nthe target set in finitely many steps.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 14:18:37 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Asadi", "Ali", ""], ["Chatterjee", "Krishnendu", ""], ["Fu", "Hongfei", ""], ["Goharshady", "Amir Kafshdar", ""], ["Mahdavi", "Mohammad", ""]]}, {"id": "2007.14381", "submitter": "Kensen Shi", "authors": "Augustus Odena, Kensen Shi, David Bieber, Rishabh Singh, Charles\n  Sutton", "title": "BUSTLE: Bottom-Up Program Synthesis Through Learning-Guided Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program synthesis is challenging largely because of the difficulty of search\nin a large space of programs. Human programmers routinely tackle the task of\nwriting complex programs by writing sub-programs and then analyzing their\nintermediate results to compose them in appropriate ways. Motivated by this\nintuition, we present a new synthesis approach that leverages learning to guide\na bottom-up search over programs. In particular, we train a model to prioritize\ncompositions of intermediate values during search conditioned on a given set of\ninput-output examples. This is a powerful combination because of several\nemergent properties. First, in bottom-up search, intermediate programs can be\nexecuted, providing semantic information to the neural network. Second, given\nthe concrete values from those executions, we can exploit rich features based\non recent work on property signatures. Finally, bottom-up search allows the\nsystem substantial flexibility in what order to generate the solution, allowing\nthe synthesizer to build up a program from multiple smaller sub-programs.\nOverall, our empirical evaluation finds that the combination of learning and\nbottom-up search is remarkably effective, even with simple supervised learning\napproaches. We demonstrate the effectiveness of our technique on two datasets,\none from the SyGuS competition and one of our own creation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 17:46:18 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 00:23:41 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Odena", "Augustus", ""], ["Shi", "Kensen", ""], ["Bieber", "David", ""], ["Singh", "Rishabh", ""], ["Sutton", "Charles", ""]]}, {"id": "2007.15126", "submitter": "Milijana Surbatovich", "authors": "Milijana Surbatovich (Carnegie Mellon University), Limin Jia (Carnegie\n  Mellon University), Brandon Lucia (Carnegie Mellon University)", "title": "Towards a Formal Foundation of Intermittent Computing", "comments": "Update acknowledgements with link to repo of Coq mechanization", "journal-ref": "Proc. ACM Program. Lang. 4, OOPSLA, Article 163 (November 2020),\n  31 pages (2020)", "doi": "10.1145/3428231", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intermittently powered devices enable new applications in harsh or\ninaccessible environments, such as space or in-body implants, but also\nintroduce problems in programmability and correctness. Researchers have\ndeveloped programming models to ensure that programs make progress and do not\nproduce erroneous results due to memory inconsistencies caused by intermittent\nexecutions. As the technology has matured, more and more features are added to\nintermittently powered devices, such as I/O. Prior work has shown that all\nexisting intermittent execution models have problems with repeated device or\nsensor inputs (RIO). RIOs could leave intermittent executions in an\ninconsistent state. Such problems and the proliferation of existing\nintermittent execution models necessitate a formal foundation for intermittent\ncomputing.\n  In this paper, we formalize intermittent execution models, their correctness\nproperties with respect to memory consistency and inputs, and identify the\ninvariants needed to prove systems correct. We prove equivalence between\nseveral existing intermittent systems. To address RIO problems, we define an\nalgorithm for identifying variables affected by RIOs that need to be restored\nafter reboot and prove the algorithm correct. Finally, we implement the\nalgorithm in a novel intermittent runtime system that is correct with respect\nto input operations and evaluate its performance.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 21:46:49 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 17:36:15 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 21:47:02 GMT"}, {"version": "v4", "created": "Sun, 27 Jun 2021 00:08:00 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Surbatovich", "Milijana", "", "Carnegie Mellon University"], ["Jia", "Limin", "", "Carnegie\n  Mellon University"], ["Lucia", "Brandon", "", "Carnegie Mellon University"]]}, {"id": "2007.15617", "submitter": "Tobias Reinhard", "authors": "Tobias Reinhard", "title": "A Core Calculus for Static Latency Tracking with Placement Types", "comments": "3 pages, 0 figures, accepted at Student Research Competition @ POPL\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing efficient geo-distributed applications is challenging as\nprogrammers can easily introduce computations that entail high latency\ncommunication. We propose a language design which makes latency explicit and\nextracts type-level bounds for a computation's runtime latency. We present our\ninitial steps with a core calculus that enables extracting provably correct\nlatency bounds and outline future work.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 17:31:31 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Reinhard", "Tobias", ""]]}, {"id": "2007.16171", "submitter": "Germ\\'an Vidal", "authors": "Germ\\'an Vidal", "title": "Reversible Debugging in Logic Programming", "comments": "15 pages, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reversible debugging is becoming increasingly popular for locating the source\nof errors. This technique proposes a more natural approach to debugging, where\none can explore a computation from the observable misbehaviour backwards to the\nsource of the error. In this work, we propose a reversible debugging scheme for\nlogic programs. For this purpose, we define an appropriate instrumented\nsemantics (a so-called Landauer embedding) that makes SLD resolution\nreversible. An implementation of a reversible debugger for Prolog, rever, has\nbeen developed and is publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 16:47:05 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Vidal", "Germ\u00e1n", ""]]}]