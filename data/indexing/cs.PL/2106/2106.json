[{"id": "2106.00412", "submitter": "Vashti Galpin", "authors": "Vashti Galpin, James Cheney", "title": "Curating Covid-19 data in Links", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Curated scientific databases play an important role in the scientific\nendeavour and support is needed for the significant effort that goes into their\ncreation and maintenance. This demonstration and case study illustrate how\ncuration support has been developed in the Links cross-tier programming\nlanguage, a functional, strongly typed language with language-integrated query\nand support for temporal databases. The chosen case study uses weekly released\nCovid-19 fatality figures from the Scottish government which exhibit updates to\npreviously released data. This data allows the capture and query of update\nprovenance in our prototype. This demonstration will highlight the potential\nfor language-integrated support for curation to simplify and streamline\nprototyping of web-applications in support of scientific databases\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 11:52:59 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Galpin", "Vashti", ""], ["Cheney", "James", ""]]}, {"id": "2106.00664", "submitter": "Sharon Shoham Buchbinder", "authors": "Arie Gurfinkel, Sharon Shoham and Yakir Vizel", "title": "Quantifiers on Demand", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated program verification is a difficult problem. It is undecidable even\nfor transition systems over Linear Integer Arithmetic (LIA). Extending the\ntransition system with theory of Arrays, further complicates the problem by\nrequiring inference and reasoning with universally quantified formulas. In this\npaper, we present a new algorithm, Quic3, that extends IC3 to infer universally\nquantified invariants over the combined theory of LIA and Arrays. Unlike other\napproaches that use either IC3 or an SMT solver as a black box, Quic3 carefully\nmanages quantified generalization (to construct quantified invariants) and\nquantifier instantiation (to detect convergence in the presence of\nquantifiers). While Quic3 is not guaranteed to converge, it is guaranteed to\nmake progress by exploring longer and longer executions. We have implemented\nQuic3 within the Constrained Horn Clause solver engine of Z3 and experimented\nwith it by applying Quic3 to verifying a variety of public benchmarks of array\nmanipulating C programs.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 17:51:05 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Gurfinkel", "Arie", ""], ["Shoham", "Sharon", ""], ["Vizel", "Yakir", ""]]}, {"id": "2106.00732", "submitter": "Sharon Shoham Buchbinder", "authors": "Dan Rasin, Orna Grumberg and Sharon Shoham", "title": "Modular Verification of Concurrent Programs via Sequential Model\n  Checking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work utilizes the plethora of work on verification of sequential\nprograms for the purpose of verifying concurrent programs. We reduce the\nverification of a concurrent program to a series of verification tasks of\nsequential programs. Our approach is modular in the sense that each sequential\nverification task roughly corresponds to the verification of a single thread,\nwith some additional information about the environment in which it operates.\nInformation regarding the environment is gathered during the run of the\nalgorithm, by need.\n  While our approach is general, it specializes on concurrent programs where\nthe threads are structured hierarchically. The idea is to exploit the hierarchy\nin order to minimize the amount of information that needs to be transferred\nbetween threads. To that end, we verify one of the threads, considered \"main\",\nas a sequential program. Its verification process initiates queries to its\n\"environment\" (which may contain multiple threads). Those queries are answered\nby sequential verification, if the environment consists of a single thread, or,\notherwise, by applying the same hierarchical algorithm on the environment.\n  Our technique is fully automatic, and allows us to use any off-the-shelf\nsequential model checker. We implemented our technique in a tool called CoMuS\nand evaluated it against established tools for concurrent verification. Our\nexperiments show that it works particularly well on hierarchically structured\nprograms.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 19:15:34 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Rasin", "Dan", ""], ["Grumberg", "Orna", ""], ["Shoham", "Sharon", ""]]}, {"id": "2106.00937", "submitter": "Oren Ish Shalom", "authors": "Oren Ish Shalom, Shachar Itzhaky, Noam Rinetzky, Sharon Shoham", "title": "Putting the Squeeze on Array Programs: Loop Verification via Inductive\n  Rank Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic verification of array manipulating programs is a challenging\nproblem because it often amounts to the inference of in ductive quantified loop\ninvariants which, in some cases, may not even be firstorder expressible. In\nthis paper, we suggest a novel verification tech nique that is based on\ninduction on userdefined rank of program states as an alternative to\nloopinvariants. Our technique, dubbed inductive rank reduction, works in two\nsteps. Firstly, we simplify the verification problem and prove that the program\nis correct when the input state con tains an input array of length B or less,\nusing the length of the array as the rank of the state. Secondly, we employ a\nsqueezing function g which converts a program state sigma with an array of\nlength > B to a state g(sigma) containing an array of length minus 1 or less.\nWe prove that when g satisfies certain natural conditions then if the program\nviolates its specification on sigma then it does so also on g(sigma). The\ncorrectness of the program on inputs with arrays of arbitrary lengths follows\nby induction. We make our technique automatic for array programs whose length\nof execution is proportional to the length of the input arrays by (i) perform\ning the first step using symbolic execution, (ii) verifying the conditions\nrequired of g using Z3, and (iii) providing a heuristic procedure for syn\nthesizing g. We implemented our technique and applied it successfully to\nseveral interesting arraymanipulating programs, including a bidirec tional\nsummation program whose loop invariant cannot be expressed in firstorder logic\nwhile its specification is quantifier free.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 04:51:35 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Shalom", "Oren Ish", ""], ["Itzhaky", "Shachar", ""], ["Rinetzky", "Noam", ""], ["Shoham", "Sharon", ""]]}, {"id": "2106.01030", "submitter": "Kalev Alpernas", "authors": "Kalev Alpernas (Tel Aviv University), Aurojit Panda (NYU), Alexander\n  Rabinovich (Tel Aviv University), Mooly Sagiv (Tel Aviv University), Scott\n  Shenker (UC Berkeley), Sharon Shoham (Tel Aviv University), Yaron Velner\n  (Hebrew University of Jerusalem)", "title": "Some Complexity Results for Stateful Network Verification", "comments": "This is a pre-print of an article published in Formal Methods in\n  System Design. The final authenticated version is available online at:\n  https://doi.org/10.1007/s10703-018-00330-9", "journal-ref": "Formal Methods in System Design 54 (2019) 191-231", "doi": "10.1007/s10703-018-00330-9", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern networks, forwarding of packets often depends on the history of\npreviously transmitted traffic. Such networks contain stateful middleboxes,\nwhose forwarding behaviour depends on a mutable internal state. Firewalls and\nload balancers are typical examples of stateful middleboxes.\n  This work addresses the complexity of verifying safety properties, such as\nisolation, in networks with finite-state middleboxes. Unfortunately, we show\nthat even in the absence of forwarding loops, reasoning about such networks is\nundecidable due to interactions between middleboxes connected by unbounded\nordered channels. We therefore abstract away channel ordering. This abstraction\nis sound for safety, and makes the problem decidable. Specifically, safety\nchecking becomes EXPSPACE-complete in the number of hosts and middleboxes in\nthe network. To tackle the high complexity, we identify two useful subclasses\nof finite-state middleboxes which admit better complexities. The simplest class\nincludes, e.g., firewalls and permits polynomial-time verification. The second\nclass includes, e.g., cache servers and learning switches, and makes the safety\nproblem coNP-complete.\n  Finally, we implement a tool for verifying the correctness of stateful\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 08:52:49 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Alpernas", "Kalev", "", "Tel Aviv University"], ["Panda", "Aurojit", "", "NYU"], ["Rabinovich", "Alexander", "", "Tel Aviv University"], ["Sagiv", "Mooly", "", "Tel Aviv University"], ["Shenker", "Scott", "", "UC Berkeley"], ["Shoham", "Sharon", "", "Tel Aviv University"], ["Velner", "Yaron", "", "Hebrew University of Jerusalem"]]}, {"id": "2106.01115", "submitter": "Chukri Soueidi", "authors": "Chukri Soueidi, Marius Monnier, Ali Kassem, Yli\\`es Falcone (Univ.\n  Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG, Grenoble, France)", "title": "Efficient and Expressive Bytecode-Level Instrumentation for Java\n  Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient and expressive tool for the instrumentation of Java\nprograms at the bytecode-level. BISM (Bytecode-Level Instrumentation for\nSoftware Monitoring) is a light-weight Java bytecode instrumentation tool that\nfeatures an expressive high-level control-flow-aware instrumentation language.\nThe language is inspired by the aspect-oriented programming paradigm in\nmodularizing instrumentation into separate transformers, that encapsulate\njoinpoint selection and advice inlining. BISM allows capturing joinpoints\nranging from bytecode instructions to methods execution and provides\ncomprehensive static and dynamic context information. It runs in two\ninstrumentation modes: build-time and load-time. BISM also provides a mechanism\nto compose transformers and automatically detect their collision in the base\nprogram. Transformers in a composition can control the visibility of their\nadvice and other instructions from the base program. We show several example\napplications for BISM and demonstrate its effectiveness using three\nexperiments: a security scenario, a financial transaction system, and a general\nruntime verification case. The results show that BISM instrumentation incurs\nlow runtime and memory overheads.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 12:27:23 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Soueidi", "Chukri", "", "Univ.\n  Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG, Grenoble, France"], ["Monnier", "Marius", "", "Univ.\n  Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG, Grenoble, France"], ["Kassem", "Ali", "", "Univ.\n  Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG, Grenoble, France"], ["Falcone", "Yli\u00e8s", "", "Univ.\n  Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG, Grenoble, France"]]}, {"id": "2106.01250", "submitter": "Dmitrii Kosarev", "authors": "Dmitrii Kosarev and Dmitry Boulytchev", "title": "Generic Programming with Combinators and Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a generic programming framework for OCAML which makes it possible\nto implement extensible transformations for a large scale of type definitions.\nOur framework makes use of objectoriented features of OCAML, utilising late\nbinding to override the default behaviour of generated transformations. The\nsupport for polymorphic variant types complements the ability to describe\ncomposable data types with the ability to implement composable transformations.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 16:01:33 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Kosarev", "Dmitrii", ""], ["Boulytchev", "Dmitry", ""]]}, {"id": "2106.01367", "submitter": "David Coimbra", "authors": "David Coimbra, Sofia Reis, Rui Abreu, Corina P\\u{a}s\\u{a}reanu, Hakan\n  Erdogmus", "title": "On using distributed representations of source code for the detection of\n  C security vulnerabilities", "comments": "Submitted to DX 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an evaluation of the code representation model Code2vec\nwhen trained on the task of detecting security vulnerabilities in C source\ncode. We leverage the open-source library astminer to extract path-contexts\nfrom the abstract syntax trees of a corpus of labeled C functions. Code2vec is\ntrained on the resulting path-contexts with the task of classifying a function\nas vulnerable or non-vulnerable. Using the CodeXGLUE benchmark, we show that\nthe accuracy of Code2vec for this task is comparable to simple\ntransformer-based methods such as pre-trained RoBERTa, and outperforms more\nnaive NLP-based methods. We achieved an accuracy of 61.43% while maintaining\nlow computational requirements relative to larger models.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 21:18:23 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Coimbra", "David", ""], ["Reis", "Sofia", ""], ["Abreu", "Rui", ""], ["P\u0103s\u0103reanu", "Corina", ""], ["Erdogmus", "Hakan", ""]]}, {"id": "2106.01710", "submitter": "Milind Chabbi", "authors": "Zhizhou Zhang, Milind Chabbi, Adam Welc, Timothy Sherwood", "title": "Optimistic Concurrency Control for Real-world Go Programs (Extended\n  Version with Appendix)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a source-to-source transformation framework, GOCC, that consumes\nlock-based pessimistic concurrency programs in the Go language and transforms\nthem into optimistic concurrency programs that use Hardware Transactional\nMemory (HTM). The choice of the Go language is motivated by the fact that\nconcurrency is a first-class citizen in Go, and it is widely used in Go\nprograms. GOCC performs rich inter-procedural program analysis to detect and\nfilter lock-protected regions and performs AST-level code transformation of the\nsurrounding locks when profitable. Profitability is driven by both static\nanalyses of critical sections and dynamic analysis via execution profiles. A\ncustom HTM library, using perceptron, learns concurrency behavior and\ndynamically decides whether to use HTM in the rewritten lock/unlock points.\nGiven the rich history of transactional memory research but its lack of\nadoption in any industrial setting, we believe this workflow, which ultimately\nproduces source-code patches, is more apt for industry-scale adoption. Results\non widely adopted Go libraries and applications demonstrate significant (up to\n10x) and scalable performance gains resulting from our automated transformation\nwhile avoiding major performance regressions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 09:27:37 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhang", "Zhizhou", ""], ["Chabbi", "Milind", ""], ["Welc", "Adam", ""], ["Sherwood", "Timothy", ""]]}, {"id": "2106.01726", "submitter": "Ra\\'ul Nozal", "authors": "Ra\\'ul Nozal and Jose Luis Bosque", "title": "Exploiting co-execution with oneAPI: heterogeneity from a modern\n  perspective", "comments": "Accepted in Euro-Par 2021 (27th International Conference on Parallel\n  and Distributed Computing). 16 pages, 9 figures, 1 listing. Conference paper\n  - extended with API", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming efficiently heterogeneous systems is a major challenge, due to\nthe complexity of their architectures. Intel oneAPI, a new and powerful\nstandards-based unified programming model, built on top of SYCL, addresses\nthese issues. In this paper, oneAPI is provided with co-execution strategies to\nrun the same kernel between different devices, enabling the exploitation of\nstatic and dynamic policies. On top of that, static and dynamic load-balancing\nalgorithms are integrated and analyzed.\n  This work evaluates the performance and energy efficiency for a well-known\nset of regular and irregular HPC benchmarks, using an integrated GPU and CPU.\nExperimental results show that co-execution is worthwhile when using dynamic\nalgorithms, improving efficiency even more when using unified shared memory.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 09:56:01 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 20:24:18 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Nozal", "Ra\u00fal", ""], ["Bosque", "Jose Luis", ""]]}, {"id": "2106.01768", "submitter": "Aman Nougrahiya", "authors": "Aman Nougrahiya, V. Krishna Nandivada", "title": "Homeostasis: Design and Implementation of a Self-Stabilizing Compiler", "comments": "33 pages, 16 figures. Patent filed (application no: 202041054066).\n  For associated code, see https://github.com/anonymousoopsla21/homeostasis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Mainstream compilers perform a multitude of analyses and optimizations on the\ngiven input program. Each analysis pass may generate a program-abstraction.\nEach optimization pass is typically composed of multiple alternating phases of\ninspection of program-abstractions and transformations of the program. Upon\ntransformation of a program, the program-abstractions generated by various\nanalysis passes may become inconsistent with the program's modified state.\nConsequently, the downstream transformations may be considered unsafe until the\nrelevant program-abstractions are stabilized, i.e., the program-abstractions\nare made consistent with the modified program. In general, the existing\ncompiler frameworks do not perform automated stabilization of the\nprogram-abstractions and instead leave it to the optimization writer to deal\nwith the complex task of identifying the relevant program-abstractions to\nstabilize, the points where the stabilization is to be performed, and the exact\nprocedure of stabilization. Similarly, adding new analyses becomes a challenge\nas one has to understand which all existing optimizations may impact the newly\nadded program-abstractions. In this paper, we address these challenges by\nproviding the design and implementation of a novel generalized compiler-design\nframework called Homeostasis.\n  Homeostasis can be used to guarantee the trigger of automated stabilization\nof relevant program-abstractions under every possible transformation of the\nprogram. Interestingly, Homeostasis provides such guarantees not only for the\nexisting optimization passes but also for any future optimizations that may be\nadded to the framework. We have implemented our proposed ideas in the IMOP\ncompiler framework, for OpenMP C programs. We present an evaluation which shows\nthat Homeostasis is efficient and easy to use.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 11:49:39 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Nougrahiya", "Aman", ""], ["Nandivada", "V. Krishna", ""]]}, {"id": "2106.02452", "submitter": "Steven Kommrusch", "authors": "Steve Kommrusch, Th\\'eo Barollet and Louis-No\\\"el Pouchet", "title": "Proving Equivalence Between Complex Expressions Using Graph-to-Sequence\n  Neural Models", "comments": "10 pages (24 including references and appendices), 8 figures, 17\n  tables. arXiv admin note: substantial text overlap with arXiv:2002.06799.\n  Updated to include funding acknowledgement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We target the problem of provably computing the equivalence between two\ncomplex expression trees. To this end, we formalize the problem of equivalence\nbetween two such programs as finding a set of semantics-preserving rewrite\nrules from one into the other, such that after the rewrite the two programs are\nstructurally identical, and therefore trivially equivalent.We then develop a\ngraph-to-sequence neural network system for program equivalence, trained to\nproduce such rewrite sequences from a carefully crafted automatic example\ngeneration algorithm. We extensively evaluate our system on a rich multi-type\nlinear algebra expression language, using arbitrary combinations of 100+\ngraph-rewriting axioms of equivalence. Our machine learning system guarantees\ncorrectness for all true negatives, and ensures 0 false positive by design. It\noutputs via inference a valid proof of equivalence for 93% of the 10,000\nequivalent expression pairs isolated for testing, using up to 50-term\nexpressions. In all cases, the validity of the sequence produced and therefore\nthe provable assertion of program equivalence is always computable, in\nnegligible time.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 20:45:42 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 02:42:43 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Kommrusch", "Steve", ""], ["Barollet", "Th\u00e9o", ""], ["Pouchet", "Louis-No\u00ebl", ""]]}, {"id": "2106.02628", "submitter": "Eric Koskinen", "authors": "Hiroshi Unno, Tachio Terauchi, Eric Koskinen", "title": "Constraint-based Relational Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years they have been numerous works that aim to automate relational\nverification. Meanwhile, although Constrained Horn Clauses (CHCs) empower a\nwide range of verification techniques and tools, they lack the ability to\nexpress hyperproperties beyond $k$-safety such as generalized non-interference\nand co-termination.\n  This paper describes a novel and fully automated constraint-based approach to\nrelational verification. We first introduce a new class of predicate Constraint\nSatisfaction Problems called pfwCSP where constraints are represented as\nclauses modulo first-order theories over predicate variables of three kinds:\nordinary, well-founded, or functional. This generalization over CHCs permits\narbitrary (i.e., possibly non-Horn) clauses, well-foundedness constraints,\nfunctionality constraints, and is capable of expressing these relational\nverification problems. Our approach enables us to express and automatically\nverify problem instances that require non-trivial (i.e., non-sequential and\nnon-lock-step) self-composition by automatically inferring appropriate\nschedulers (or alignment) that dictate when and which program copies move. To\nsolve problems in this new language, we present a constraint solving method for\npfwCSP based on stratified CounterExample-Guided Inductive Synthesis (CEGIS) of\nordinary, well-founded, and functional predicates.\n  We have implemented the proposed framework and obtained promising results on\ndiverse relational verification problems that are beyond the scope of the\nprevious verification frameworks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 17:47:55 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Unno", "Hiroshi", ""], ["Terauchi", "Tachio", ""], ["Koskinen", "Eric", ""]]}, {"id": "2106.03353", "submitter": "Md Rafiqul Islam Rabin", "authors": "Md Rafiqul Islam Rabin, Vincent J. Hellendoorn, Mohammad Amin Alipour", "title": "Understanding Neural Code Intelligence Through Program Simplification", "comments": "The 29th ACM Joint European Software Engineering Conference and\n  Symposium on the Foundations of Software Engineering (ESEC/FSE'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of code intelligence (CI) tools, powered by deep neural\nnetworks, have been developed recently to improve programming productivity and\nperform program analysis. To reliably use such tools, developers often need to\nreason about the behavior of the underlying models and the factors that affect\nthem. This is especially challenging for tools backed by deep neural networks.\nVarious methods have tried to reduce this opacity in the vein of\n\"transparent/interpretable-AI\". However, these approaches are often specific to\na particular set of network architectures, even requiring access to the\nnetwork's parameters. This makes them difficult to use for the average\nprogrammer, which hinders the reliable adoption of neural CI systems. In this\npaper, we propose a simple, model-agnostic approach to identify critical input\nfeatures for models in CI systems, by drawing on software debugging research,\nspecifically delta debugging. Our approach, SIVAND, uses simplification\ntechniques that reduce the size of input programs of a CI model while\npreserving the predictions of the model. We show that this approach yields\nremarkably small outputs and is broadly applicable across many model\narchitectures and problem domains. We find that the models in our experiments\noften rely heavily on just a few syntactic features in input programs. We\nbelieve that SIVAND's extracted features may help understand neural CI systems'\npredictions and learned behavior.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 05:44:29 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Rabin", "Md Rafiqul Islam", ""], ["Hellendoorn", "Vincent J.", ""], ["Alipour", "Mohammad Amin", ""]]}, {"id": "2106.03626", "submitter": "Miguel Grilo", "authors": "Miguel Grilo, Jo\\~ao F. Ferreira and Jos\\'e Bacelar Almeida", "title": "Towards Formal Verification of Password Generation Algorithms used in\n  Password Managers", "comments": "shortpaper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Password managers are important tools that enable us to use stronger\npasswords, freeing us from the cognitive burden of remembering them. Despite\nthis, there are still many users who do not fully trust password managers. In\nthis paper, we focus on a feature that most password managers offer that might\nimpact the user's trust, which is the process of generating a random password.\nWe survey which algorithms are most commonly used and we propose a solution for\na formally verified reference implementation of a password generation\nalgorithm. We use EasyCrypt as our framework to both specify the reference\nimplementation and to prove its functional correctness and security.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 13:57:07 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 14:43:47 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Grilo", "Miguel", ""], ["Ferreira", "Jo\u00e3o F.", ""], ["Almeida", "Jos\u00e9 Bacelar", ""]]}, {"id": "2106.04340", "submitter": "Dejan Jovanovic", "authors": "Dejan Jovanovi\\'c, Bruno Dutertre", "title": "Interpolation and Model Checking for Nonlinear Arithmetic", "comments": "To be published in CAV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL cs.SC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new model-based interpolation procedure for satisfiability\nmodulo theories (SMT). The procedure uses a new mode of interaction with the\nSMT solver that we call solving modulo a model. This either extends a given\npartial model into a full model for a set of assertions or returns an\nexplanation (a model interpolant) when no solution exists. This mode of\ninteraction fits well into the model-constructing satisfiability (MCSAT)\nframework of SMT. We use it to develop an interpolation procedure for any\nMCSAT-supported theory. In particular, this method leads to an effective\ninterpolation procedure for nonlinear real arithmetic. We evaluate the new\nprocedure by integrating it into a model checker and comparing it with\nstate-of-art model-checking tools for nonlinear arithmetic.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 13:56:56 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Jovanovi\u0107", "Dejan", ""], ["Dutertre", "Bruno", ""]]}, {"id": "2106.04655", "submitter": "Lu\\'is Pina", "authors": "Siddhanth Venkateshwaran, Ellen Kidane, Lu\\'is Pina", "title": "Dynamic Software Updates for Unmodified Browsers through Multi-Version\n  Execution", "comments": "23 pages, 5 tables, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the design, implementation, and evaluation of\nSINATRA, which supports instantaneous browser updates that do not result in any\ndata loss through a novel Multi-Version eXecution (MVX) approach for JavaScript\nprograms. SINATRA works in pure JavaScript, does not require any browser\nsupport, thus works on closed-source browsers, and requires trivial changes to\neach target page, that can be automated. First, SINATRA captures all the\nnon-determinism available to a JavaScript program (e.g., event handlers\nexecuted, expired timers, invocations of Math.random). Our evaluation shows\nthat SINATRA requires 5MB to store such events, and the memory grows at a\nmodest rate of 23.1KB/s as the user keeps interacting with each page. When an\nupdate becomes available, SINATRA transfer the state by re-executing the same\nset of non-deterministic events on the new browser. During this time, which can\nbe as long as 13 seconds, SINATRA uses MVX to allow the user to keep\ninteracting with the old browser. Finally, SINATRA changes the roles in 353ms,\nand the user starts interacting with the new browser, effectively performing a\nbrowser update with zero downtime and no loss of state.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 19:38:14 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Venkateshwaran", "Siddhanth", ""], ["Kidane", "Ellen", ""], ["Pina", "Lu\u00eds", ""]]}, {"id": "2106.04826", "submitter": "Kohei Suenaga", "authors": "Sota Sato and Ryotaro Banno and Jun Furuse and Kohei Suenaga and\n  Atsushi Igarashi", "title": "Verification of a Merkle Patricia Tree Library Using F*", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Merkle tree is a data structure for representing a key-value store as a\ntree. Each node of a Merkle tree is equipped with a hash value computed from\nthose of their descendants. A Merkle tree is often used for representing a\nstate of a blockchain system since it can be used for efficiently auditing the\nstate in a trustless manner. Due to the safety-critical nature of blockchains,\nensuring the correctness of their implementation is paramount.\n  We show our formally verified implementation of the core part of Plebeia\nusing F*. Plebeia is a library to manipulate an extension of Merkle trees\n(called Plebeia trees). It is being implemented as a part of the storage system\nof the Tezos blockchain system. To this end, we gradually ported Plebeia to F*;\nthe OCaml code extracted from the modules ported to F* is linked with the\nunverified part of Plebeia. By this gradual porting process, we can obtain a\nworking code from our partially verified implementation of Plebeia; we\nconfirmed that the binary passes all the unit tests of Plebeia.\n  More specifically, we verified the following properties on the implementation\nof Plebeia: (1) Each tree-manipulating function preserves the invariants on the\ndata structure of a Plebeia tree and satisfies the functional requirements as a\nnested key-value store; (2) Each function for serializing/deserializing a\nPlebeia tree to/from the low-level storage is implemented correctly; and (3)\nThe hash function for a Plebeia tree is relatively collision-resistant with\nrespect to the cryptographic safety of the blake2b hash function. During\nporting Plebeia to F*, we found a bug in an old version of Plebeia, which was\noverlooked by the tests bundled with the original implementation. To the best\nof our knowledge, this is the first work that verifies a production-level\nimplementation of a Merkle-tree library by F*.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 06:18:04 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Sato", "Sota", ""], ["Banno", "Ryotaro", ""], ["Furuse", "Jun", ""], ["Suenaga", "Kohei", ""], ["Igarashi", "Atsushi", ""]]}, {"id": "2106.04953", "submitter": "Tim Reichelt", "authors": "Tim Reichelt, Adam Goli\\'nski, Luke Ong, Tom Rainforth", "title": "Expectation Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building on ideas from probabilistic programming, we introduce the concept of\nan expectation programming framework (EPF) that automates the calculation of\nexpectations. Analogous to a probabilistic program, an expectation program is\ncomprised of a mix of probabilistic constructs and deterministic calculations\nthat define a conditional distribution over its variables. However, the focus\nof the inference engine in an EPF is to directly estimate the resulting\nexpectation of the program return values, rather than approximate the\nconditional distribution itself. This distinction allows us to achieve\nsubstantial performance improvements over the standard probabilistic\nprogramming pipeline by tailoring the inference to the precise expectation we\ncare about. We realize a particular instantiation of our EPF concept by\nextending the probabilistic programming language Turing to allow so-called\ntarget-aware inference to be run automatically, and show that this leads to\nsignificant empirical gains compared to conventional posterior-based inference.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 09:57:18 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Reichelt", "Tim", ""], ["Goli\u0144ski", "Adam", ""], ["Ong", "Luke", ""], ["Rainforth", "Tom", ""]]}, {"id": "2106.05421", "submitter": "Jialu Bao", "authors": "Jialu Bao, Drashti Pathak, Justin Hsu, Subhajit Roy", "title": "Data-Driven Invariant Learning for Probabilistic Programs", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morgan and McIver's weakest pre-expectation framework is one of the most\nwell-established methods for deductive verification of probabilistic programs.\nRoughly, the idea is to generalize binary state assertions to real-valued\nexpectations. While loop-free programs can be analyzed by mechanically\ntransforming expectations, verifying loops usually requires finding an\ninvariant expectation, a difficult task. We propose a new view of invariant\nexpectation synthesis as a regression problem: given an input state, predict\nthe average value of the post-expectation. Guided by this perspective, we\ndevelop the first data-driven invariant synthesis method for probabilistic\nprograms. Unlike prior work on probabilistic invariant inference, our approach\ncan learn piecewise continuous invariants without relying on template\nexpectations, and also works when only given black-box access to the program.\nWe implement our approach and demonstrate its effectiveness on a variety of\nbenchmarks from the probabilistic programming literature.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 22:27:11 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Bao", "Jialu", ""], ["Pathak", "Drashti", ""], ["Hsu", "Justin", ""], ["Roy", "Subhajit", ""]]}, {"id": "2106.05784", "submitter": "Tal Schuster", "authors": "Tal Schuster, Ashwin Kalyan, Oleksandr Polozov, Adam Tauman Kalai", "title": "Programming Puzzles", "comments": "The puzzles repo:\n  https://github.com/microsoft/PythonProgrammingPuzzles", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new type of programming challenge called programming puzzles,\nas an objective and comprehensive evaluation of program synthesis, and release\nan open-source dataset of Python Programming Puzzles (P3). Each puzzle is\ndefined by a short Python program $f$, and the goal is to find an input $x$\nwhich makes $f$ output \"True\". The puzzles are objective in that each one is\nspecified entirely by the source code of its verifier $f$, so evaluating $f(x)$\nis all that is needed to test a candidate solution $x$. They do not require an\nanswer key or input/output examples, nor do they depend on natural language\nunderstanding. The dataset is comprehensive in that it spans problems of a\nrange of difficulties and domains, ranging from trivial string manipulation\nproblems that are immediately obvious to human programmers (but not necessarily\nto AI), to classic programming puzzles (e.g., Towers of Hanoi), to\ninterview/competitive-programming problems (e.g., dynamic programming), to\nlongstanding open problems in algorithms and mathematics (e.g., factoring). The\nobjective nature of P3 readily supports self-supervised bootstrapping. We\ndevelop baseline enumerative program synthesis and GPT-3 solvers that are\ncapable of solving easy puzzles -- even without access to any reference\nsolutions -- by learning from their own past solutions. Based on a small user\nstudy, we find puzzle difficulty to correlate between human programmers and the\nbaseline AI solvers.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 14:37:28 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Schuster", "Tal", ""], ["Kalyan", "Ashwin", ""], ["Polozov", "Oleksandr", ""], ["Kalai", "Adam Tauman", ""]]}, {"id": "2106.06205", "submitter": "Adrien Guatto", "authors": "Sam van Gool, Adrien Guatto, George Metcalfe, and Simon Santschi", "title": "Time Warps, from Algebra to Algorithms", "comments": "Submitted to a conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.LO cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Graded modalities have been proposed in recent work on programming languages\nas a general framework for refining type systems with intensional properties.\nIn particular, continuous endomaps of the discrete time scale, or time warps,\ncan be used to quantify the growth of information in the course of program\nexecution. Time warps form a complete residuated lattice, with the residuals\nplaying an important role in potential programming applications. In this paper,\nwe study the algebraic structure of time warps, and prove that their equational\ntheory is decidable, a necessary condition for their use in real-world\ncompilers. We also describe how our universal-algebraic proof technique lends\nitself to a constraint-based implementation, establishing a new link between\nuniversal algebra and verification technology.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 07:20:34 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["van Gool", "Sam", ""], ["Guatto", "Adrien", ""], ["Metcalfe", "George", ""], ["Santschi", "Simon", ""]]}, {"id": "2106.06278", "submitter": "Yann Hamdaoui", "authors": "Teodoro Freund, Yann Hamdaoui and Arnaud Spiwack", "title": "Union and intersection contracts are hard, actually", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Union and intersection types are a staple of gradually typed language such as\nTypeScript. While it's long been recognized that union and intersection types\nare difficult to verify statically, it may appear at first that the dynamic\npart of gradual typing is actually pretty simple.\n  It turns out however, that in presence of higher-order contracts union and\nintersection are deceptively difficult. The literature on higher-order\ncontracts with union and intersection, while keenly aware of the fact, doesn't\nreally explain why. We point and illustrate the problems and trade-offs\ninherent to union and intersection contracts, via example and a survey of the\nliterature.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 09:48:19 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Freund", "Teodoro", ""], ["Hamdaoui", "Yann", ""], ["Spiwack", "Arnaud", ""]]}, {"id": "2106.06458", "submitter": "Chaochen Shi", "authors": "Chaochen Shi, Yong Xiang, Jiangshan Yu, Keshav Sood, Longxiang Gao", "title": "SolcTrans: Towards machine translation of Solidity smart contract source\n  code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Decentralized applications on blockchain platforms are realized\nthrough smart contracts. However, participants who lack programming knowledge\noften have difficulties reading the smart contract source codes, which leads to\npotential security risks and barriers to participation. Objective: Our\nobjective is to translate the smart contract source codes into natural language\ndescriptions to help people better understand, operate, and learn smart\ncontracts. Method: This paper proposes an automated translation tool for\nSolidity smart contracts, termed SolcTrans, based on an abstract syntax tree\nand formal grammar. We have investigated 3,000 smart contracts and determined\nthe part of speeches of corresponding blockchain terms. Among them, we further\nfiltered out contract snippets without detailed comments and left 811 snippets\nto evaluate the translation quality of SolcTrans. Results: Experimental results\nshow that even with a small corpus, SolcTrans can achieve similar performance\nto the state-of-the-art code comments generation models for other programming\nlanguages. In addition, SolcTrans has consistent performance when dealing with\ncode snippets with different lengths and gas consumption. Conclusion: SolcTrans\ncan correctly interpret Solidity codes and automatically convert them into\ncomprehensible English text. We will release our tool and dataset for\nsupporting reproduction and further studies in related fields.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 15:31:02 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Shi", "Chaochen", ""], ["Xiang", "Yong", ""], ["Yu", "Jiangshan", ""], ["Sood", "Keshav", ""], ["Gao", "Longxiang", ""]]}, {"id": "2106.06658", "submitter": "Bernardo Almeida", "authors": "Bernardo Almeida, Andreia Mordido, Peter Thiemann, Vasco T.\n  Vasconcelos", "title": "Polymorphic Context-free Session Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context-free session types provide a typing discipline for recursive\nstructured communication protocols on bidirectional channels. They overcome the\nrestriction of regular session type systems to tail recursive protocols. This\nextension enables us to implement serialisation and deserialisation of tree\nstructures in a fully type-safe manner.\n  We present the theory underlying the language FreeST 2, which features\ncontext-free session types in an extension of System F with linear types and a\nkind system to distinguish message types and channel types. The system presents\nsome metatheoretical challenges, which we address, contractivity in the\npresence of polymorphism, a non-trivial equational theory on types, and\ndecidability of type equivalence. We also establish standard results on type\npreservation, progress, and a characterisation of erroneous processes.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 01:23:48 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Almeida", "Bernardo", ""], ["Mordido", "Andreia", ""], ["Thiemann", "Peter", ""], ["Vasconcelos", "Vasco T.", ""]]}, {"id": "2106.07045", "submitter": "Pedro Lopez-Garcia", "authors": "Miguel A. Sanchez-Ordaz, Isabel Garcia-Contreras, Victor\n  Perez-Carrasco, Jose F. Morales, Pedro lopez-Garcia, Manuel V. Hermenegildo", "title": "VeriFly: On-the-fly Assertion Checking via Incrementality", "comments": "18 pages, 11 figures, 2 tables; submitted to ICLP 2021", "journal-ref": null, "doi": null, "report-no": "CLIP-1/2021.0", "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assertion checking is an invaluable programmer's tool for finding many\nclasses of errors or verifying their absence in dynamic languages such as\nProlog. For Prolog programmers this means being able to have relevant\nproperties such as modes, types, determinacy, non-failure, sharing,\nconstraints, cost, etc., checked and errors flagged without having to actually\nrun the program. Such global static analysis tools are arguably most useful the\nearlier they are used in the software development cycle, and fast response\ntimes are essential for interactive use. Triggering a full and precise semantic\nanalysis of a software project every time a change is made can be prohibitively\nexpensive. In our static analysis and verification framework this challenge is\naddressed through a combination of modular and incremental (context- and\npath-sensitive) analysis that is responsive to program edits, at different\nlevels of granularity. We describe how the combination of this framework within\nan integrated development environment (IDE) takes advantage of such\nincrementality to achieve a high level of reactivity when reflecting analysis\nand verification results back as colorings and tooltips directly on the program\ntext -- the tool's VeriFly mode. The concrete implementation that we describe\nis Emacs-based and reuses in part off-the-shelf \"on-the-fly\" syntax checking\nfacilities (flycheck). We believe that similar extensions are also reproducible\nwith low effort in other mature development environments. Our initial\nexperience with the tool shows quite promising results, with low latency times\nthat provide early, continuous, and precise assertion checking and other\nsemantic feedback to programmers during the development process. The tool\nsupports Prolog natively, as well as other languages by semantic transformation\ninto Horn clauses.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 17:13:32 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Sanchez-Ordaz", "Miguel A.", ""], ["Garcia-Contreras", "Isabel", ""], ["Perez-Carrasco", "Victor", ""], ["Morales", "Jose F.", ""], ["lopez-Garcia", "Pedro", ""], ["Hermenegildo", "Manuel V.", ""]]}, {"id": "2106.07175", "submitter": "Disha Shrivastava", "authors": "Disha Shrivastava, Hugo Larochelle, Daniel Tarlow", "title": "Learning to Combine Per-Example Solutions for Neural Program Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of program synthesis from examples is to find a computer program\nthat is consistent with a given set of input-output examples. Most\nlearning-based approaches try to find a program that satisfies all examples at\nonce. Our work, by contrast, considers an approach that breaks the problem into\ntwo stages: (a) find programs that satisfy only one example, and (b) leverage\nthese per-example solutions to yield a program that satisfies all examples. We\nintroduce the Cross Aggregator neural network module based on a multi-head\nattention mechanism that learns to combine the cues present in these\nper-example solutions to synthesize a global solution. Evaluation across\nprograms of different lengths and under two different experimental settings\nreveal that when given the same time budget, our technique significantly\nimproves the success rate over PCCoder arXiv:1809.04682v2 [cs.LG] and other\nablation baselines. The code, data and trained models for our work can be found\nat https://github.com/shrivastavadisha/N-PEPS.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 05:48:12 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Shrivastava", "Disha", ""], ["Larochelle", "Hugo", ""], ["Tarlow", "Daniel", ""]]}, {"id": "2106.07893", "submitter": "Irippuge Milinda Perera", "authors": "Shruthi Gorantala, Rob Springer, Sean Purser-Haskell, William Lam,\n  Royce Wilson, Asra Ali, Eric P. Astor, Itai Zukerman, Sam Ruth, Christoph\n  Dibak, Phillipp Schoppmann, Sasha Kulankhina, Alain Forget, David Marn,\n  Cameron Tew, Rafael Misoczki, Bernat Guillen, Xinyu Ye, Dennis Kraft, Damien\n  Desfontaines, Aishe Krishnamurthy, Miguel Guevara, Irippuge Milinda Perera,\n  Yurii Sushko, Bryant Gipson", "title": "A General Purpose Transpiler for Fully Homomorphic Encryption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fully homomorphic encryption (FHE) is an encryption scheme which enables\ncomputation on encrypted data without revealing the underlying data. While\nthere have been many advances in the field of FHE, developing programs using\nFHE still requires expertise in cryptography. In this white paper, we present a\nfully homomorphic encryption transpiler that allows developers to convert\nhigh-level code (e.g., C++) that works on unencrypted data into high-level code\nthat operates on encrypted data. Thus, our transpiler makes transformations\npossible on encrypted data.\n  Our transpiler builds on Google's open-source XLS SDK\n(https://github.com/google/xls) and uses an off-the-shelf FHE library, TFHE\n(https://tfhe.github.io/tfhe/), to perform low-level FHE operations. The\ntranspiler design is modular, which means the underlying FHE library as well as\nthe high-level input and output languages can vary. This modularity will help\naccelerate FHE research by providing an easy way to compare arbitrary programs\nin different FHE schemes side-by-side. We hope this lays the groundwork for\neventual easy adoption of FHE by software developers. As a proof-of-concept, we\nare releasing an experimental transpiler\n(https://github.com/google/fully-homomorphic-encryption/tree/main/transpiler)\nas open-source software.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 06:03:58 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Gorantala", "Shruthi", ""], ["Springer", "Rob", ""], ["Purser-Haskell", "Sean", ""], ["Lam", "William", ""], ["Wilson", "Royce", ""], ["Ali", "Asra", ""], ["Astor", "Eric P.", ""], ["Zukerman", "Itai", ""], ["Ruth", "Sam", ""], ["Dibak", "Christoph", ""], ["Schoppmann", "Phillipp", ""], ["Kulankhina", "Sasha", ""], ["Forget", "Alain", ""], ["Marn", "David", ""], ["Tew", "Cameron", ""], ["Misoczki", "Rafael", ""], ["Guillen", "Bernat", ""], ["Ye", "Xinyu", ""], ["Kraft", "Dennis", ""], ["Desfontaines", "Damien", ""], ["Krishnamurthy", "Aishe", ""], ["Guevara", "Miguel", ""], ["Perera", "Irippuge Milinda", ""], ["Sushko", "Yurii", ""], ["Gipson", "Bryant", ""]]}, {"id": "2106.08143", "submitter": "Janine Egert", "authors": "Janine Egert and Clemens Kreutz", "title": "Rcall: Calling R from Matlab", "comments": "3 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Summary: R and Matlab are two high-level scientific programming languages\nwhich are frequently applied in computational biology. To extend the wide\nvariety of available and approved implementations, we present the Rcall\ninterface which runs in MATLAB and provides direct access to methods and\nsoftware packages implemented in R. Rcall involves passing the relevant data to\nR, executing the specified R commands and forwarding the results to MATLAB for\nfurther use. The evaluation and conversion of the basic data types in R and\nMATLAB are provided. Due to the easy embedding of R facilities, Rcall greatly\nextends the functionality of the MATLAB programming language.\n  Availability: Source code is freely available at\nhttps://github.com/kreutz-lab/Rcall, implemented in MATLAB and supported on\nLinux, MS Windows and Mac OS X.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 13:47:23 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Egert", "Janine", ""], ["Kreutz", "Clemens", ""]]}, {"id": "2106.08470", "submitter": "Aziz Akhmedkhodjaev", "authors": "Aziz Akhmedkhodjaev", "title": "Introducing Type Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In type theory, we can express many practical ideas by attributing some\nadditional data to expressions we operate on during compilation. For instance,\nsome substructural type theories augment variables' typing judgments with the\ninformation of their usage. That is, they allow one to explicitly state how\nmany times - 0, 1, or many - a variable can be used. This solves the problem of\nresource usage control and allows us to treat variables as resources.\n  What's more, it often happens that this attributed information is interpreted\n(used) during the same compilation and erased before we run a program. A case\nin the point is that in the same substructural type theories, their type\ncheckers use these 0, 1, or many, to ensure that all variables are used as many\ntimes as these attributions say them to be.\n  Yet, there wasn't any programming language concept whose concern would be to\nallow a programmer to express these attributions in the language itself. That\nis, to let the programmer express which data the one wants to attribute to what\nexpressions and, most importantly, the meaning of the attributed data in their\nprogram.\n  As it turned out, the presence of such a concept allows us to express many\npractical ideas in the language itself. For instance, with appropriate means\nfor assigning the meaning of these attributions, this concept would allow one\nto express linear types as functionality in a separate program module, without\nthe need to refine the whole type system to add them.\n  In this paper, we present such a concept - we propose type properties. It\nallows a programmer to express these attributions while fulfilling the\nrequirement of being fully on the static level. That is, it allows one to\nexpress how to interpret these attributions during compilation and erases them\nbefore a program is passed to the runtime.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 22:29:35 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Akhmedkhodjaev", "Aziz", ""]]}, {"id": "2106.08704", "submitter": "Md Rafiqul Islam Rabin", "authors": "Md Rafiqul Islam Rabin, Aftab Hussain, Vincent J. Hellendoorn and\n  Mohammad Amin Alipour", "title": "Memorization and Generalization in Neural Code Intelligence Models", "comments": "manuscript in preparation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNN) are increasingly commonly used in software\nengineering and code intelligence tasks. These are powerful tools that are\ncapable of learning highly generalizable patterns from large datasets through\nmillions of parameters. At the same time, training DNNs means walking a knife's\nedges, because their large capacity also renders them prone to memorizing data\npoints. While traditionally thought of as an aspect of over-training, recent\nwork suggests that the memorization risk manifests especially strongly when the\ntraining datasets are noisy and memorization is the only recourse.\nUnfortunately, most code intelligence tasks rely on rather noise-prone and\nrepetitive data sources, such as GitHub, which, due to their sheer size, cannot\nbe manually inspected and evaluated. We evaluate the memorization and\ngeneralization tendencies in neural code intelligence models through a case\nstudy across several benchmarks and model families by leveraging established\napproaches from other fields that use DNNs, such as introducing targeted noise\ninto the training dataset. In addition to reinforcing prior general findings\nabout the extent of memorization in DNNs, our results shed light on the impact\nof noisy dataset in training.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 11:11:41 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Rabin", "Md Rafiqul Islam", ""], ["Hussain", "Aftab", ""], ["Hellendoorn", "Vincent J.", ""], ["Alipour", "Mohammad Amin", ""]]}, {"id": "2106.09164", "submitter": "Dmitri Soshnikov", "authors": "Dmitry Soshnikov and Yana Valieva", "title": "mPyPl: Python Monadic Pipeline Library for Complex Functional Data\n  Processing", "comments": "Published in Microsoft Journal of Applied Research, Dec.2019., Vol.\n  12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new Python library called mPyPl, which is\nintended to simplify complex data processing tasks using functional approach.\nThis library defines operations on lazy data streams of named dictionaries\nrepresented as generators (so-called multi-field datastreams), and allows\nenriching those data streams with more 'fields' in the process of data\npreparation and feature extraction. Thus, most data preparation tasks can be\nexpressed in the form of neat linear 'pipeline', similar in syntax to UNIX\npipes, or |> functional composition operator in F#.\n  We define basic operations on multi-field data streams, which resemble\nclassical monadic operations, and show similarity of the proposed approach to\nmonads in functional programming. We also show how the library was used in\ncomplex deep learning tasks of event detection in video, and discuss different\nevaluation strategies that allow for different compromises in terms of memory\nand performance.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 22:34:01 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Soshnikov", "Dmitry", ""], ["Valieva", "Yana", ""]]}, {"id": "2106.09282", "submitter": "Peng Qian", "authors": "Zhenguang Liu, Peng Qian, Xiang Wang, Lei Zhu, Qinming He, Shouling Ji", "title": "Smart Contract Vulnerability Detection: From Pure Neural Network to\n  Interpretable Graph Feature and Expert Pattern Fusion", "comments": "This paper has been accepted by IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart contracts hold digital coins worth billions of dollars, their security\nissues have drawn extensive attention in the past years. Towards smart contract\nvulnerability detection, conventional methods heavily rely on fixed expert\nrules, leading to low accuracy and poor scalability. Recent deep learning\napproaches alleviate this issue but fail to encode useful expert knowledge. In\nthis paper, we explore combining deep learning with expert patterns in an\nexplainable fashion. Specifically, we develop automatic tools to extract expert\npatterns from the source code. We then cast the code into a semantic graph to\nextract deep graph features. Thereafter, the global graph feature and local\nexpert patterns are fused to cooperate and approach the final prediction, while\nyielding their interpretable weights. Experiments are conducted on all\navailable smart contracts with source code in two platforms, Ethereum and VNT\nChain. Empirically, our system significantly outperforms state-of-the-art\nmethods. Our code is released.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 07:12:13 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Liu", "Zhenguang", ""], ["Qian", "Peng", ""], ["Wang", "Xiang", ""], ["Zhu", "Lei", ""], ["He", "Qinming", ""], ["Ji", "Shouling", ""]]}, {"id": "2106.10238", "submitter": "Fabian Zaiser", "authors": "Carol Mak, Fabian Zaiser, Luke Ong", "title": "Nonparametric Hamiltonian Monte Carlo", "comments": "33 pages, 13 figures. To appear in Proceedings of the 38th\n  International Conference on Machine Learning, PMLR 139, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic programming uses programs to express generative models whose\nposterior probability is then computed by built-in inference engines. A\nchallenging goal is to develop general purpose inference algorithms that work\nout-of-the-box for arbitrary programs in a universal probabilistic programming\nlanguage (PPL). The densities defined by such programs, which may use\nstochastic branching and recursion, are (in general) nonparametric, in the\nsense that they correspond to models on an infinite-dimensional parameter\nspace. However standard inference algorithms, such as the Hamiltonian Monte\nCarlo (HMC) algorithm, target distributions with a fixed number of parameters.\nThis paper introduces the Nonparametric Hamiltonian Monte Carlo (NP-HMC)\nalgorithm which generalises HMC to nonparametric models. Inputs to NP-HMC are a\nnew class of measurable functions called \"tree representable\", which serve as a\nlanguage-independent representation of the density functions of probabilistic\nprograms in a universal PPL. We provide a correctness proof of NP-HMC, and\nempirically demonstrate significant performance improvements over existing\napproaches on several nonparametric examples.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 17:03:05 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Mak", "Carol", ""], ["Zaiser", "Fabian", ""], ["Ong", "Luke", ""]]}, {"id": "2106.11455", "submitter": "Chia-Hsuan Lee", "authors": "Chia-Hsuan Lee, Oleksandr Polozov, Matthew Richardson", "title": "KaggleDBQA: Realistic Evaluation of Text-to-SQL Parsers", "comments": "Published as a conference paper at ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of database question answering is to enable natural language\nquerying of real-life relational databases in diverse application domains.\nRecently, large-scale datasets such as Spider and WikiSQL facilitated novel\nmodeling techniques for text-to-SQL parsing, improving zero-shot generalization\nto unseen databases. In this work, we examine the challenges that still prevent\nthese techniques from practical deployment. First, we present KaggleDBQA, a new\ncross-domain evaluation dataset of real Web databases, with domain-specific\ndata types, original formatting, and unrestricted questions. Second, we\nre-examine the choice of evaluation tasks for text-to-SQL parsers as applied in\nreal-life settings. Finally, we augment our in-domain evaluation task with\ndatabase documentation, a naturally occurring source of implicit domain\nknowledge. We show that KaggleDBQA presents a challenge to state-of-the-art\nzero-shot parsers but a more realistic evaluation setting and creative use of\nassociated database documentation boosts their accuracy by over 13.2%, doubling\ntheir performance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 00:08:03 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Lee", "Chia-Hsuan", ""], ["Polozov", "Oleksandr", ""], ["Richardson", "Matthew", ""]]}, {"id": "2106.11610", "submitter": "Bo Wang", "authors": "Bo Wang, Teodora Baluta, Aashish Kolluri, Prateek Saxena", "title": "SynGuar: Guaranteeing Generalization in Programming by Example", "comments": null, "journal-ref": null, "doi": "10.1145/3468264.3468621", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming by Example (PBE) is a program synthesis paradigm in which the\nsynthesizer creates a program that matches a set of given examples. In many\napplications of such synthesis (e.g., program repair or reverse engineering),\nwe are to reconstruct a program that is close to a specific target program, not\nmerely to produce some program that satisfies the seen examples. In such\nsettings, we wish that the synthesized program generalizes well, i.e., has as\nfew errors as possible on the unobserved examples capturing the target function\nbehavior. In this paper, we propose the first framework (called SynGuar) for\nPBE synthesizers that guarantees to achieve low generalization error with high\nprobability. Our main contribution is a procedure to dynamically calculate how\nmany additional examples suffice to theoretically guarantee generalization. We\nshow how our techniques can be used in 2 well-known synthesis approaches: PROSE\nand STUN (synthesis through unification), for common string-manipulation\nprogram benchmarks. We find that often a few hundred examples suffice to\nprovably bound generalization error below $5\\%$ with high ($\\geq 98\\%$)\nprobability on these benchmarks. Further, we confirm this empirically: SynGuar\nsignificantly improves the accuracy of existing synthesizers in generating the\nright target programs. But with fewer examples chosen arbitrarily, the same\nbaseline synthesizers (without SynGuar) overfit and lose accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 08:44:05 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wang", "Bo", ""], ["Baluta", "Teodora", ""], ["Kolluri", "Aashish", ""], ["Saxena", "Prateek", ""]]}, {"id": "2106.12434", "submitter": "Dimitri Racordon", "authors": "Dimitri Racordon and Aur\\'elien Coet and Didier Buchs", "title": "Fuel: A Compiler Framework for Safe Memory Management", "comments": "4 pages; to be published at ICOOOLPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Flow-sensitive type systems offer an elegant way to ensure memory-safety in\nprogramming languages. Unfortunately, their adoption in new or existing\nlanguages is often hindered by a painful effort to implement or integrate them\ninto compilers. This paper presents early results in our effort to alleviate\nthis task. We introduce Fuel, a type capability-based library that can be\nplugged onto a compiler toolchain to check for memory-safety properties. Fuel\nbuilds upon well-established ideas in the domain of capability-based system,\nand adds a mechanism leveraging dynamic checks to recover capabilities where\nstatic reasoning is either too difficult or impossible. This approach allows\nthe analysis to potentially cover situations where a typical type system might\nnot be expressive enough to statically reason about memory safety.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 14:36:31 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Racordon", "Dimitri", ""], ["Coet", "Aur\u00e9lien", ""], ["Buchs", "Didier", ""]]}, {"id": "2106.12496", "submitter": "Yusuke Izawa", "authors": "Yusuke Izawa, Hidehiko Masuhara, Carl Friedrich Bolz-Tereick, Youyou\n  Cong", "title": "Threaded Code Generation with a Meta-tracing JIT Compiler", "comments": "In Proceedings of ICOOOLPS '21: Workshop on Implementation,\n  Compilation, Optimization of OO Languages, Programs and Systems (ICOOOLPS\n  '21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language implementation frameworks such as RPython and Truffle/Graal are\neffective tools for creating a high-performance language with lower effort than\nimplementing from scratch. The two frameworks support only a single JIT\ncompilation strategy, trace-based compilation and method-based compilation, but\nthey have its own advantages and disadvantages. We proposed a meta-hybrid JIT\ncompiler framework to take advantages of the two strategies as a language\nimplementation framework. We also implemented a proof-of-concept framework\ncalled BacCaml. As a next step, in this position paper, we propose a new\napproach to realize a method-based baseline JIT compiler along with a\ntrace-based JIT compilation. We aim to use it for further speed-up by\npreventing the path-divergence problem, which causes serious slow-down. We also\nshow how to implement the baseline JIT compiler with minimal changes on top of\nRPython.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 16:11:06 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 11:23:09 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 11:21:44 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Izawa", "Yusuke", ""], ["Masuhara", "Hidehiko", ""], ["Bolz-Tereick", "Carl Friedrich", ""], ["Cong", "Youyou", ""]]}, {"id": "2106.12678", "submitter": "Dimitri Racordon", "authors": "Dimitri Racordon and Denys Shabalin and Daniel Zheng and Dave Abrahams\n  and Brennan Saeta", "title": "Native Implementation of Mutable Value Semantics", "comments": "4 pages; to be published at ICOOOLPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unrestricted mutation of shared state is a source of many well-known\nproblems. The predominant safe solutions are pure functional programming, which\nbans mutation outright, and flow sensitive type systems, which depend on\nsophisticated typing rules. Mutable value semantics is a third approach that\nbans sharing instead of mutation, thereby supporting part-wise in-place\nmutation and local reasoning, while maintaining a simple type system. In the\npurest form of mutable value semantics, references are second-class: they are\nonly created implicitly, at function boundaries, and cannot be stored in\nvariables or object fields. Hence, variables can never share mutable state.\n  Because references are often regarded as an indispensable tool to write\nefficient programs, it is legitimate to wonder whether such a discipline can\ncompete other approaches. As a basis for answering that question, we\ndemonstrate how a language featuring mutable value semantics can be compiled to\nefficient native code. This approach relies on stack allocation for static\ngarbage collection and leverages runtime knowledge to sidestep unnecessary\ncopies.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 22:40:06 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Racordon", "Dimitri", ""], ["Shabalin", "Denys", ""], ["Zheng", "Daniel", ""], ["Abrahams", "Dave", ""], ["Saeta", "Brennan", ""]]}, {"id": "2106.12849", "submitter": "Francesco Gavazzo", "authors": "Ugo Dal Lago and Francesco Gavazzo", "title": "Resource Transition Systems and Full Abstraction for Linear Higher-Order\n  Effectful Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We investigate program equivalence for linear higher-order(sequential)\nlanguages endowed with primitives for computational effects. More specifically,\nwe study operationally-based notions of program equivalence for a linear\n$\\lambda$-calculus with explicit copying and algebraic effects \\emph{\\`a la}\nPlotkin and Power. Such a calculus makes explicit the interaction between\ncopying and linearity, which are intensional aspects of computation, with\neffects, which are, instead, \\emph{extensional}. We review some of the notions\nof equivalences for linear calculi proposed in the literature and show their\nlimitations when applied to effectful calculi where copying is a first-class\ncitizen. We then introduce resource transition systems, namely transition\nsystems whose states are built over tuples of programs representing the\navailable resources, as an operational semantics accounting for both\nintensional and extensional interactive behaviors of programs. Our main result\nis a sound and complete characterization of contextual equivalence as trace\nequivalence defined on top of resource transition systems.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 09:21:13 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Lago", "Ugo Dal", ""], ["Gavazzo", "Francesco", ""]]}, {"id": "2106.12934", "submitter": "Jeppe Blaabjerg", "authors": "Jeppe Fredsgaard Blaabjerg, Aslan Askarov", "title": "Towards Language-Based Mitigation of Traffic Analysis Attacks", "comments": "22 pages, 5 figures, submitted to CSF 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic analysis attacks pose a major risk for online security. Distinctive\npatterns in communication act as fingerprints, enabling adversaries to\nde-anonymise communicating parties or to infer sensitive information. Despite\nthe attacks being known for decades, practical solution are scarce. Network\nlayer countermeasures have relied on black box padding schemes that require\nsignificant overheads in latency and bandwidth to mitigate the attacks, without\nfundamentally preventing them, and the problem has received little attention in\nthe language-based information flow literature. Language-based methods provide\na strong foundation for fundamentally addressing security issues, but previous\nwork has overwhelmingly assumed that interactive programs communicate over\nsecure channels, where messages are undetectable by unprivileged adversaries.\nThis assumption is too strong for online communication where packets can be\ntrivially observed by eavesdropping. In this paper we introduce SELENE, a small\nlanguage for principled, provably secure communication over channels where\npackets are publicly observable, and we demonstrate how our program level\ndefence can reduce the latency and bandwidth overheads induced compared with\nprogram-agnostic defence mechanisms. We believe that our results constitute a\nstep towards practical, secure online communication.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 11:58:17 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Blaabjerg", "Jeppe Fredsgaard", ""], ["Askarov", "Aslan", ""]]}, {"id": "2106.12973", "submitter": "Bruno Bernardo", "authors": "Bruno Bernardo, Rapha\\\"el Cauderlier, Guillaume Claret, Arvid\n  Jakobsson, Basile Pesin, and Julien Tesson", "title": "Making Tezos smart contracts more reliable with Coq", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-61467-6_5", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tezos is a smart-contract blockchain. Tezos smart contracts are written in a\nlow-level stack-based language called Michelson. This article gives an overview\nof efforts using the Coq proof assistant to have stronger guarantees on\nMichelson smart contracts: the Mi-Cho-Coq framework, a Coq library defining\nformal semantics of Michelson, as well as an interpreter, a simple optimiser\nand a weakest-precondition calculus to reason about Michelson smart contracts;\nAlbert, an intermediate language that abstracts Michelson stacks with a\ncompiler written in Coq that targets Mi-Cho-Coq.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 12:47:23 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Bernardo", "Bruno", ""], ["Cauderlier", "Rapha\u00ebl", ""], ["Claret", "Guillaume", ""], ["Jakobsson", "Arvid", ""], ["Pesin", "Basile", ""], ["Tesson", "Julien", ""]]}, {"id": "2106.12995", "submitter": "Konrad Siek", "authors": "Konrad Siek, Colette Kerr", "title": "Userfault Objects: Transparent Programmable Memory", "comments": "In Proceedings of ICOOLPS '21: Workshop on Implementation,\n  Compilation, Optimization of OO Languages, Programs and Systems (ICOOOLPS\n  '21). UFOs repository: https://github.com/PRL-PRG/ufos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Userfault Object (UFO) framework explores avenues of cooperating with the\noperating system to use memory in non-traditional ways. We implement a\nframework that employs the Linux kernel's userfault mechanism to fill the\ncontents of runtime objects on demand. When an object's memory is accessed the\nframework executes a user-defined function that generates a slice of the\nobject. The back-end can generate data from thin air, calculate it from a\nformula, or retrieve it from persistent storage, the network, or other sources\n(with or without post-processing). UFOs follow the memory layout of standard\nruntime objects, so they can be introspected and written to safely. The\nframework manages the loading and unloading of object segments to ensure that\nmemory is reclaimed as needed and data is never lost. This allows the UFO\nframework to implement larger-than-memory data structures that never\nmaterialize into memory in full. Implementing objects as UFOs also impacts\nperformance, since overhead of populating memory is amortized by loading entire\npages of data at a time. The host runtime can also rely on direct memory\naccesses into userfault object obviating the need for a special dispatch\nmechanism. We provide a proof-of-concept implementation of the UFO framework\nfor the R language.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 13:09:18 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Siek", "Konrad", ""], ["Kerr", "Colette", ""]]}, {"id": "2106.13309", "submitter": "Micha{\\l} Gajda", "authors": "Micha{\\l} J. Gajda", "title": "Consistent ultrafinitist logic", "comments": "First submitted to CiE2021 in January, under review for GandALF2021.\n  Contains inference rules, and polynomial reduction rules", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrafinitism postulates that we can only compute on relatively short\nobjects, and numbers beyond certain value are not available. This approach\nwould also forbid many forms of infinitary reasoning and allow to remove\ncertain paradoxes stemming from enumeration theorems.\n  However, philosophers still disagree of whether such a finitist logic would\nbe consistent. We present preliminary work on a proof system based on\nCurry-Howard isomorphism. We also try to present some well-known theorems that\nstop being true in such systems, whereas opposite statements become provable.\n  This approach presents certain impossibility results as logical paradoxes\nstemming from a profligate use of transfinite reasoning.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 20:37:36 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Gajda", "Micha\u0142 J.", ""]]}, {"id": "2106.13928", "submitter": "Rui Huang", "authors": "Jingxuan Li, Rui Huang, Wei Li, Kai Yao, Weiguo Tan", "title": "Toward Less Hidden Cost of Code Completion with Acceptance and Ranking\n  Models", "comments": "10 pages, 7 figures, accepted by ICSME 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Code completion is widely used by software developers to provide coding\nsuggestions given a partially written code snippet. Apart from the traditional\ncode completion methods, which only support single token completion at minimal\npositions, recent studies show the ability to provide longer code completion at\nmore flexible positions. However, such frequently triggered and longer\ncompletion results reduce the overall precision as they generate more invalid\nresults. Moreover, different studies are mostly incompatible with each other.\nThus, it is vital to develop an ensemble framework that can combine results\nfrom multiple models to draw merits and offset defects of each model.\n  This paper conducts a coding simulation to collect data from code context and\ndifferent code completion models and then apply the data in two tasks. First,\nwe introduce an acceptance model which can dynamically control whether to\ndisplay completion results to the developer. It uses simulation features to\npredict whether correct results exist in the output of these models. Our best\nmodel reduces the percentage of false-positive completion from 55.09% to\n17.44%. Second, we design a fusion ranking scheme that can automatically\nidentify the priority of the completion results and reorder the candidates from\nmultiple code completion models. This scheme is flexible in dealing with\nvarious models, regardless of the type or the length of their completion\nresults. We integrate this ranking scheme with two frequency models and a GPT-2\nstyled language model, along with the acceptance model to yield 27.80% and\n37.64% increase in TOP1 and TOP5 accuracy, respectively. In addition, we\npropose a new code completion evaluation metric, Benefit-Cost Ratio(BCR),\ntaking into account the benefit of keystrokes saving and hidden cost of\ncompletion list browsing, which is closer to real coder experience scenario.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 03:02:49 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Li", "Jingxuan", ""], ["Huang", "Rui", ""], ["Li", "Wei", ""], ["Yao", "Kai", ""], ["Tan", "Weiguo", ""]]}, {"id": "2106.13936", "submitter": "David Kahn", "authors": "David M Kahn and Jan Hoffmann", "title": "Automatic Amortized Resource Analysis with the Quantum Physicist's\n  Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for working with the physicist's method of\namortized resource analysis, which we call the quantum physicist's method.\nThese principles allow for more precise analyses of resources that are not\nmonotonically consumed, like stack. This method takes its name from its two\nmajor features, worldviews and resource tunneling, which behave analogously to\nquantum superposition and quantum tunneling. We use the quantum physicist's\nmethod to extend the Automatic Amortized Resource Analysis (AARA) type system,\nenabling the derivation of resource bounds based on tree depth. In doing so, we\nalso introduce remainder contexts, which aid bookkeeping in linear type\nsystems. We then evaluate this new type system's performance by bounding stack\nuse of functions in the Set module of OCaml's standard library. Compared to\nstate-of-the-art implementations of AARA, our new system derives tighter bounds\nwith only moderate overhead.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 03:50:00 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kahn", "David M", ""], ["Hoffmann", "Jan", ""]]}, {"id": "2106.14586", "submitter": "Martin Sulzmann", "authors": "Martin Sulzmann and Stefan Wehr", "title": "A Dictionary-Passing Translation of Featherweight Go", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Go programming language is an increasingly popular language but some of\n  its features lack a formal investigation.\n  This article explains Go's resolution mechanism for overloaded methods and\n  its support for structural subtyping by\n  means of translation from Featherweight Go to a simple target language.\n  The translation employs a form of dictionary passing known from type classes\n  in Haskell and preserves the\n  dynamic behavior of Featherweight Go programs.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 11:48:01 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Sulzmann", "Martin", ""], ["Wehr", "Stefan", ""]]}, {"id": "2106.14938", "submitter": "Gert-Jan Bottu", "authors": "Gert-Jan Bottu and Richard A. Eisenberg", "title": "Seeking Stability by being Lazy and Shallow", "comments": "Haskell Symposium 21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Designing a language feature often requires a choice between several,\nsimilarly expressive possibilities. Given that user studies are generally\nimpractical, we propose using stability as a way of making such decisions.\nStability is a measure of whether the meaning of a program alters under small,\nseemingly innocuous changes in the code. Directly motivated by a need to pin\ndown a feature in GHC/Haskell, we apply this notion of stability to analyse\nfour approaches to the instantiation of polymorphic types, concluding that the\nmost stable approach is lazy (instantiate a polytype only when absolutely\nnecessary) and shallow (instantiate only top-level type variables, not\nvariables that appear after explicit arguments).\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 18:39:16 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 14:31:19 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Bottu", "Gert-Jan", ""], ["Eisenberg", "Richard A.", ""]]}, {"id": "2106.15339", "submitter": "Xinyun Chen", "authors": "Xinyun Chen, Petros Maniatis, Rishabh Singh, Charles Sutton, Hanjun\n  Dai, Max Lin, Denny Zhou", "title": "SpreadsheetCoder: Formula Prediction from Semi-structured Context", "comments": "Published in ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spreadsheet formula prediction has been an important program synthesis\nproblem with many real-world applications. Previous works typically utilize\ninput-output examples as the specification for spreadsheet formula synthesis,\nwhere each input-output pair simulates a separate row in the spreadsheet.\nHowever, this formulation does not fully capture the rich context in real-world\nspreadsheets. First, spreadsheet data entries are organized as tables, thus\nrows and columns are not necessarily independent from each other. In addition,\nmany spreadsheet tables include headers, which provide high-level descriptions\nof the cell data. However, previous synthesis approaches do not consider\nheaders as part of the specification. In this work, we present the first\napproach for synthesizing spreadsheet formulas from tabular context, which\nincludes both headers and semi-structured tabular data. In particular, we\npropose SpreadsheetCoder, a BERT-based model architecture to represent the\ntabular context in both row-based and column-based formats. We train our model\non a large dataset of spreadsheets, and demonstrate that SpreadsheetCoder\nachieves top-1 prediction accuracy of 42.51%, which is a considerable\nimprovement over baselines that do not employ rich tabular context. Compared to\nthe rule-based system, SpreadsheetCoder assists 82% more users in composing\nformulas on Google Sheets.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 11:26:27 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Chen", "Xinyun", ""], ["Maniatis", "Petros", ""], ["Singh", "Rishabh", ""], ["Sutton", "Charles", ""], ["Dai", "Hanjun", ""], ["Lin", "Max", ""], ["Zhou", "Denny", ""]]}, {"id": "2106.15878", "submitter": "Benjamin Maschler", "authors": "Matthias Wei{\\ss}, Philipp Marks, Benjamin Maschler, Dustin White,\n  Pascal Kesseli and Michael Weyrich", "title": "Towards establishing formal verification and inductive code synthesis in\n  the PLC domain", "comments": "8 pages, 6 figures, 1 table. Accepted for publication at IEEE INDIN\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.FL cs.PL cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, formal methods are used in various areas for the verification of\nprograms or for code generation from models in order to increase the quality of\nsoftware and to reduce costs. However, there are still fields in which formal\nmethods have not been widely adopted, despite the large set of possible\nbenefits offered. This is the case for the area of programmable logic\ncontrollers (PLC). This article aims to evaluate the potential of formal\nmethods in the context of PLC development. For this purpose, the general\nconcepts of formal methods are first introduced and then transferred to the PLC\narea, resulting in an engineering-oriented description of the technology that\nis based on common concepts from PLC development. Based on this description,\nPLC professionals with varying degrees of experience were interviewed for their\nperspective on the topic and to identify possible use cases within the PLC\ndomain. The survey results indicate the technology's high potential in the PLC\narea, either as a tool to directly support the developer or as a key element\nwithin a model-based systems engineering toolchain. The evaluation of the\nsurvey results is performed with the aid of a demo application that\ncommunicates with the Totally Integrated Automation Portal from Siemens and\ngenerates programs via Fastsynth, a model-based open source code generator.\nBenchmarks based on an industry-related PLC project show satisfactory synthesis\ntimes and a successful integration into the workflow of a PLC developer.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 08:13:10 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Wei\u00df", "Matthias", ""], ["Marks", "Philipp", ""], ["Maschler", "Benjamin", ""], ["White", "Dustin", ""], ["Kesseli", "Pascal", ""], ["Weyrich", "Michael", ""]]}]