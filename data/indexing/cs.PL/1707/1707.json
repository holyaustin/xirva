[{"id": "1707.00639", "submitter": "Constantin Enea", "authors": "Ahmed Bouajjani, Constantin Enea, and Chao Wang", "title": "Checking Linearizability of Concurrent Priority Queues", "comments": "An extended abstract is published in the Proceedings of CONCUR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient implementations of concurrent objects such as atomic collections\nare essential to modern computing. Programming such objects is error prone: in\nminimizing the synchronization overhead between concurrent object invocations,\none risks the conformance to sequential specifications -- or in formal terms,\none risks violating linearizability. Unfortunately, verifying linearizability\nis undecidable in general, even on classes of implementations where the usual\ncontrol-state reachability is decidable. In this work we consider concurrent\npriority queues which are fundamental to many multi-threaded applications such\nas task scheduling or discrete event simulation, and show that verifying\nlinearizability of such implementations can be reduced to control-state\nreachability. This reduction entails the first decidability results for\nverifying concurrent priority queues in the context of an unbounded number of\nthreads, and it enables the application of existing safety-verification tools\nfor establishing their correctness.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 16:40:42 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Bouajjani", "Ahmed", ""], ["Enea", "Constantin", ""], ["Wang", "Chao", ""]]}, {"id": "1707.01277", "submitter": "Alexey Bakhirkin", "authors": "Alexey Bakhirkin (VERIMAG - IMAG), David Monniaux (VERIMAG - IMAG)", "title": "Combining Forward and Backward Abstract Interpretation of Horn Clauses", "comments": "Francesco Ranzato. 24th International Static Analysis Symposium\n  (SAS), Aug 2017, New York City, United States. Springer, Static Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alternation of forward and backward analyses is a standard technique in\nabstract interpretation of programs, which is in particular useful when we wish\nto prove unreachability of some undesired program states. The current\nstate-of-the-art technique for combining forward (bottom-up, in logic\nprogramming terms) and backward (top-down) abstract interpretation of Horn\nclauses is query-answer transformation. It transforms a system of Horn clauses,\nsuch that standard forward analysis can propagate constraints both forward, and\nbackward from a goal. Query-answer transformation is effective, but has issues\nthat we wish to address. For that, we introduce a new backward collecting\nsemantics, which is suitable for alternating forward and backward abstract\ninterpretation of Horn clauses. We show how the alternation can be used to\nprove unreachability of the goal and how every subsequent run of an analysis\nyields a refined model of the system. Experimentally, we observe that combining\nforward and backward analyses is important for analysing systems that encode\nquestions about reachability in C programs. In particular, the combination that\nfollows our new semantics improves the precision of our own abstract\ninterpreter, including when compared to a forward analysis of a\nquery-answer-transformed system.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 09:30:31 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 14:17:49 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Bakhirkin", "Alexey", "", "VERIMAG - IMAG"], ["Monniaux", "David", "", "VERIMAG - IMAG"]]}, {"id": "1707.01469", "submitter": "Xinyu Wang", "authors": "Xinyu Wang, Isil Dillig, Rishabh Singh", "title": "Synthesis of Data Completion Scripts using Finite Tree Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In application domains that store data in a tabular format, a common task is\nto fill the values of some cells using values stored in other cells. For\ninstance, such data completion tasks arise in the context of missing value\nimputation in data science and derived data computation in spreadsheets and\nrelational databases. Unfortunately, end-users and data scientists typically\nstruggle with many data completion tasks that require non-trivial programming\nexpertise. This paper presents a synthesis technique for automating data\ncompletion tasks using programming-by-example (PBE) and a very lightweight\nsketching approach. Given a formula sketch (e.g., AVG($?_1$, $?_2$)) and a few\ninput-output examples for each hole, our technique synthesizes a program to\nautomate the desired data completion task. Towards this goal, we propose a\ndomain-specific language (DSL) that combines spatial and relational reasoning\nover tabular data and a novel synthesis algorithm that can generate DSL\nprograms that are consistent with the input-output examples. The key technical\nnovelty of our approach is a new version space learning algorithm that is based\non finite tree automata (FTA). The use of FTAs in the learning algorithm leads\nto a more compact representation that allows more sharing between programs that\nare consistent with the examples. We have implemented the proposed approach in\na tool called DACE and evaluate it on 84 benchmarks taken from online help\nforums. We also illustrate the advantages of our approach by comparing our\ntechnique against two existing synthesizers, namely PROSE and SKETCH.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 17:05:54 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Wang", "Xinyu", ""], ["Dillig", "Isil", ""], ["Singh", "Rishabh", ""]]}, {"id": "1707.01550", "submitter": "Anthony Di Franco", "authors": "Anthony Di Franco", "title": "Information-gain computation", "comments": "Accepted, PLP 2017 (http://www.stoics.org.uk/plp/plp2017/) Revised\n  version of project for Phys 256B @ Davis\n  (http://csc.ucdavis.edu/~chaos/courses/ncaso/) Reduces to practice ideas\n  previously presented in arXiv:1505.00002 and at\n  https://www.meetup.com/SF-Types-Theorems-and-Programming-Languages/events/232908199/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite large incentives, ecorrectness in software remains an elusive goal.\nDeclarative programming techniques, where algorithms are derived from a\nspecification of the desired behavior, offer hope to address this problem,\nsince there is a combinatorial reduction in complexity in programming in terms\nof specifications instead of algorithms, and arbitrary desired properties can\nbe expressed and enforced in specifications directly. However, limitations on\nperformance have prevented programming with declarative specifications from\nbecoming a mainstream technique for general-purpose programming. To address the\nperformance bottleneck in deriving an algorithm from a specification, I propose\ninformation-gain computation, a framework where an adaptive evaluation strategy\nis used to efficiently perform a search which derives algorithms that provide\ninformation about a query most directly. Within this framework, opportunities\nto compress the search space present themselves, which suggest that\ninformation-theoretic bounds on the performance of such a system might be\narticulated and a system designed to achieve them. In a preliminary empirical\nstudy of adaptive evaluation for a simple test program, the evaluation strategy\nadapts successfully to evaluate a query efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 19:26:14 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 00:52:56 GMT"}, {"version": "v3", "created": "Mon, 14 Aug 2017 01:16:23 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Di Franco", "Anthony", ""]]}, {"id": "1707.01989", "submitter": "Alastair Donaldson", "authors": "Tyler Sorensen, Hugues Evrard, Alastair F. Donaldson", "title": "Cooperative Kernels: GPU Multitasking for Blocking Algorithms (Extended\n  Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing interest in accelerating irregular data-parallel algorithms\non GPUs. These algorithms are typically blocking, so they require fair\nscheduling. But GPU programming models (e.g.\\ OpenCL) do not mandate fair\nscheduling, and GPU schedulers are unfair in practice. Current approaches avoid\nthis issue by exploiting scheduling quirks of today's GPUs in a manner that\ndoes not allow the GPU to be shared with other workloads (such as graphics\nrendering tasks). We propose cooperative kernels, an extension to the\ntraditional GPU programming model geared towards writing blocking algorithms.\nWorkgroups of a cooperative kernel are fairly scheduled, and multitasking is\nsupported via a small set of language extensions through which the kernel and\nscheduler cooperate. We describe a prototype implementation of a cooperative\nkernel framework implemented in OpenCL 2.0 and evaluate our approach by porting\na set of blocking GPU applications to cooperative kernels and examining their\nperformance under multitasking. Our prototype exploits no vendor-specific\nhardware, driver or compiler support, thus our results provide a lower-bound on\nthe efficiency with which cooperative kernels can be implemented in practice.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 22:50:36 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Sorensen", "Tyler", ""], ["Evrard", "Hugues", ""], ["Donaldson", "Alastair F.", ""]]}, {"id": "1707.02029", "submitter": "Saswat Padhi", "authors": "Saswat Padhi and Rahul Sharma and Todd Millstein", "title": "LoopInvGen: A Loop Invariant Generator based on Precondition Inference", "comments": "Tool Description ( for technical details, see our PLDI paper at\n  https://doi.org/10.1145/2908080.2908099 ), SyGuS-COMP'19 Competition\n  Contribution, 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe the LoopInvGen tool for generating loop invariants that can\nprovably guarantee correctness of a program with respect to a given\nspecification. LoopInvGen is an efficient implementation of the inference\ntechnique originally proposed in our earlier work on PIE\n(https://doi.org/10.1145/2908080.2908099).\n  In contrast to existing techniques, LoopInvGen is not restricted to a fixed\nset of features -- atomic predicates that are composed together to build\ncomplex loop invariants. Instead, we start with no initial features, and use\nprogram synthesis techniques to grow the set on demand. This not only enables a\nless onerous and more expressive approach, but also appears to be significantly\nfaster than the existing tools over the SyGuS-COMP 2018 benchmarks from the INV\ntrack.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 03:51:39 GMT"}, {"version": "v2", "created": "Sat, 23 Jun 2018 12:00:06 GMT"}, {"version": "v3", "created": "Sun, 8 Jul 2018 07:15:41 GMT"}, {"version": "v4", "created": "Thu, 31 Oct 2019 10:16:52 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Padhi", "Saswat", ""], ["Sharma", "Rahul", ""], ["Millstein", "Todd", ""]]}, {"id": "1707.02034", "submitter": "Christoph Rauch", "authors": "Ryu Hasegawa", "title": "Complete Call-by-Value Calculi of Control Operators II: Strong\n  Termination", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 17, Issue 1 (March 2,\n  2021) lmcs:7234", "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We provide characterization of the strong termination property of the CCV\n(complete call-by-value) lambda-mu calculus introduced in the first part of\nthis series of the paper. The calculus is complete with respect to the standard\ncontinuation-passing style (CPS) semantics. The union-intersection type systems\nfor the calculus is developed in the previous paper. We characterize the strong\nnormalizability of terms of the calculus in terms of the CPS semantics and\ntypeability.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 04:45:45 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 05:18:08 GMT"}, {"version": "v3", "created": "Mon, 24 Aug 2020 01:25:12 GMT"}, {"version": "v4", "created": "Mon, 1 Mar 2021 14:57:51 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Hasegawa", "Ryu", ""]]}, {"id": "1707.02056", "submitter": "Ryu Hasegawa", "authors": "Ryu Hasegawa", "title": "Complete Call-by-Value Calculi of Control Operators, I", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give new call-by-value calculi of control operators that are complete for\nthe continuation-passing style semantics. Various anticipated computational\nproperties are induced from the completeness. In the first part of a series of\npapers, we give the characterization of termination properties using the\ncontinuation-passing style semantics as well as the union-intersection type\ndiscipline.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 06:44:52 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 06:41:28 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Hasegawa", "Ryu", ""]]}, {"id": "1707.02115", "submitter": "Heiko Becker", "authors": "Heiko Becker, Nikita Zyuzin, Raphael Monat, Eva Darulova, Magnus O.\n  Myreen, Anthony Fox", "title": "A Verified Certificate Checker for Finite-Precision Error Bounds in Coq\n  and HOL4", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to soundly estimate roundoff errors of finite-precision\ncomputations is important for many applications in embedded systems and\nscientific computing. Due to the discrepancy between continuous reals and\ndiscrete finite-precision values, automated static analysis tools are highly\nvaluable to estimate roundoff errors. The results, however, are only as correct\nas the implementations of the static analysis tools. This paper presents a\nformally verified and modular tool which fully automatically checks the\ncorrectness of finite-precision roundoff error bounds encoded in a certificate.\nWe present implementations of certificate generation and checking for both Coq\nand HOL4 and evaluate it on a number of examples from the literature. The\nexperiments use both in-logic evaluation of Coq and HOL4, and execution of\nextracted code outside of the logics: we benchmark Coq extracted unverified\nOCaml code and a CakeML-generated verified binary.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 10:53:13 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 12:53:57 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Becker", "Heiko", ""], ["Zyuzin", "Nikita", ""], ["Monat", "Raphael", ""], ["Darulova", "Eva", ""], ["Myreen", "Magnus O.", ""], ["Fox", "Anthony", ""]]}, {"id": "1707.02118", "submitter": "Eva Darulova", "authors": "Eva Darulova, Einar Horn, Saksham Sharma", "title": "Sound Mixed-Precision Optimization with Rewriting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite-precision arithmetic computations face an inherent tradeoff between\naccuracy and efficiency. The points in this tradeoff space are determined,\namong other factors, by different data types but also evaluation orders. To put\nit simply, the shorter a precision's bit-length, the larger the roundoff error\nwill be, but the faster the program will run. Similarly, the fewer arithmetic\noperations the program performs, the faster it will run; however, the effect on\nthe roundoff error is less clear-cut. Manually optimizing the efficiency of\nfinite-precision programs while ensuring that results remain accurate enough is\nchallenging. The unintuitive and discrete nature of finite-precision makes\nestimation of roundoff errors difficult; furthermore the space of possible data\ntypes and evaluation orders is prohibitively large. We present the first fully\nautomated and sound technique and tool for optimizing the performance of\nfloating-point and fixed-point arithmetic kernels. Our technique combines\nrewriting and mixed-precision tuning. Rewriting searches through different\nevaluation orders to find one which minimizes the roundoff error at no\nadditional runtime cost. Mixed-precision tuning assigns different finite\nprecisions to different variables and operations and thus provides\nfiner-grained control than uniform precision. We show that when these two\ntechniques are designed and applied together, they can provide higher\nperformance improvements than each alone.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 11:17:02 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Darulova", "Eva", ""], ["Horn", "Einar", ""], ["Sharma", "Saksham", ""]]}, {"id": "1707.02121", "submitter": "Anastasiia Izycheva", "authors": "Anastasiia Izycheva, Eva Darulova", "title": "On Sound Relative Error Bounds for Floating-Point Arithmetic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art static analysis tools for verifying finite-precision code\ncompute worst-case absolute error bounds on numerical errors. These are,\nhowever, often not a good estimate of accuracy as they do not take into account\nthe magnitude of the computed values. Relative errors, which compute errors\nrelative to the value's magnitude, are thus preferable. While today's tools do\nreport relative error bounds, these are merely computed via absolute errors and\nthus not necessarily tight or more informative. Furthermore, whenever the\ncomputed value is close to zero on part of the domain, the tools do not report\nany relative error estimate at all. Surprisingly, the quality of relative error\nbounds computed by today's tools has not been systematically studied or\nreported to date. In this paper, we investigate how state-of-the-art static\ntechniques for computing sound absolute error bounds can be used, extended and\ncombined for the computation of relative errors. Our experiments on a standard\nbenchmark set show that computing relative errors directly, as opposed to via\nabsolute errors, is often beneficial and can provide error estimates up to six\norders of magnitude tighter, i.e. more accurate. We also show that interval\nsubdivision, another commonly used technique to reduce over-approximations, has\nless benefit when computing relative errors directly, but it can help to\nalleviate the effects of the inherent issue of relative error estimates close\nto zero.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 11:24:49 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 14:23:50 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Izycheva", "Anastasiia", ""], ["Darulova", "Eva", ""]]}, {"id": "1707.02347", "submitter": "Dylan McCormick", "authors": "Dylan McCormick", "title": "Applying the Polyhedral Model to Tile Time Loops in Devito", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The run time of many scientific computation applications for numerical\nmethods is heavily dependent on just a few multi-dimensional loop nests. Since\nthese applications are often limited by memory bandwidth rather than\ncomputational resources they can benefit greatly from any optimizations which\ndecrease the run time of their loops by improving data reuse and thus reducing\nthe total memory traffic. Some of the most effective of these optimizations are\nnot suitable for development by hand or require advanced software engineering\nknowledge which is beyond the level of many researchers who are not specialists\nin code optimization. Several tools exist to automate the generation of\nhigh-performance code for numerical methods, such as Devito which produces code\nfor finite-difference approximations typically used in the seismic imaging\ndomain. We present a loop-tiling optimization which can be applied to\nDevito-generated loops and improves run time by up to 27.5%, and options for\nautomating this optimization in the Devito framework.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 14:51:26 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["McCormick", "Dylan", ""]]}, {"id": "1707.02466", "submitter": "Catalin Hritcu", "authors": "Danel Ahman, C\\'edric Fournet, Catalin Hritcu, Kenji Maillard, Aseem\n  Rastogi, Nikhil Swamy", "title": "Recalling a Witness: Foundations and Applications of Monotonic State", "comments": "POPL'18 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We provide a way to ease the verification of programs whose state evolves\nmonotonically. The main idea is that a property witnessed in a prior state can\nbe soundly recalled in the current state, provided (1) state evolves according\nto a given preorder, and (2) the property is preserved by this preorder. In\nmany scenarios, such monotonic reasoning yields concise modular proofs, saving\nthe need for explicit program invariants. We distill our approach into the\nmonotonic-state monad, a general yet compact interface for Hoare-style\nreasoning about monotonic state in a dependently typed language. We prove the\nsoundness of the monotonic-state monad and use it as a unified foundation for\nreasoning about monotonic state in the F* verification system. Based on this\nfoundation, we build libraries for various mutable data structures like\nmonotonic references and apply these libraries at scale to the verification of\nseveral distributed applications.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 16:48:54 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 14:12:36 GMT"}, {"version": "v3", "created": "Thu, 9 Nov 2017 09:50:56 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Ahman", "Danel", ""], ["Fournet", "C\u00e9dric", ""], ["Hritcu", "Catalin", ""], ["Maillard", "Kenji", ""], ["Rastogi", "Aseem", ""], ["Swamy", "Nikhil", ""]]}, {"id": "1707.02590", "submitter": "Hiun Kim", "authors": "Hiun Kim", "title": "Refinable Function : An Object-oriented Approach to Procedure Modularity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modularity is the fundamental aspect of modern software engineering, however\nmany advanced modularity techniques requires prospective technologies as part\nof development and operation process. In this paper, we present Refinable\nFunction, an object-oriented approach to advanced language-based, symmetric\nmodularity technique for the procedure. We conceptually compare Refinable\nFunction to existing technique to substantiate benefits of modularity can be\nimplemented in on well-established object-oriented language without compiler\nsupport. We introduce concepts of inheritance, encapsulation, and polymorphism\nof function for bringing object-orientation to procedure modularity and\ndescribe the design and implementation of Refinable Function in JavaScript to\nvalidate our approach to practical web application development. We introduce\nthe practical aspect of Refinable Function implementation by discussing\nconcerns of applying modularity on asynchronous processing. We tested and\nimplemented Refinable Function to substantiate its relevance to web application\ndevelopment and product line implementation.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 14:56:41 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Kim", "Hiun", ""]]}, {"id": "1707.02652", "submitter": "Carlos Martin", "authors": "Carlos Martin", "title": "Generation and analysis of lamplighter programs", "comments": "Added section 7 (sampling random programs) and section 8\n  (distribution of runtimes)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.FL cs.PL math.CO math.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a programming language based on the lamplighter group that uses\nonly composition and iteration as control structures. We derive generating\nfunctions and counting formulas for this language and special subsets of it,\nestablishing lower and upper bounds on the growth rate of semantically distinct\nprograms. Finally, we show how to sample random programs and analyze the\ndistribution of runtimes induced by such sampling.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 22:38:22 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 22:02:40 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Martin", "Carlos", ""]]}, {"id": "1707.02772", "submitter": "Steffen Smolka", "authors": "Steffen Smolka, Praveen Kumar, Nate Foster, Justin Hsu, David Kahn,\n  Dexter Kozen, Alexandra Silva", "title": "Probabilistic Program Equivalence for NetKAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of deciding whether two probabilistic programs are\nequivalent in Probabilistic NetKAT, a formal language for specifying and\nreasoning about the behavior of packet-switched networks. We show that the\nproblem is decidable for the history-free fragment of the language by\ndeveloping an effective decision procedure based on stochastic matrices. The\nmain challenge lies in reasoning about iteration, which we address by designing\nan encoding of the program semantics as a finite-state absorbing Markov chain,\nwhose limiting distribution can be computed exactly. In an extended case study\non a real-world data center network, we automatically verify various\nquantitative properties of interest, including resilience in the presence of\nfailures, by analyzing the Markov chain semantics.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 09:30:21 GMT"}, {"version": "v2", "created": "Sat, 24 Mar 2018 22:21:01 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Smolka", "Steffen", ""], ["Kumar", "Praveen", ""], ["Foster", "Nate", ""], ["Hsu", "Justin", ""], ["Kahn", "David", ""], ["Kozen", "Dexter", ""], ["Silva", "Alexandra", ""]]}, {"id": "1707.02894", "submitter": "Michael Greenberg", "authors": "Michael Greenberg, Ryan Beckett, Eric Campbell", "title": "Kleene Algebra Modulo Theories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kleene algebras with tests (KATs) offer sound, complete, and decidable\nequational reasoning about regularly structured programs. Since NetKAT\ndemonstrated how well various extensions of KATs apply to computer networks,\ninterest in KATs has increased greatly. Unfortunately, extending a KAT to a\nparticular domain by adding custom primitives, proving its equational theory\nsound and complete, and coming up with efficient automata-theoretic\nimplementations is still an expert's task.\n  We present a general framework for deriving KATs we call Kleene algebra\nmodulo theories: given primitives and notions of state, we can automatically\nderive a corresponding KAT's semantics, prove its equational theory sound and\ncomplete, and generate an automata-based implementation of equivalence\nchecking. Our framework is based on pushback, a way of specifying how\npredicates and actions interact, first used in Temporal NetKAT. We offer\nseveral case studies, including theories for bitvectors, increasing natural\nnumbers, unbounded sets and maps, temporal logic, and network protocols.\nFinally, we provide an OCaml implementation that closely matches the theory:\nwith only a few declarations, users can automatically derive an\nautomata-theoretic decision procedure for a KAT.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 15:00:32 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 15:59:46 GMT"}, {"version": "v3", "created": "Wed, 12 Jul 2017 16:55:02 GMT"}, {"version": "v4", "created": "Mon, 12 Jul 2021 14:23:32 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Greenberg", "Michael", ""], ["Beckett", "Ryan", ""], ["Campbell", "Eric", ""]]}, {"id": "1707.03555", "submitter": "Divyesh Unadkat", "authors": "Supratik Chakraborty, Ashutosh Gupta, Divyesh Unadkat", "title": "Verifying Array Manipulating Programs by Tiling", "comments": "Published in SAS 2017", "journal-ref": null, "doi": "10.1007/978-3-319-66706-5_21", "report-no": null, "categories": "cs.SE cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Formally verifying properties of programs that manipulate arrays in loops is\ncomputationally challenging. In this paper, we focus on a useful class of such\nprograms, and present a novel property-driven verification method that first\ninfers array access patterns in loops using simple heuristics, and then uses\nthis information to compositionally prove universally quantified assertions\nabout arrays. Specifically, we identify tiles of array accesses patterns in a\nloop, and use the tiling information to reduce the problem of checking a\nquantified assertion at the end of a loop to an inductive argument that checks\nonly a slice of the assertion for a single iteration of the loop body. We show\nthat this method can be extended to programs with sequentially composed loops\nand nested loops as well. We have implemented our method in a tool called\nTiler. Initial experiments show that Tiler outperforms several state-of-the-art\ntools on a suite of interesting benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 06:06:34 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 08:32:49 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Chakraborty", "Supratik", ""], ["Gupta", "Ashutosh", ""], ["Unadkat", "Divyesh", ""]]}, {"id": "1707.03668", "submitter": "EPTCS", "authors": "Herbert Wiklicky (1), Erik de Vink (2) ((1) Imperial College London\n  (2) Eindhoven University of Technology)", "title": "Proceedings 15th Workshop on Quantitative Aspects of Programming\n  Languages and Systems", "comments": null, "journal-ref": "EPTCS 250, 2017", "doi": "10.4204/EPTCS.250", "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume of the EPTCS contains the proceedings of the 15th international\nworkshop on Qualitative Aspects of Programming Languages and Systems, QAPL\n2017, held at April 23, 2017 in Uppsala, Sweden as a satellite event of ETAPS\n2017, the 20th European Joint Conferencec on Theory and Practice of Software.\nThe volume contains two invited contributions by Erika Abraham and by Andrea\nVandin as well as six technical papers selected by the QAPL 2017 program\ncommittee.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 12:14:20 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Wiklicky", "Herbert", ""], ["de Vink", "Erik", ""]]}, {"id": "1707.03762", "submitter": "Jeremy Siek", "authors": "Jeremy G. Siek", "title": "Revisiting Elementary Denotational Semantics", "comments": "25 pages, revision of POPL 2018 submission, now under submission to\n  ESOP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Operational semantics have been enormously successful, in large part due to\nits flexibility and simplicity, but they are not compositional. Denotational\nsemantics, on the other hand, are compositional but the lattice-theoretic\nmodels are complex and difficult to scale to large languages. However, there\nare elementary models of the $\\lambda$-calculus that are much less complex: by\nCoppo, Dezani-Ciancaglini, and Salle (1979), Engeler (1981), and Plotkin\n(1993).\n  This paper takes first steps toward answering the question: can elementary\nmodels be good for the day-to-day work of language specification,\nmechanization, and compiler correctness? The elementary models in the\nliterature are simple, but they are not as intuitive as they could be. To\nremedy this, we create a new model that represents functions literally as\nfinite graphs. Regarding mechanization, we give the first machine-checked proof\nof soundness and completeness of an elementary model with respect to an\noperational semantics. Regarding compiler correctness, we define a polyvariant\ninliner for the call-by-value $\\lambda$-calculus and prove that its output is\ncontextually equivalent to its input. Toward scaling elementary models to\nlarger languages, we formulate our semantics in a monadic style, give a\nsemantics for System F with general recursion, and mechanize the proof of type\nsoundness.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 15:28:23 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 21:42:21 GMT"}, {"version": "v3", "created": "Sun, 15 Oct 2017 21:20:37 GMT"}, {"version": "v4", "created": "Sat, 21 Oct 2017 03:18:02 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Siek", "Jeremy G.", ""]]}, {"id": "1707.04126", "submitter": "EPTCS", "authors": "Diego Latella (Consiglio Nazionale delle Ricerche - Istituto di\n  Scienza e Tecnologie dell'Informazione \"A. Faedo\"), Mieke Massink (Consiglio\n  Nazionale delle Ricerche - Istituto di Scienza e Tecnologie dell'Informazione\n  \"A. Faedo\")", "title": "Design and Optimisation of the FlyFast Front-end for Attribute-based\n  Coordination", "comments": "In Proceedings QAPL 2017, arXiv:1707.03668", "journal-ref": "EPTCS 250, 2017, pp. 92-110", "doi": "10.4204/EPTCS.250.6", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective Adaptive Systems (CAS) consist of a large number of interacting\nobjects. The design of such systems requires scalable analysis tools and\nmethods, which have necessarily to rely on some form of approximation of the\nsystem's actual behaviour. Promising techniques are those based on mean-field\napproximation. The FlyFast model-checker uses an on-the-fly algorithm for\nbounded PCTL model-checking of selected individual(s) in the context of very\nlarge populations whose global behaviour is approximated using deterministic\nlimit mean-field techniques. Recently, a front-end for FlyFast has been\nproposed which provides a modelling language, PiFF in the sequel, for the\nPredicate-based Interaction for FlyFast. In this paper we present details of\nPiFF design and an approach to state-space reduction based on probabilistic\nbisimulation for inhomogeneous DTMCs.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 13:52:50 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Latella", "Diego", "", "Consiglio Nazionale delle Ricerche - Istituto di\n  Scienza e Tecnologie dell'Informazione \"A. Faedo\""], ["Massink", "Mieke", "", "Consiglio\n  Nazionale delle Ricerche - Istituto di Scienza e Tecnologie dell'Informazione\n  \"A. Faedo\""]]}, {"id": "1707.04127", "submitter": "EPTCS", "authors": "Jacob Lidman (Chalmers University of Technology), Josef Svenningsson\n  (Chalmers University of Technology)", "title": "Bridging Static and Dynamic Program Analysis using Fuzzy Logic", "comments": "In Proceedings QAPL 2017, arXiv:1707.03668", "journal-ref": "EPTCS 250, 2017, pp. 111-126", "doi": "10.4204/EPTCS.250.7", "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static program analysis is used to summarize properties over all dynamic\nexecutions. In a unifying approach based on 3-valued logic properties are\neither assigned a definite value or unknown. But in summarizing a set of\nexecutions, a property is more accurately represented as being biased towards\ntrue, or towards false. Compilers use program analysis to determine benefit of\nan optimization. Since benefit (e.g., performance) is justified based on the\ncommon case understanding bias is essential in guiding the compiler.\nFurthermore, successful optimization also relies on understanding the quality\nof the information, i.e. the plausibility of the bias. If the quality of the\nstatic information is too low to form a decision we would like a mechanism that\nimproves dynamically.\n  We consider the problem of building such a reasoning framework and present\nthe fuzzy data-flow analysis. Our approach generalize previous work that use\n3-valued logic. We derive fuzzy extensions of data-flow analyses used by the\nlazy code motion optimization and unveil opportunities previous work would not\ndetect due to limited expressiveness. Furthermore we show how the results of\nour analysis can be used in an adaptive classifier that improve as the\napplication executes.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 13:53:07 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Lidman", "Jacob", "", "Chalmers University of Technology"], ["Svenningsson", "Josef", "", "Chalmers University of Technology"]]}, {"id": "1707.04148", "submitter": "Manos Koukoutos", "authors": "Manos Koukoutos, Mukund Raghothaman, Etienne Kneuss and Viktor Kuncak", "title": "On Repair with Probabilistic Attribute Grammars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program synthesis and repair have emerged as an exciting area of research,\ndriven by the potential for revolutionary advances in programmer productivity.\nAmong most promising ideas emerging for synthesis are syntax-driven search,\nprobabilistic models of code, and the use of input-output examples. Our paper\nshows how to combine these techniques and use them for program repair, which is\namong the most relevant applications of synthesis to general-purpose code. Our\napproach combines semantic specifications, in the form of pre- and\npost-conditions and input-output examples with syntactic specifications in the\nform of term grammars and AST-level statistics extracted from code corpora. We\nshow that synthesis in this framework can be viewed as an instance of graph\nsearch, permitting the use of well-understood families of techniques such as\nA*. We implement our algorithm in a framework for verification, synthesis and\nrepair of functional programs, demonstrating that our approach can repair\nprograms that are beyond the reach of previous tools.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 14:37:27 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Koukoutos", "Manos", ""], ["Raghothaman", "Mukund", ""], ["Kneuss", "Etienne", ""], ["Kuncak", "Viktor", ""]]}, {"id": "1707.04245", "submitter": "Chris Fawcett", "authors": "Chris Fawcett, Lars Kotthoff, Holger H. Hoos", "title": "Hot-Rodding the Browser Engine: Automatic Configuration of JavaScript\n  Compilers", "comments": "11 pages, long version of a poster presented at CGO 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern software systems in many application areas offer to the user a\nmultitude of parameters, switches and other customisation hooks. Humans tend to\nhave difficulties determining the best configurations for particular\napplications. Modern optimising compilers are an example of such software\nsystems; their many parameters need to be tuned for optimal performance, but\nare often left at the default values for convenience. In this work, we\nautomatically determine compiler parameter settings that result in optimised\nperformance for particular applications. Specifically, we apply a\nstate-of-the-art automated parameter configuration procedure based on\ncutting-edge machine learning and optimisation techniques to two prominent\nJavaScript compilers and demonstrate that significant performance improvements,\nmore than 35% in some cases, can be achieved over the default parameter\nsettings on a diverse set of benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 23:31:23 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Fawcett", "Chris", ""], ["Kotthoff", "Lars", ""], ["Hoos", "Holger H.", ""]]}, {"id": "1707.04314", "submitter": "Tom Rainforth", "authors": "Tom Rainforth, Tuan Anh Le, Jan-Willem van de Meent, Michael A.\n  Osborne, Frank Wood", "title": "Bayesian Optimization for Probabilistic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.PL stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first general purpose framework for marginal maximum a\nposteriori estimation of probabilistic program variables. By using a series of\ncode transformations, the evidence of any probabilistic program, and therefore\nof any graphical model, can be optimized with respect to an arbitrary subset of\nits sampled variables. To carry out this optimization, we develop the first\nBayesian optimization package to directly exploit the source code of its\ntarget, leading to innovations in problem-independent hyperpriors, unbounded\noptimization, and implicit constraint satisfaction; delivering significant\nperformance improvements over prominent existing packages. We present\napplications of our method to a number of tasks including engineering design\nand parameter optimization.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 20:49:29 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Rainforth", "Tom", ""], ["Le", "Tuan Anh", ""], ["van de Meent", "Jan-Willem", ""], ["Osborne", "Michael A.", ""], ["Wood", "Frank", ""]]}, {"id": "1707.04600", "submitter": "James Koppel", "authors": "James Koppel, Varot Premtoon, Armando Solar-Lezama", "title": "One Tool, Many Languages: Language-Parametric Transformation with\n  Incremental Parametric Syntax", "comments": null, "journal-ref": "Proc. ACM Program. Lang., Vol. 2, No. OOPSLA, Article 122.\n  Publication date: November 2018", "doi": "10.1145/3276492", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach for building source-to-source transformations that\ncan run on multiple programming languages, based on a new way of representing\nprograms called incremental parametric syntax. We implement this approach in\nHaskell in our Cubix system, and construct incremental parametric syntaxes for\nC, Java, JavaScript, Lua, and Python. We demonstrate a whole-program\nrefactoring tool that runs on all of them, along with three smaller\ntransformations that each run on several. Our evaluation shows that (1) once a\ntransformation is written, little work is required to configure it for a new\nlanguage (2) transformations built this way output readable code which preserve\nthe structure of the original, according to participants in our human study,\nand (3) our transformations can still handle language corner-cases, as\nvalidated on compiler test suites.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 18:26:41 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 22:50:00 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Koppel", "James", ""], ["Premtoon", "Varot", ""], ["Solar-Lezama", "Armando", ""]]}, {"id": "1707.04704", "submitter": "Ioanna Symeonidou", "authors": "Panos Rondogiannis and Ioanna Symeonidou", "title": "The Intricacies of 3-Valued Extensional Semantics for Higher-Order Logic\n  Programs", "comments": "Paper presented at the 33rd International Conference on Logic\n  Programming (ICLP 2017), Melbourne, Australia, August 28 to September 1,\n  2017. Under consideration for acceptance in TPLP. 16 pages (article) + 9\n  pages (appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In (Bezem 1999; Bezem 2001), M. Bezem defined an extensional semantics for\npositive higher-order logic programs. Recently, it was demonstrated in\n(Rondogiannis and Symeonidou 2016) that Bezem's technique can be extended to\nhigher-order logic programs with negation, retaining its extensional\nproperties, provided that it is interpreted under a logic with an infinite\nnumber of truth values. In (Rondogiannis and Symeonidou 2017) it was also\ndemonstrated that Bezem's technique, when extended under the stable model\nsemantics, does not in general lead to extensional stable models. In this paper\nwe consider the problem of extending Bezem's technique under the well-founded\nsemantics. We demonstrate that the well-founded extension fails to retain\nextensionality in the general case. On the positive side, we demonstrate that\nfor stratified higher-order logic programs, extensionality is indeed achieved.\nWe analyze the reasons of the failure of extensionality in the general case,\narguing that a three-valued setting can not distinguish between certain\npredicates that appear to have a different behaviour inside a program context,\nbut which happen to be identical as three-valued relations. The paper is under\nconsideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 07:52:28 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Rondogiannis", "Panos", ""], ["Symeonidou", "Ioanna", ""]]}, {"id": "1707.04724", "submitter": "Samer Abdallah", "authors": "Samer Abdallah", "title": "Memoisation: Purely, Left-recursively, and with (Continuation Passing)\n  Style", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memoisation, or tabling, is a well-known technique that yields large\nimprovements in the performance of some recursive computations. Tabled\nresolution in Prologs such as XSB and B-Prolog can transform so called\nleft-recursive predicates from non-terminating computations into finite and\nwell-behaved ones. In the functional programming literature, memoisation has\nusually been implemented in a way that does not handle left-recursion,\nrequiring supplementary mechanisms to prevent non-termination. A notable\nexception is Johnson's (1995) continuation passing approach in Scheme. This,\nhowever, relies on mutation of a memo table data structure and coding in\nexplicit continuation passing style. We show how Johnson's approach can be\nimplemented purely functionally in a modern, strongly typed functional language\n(OCaml), presented via a monadic interface that hides the implementation\ndetails, yet providing a way to return a compact represention of the memo\ntables at the end of the computation.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 11:03:49 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Abdallah", "Samer", ""]]}, {"id": "1707.04755", "submitter": "David Van Horn", "authors": "David Darais, Nicholas Labich, Phuc C. Nguyen, David Van Horn", "title": "Abstracting Definitional Interpreters", "comments": null, "journal-ref": "Proc. ACM Program. Lang. 1, ICFP, Article 12 (September 2017)", "doi": "10.1145/3110256", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this functional pearl, we examine the use of definitional interpreters as\na basis for abstract interpretation of higher-order programming languages. As\nit turns out, definitional interpreters, especially those written in monadic\nstyle, can provide a nice basis for a wide variety of collecting semantics,\nabstract interpretations, symbolic executions, and their intermixings.\n  But the real insight of this story is a replaying of an insight from\nReynold's landmark paper, Definitional Interpreters for Higher-Order\nProgramming Languages, in which he observes definitional interpreters enable\nthe defined-language to inherit properties of the defining-language. We show\nthe same holds true for definitional abstract interpreters. Remarkably, we\nobserve that abstract definitional interpreters can inherit the so-called\n\"pushdown control flow\" property, wherein function calls and returns are\nprecisely matched in the abstract semantics, simply by virtue of the function\ncall mechanism of the defining-language.\n  The first approaches to achieve this property for higher-order languages\nappeared within the last ten years, and have since been the subject of many\npapers. These approaches start from a state-machine semantics and uniformly\ninvolve significant technical engineering to recover the precision of pushdown\ncontrol flow. In contrast, starting from a definitional interpreter, the\npushdown control flow property is inherent in the meta-language and requires no\nfurther technical mechanism to achieve.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 16:04:51 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Darais", "David", ""], ["Labich", "Nicholas", ""], ["Nguyen", "Phuc C.", ""], ["Van Horn", "David", ""]]}, {"id": "1707.04984", "submitter": "Gabriel Scherer", "authors": "Gabriel Scherer and Max New and Nick Rioux and Amal Ahmed", "title": "FabULous Interoperability for ML and a Linear Language", "comments": "Published in Fossacs 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Instead of a monolithic programming language trying to cover all features of\ninterest, some programming systems are designed by combining together simpler\nlanguages that cooperate to cover the same feature space. This can improve\nusability by making each part simpler than the whole, but there is a risk of\nabstraction leaks from one language to another that would break expectations of\nthe users familiar with only one or some of the involved languages.\n  We propose a formal specification for what it means for a given language in a\nmulti-language system to be usable without leaks: it should embed into the\nmulti-language in a fully abstract way, that is, its contextual equivalence\nshould be unchanged in the larger system.\n  To demonstrate our proposed design principle and formal specification\ncriterion, we design a multi-language programming system that combines an\nML-like statically typed functional language and another language with linear\ntypes and linear state. Our goal is to cover a good part of the expressiveness\nof languages that mix functional programming and linear state (ownership), at\nonly a fraction of the complexity. We prove that the embedding of ML into the\nmulti-language system is fully abstract: functional programmers should not fear\nabstraction leaks. We show examples of combined programs demonstrating in-place\nmemory updates and safe resource handling, and an implementation extending\nOCaml with our linear language.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 02:50:52 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 14:47:04 GMT"}, {"version": "v3", "created": "Thu, 12 Apr 2018 15:27:42 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Scherer", "Gabriel", ""], ["New", "Max", ""], ["Rioux", "Nick", ""], ["Ahmed", "Amal", ""]]}, {"id": "1707.05599", "submitter": "Angel Cuenca Ortega", "authors": "Mar\\'ia Alpuente, Angel Cuenca-Ortega, Santiago Escobar, Julia\n  Sapi\\~na", "title": "Inspecting Maude Variants with GLINTS", "comments": "Paper presented at the 33nd International Conference on Logic\n  Programming (ICLP 2017), Melbourne, Australia, August 28 to September 1, 2017\n  15 pages, LaTeX, 7 PDF figures, 2 PNG figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces GLINTS, a graphical tool for exploring variant\nnarrowing computations in Maude. The most recent version of Maude, version\n2.7.1, provides quite sophisticated unification features, including\norder-sorted equational unification for convergent theories modulo axioms such\nas associativity, commutativity, and identity (ACU). This novel equational\nunification relies on built-in generation of the set of 'variants' of a term\n$t$, i.e., the canonical form of $t \\sigma$ for a computed substitution\n$\\sigma$. Variant generation relies on a novel narrowing strategy called\n'folding variant narrowing' that opens up new applications in formal reasoning,\ntheorem proving, testing, protocol analysis, and model checking, especially\nwhen the theory satisfies the 'finite variant property', i.e., there is a\nfinite number of most general variants for every term in the theory. However,\nvariant narrowing computations can be extremely involved and are simply\npresented in text format by Maude, often being too heavy to be debugged or even\nunderstood. The GLINTS system provides support for (i) determining whether a\ngiven theory satisfies the finite variant property, (ii) thoroughly exploring\nvariant narrowing computations, (iii) automatic checking of node 'embedding'\nand 'closedness' modulo axioms, and (iv) querying and inspecting selected parts\nof the variant trees. This paper is under consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 13:20:39 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Alpuente", "Mar\u00eda", ""], ["Cuenca-Ortega", "Angel", ""], ["Escobar", "Santiago", ""], ["Sapi\u00f1a", "Julia", ""]]}, {"id": "1707.05881", "submitter": "James Riely", "authors": "Alan Jeffrey and James Riely", "title": "On Thin Air Reads: Towards an Event Structures Model of Relaxed Memory", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 1 (March 29,\n  2019) lmcs:5327", "doi": "10.23638/LMCS-15(1:33)2019", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To model relaxed memory, we propose confusion-free event structures over an\nalphabet with a justification relation. Executions are modeled by justified\nconfigurations, where every read event has a justifying write event.\nJustification alone is too weak a criterion, since it allows cycles of the kind\nthat result in so-called thin-air reads. Acyclic justification forbids such\ncycles, but also invalidates event reorderings that result from compiler\noptimizations and dynamic instruction scheduling. We propose the notion of\nwell-justification, based on a game-like model, which strikes a middle ground.\n  We show that well-justified configurations satisfy the DRF theorem: in any\ndata-race free program, all well-justified configurations are sequentially\nconsistent. We also show that rely-guarantee reasoning is sound for\nwell-justified configurations, but not for justified configurations. For\nexample, well-justified configurations are type-safe.\n  Well-justification allows many, but not all reorderings performed by relaxed\nmemory. In particular, it fails to validate the commutation of independent\nreads. We discuss variations that may address these shortcomings.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 22:19:34 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 18:45:30 GMT"}, {"version": "v3", "created": "Tue, 11 Dec 2018 01:56:46 GMT"}, {"version": "v4", "created": "Tue, 8 Jan 2019 21:53:02 GMT"}, {"version": "v5", "created": "Wed, 27 Mar 2019 18:29:02 GMT"}, {"version": "v6", "created": "Tue, 4 Jun 2019 21:35:20 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Jeffrey", "Alan", ""], ["Riely", "James", ""]]}, {"id": "1707.05923", "submitter": "Sizhuo Zhang", "authors": "Sizhuo Zhang, Muralidaran Vijayaraghavan, Arvind", "title": "Weak Memory Models: Balancing Definitional Simplicity and Implementation\n  Flexibility", "comments": null, "journal-ref": "Parallel Architectures and Compilation Techniques (PACT), 2017\n  26th International Conference on, pp. 288-302. IEEE, 2017", "doi": "10.1109/PACT.2017.29", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The memory model for RISC-V, a newly developed open source ISA, has not been\nfinalized yet and thus, offers an opportunity to evaluate existing memory\nmodels. We believe RISC-V should not adopt the memory models of POWER or ARM,\nbecause their axiomatic and operational definitions are too complicated. We\npropose two new weak memory models: WMM and WMM-S, which balance definitional\nsimplicity and implementation flexibility differently. Both allow all\ninstruction reorderings except overtaking of loads by a store. We show that\nthis restriction has little impact on performance and it considerably\nsimplifies operational definitions. It also rules out the out-of-thin-air\nproblem that plagues many definitions. WMM is simple (it is similar to the\nAlpha memory model), but it disallows behaviors arising due to shared store\nbuffers and shared write-through caches (which are seen in POWER processors).\nWMM-S, on the other hand, is more complex and allows these behaviors. We give\nthe operational definitions of both models using Instantaneous Instruction\nExecution (I2E), which has been used in the definitions of SC and TSO. We also\nshow how both models can be implemented using conventional cache-coherent\nmemory systems and out-of-order processors, and encompasses the behaviors of\nmost known optimizations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 03:04:17 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 18:13:02 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Zhang", "Sizhuo", ""], ["Vijayaraghavan", "Muralidaran", ""], ["Arvind", "", ""]]}, {"id": "1707.06685", "submitter": "Dylan McDermott", "authors": "Ohad Kammar and Dylan McDermott", "title": "A monadic solution to the Cartwright-Felleisen-Wadler conjecture", "comments": "Talk proposal uploaded for archival purposes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a programming language, can we give a monadic denotational semantics\nthat is stable under language extension? Models containing only a single monad\nare not stable. Models based on type-and-effect systems, in which there is a\nmonad for every set of operations in the language, are. Cartwright and\nFelleisen, and Wadler, conjectured such monadic semantics can be generated. We\ndescribe a new general method of constructing stable models from standard\nmonadic models, based on factorizations of monad morphisms. We show that under\ncertain conditions factorizations induce a monad for every set of operations,\nand explain why the conditions usually hold. We also describe preliminary work\nusing fibrations for logical relations generated from these factorization\nsystems for proving the correctness of the resulting model.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 19:26:41 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Kammar", "Ohad", ""], ["McDermott", "Dylan", ""]]}, {"id": "1707.06901", "submitter": "Manuel Eberl", "authors": "Manuel Eberl, Johannes H\\\"olzl, Tobias Nipkow", "title": "A Verified Compiler for Probability Density Functions", "comments": "Presented at ESOP 2015", "journal-ref": null, "doi": "10.1007/978-3-662-46669-8_4", "report-no": null, "categories": "cs.PL cs.LO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bhat et al. developed an inductive compiler that computes density functions\nfor probability spaces described by programs in a simple probabilistic\nfunctional language. In this work, we implement such a compiler for a modified\nversion of this language within the theorem prover Isabelle and give a formal\nproof of its soundness w.r.t. the semantics of the source and target language.\nTogether with Isabelle's code generation for inductive predicates, this yields\na fully verified, executable density compiler. The proof is done in two steps,\nusing a standard refinement approach: first, an abstract compiler working with\nabstract functions modelled directly in the theorem prover's logic is defined\nand proven sound. Then, this compiler is refined to a concrete version that\nreturns a target-language expression.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 14:00:35 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Eberl", "Manuel", ""], ["H\u00f6lzl", "Johannes", ""], ["Nipkow", "Tobias", ""]]}, {"id": "1707.07845", "submitter": "Tue Haulund", "authors": "Tue Haulund", "title": "Design and Implementation of a Reversible Object-Oriented Programming\n  Language", "comments": "Master's Thesis, 110 pages, 55 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-level reversible programming languages are few and far between and in\ngeneral offer only rudimentary abstractions from the details of the underlying\nmachine. Modern programming languages offer a wide array of language constructs\nand paradigms to facilitate the design of abstract interfaces, but we currently\nhave a very limited understanding of the applicability of such features for\nreversible programming languages.\n  We introduce the first reversible object-oriented programming language,\nROOPL, with support for user-defined data types, class inheritance and\nsubtype-polymorphism. The language extends the design of existing reversible\nimperative languages and it allows for effective implementation on reversible\nmachines.\n  We provide a formalization of the language semantics, the type system and we\ndemonstrate the computational universality of the language by implementing a\nreversible Turing machine simulator. ROOPL statements are locally invertible at\nno extra cost to program size or computational complexity and the language\nprovides direct access to the inverse semantics of each class method.\n  We describe the techniques required for a garbage-free translation from ROOPL\nto the reversible assembly language PISA and provide a full implementation of\nsaid techniques. Our results indicate that core language features for\nobject-oriented programming carries over to the field of reversible computing\nin some capacity.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 08:19:05 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Haulund", "Tue", ""]]}, {"id": "1707.07872", "submitter": "Ki Yung Ahn", "authors": "Ki Yung Ahn", "title": "An Executable Specification of Typing Rules for Extensible Records based\n  on Row Polymorphism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Type inference is an application domain that is a natural fit for logic\nprogramming (LP). LP systems natively support unification, which serves as a\nbasic building block of typical type inference algorithms. In particular,\npolymorphic type inference in the Hindley--Milner type system (HM) can be\nsuccinctly specified and executed in Prolog. In our previous work, we have\ndemonstrated that more advanced features of parametric polymorphism beyond HM,\nsuch as type-constructor polymorphism and kind polymorphism, can be similarly\nspecified in Prolog. Here, we demonstrate a specification for records, which is\none of the most widely supported compound data structures in real-world\nprogramming languages, and discuss the advantages and limitations of Prolog as\na specification language for type systems. Record types are specified as\norder-irrelevant collections of named fields mapped to their corresponding\ntypes. In addition, an open-ended collection is used to support row\npolymorphism for record types to be extensible.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 09:36:10 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 02:57:44 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Ahn", "Ki Yung", ""]]}]