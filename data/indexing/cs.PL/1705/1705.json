[{"id": "1705.00133", "submitter": "Thorsten Wissmann", "authors": "Gilles Barthe, Thomas Espitau, Justin Hsu, Tetsuya Sato, Pierre-Yves\n  Strub", "title": "Relational $\\star$-Liftings for Differential Privacy", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 15, Issue 4 (December\n  19, 2019) lmcs:5989", "doi": "10.23638/LMCS-15(4:18)2019", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent developments in formal verification have identified approximate\nliftings (also known as approximate couplings) as a clean, compositional\nabstraction for proving differential privacy. This construction can be defined\nin two styles. Earlier definitions require the existence of one or more witness\ndistributions, while a recent definition by Sato uses universal quantification\nover all sets of samples. These notions have each have their own strengths: the\nuniversal version is more general than the existential ones, while existential\nliftings are known to satisfy more precise composition principles.\n  We propose a novel, existential version of approximate lifting, called\n$\\star$-lifting, and show that it is equivalent to Sato's construction for\ndiscrete probability measures. Our work unifies all known notions of\napproximate lifting, yielding cleaner properties, more general constructions,\nand more precise composition theorems for both styles of lifting, enabling\nricher proofs of differential privacy. We also clarify the relation between\nexisting definitions of approximate lifting, and consider more general\napproximate liftings based on $f$-divergences.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 05:35:06 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 03:22:35 GMT"}, {"version": "v3", "created": "Tue, 11 Jul 2017 16:18:35 GMT"}, {"version": "v4", "created": "Wed, 14 Mar 2018 23:19:47 GMT"}, {"version": "v5", "created": "Mon, 25 Feb 2019 18:25:01 GMT"}, {"version": "v6", "created": "Tue, 26 Feb 2019 19:59:29 GMT"}, {"version": "v7", "created": "Wed, 24 Jul 2019 16:34:40 GMT"}, {"version": "v8", "created": "Mon, 5 Aug 2019 15:54:52 GMT"}, {"version": "v9", "created": "Wed, 18 Dec 2019 18:45:10 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Barthe", "Gilles", ""], ["Espitau", "Thomas", ""], ["Hsu", "Justin", ""], ["Sato", "Tetsuya", ""], ["Strub", "Pierre-Yves", ""]]}, {"id": "1705.00314", "submitter": "Hongfei Fu", "authors": "Krishnendu Chatterjee, Hongfei Fu, Aniket Murhekar", "title": "Automated Recurrence Analysis for Almost-Linear Expected-Runtime Bounds", "comments": "41 pages, Full Version to CAV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of developing automated techniques for solving\nrecurrence relations to aid the expected-runtime analysis of programs. Several\nclassical textbook algorithms have quite efficient expected-runtime complexity,\nwhereas the corresponding worst-case bounds are either inefficient (e.g.,\nQUICK-SORT), or completely ineffective (e.g., COUPON-COLLECTOR). Since the main\nfocus of expected-runtime analysis is to obtain efficient bounds, we consider\nbounds that are either logarithmic, linear, or almost-linear ($\\mathcal{O}(\\log\nn)$, $\\mathcal{O}(n)$, $\\mathcal{O}(n\\cdot\\log n)$, respectively, where n\nrepresents the input size). Our main contribution is an efficient (simple\nlinear-time algorithm) sound approach for deriving such expected-runtime bounds\nfor the analysis of recurrence relations induced by randomized algorithms. Our\napproach can infer the asymptotically optimal expected-runtime bounds for\nrecurrences of classical randomized algorithms, including RANDOMIZED-SEARCH,\nQUICK-SORT, QUICK-SELECT, COUPONCOLLECTOR, where the worst-case bounds are\neither inefficient (such as linear as compared to logarithmic of\nexpected-runtime, or quadratic as compared to linear or almost-linear of\nexpected-runtime), or ineffective. We have implemented our approach, and the\nexperimental results show that we obtain the bounds efficiently for the\nrecurrences of various classical algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 13:30:01 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Fu", "Hongfei", ""], ["Murhekar", "Aniket", ""]]}, {"id": "1705.00317", "submitter": "Hongfei Fu", "authors": "Krishnendu Chatterjee, Hongfei Fu, Amir Kafshdar Goharshady", "title": "Non-polynomial Worst-Case Analysis of Recursive Programs", "comments": "54 Pages, Full Version to CAV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of developing efficient approaches for proving\nworst-case bounds of non-deterministic recursive programs. Ranking functions\nare sound and complete for proving termination and worst-case bounds of\nnonrecursive programs. First, we apply ranking functions to recursion,\nresulting in measure functions. We show that measure functions provide a sound\nand complete approach to prove worst-case bounds of non-deterministic recursive\nprograms. Our second contribution is the synthesis of measure functions in\nnonpolynomial forms. We show that non-polynomial measure functions with\nlogarithm and exponentiation can be synthesized through abstraction of\nlogarithmic or exponentiation terms, Farkas' Lemma, and Handelman's Theorem\nusing linear programming. While previous methods obtain worst-case polynomial\nbounds, our approach can synthesize bounds of the form $\\mathcal{O}(n\\log n)$\nas well as $\\mathcal{O}(n^r)$ where $r$ is not an integer. We present\nexperimental results to demonstrate that our approach can obtain efficiently\nworst-case bounds of classical recursive algorithms such as (i) Merge-Sort, the\ndivide-and-conquer algorithm for the Closest-Pair problem, where we obtain\n$\\mathcal{O}(n \\log n)$ worst-case bound, and (ii) Karatsuba's algorithm for\npolynomial multiplication and Strassen's algorithm for matrix multiplication,\nwhere we obtain $\\mathcal{O}(n^r)$ bound such that $r$ is not an integer and\nclose to the best-known bounds for the respective algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 14:09:14 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Fu", "Hongfei", ""], ["Goharshady", "Amir Kafshdar", ""]]}, {"id": "1705.00556", "submitter": "Jose Zalacain", "authors": "Jos\\'e E. Zalacain Llanes", "title": "Mapping Objects to Persistent Predicates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Logic Programming through Prolog has been widely used for supply\npersistence in many systems that need store knowledge. Some implementations of\nProlog Programming Language used for supply persistence have bidirectional\ninterfaces with other programming languages over all with Object Oriented\nPrograming Languages. In present days is missing tools and frameworks for the\nsystems development that use logic predicate persistence in easy and agile\nform. More specifically an object oriented and logic persistence provider is\nneed in present days that allow the object manipulation in main memory and the\npersistence for this objects have a Logic Programming predicates aspect. The\npresent work introduce an object-prolog declarative mappings alternative to\nsupport by an object oriented and logic persistence provider. The proposed\nalternative consists in a correspondence of the Logic Programming predicates\nwith an Object Oriented approach, where for each element of the Logic\nProgramming one Object Oriented element makes to reciprocate. The Object\nOriented representation of Logic Programming predicates offers facility of\nmanipulation on the elements that compose a knowledge.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 15:01:10 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Llanes", "Jos\u00e9 E. Zalacain", ""]]}, {"id": "1705.00595", "submitter": "Marcelo Sousa", "authors": "Marcelo Sousa, C\\'esar Rodr\\'iguez, Vijay D'Silva, Daniel Kroening", "title": "Abstract Interpretation with Unfoldings", "comments": "Extended version of the paper (with the same title and authors) to\n  appear at CAV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and evaluate a technique for computing path-sensitive interference\nconditions during abstract interpretation of concurrent programs. In lieu of\nfixed point computation, we use prime event structures to compactly represent\ncausal dependence and interference between sequences of transformers. Our main\ncontribution is an unfolding algorithm that uses a new notion of independence\nto avoid redundant transformer application, thread-local fixed points to reduce\nthe size of the unfolding, and a novel cutoff criterion based on subsumption to\nguarantee termination of the analysis. Our experiments show that the abstract\nunfolding produces an order of magnitude fewer false alarms than a mature\nabstract interpreter, while being several orders of magnitude faster than\nsolver-based tools that have the same precision.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 17:13:29 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Sousa", "Marcelo", ""], ["Rodr\u00edguez", "C\u00e9sar", ""], ["D'Silva", "Vijay", ""], ["Kroening", "Daniel", ""]]}, {"id": "1705.00959", "submitter": "Hasan Jamil", "authors": "Hasan M. Jamil", "title": "Smart Assessment of and Tutoring for Computational Thinking MOOC\n  Assignments using MindReader", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major hurdles toward automatic semantic understanding of computer\nprograms is the lack of knowledge about what constitutes functional equivalence\nof code segments. We postulate that a sound knowledgebase can be used to\ndeductively understand code segments in a hierarchical fashion by first\nde-constructing a code and then reconstructing it from elementary knowledge and\nequivalence rules of elementary code segments. The approach can also be\nengineered to produce computable programs from conceptual and abstract\nalgorithms as an inverse function. In this paper, we introduce the core idea\nbehind the MindReader online assessment system that is able to understand a\nwide variety of elementary algorithms students learn in their entry level\nprogramming classes such as Java, C++ and Python. The MindReader system is able\nto assess student assignments and guide them how to develop correct and better\ncode in real time without human assistance.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 20:21:11 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Jamil", "Hasan M.", ""]]}, {"id": "1705.00961", "submitter": "EPTCS", "authors": "Bernard van Gastel (Open University NL and Radboud University,\n  Nijmegen NL), Marko van Eekelen (Open University NL and Radboud University,\n  Nijmegen NL)", "title": "Towards Practical, Precise and Parametric Energy Analysis of IT\n  Controlled Systems", "comments": "In Proceedings DICE-FOPARA 2017, arXiv:1704.05169", "journal-ref": "EPTCS 248, 2017, pp. 24-37", "doi": "10.4204/EPTCS.248.7", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy consumption analysis of IT-controlled systems can play a major role in\nminimising the overall energy consumption of such IT systems, during the\ndevelopment phase, or for optimisation in the field. Recently, a precise energy\nanalysis was developed, with the property of being parametric in the hardware.\nIn principle, this creates the opportunity to analyse which is the best\nsoftware implementation for given hardware, or the other way around: choose the\nbest hardware for a given algorithm.\n  The precise analysis was introduced for a very limited language: ECA. In this\npaper, several important steps are taken towards practical energy analysis. The\nECA language is extended with common programming language features. The\napplication domain is further explored, and threats to the validity are\nidentified and discussed. Altogether, this constitutes an important step\ntowards analysing energy consumption of IT-controlled systems in practice.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 04:28:56 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["van Gastel", "Bernard", "", "Open University NL and Radboud University,\n  Nijmegen NL"], ["van Eekelen", "Marko", "", "Open University NL and Radboud University,\n  Nijmegen NL"]]}, {"id": "1705.01225", "submitter": "EPTCS", "authors": "Shilpi Goel (The University of Texas at Austin)", "title": "The x86isa Books: Features, Usage, and Future Plans", "comments": "In Proceedings ACL2Workshop 2017, arXiv:1705.00766", "journal-ref": "EPTCS 249, 2017, pp. 1-17", "doi": "10.4204/EPTCS.249.1", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The x86isa library, incorporated in the ACL2 community books project,\nprovides a formal model of the x86 instruction-set architecture and supports\nreasoning about x86 machine-code programs. However, analyzing x86 programs can\nbe daunting -- even for those familiar with program verification, in part due\nto the complexity of the x86 ISA. Furthermore, the x86isa library is a large\nframework, and using and/or contributing to it may not seem straightforward. We\npresent some typical ways of working with the x86isa library, and describe some\nof its salient features that can make the analysis of x86 machine-code programs\nless arduous. We also discuss some capabilities that are currently missing from\nthese books -- we hope that this will encourage the community to get involved\nin this project.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 01:48:28 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Goel", "Shilpi", "", "The University of Texas at Austin"]]}, {"id": "1705.01228", "submitter": "EPTCS", "authors": "Alessandro Coglio (Kestrel Institute), Matt Kaufmann (Department of\n  Computer Science, The University of Texas at Austin), Eric W. Smith (Kestrel\n  Institute)", "title": "A Versatile, Sound Tool for Simplifying Definitions", "comments": "In Proceedings ACL2Workshop 2017, arXiv:1705.00766", "journal-ref": "EPTCS 249, 2017, pp. 61-77", "doi": "10.4204/EPTCS.249.5", "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a tool, simplify-defun, that transforms the definition of a given\nfunction into a simplified definition of a new function, providing a proof\nchecked by ACL2 that the old and new functions are equivalent. When appropriate\nit also generates termination and guard proofs for the new function. We explain\nhow the tool is engineered so that these proofs will succeed. Examples\nillustrate its utility, in particular for program transformation in synthesis\nand verification.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 01:49:29 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Coglio", "Alessandro", "", "Kestrel Institute"], ["Kaufmann", "Matt", "", "Department of\n  Computer Science, The University of Texas at Austin"], ["Smith", "Eric W.", "", "Kestrel\n  Institute"]]}, {"id": "1705.01522", "submitter": "Santosh Nagarakatte", "authors": "Adarsh Yoga and Santosh Nagarakatte", "title": "A Fast Causal Profiler for Task Parallel Programs", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": "Rutgers CS Technical Report: DCS-TR-728", "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes TASKPROF, a profiler that identifies parallelism\nbottlenecks in task parallel programs. It leverages the structure of a task\nparallel execution to perform fine-grained attribution of work to various parts\nof the program. TASKPROF's use of hardware performance counters to perform\nfine-grained measurements minimizes perturbation. TASKPROF's profile execution\nruns in parallel using multi-cores. TASKPROF's causal profile enables users to\nestimate improvements in parallelism when a region of code is optimized even\nwhen concrete optimizations are not yet known. We have used TASKPROF to isolate\nparallelism bottlenecks in twenty three applications that use the Intel\nThreading Building Blocks library. We have designed parallelization techniques\nin five applications to in- crease parallelism by an order of magnitude using\nTASKPROF. Our user study indicates that developers are able to isolate\nperformance bottlenecks with ease using TASKPROF.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 17:37:52 GMT"}, {"version": "v2", "created": "Sat, 27 May 2017 14:45:23 GMT"}, {"version": "v3", "created": "Sun, 2 Jul 2017 05:10:49 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Yoga", "Adarsh", ""], ["Nagarakatte", "Santosh", ""]]}, {"id": "1705.01629", "submitter": "Marco Aldinucci", "authors": "Maurizio Drocco and Claudia Misale and Guy Tremblay and Marco\n  Aldinucci", "title": "A Formal Semantics for Data Analytics Pipelines", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we present a new programming model based on Pipelines and\nOperators, which are the building blocks of programs written in PiCo, a DSL for\nData Analytics Pipelines. In the model we propose, we use the term Pipeline to\ndenote a workflow that processes data collections -- rather than a\ncomputational process -- as is common in the data processing community. The\nnovelty with respect to other frameworks is that all PiCo operators are\npolymorphic with respect to data types. This makes it possible to 1) re-use the\nsame algorithms and pipelines on different data models (e.g., streams, lists,\nsets, etc); 2) reuse the same operators in different contexts, and 3) update\noperators without affecting the calling context, i.e., the previous and\nfollowing stages in the pipeline. Notice that in other mainstream frameworks,\nsuch as Spark, the update of a pipeline by changing a transformation with\nanother is not necessarily trivial, since it may require the development of an\ninput and output proxy to adapt the new transformation for the calling context.\nIn the same line, we provide a formal framework (i.e., typing and semantics)\nthat characterizes programs from the perspective of how they transform the data\nstructures they process -- rather than the computational processes they\nrepresent. This approach allows to reason about programs at an abstract level,\nwithout taking into account any aspect from the underlying execution model or\nimplementation.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 21:14:13 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Drocco", "Maurizio", ""], ["Misale", "Claudia", ""], ["Tremblay", "Guy", ""], ["Aldinucci", "Marco", ""]]}, {"id": "1705.02171", "submitter": "Ivaylo Hristakiev", "authors": "Ivaylo Hristakiev and Detlef Plump", "title": "A Unification Algorithm for GP 2 (Long Version)", "comments": "This paper is an extended version of the workshop paper \"A\n  Unification Algorithm for GP 2\", published in Graph Computation Models (GCM\n  2014), Revised Selected Papers, Electronic Communications of the EASST,\n  volume 71, 2015, doi: 10.14279/tuj.eceasst.71.1002", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graph programming language GP 2 allows to apply sets of rule schemata (or\n\"attributed\" rules) non-deterministically. To analyse conflicts of programs\nstatically, graphs labelled with expressions are overlayed to construct\ncritical pairs of rule applications. Each overlay induces a system of equations\nwhose solutions represent different conflicts. We present a rule-based\nunification algorithm for GP expressions that is terminating, sound and\ncomplete. For every input equation, the algorithm generates a finite set of\nsubstitutions. Soundness means that each of these substitutions solves the\ninput equation. Since GP labels are lists constructed by concatenation,\nunification modulo associativity and unit law is required. This problem, which\nis also known as word unification, is infinitary in general but becomes\nfinitary due to GP's rule schema syntax and the assumption that rule schemata\nare left-linear. Our unification algorithm is complete in that every solution\nof an input equation is an instance of some substitution in the generated set.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 11:18:46 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Hristakiev", "Ivaylo", ""], ["Plump", "Detlef", ""]]}, {"id": "1705.02264", "submitter": "Colin Gordon", "authors": "Colin S. Gordon", "title": "A Generic Approach to Flow-Sensitive Polymorphic Effects (Extended\n  Version)", "comments": "Extended version with proofs for a paper to appear at ECOOP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effect systems are lightweight extensions to type systems that can verify a\nwide range of important properties with modest developer burden. But our\ngeneral understanding of effect systems is limited primarily to systems where\nthe order of effects is irrelevant. Understanding such systems in terms of a\nlattice of effects grounds understanding of the essential issues, and provides\nguidance when designing new effect systems. By contrast, sequential effect\nsystems --- where the order of effects is important --- lack a clear algebraic\ncharacterization.\n  We derive an algebraic characterization from the shape of prior concrete\nsequential effect systems. We present an abstract polymorphic effect system\nwith singleton effects parameterized by an effect quantale --- an algebraic\nstructure with well-defined properties that can model a range of existing\norder-sensitive effect systems. We define effect quantales, derive useful\nproperties, and show how they cleanly model a variety of known sequential\neffect systems. We show that effect quantales provide a free, general notion of\niterating a sequential effect, and that for systems we consider the derived\niteration agrees with the manually designed iteration operators in prior work.\nIdentifying and applying the right algebraic structure led us to subtle\ninsights into the design of order-sensitive effect systems, which provides\nguidance on non-obvious points of designing order-sensitive effect systems.\nEffect quantales have clear relationships to the recent category theoretic work\non order-sensitive effect systems, but are explained without recourse to\ncategory theory. In addition, our derived iteration construct should generalize\nto these semantic structures, addressing limitations of that work.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 15:30:44 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Gordon", "Colin S.", ""]]}, {"id": "1705.02861", "submitter": "Mauricio Toro", "authors": "Mauricio Toro, Myriam Desainte-Catherine", "title": "Concurrent Constraint Conditional-Branching Timed Interactive Scores", "comments": "Extended version of paper accepted in the Sound and Music Conference\n  2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.MM cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia scenarios have multimedia content and interactive events\nassociated with computer programs. Interactive Scores (IS) is a formalism to\nrepresent such scenarios by temporal objects, temporal relations (TRs) and\ninteractive events. IS describe TRs, but IS cannot represent TRs together with\nconditional branching. We propose a model for conditional branching timed IS in\nthe Non-deterministic Timed Concurrent Constraint (ntcc) calculus. We ran a\nprototype of our model in Ntccrt (a real-time capable interpreter for ntcc) and\nthe response time was acceptable for real-time interaction. An advantage of\nntcc over Max/MSP or Petri Nets is that conditions and global constraints are\nrepresented declaratively.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 22:36:44 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Toro", "Mauricio", ""], ["Desainte-Catherine", "Myriam", ""]]}, {"id": "1705.03110", "submitter": "William Harris", "authors": "Qi Zhou, David Heath, William Harris", "title": "Completely Automated Equivalence Proofs", "comments": "24 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verifying partial (i.e., termination-insensitive) equivalence of programs has\nsignificant practical applications in software development and education.\nConventional equivalence verifiers typically rely on a combination of given\nrelational summaries and suggested synchronization points; such information can\nbe extremely difficult for programmers without a background in formal methods\nto provide for pairs of programs with dissimilar logic.\n  In this work, we propose a completely automated verifier for determining\npartial equivalence, named Pequod. Pequod automatically synthesizes expressive\nproofs of equivalence conventionally only achievable via careful, manual\nconstructions of product programs To do so, Pequod syntheses relational proofs\nfor selected pairs of program paths and combines the per-path relational proofs\nto synthesize relational program invariants. To evaluate Pequod, we implemented\nit as a tool that targets Java Virtual Machine bytecode and applied it to\nverify the equivalence of hundreds of pairs of solutions submitted by students\nfor problems hosted on popular online coding platforms, most of which could not\nbe verified by existing techniques.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 22:36:19 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Zhou", "Qi", ""], ["Heath", "David", ""], ["Harris", "William", ""]]}, {"id": "1705.03701", "submitter": "Sebastian Wolff", "authors": "Luk\\'a\\v{s} Hol\\'ik, Roland Meyer, Tom\\'a\\v{s} Vojnar, and Sebastian\n  Wolff", "title": "Effect Summaries for Thread-Modular Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel guess-and-check principle to increase the efficiency of\nthread-modular verification of lock-free data structures. We build on a\nheuristic that guesses candidates for stateless effect summaries of programs by\nsearching the code for instances of a copy-and-check programming idiom common\nin lock-free data structures. These candidate summaries are used to compute the\ninterference among threads in linear time. Since a candidate summary need not\nbe a sound effect summary, we show how to fully automatically check whether the\nprecision of candidate summaries is sufficient. We can thus perform sound\nverification despite relying on an unsound heuristic. We have implemented our\napproach and found it up to two orders of magnitude faster than existing ones.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 11:17:57 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Hol\u00edk", "Luk\u00e1\u0161", ""], ["Meyer", "Roland", ""], ["Vojnar", "Tom\u00e1\u0161", ""], ["Wolff", "Sebastian", ""]]}, {"id": "1705.03754", "submitter": "Christoph Matheja", "authors": "Hannah Arndt, Christina Jansen, Christoph Matheja, Thomas Noll", "title": "Graph-Based Shape Analysis Beyond Context-Freeness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a shape analysis for reasoning about relational properties of data\nstructures. Both the concrete and the abstract domain are represented by\nhypergraphs. The analysis is parameterized by user-supplied indexed graph\ngrammars to guide concretization and abstraction. This novel extension of\ncontext-free graph grammars is powerful enough to model complex data structures\nsuch as balanced binary trees with parent pointers, while preserving most\ndesirable properties of context-free graph grammars. One strength of our\nanalysis is that no artifacts apart from grammars are required from the user;\nit thus offers a high degree of automation. We implemented our analysis and\nsuccessfully applied it to various programs manipulating AVL trees,\n(doubly-linked) lists, and combinations of both.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 13:26:31 GMT"}, {"version": "v2", "created": "Mon, 23 Oct 2017 12:40:04 GMT"}, {"version": "v3", "created": "Thu, 19 Apr 2018 12:55:01 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Arndt", "Hannah", ""], ["Jansen", "Christina", ""], ["Matheja", "Christoph", ""], ["Noll", "Thomas", ""]]}, {"id": "1705.04680", "submitter": "Ekaterina Komendantskaya Dr", "authors": "Ekaterina Komendantskaya and Jonathan Heras", "title": "Proof Mining with Dependent Types", "comments": "Accepted at CICM'17, Edinburgh, 17-21 July 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several approaches exist to data-mining big corpora of formal proofs. Some of\nthese approaches are based on statistical machine learning, and some -- on\ntheory exploration. However, most are developed for either untyped or\nsimply-typed theorem provers. In this paper, we present a method that combines\nstatistical data mining and theory exploration in order to analyse and automate\nproofs in dependently typed language of Coq.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 17:59:41 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 22:00:25 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Komendantskaya", "Ekaterina", ""], ["Heras", "Jonathan", ""]]}, {"id": "1705.05137", "submitter": "Jun Inoue", "authors": "Jun Inoue and Yoriyuki Yamagata", "title": "Operational Semantics of Process Monitors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CSPe is a specification language for runtime monitors that can directly\nexpress concurrency in a bottom-up manner that composes the system from\nsimpler, interacting components. It includes constructs to explicitly flag\nfailures to the monitor, which unlike deadlocks and livelocks in conventional\nprocess algebras, propagate globally and aborts the whole system's execution.\nAlthough CSPe has a trace semantics along with an implementation demonstrating\nacceptable performance, it lacks an operational semantics. An operational\nsemantics is not only more accessible than trace semantics but also\nindispensable for ensuring the correctness of the implementation. Furthermore,\na process algebra like CSPe admits multiple denotational semantics appropriate\nfor different purposes, and an operational semantics is the basis for\njustifying such semantics' integrity and relevance. In this paper, we develop\nan SOS-style operational semantics for CSPe, which properly accounts for\nexplicit failures and will serve as a basis for further study of its\nproperties, its optimization, and its use in runtime verification.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 09:44:50 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Inoue", "Jun", ""], ["Yamagata", "Yoriyuki", ""]]}, {"id": "1705.05828", "submitter": "Edlira Kuci", "authors": "Edlira Kuci, Sebastian Erdweg, Oliver Bra\\v{c}evac, Andi Bejleri, and\n  Mira Mezini", "title": "A Co-contextual Type Checker for Featherweight Java (incl. Proofs)", "comments": "54 pages, 10 figures, ECOOP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses compositional and incremental type checking for\nobject-oriented programming languages. Recent work achieved incremental type\nchecking for structurally typed functional languages through co-contextual\ntyping rules, a constraint-based formulation that removes any context\ndependency for expression typings. However, that work does not cover key\nfeatures of object-oriented languages: Subtype polymorphism, nominal typing,\nand implementation inheritance. Type checkers encode these features in the form\nof class tables, an additional form of typing context inhibiting\nincrementalization. In the present work, we demonstrate that an appropriate\nco-contextual notion to class tables exists, paving the way to efficient\nincremental type checkers for object-oriented languages. This yields a novel\nformulation of Igarashi et al.'s Featherweight Java (FJ) type system, where we\nreplace class tables by the dual concept of class table requirements and class\ntable operations by dual operations on class table requirements. We prove the\nequivalence of FJ's type system and our co-contextual formulation. Based on our\nformulation, we implemented an incremental FJ type checker and compared its\nperformance against javac on a number of realistic example programs.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 17:59:40 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 12:48:52 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Kuci", "Edlira", ""], ["Erdweg", "Sebastian", ""], ["Bra\u010devac", "Oliver", ""], ["Bejleri", "Andi", ""], ["Mezini", "Mira", ""]]}, {"id": "1705.05937", "submitter": "Robert O'Callahan", "authors": "Robert O'Callahan, Chris Jones, Nathan Froyd, Kyle Huey, Albert Noll,\n  Nimrod Partush", "title": "Engineering Record And Replay For Deployability: Extended Technical\n  Report", "comments": "This extended technical report is based on our previous arXiv paper\n  arXiv:1610.02144 but contains much deeper technical detail and a \"lessons\n  learned\" section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to record and replay program executions with low overhead enables\nmany applications, such as reverse-execution debugging, debugging of\nhard-to-reproduce test failures, and \"black box\" forensic analysis of failures\nin deployed systems. Existing record-and-replay approaches limit deployability\nby recording an entire virtual machine (heavyweight), modifying the OS kernel\n(adding deployment and maintenance costs), requiring pervasive code\ninstrumentation (imposing significant performance and complexity overhead), or\nmodifying compilers and runtime systems (limiting generality). We investigated\nwhether it is possible to build a practical record-and-replay system avoiding\nall these issues. The answer turns out to be yes - if the CPU and operating\nsystem meet certain non-obvious constraints. Fortunately modern Intel CPUs,\nLinux kernels and user-space frameworks do meet these constraints, although\nthis has only become true recently. With some novel optimizations, our system\n'rr' records and replays real-world low-parallelism workloads with low\noverhead, with an entirely user-space implementation, using stock hardware,\ncompilers, runtimes and operating systems. \"rr\" forms the basis of an\nopen-source reverse-execution debugger seeing significant use in practice. We\npresent the design and implementation of 'rr', describe its performance on a\nvariety of workloads, and identify constraints on hardware and operating system\ndesign required to support our approach.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 22:00:00 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["O'Callahan", "Robert", ""], ["Jones", "Chris", ""], ["Froyd", "Nathan", ""], ["Huey", "Kyle", ""], ["Noll", "Albert", ""], ["Partush", "Nimrod", ""]]}, {"id": "1705.06158", "submitter": "Sizhuo Zhang", "authors": "Sizhuo Zhang, Muralidaran Vijayaraghavan, Arvind", "title": "An Operational Framework for Specifying Memory Models using\n  Instantaneous Instruction Execution", "comments": "arXiv admin note: text overlap with arXiv:1606.05416", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been great progress recently in formally specifying the memory\nmodel of microprocessors like ARM and POWER. These specifications are, however,\ntoo complicated for reasoning about program behaviors, verifying compilers\netc., because they involve microarchitectural details like the reorder buffer\n(ROB), partial and speculative execution, instruction replay on speculation\nfailure, etc. In this paper we present a new Instantaneous Instruction\nExecution (I2E) framework which allows us to specify weak memory models in the\nsame style as SC and TSO. Each instruction in I2E is executed instantaneously\nand in-order such that the state of the processor is always correct. The effect\nof instruction reordering is captured by the way data is moved between the\nprocessors and the memory non-deterministically, using three conceptual\ndevices: invalidation buffers, timestamps and dynamic store buffers. We prove\nthat I2E models capture the behaviors of modern microarchitectures and\ncache-coherent memory systems accurately, thus eliminating the need to think\nabout microarchitectural details.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 15:35:32 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Zhang", "Sizhuo", ""], ["Vijayaraghavan", "Muralidaran", ""], ["Arvind", "", ""]]}, {"id": "1705.06216", "submitter": "Steven Ramsay", "authors": "Toby Cathcart Burn, C.-H. Luke Ong and Steven J. Ramsay", "title": "Higher-Order Constrained Horn Clauses and Refinement Types", "comments": "Completely rewritten Section 4.3 on the correspondence between models\n  to improve the clarity of the exposition. Various other minor improvements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in automated verification of higher-order\nfunctional programs, we develop a notion of constrained Horn clauses in\nhigher-order logic and a decision problem concerning their satisfiability. We\nshow that, although satisfiable systems of higher-order clauses do not\ngenerally have least models, there is a notion of canonical model obtained\nthrough a reduction to a problem concerning a kind of monotone logic program.\nFollowing work in higher-order program verification, we develop a refinement\ntype system in order to reason about and automate the search for models. This\nprovides a sound but incomplete method for solving the decision problem.\nFinally, we show that an extension of the decision problem in which refinement\ntypes are used directly as guards on existential quantifiers can be reduced to\nthe original problem. This result can be used to show that properties of\nhigher-order functions that are definable using refinement types are also\nexpressible using higher-order constrained Horn clauses.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 15:36:22 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 09:38:11 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Burn", "Toby Cathcart", ""], ["Ong", "C. -H. Luke", ""], ["Ramsay", "Steven J.", ""]]}, {"id": "1705.06564", "submitter": "J\\\"org P\\\"uhrer", "authors": "Johannes Oetsch, J\\\"org P\\\"uhrer, Hans Tompits", "title": "Stepwise Debugging of Answer-Set Programs", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a stepping methodology for answer-set programming (ASP) that\nallows for debugging answer-set programs and is based on the stepwise\napplication of rules. Similar to debugging in imperative languages, where the\nbehaviour of a program is observed during a step-by-step execution, stepping\nfor ASP allows for observing the effects that rule applications have in the\ncomputation of an answer set. While the approach is inspired from debugging in\nimperative programming, it is conceptually different to stepping in other\nparadigms due to non-determinism and declarativity that are inherent to ASP. In\nparticular, unlike statements in an imperative program that are executed\nfollowing a strict control flow, there is no predetermined order in which to\nconsider rules in ASP during a computation. In our approach, the user is free\nto decide which rule to consider active in the next step following his or her\nintuition. This way, one can focus on interesting parts of the debugging search\nspace. Bugs are detected during stepping by revealing differences between the\nactual semantics of the program and the expectations of the user. As a solid\nformal basis for stepping, we develop a framework of computations for\nanswer-set programs. For fully supporting different solver languages, we build\nour framework on an abstract ASP language that is sufficiently general to\ncapture different solver languages. To this end, we make use of abstract\nconstraints as an established abstraction for popular language constructs such\nas aggregates. Stepping has been implemented in SeaLion, an integrated\ndevelopment environment for ASP. We illustrate stepping using an example\nscenario and discuss the stepping plugin of SeaLion. Moreover, we elaborate on\nmethodological aspects and the embedding of stepping in the ASP development\nprocess.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 13:02:19 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Oetsch", "Johannes", ""], ["P\u00fchrer", "J\u00f6rg", ""], ["Tompits", "Hans", ""]]}, {"id": "1705.06575", "submitter": "Kazem Cheshmi", "authors": "Kazem Cheshmi, Shoaib Kamil, Michelle Mills Strout, Maryam Mehri\n  Dehnavi", "title": "Sympiler: Transforming Sparse Matrix Codes by Decoupling Symbolic\n  Analysis", "comments": "12 pages", "journal-ref": "in SC 2017, Proceedings of the International Conference for High\n  Performance Computing, Networking, Storage and Analysis", "doi": "10.1145/3126908.3126936", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sympiler is a domain-specific code generator that optimizes sparse matrix\ncomputations by decoupling the symbolic analysis phase from the numerical\nmanipulation stage in sparse codes. The computation patterns in sparse\nnumerical methods are guided by the input sparsity structure and the sparse\nalgorithm itself. In many real-world simulations, the sparsity pattern changes\nlittle or not at all. Sympiler takes advantage of these properties to\nsymbolically analyze sparse codes at compile-time and to apply inspector-guided\ntransformations that enable applying low-level transformations to sparse codes.\nAs a result, the Sympiler-generated code outperforms highly-optimized matrix\nfactorization codes from commonly-used specialized libraries, obtaining average\nspeedups over Eigen and CHOLMOD of 3.8X and 1.5X respectively.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 13:16:14 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Cheshmi", "Kazem", ""], ["Kamil", "Shoaib", ""], ["Strout", "Michelle Mills", ""], ["Dehnavi", "Maryam Mehri", ""]]}, {"id": "1705.06662", "submitter": "Nataliia Stulova", "authors": "Nataliia Stulova, Jos\\'e F. Morales and Manuel V. Hermenegildo", "title": "Exploiting Term Hiding to Reduce Run-time Checking Overhead", "comments": "26 pages, 10 figures, 2 tables; an extension of the paper version\n  accepted to PADL'18 (includes proofs, extra figures and examples omitted due\n  to space reasons)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most attractive features of untyped languages is the flexibility\nin term creation and manipulation. However, with such power comes the\nresponsibility of ensuring the correctness of these operations. A solution is\nadding run-time checks to the program via assertions, but this can introduce\noverheads that are in many cases impractical. While static analysis can greatly\nreduce such overheads, the gains depend strongly on the quality of the\ninformation inferred. Reusable libraries, i.e., library modules that are\npre-compiled independently of the client, pose special challenges in this\ncontext. We propose a technique which takes advantage of module systems which\ncan hide a selected set of functor symbols to significantly enrich the shape\ninformation that can be inferred for reusable libraries, as well as an improved\nrun-time checking approach that leverages the proposed mechanisms to achieve\nlarge reductions in overhead, closer to those of static languages, even in the\nreusable-library context. While the approach is general and system-independent,\nwe present it for concreteness in the context of the Ciao assertion language\nand combined static/dynamic checking framework. Our method maintains the full\nexpressiveness of the assertion language in this context. In contrast to other\napproaches it does not introduce the need to switch the language to a (static)\ntype system, which is known to change the semantics in languages like Prolog.\nWe also study the approach experimentally and evaluate the overhead reduction\nachieved in the run-time checks.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 15:54:44 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 14:24:01 GMT"}, {"version": "v3", "created": "Mon, 16 Oct 2017 00:24:08 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Stulova", "Nataliia", ""], ["Morales", "Jos\u00e9 F.", ""], ["Hermenegildo", "Manuel V.", ""]]}, {"id": "1705.06738", "submitter": "Andrei Nemytykh", "authors": "Alexei P. Lisitsa, Andrei P. Nemytykh", "title": "Verifying Programs via Intermediate Interpretation", "comments": "Fifth International Workshop on Verification and Program\n  Transformation (VPT-2017), April 29th, 2017, Uppsala, Sweden, 37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore an approach to verification of programs via program transformation\napplied to an interpreter of a programming language. A specialization technique\nknown as Turchin's supercompilation is used to specialize some interpreters\nwith respect to the program models. We show that several safety properties of\nfunctional programs modeling a class of cache coherence protocols can be proved\nby a supercompiler and compare the results with our earlier work on direct\nverification via supercompilation not using intermediate interpretation.\n  Our approach was in part inspired by an earlier work by De E. Angelis et al.\n(2014-2015) where verification via program transformation and intermediate\ninterpretation was studied in the context of specialization of constraint logic\nprograms.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 11:20:30 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Lisitsa", "Alexei P.", ""], ["Nemytykh", "Andrei P.", ""]]}, {"id": "1705.07226", "submitter": "Tjitze Rienstra", "authors": "Tjitze Rienstra", "title": "RankPL: A Qualitative Probabilistic Programming Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce RankPL, a modeling language that can be thought of\nas a qualitative variant of a probabilistic programming language with a\nsemantics based on Spohn's ranking theory. Broadly speaking, RankPL can be used\nto represent and reason about processes that exhibit uncertainty expressible by\ndistinguishing \"normal\" from\" surprising\" events. RankPL allows (iterated)\nrevision of rankings over alternative program states and supports various types\nof reasoning, including abduction and causal inference. We present the\nlanguage, its denotational semantics, and a number of practical examples. We\nalso discuss an implementation of RankPL that is available for download.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 23:58:26 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Rienstra", "Tjitze", ""]]}, {"id": "1705.07354", "submitter": "Catalin Hritcu", "authors": "Arthur Azevedo de Amorim, Catalin Hritcu, Benjamin C. Pierce", "title": "The Meaning of Memory Safety", "comments": "POST'18 final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give a rigorous characterization of what it means for a programming\nlanguage to be memory safe, capturing the intuition that memory safety supports\nlocal reasoning about state. We formalize this principle in two ways. First, we\nshow how a small memory-safe language validates a noninterference property: a\nprogram can neither affect nor be affected by unreachable parts of the state.\nSecond, we extend separation logic, a proof system for heap-manipulating\nprograms, with a memory-safe variant of its frame rule. The new rule is\nstronger because it applies even when parts of the program are buggy or\nmalicious, but also weaker because it demands a stricter form of separation\nbetween parts of the program state. We also consider a number of pragmatically\nmotivated variations on memory safety and the reasoning principles they\nsupport. As an application of our characterization, we evaluate the security of\na previously proposed dynamic monitor for memory safety of heap-allocated data.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 20:08:30 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 17:30:41 GMT"}, {"version": "v3", "created": "Fri, 6 Apr 2018 18:22:15 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["de Amorim", "Arthur Azevedo", ""], ["Hritcu", "Catalin", ""], ["Pierce", "Benjamin C.", ""]]}, {"id": "1705.07678", "submitter": "Roly Perera", "authors": "Wilmer Ricciotti, Jan Stolarek, Roly Perera, James Cheney", "title": "Imperative Functional Programs that Explain their Work", "comments": "Full version of ICFP 2017 paper, with appendices", "journal-ref": null, "doi": "10.1145/3110258", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program slicing provides explanations that illustrate how program outputs\nwere produced from inputs. We build on an approach introduced in prior work by\nPerera et al., where dynamic slicing was defined for pure higher-order\nfunctional programs as a Galois connection between lattices of partial inputs\nand partial outputs. We extend this approach to imperative functional programs\nthat combine higher-order programming with references and exceptions. We\npresent proofs of correctness and optimality of our approach and a\nproof-of-concept implementation and experimental evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 11:30:54 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Ricciotti", "Wilmer", ""], ["Stolarek", "Jan", ""], ["Perera", "Roly", ""], ["Cheney", "James", ""]]}, {"id": "1705.07686", "submitter": "Michael Laurence", "authors": "Sebastian Danicic, Robert M. Hierons, Michael R. Laurence", "title": "On the computational complexity of dynamic slicing problems for program\n  schemas", "comments": "26 pages, already published", "journal-ref": "Mathematical Structures in Computer Science, vol. 21, pp.\n  1339-1362, 2011", "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a program, a quotient can be obtained from it by deleting zero or more\nstatements. The field of program slicing is concerned with computing a quotient\nof a program which preserves part of the behaviour of the original program. All\nprogram slicing algorithms take account of the structural properties of a\nprogram such as control dependence and data dependence rather than the\nsemantics of its functions and predicates, and thus work, in effect, with\nprogram schemas. The dynamic slicing criterion of Korel and Laski requires only\nthat program behaviour is preserved in cases where the original program follows\na particular path, and that the slice/quotient follows this path. In this paper\nwe formalise Korel and Laski's definition of a dynamic slice as applied to\nlinear schemas, and also formulate a less restrictive definition in which the\npath through the original program need not be preserved by the slice. The less\nrestrictive definition has the benefit of leading to smaller slices. For both\ndefinitions, we compute complexity bounds for the problems of establishing\nwhether a given slice of a linear schema is a dynamic slice and whether a\nlinear schema has a non-trivial dynamic slice and prove that the latter problem\nis NP-hard in both cases. We also give an example to prove that minimal dynamic\nslices (whether or not they preserve the original path) need not be unique.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 11:57:19 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Danicic", "Sebastian", ""], ["Hierons", "Robert M.", ""], ["Laurence", "Michael R.", ""]]}, {"id": "1705.08632", "submitter": "J\\\"urgen Koslowski", "authors": "Horatiu Cirstea, Serguei Lenglet, Pierre-Etienne Moreau", "title": "Faithful (meta-)encodings of programmable strategies into term rewriting\n  systems", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 13, Issue 4 (November\n  28, 2017) lmcs:4096", "doi": "10.23638/LMCS-13(4:16)2017", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rewriting is a formalism widely used in computer science and mathematical\nlogic. When using rewriting as a programming or modeling paradigm, the rewrite\nrules describe the transformations one wants to operate and rewriting\nstrategies are used to con- trol their application. The operational semantics\nof these strategies are generally accepted and approaches for analyzing the\ntermination of specific strategies have been studied. We propose in this paper\na generic encoding of classic control and traversal strategies used in rewrite\nbased languages such as Maude, Stratego and Tom into a plain term rewriting\nsystem. The encoding is proven sound and complete and, as a direct consequence,\nestab- lished termination methods used for term rewriting systems can be\napplied to analyze the termination of strategy controlled term rewriting\nsystems. We show that the encoding of strategies into term rewriting systems\ncan be easily adapted to handle many-sorted signa- tures and we use a\nmeta-level representation of terms to reduce the size of the encodings. The\ncorresponding implementation in Tom generates term rewriting systems compatible\nwith the syntax of termination tools such as AProVE and TTT2, tools which\nturned out to be very effective in (dis)proving the termination of the\ngenerated term rewriting systems. The approach can also be seen as a generic\nstrategy compiler which can be integrated into languages providing pattern\nmatching primitives; experiments in Tom show that applying our encoding leads\nto performances comparable to the native Tom strategies.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 07:06:41 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 01:07:37 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Cirstea", "Horatiu", ""], ["Lenglet", "Serguei", ""], ["Moreau", "Pierre-Etienne", ""]]}, {"id": "1705.08708", "submitter": "Chun Tian", "authors": "Chun Tian", "title": "SNMP for Common Lisp", "comments": "10 pages; reprinted from ILC '09, Proceedings of the International\n  Lisp Conference, March 22-25, 2009, Cambridge, Massachusetts, USA", "journal-ref": "ILC '09, Proceedings of the International Lisp Conference, March\n  22-25, 2009, Cambridge, Massachusetts, USA", "doi": null, "report-no": null, "categories": "cs.NI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple Network Management Protocol (SNMP) is widely used for management of\nInternet-based network today. In Lisp community, there're large Lisp-based\napplications which may need be monitored, and there're Lispers who may need to\nmonitor other remote systems which are either Lisp-based or not. However, the\nrelationship between Lisp and SNMP haven't been studied enough during past 20\nyears.\n  The cl-net-snmp project has developed a new Common Lisp package which\nimplemented the SNMP protocol. On client side, it can be used to query remote\nSNMP peers, and on server side, it brings SNMP capability into Common Lisp\nbased applications, which could be monitored from remote through any SNMP-based\nmanagement system. It's also a flexible platform for researches on network\nmanagement and SNMP itself. But the most important, this project tries to\nprove: Common Lisp is the most fit language to implement SNMP.\n  Different from other exist SNMP projects on Common Lisp, cl-net-snmp is\nclearly targeted on full SNMP protocol support include SNMPv3 and server-side\nwork (agent). During the development, an general ASN.1 compiler and runtime\npackage and an portable UDP networking package are also implemented, which\nwould be useful for other related projects.\n  In this paper, the author first introduces the SNMP protocol and a quick\ntutorial of cl-net-snmp on both client and server sides, and then the Lisp\nnative design and the implementation details of the ASN.1 and SNMP package,\nespecially the \"code generation\"' approach on compiling SNMP MIB definitions\nfrom ASN.1 into Common Lisp.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 11:46:01 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Tian", "Chun", ""]]}, {"id": "1705.08801", "submitter": "Charisee Chiw", "authors": "Charisee Chiw and John Reppy", "title": "Properties of Normalization for a math based intermediate representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Normalization transformation plays a key role in the compilation of\nDiderot programs. The transformations are complicated and it would be easy for\na bug to go undetected. To increase our confidence in normalization part of the\ncompiler we provide a formal analysis on the rewriting system. We proof that\nthe rewrite system is type preserving, value preserving (for tensor-valued\nexpressions), and terminating.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 14:49:25 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Chiw", "Charisee", ""], ["Reppy", "John", ""]]}, {"id": "1705.09042", "submitter": "Yanxin Lu", "authors": "Yanxin Lu, Swarat Chaudhuri, Chris Jermaine, David Melski", "title": "Data-Driven Program Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce program splicing, a programming methodology that aims to\nautomate the commonly used workflow of copying, pasting, and modifying code\navailable online. Here, the programmer starts by writing a \"draft\" that mixes\nunfinished code, natural language comments, and correctness requirements in the\nform of test cases or API call sequence constraints. A program synthesizer that\ninteracts with a large, searchable database of program snippets is used to\nautomatically complete the draft into a program that meets the requirements.\nThe synthesis process happens in two stages. First, the synthesizer identifies\na small number of programs in the database that are relevant to the synthesis\ntask. Next it uses an enumerative search to systematically fill the draft with\nexpressions and statements from these relevant programs. The resulting program\nis returned to the programmer, who can modify it and possibly invoke additional\nrounds of synthesis.\n  We present an implementation of program splicing for the Java programming\nlanguage. The implementation uses a corpus of over 3.5 million procedures from\nan open-source software repository. Our evaluation uses the system in a suite\nof everyday programming tasks, and includes a comparison with a\nstate-of-the-art competing approach as well as a user study. The results point\nto the broad scope and scalability of program splicing and indicate that the\napproach can significantly boost programmer productivity.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 04:34:39 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Lu", "Yanxin", ""], ["Chaudhuri", "Swarat", ""], ["Jermaine", "Chris", ""], ["Melski", "David", ""]]}, {"id": "1705.09231", "submitter": "Swarat Chaudhuri", "authors": "Rohan Mukherjee, Dipak Chaudhari, Matthew Amodio, Thomas Reps, Swarat\n  Chaudhuri, Chris Jermaine", "title": "Neural Attribute Grammars for Semantics-Guided Program Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep models for code tend to be trained on syntactic program\nrepresentations. We present an alternative, called Neural Attribute Grammars,\nthat exposes the semantics of the target language to the training procedure\nusing an attribute grammar. During training, our model learns to replicate the\nrelationship between the syntactic rules used to construct a program, and the\nsemantic attributes (for example, symbol tables) constructed from the context\nin which the rules are fired. We implement the approach as a system for\nconditional generation of Java programs modulo eleven natural requirements. Our\nexperiments show that the system generates constraint-abiding programs with\nsignificantly higher frequency than a baseline model trained on syntactic\nprogram representations, and also in terms of generation accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 15:35:16 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 04:00:12 GMT"}, {"version": "v3", "created": "Sat, 20 Feb 2021 20:32:07 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Mukherjee", "Rohan", ""], ["Chaudhari", "Dipak", ""], ["Amodio", "Matthew", ""], ["Reps", "Thomas", ""], ["Chaudhuri", "Swarat", ""], ["Jermaine", "Chris", ""]]}, {"id": "1705.09413", "submitter": "David Bau iii", "authors": "David Bau, Jeff Gray, Caitlin Kelleher, Josh Sheldon, Franklyn Turbak", "title": "Learnable Programming: Blocks and Beyond", "comments": null, "journal-ref": "Communications of the ACM, June 2017, pp. 72-80", "doi": "10.1145/3015455", "report-no": null, "categories": "cs.PL cs.CY cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blocks-based programming has become the lingua franca for introductory\ncoding. Studies have found that experience with blocks-based programming can\nhelp beginners learn more traditional text-based languages. We explore how\nblocks environments improve learnability for novices by 1) favoring recognition\nover recall, 2) reducing cognitive load, and 3) preventing errors. Increased\nusability of blocks programming has led to widespread adoption within\nintroductory programming contexts across a range of ages. Ongoing work explores\nfurther reducing barriers to programming, supporting novice programmers in\nexpanding their programming skills, and transitioning to textual programming.\nNew blocks frameworks are making it easier to access a variety of APIs through\nblocks environments, opening the doors to a greater diversity of programming\ndomains and supporting greater experimentation for novices and professionals\nalike.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 02:25:19 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Bau", "David", ""], ["Gray", "Jeff", ""], ["Kelleher", "Caitlin", ""], ["Sheldon", "Josh", ""], ["Turbak", "Franklyn", ""]]}, {"id": "1705.09704", "submitter": "Joachim Breitner", "authors": "Joachim Breitner and Chris Smith", "title": "Lock-step simulation is child's play", "comments": "Conditionally accepted as an experience report at ICFP'17", "journal-ref": null, "doi": "10.1145/3110247", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implementing multi-player networked games by broadcasting the player's input\nand letting each client calculate the game state - a scheme known as lock-step\nsimulation - is an established technique. However, ensuring that every client\nin this scheme obtains a consistent state is infamously hard and in general\nrequires great discipline from the game programmer. The thesis of this report\nis that in the realm of functional programming - in particular with Haskell's\npurity and static pointers - this hard problem becomes almost trivially easy.\n  We support this thesis by implementing lock-step simulation under very\nadverse conditions. We extended the educational programming environment\nCodeWorld, which is used to teach math and programming to middle school\nstudents, with the ability to create and run interactive, networked multi-user\ngames. Despite providing a very abstract and high-level interface, and without\nrequiring\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 20:11:00 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Breitner", "Joachim", ""], ["Smith", "Chris", ""]]}, {"id": "1705.09902", "submitter": "Nik Sultana", "authors": "Nik Sultana, Salvator Galea, David Greaves, Marcin Wojcik, Noa\n  Zilberman, Richard Clegg, Luo Mai, Richard Mortier, Peter Pietzuch, Jon\n  Crowcroft, Andrew W Moore", "title": "Extending programs with debug-related features, with application to\n  hardware development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The capacity and programmability of reconfigurable hardware such as FPGAs has\nimproved steadily over the years, but they do not readily provide any\nmechanisms for monitoring or debugging running programs. Such mechanisms need\nto be written into the program itself. This is done using ad hoc methods and\nprimitive tools when compared to CPU programming. This complicates the\nprogramming and debugging of reconfigurable hardware. We introduce\nProgram-hosted Directability (PhD), the extension of programs to interpret\ndirection commands at runtime to enable debugging, monitoring and profiling.\nNormally in hardware development such features are fixed at compile time. We\npresent a language of directing commands, specify its semantics in terms of a\nsimple controller that is embedded with programs, and implement a prototype for\ndirecting network programs running in hardware. We show that this approach\naffords significant flexibility with low impact on hardware utilisation and\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 07:21:34 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Sultana", "Nik", ""], ["Galea", "Salvator", ""], ["Greaves", "David", ""], ["Wojcik", "Marcin", ""], ["Zilberman", "Noa", ""], ["Clegg", "Richard", ""], ["Mai", "Luo", ""], ["Mortier", "Richard", ""], ["Pietzuch", "Peter", ""], ["Crowcroft", "Jon", ""], ["Moore", "Andrew W", ""]]}, {"id": "1705.10416", "submitter": "Alex Sanchez-Stern", "authors": "Alex Sanchez-Stern, Pavel Panchekha, Sorin Lerner, Zachary Tatlock", "title": "Finding Root Causes of Floating Point Error with Herbgrind", "comments": "15 pages published at PLDI 18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Floating-point arithmetic plays a central role in science, engineering, and\nfinance by enabling developers to approximate real arithmetic. To address\nnumerical issues in large floating-point applications, developers must identify\nroot causes, which is difficult because floating-point errors are generally\nnon-local, non-compositional, and non-uniform.\n  This paper presents Herbgrind, a tool to help developers identify and address\nroot causes in numerical code written in low-level C/C++ and Fortran. Herbgrind\ndynamically tracks dependencies between operations and program outputs to avoid\nfalse positives and abstracts erroneous computations to a simplified program\nfragment whose improvement can reduce output error. We perform several case\nstudies applying Herbgrind to large, expert-crafted numerical programs and show\nthat it scales to applications spanning hundreds of thousands of lines,\ncorrectly handling the low-level details of modern floating point hardware and\nmathematical libraries, and tracking error across function boundaries and\nthrough the heap.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 23:43:04 GMT"}, {"version": "v2", "created": "Sun, 16 Jul 2017 21:41:11 GMT"}, {"version": "v3", "created": "Thu, 8 Mar 2018 18:04:56 GMT"}, {"version": "v4", "created": "Thu, 28 Jun 2018 19:09:49 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Sanchez-Stern", "Alex", ""], ["Panchekha", "Pavel", ""], ["Lerner", "Sorin", ""], ["Tatlock", "Zachary", ""]]}, {"id": "1705.10482", "submitter": "Adrien Koutsos", "authors": "Stefano Calzavara, Ilya Grishchenko, Adrien Koutsos, Matteo Maffei", "title": "A Sound Flow-Sensitive Heap Abstraction for the Static Analysis of\n  Android Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper proposes the first static analysis for Android applications\nwhich is both flow-sensitive on the heap abstraction and provably sound with\nrespect to a rich formal model of the Android platform. We formulate the\nanalysis as a set of Horn clauses defining a sound over-approximation of the\nsemantics of the Android application to analyse, borrowing ideas from recency\nabstraction and extending them to our concurrent setting. Moreover, we\nimplement the analysis in HornDroid, a state-of-the-art information flow\nanalyser for Android applications. Our extension allows HornDroid to perform\nstrong updates on heap-allocated data structures, thus significantly increasing\nits precision, without sacrificing its soundness guarantees. We test our\nimplementation on DroidBench, a popular benchmark of Android applications\ndeveloped by the research community, and we show that our changes to HornDroid\nlead to an improvement in the precision of the tool, while having only a\nmoderate cost in terms of efficiency. Finally, we assess the scalability of our\ntool to the analysis of real applications.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 07:23:08 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 16:02:41 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Calzavara", "Stefano", ""], ["Grishchenko", "Ilya", ""], ["Koutsos", "Adrien", ""], ["Maffei", "Matteo", ""]]}]