[{"id": "1904.00396", "submitter": "EPTCS", "authors": "Francisco Martins (University of the Azores), Dominic Orchard\n  (University of Kent)", "title": "Proceedings Programming Language Approaches to Concurrency- and\n  Communication-cEntric Software", "comments": null, "journal-ref": "EPTCS 291, 2019", "doi": "10.4204/EPTCS.291", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern hardware platforms, from the very small to the very large,\nincreasingly provide parallel and distributed computing resources for\napplications to maximise performance. Many applications therefore need to make\neffective use of tens, hundreds, and even thousands of compute nodes.\nComputation in such systems is thus inherently concurrent and communication\ncentric. Effectively programming such applications is challenging; performance,\ncorrectness, and scalability are difficult to achieve.\n  The development of effective programming methodologies for this increasingly\nparallel landscape therefore demands exploration and understanding of a wide\nvariety of foundational and practical ideas. The International Workshop on\nProgramming Language Approaches to Concurrency- and Communication-cEntric\nSoftware (PLACES) is dedicated to work in this area. The workshop offers a\nforum for researchers from different fields to exchange new ideas about these\nchallenges to modern and future programming, where concurrency and distribution\nare the norm rather than a marginal concern.\n  This proceedings covers the 11th edition of PLACES, which was co-located with\nETAPS 2019 in Prague, Czech Republic.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 12:35:33 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Martins", "Francisco", "", "University of the Azores"], ["Orchard", "Dominic", "", "University of Kent"]]}, {"id": "1904.01009", "submitter": "L\\'eon Gondelman", "authors": "Marko van Eekelen, Daniil Frumin, Herman Geuvers, L\\'eon Gondelman,\n  Robbert Krebbers, Marc Schoolderman, Sjaak Smetsers, Freek Verbeek, Beno\\^it\n  Viguier, Freek Wiedijk", "title": "A benchmark for C program verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present twenty-five C programs, as a benchmark for C program verification\nusing formal methods. This benchmark can be used for system demonstration, for\ncomparison of verification effort between systems, and as a friendly\ncompetition. For this last purpose, we give a scoring formula that allows a\nverification system to score up to a hundred points.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 12:07:40 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["van Eekelen", "Marko", ""], ["Frumin", "Daniil", ""], ["Geuvers", "Herman", ""], ["Gondelman", "L\u00e9on", ""], ["Krebbers", "Robbert", ""], ["Schoolderman", "Marc", ""], ["Smetsers", "Sjaak", ""], ["Verbeek", "Freek", ""], ["Viguier", "Beno\u00eet", ""], ["Wiedijk", "Freek", ""]]}, {"id": "1904.01031", "submitter": "Victor Nicolet", "authors": "Azadeh Farzan and Victor Nicolet", "title": "Modular Synthesis of Divide-and-Conquer Parallelism for Nested Loops\n  (Extended Version)", "comments": "This is the extended version of PLDI 2019 paper by the same authors\n  which includes the proofs of theorems and additional details", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a methodology for automatic generation of divide-and-conquer\nparallel implementations of sequential nested loops. We focus on a class of\nloops that traverse read-only multidimensional collections (lists or arrays)\nand compute a function over these collections. Our approach is modular, in\nthat, the inner loop nest is abstracted away to produce a simpler loop nest for\nparallelization. Then, the summarized version of the loop nest is parallelized.\nThe main challenge addressed by this paper is that to perform the code\ntransformations necessary in each step, the loop nest may have to be augmented\n(automatically) with extra computation to make possible the abstraction and/or\nthe parallelization tasks. We present theoretical results to justify the\ncorrectness of our modular approach, and algorithmic solutions for automation.\nExperimental results demonstrate that our approach can parallelize highly\nnon-trivial loop nests efficiently.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 18:01:02 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Farzan", "Azadeh", ""], ["Nicolet", "Victor", ""]]}, {"id": "1904.01117", "submitter": "Marcel Hark", "authors": "Marcel Hark, Benjamin Lucien Kaminski, J\\\"urgen Giesl, Joost-Pieter\n  Katoen", "title": "Aiming Low Is Harder -- Induction for Lower Bounds in Probabilistic\n  Program Verification", "comments": null, "journal-ref": null, "doi": "10.1145/3371105", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new inductive rule for verifying lower bounds on expected values\nof random variables after execution of probabilistic loops as well as on their\nexpected runtimes. Our rule is simple in the sense that loop body semantics\nneed to be applied only finitely often in order to verify that the candidates\nare indeed lower bounds. In particular, it is not necessary to find the limit\nof a sequence as in many previous rules.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 21:35:11 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 10:43:43 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Hark", "Marcel", ""], ["Kaminski", "Benjamin Lucien", ""], ["Giesl", "J\u00fcrgen", ""], ["Katoen", "Joost-Pieter", ""]]}, {"id": "1904.01283", "submitter": "EPTCS", "authors": "Assel Altayeva (Imperial College London), Nobuko Yoshida (Imperial\n  College London)", "title": "Service Equivalence via Multiparty Session Type Isomorphisms", "comments": "In Proceedings PLACES 2019, arXiv:1904.00396", "journal-ref": "EPTCS 291, 2019, pp. 1-11", "doi": "10.4204/EPTCS.291.1", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses a problem found within the construction of Service\nOriented Architecture: the adaptation of service protocols with respect to\nfunctional redundancy and heterogeneity of global communication patterns. We\nutilise the theory of Multiparty Session Types (MPST). Our approach is based\nupon the notion of a multiparty session type isomorphism, utilising a novel\nconstructive realisation of service adapter code to establishing equivalence.\nWe achieve this by employing trace semantics over a collection of local types\nand introducing meta abstractions over the syntax of global types. We develop a\ncorresponding equational theory for MPST isomorphisms. The main motivation for\nthis line of work is to define a type isomorphism that affords the assessment\nof whether two components/services are substitutables, modulo adaptation code\ngiven software components formalised as session types.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 08:43:51 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Altayeva", "Assel", "", "Imperial College London"], ["Yoshida", "Nobuko", "", "Imperial\n  College London"]]}, {"id": "1904.01284", "submitter": "EPTCS", "authors": "Bernardo Almeida (LASIGE, Faculdade de Ciencias, Universidade de\n  Lisboa, Portugal), Andreia Mordido (LASIGE, Faculdade de Ciencias,\n  Universidade de Lisboa, Portugal), Vasco T. Vasconcelos (LASIGE, Faculdade de\n  Ciencias, Universidade de Lisboa, Portugal)", "title": "FreeST: Context-free Session Types in a Functional Language", "comments": "In Proceedings PLACES 2019, arXiv:1904.00396", "journal-ref": "EPTCS 291, 2019, pp. 12-23", "doi": "10.4204/EPTCS.291.2", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FreeST is an experimental concurrent programming language. Based on a core\nlinear functional programming language, it features primitives to fork new\nthreads, and for channel creation and communication. A powerful type system of\ncontext-free session types governs the interaction on channels. The compiler\nbuilds on a novel algorithm for deciding type equivalence of context-free\nsession types. This abstract provides a gentle introduction to the language and\ndiscusses the validation process and runtime system.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 08:44:24 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Almeida", "Bernardo", "", "LASIGE, Faculdade de Ciencias, Universidade de\n  Lisboa, Portugal"], ["Mordido", "Andreia", "", "LASIGE, Faculdade de Ciencias,\n  Universidade de Lisboa, Portugal"], ["Vasconcelos", "Vasco T.", "", "LASIGE, Faculdade de\n  Ciencias, Universidade de Lisboa, Portugal"]]}, {"id": "1904.01286", "submitter": "EPTCS", "authors": "Rosita Gerbo (Universit\\`a di Torino), Luca Padovani (Universit\\`a di\n  Torino)", "title": "Concurrent Typestate-Oriented Programming in Java", "comments": "In Proceedings PLACES 2019, arXiv:1904.00396", "journal-ref": "EPTCS 291, 2019, pp. 24-34", "doi": "10.4204/EPTCS.291.3", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a generative approach that enables concurrent typestate-oriented\nprogramming in Java and other mainstream languages. The approach allows\nprogrammers to implement objects exposing a state-sensitive interface using a\nhigh-level synchronization abstraction that synchronizes methods with the\nstates of the receiver object in which those methods have an effect. An\nexternal tool takes care of generating all the boilerplate code that implements\nthe synchronization logic. Behavioral types are used to specify object\nprotocols. The tool integrates protocol conformance verification with the\nsynchronization logic so that protocol violations are promptly detected at\nruntime.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 08:44:52 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Gerbo", "Rosita", "", "Universit\u00e0 di Torino"], ["Padovani", "Luca", "", "Universit\u00e0 di\n  Torino"]]}, {"id": "1904.01287", "submitter": "EPTCS", "authors": "Jonathan King (Imperial College London & Habito), Nicholas Ng\n  (Imperial College London), Nobuko Yoshida (Imperial College London)", "title": "Multiparty Session Type-safe Web Development with Static Linearity", "comments": "In Proceedings PLACES 2019, arXiv:1904.00396", "journal-ref": "EPTCS 291, 2019, pp. 35-46", "doi": "10.4204/EPTCS.291.4", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern web applications can now offer desktop-like experiences from within\nthe browser, thanks to technologies such as WebSockets, which enable\nlow-latency duplex communication between the browser and the server. While\nthese advances are great for the user experience, they represent a new\nresponsibility for web developers who now need to manage and verify the\ncorrectness of more complex and potentially stateful interactions in their\napplication. In this paper, we present a technique for developing interactive\nweb applications that are statically guaranteed to communicate following a\ngiven protocol. First, the global interaction protocol is described in the\nScribble protocol language -- based on multiparty session types. Scribble\nprotocols are checked for well-formedness, and then each role is projected to a\nFinite State Machine representing the structure of communication from the\nperspective of the role. We use source code generation and a novel type-level\nencoding of FSMs using multi-parameter type classes to leverage the type system\nof the target language and guarantee only programs that communicate following\nthe protocol will type check.\n  Our work targets PureScript -- a functional language that compiles to\nJavaScript -- which crucially has an expressive enough type system to provide\nstatic linearity guarantees. We demonstrate the effectiveness of our approach\nthrough a web-based Battleship game where communication is performed through\nWebSocket connections.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 08:45:23 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["King", "Jonathan", "", "Imperial College London & Habito"], ["Ng", "Nicholas", "", "Imperial College London"], ["Yoshida", "Nobuko", "", "Imperial College London"]]}, {"id": "1904.01288", "submitter": "EPTCS", "authors": "Jan de Muijnck-Hughes (University of Glasgow), Edwin Brady (University\n  of St Andrews), Wim Vanderbauwhede (University of Glasgow)", "title": "Value-Dependent Session Design in a Dependently Typed Language", "comments": "In Proceedings PLACES 2019, arXiv:1904.00396", "journal-ref": "EPTCS 291, 2019, pp. 47-59", "doi": "10.4204/EPTCS.291.5", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session Types offer a typing discipline that allows protocol specifications\nto be used during type-checking, ensuring that implementations adhere to a\ngiven specification. When looking to realise global session types in a\ndependently typed language care must be taken that values introduced in the\ndescription are used by roles that know about the value.\n  We present Sessions, a Resource Dependent EDSL for describing global session\ndescriptions in the dependently typed language Idris. As we construct session\ndescriptions the values parameterising the EDSLs' type keeps track of roles and\nmessages they have encountered. We can use this knowledge to ensure that\nmessage values are only used by those who know the value. Sessions supports\nprotocol descriptions that are computable, composable, higher-order, and\nvalue-dependent. We demonstrate Sessions expressiveness by describing the TCP\nHandshake, a multi-modal server providing echo and basic arithmetic operations,\nand a Higher-Order protocol that supports an authentication interaction step.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 08:46:14 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["de Muijnck-Hughes", "Jan", "", "University of Glasgow"], ["Brady", "Edwin", "", "University\n  of St Andrews"], ["Vanderbauwhede", "Wim", "", "University of Glasgow"]]}, {"id": "1904.01290", "submitter": "EPTCS", "authors": "Klaas Pruiksma (Carnegie Mellon University), Frank Pfenning (Carnegie\n  Mellon University)", "title": "A Message-Passing Interpretation of Adjoint Logic", "comments": "In Proceedings PLACES 2019, arXiv:1904.00396", "journal-ref": "EPTCS 291, 2019, pp. 60-79", "doi": "10.4204/EPTCS.291.6", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system of session types based on adjoint logic which generalize\nstandard binary session types. Our system allows us to uniformly capture\nseveral new behaviors in the space of asynchronous message-passing\ncommunication, including multicast, where a process sends a single message to\nmultiple clients, replicable services, which have multiple clients and\nreplicate themselves on-demand to handle requests from those clients, and\ncancellation, where a process discards a channel without communicating along\nit. We provide session fidelity and deadlock-freedom results for this system,\nfrom which we then derive a logically justified form of garbage collection.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 08:47:53 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Pruiksma", "Klaas", "", "Carnegie Mellon University"], ["Pfenning", "Frank", "", "Carnegie\n  Mellon University"]]}, {"id": "1904.02079", "submitter": "Steven Holtzen", "authors": "Steven Holtzen, Todd Millstein, Guy Van den Broeck", "title": "Symbolic Exact Inference for Discrete Probabilistic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational burden of probabilistic inference remains a hurdle for\napplying probabilistic programming languages to practical problems of interest.\nIn this work, we provide a semantic and algorithmic foundation for efficient\nexact inference on discrete-valued finite-domain imperative probabilistic\nprograms. We leverage and generalize efficient inference procedures for\nBayesian networks, which exploit the structure of the network to decompose the\ninference task, thereby avoiding full path enumeration. To do this, we first\ncompile probabilistic programs to a symbolic representation. Then we adapt\ntechniques from the probabilistic logic programming and artificial intelligence\ncommunities in order to perform inference on the symbolic representation. We\nformalize our approach, prove it sound, and experimentally validate it against\nexisting exact and approximate inference techniques. We show that our inference\napproach is competitive with inference procedures specialized for Bayesian\nnetworks, thereby expanding the class of probabilistic programs that can be\npractically analyzed.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 16:19:12 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 20:33:38 GMT"}, {"version": "v3", "created": "Sun, 30 Jun 2019 23:26:44 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Holtzen", "Steven", ""], ["Millstein", "Todd", ""], ["Broeck", "Guy Van den", ""]]}, {"id": "1904.02809", "submitter": "Xuanrui Qi", "authors": "Reynald Affeldt, Jacques Garrigue, Xuanrui Qi, Kazunari Tanaka", "title": "Proving tree algorithms for succinct data structures", "comments": "Accepted to the 10th International Conference on Interactive Theorem\n  Proving (ITP 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Succinct data structures give space-efficient representations of large\namounts of data without sacrificing performance. They rely one cleverly\ndesigned data representations and algorithms. We present here the formalization\nin Coq/SSReflect of two different tree-based succinct representations and their\naccompanying algorithms. One is the Level-Order Unary Degree Sequence, which\nencodes the structure of a tree in breadth-first order as a sequence of bits,\nwhere access operations can be defined in terms of Rank and Select, which work\nin constant time for static bit sequences. The other represents dynamic bit\nsequences as binary balanced trees, where Rank and Select present a low\nlogarithmic overhead compared to their static versions, and with efficient\ninsertion and deletion. The two can be stacked to provide a dynamic\nrepresentation of dictionaries for instance. While both representations are\nwell-known, we believe this to be their first formalization and a needed step\ntowards provably-safe implementations of big data.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 22:20:12 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 09:49:48 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Affeldt", "Reynald", ""], ["Garrigue", "Jacques", ""], ["Qi", "Xuanrui", ""], ["Tanaka", "Kazunari", ""]]}, {"id": "1904.02830", "submitter": "Konstantinos Krommydas", "authors": "Ruchira Sasanka, Konstantinos Krommydas", "title": "An Evolutionary Framework for Automatic and Guided Discovery of\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Automatic Algorithm Discoverer (AAD), an evolutionary\nframework for synthesizing programs of high complexity. To guide evolution,\nprior evolutionary algorithms have depended on fitness (objective) functions,\nwhich are challenging to design. To make evolutionary progress, instead, AAD\nemploys Problem Guided Evolution (PGE), which requires introduction of a group\nof problems together. With PGE, solutions discovered for simpler problems are\nused to solve more complex problems in the same group. PGE also enables several\nnew evolutionary strategies, and naturally yields to High-Performance Computing\n(HPC) techniques.\n  We find that PGE and related evolutionary strategies enable AAD to discover\nalgorithms of similar or higher complexity relative to the state-of-the-art.\nSpecifically, AAD produces Python code for 29 array/vector problems ranging\nfrom min, max, reverse, to more challenging problems like sorting and\nmatrix-vector multiplication. Additionally, we find that AAD shows adaptability\nto constrained environments/inputs and demonstrates outside-of-the-box problem\nsolving abilities.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 00:03:23 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Sasanka", "Ruchira", ""], ["Krommydas", "Konstantinos", ""]]}, {"id": "1904.03061", "submitter": "Zimin Chen", "authors": "Zimin Chen and Martin Monperrus", "title": "A Literature Study of Embeddings on Source Code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language processing has improved tremendously after the success of\nword embedding techniques such as word2vec. Recently, the same idea has been\napplied on source code with encouraging results. In this survey, we aim to\ncollect and discuss the usage of word embedding techniques on programs and\nsource code. The articles in this survey have been collected by asking authors\nof related work and with an extensive search on Google Scholar. Each article is\ncategorized into five categories: 1. embedding of tokens 2. embedding of\nfunctions or methods 3. embedding of sequences or sets of method calls 4.\nembedding of binary code 5. other embeddings. We also provide links to\nexperimental data and show some remarkable visualization of code embeddings. In\nsummary, word embedding has been successfully applied on different\ngranularities of source code. With access to countless open-source\nrepositories, we see a great potential of applying other data-driven natural\nlanguage processing techniques on source code in the future.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 13:37:42 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Chen", "Zimin", ""], ["Monperrus", "Martin", ""]]}, {"id": "1904.03343", "submitter": "Rahim Charania", "authors": "Rahim K. Charania", "title": "Exploring and Benchmarking High Performance & Scientific Computing using\n  R R HPC Packages and Lower level compiled languages A Comparative Study", "comments": "Code accompanying this paper's benchmarks can be found here:\n  https://github.com/RahimCharania/HighPerformanceComputing", "journal-ref": null, "doi": "10.13140/RG.2.2.16143.43680", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  R is a robust open-source programming language mainly used for statistical\ncomputing . Many areas of statistical research are experiencing rapid growth in\nthe size of data sets. Methodological advances drive increased use of\nsimulations. A common approach is to use parallel/concurrent computing. This\npaper presents an overview of techniques for parallel computing with R on ACI\n(a PSU Infrastructure) and benchmark it with C/C++. We review the scalabilty\nconcern of R, and look at the simplicity of using R as a primary language in\nCoding for HPC. We will look at the various R packages for HPC like Rmpi, Rcpp,\nsnow and snowfall. We utilize a series of algorithms to benchmark and will\nillustrate each benchmark with a representative graph for ease of\nunderstanding. The paper concludes with a better understanding of which\nlanguage to use when in high performance computing .\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 02:43:30 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Charania", "Rahim K.", ""]]}, {"id": "1904.03383", "submitter": "Albert Cohen", "authors": "Ulysse Beaugnon, Basile Cl\\'ement, Nicolas Tollenaere, Albert Cohen", "title": "On the Representation of Partially Specified Implementations and its\n  Application to the Optimization of Linear Algebra Kernels on GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Traditional optimizing compilers rely on rewrite rules to iteratively apply\nprogram transformations. This iterative approach hides optimization\nopportunities behind intermediate transformation steps. For instance,\nvectorization can only be applied to the innermost loop in a nest: one must\nfirst perform a loop interchange before even considering vectorization of an\nouter loop. In contrast, we propose an implementation framework representing\nprograms as sets of possible implementation decisions. Specifying one decision\ncan have an impact on others in a bidirectional manner: specifying that a loop\nmust be vectorized prevents other loops from being nested inside it;\nconversely, specifying a loop as an outer loop will prevent it from being\nvectorized. These optimization decisions commute, obviating the pass ordering\nproblem. We present a constraint programming system to formally define,\nrepresent and explore such implementation spaces. We also propose an\nexploration strategy combining tree search and branch-and-bound; the strength\nand novelty of this strategy reside in an analytical model of the lower bound\non the execution time of a set of possible implementations. We showcase our\napproach on the construction and exploration of an implementation space for\nlinear algebra kernels running on GPUs. We show this search space is expressive\nenough to represent complex decisions that fundamentally change the structure\nof the generated code. We also present preliminary results competitive with the\nperformance of native GPU libraries.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 07:52:01 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Beaugnon", "Ulysse", ""], ["Cl\u00e9ment", "Basile", ""], ["Tollenaere", "Nicolas", ""], ["Cohen", "Albert", ""]]}, {"id": "1904.03521", "submitter": "Milod Kazerounian", "authors": "Milod Kazerounian, Sankha Narayan Guria, Niki Vazou, Jeffrey S.\n  Foster, David Van Horn", "title": "Type-Level Computations for Ruby Libraries", "comments": "22 pages, this is a technical report (with appendix) of a paper to\n  appear in Programming Language Design and Implementation (PLDI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many researchers have explored ways to bring static typing to dynamic\nlanguages. However, to date, such systems are not precise enough when types\ndepend on values, which often arises when using certain Ruby libraries. For\nexample, the type safety of a database query in Ruby on Rails depends on the\ntable and column names used in the query. To address this issue, we introduce\nCompRDL, a type system for Ruby that allows library method type signatures to\ninclude type-level computations (or comp types for short). Combined with\nsingleton types for table and column names, comp types let us give database\nquery methods type signatures that compute a table's schema to yield very\nprecise type information. Comp types for hash, array, and string libraries can\nalso increase precision and thereby reduce the need for type casts. We\nformalize CompRDL and prove its type system sound. Rather than type check the\nbodies of library methods with comp types---those methods may include native\ncode or be complex---CompRDL inserts run-time checks to ensure library methods\nabide by their computed types. We evaluated CompRDL by writing annotations with\ntype-level computations for several Ruby core libraries and database query\nAPIs. We then used those annotations to type check two popular Ruby libraries\nand four Ruby on Rails web apps. We found the annotations were relatively\ncompact and could successfully type check 132 methods across our subject\nprograms. Moreover, the use of type-level computations allowed us to check more\nexpressive properties, with fewer manually inserted casts, than was possible\nwithout type-level computations. In the process, we found two type errors and a\ndocumentation error that were confirmed by the developers. Thus, we believe\nCompRDL is an important step forward in bringing precise static type checking\nto dynamic languages.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 20:06:07 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Kazerounian", "Milod", ""], ["Guria", "Sankha Narayan", ""], ["Vazou", "Niki", ""], ["Foster", "Jeffrey S.", ""], ["Van Horn", "David", ""]]}, {"id": "1904.03540", "submitter": "Rokas Volkovas", "authors": "Rokas Volkovas, Michael Fairbank, John Woodward, Simon Lucas", "title": "Mek: Mechanics Prototyping Tool for 2D Tile-Based Turn-Based\n  Deterministic Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are few digital tools to help designers create game mechanics. A\ngeneral language to express game mechanics is necessary for rapid game design\niteration. The first iteration of a mechanics-focused language, together with\nits interfacing tool, are introduced in this paper. The language is restricted\nto two-dimensional, turn-based, tile-based, deterministic, complete-information\ngames. The tool is compared to the existing alternatives for game mechanics\nprototyping and shown to be capable of succinctly implementing a range of\nwell-known game mechanics.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 22:09:53 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Volkovas", "Rokas", ""], ["Fairbank", "Michael", ""], ["Woodward", "John", ""], ["Lucas", "Simon", ""]]}, {"id": "1904.04304", "submitter": "Robert Rand", "authors": "Robert Rand", "title": "Verification Logics for Quantum Programs", "comments": "Originally submitted in March 2016 as a qualifying examination\n  (WPE-II) for the PhD program at the University of Pennsylvania", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.ET cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey the landscape of Hoare logics for quantum programs. We review three\npapers: \"Reasoning about imperative quantum programs\" by Chadha, Mateus and\nSernadas; \"A logic for formal verification of quantum programs\" by Yoshihiko\nKakutani; and \"Floyd-hoare logic for quantum programs\" by Mingsheng Ying. We\ncompare the mathematical foundations of the logics, their underlying languages,\nand the expressivity of their assertions. We also use the languages to verify\nthe Deutsch-Jozsa Algorithm, and discuss their relative usability in practice.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 19:13:07 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Rand", "Robert", ""]]}, {"id": "1904.04371", "submitter": "Jennifer Paykin", "authors": "Jennifer Paykin and Steve Zdancewic", "title": "A HoTT Quantum Equational Theory (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an equational theory for the QRAM model of quantum\ncomputation, formulated as an embedded language inside of homotopy type theory.\nThe embedded language approach is highly expressive, and reflects the style of\nstate-of-the art quantum languages like Quipper and QWIRE. The embedding takes\nadvantage of features of homotopy type theory to encode unitary transformations\nas higher inductive paths, simplifying the presentation of an equational\ntheory. We prove that this equational theory is sound and complete with respect\nto established models of quantum computation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 21:41:40 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Paykin", "Jennifer", ""], ["Zdancewic", "Steve", ""]]}, {"id": "1904.05387", "submitter": "Emery Berger", "authors": "Eunice Jun, Maureen Daum, Jared Roesch, Sarah E. Chasins, Emery D.\n  Berger, Rene Just, Katharina Reinecke", "title": "Tea: A High-level Language and Runtime System for Automating Statistical\n  Analysis", "comments": "11 pages", "journal-ref": null, "doi": "10.1145/3332165.3347940", "report-no": null, "categories": "cs.PL cs.HC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though statistical analyses are centered on research questions and\nhypotheses, current statistical analysis tools are not. Users must first\ntranslate their hypotheses into specific statistical tests and then perform API\ncalls with functions and parameters. To do so accurately requires that users\nhave statistical expertise. To lower this barrier to valid, replicable\nstatistical analysis, we introduce Tea, a high-level declarative language and\nruntime system. In Tea, users express their study design, any parametric\nassumptions, and their hypotheses. Tea compiles these high-level specifications\ninto a constraint satisfaction problem that determines the set of valid\nstatistical tests, and then executes them to test the hypothesis. We evaluate\nTea using a suite of statistical analyses drawn from popular tutorials. We show\nthat Tea generally matches the choices of experts while automatically switching\nto non-parametric tests when parametric assumptions are not met. We simulate\nthe effect of mistakes made by non-expert users and show that Tea automatically\navoids both false negatives and false positives that could be produced by the\napplication of incorrect statistical tests.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 18:44:55 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Jun", "Eunice", ""], ["Daum", "Maureen", ""], ["Roesch", "Jared", ""], ["Chasins", "Sarah E.", ""], ["Berger", "Emery D.", ""], ["Just", "Rene", ""], ["Reinecke", "Katharina", ""]]}, {"id": "1904.05389", "submitter": "Karl Crary", "authors": "Michael J. Sullivan, Karl Crary, and Salil Joshi", "title": "Compiling a Calculus for Relaxed Memory: Practical constraint-based\n  low-level concurrency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crary and Sullivan's Relaxed Memory Calculus (RMC) proposed a new declarative\napproach for writing low-level shared memory concurrent programs in the\npresence of modern relaxed-memory multi-processor architectures and optimizing\ncompilers. In RMC, the programmer explicitly specifies constraints on the order\nof execution of operations and on the visibility of memory writes. These\nconstraints are then enforced by the compiler, which has a wide degree of\nlatitude in how to accomplish its goals.\n  We present rmc-compiler, a Clang and LLVM-based compiler for RMC-extended C\nand C++. In addition to using barriers to enforce ordering, rmc-compiler can\ntake advantage of control and data dependencies, something that is beyond the\nabilities of current C/C++ compilers. In rmc-compiler, RMC compilation is\nmodeled as an SMT problem with a cost term; the solution with the minimum cost\ndetermines the compilation strategy. In testing on ARM and POWER devices, RMC\nperforms quite well, with modest performance improvements relative to C++11 on\nmost of our data structure benchmarks and (on some architectures) dramatic\nimprovements on a read-mostly list test that heavily benefits from use of data\ndependencies for ordering.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 18:52:48 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Sullivan", "Michael J.", ""], ["Crary", "Karl", ""], ["Joshi", "Salil", ""]]}, {"id": "1904.05498", "submitter": "Yuepeng Wang", "authors": "Yuepeng Wang, James Dong, Rushi Shah, Isil Dillig", "title": "Synthesizing Database Programs for Schema Refactoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many programs that interact with a database need to undergo schema\nrefactoring several times during their life cycle. Since this process typically\nrequires making significant changes to the program's implementation, schema\nrefactoring is often non-trivial and error-prone. Motivated by this problem, we\npropose a new technique for automatically synthesizing a new version of a\ndatabase program given its original version and the source and target schemas.\nOur method does not require manual user guidance and ensures that the\nsynthesized program is equivalent to the original one. Furthermore, our method\nis quite efficient and can synthesize new versions of database programs\n(containing up to 263 functions) that are extracted from real-world web\napplications with an average synthesis time of 69.4 seconds.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 02:16:17 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Wang", "Yuepeng", ""], ["Dong", "James", ""], ["Shah", "Rushi", ""], ["Dillig", "Isil", ""]]}, {"id": "1904.05980", "submitter": "Issam Damaj", "authors": "Issam Damaj", "title": "Parallel algorithms development for programmable logic devices", "comments": "47 Pages, 25 Figures, 2 Tables. arXiv admin note: substantial text\n  overlap with arXiv:1904.03756, arXiv:1904.05437", "journal-ref": "Advances in Engineering Software, Elsevier. 37 (2006) 561-582", "doi": "10.1016/j.advengsoft.2006.01.009", "report-no": null, "categories": "cs.PL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programmable Logic Devices (PLDs) continue to grow in size and currently\ncontain several millions of gates. At the same time, research effort is going\ninto higher-level hardware synthesis methodologies for reconfigurable computing\nthat can exploit PLD technology. In this paper, we explore the effectiveness\nand extend one such formal methodology in the design of massively parallel\nalgorithms. We take a step-wise refinement approach to the development of\ncorrect reconfigurable hardware circuits from formal specifications. A\nfunctional programming notation is used for specifying algorithms and for\nreasoning about them. The specifications are realised through the use of a\ncombination of function decomposition strategies, data refinement techniques,\nand off-the-shelf refinements based upon higher-order functions. The\noff-the-shelf refinements are inspired by the operators of Communicating\nSequential Processes (CSP) and map easily to programs in Handel-C (a hardware\ndescription language). The Handel-C descriptions are directly compiled into\nreconfigurable hardware. The practical realisation of this methodology is\nevidenced by a case studying the matrix multiplication algorithm as it is\nrelatively simple and well known. In this paper, we obtain several hardware\nimplementations with different performance characteristics by applying\ndifferent refinements to the algorithm. The developed designs are compiled and\ntested under Celoxica's RC-1000 reconfigurable computer with its 2 million\ngates Virtex-E FPGA. Performance analysis and evaluation of these\nimplementations are included.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 13:48:08 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Damaj", "Issam", ""]]}, {"id": "1904.06159", "submitter": "EPTCS", "authors": "Thomas Ehrhard, Maribel Fern\\'andez, Valeria de Paiva, Lorenzo Tortora\n  de Falco", "title": "Proceedings Joint International Workshop on Linearity & Trends in Linear\n  Logic and Applications", "comments": null, "journal-ref": "EPTCS 292, 2019", "doi": "10.4204/EPTCS.292", "report-no": null, "categories": "cs.LO cs.CC cs.PL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains a selection of papers presented at Linearity/TLLA 2018:\nJoint Linearity and TLLA workshops (part of FLOC 2018) held on July 7-8, 2018\nin Oxford. Linearity has been a key feature in several lines of research in\nboth theoretical and practical approaches to computer science. On the\ntheoretical side there is much work stemming from linear logic dealing with\nproof technology, complexity classes and more recently quantum computation. On\nthe practical side there is work on program analysis, expressive operational\nsemantics for programming languages, linear programming languages, program\ntransformation, update analysis and efficient implementation techniques. Linear\nlogic is not only a theoretical tool to analyse the use of resources in logic\nand computation. It is also a corpus of tools, approaches, and methodologies\n(proof nets, exponential decomposition, geometry of interaction, coherent\nspaces, relational models, etc.) that were originally developed for the study\nof linear logic's syntax and semantics and are nowadays applied in several\nother fields.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 11:29:34 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Ehrhard", "Thomas", ""], ["Fern\u00e1ndez", "Maribel", ""], ["de Paiva", "Valeria", ""], ["de Falco", "Lorenzo Tortora", ""]]}, {"id": "1904.06319", "submitter": "Robert Rand", "authors": "Kesha Hietala, Robert Rand, Shih-Han Hung, Xiaodi Wu, Michael Hicks", "title": "Verified Optimization in a Quantum Intermediate Representation", "comments": "Superceded by arXiv:1912.02250", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.ET cs.PL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present sqire, a low-level language for quantum computing and\nverification. sqire uses a global register of quantum bits, allowing easy\ncompilation to and from existing `quantum assembly' languages and simplifying\nthe verification process. We demonstrate the power of sqire as an intermediate\nrepresentation of quantum programs by verifying a number of useful\noptimizations, and we demonstrate sqire's use as a tool for general\nverification by proving several quantum programs correct.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 16:54:17 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 20:59:06 GMT"}, {"version": "v3", "created": "Fri, 31 May 2019 16:08:47 GMT"}, {"version": "v4", "created": "Fri, 6 Dec 2019 06:17:23 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Hietala", "Kesha", ""], ["Rand", "Robert", ""], ["Hung", "Shih-Han", ""], ["Wu", "Xiaodi", ""], ["Hicks", "Michael", ""]]}, {"id": "1904.06534", "submitter": "Susan Eisenbach", "authors": "Franklin Schrans, Daniel Hails, Alexander Harkness, Sophia\n  Drossopoulou, and Susan Eisenbach", "title": "Flint for Safer Smart Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ethereum blockchain platform supports the execution of decentralised\napplications or smart contracts. These typically hold and transfer digital\ncurrency to other parties on the platform; however, they have been subject to\nnumerous attacks due to the unintentional introduction of bugs. Over a billion\ndollars worth of currency has been stolen since its release in July 2015. As\nsmart contracts cannot be updated after deployment, it is imperative that the\nprogramming language supports the development of robust contracts.\n  We propose Flint, a new statically-typed programming language specifically\ndesigned for writing robust smart contracts. Flint's features enforce the\nwriting of safe and predictable code. To encourage good practices, we introduce\nprotection blocks. Protection blocks restrict who can run code and when (using\ntypestate) it can be executed. To prevent vulnerabilities relating to the\nunintentional loss of currency, Flint Asset traits provide safe atomic\noperations, ensuring the state of contracts is always consistent. Writes to\nstate are restricted, simplifying reasoning about smart contracts.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 12:44:59 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Schrans", "Franklin", ""], ["Hails", "Daniel", ""], ["Harkness", "Alexander", ""], ["Drossopoulou", "Sophia", ""], ["Eisenbach", "Susan", ""]]}, {"id": "1904.06584", "submitter": "Rohan Achar", "authors": "Rohan Achar and Cristina V. Lopes", "title": "Got: Git, but for Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We look at one important category of distributed applications characterized\nby the existence of multiple collaborating, and competing, components sharing\nmutable, long-lived, replicated objects. The problem addressed by our work is\nthat of object state synchronization among the components. As an organizing\nprinciple for replicated objects, we formally specify the Global Object Tracker\n(GoT) model, an object-oriented programming model based on causal consistency\nwith application-level conflict resolution strategies, whose elements and\ninterfaces mirror those found in decentralized version control systems: a\nversion graph, working data, diffs, commit, checkout, fetch, push, and merge.\nWe have implemented GoT in a framework called Spacetime, written in Python.\n  In its purest form, GoT is impractical for real systems, because of the\nunbounded growth of the version graph and because passing diff'ed histories\nover the network makes remote communication too slow. We present our solution\nto these problems that adds some constraints to GoT applications, but that\nmakes the model feasible in practice. We present a performance analysis of\nSpacetime for representative workloads, which shows that the additional\nconstraints added to GoT make it not just feasible, but viable for real\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 18:51:27 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Achar", "Rohan", ""], ["Lopes", "Cristina V.", ""]]}, {"id": "1904.06750", "submitter": "Will Crichton", "authors": "Will Crichton", "title": "From Theory to Systems: A Grounded Approach to Programming Language\n  Education", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  I present a new approach to teaching a graduate-level programming languages\ncourse focused on using systems programming ideas and languages like\nWebAssembly and Rust to motivate PL theory. Drawing on students' prior\nexperience with low-level languages, the course shows how type systems and PL\ntheory are used to avoid tricky real-world errors that students encounter in\npractice. I reflect on the curricular design and lessons learned from two years\nof teaching at Stanford, showing that integrating systems ideas can provide\nstudents a more grounded and enjoyable education in programming languages. The\ncurriculum, course notes, and assignments are freely available:\nhttp://cs242.stanford.edu/f18/\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 20:08:04 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Crichton", "Will", ""]]}, {"id": "1904.06845", "submitter": "EPTCS", "authors": "Giulio Guerrieri (Dipartimento di Informatica -- Scienza e Ingegneria\n  (DISI), Universit\\`a di Bologna, Bologna, Italy), Giulio Manzonetto (LIPN,\n  UMR 7030, Universit\\'e Paris 13, Sorbonne Paris Cit\\'e, Villetaneuse, France)", "title": "The Bang Calculus and the Two Girard's Translations", "comments": "In Proceedings Linearity-TLLA 2018, arXiv:1904.06159", "journal-ref": "EPTCS 292, 2019, pp. 15-30", "doi": "10.4204/EPTCS.292.2", "report-no": null, "categories": "cs.LO cs.DM cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the two Girard's translations of intuitionistic implication into\nlinear logic by exploiting the bang calculus, a paradigmatic functional\nlanguage with an explicit box-operator that allows both call-by-name and\ncall-by-value lambda-calculi to be encoded in. We investigate how the bang\ncalculus subsumes both call-by-name and call-by-value lambda-calculi from a\nsyntactic and a semantic viewpoint.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 05:15:31 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Guerrieri", "Giulio", "", "Dipartimento di Informatica -- Scienza e Ingegneria"], ["Manzonetto", "Giulio", "", "LIPN,\n  UMR 7030, Universit\u00e9 Paris 13, Sorbonne Paris Cit\u00e9, Villetaneuse, France"]]}, {"id": "1904.06846", "submitter": "EPTCS", "authors": "Masahito Hasegawa (RIMS, Kyoto University)", "title": "From Linear Logic to Cyclic Sharing", "comments": "In Proceedings Linearity-TLLA 2018, arXiv:1904.06159", "journal-ref": "EPTCS 292, 2019, pp. 31-42", "doi": "10.4204/EPTCS.292.3", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a translation from Multiplicative Exponential Linear Logic to a\nsimply-typed lambda calculus with cyclic sharing. This translation is derived\nfrom a simple observation on the Int-construction on traced monoidal\ncategories. It turns out that the translation is a mixture of the call-by-name\nCPS translation and the Geometry of Interaction-based interpretation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 05:15:57 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Hasegawa", "Masahito", "", "RIMS, Kyoto University"]]}, {"id": "1904.06847", "submitter": "EPTCS", "authors": "Jiaming Jiang (North Carolina State University), Harley Eades III\n  (Augusta University), Valeria de Paiva (Nuance Communications)", "title": "On the Lambek Calculus with an Exchange Modality", "comments": "In Proceedings Linearity-TLLA 2018, arXiv:1904.06159", "journal-ref": "EPTCS 292, 2019, pp. 43-89", "doi": "10.4204/EPTCS.292.4", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce Commutative/Non-Commutative Logic (CNC logic) and\ntwo categorical models for CNC logic. This work abstracts Benton's\nLinear/Non-Linear Logic by removing the existence of the exchange structural\nrule. One should view this logic as composed of two logics; one sitting to the\nleft of the other. On the left, there is intuitionistic linear logic, and on\nthe right is a mixed commutative/non-commutative formalization of the Lambek\ncalculus. Then both of these logics are connected via a pair of monoidal\nadjoint functors. An exchange modality is then derivable within the logic using\nthe adjunction between both sides. Thus, the adjoint functors allow one to pull\nthe exchange structural rule from the left side to the right side. We then give\na categorical model in terms of a monoidal adjunction, and then a concrete\nmodel in terms of dialectica Lambek spaces.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 05:16:15 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Jiang", "Jiaming", "", "North Carolina State University"], ["Eades", "Harley", "III", "Augusta University"], ["de Paiva", "Valeria", "", "Nuance Communications"]]}, {"id": "1904.06848", "submitter": "EPTCS", "authors": "Wen Kokke (University of Edinburgh), Fabrizio Montesi (University of\n  Southern Denmark), Marco Peressotti (University of Southern Denmark)", "title": "Taking Linear Logic Apart", "comments": "In Proceedings Linearity-TLLA 2018, arXiv:1904.06159", "journal-ref": "EPTCS 292, 2019, pp. 90-103", "doi": "10.4204/EPTCS.292.5", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process calculi based on logic, such as $\\pi$DILL and CP, provide a\nfoundation for deadlock-free concurrent programming. However, in previous work,\nthere is a mismatch between the rules for constructing proofs and the term\nconstructors of the $\\pi$-calculus: the fundamental operator for parallel\ncomposition does not correspond to any rule of linear logic. Kokke et al.\n(2019) introduced Hypersequent Classical Processes (HCP), which addresses this\nmismatch using hypersequents (collections of sequents) to register parallelism\nin the typing judgements. However, the step from CP to HCP is a big one. As of\nyet, HCP does not have reduction semantics, and the addition of delayed actions\nmeans that CP processes interpreted as HCP processes do not behave as they\nwould in CP. We introduce HCP-, a variant of HCP with reduction semantics and\nwithout delayed actions. We prove progress, preservation, and termination, and\nshow that HCP- supports the same communication protocols as CP.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 05:16:51 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Kokke", "Wen", "", "University of Edinburgh"], ["Montesi", "Fabrizio", "", "University of\n  Southern Denmark"], ["Peressotti", "Marco", "", "University of Southern Denmark"]]}, {"id": "1904.07061", "submitter": "Laith Sakka", "authors": "Laith Sakka, Kirshanthan Sundararajah, Ryan R. Newton, Milind Kulkarni", "title": "Sound, Fine-Grained Traversal Fusion for Heterogeneous Trees - Extended\n  Version", "comments": "Extended version of \"Sound Fine-Grained Traversal Fusion for\n  Heterogeneous Trees\" Sakka et al., PLDI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications in many domains are based on a series of traversals of tree\nstructures, and fusing these traversals together to reduce the total number of\npasses over the tree is a common, important optimization technique. In\napplications such as compilers and render trees, these trees are heterogeneous:\ndifferent nodes of the tree have different types. Unfortunately, prior work for\nfusing traversals falls short in different ways: they do not handle\nheterogeneity; they require using domain-specific languages to express an\napplication; they rely on the programmer to aver that fusing traversals is\nsafe, without any soundness guarantee; or they can only perform coarse-grain\nfusion, leading to missed fusion opportunities. This paper addresses these\nshortcomings to build a framework for fusing traversals of heterogeneous trees\nthat is automatic, sound, and fine-grained. We show across several case studies\nthat our approach is able to allow programmers to write simple, intuitive\ntraversals, and then automatically fuse them to substantially improve\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 18:36:58 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Sakka", "Laith", ""], ["Sundararajah", "Kirshanthan", ""], ["Newton", "Ryan R.", ""], ["Kulkarni", "Milind", ""]]}, {"id": "1904.07136", "submitter": "Germ\\'an Andr\\'es Delbianco", "authors": "Aleksandar Nanevski and Anindya Banerjee and Germ\\'an Andr\\'es\n  Delbianco and Ignacio F\\'abregas", "title": "Specifying Concurrent Programs in Separation Logic: Morphisms and\n  Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In addition to pre- and postconditions, program specifications in recent\nseparation logics for concurrency have employed an algebraic structure of\nresources---a form of state transition system---to describe the state-based\nprogram invariants that must be preserved, and to record the permissible atomic\nchanges to program state. In this paper we introduce a novel notion of resource\nmorphism, i.e. structure-preserving function on resources, and show how to\neffectively integrate it into separation logic, using an associated notion of\nmorphism-specific simulation. We apply morphisms and simulations to programs\nverified under one resource, to compositionally adapt them to operate under\nanother resource, thus facilitating proof reuse.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 15:42:33 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 18:28:26 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2019 15:46:02 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Nanevski", "Aleksandar", ""], ["Banerjee", "Anindya", ""], ["Delbianco", "Germ\u00e1n Andr\u00e9s", ""], ["F\u00e1bregas", "Ignacio", ""]]}, {"id": "1904.07146", "submitter": "Saswat Padhi", "authors": "Rajeev Alur and Dana Fisman and Saswat Padhi and Rishabh Singh and\n  Abhishek Udupa", "title": "SyGuS-Comp 2018: Results and Analysis", "comments": "18 pages. Satellite event of CAV'18 and SYNT'18. arXiv admin note:\n  substantial text overlap with arXiv:1711.11438", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Syntax-guided synthesis (SyGuS) is the computational problem of finding an\nimplementation $f$ that meets both a semantic constraint given by a logical\nformula $\\phi$ in a background theory $\\mathbb{T}$, and a syntactic constraint\ngiven by a grammar $G$, which specifies the allowed set of candidate\nimplementations. Such a synthesis problem can be formally defined in the SyGuS\ninput format (SyGuS-IF), a language that is built on top of SMT-LIB.\n  The Syntax-Guided Synthesis competition (SyGuS-Comp) is an effort to\nfacilitate, bring together and accelerate research and development of efficient\nsolvers for SyGuS by providing a platform for evaluating different synthesis\ntechniques on a comprehensive set of benchmarks. In the 5th SyGuS-Comp, five\nsolvers competed on over 1600 benchmarks across various tracks. This paper\npresents and analyses the results of this year's (2018) SyGuS competition.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 17:59:08 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Alur", "Rajeev", ""], ["Fisman", "Dana", ""], ["Padhi", "Saswat", ""], ["Singh", "Rishabh", ""], ["Udupa", "Abhishek", ""]]}, {"id": "1904.07298", "submitter": "Marianna Rapoport", "authors": "Marianna Rapoport, Ond\\v{r}ej Lhot\\'ak", "title": "A Path To DOT: Formalizing Fully Path-Dependent Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dependent Object Types (DOT) calculus aims to formalize the Scala\nprogramming language with a focus on path-dependent types $-$ types such as\n$x.a_1\\dots a_n.T$ that depend on the runtime value of a path $x.a_1\\dots a_n$\nto an object. Unfortunately, existing formulations of DOT can model only types\nof the form $x.A$ which depend on variables rather than general paths. This\nrestriction makes it impossible to model nested module dependencies. Nesting\nsmall components inside larger ones is a necessary ingredient of a modular,\nscalable language. DOT's variable restriction thus undermines its ability to\nfully formalize a variety of programming-language features including Scala's\nmodule system, family polymorphism, and covariant specialization.\n  This paper presents the pDOT calculus, which generalizes DOT to support types\nthat depend on paths of arbitrary length, as well as singleton types to track\npath equality. We show that naive approaches to add paths to DOT make it\ninherently unsound, and present necessary conditions for such a calculus to be\nsound. We discuss the key changes necessary to adapt the techniques of the DOT\nsoundness proofs so that they can be applied to pDOT. Our paper comes with a\nCoq-mechanized type-safety proof of pDOT. With support for paths of arbitrary\nlength, pDOT can realize DOT's full potential for formalizing Scala-like\ncalculi.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 19:31:34 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 21:26:02 GMT"}, {"version": "v3", "created": "Wed, 17 Jul 2019 03:19:20 GMT"}, {"version": "v4", "created": "Tue, 13 Aug 2019 21:21:59 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Rapoport", "Marianna", ""], ["Lhot\u00e1k", "Ond\u0159ej", ""]]}, {"id": "1904.07404", "submitter": "Changxi Liu", "authors": "Changxi Liu, Hailong Yang, Rujun Sun, Zhongzhi Luan, Lin Gan, Guangwen\n  Yang, Depei Qian", "title": "swTVM: Exploring the Automated Compilation for Deep Learning on Sunway\n  Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The flourish of deep learning frameworks and hardware platforms has been\ndemanding an efficient compiler that can shield the diversity in both software\nand hardware in order to provide application portability. Among the exiting\ndeep learning compilers, TVM is well known for its efficiency in code\ngeneration and optimization across diverse hardware devices. In the meanwhile,\nthe Sunway many-core processor renders itself as a competitive candidate for\nits attractive computational power in both scientific and deep learning\napplications. This paper combines the trends in these two directions.\nSpecifically, we propose swTVM that extends the original TVM to support\nahead-of-time compilation for architecture requiring cross-compilation such as\nSunway. In addition, we leverage the architecture features during the\ncompilation such as core group for massive parallelism, DMA for high bandwidth\nmemory transfer and local device memory for data locality, in order to generate\nefficient code for deep learning application on Sunway. The experimental\nresults show the ability of swTVM to automatically generate code for various\ndeep neural network models on Sunway. The performance of automatically\ngenerated code for AlexNet and VGG-19 by swTVM achieves 6.71x and 2.45x speedup\non average than hand-optimized OpenACC implementations on convolution and fully\nconnected layers respectively. This work is the first attempt from the compiler\nperspective to bridge the gap of deep learning and high performance\narchitecture particularly with productivity and efficiency in mind. We would\nlike to open source the implementation so that more people can embrace the\npower of deep learning compiler and Sunway many-core processor.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 02:13:05 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 13:09:43 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Liu", "Changxi", ""], ["Yang", "Hailong", ""], ["Sun", "Rujun", ""], ["Luan", "Zhongzhi", ""], ["Gan", "Lin", ""], ["Yang", "Guangwen", ""], ["Qian", "Depei", ""]]}, {"id": "1904.07415", "submitter": "Tristan Knoth", "authors": "Tristan Knoth, Di Wang, Nadia Polikarpova, Jan Hoffmann", "title": "Resource-Guided Program Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents resource-guided synthesis, a technique for synthesizing\nrecursive programs that satisfy both a functional specification and a symbolic\nresource bound. The technique is type-directed and rests upon a novel type\nsystem that combines polymorphic refinement types with potential annotations of\nautomatic amortized resource analysis. The type system enables efficient\nconstraint-based type checking and can express precise refinement-based\nresource bounds. The proof of type soundness shows that synthesized programs\nare correct by construction. By tightly integrating program exploration and\ntype checking, the synthesizer can leverage the user-provided resource bound to\nguide the search, eagerly rejecting incomplete programs that consume too many\nresources. An implementation in the resource-guided synthesizer ReSyn is used\nto evaluate the technique on a range of recursive data structure manipulations.\nThe experiments show that ReSyn synthesizes programs that are asymptotically\nmore efficient than those generated by a resource-agnostic synthesizer.\nMoreover, synthesis with ReSyn is faster than a naive combination of synthesis\nand resource analysis. ReSyn is also able to generate implementations that have\na constant resource consumption for fixed input sizes, which can be used to\nmitigate side-channel attacks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 02:34:47 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 02:31:37 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Knoth", "Tristan", ""], ["Wang", "Di", ""], ["Polikarpova", "Nadia", ""], ["Hoffmann", "Jan", ""]]}, {"id": "1904.07425", "submitter": "Naohiko Hoshino", "authors": "Ugo Dal Lago, Naohiko Hoshino", "title": "The Geometry of Bayesian Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a geometry of interaction model for a typed lambda-calculus endowed\nwith operators for sampling from a continuous uniform distribution and soft\nconditioning, namely a paradigmatic calculus for higher-order Bayesian\nprogramming. The model is based on the category of measurable spaces and\npartial measurable functions, and is proved adequate with respect to both a\ndistribution-based and a sampling based operational semantics.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 03:10:48 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Lago", "Ugo Dal", ""], ["Hoshino", "Naohiko", ""]]}, {"id": "1904.07463", "submitter": "ThanhVu Nguyen", "authors": "ThanhVu Nguyen and Deepak Kapur and Westley Weimer and Stephanie\n  Forrest", "title": "Using Dynamic Analysis to Generate Disjunctive Invariants", "comments": "appear in ICSE 2014", "journal-ref": null, "doi": "10.1145/2568225.2568275", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program invariants are important for defect detection, program verification,\nand program repair. However, existing techniques have limited support for\nimportant classes of invariants such as disjunctions, which express the\nsemantics of conditional statements. We propose a method for generating\ndisjunctive invariants over numerical domains, which are inexpressible using\nclassical convex polyhedra. Using dynamic analysis and reformulating the\nproblem in non-standard \"max-plus\" and \"min-plus\" algebras, our method\nconstructs hulls over program trace points. Critically, we introduce and infer\na weak class of such invariants that balances expressive power against the\ncomputational cost of generating nonconvex shapes in high dimensions.\n  Existing dynamic inference techniques often generate spurious invariants that\nfit some program traces but do not generalize. With the insight that generating\ndynamic invariants is easy, we propose to verify these invariants statically\nusing k-inductive SMT theorem proving which allows us to validate invariants\nthat are not classically inductive.\n  Results on difficult kernels involving nonlinear arithmetic and abstract\narrays suggest that this hybrid approach efficiently generates and proves\ncorrect program invariants.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 04:59:57 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Nguyen", "ThanhVu", ""], ["Kapur", "Deepak", ""], ["Weimer", "Westley", ""], ["Forrest", "Stephanie", ""]]}, {"id": "1904.08083", "submitter": "Soichiro Fujii", "authors": "Soichiro Fujii", "title": "A 2-Categorical Study of Graded and Indexed Monads", "comments": "105 pages, master's thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the study of computational effects, it is important to consider the notion\nof computational effects with parameters. The need of such a notion arises\nwhen, for example, statically estimating the range of effects caused by a\nprogram, or studying the ways in which effects with local scopes are derived\nfrom effects with only the global scope. Extending the classical observation\nthat computational effects can be modeled by monads, these computational\neffects with parameters are modeled by various mathematical structures\nincluding graded monads and indexed monads, which are two different\ngeneralizations of ordinary monads. The former has been employed in the\nsemantics of effect systems, whereas the latter in the study of the\nrelationship between the local state monads and the global state monads, each\nexemplifying the two situations mentioned above. However, despite their\nimportance, the mathematical theory of graded and indexed monads is far less\ndeveloped than that of ordinary monads.\n  Here we develop the mathematical theory of graded and indexed monads from a\n2-categorical viewpoint. We first introduce four 2-categories and observe that\nin two of them graded monads are in fact monads in the 2-categorical sense, and\nsimilarly indexed monads are monads in the 2-categorical sense in the other\ntwo. We then construct explicitly the Eilenberg--Moore and the Kleisli objects\nof graded monads, and the Eilenberg--Moore objects of indexed monads in the\nsense of Street in appropriate 2-categories among these four. The corresponding\nresults for graded and indexed comonads also follow.\n  We expect that the current work will provide a theoretical foundation to a\nunified study of computational effects with parameters, or dually (using the\ncomonad variants), of computational resources with parameters, arising for\nexample in Bounded Linear Logic.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 05:11:42 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Fujii", "Soichiro", ""]]}, {"id": "1904.08096", "submitter": "Steffen Smolka", "authors": "Steffen Smolka, Praveen Kumar, David M Kahn, Nate Foster, Justin Hsu,\n  Dexter Kozen, Alexandra Silva", "title": "Scalable Verification of Probabilistic Networks", "comments": "extended version with appendix", "journal-ref": "In Proceedings of the 40th ACM SIGPLAN Conference on Programming\n  Language Design and Implementation (PLDI '19), June 22-26, 2019, Phoenix, AZ,\n  USA. ACM, New York, NY, USA", "doi": "10.1145/3314221.3314639", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents McNetKAT, a scalable tool for verifying probabilistic\nnetwork programs. McNetKAT is based on a new semantics for the guarded and\nhistory-free fragment of Probabilistic NetKAT in terms of finite-state,\nabsorbing Markov chains. This view allows the semantics of all programs to be\ncomputed exactly, enabling construction of an automatic verification tool.\nDomain-specific optimizations and a parallelizing backend enable McNetKAT to\nanalyze networks with thousands of nodes, automatically reasoning about general\nproperties such as probabilistic program equivalence and refinement, as well as\nnetworking properties such as resilience to failures. We evaluate McNetKAT's\nscalability using real-world topologies, compare its performance against\nstate-of-the-art tools, and develop an extended case study on a recently\nproposed data center network design.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 06:12:57 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Smolka", "Steffen", ""], ["Kumar", "Praveen", ""], ["Kahn", "David M", ""], ["Foster", "Nate", ""], ["Hsu", "Justin", ""], ["Kozen", "Dexter", ""], ["Silva", "Alexandra", ""]]}, {"id": "1904.08368", "submitter": "Jared Roesch", "authors": "Jared Roesch, Steven Lyubomirsky, Marisa Kirisame, Logan Weber, Josh\n  Pollock, Luis Vega, Ziheng Jiang, Tianqi Chen, Thierry Moreau, Zachary\n  Tatlock", "title": "Relay: A High-Level Compiler for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frameworks for writing, compiling, and optimizing deep learning (DL) models\nhave recently enabled progress in areas like computer vision and natural\nlanguage processing. Extending these frameworks to accommodate the rapidly\ndiversifying landscape of DL models and hardware platforms presents challenging\ntradeoffs between expressivity, composability, and portability. We present\nRelay, a new compiler framework for DL. Relay's functional, statically typed\nintermediate representation (IR) unifies and generalizes existing DL IRs to\nexpress state-of-the-art models. The introduction of Relay's expressive IR\nrequires careful design of domain-specific optimizations, addressed via Relay's\nextension mechanisms. Using these extension mechanisms, Relay supports a\nunified compiler that can target a variety of hardware platforms. Our\nevaluation demonstrates Relay's competitive performance for a broad class of\nmodels and devices (CPUs, GPUs, and emerging accelerators). Relay's design\ndemonstrates how a unified IR can provide expressivity, composability, and\nportability without compromising performance.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 17:08:23 GMT"}, {"version": "v2", "created": "Sat, 24 Aug 2019 23:30:04 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Roesch", "Jared", ""], ["Lyubomirsky", "Steven", ""], ["Kirisame", "Marisa", ""], ["Weber", "Logan", ""], ["Pollock", "Josh", ""], ["Vega", "Luis", ""], ["Jiang", "Ziheng", ""], ["Chen", "Tianqi", ""], ["Moreau", "Thierry", ""], ["Tatlock", "Zachary", ""]]}, {"id": "1904.08380", "submitter": "Laxman Dhulipala", "authors": "Laxman Dhulipala, Julian Shun, Guy Blelloch", "title": "Low-Latency Graph Streaming Using Compressed Purely-Functional Trees", "comments": "This is the full version of the paper appearing in the ACM SIGPLAN\n  conference on Programming Language Design and Implementation (PLDI), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the dynamic nature of real-world graphs, there has been a growing\ninterest in the graph-streaming setting where a continuous stream of graph\nupdates is mixed with arbitrary graph queries. In principle, purely-functional\ntrees are an ideal choice for this setting due as they enable safe parallelism,\nlightweight snapshots, and strict serializability for queries. However,\ndirectly using them for graph processing would lead to significant space\noverhead and poor cache locality.\n  This paper presents C-trees, a compressed purely-functional search tree data\nstructure that significantly improves on the space usage and locality of\npurely-functional trees. The key idea is to use a chunking technique over trees\nin order to store multiple entries per tree-node. We design\ntheoretically-efficient and practical algorithms for performing batch updates\nto C-trees, and also show that we can store massive dynamic real-world graphs\nusing only a few bytes per edge, thereby achieving space usage close to that of\nthe best static graph processing frameworks.\n  To study the efficiency and applicability of our data structure, we designed\nAspen, a graph-streaming framework that extends the interface of Ligra with\noperations for updating graphs. We show that Aspen is faster than two\nstate-of-the-art graph-streaming systems, Stinger and LLAMA, while requiring\nless memory, and is competitive in performance with the state-of-the-art static\ngraph frameworks, Galois, GAP, and Ligra+. With Aspen, we are able to\nefficiently process the largest publicly-available graph with over two hundred\nbillion edges in the graph-streaming setting using a single commodity multicore\nserver with 1TB of memory.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 17:28:18 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Dhulipala", "Laxman", ""], ["Shun", "Julian", ""], ["Blelloch", "Guy", ""]]}, {"id": "1904.08555", "submitter": "Hal Finkel", "authors": "Hal Finkel, David Poliakoff, David F. Richards", "title": "ClangJIT: Enhancing C++ with Just-in-Time Compilation", "comments": null, "journal-ref": null, "doi": null, "report-no": "LLNL-CONF-772305, APT-151745", "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The C++ programming language is not only a keystone of the\nhigh-performance-computing ecosystem but has proven to be a successful base for\nportable parallel-programming frameworks. As is well known, C++ programmers use\ntemplates to specialize algorithms, thus allowing the compiler to generate\nhighly-efficient code for specific parameters, data structures, and so on. This\ncapability has been limited to those specializations that can be identified\nwhen the application is compiled, and in many critical cases, compiling all\npotentially-relevant specializations is not practical. ClangJIT provides a\nwell-integrated C++ language extension allowing template-based specialization\nto occur during program execution. This capability has been implemented for use\nin large-scale applications, and we demonstrate that\njust-in-time-compilation-based dynamic specialization can be integrated into\napplications, often requiring minimal changes (or no changes) to the\napplications themselves, providing significant performance improvements,\nprogrammer-productivity improvements, and decreased compilation time.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 01:20:26 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2019 12:23:16 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Finkel", "Hal", ""], ["Poliakoff", "David", ""], ["Richards", "David F.", ""]]}, {"id": "1904.08722", "submitter": "Jan Bergstra", "authors": "Jan A. Bergstra", "title": "Quantitative Expressiveness of Instruction Sequence Classes for\n  Computation on Single Bit Registers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of instructions of an instruction sequence is taken for its\nlogical SLOC, and is abbreviated with LLOC. A notion of quantitative\nexpressiveness is based on LLOC and in the special case of operation over a\nfamily of single bit registers a collection of elementary properties are\nestablished. A dedicated notion of interface is developed and is used for\nstating relevant properties of classes of instruction sequences\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 12:20:55 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Bergstra", "Jan A.", ""]]}, {"id": "1904.09041", "submitter": "Yu Zhang", "authors": "Yu Zhang, Haowei Deng, Quanxi Li, Haoze Song and Leihai Nie", "title": "Optimizing Quantum Programs against Decoherence: Delaying Qubits into\n  Quantum Superposition", "comments": "To appear in TASE2019 - the 13th International Symposium on\n  Theoretical Aspects of Software Engineering (submitted on Jan 25, 2019, and\n  this is camera-ready version)", "journal-ref": "13th International Symposium on Theoretical Aspects of Software\n  Engineering(TASE), July 29 - August 1, 2019", "doi": "10.1109/TASE.2019.000-2", "report-no": null, "categories": "quant-ph cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Quantum computing technology has reached a second renaissance in the last\ndecade. However, in the NISQ era pointed out by John Preskill in 2018, quantum\nnoise and decoherence, which affect the accuracy and execution effect of\nquantum programs, cannot be ignored and corrected by the near future NISQ\ncomputers. In order to let users more easily write quantum programs, the\ncompiler and runtime system should consider underlying quantum hardware\nfeatures such as decoherence. To address the challenges posed by decoherence,\nin this paper, we propose and prototype QLifeReducer to minimize the qubit\nlifetime in the input OpenQASM program by delaying qubits into quantum\nsuperposition. QLifeReducer includes three core modules, i.e.,the parser,\nparallelism analyzer and transformer. It introduces the layered bundle format\nto express the quantum program, where a set of parallelizable quantum\noperations is packaged into a bundle. We evaluate quantum programs before and\nafter transformed by QLifeReducer on both real IBM Q 5 Tenerife and the\nself-developed simulator. The experimental results show that QLifeReducer\nreduces the error rate of a quantum program when executed on IBMQ 5 Tenerife by\n11%; and can reduce the longest qubit lifetime as well as average qubit\nlifetime by more than 20% on most quantum workloads.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 23:40:00 GMT"}, {"version": "v2", "created": "Sat, 27 Jul 2019 14:52:42 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Zhang", "Yu", ""], ["Deng", "Haowei", ""], ["Li", "Quanxi", ""], ["Song", "Haoze", ""], ["Nie", "Leihai", ""]]}, {"id": "1904.09429", "submitter": "Peter Breuer", "authors": "Peter T. Breuer", "title": "Chaotic Compilation for Encrypted Computing: Obfuscation but Not in Name", "comments": "31 pages. Version update adds \"Chaotic\" in title and throughout\n  paper, and recasts abstract and Intro and other sections of the text for\n  better access by cryptologists. To the same end it introduces the polynomial\n  time defense argument explicitly in the final section, having now set that\n  denouement out in the abstract and intro", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An `obfuscation' for encrypted computing is quantified exactly here, leading\nto an argument that security against polynomial-time attacks has been achieved\nfor user data via the deliberately `chaotic' compilation required for security\nproperties in that environment. Encrypted computing is the emerging science and\ntechnology of processors that take encrypted inputs to encrypted outputs via\nencrypted intermediate values (at nearly conventional speeds). The aim is to\nmake user data in general-purpose computing secure against the operator and\noperating system as potential adversaries. A stumbling block has always been\nthat memory addresses are data and good encryption means the encrypted value\nvaries randomly, and that makes hitting any target in memory problematic\nwithout address decryption, yet decryption anywhere on the memory path would\nopen up many easily exploitable vulnerabilities. This paper `solves (chaotic)\ncompilation' for processors without address decryption, covering all of ANSI C\nwhile satisfying the required security properties and opening up the field for\nthe standard software tool-chain and infrastructure. That produces the argument\nreferred to above, which may also hold without encryption.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 10:28:51 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 14:55:54 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Breuer", "Peter T.", ""]]}, {"id": "1904.09561", "submitter": "EPTCS", "authors": "Michele Pagani (IRIF, Universit\\'e Paris Diderot, France), Sandra\n  Alves (Porto University)", "title": "Proceedings Twelfth Workshop on Developments in Computational Models and\n  Ninth Workshop on Intersection Types and Related Systems", "comments": null, "journal-ref": "EPTCS 293, 2019", "doi": "10.4204/EPTCS.293", "report-no": null, "categories": "cs.LO cs.CC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains a final and revised selection of papers presented at\nTwelfth Workshop on Developments in Computational Models (DCM 2018) and the\nNinth Workshop on Intersection Types and Related Systems (ITRS 2018), held on\nJuly 8, 2018 in Oxford, in affiliation with FLOC 2018.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 07:53:20 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Pagani", "Michele", "", "IRIF, Universit\u00e9 Paris Diderot, France"], ["Alves", "Sandra", "", "Porto University"]]}, {"id": "1904.09818", "submitter": "Artur Andrzejak", "authors": "Artur Andrzejak, Oliver Wenz, Diego Costa", "title": "One DSL to Rule Them All: IDE-Assisted Code Generation for Agile Data\n  Analysis", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC cs.HC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analysis is at the core of scientific studies, a prominent task that\nresearchers and practitioners typically undertake by programming their own set\nof automated scripts. While there is no shortage of tools and languages\navailable for designing data analysis pipelines, users spend substantial effort\nin learning the specifics of such languages/tools and often design solutions\ntoo project specific to be reused in future studies. Furthermore, users need to\nput further effort into making their code scalable, as parallel implementations\nare typically more complex.\n  We address these problems by proposing an advanced code recommendation tool\nwhich facilitates developing data science scripts. Users formulate their\nintentions in a human-readable Domain Specific Language (DSL) for dataframe\nmanipulation and analysis. The DSL statements can be converted into executable\nPython code during editing. To avoid the need to learn the DSL and increase\nuser-friendliness, our tool supports code completion in mainstream IDEs and\neditors. Moreover, DSL statements can generate executable code for different\ndata analysis frameworks (currently we support Pandas and PySpark). Overall,\nour approach attempts to accelerate programming of common data analysis tasks\nand to facilitate the conversion of the implementations between frameworks.\n  In a preliminary assessment based on a popular data processing tutorial, our\ntool was able to fully cover 9 out of 14 processing steps for Pandas and 10 out\nof 16 for PySpark, while partially covering 4 processing steps for each of the\nframeworks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 13:10:59 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Andrzejak", "Artur", ""], ["Wenz", "Oliver", ""], ["Costa", "Diego", ""]]}, {"id": "1904.09946", "submitter": "Santiago Escobar", "authors": "Fan Yang and Santiago Escobar and Catherine Meadows and Jos\\'e\n  Meseguer", "title": "Strand Spaces with Choice via a Process Algebra Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Roles in cryptographic protocols do not always have a linear execution, but\nmay include choice points causing the protocol to continue along different\npaths. In this paper we address the problem of representing choice in the\nstrand space model of cryptographic protocols, particularly as it is used in\nthe Maude-NPA cryptographic protocol analysis tool. To achieve this goal, we\ndevelop and give formal semantics to a process algebra for cryptographic\nprotocols that supports a rich taxonomy of choice primitives for composing\nstrand spaces. In our taxonomy, deterministic and non-deterministic choices are\nbroken down further. Non-deterministic choice can be either explicit, i.e., one\nof two paths is chosen, or implicit, i.e., the value of a variable is chosen\nnon-deterministically. Likewise, deterministic choice can be either an explicit\nif-then-else choice, i.e., one path is chosen if a predicate is satisfied,\nwhile the other is chosen if it is not, or implicit deterministic choice, i.e.,\nexecution continues only if a certain pattern is matched. We have identified a\nclass of choices which includes finite branching and some cases of infinite\nbranching, which we address in this paper. We provide a bisimulation result\nbetween the expected forwards execution semantics of the new process algebra\nand the original symbolic backwards semantics of Maude-NPA that preserves\nattack reachability. We have fully integrated the process algebra syntax and\nits transformation into strands in Maude-NPA. We illustrate its expressive\npower and naturalness with various examples, and show how it can be effectively\nused in formal analysis. This allows users to write protocols from now on using\nthe process syntax, which is more convenient for expressing choice than the\nstrand space syntax, in which choice can only be specified implicitly, via two\nor more strands that are identical until the choice point.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 16:46:03 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Yang", "Fan", ""], ["Escobar", "Santiago", ""], ["Meadows", "Catherine", ""], ["Meseguer", "Jos\u00e9", ""]]}, {"id": "1904.09959", "submitter": "Greg Anderson", "authors": "Greg Anderson, Shankara Pailoor, Isil Dillig, Swarat Chaudhuri", "title": "Optimization and Abstraction: A Synergistic Approach for Analyzing\n  Neural Network Robustness", "comments": null, "journal-ref": null, "doi": "10.1145/3314221.3314614", "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the notion of local robustness (or robustness for short) has\nemerged as a desirable property of deep neural networks. Intuitively,\nrobustness means that small perturbations to an input do not cause the network\nto perform misclassifications. In this paper, we present a novel algorithm for\nverifying robustness properties of neural networks. Our method synergistically\ncombines gradient-based optimization methods for counterexample search with\nabstraction-based proof search to obtain a sound and ({\\delta}-)complete\ndecision procedure. Our method also employs a data-driven approach to learn a\nverification policy that guides abstract interpretation during proof search. We\nhave implemented the proposed approach in a tool called Charon and\nexperimentally evaluated it on hundreds of benchmarks. Our experiments show\nthat the proposed approach significantly outperforms three state-of-the-art\ntools, namely AI^2 , Reluplex, and Reluval.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 17:21:52 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 15:25:46 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Anderson", "Greg", ""], ["Pailoor", "Shankara", ""], ["Dillig", "Isil", ""], ["Chaudhuri", "Swarat", ""]]}, {"id": "1904.10107", "submitter": "EPTCS", "authors": "Paola Giannini (DiSIT, Universita' del Piemonte Orientale), Marco\n  Servetto (School of Engineering and Computer Science, Victoria University of\n  Wellington), Elena Zucca (DIBRIS, Universita' di Genova)", "title": "A Syntactic Model of Mutation and Aliasing", "comments": "In Proceedings DCM 2018 and ITRS 2018 , arXiv:1904.09561", "journal-ref": "EPTCS 293, 2019, pp. 39-55", "doi": "10.4204/EPTCS.293.4", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, semantic models of imperative languages use an auxiliary\nstructure which mimics memory. In this way, ownership and other encapsulation\nproperties need to be reconstructed from the graph structure of such global\nmemory. We present an alternative \"syntactic\" model where memory is encoded as\npart of the program rather than as a separate resource. This means that\nexecution can be modelled by just rewriting source code terms, as in semantic\nmodels for functional programs. Formally, this is achieved by the block\nconstruct, introducing local variable declarations, which play the role of\nmemory when their initializing expressions have been evaluated. In this way, we\nobtain a language semantics which directly represents at the syntactic level\nconstraints on aliasing, allowing simpler reasoning about related properties.\nTo illustrate this advantage, we consider the issue, widely studied in the\nliterature, of characterizing an isolated portion of memory, which cannot be\nreached through external references. In the syntactic model, closed block\nvalues, called \"capsules\", provide a simple representation of isolated portions\nof memory, and capsules can be safely moved to another location in the memory,\nwithout introducing sharing, by means of \"affine' variables. We prove that the\nsyntactic model can be encoded in the conventional one, hence efficiently\nimplemented.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 00:53:48 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Giannini", "Paola", "", "DiSIT, Universita' del Piemonte Orientale"], ["Servetto", "Marco", "", "School of Engineering and Computer Science, Victoria University of\n  Wellington"], ["Zucca", "Elena", "", "DIBRIS, Universita' di Genova"]]}, {"id": "1904.10800", "submitter": "EPTCS", "authors": "Giulio Guerrieri (University of Bath, Department of Computer Science,\n  Bath, United Kingdom)", "title": "Towards a Semantic Measure of the Execution Time in Call-by-Value\n  lambda-Calculus", "comments": "In Proceedings DCM 2018 and ITRS 2018 , arXiv:1904.09561. arXiv admin\n  note: substantial text overlap with arXiv:1812.10799", "journal-ref": "EPTCS 293, 2019, pp. 57-72", "doi": "10.4204/EPTCS.293.5", "report-no": null, "categories": "cs.LO cs.DM cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the possibility of a semantic account of the execution time\n(i.e. the number of beta-steps leading to the normal form, if any) for the\nshuffling calculus, an extension of Plotkin's call-by-value lambda-calculus.\nFor this purpose, we use a linear logic based denotational model that can be\nseen as a non-idempotent intersection type system: relational semantics. Our\ninvestigation is inspired by similar ones for linear logic proof-nets and\nuntyped call-by-name lambda-calculus. We first prove a qualitative result: a\n(possibly open) term is normalizable for weak reduction (which does not reduce\nunder abstractions) if and only if its interpretation is not empty. We then\nshow that the size of type derivations can be used to measure the execution\ntime. Finally, we show that, differently from the case of linear logic and\ncall-by-name lambda-calculus, the quantitative information enclosed in type\nderivations does not lift to types (i.e. to the interpretation of terms). To\nget a truly semantic measure of execution time in a call-by-value setting, we\nconjecture that a refinement of its syntax and operational semantics is needed.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 00:54:11 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Guerrieri", "Giulio", "", "University of Bath, Department of Computer Science,\n  Bath, United Kingdom"]]}, {"id": "1904.11170", "submitter": "Chao Wang", "authors": "Meng Wu and Chao Wang", "title": "Abstract Interpretation under Speculative Execution", "comments": "updated version after PLDI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing the behavior of a program running on a processor that supports\nspeculative execution is crucial for applications such as execution time\nestimation and side channel detection. Unfortunately, existing static analysis\ntechniques based on abstract interpretation do not model speculative execution\nsince they focus on functional properties of a program while speculative\nexecution does not change the functionality. To fill the gap, we propose a\nmethod to make abstract interpretation sound under speculative execution. There\nare two contributions. First, we introduce the notion of virtual control flow\nto augment instructions that may be speculatively executed and thus affect\nsubsequent instructions. Second, to make the analysis efficient, we propose\noptimizations to handle merges and loops and to safely bound the speculative\nexecution depth. We have implemented and evaluated the proposed method in a\nstatic cache analysis for execution time estimation and side channel detection.\nOur experiments show that the new method, while guaranteed to be sound under\nspeculative execution, outperforms state-of-the-art abstract interpretation\ntechniques that may be unsound.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 06:40:31 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 03:49:36 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Wu", "Meng", ""], ["Wang", "Chao", ""]]}, {"id": "1904.11254", "submitter": "David Kelly", "authors": "David Kelly, Mark Marron, David Clark and Earl T. Barr", "title": "SafeStrings: Representing Strings as Structured Data", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strings are ubiquitous in code. Not all strings are created equal, some\ncontain structure that makes them incompatible with other strings. CSS units\nare an obvious example. Worse, type checkers cannot see this structure: this is\nthe latent structure problem. We introduce SafeStrings to solve this problem\nand expose latent structure in strings. Once visible, operations can leverage\nthis structure to efficiently manipulate it; further, SafeStrings permit the\nestablishment of closure properties. SafeStringsharness the subtyping and\ninheritance mechanics of their host language to create a natural hierarchy of\nstring subtypes. SafeStrings define an elegant programming model over strings:\nthe front end use of a SafeString is clear and uncluttered, with complexity\nconfined inside the definition of a particular SafeString. They are\nlightweight, language-agnostic and deployable, as we demonstrate by\nimplementing SafeStrings in TypeScript. SafeStrings reduce the surface area for\ncross-site scripting, argument selection defects, and they can facilitate\nfuzzing and analysis.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 10:46:41 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Kelly", "David", ""], ["Marron", "Mark", ""], ["Clark", "David", ""], ["Barr", "Earl T.", ""]]}, {"id": "1904.11281", "submitter": "Zeinab Nehai", "authors": "Zeinab Nehai (DILS, UPD7), Fran\\c{c}ois Bobot (DILS)", "title": "Deductive Proof of Ethereum Smart Contracts Using Why3", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bug or error is a common problem that any software or computer program may\nencounter. It can occur from badly writing the program, a typing error or bad\nmemory management. However, errors can become a significant issue if the unsafe\nprogram is used for critical systems. Therefore, formal methods for these kinds\nof systems are greatly required. In this paper, we use a formal language that\nperforms deductive verification on an Ethereum Blockchain application based on\nsmart contracts, which are self-executing digital contracts. Blockchain systems\nmanipulate cryptocurrency and transaction information. Therefore , if a bug\noccurs in the blockchain, serious consequences such as a loss of money can\nhappen. Thus, the aim of this paper is to propose a language dedicated to\ndeductive verification, called Why3, as a new language for writing formal and\nverified smart contracts, thereby avoiding attacks exploiting such contract\nexecution vulnerabilities. We first write a Why3 smart contracts program; next\nwe formulate specifications to be proved as absence of RunTime Error properties\nand functional properties, then we verify the behavior of the program using the\nWhy3 system. Finally we compile the Why3 contracts to the Ethereum Virtual\nMachine (EVM). Moreover, we give a set of generic mathematical statements that\nallows verifying functional properties suited to any type of smart contracts\nholding cryptocurrency, showing that Why3 can be a suitable language to write\nsmart contracts. To illustrate our approach, we describe its application to a\nrealistic industrial use case.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 12:04:16 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 13:55:09 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Nehai", "Zeinab", "", "DILS, UPD7"], ["Bobot", "Fran\u00e7ois", "", "DILS"]]}, {"id": "1904.11327", "submitter": "Saverio Giallorenzo", "authors": "Saverio Giallorenzo, Fabrizio Montesi, Larisa Safina, Stefano Pio\n  Zingaro", "title": "Ephemeral Data Handling in Microservices - Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern application areas for software systems --- like eHealth, the\nInternet-of-Things, and Edge Computing --- data is encoded in heterogeneous,\ntree-shaped data-formats, it must be processed in real-time, and it must be\nephemeral, i.e., not persist in the system. While it is preferable to use a\nquery language to express complex data-handling logic, their typical execution\nengine, a database external from the main application, is unfit in scenarios of\nephemeral data-handling. A better option is represented by integrated query\nframeworks, which benefit from existing development support tools (e.g., syntax\nand type checkers) and execute within the application memory. In this paper, we\npropose one such framework that, for the first time, targets tree-shaped,\ndocument-oriented queries. We formalise an instantiation of MQuery, a sound\nvariant of the widely-used MongoDB query language, which we implemented in the\nJolie language. Jolie programs are microservices, the building blocks of modern\nsoftware systems. Moreover, since Jolie supports native tree data-structures\nand automatic management of heterogeneous data-encodings, we can provide a\nuniform way to use MQuery on any data-format supported by the language. We\npresent a non-trivial use case from eHealth, use it to concretely evaluate our\nmodel, and to illustrate our formalism.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 13:31:33 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Giallorenzo", "Saverio", ""], ["Montesi", "Fabrizio", ""], ["Safina", "Larisa", ""], ["Zingaro", "Stefano Pio", ""]]}, {"id": "1904.11818", "submitter": "Fabian Kunze", "authors": "Yannick Forster, Fabian Kunze", "title": "A certifying extraction with time bounds from Coq to call-by-value\n  $\\lambda$-calculus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a plugin extracting Coq functions of simple polymorphic types to\nthe (untyped) call-by-value $\\lambda$-calculus L. The plugin is implemented in\nthe MetaCoq framework and entirely written in Coq. We provide Ltac tactics to\nautomatically verify the extracted terms w.r.t a logical relation connecting\nCoq functions with correct extractions and time bounds, essentially performing\na certifying translation and running time validation. We provide three case\nstudies: A universal L-term obtained as extraction from the Coq definition of a\nstep-indexed self-interpreter for \\L, a many-reduction from solvability of\nDiophantine equations to the halting problem of L, and a polynomial-time\nsimulation of Turing machines in L.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 13:00:47 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 12:46:20 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Forster", "Yannick", ""], ["Kunze", "Fabian", ""]]}, {"id": "1904.11968", "submitter": "David Wehr", "authors": "David Wehr, Halley Fede, Eleanor Pence, Bo Zhang, Guilherme Ferreira,\n  John Walczyk, Joseph Hughes", "title": "Learning Semantic Vector Representations of Source Code via a Siamese\n  Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of open-source code, coupled with the success of recent\nadvances in deep learning for natural language processing, has given rise to a\npromising new application of machine learning to source code. In this work, we\nexplore the use of a Siamese recurrent neural network model on Python source\ncode to create vectors which capture the semantics of code. We evaluate the\nquality of embeddings by identifying which problem from a programming\ncompetition the code solves. Our model significantly outperforms a\nbag-of-tokens embedding, providing promising results for improving code\nembeddings that can be used in future software engineering tasks.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 17:52:06 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Wehr", "David", ""], ["Fede", "Halley", ""], ["Pence", "Eleanor", ""], ["Zhang", "Bo", ""], ["Ferreira", "Guilherme", ""], ["Walczyk", "John", ""], ["Hughes", "Joseph", ""]]}, {"id": "1904.12137", "submitter": "Ugo Dal Lago", "authors": "Ugo Dal Lago, Francesco Gavazzo, Akira Yoshimizu", "title": "Differential Logical Relations, Part I: The Simply-Typed Case (Long\n  Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new form of logical relation which, in the spirit of metric\nrelations, allows us to assign each pair of programs a quantity measuring their\ndistance, rather than a boolean value standing for their being equivalent. The\nnovelty of differential logical relations consists in measuring the distance\nbetween terms not (necessarily) by a numerical value, but by a mathematical\nobject which somehow reflects the interactive complexity, i.e. the type, of the\ncompared terms. We exemplify this concept in the simply-typed lambda-calculus,\nand show a form of soundness theorem. We also see how ordinary logical\nrelations and metric relations can be seen as instances of differential logical\nrelations. Finally, we show that differential logical relations can be\norganised in a cartesian closed category, contrarily to metric relations, which\nare well-known not to have such a structure, but only that of a monoidal closed\ncategory.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 09:48:36 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Lago", "Ugo Dal", ""], ["Gavazzo", "Francesco", ""], ["Yoshimizu", "Akira", ""]]}, {"id": "1904.12210", "submitter": "Aditya Saligrama", "authors": "Aditya Saligrama, Andrew Shen, Jon Gjengset", "title": "A Practical Analysis of Rust's Concurrency Story", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correct concurrent programs are difficult to write; when multiple threads\nmutate shared data, they may lose writes, corrupt data, or produce erratic\nprogram behavior. While many of the data-race issues with concurrency can be\navoided by the placing of locks throughout the code, these often serialize\nprogram execution, and can significantly slow down performance-critical\napplications. Programmers also make mistakes, and often forget locks in\nless-executed code paths, which leads to programs that misbehave only in rare\nsituations.\n  Rust is a recent programming language from Mozilla that attempts to solve\nthese intertwined issues by detecting data-races at compile time. Rust's type\nsystem encodes a data-structure's ability to be shared between threads in the\ntype system, which in turn allows the compiler to reject programs where threads\ndirectly mutate shared state without locks or other protection mechanisms. In\nthis work, we examine how this aspect of Rust's type system impacts the\ndevelopment and refinement of a concurrent data structure, as well as its\nability to adapt to situations where correctness is guaranteed by lower-level\ninvariants (e.g., in lock-free algorithms) that are not directly expressible in\nthe type system itself. We detail the implementation of a concurrent lock-free\nhashmap in order to describe these traits of the Rust language. Our code is\npublicly available at https://github.com/saligrama/concache and is one of the\nfastest concurrent hashmaps for the Rust language, which leads to mitigating\nbottlenecks in concurrent programs.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 20:37:20 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Saligrama", "Aditya", ""], ["Shen", "Andrew", ""], ["Gjengset", "Jon", ""]]}, {"id": "1904.12501", "submitter": "Mohammad Riyaz Belgaum", "authors": "Safeeullah Soomro, Mohammad Riyaz Belgaum, Zainab Alansari, Mahdi H\n  Miraz", "title": "A Framework for Debugging Java Programs in a Bytecode", "comments": "6 pages, 2018 International Conference on Computing, Electronics &\n  Communications Engineering (iCCECE)", "journal-ref": null, "doi": "10.1109/iCCECOME.2018.8658589", "report-no": "18494083", "categories": "cs.SE cs.FL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the domain of Software Engineering, program analysis and understanding has\nbeen considered to be a very challenging task since decade, as it demands\ndedicated time and efforts. The analysis of source code may occasionally be\ncomparatively easier due to its static nature, however, the back end code\n(Bytecode), especially in terms of Java programming, is complicated to be\nanalysed. In this paper, we present a methodological approach towards\nunderstanding the Bytecode of Java programs. We put forward a framework for the\ndebugging process of Java Bytecode. Furthermore, we discuss the debugging\nprocess of Bytecode understanding from simple to multiple statements with\nregards to data flow analysis. Finally, we present a comparative analysis of\nBytecode along with the simulation of the proposed framework for the debugging\nprocess\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 08:51:01 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Soomro", "Safeeullah", ""], ["Belgaum", "Mohammad Riyaz", ""], ["Alansari", "Zainab", ""], ["Miraz", "Mahdi H", ""]]}, {"id": "1904.13049", "submitter": "John Sarracino", "authors": "John Sarracino, Shraddha Barke, Hila Peleg, Sorin Lerner, Nadia\n  Polikarpova", "title": "Targeted Synthesis for Programming with Data Invariants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Programmers frequently maintain implicit data invariants, which are relations\nbetween different data structures in a program. Traditionally, such invariants\nare manually enforced and checked by programmers. This ad-hoc practice is\ndifficult because the programmer must manually account for all the locations\nand configurations that break an invariant. Moreover, implicit invariants are\nbrittle under code-evolution: when the invariants and data structures change,\nthe programmer must repeat the process of manually repairing all of the code\nlocations where invariants are violated.\n  A much better approach is to introduce data invariants as a language feature\nand rely on language support to maintain invariants. To handle this challenge,\nwe introduce Targeted Synthesis, a technique for integrating data invariants\nwith invariant-agnostic imperative code at compile-time. This technique is\nnontrivial due to the complex structure of both invariant specifications, as\nwell as general imperative code.\n  The key insight is to take a language co-design approach involving both the\nlanguage of data invariants, as well as the imperative language. We leverage\nthis insight to produce two high-level results: first, we support a language\nwith iterators without requiring general quantified reasoning, and second, we\ninfer complicated invariant-preserving patches. We evaluate these claims\nthrough a language termed Spyder, a core calculus of data invariants over\nimperative iterator programs. We evaluate the expressiveness and performance of\nSpyder on a variety of programs inspired by web applications, and we find that\nSpyder efficiently compiles and maintains data invariants.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 04:58:26 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 18:27:17 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Sarracino", "John", ""], ["Barke", "Shraddha", ""], ["Peleg", "Hila", ""], ["Lerner", "Sorin", ""], ["Polikarpova", "Nadia", ""]]}, {"id": "1904.13088", "submitter": "Kaan Gen\\c{c}", "authors": "Kaan Gen\\c{c}, Jake Roemer, Yufan Xu, Michael D. Bond (Ohio State\n  University)", "title": "Dependence-Aware, Unbounded Sound Predictive Race Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data races are a real problem for parallel software, yet hard to detect.\nSound predictive analysis observes a program execution and detects data races\nthat exist in some other, unobserved execution. However, existing predictive\nanalyses miss races because they do not scale to full program executions or do\nnot precisely incorporate data and control dependence.\n  This paper introduces two novel, sound predictive approaches that incorporate\ndata and control dependence and handle full program executions. An evaluation\nusing real, large Java programs shows that these approaches detect more data\nraces than the closest related approaches, thus advancing the state of the art\nin sound predictive race detection.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 07:46:51 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 19:54:33 GMT"}, {"version": "v3", "created": "Wed, 2 Oct 2019 18:42:54 GMT"}, {"version": "v4", "created": "Mon, 14 Jun 2021 04:49:15 GMT"}, {"version": "v5", "created": "Mon, 28 Jun 2021 04:46:36 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Gen\u00e7", "Kaan", "", "Ohio State\n  University"], ["Roemer", "Jake", "", "Ohio State\n  University"], ["Xu", "Yufan", "", "Ohio State\n  University"], ["Bond", "Michael D.", "", "Ohio State\n  University"]]}, {"id": "1904.13338", "submitter": "Eduard Kamburjan", "authors": "Eduard Kamburjan", "title": "Behavioral Program Logic and LAGC Semantics without Continuations\n  (Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Behavioral Program Logic (BPL), a dynamic logic for trace\nproperties that incorporates concepts from behavioral types and allows\nreasoning about non-functional properties within a sequent calculus. BPL uses\nbehavioral modalities [s |- {\\tau} ], to verify statements s against behavioral\nspecifications {\\tau}. Behavioral specifications generalize postconditions and\nbehavioral types. They can be used to specify other static analyses, e.g., data\nflow analyses. This enables deductive reasoning about the results of multiple\nanalyses on the same program, potentially implemented in different formalisms.\nOur calculus for BPL verifies the behavioral specification gradually, as common\nfor behavioral types. This vastly simplifies specification, calculus and\ncomposition of local results. We present a sequent calculus for object-oriented\nactors with futures that integrates a pointer analysis and bridges the gap\nbetween behavioral types and deductive verification. This technical report\nintroduces (1) complete LAGC semantics of a Core Active Object language (CAO)\nwithout continuations (2) Behavioral Program Logic and (3) gives an example for\na behavioral type expressed in Behavioral Program Logic, method types. This\nreport contains the soundness proofs for method types. While the semantics\ncover CAO with suspension, the method types do not, to simplify the\npresentation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 16:09:06 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Kamburjan", "Eduard", ""]]}]