[{"id": "1811.00192", "submitter": "Umang Mathur", "authors": "Umang Mathur, P. Madhusudan, Mahesh Viswanathan", "title": "Decidable Verification of Uninterpreted Programs", "comments": null, "journal-ref": null, "doi": "10.1145/3290359", "report-no": null, "categories": "cs.PL cs.FL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of completely automatically verifying uninterpreted\nprograms---programs that work over arbitrary data models that provide an\ninterpretation for the constants, functions and relations the program uses. The\nverification problem asks whether a given program satisfies a postcondition\nwritten using quantifier-free formulas with equality on the final state, with\nno loop invariants, contracts, etc. being provided. We show that this problem\nis undecidable in general. The main contribution of this paper is a subclass of\nprograms, called coherent programs that admits decidable verification, and can\nbe decided in PSPACE. We then extend this class of programs to classes of\nprograms that are $k$-coherent, where $k \\in \\mathbb{N}$, obtained by\n(automatically) adding $k$ ghost variables and assignments that make them\ncoherent. We also extend the decidability result to programs with recursive\nfunction calls and prove several undecidability results that show why our\nrestrictions to obtain decidability seem necessary.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 02:37:27 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2018 16:26:35 GMT"}, {"version": "v3", "created": "Wed, 7 Nov 2018 20:19:00 GMT"}, {"version": "v4", "created": "Wed, 26 Aug 2020 17:31:13 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Mathur", "Umang", ""], ["Madhusudan", "P.", ""], ["Viswanathan", "Mahesh", ""]]}, {"id": "1811.00624", "submitter": "Michael Kruse", "authors": "Michael Kruse and Hal Finkel", "title": "User-Directed Loop-Transformations in Clang", "comments": "LLVM-HPC Workshop 2018 preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directives for the compiler such as pragmas can help programmers to separate\nan algorithm's semantics from its optimization. This keeps the code\nunderstandable and easier to optimize for different platforms. Simple\ntransformations such as loop unrolling are already implemented in most\nmainstream compilers. We recently submitted a proposal to add generalized loop\ntransformations to the OpenMP standard. We are also working on an\nimplementation in LLVM/Clang/Polly to show its feasibility and usefulness. The\ncurrent prototype allows applying patterns common to matrix-matrix\nmultiplication optimizations.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 20:23:48 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Kruse", "Michael", ""], ["Finkel", "Hal", ""]]}, {"id": "1811.00632", "submitter": "Michael Kruse", "authors": "Michael Kruse and Hal Finkel", "title": "Loop Optimization Framework", "comments": "LCPC'18 preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The LLVM compiler framework supports a selection of loop transformations such\nas vectorization, distribution and unrolling. Each transformation is\ncarried-out by specialized passes that have been developed independently. In\nthis paper we propose an integrated approach to loop optimizations: A single\ndedicated pass that mutates a Loop Structure DAG. Each transformation can make\nuse of a common infrastructure such as dependency analysis, transformation\npreconditions, etc.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 20:58:56 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Kruse", "Michael", ""], ["Finkel", "Hal", ""]]}, {"id": "1811.00890", "submitter": "Maria I. Gorinova", "authors": "Maria I. Gorinova, Andrew D. Gordon, Charles Sutton", "title": "Probabilistic Programming with Densities in SlicStan: Efficient,\n  Flexible and Deterministic", "comments": null, "journal-ref": "Proc. ACM Program. Lang. 3, POPL, Article 35 (January 2019)", "doi": "10.1145/3290348", "report-no": null, "categories": "cs.PL stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stan is a probabilistic programming language that has been increasingly used\nfor real-world scalable projects. However, to make practical inference\npossible, the language sacrifices some of its usability by adopting a block\nsyntax, which lacks compositionality and flexible user-defined functions.\nMoreover, the semantics of the language has been mainly given in terms of\nintuition about implementation, and has not been formalised.\n  This paper provides a formal treatment of the Stan language, and introduces\nthe probabilistic programming language SlicStan --- a compositional,\nself-optimising version of Stan. Our main contributions are: (1) the\nformalisation of a core subset of Stan through an operational density-based\nsemantics; (2) the design and semantics of the Stan-like language SlicStan,\nwhich facilities better code reuse and abstraction through its compositional\nsyntax, more flexible functions, and information-flow type system; and (3) a\nformal, semantic-preserving procedure for translating SlicStan to Stan.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 14:34:34 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Gorinova", "Maria I.", ""], ["Gordon", "Andrew D.", ""], ["Sutton", "Charles", ""]]}, {"id": "1811.01318", "submitter": "Aaron Stump", "authors": "Aaron Stump", "title": "Syntax and Typing for Cedille Core", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document specifies a core version of the type theory implemented in the\nCedille tool. Cedille is a language for dependently typed programming and\ncomputer-checked proof. Cedille can elaborate source programs down to Cedille\nCore, which can be checked in a straightforward way by a small checker (a\nreference implementation included with Cedille is under 1000 lines of Haskell).\nOther tools could also target Cedille Core as an expressive backend type\ntheory. The document describes syntax and typing rules for Cedille Core.\n", "versions": [{"version": "v1", "created": "Sun, 4 Nov 2018 03:49:51 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Stump", "Aaron", ""]]}, {"id": "1811.01457", "submitter": "Viral Shah", "authors": "Michael Innes, Elliot Saba, Keno Fischer, Dhairya Gandhi, Marco\n  Concetto Rudilosso, Neethu Mariya Joy, Tejan Karmali, Avik Pal, Viral Shah", "title": "Fashionable Modelling with Flux", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning as a discipline has seen an incredible surge of interest in\nrecent years due in large part to a perfect storm of new theory, superior\ntooling, renewed interest in its capabilities. We present in this paper a\nframework named Flux that shows how further refinement of the core ideas of\nmachine learning, built upon the foundation of the Julia programming language,\ncan yield an environment that is simple, easily modifiable, and performant. We\ndetail the fundamental principles of Flux as a framework for differentiable\nprogramming, give examples of models that are implemented within Flux to\ndisplay many of the language and framework-level features that contribute to\nits ease of use and high productivity, display internal compiler techniques\nused to enable the acceleration and performance that lies at the heart of Flux,\nand finally give an overview of the larger ecosystem that Flux fits inside of.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 00:32:27 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 15:04:35 GMT"}, {"version": "v3", "created": "Sat, 10 Nov 2018 18:49:35 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Innes", "Michael", ""], ["Saba", "Elliot", ""], ["Fischer", "Keno", ""], ["Gandhi", "Dhairya", ""], ["Rudilosso", "Marco Concetto", ""], ["Joy", "Neethu Mariya", ""], ["Karmali", "Tejan", ""], ["Pal", "Avik", ""], ["Shah", "Viral", ""]]}, {"id": "1811.01670", "submitter": "Valentin Touzeau", "authors": "Claire Ma\\\"iza (VERIMAG - IMAG), Valentin Touzeau (VERIMAG - IMAG),\n  David Monniaux (VERIMAG - IMAG), Jan Reineke", "title": "Fast and exact analysis for LRU caches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For applications in worst-case execution time analysis and in security, it is\ndesirable to statically classify memory accesses into those that result in\ncache hits, and those that result in cache misses. Among cache replacement\npolicies, the least recently used (LRU) policy has been studied the most and is\nconsidered to be the most predictable. The state-of-the-art in LRU cache\nanalysis presents a tradeoff between precision and analysis efficiency: The\nclassical approach to analyzing programs running on LRU caches, an abstract\ninterpretation based on a range abstraction, is very fast but can be imprecise.\nAn exact analysis was recently presented, but, as a last resort, it calls a\nmodel checker, which is expensive. In this paper, we develop an analysis based\non abstract interpretation that comes close to the efficiency of the classical\napproach, while achieving exact classification of all memory accesses as the\nmodel-checking approach. Compared with the model-checking approach we observe\nspeedups of several orders of magnitude. As a secondary contribution we show\nthat LRU cache analysis problems are in general NP-complete.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 13:34:21 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 08:57:26 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Ma\u00efza", "Claire", "", "VERIMAG - IMAG"], ["Touzeau", "Valentin", "", "VERIMAG - IMAG"], ["Monniaux", "David", "", "VERIMAG - IMAG"], ["Reineke", "Jan", ""]]}, {"id": "1811.01740", "submitter": "David Monniaux", "authors": "David Monniaux (VERIMAG - IMAG), Valentin Touzeau (VERIMAG - IMAG)", "title": "On the complexity of cache analysis for different replacement policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern processors use cache memory: a memory access that \"hits\" the cache\nreturns early, while a \"miss\" takes more time. Given a memory access in a\nprogram, cache analysis consists in deciding whether this access is always a\nhit, always a miss, or is a hit or a miss depending on execution. Such an\nanalysis is of high importance for bounding the worst-case execution time of\nsafety-critical real-time programs.There exist multiple possible policies for\nevicting old data from the cache when new data are brought in, and different\npolicies, though apparently similar in goals and performance, may be very\ndifferent from the analysis point of view. In this paper, we explore these\ndifferences from a complexity-theoretical point of view. Specifically, we show\nthat, among the common replacement policies, LRU (Least Recently Used) is the\nonly one whose analysis is NP-complete, whereas the analysis problems for the\nother policies are PSPACE-complete.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 14:38:14 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 09:45:45 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Monniaux", "David", "", "VERIMAG - IMAG"], ["Touzeau", "Valentin", "", "VERIMAG - IMAG"]]}, {"id": "1811.01793", "submitter": "William Waites", "authors": "William Waites, Goksel Misirli, Matteo Cavaliere, Vincent Danos, Anil\n  Wipat", "title": "Compiling Combinatorial Genetic Circuits with Semantic Inference", "comments": null, "journal-ref": null, "doi": "10.1021/acssynbio.8b00201", "report-no": null, "categories": "q-bio.QM cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central strategy of synthetic biology is to understand the basic processes\nof living creatures through engineering organisms using the same building\nblocks. Biological machines described in terms of parts can be studied by\ncomputer simulation in any of several languages or robotically assembled in\nvitro. In this paper we present a language, the Genetic Circuit Description\nLanguage (GCDL) and a compiler, the Genetic Circuit Compiler (GCC). This\nlanguage describes genetic circuits at a level of granularity appropriate both\nfor automated assembly in the laboratory and deriving simulation code. The GCDL\nfollows Semantic Web practice and the compiler makes novel use of the logical\ninference facilities that are therefore available. We present the GCDL and\ncompiler structure as a study of a tool for generating $\\kappa$-language\nsimulations from semantic descriptions of genetic circuits.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 15:31:17 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 11:40:13 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Waites", "William", ""], ["Misirli", "Goksel", ""], ["Cavaliere", "Matteo", ""], ["Danos", "Vincent", ""], ["Wipat", "Anil", ""]]}, {"id": "1811.02091", "submitter": "Dustin Tran", "authors": "Dustin Tran, Matthew Hoffman, Dave Moore, Christopher Suter, Srinivas\n  Vasudevan, Alexey Radul, Matthew Johnson, Rif A. Saurous", "title": "Simple, Distributed, and Accelerated Probabilistic Programming", "comments": "Appears in Neural Information Processing Systems, 2018. Code\n  available at http://bit.ly/2JpFipt", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a simple, low-level approach for embedding probabilistic\nprogramming in a deep learning ecosystem. In particular, we distill\nprobabilistic programming down to a single abstraction---the random variable.\nOur lightweight implementation in TensorFlow enables numerous applications: a\nmodel-parallel variational auto-encoder (VAE) with 2nd-generation tensor\nprocessing units (TPUv2s); a data-parallel autoregressive model (Image\nTransformer) with TPUv2s; and multi-GPU No-U-Turn Sampler (NUTS). For both a\nstate-of-the-art VAE on 64x64 ImageNet and Image Transformer on 256x256\nCelebA-HQ, our approach achieves an optimal linear speedup from 1 to 256 TPUv2\nchips. With NUTS, we see a 100x speedup on GPUs over Stan and 37x over PyMC3.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 23:53:59 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 02:56:29 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Tran", "Dustin", ""], ["Hoffman", "Matthew", ""], ["Moore", "Dave", ""], ["Suter", "Christopher", ""], ["Vasudevan", "Srinivas", ""], ["Radul", "Alexey", ""], ["Johnson", "Matthew", ""], ["Saurous", "Rif A.", ""]]}, {"id": "1811.02133", "submitter": "Thorsten Wissmann", "authors": "Naoki Kobayashi, Ugo Dal Lago, Charles Grellois", "title": "On the Termination Problem for Probabilistic Higher-Order Recursive\n  Programs", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 4 (October\n  2, 2020) lmcs:6817", "doi": "10.23638/LMCS-16(4:2)2020", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last two decades, there has been much progress on model checking of\nboth probabilistic systems and higher-order programs. In spite of the emergence\nof higher-order probabilistic programming languages, not much has been done to\ncombine those two approaches. In this paper, we initiate a study on the\nprobabilistic higher-order model checking problem, by giving some first\ntheoretical and experimental results. As a first step towards our goal, we\nintroduce PHORS, a probabilistic extension of higher-order recursion schemes\n(HORS), as a model of probabilistic higher-order programs. The model of PHORS\nmay alternatively be viewed as a higher-order extension of recursive Markov\nchains. We then investigate the probabilistic termination problem -- or,\nequivalently, the probabilistic reachability problem. We prove that almost sure\ntermination of order-2 PHORS is undecidable. We also provide a fixpoint\ncharacterization of the termination probability of PHORS, and develop a sound\n(but possibly incomplete) procedure for approximately computing the termination\nprobability. We have implemented the procedure for order-2 PHORSs, and\nconfirmed that the procedure works well through preliminary experiments that\nare reported at the end of the article.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 02:40:29 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 06:06:36 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 05:14:43 GMT"}, {"version": "v4", "created": "Thu, 1 Oct 2020 17:01:36 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Kobayashi", "Naoki", ""], ["Lago", "Ugo Dal", ""], ["Grellois", "Charles", ""]]}, {"id": "1811.02300", "submitter": "Gabriel Scherer", "authors": "Simon Colin, Rodolphe Lepigre, Gabriel Scherer", "title": "Unboxing Mutually Recursive Type Definitions in OCaml", "comments": "accepted at JFLA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In modern OCaml, single-argument datatype declarations (variants with a\nsingle constructor, records with a single field) can sometimes be `unboxed'.\nThis means that their memory representation is the same as their single\nargument (omitting the variant or record constructor and an indirection), thus\nachieving better time and memory efficiency.\n  However, in the case of generalized/guarded algebraic datatypes (GADTs),\nunboxing is not always possible due to a subtle assumption about the runtime\nrepresentation of OCaml values. The current correctness check is incomplete,\nrejecting many valid definitions, in particular those involving\nmutually-recursive datatype declarations.\n  In this paper, we explain the notion of separability as a semantic for the\nunboxing criterion, and propose a set of inference rules to check separability.\nFrom these inference rules, we derive a new implementation of the unboxing\ncheck that properly supports mutually-recursive definitions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 11:36:30 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 14:27:44 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Colin", "Simon", ""], ["Lepigre", "Rodolphe", ""], ["Scherer", "Gabriel", ""]]}, {"id": "1811.02440", "submitter": "Max New", "authors": "Max S. New, Daniel R. Licata, Amal Ahmed", "title": "Gradual Type Theory (Extended Version)", "comments": "Extended version of \"Gradual Type Theory\", to appear POPL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gradually typed languages are designed to support both dynamically typed and\nstatically typed programming styles while preserving the benefits of each.\nWhile existing gradual type soundness theorems for these languages aim to show\nthat type-based reasoning is preserved when moving from the fully static\nsetting to a gradual one, these theorems do not imply that correctness of\ntype-based refactorings and optimizations is preserved. Establishing\ncorrectness of program transformations is technically difficult, and is often\nneglected in the metatheory of gradual languages.\n  In this paper, we propose an axiomatic account of program equivalence in a\ngradual cast calculus, which we formalize in a logic we call gradual type\ntheory (GTT). Based on Levy's call-by-push-value, GTT gives an axiomatic\naccount of both call-by-value and call-by-name gradual languages. We then prove\ntheorems that justify optimizations and refactorings in gradually typed\nlanguages. For example, uniqueness principles for gradual type connectives show\nthat if the $\\beta\\eta$ laws hold for a connective, then casts between that\nconnective must be equivalent to the lazy cast semantics. Contrapositively,\nthis shows that eager cast semantics violates the extensionality of function\ntypes. As another example, we show that gradual upcasts are pure and dually,\ngradual downcasts are strict. We show the consistency and applicability of our\ntheory by proving that an implementation using the lazy cast semantics gives a\nlogical relations model of our type theory, where equivalence in GTT implies\ncontextual equivalence of the programs. Since GTT also axiomatizes the dynamic\ngradual guarantee, our model also establishes this central theorem of gradual\ntyping. The model is parametrized by the implementation of the dynamic types,\nand so gives a family of implementations that validate type-based optimization\nand the gradual guarantee.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 15:44:07 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["New", "Max S.", ""], ["Licata", "Daniel R.", ""], ["Ahmed", "Amal", ""]]}, {"id": "1811.02605", "submitter": "Ronald Caplan", "authors": "R. M. Caplan, J. A. Linker, Z. Miki\\'c, C. Downs, T. T\\\"or\\\"ok, and V.\n  S. Titov", "title": "GPU Acceleration of an Established Solar MHD Code using OpenACC", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": "10.1088/1742-6596/1225/1/012012", "report-no": null, "categories": "physics.comp-ph astro-ph.SR cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GPU accelerators have had a notable impact on high-performance computing\nacross many disciplines. They provide high performance with low cost/power, and\ntherefore have become a primary compute resource on many of the largest\nsupercomputers. Here, we implement multi-GPU acceleration into our Solar MHD\ncode (MAS) using OpenACC in a fully portable, single-source manner. Our\npreliminary implementation is focused on MAS running in a reduced physics\n\"zero-beta\" mode. While valuable on its own, our main goal is to pave the way\nfor a full physics, thermodynamic MHD implementation. We describe the OpenACC\nimplementation methodology and challenges. \"Time-to-solution\" performance\nresults of a production-level flux rope eruption simulation on multi-CPU and\nmulti-GPU systems are shown. We find that the GPU-accelerated MAS code has the\nability to run \"zero-beta\" simulations on a single multi-GPU server at speeds\npreviously requiring multiple CPU server-nodes of a supercomputer.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 19:30:50 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Caplan", "R. M.", ""], ["Linker", "J. A.", ""], ["Miki\u0107", "Z.", ""], ["Downs", "C.", ""], ["T\u00f6r\u00f6k", "T.", ""], ["Titov", "V. S.", ""]]}, {"id": "1811.02787", "submitter": "Dominique Devriese", "authors": "Lau Skorstengaard, Dominique Devriese, Lars Birkedal", "title": "StkTokens: Enforcing Well-bracketed Control Flow and Stack Encapsulation\n  using Linear Capabilities - Technical Report with Proofs and Details", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose and study StkTokens: a new calling convention that provably\nenforces well-bracketed control flow and local state encapsulation on a\ncapability machine. The calling convention is based on linear capabilities: a\ntype of capabilities that are prevented from being duplicated by the hardware.\nIn addition to designing and formalizing this new calling convention, we also\ncontribute a new way to formalize and prove that it effectively enforces\nwell-bracketed control flow and local state encapsulation using what we call a\nfully abstract overlay semantics.\n  This document is a technical report accompanying a paper by the same title\nand authors, published at POPL 2019. It contains proofs and details that were\nomitted from the paper for space and presentation reasons.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 07:52:30 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Skorstengaard", "Lau", ""], ["Devriese", "Dominique", ""], ["Birkedal", "Lars", ""]]}, {"id": "1811.03479", "submitter": "Conrad Watt", "authors": "Conrad Watt, Petar Maksimovi\\'c, Neelakantan R. Krishnaswami, Philippa\n  Gardner", "title": "A Program Logic for First-Order Encapsulated WebAssembly", "comments": "50 pages, 17 figures", "journal-ref": null, "doi": "10.4230/LIPIcs.ECOOP.2019.9", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Wasm Logic, a sound program logic for first-order, encapsulated\nWebAssembly. We design a novel assertion syntax, tailored to WebAssembly's\nstack-based semantics and the strong guarantees given by WebAssembly's type\nsystem, and show how to adapt the standard separation logic triple and proof\nrules in a principled way to capture WebAssembly's uncommon structured control\nflow. Using Wasm Logic, we specify and verify a simple WebAssembly B-tree\nlibrary, giving abstract specifications independent of the underlying\nimplementation. We mechanise Wasm Logic and its soundness proof in full in\nIsabelle/HOL. As part of the soundness proof, we formalise and fully mechanise\na novel, big-step semantics of WebAssembly, which we prove equivalent, up to\ntransitive closure, to the original WebAssembly small-step semantics. Wasm\nLogic is the first program logic for WebAssembly, and represents a first step\ntowards the creation of static analysis tools for WebAssembly.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 15:04:20 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 05:53:29 GMT"}, {"version": "v3", "created": "Sat, 13 Jul 2019 20:54:07 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Watt", "Conrad", ""], ["Maksimovi\u0107", "Petar", ""], ["Krishnaswami", "Neelakantan R.", ""], ["Gardner", "Philippa", ""]]}, {"id": "1811.03503", "submitter": "Ilya Sergey", "authors": "Nikos Gorogiannis, Peter W. O'Hearn, Ilya Sergey", "title": "A True Positives Theorem for a Static Race Detector - Extended Version", "comments": null, "journal-ref": "Proc. ACM Program. Lang. 3, POPL, Article 57 (January 2019)", "doi": "10.1145/3290370", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  RacerD is a static race detector that has been proven to be effective in\nengineering practice: it has seen thousands of data races fixed by developers\nbefore reaching production, and has supported the migration of Facebook's\nAndroid app rendering infrastructure from a single-threaded to a multi-threaded\narchitecture. We prove a True Positives Theorem stating that, under certain\nassumptions, an idealized theoretical version of the analysis never reports a\nfalse positive. We also provide an empirical evaluation of an implementation of\nthis analysis, versus the original RacerD.\n  The theorem was motivated in the first case by the desire to understand the\nobservation from production that RacerD was providing remarkably accurate\nsignal to developers, and then the theorem guided further analyzer design\ndecisions. Technically, our result can be seen as saying that the analysis\ncomputes an under-approximation of an over-approximation, which is the reverse\nof the more usual (over of under) situation in static analysis. Until now,\nstatic analyzers that are effective in practice but unsound have often been\nregarded as ad hoc; in contrast, we suggest that, in the future, theorems of\nthis variety might be generally useful in understanding, justifying and\ndesigning effective static analyses for bug catching.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 15:44:59 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Gorogiannis", "Nikos", ""], ["O'Hearn", "Peter W.", ""], ["Sergey", "Ilya", ""]]}, {"id": "1811.03585", "submitter": "Kesha Hietala", "authors": "Shih-Han Hung and Kesha Hietala and Shaopeng Zhu and Mingsheng Ying\n  and Michael Hicks and Xiaodi Wu", "title": "Quantitative Robustness Analysis of Quantum Programs (Extended Version)", "comments": "34 pages, LaTeX; v2: fixed typos", "journal-ref": null, "doi": "10.1145/3290344", "report-no": null, "categories": "cs.PL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum computation is a topic of significant recent interest, with practical\nadvances coming from both research and industry. A major challenge in quantum\nprogramming is dealing with errors (quantum noise) during execution. Because\nquantum resources (e.g., qubits) are scarce, classical error correction\ntechniques applied at the level of the architecture are currently\ncost-prohibitive. But while this reality means that quantum programs are almost\ncertain to have errors, there as yet exists no principled means to reason about\nerroneous behavior. This paper attempts to fill this gap by developing a\nsemantics for erroneous quantum while-programs, as well as a logic for\nreasoning about them. This logic permits proving a property we have identified,\ncalled $\\epsilon$-robustness, which characterizes possible \"distance\" between\nan ideal program and an erroneous one. We have proved the logic sound, and\nshowed its utility on several case studies, notably: (1) analyzing the\nrobustness of noisy versions of the quantum Bernoulli factory (QBF) and quantum\nwalk (QW); (2) demonstrating the (in)effectiveness of different error\ncorrection schemes on single-qubit errors; and (3) analyzing the robustness of\na fault-tolerant version of QBF.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 18:12:45 GMT"}, {"version": "v2", "created": "Sat, 1 Dec 2018 16:28:59 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Hung", "Shih-Han", ""], ["Hietala", "Kesha", ""], ["Zhu", "Shaopeng", ""], ["Ying", "Mingsheng", ""], ["Hicks", "Michael", ""], ["Wu", "Xiaodi", ""]]}, {"id": "1811.03678", "submitter": "Jacques Carette", "authors": "Jacques Carette, Roshan P. James, Amr Sabry", "title": "Embracing the Laws of Physics: Three Reversible Models of Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO math.CT quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our main models of computation (the Turing Machine and the RAM) make\nfundamental assumptions about which primitive operations are realizable. The\nconsensus is that these include logical operations like conjunction,\ndisjunction and negation, as well as reading and writing to memory locations.\nThis perspective conforms to a macro-level view of physics and indeed these\noperations are realizable using macro-level devices involving thousands of\nelectrons. This point of view is however incompatible with quantum mechanics,\nor even elementary thermodynamics, as both imply that information is a\nconserved quantity of physical processes, and hence of primitive computational\noperations.\n  Our aim is to re-develop foundational computational models that embraces the\nprinciple of conservation of information. We first define what conservation of\ninformation means in a computational setting. We emphasize that computations\nmust be reversible transformations on data. One can think of data as modeled\nusing topological spaces and programs as modeled by reversible deformations. We\nillustrate this idea using three notions of data. The first assumes\nunstructured finite data, i.e., discrete topological spaces. The corresponding\nnotion of reversible computation is that of permutations. We then consider a\nstructured notion of data based on the Curry-Howard correspondence; here\nreversible deformations, as a programming language for witnessing type\nisomorphisms, comes from proof terms for commutative semirings. We then \"move\nup a level\" to treat programs as data. The corresponding notion of reversible\nprograms equivalences comes from the \"higher dimensional\" analog to commutative\nsemirings: symmetric rig groupoids. The coherence laws for these are exactly\nthe program equivalences we seek.\n  We conclude with some generalizations inspired by homotopy type theory and\nsurvey directions for further research.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 21:03:18 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 19:22:25 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Carette", "Jacques", ""], ["James", "Roshan P.", ""], ["Sabry", "Amr", ""]]}, {"id": "1811.04196", "submitter": "Ohad Kammar", "authors": "Matthijs V\\'ak\\'ar, Ohad Kammar, and Sam Staton", "title": "A Domain Theory for Statistical Probabilistic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We give an adequate denotational semantics for languages with recursive\nhigher-order types, continuous probability distributions, and soft constraints.\nThese are expressive languages for building Bayesian models of the kinds used\nin computational statistics and machine learning. Among them are untyped\nlanguages, similar to Church and WebPPL, because our semantics allows recursive\nmixed-variance datatypes. Our semantics justifies important program\nequivalences including commutativity.\n  Our new semantic model is based on `quasi-Borel predomains'. These are a\nmixture of chain-complete partial orders (cpos) and quasi-Borel spaces.\nQuasi-Borel spaces are a recent model of probability theory that focuses on\nsets of admissible random elements. Probability is traditionally treated in cpo\nmodels using probabilistic powerdomains, but these are not known to be\ncommutative on any class of cpos with higher order functions. By contrast,\nquasi-Borel predomains do support both a commutative probabilistic powerdomain\nand higher-order functions. As we show, quasi-Borel predomains form both a\nmodel of Fiore's axiomatic domain theory and a model of Kock's synthetic\nmeasure theory.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 05:09:54 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 17:09:32 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["V\u00e1k\u00e1r", "Matthijs", ""], ["Kammar", "Ohad", ""], ["Staton", "Sam", ""]]}, {"id": "1811.04626", "submitter": "Phillip Stanley-Marbell", "authors": "Jonathan Lim and Phillip Stanley-Marbell", "title": "Newton: A Language for Describing Physics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL eess.SP physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces Newton, a specification language for notating the\nanalytic form, units of measure, and sensor signal properties for\nphysical-object-specific invariants and general physical laws. We designed\nNewton to provide a means for hardware designers (e.g., sensor integrated\ncircuit manufacturers, computing hardware architects, or mechanical engineers)\nto specify properties of the physical environments in which embedded computing\nsystems will be deployed (e.g., a sensing platform deployed on a bridge versus\nworn by a human). Compilers and other program analysis tools for embedded\nsystems can use a library interface to the Newton compiler to obtain\ninformation about the sensors, sensor signals, and inter-signal relationships\nimposed by the structure and materials properties of a given physical system.\nThe information encoded within Newton specifications could enable new\ncompile-time transformations that exploit information about the physical world.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 09:47:57 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Lim", "Jonathan", ""], ["Stanley-Marbell", "Phillip", ""]]}, {"id": "1811.05175", "submitter": "Kensen Shi", "authors": "Kensen Shi, Jacob Steinhardt, Percy Liang", "title": "FrAngel: Component-Based Synthesis with Control Structures", "comments": "30 pages, 12 figures, to appear in the 46th ACM SIGPLAN Symposium on\n  Principles of Programming Languages (POPL 2019)", "journal-ref": null, "doi": "10.1145/3290386", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In component-based program synthesis, the synthesizer generates a program\ngiven a library of components (functions). Existing component-based\nsynthesizers have difficulty synthesizing loops and other control structures,\nand they often require formal specifications of the components, which can be\nexpensive to generate. We present FrAngel, a new approach to component-based\nsynthesis that can synthesize short Java functions with control structures when\ngiven a desired signature, a set of input-output examples, and a collection of\nlibraries (without formal specifications). FrAngel aims to discover programs\nwith many distinct behaviors by combining two main ideas. First, it mines code\nfragments from partially-successful programs that only pass some of the\nexamples. These extracted fragments are often useful for synthesis due to a\nproperty that we call special-case similarity. Second, FrAngel uses angelic\nconditions as placeholders for control structure conditions and optimistically\nevaluates the resulting program sketches. Angelic conditions decompose the\nsynthesis process: FrAngel first finds promising partial programs and later\nfills in their missing conditions. We demonstrate that FrAngel can synthesize a\nvariety of interesting programs with combinations of control structures within\nseconds, significantly outperforming prior state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 09:27:09 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 08:52:16 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Shi", "Kensen", ""], ["Steinhardt", "Jacob", ""], ["Liang", "Percy", ""]]}, {"id": "1811.05307", "submitter": "Takuma Imamura", "authors": "Takuma Imamura", "title": "On the extreme power of nonstandard programming languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suenaga and Hasuo introduced a nonstandard programming language ${\\bf\nWhile}^{{\\bf dt}}$ which models hybrid systems. We demonstrate why ${\\bf\nWhile}^{{\\bf dt}}$ is not suitable for modeling actual computations.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 07:56:04 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 15:49:24 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Imamura", "Takuma", ""]]}, {"id": "1811.05447", "submitter": "Yipeng Huang", "authors": "Yipeng Huang and Margaret Martonosi", "title": "QDB: From Quantum Algorithms Towards Correct Quantum Programs", "comments": null, "journal-ref": "http://drops.dagstuhl.de/opus/volltexte/2019/10196/", "doi": "10.4230/OASIcs.PLATEAU.2018.4", "report-no": null, "categories": "cs.PL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of small-scale prototype quantum computers, researchers can\nnow code and run quantum algorithms that were previously proposed but not fully\nimplemented. In support of this growing interest in quantum computing\nexperimentation, programmers need new tools and techniques to write and debug\nQC code. In this work, we implement a range of QC algorithms and programs in\norder to discover what types of bugs occur and what defenses against those bugs\nare possible in QC programs. We conduct our study by running small-sized QC\nprograms in QC simulators in order to replicate published results in QC\nimplementations. Where possible, we cross-validate results from programs\nwritten in different QC languages for the same problems and inputs. Drawing on\nthis experience, we provide a taxonomy for QC bugs, and we propose QC language\nfeatures that would aid in writing correct code.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 18:32:49 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 17:56:00 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Huang", "Yipeng", ""], ["Martonosi", "Margaret", ""]]}, {"id": "1811.05536", "submitter": "Saverio Perugini", "authors": "Brandon M. Williams and Saverio Perugini", "title": "Staging Human-computer Dialogs: An Application of the Futamura\n  Projections", "comments": "13 pages, 6 figures, and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL cs.HC cs.SC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate an application of the Futamura Projections to human-computer\ninteraction, and particularly to staging human-computer dialogs. Specifically,\nby providing staging analogs to the classical Futamura Projections, we\ndemonstrate that the Futamura Projections can be applied to the staging of\nhuman-computer dialogs in addition to the execution of programs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 21:45:20 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Williams", "Brandon M.", ""], ["Perugini", "Saverio", ""]]}, {"id": "1811.05965", "submitter": "Eli Sennesh", "authors": "Eli Sennesh, Adam \\'Scibior, Hao Wu, Jan-Willem van de Meent", "title": "Composing Modeling and Inference Operations with Probabilistic Program\n  Combinators", "comments": "Published at the NeurIPS workshop \"All of Bayesian Nonparametrics\n  (Especially the Useful Bits)\" 2018\n  (https://sites.google.com/view/nipsbnp2018/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic programs with dynamic computation graphs can define measures\nover sample spaces with unbounded dimensionality, which constitute programmatic\nanalogues to Bayesian nonparametrics. Owing to the generality of this model\nclass, inference relies on `black-box' Monte Carlo methods that are often not\nable to take advantage of conditional independence and exchangeability, which\nhave historically been the cornerstones of efficient inference. We here seek to\ndevelop a `middle ground' between probabilistic models with fully dynamic and\nfully static computation graphs. To this end, we introduce a combinator library\nfor the Probabilistic Torch framework. Combinators are functions that accept\nmodels and return transformed models. We assume that models are dynamic, but\nthat model composition is static, in the sense that combinator application\ntakes place prior to evaluating the model on data. Combinators provide\nprimitives for both model and inference composition. Model combinators take the\nform of classic functional programming constructs such as map and reduce. These\nconstructs define a computation graph at a coarsened level of representation,\nin which nodes correspond to models, rather than individual variables.\nInference combinators implement operations such as importance resampling and\napplication of a transition kernel, which alter the evaluation strategy for a\nmodel whilst preserving proper weighting. Owing to this property, models\ndefined using combinators can be trained using stochastic methods that optimize\neither variational or wake-sleep style objectives. As a validation of this\nprinciple, we use combinators to implement black box inference for hidden\nMarkov models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 18:53:28 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 14:16:04 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 01:05:58 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Sennesh", "Eli", ""], ["\u015acibior", "Adam", ""], ["Wu", "Hao", ""], ["van de Meent", "Jan-Willem", ""]]}, {"id": "1811.06069", "submitter": "Mario Alvarez-Picallo", "authors": "Mario Alvarez-Picallo, Alex Eyers-Taylor, Michael Peyton Jones, C.-H.\n  Luke Ong", "title": "Fixing Incremental Computation: Derivatives of Fixpoints, and the\n  Recursive Semantics of Datalog", "comments": "34 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental computation has recently been studied using the concepts of\nchange structures and derivatives of programs, where the derivative of a\nfunction allows updating the output of the function based on a change to its\ninput.\n  We generalise change structures to change actions, and study their algebraic\nproperties. We develop change actions for common structures in computer\nscience, including directed-complete partial orders and Boolean algebras.\n  We then show how to compute derivatives of fixpoints. This allows us to\nperform incremental evaluation and maintenance of recursively defined functions\nwith particular application to generalised Datalog programs.\n  Moreover, unlike previous results, our techniques are modular in that they\nare easy to apply both to variants of Datalog and to other programming\nlanguages.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 21:13:57 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 00:48:04 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Alvarez-Picallo", "Mario", ""], ["Eyers-Taylor", "Alex", ""], ["Jones", "Michael Peyton", ""], ["Ong", "C. -H. Luke", ""]]}, {"id": "1811.06150", "submitter": "David Moore", "authors": "Dave Moore and Maria I. Gorinova", "title": "Effect Handling for Composable Program Transformations in Edward2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algebraic effects and handlers have emerged in the programming languages\ncommunity as a convenient, modular abstraction for controlling computational\neffects. They have found several applications including concurrent programming,\nmeta programming, and more recently, probabilistic programming, as part of\nPyro's Poutines library. We investigate the use of effect handlers as a\nlightweight abstraction for implementing probabilistic programming languages\n(PPLs). We interpret the existing design of Edward2 as an accidental\nimplementation of an effect-handling mechanism, and extend that design to\nsupport nested, composable transformations. We demonstrate that this enables\nstraightforward implementation of sophisticated model transformations and\ninference algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 02:51:29 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Moore", "Dave", ""], ["Gorinova", "Maria I.", ""]]}, {"id": "1811.07142", "submitter": "Zeinab Ganjei", "authors": "Zeinab Ganjei, Ahmed Rezine, Ludovic Henrio, Petru Eles, Zebo Peng", "title": "On Reachability in Parameterized Phaser Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of statically checking safety properties (such as\nassertions or deadlocks) for parameterized phaser programs. Phasers embody a\nnon-trivial and modern synchronization construct used to orchestrate executions\nof parallel tasks. This generic construct supports dynamic parallelism with\nruntime registrations and deregistrations of spawned tasks. It generalizes many\nsynchronization patterns such as collective and point-to-point schemes. For\ninstance, phasers can enforce barriers or producer-consumer synchronization\npatterns among all or subsets of the running tasks. We consider in this work\nprograms that may generate arbitrarily many tasks and phasers. We study\ndifferent formulations of the verification problem and propose an exact\nprocedure that is guaranteed to terminate for some reachability problems even\nin the presence of unbounded phases and arbitrarily many spawned tasks. In\naddition, we prove undecidability results for several problems on which our\nprocedure cannot be guaranteed to terminate.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 10:39:33 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 21:12:59 GMT"}, {"version": "v3", "created": "Wed, 12 May 2021 09:51:41 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Ganjei", "Zeinab", ""], ["Rezine", "Ahmed", ""], ["Henrio", "Ludovic", ""], ["Eles", "Petru", ""], ["Peng", "Zebo", ""]]}, {"id": "1811.07332", "submitter": "Taro Sekiyama", "authors": "Taro Sekiyama and Atsushi Igarashi", "title": "Handling polymorphic algebraic effects", "comments": "Added the errata for the ESOP'19 paper (page 28)", "journal-ref": "Proc. of ESOP 2019: 353-380", "doi": "10.1007/978-3-030-17184-1_13", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algebraic effects and handlers are a powerful abstraction mechanism to\nrepresent and implement control effects. In this work, we study their extension\nwith parametric polymorphism that allows abstracting not only expressions but\nalso effects and handlers. Although polymorphism makes it possible to reuse and\nreason about effect implementations more effectively, it has long been known\nthat a naive combination of polymorphic effects and let-polymorphism breaks\ntype safety. Although type safety can often be gained by restricting let-bound\nexpressions---e.g., by adopting value restriction or weak polymorphism---we\npropose a complementary approach that restricts handlers instead of let-bound\nexpressions. Our key observation is that, informally speaking, a handler is\nsafe if resumptions from the handler do not interfere with each other. To\nformalize our idea, we define a call-by-value lambda calculus that supports\nlet-polymorphism and polymorphic algebraic effects and handlers, design a type\nsystem that rejects interfering handlers, and prove type safety of our\ncalculus.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 14:13:36 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 04:35:04 GMT"}, {"version": "v3", "created": "Wed, 15 Jan 2020 13:14:13 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Sekiyama", "Taro", ""], ["Igarashi", "Atsushi", ""]]}, {"id": "1811.07340", "submitter": "Samir Genaim", "authors": "Amir M. Ben-Amram, Jes\\'us J. Dom\\'enech and Samir Genaim", "title": "Multiphase-Linear Ranking Functions and their Relation to Recurrent Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiphase ranking functions (M$\\Phi$RFs) are tuples $\\langle f_1,\\ldots,f_d\n\\rangle$ of linear functions that are often used to prove termination of loops\nin which the computation progresses through a number of \"phases\". Our work\nprovides new insights regarding such functions for loops described by a\nconjunction of linear constraints (Single-Path Constraint loops). The decision\nproblem existence of a M$\\Phi$RF asks to determine whether a given SLC loop\nadmits a M$\\Phi$RF, and the corresponding bounded decision problem restricts\nthe search to M$\\Phi$RFs of depth $d$, where the parameter $d$ is part of the\ninput. The algorithmic and complexity aspects of the bounded problem have been\ncompletely settled in a recent work. In this paper we make progress regarding\nthe existence problem, without a given depth bound. In particular, we present\nan approach that reveals some important insights into the structure of these\nfunctions. Interestingly, it relates the problem of seeking M$\\Phi$RFs to that\nof seeking recurrent sets (used to prove non-termination). It also helps in\nidentifying classes of loops for which M$\\Phi$RFs are sufficient. Our research\nhas led to a new representation for single-path loops, the difference\npolyhedron replacing the customary transition polyhedron. This representation\nyields new insights on M$\\Phi$RFs and SLC loops in general. For example, a\nresult on bounded SLC loops becomes straightforward.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 15:34:16 GMT"}, {"version": "v2", "created": "Sun, 13 Jan 2019 22:16:02 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Ben-Amram", "Amir M.", ""], ["Dom\u00e9nech", "Jes\u00fas J.", ""], ["Genaim", "Samir", ""]]}, {"id": "1811.08134", "submitter": "Gabriel Scherer", "authors": "Alban Reynaud, Gabriel Scherer, Jeremy Yallop", "title": "A Practical Mode System for Recursive Definitions", "comments": "Author version of POPL'21 article. 29 pages + Appendices", "journal-ref": null, "doi": "10.1145/3434326", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In call-by-value languages, some mutually-recursive value definitions can be\nsafely evaluated to build recursive functions or cyclic data structures, but\nsome definitions (let rec x = x + 1) contain vicious circles and their\nevaluation fails at runtime. We propose a new static analysis to check the\nabsence of such runtime failures.\n  We present a set of declarative inference rules, prove its soundness with\nrespect to the reference source-level semantics of Nordlander, Carlsson, and\nGill (2008), and show that it can be (right-to-left) directed into an\nalgorithmic check in a surprisingly simple way.\n  Our implementation of this new check replaced the existing check used by the\nOCaml programming language, a fragile syntactic/grammatical criterion which let\nseveral subtle bugs slip through as the language kept evolving. We document\nsome issues that arise when advanced features of a real-world functional\nlanguage (exceptions in first-class modules, GADTs, etc.) interact with safety\nchecking for recursive definitions.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 09:14:29 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 16:34:28 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 13:02:33 GMT"}, {"version": "v4", "created": "Wed, 23 Dec 2020 13:54:05 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Reynaud", "Alban", ""], ["Scherer", "Gabriel", ""], ["Yallop", "Jeremy", ""]]}, {"id": "1811.09014", "submitter": "EPTCS", "authors": "Paolo Masci (HASLab/INESC TEC and Universidade do Minho, Portugal.),\n  Rosemary Monahan (Maynooth University, Ireland), Virgile Prevosto (Software\n  Safety and Security Lab, France)", "title": "Proceedings 4th Workshop on Formal Integrated Development Environment", "comments": null, "journal-ref": "EPTCS 284, 2018", "doi": "10.4204/EPTCS.284", "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of F-IDE 2018, the fourth international\nworkshop on Formal Integrated Development Environment, which was held as a FLoC\n2018 satellite event, on July 14, 2018, in Oxford, England.\n  High levels of safety, security and also privacy standards require the use of\nformal methods to specify and develop compliant software (sub)systems. Any\nstandard comes with an assessment process, which requires a complete\ndocumentation of the application in order to ease the justification of design\nchoices and the review of code and proofs. Thus tools are needed for handling\nspecifications, program constructs and verification artifacts. The aim of the\nF-IDE workshop is to provide a forum for presenting and discussing research\nefforts as well as experience returns on design, development and usage of\nformal IDE aiming at making formal methods \"easier\" for both specialists and\nnon-specialists.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 04:00:38 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Masci", "Paolo", "", "HASLab/INESC TEC and Universidade do Minho, Portugal."], ["Monahan", "Rosemary", "", "Maynooth University, Ireland"], ["Prevosto", "Virgile", "", "Software\n  Safety and Security Lab, France"]]}, {"id": "1811.09143", "submitter": "Simon Doherty", "authors": "Simon Doherty, Brijesh Dongol, Heike Wehrheim, John Derrick", "title": "Verifying C11 Programs Operationally", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops an operational semantics for a release-acquire fragment\nof the C11 memory model with relaxed accesses. We show that the semantics is\nboth sound and complete with respect to the axiomatic model. The semantics\nrelies on a per-thread notion of observability, which allows one to reason\nabout a weak memory C11 program in program order. On top of this, we develop a\nproof calculus for invariant-based reasoning, which we use to verify the\nrelease-acquire version of Peterson's mutual exclusion algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 12:47:18 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Doherty", "Simon", ""], ["Dongol", "Brijesh", ""], ["Wehrheim", "Heike", ""], ["Derrick", "John", ""]]}, {"id": "1811.09303", "submitter": "Edward Givelberg", "authors": "Edward Givelberg", "title": "Object-oriented design for massively parallel computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define an abstract framework for object-oriented programming and show that\nobject-oriented languages, such as C++, can be interpreted as parallel\nprogramming languages. Parallel C++ code is typically more than ten times\nshorter than the equivalent C++ code with MPI. The large reduction in the\nnumber of lines of code in parallel C++ is primarily due to the fact that\ncoordination of concurrency, and the communications instructions, including\npacking and unpacking of messages, are automatically generated in the\nimplementation of object operations. We implemented a prototype of a compiler\nand a runtime system for parallel C++ and used them to create complex\ndata-intensive and HPC applications. These results indicate that adoption of\nthe parallel object-oriented framework has the potential to drastically reduce\nthe cost of parallel programming. We also show that standard sequential\nobject-oriented programs can be ported to parallel architecture, parallelized\nautomatically, and potentially sped up. The parallel object-oriented framework\nenables an implementation of a compiler with a dedicated backend for the\ninterconnect fabric, which exposes the network hardware features directly to\nthe application. We discuss the potential implications for computer\narchitecture.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 20:46:34 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 18:55:36 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Givelberg", "Edward", ""]]}, {"id": "1811.09840", "submitter": "Ivano Salvo", "authors": "Ivano Salvo and Agnese Pacifico", "title": "Three Euler's Sieves and a Fast Prime Generator (Functional Pearl)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Euler's Sieve refines the Sieve of Eratosthenes to compute prime numbers,\nby crossing off each non prime number just once. Euler's Sieve is considered\nhard to be faithfully and efficiently coded as a purely functional stream based\nprogram. We propose three Haskell programs implementing the Euler's Sieve, all\nbased on the idea of generating just once each composite to be crossed off.\nTheir faithfulness with respect to the Euler's Sieve is up to costly stream\nunions imposed by the sequential nature of streams. Our programs outperform\nclassical naive stream based prime generators such as trial division, but they\nare asymptotically worse than the O'Neill `faithful' Sieve of Eratosthenes. To\ncircumvent the bottleneck of union of streams, we integrate our techniques\ninside the O'Neill program, thus obtaining a fast prime generator based on the\nEuler's Sieve and priority queues.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 14:11:34 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 16:08:08 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 08:27:36 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Salvo", "Ivano", ""], ["Pacifico", "Agnese", ""]]}, {"id": "1811.10274", "submitter": "Eva Darulova", "authors": "Eva Darulova, Anastasia Volkova", "title": "Sound Approximation of Programs with Elementary Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.MS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elementary function calls are a common feature in numerical programs. While\ntheir implementions in library functions are highly optimized, their\ncomputation is nonetheless very expensive compared to plain arithmetic. Full\naccuracy is, however, not always needed. Unlike arithmetic, where the\nperformance difference between for example single and double precision\nfloating-point arithmetic is relatively small, elementary function calls\nprovide a much richer tradeoff space between accuracy and efficiency.\nNavigating this space is challenging. First, generating approximations of\nelementary function calls which are guaranteed to satisfy accuracy error bounds\nis highly nontrivial. Second, the performance of such approximations generally\ndepends on several parameters which are unintuitive to choose manually,\nespecially for non-experts.\n  We present a fully automated approach and tool which approximates elementary\nfunction calls inside small programs while guaranteeing overall user provided\nerror bounds. Our tool leverages existing techniques for roundoff error\ncomputation and approximation of individual elementary function calls, and\nprovides automated selection of many parameters. Our experiments show that\nsignificant efficiency improvements are possible in exchange for reduced, but\nguaranteed, accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 10:30:04 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Darulova", "Eva", ""], ["Volkova", "Anastasia", ""]]}, {"id": "1811.10403", "submitter": "Pablo Gordillo", "authors": "Elvira Albert and Pablo Gordillo and Albert Rubio and Ilya Sergey", "title": "Running on Fumes--Preventing Out-of-Gas Vulnerabilities in Ethereum\n  Smart Contracts using Static Resource Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gas is a measurement unit of the computational effort that it will take to\nexecute every single operation that takes part in the Ethereum blockchain\nplatform. Each instruction executed by the Ethereum Virtual Machine (EVM) has\nan associated gas consumption specified by Ethereum. If a transaction exceeds\nthe amount of gas allotted by the user (known as gas limit), an out-of-gas\nexception is raised. There is a wide family of contract vulnerabilities due to\nout-of-gas behaviours. We report on the design and implementation of GASTAP, a\nGas-Aware Smart contracT Analysis Platform, which takes as input a smart\ncontract (either in EVM, disassembled EVM, or in Solidity source code) and\nautomatically infers sound gas upper bounds for all its public functions. Our\nbounds ensure that if the gas limit paid by the user is higher than our\ninferred gas bounds, the contract is free of out-of-gas vulnerabilities.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 13:19:07 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 09:42:09 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Albert", "Elvira", ""], ["Gordillo", "Pablo", ""], ["Rubio", "Albert", ""], ["Sergey", "Ilya", ""]]}, {"id": "1811.10665", "submitter": "Christopher Rosin", "authors": "Christopher D. Rosin", "title": "Stepping Stones to Inductive Synthesis of Low-Level Looping Programs", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inductive program synthesis, from input/output examples, can provide an\nopportunity to automatically create programs from scratch without presupposing\nthe algorithmic form of the solution. For induction of general programs with\nloops (as opposed to loop-free programs, or synthesis for domain-specific\nlanguages), the state of the art is at the level of introductory programming\nassignments. Most problems that require algorithmic subtlety, such as fast\nsorting, have remained out of reach without the benefit of significant\nproblem-specific background knowledge. A key challenge is to identify cues that\nare available to guide search towards correct looping programs. We present\nMAKESPEARE, a simple delayed-acceptance hillclimbing method that synthesizes\nlow-level looping programs from input/output examples. During search, delayed\nacceptance bypasses small gains to identify significantly-improved stepping\nstone programs that tend to generalize and enable further progress. The method\nperforms well on a set of established benchmarks, and succeeds on the\npreviously unsolved \"Collatz Numbers\" program synthesis problem. Additional\nbenchmarks include the problem of rapidly sorting integer arrays, in which we\nobserve the emergence of comb sort (a Shell sort variant that is empirically\nfast). MAKESPEARE has also synthesized a record-setting program on one of the\npuzzles from the TIS-100 assembly language programming game.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 19:51:27 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Rosin", "Christopher D.", ""]]}, {"id": "1811.10814", "submitter": "EPTCS", "authors": "Sylvain Dailler (Inria, Universit\\'e Paris-Saclay, F-91120 Palaiseau),\n  Claude March\\'e (Inria, Universit\\'e Paris-Saclay, F-91120 Palaiseau),\n  Yannick Moy (AdaCore, F-75009 Paris)", "title": "Lightweight Interactive Proving inside an Automatic Program Verifier", "comments": "In Proceedings F-IDE 2018, arXiv:1811.09014", "journal-ref": "EPTCS 284, 2018, pp. 1-15", "doi": "10.4204/EPTCS.284.1", "report-no": null, "categories": "cs.SE cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among formal methods, the deductive verification approach allows establishing\nthe strongest possible formal guarantees on critical software. The downside is\nthe cost in terms of human effort required to design adequate formal\nspecifications and to successfully discharge the required proof obligations. To\npopularize deductive verification in an industrial software development\nenvironment, it is essential to provide means to progressively transition from\nsimple and automated approaches to deductive verification. The SPARK\nenvironment, for development of critical software written in Ada, goes towards\nthis goal by providing automated tools for formally proving that some code\nfulfills the requirements expressed in Ada contracts. In a program verifier\nthat makes use of automatic provers to discharge the proof obligations, a need\nfor some additional user interaction with proof tasks shows up: either to help\nanalyzing the reason of a proof failure or, ultimately, to discharge the\nverification conditions that are out-of-reach of state-of-the-art automatic\nprovers. Adding interactive proof features in SPARK appears to be complicated\nby the fact that the proof toolchain makes use of the independent, intermediate\nverification tool Why3, which is generic enough to accept multiple front-ends\nfor different input languages. This paper reports on our approach to extend\nWhy3 with interactive proof features and also with a generic client-server\ninfrastructure allowing integration of proof interaction into an external,\nfront-end graphical user interface such as the one of SPARK.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 05:00:07 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Dailler", "Sylvain", "", "Inria, Universit\u00e9 Paris-Saclay, F-91120 Palaiseau"], ["March\u00e9", "Claude", "", "Inria, Universit\u00e9 Paris-Saclay, F-91120 Palaiseau"], ["Moy", "Yannick", "", "AdaCore, F-75009 Paris"]]}, {"id": "1811.10817", "submitter": "EPTCS", "authors": "Rui Couto (HASLab/INESC TEC & University of Minho), Jos\\'e C. Campos\n  (HASLab/INESC TEC & University of Minho), Nuno Macedo (HASLab/INESC TEC &\n  University of Minho), Alcino Cunha (HASLab/INESC TEC & University of Minho)", "title": "Improving the Visualization of Alloy Instances", "comments": "In Proceedings F-IDE 2018, arXiv:1811.09014", "journal-ref": "EPTCS 284, 2018, pp. 37-52", "doi": "10.4204/EPTCS.284.4", "report-no": null, "categories": "cs.HC cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alloy is a lightweight formal specification language, supported by an IDE,\nwhich has proven well-suited for reasoning about software design in early\ndevelopment stages. The IDE provides a visualizer that produces graphical\nrepresentations of analysis results, which is essential for the proper\nvalidation of the model. Alloy is a rich language but inherently static, so\nbehavior needs to be explicitly encoded and reasoned about. Even though this is\na common scenario, the visualizer presents limitations when dealing with such\nmodels. The main contribution of this paper is a principled approach to\ngenerate instance visualizations, which improves the current Alloy Visualizer,\nfocusing on the representation of behavior.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 05:01:04 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Couto", "Rui", "", "HASLab/INESC TEC & University of Minho"], ["Campos", "Jos\u00e9 C.", "", "HASLab/INESC TEC & University of Minho"], ["Macedo", "Nuno", "", "HASLab/INESC TEC &\n  University of Minho"], ["Cunha", "Alcino", "", "HASLab/INESC TEC & University of Minho"]]}, {"id": "1811.10818", "submitter": "EPTCS", "authors": "Alexander Kn\\\"uppel, Thomas Th\\\"um, Carsten Pardylla, Ina Schaefer", "title": "Experience Report on Formally Verifying Parts of OpenJDK's API with KeY", "comments": "In Proceedings F-IDE 2018, arXiv:1811.09014", "journal-ref": "EPTCS 284, 2018, pp. 53-70", "doi": "10.4204/EPTCS.284.5", "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deductive verification of software has not yet found its way into industry,\nas complexity and scalability issues require highly specialized experts. The\nlong-term perspective is, however, to develop verification tools aiding\nindustrial software developers to find bugs or bottlenecks in software systems\nfaster and more easily. The KeY project constitutes a framework for specifying\nand verifying software systems, aiming at making formal verification tools\napplicable for mainstream software development. To help the developers of KeY,\nits users, and the deductive verification community, we summarize our\nexperiences with KeY 2.6.1 in specifying and verifying real-world Java code\nfrom a users perspective. To this end, we concentrate on parts of the\nCollections-API of OpenJDK 6, where an informal specification exists. While we\ndescribe how we bridged informal and formal specification, we also exhibit\naccompanied challenges that we encountered. Our experiences are that (a) in\nprinciple, deductive verification for API-like code bases is feasible, but\nrequires high expertise, (b) developing formal specifications for existing code\nbases is still notoriously hard, and (c) the under-specification of certain\nlanguage constructs in Java is challenging for tool builders. Our initial\neffort in specifying parts of OpenJDK 6 constitutes a stepping stone towards a\ncase study for future research.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 05:01:21 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Kn\u00fcppel", "Alexander", ""], ["Th\u00fcm", "Thomas", ""], ["Pardylla", "Carsten", ""], ["Schaefer", "Ina", ""]]}, {"id": "1811.10820", "submitter": "EPTCS", "authors": "Spencer Park (McMaster University), Emil Sekerinski (McMaster\n  University)", "title": "A Notebook Format for the Holistic Design of Embedded Systems (Tool\n  Paper)", "comments": "In Proceedings F-IDE 2018, arXiv:1811.09014", "journal-ref": "EPTCS 284, 2018, pp. 85-94", "doi": "10.4204/EPTCS.284.7", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes the use of notebooks for the design documentation and\ntool interaction in the rigorous design of embedded systems. Conventionally, a\nnotebook is a sequence of cells alternating between (textual) code and prose to\nform a document that is meant to be read from top to bottom, in the spirit of\nliterate programming. We extend the use of notebooks to embedded systems\nspecified by pCharts. The charts are visually edited in cells inline. Other\ncells can contain statements that generate code and analyze the charts\nqualitatively and quantitatively; in addition, notebook cells can contain other\ninstructions to build the product from the generated code. This allows a\nnotebook to be replayed to re-analyze the design and re-build the product, like\na script, but also allows the notebook to be used for presentations, as for\nthis paper, and for the inspection of the design. The interaction with the\nnotebook is done through a web browser that connects to a local or remote\nserver, thus allowing a computationally intensive analysis to run remotely if\nneeded. The pState notebooks are implemented as an extension to Jupyter. The\nunderlying software architecture is described and the issue of proper placement\nof transition labels in charts embedded in notebooks is discussed.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 05:01:59 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Park", "Spencer", "", "McMaster University"], ["Sekerinski", "Emil", "", "McMaster\n  University"]]}, {"id": "1811.11317", "submitter": "Danil Annenkov", "authors": "Danil Annenkov", "title": "Adventures in Formalisation: Financial Contracts, Modules, and Two-Level\n  Type Theory", "comments": "PhD thesis defended in January 2018 at University of Copenhagen,\n  Department of Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present three projects concerned with applications of proof assistants in\nthe area of programming language theory and mathematics. The first project is\nabout a certified compilation technique for a domain-specific programming\nlanguage for financial contracts (the CL language). The code in CL is\ntranslated into a simple expression language well-suited for integration with\nsoftware components implementing Monte Carlo simulation techniques (pricing\nengines). The compilation procedure is accompanied with formal proofs of\ncorrectness carried out in Coq. The second project presents techniques that\nallow for formal reasoning with nested and mutually inductive structures built\nup from finite maps and sets. The techniques, which build on the theory of\nnominal sets combined with the ability to work with isomorphic representations\nof finite maps, make it possible to give a formal treatment, in Coq, of a\nhigher-order module system, including the ability to eliminate at compile time\nabstraction barriers introduced by the module system. The development is based\non earlier work on static interpretation of modules and provides the foundation\nfor a higher-order module language for Futhark, an optimising compiler\ntargeting data-parallel architectures. The third project presents an\nimplementation of two-level type theory, a version of Martin-Lof type theory\nwith two equality types: the first acts as the usual equality of homotopy type\ntheory, while the second allows us to reason about strict equality. In this\nsystem, we can formalise results of partially meta-theoretic nature. We develop\nand explore in details how two-level type theory can be implemented in a proof\nassistant, providing a prototype implementation in the proof assistant Lean. We\ndemonstrate an application of two-level type theory by developing some results\non the theory of inverse diagrams using our Lean implementation.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 00:05:16 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Annenkov", "Danil", ""]]}, {"id": "1811.11853", "submitter": "Ismail Kuru", "authors": "Ismail Kuru and Colin S. Gordon", "title": "Safe Deferred Memory Reclamation with Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Memory management in lock-free data structures remains a major challenge in\nconcurrent programming. Design techniques including read-copy-update (RCU) and\nhazard pointers provide workable solutions, and are widely used to great\neffect. These techniques rely on the concept of a grace period: nodes that\nshould be freed are not deallocated immediately, and all threads obey a\nprotocol to ensure that the deallocating thread can detect when all possible\nreaders have completed their use of the object. This provides an approach to\nsafe deallocation, but only when these subtle protocols are implemented\ncorrectly.\n  We present a static type system to ensure the correct use of RCU memory\nmanagement: that nodes removed from a data structure are always scheduled for\nsubsequent deallocation, and that nodes are scheduled for deallocation at most\nonce. As part of our soundness proof, we give an abstract semantics for RCU\nmemory management primitives which captures the fundamental properties of RCU.\nOur type system allows us to give the first proofs of memory safety for RCU\nlinked list and binary search tree implementations without requiring full\nverification.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 21:50:54 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 23:18:50 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Kuru", "Ismail", ""], ["Gordon", "Colin S.", ""]]}, {"id": "1811.11911", "submitter": "Yao Li", "authors": "Nicolas Koh, Yao Li, Yishuai Li, Li-yao Xia, Lennart Beringer, Wolf\n  Honor\\'e, William Mansky, Benjamin C. Pierce, Steve Zdancewic", "title": "From C to Interaction Trees: Specifying, Verifying, and Testing a\n  Networked Server", "comments": "13 pages + references", "journal-ref": "Proceedings of the 8th ACM SIGPLAN International Conference on\n  Certified Programs and Proofs (CPP '19), January 14--15, 2019, Cascais,\n  Portugal", "doi": "10.1145/3293880.3294106", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first formal verification of a networked server implemented in\nC. Interaction trees, a general structure for representing reactive\ncomputations, are used to tie together disparate verification and testing tools\n(Coq, VST, and QuickChick) and to axiomatize the behavior of the operating\nsystem on which the server runs (CertiKOS). The main theorem connects a\nspecification of acceptable server behaviors, written in a straightforward \"one\nclient at a time\" style, with the CompCert semantics of the C program. The\nvariability introduced by low-level buffering of messages and interleaving of\nmultiple TCP connections is captured using network refinement, a variant of\nobservational refinement.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 01:27:45 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Koh", "Nicolas", ""], ["Li", "Yao", ""], ["Li", "Yishuai", ""], ["Xia", "Li-yao", ""], ["Beringer", "Lennart", ""], ["Honor\u00e9", "Wolf", ""], ["Mansky", "William", ""], ["Pierce", "Benjamin C.", ""], ["Zdancewic", "Steve", ""]]}, {"id": "1811.11926", "submitter": "Dustin Tran", "authors": "Matthew D. Hoffman and Matthew J. Johnson and Dustin Tran", "title": "Autoconj: Recognizing and Exploiting Conjugacy Without a Domain-Specific\n  Language", "comments": "Appears in Neural Information Processing Systems, 2018. Code\n  available at https://github.com/google-research/autoconj", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deriving conditional and marginal distributions using conjugacy relationships\ncan be time consuming and error prone. In this paper, we propose a strategy for\nautomating such derivations. Unlike previous systems which focus on\nrelationships between pairs of random variables, our system (which we call\nAutoconj) operates directly on Python functions that compute log-joint\ndistribution functions. Autoconj provides support for conjugacy-exploiting\nalgorithms in any Python embedded PPL. This paves the way for accelerating\ndevelopment of novel inference algorithms and structure-exploiting modeling\nstrategies.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 02:13:37 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Hoffman", "Matthew D.", ""], ["Johnson", "Matthew J.", ""], ["Tran", "Dustin", ""]]}, {"id": "1811.12285", "submitter": "Colin Gordon", "authors": "Colin S. Gordon", "title": "Sequential Effect Systems with Control Operators", "comments": "Extended technical report corresponding to ECOOP 2020 paper \"Lifting\n  Sequential Effects to Control Operators\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential effect systems are a class of effect system that exploits\ninformation about program order, rather than discarding it as traditional\ncommutative effect systems do. This extra expressive power allows effect\nsystems to reason about behavior over time, capturing properties such as\natomicity, unstructured lock ownership, or even general safety properties.\nWhile we now understand the essential denotational (categorical) models fairly\nwell, application of these ideas to real software is hampered by the sheer\nvariety of source level control flow constructs in real languages. Denotational\napproaches are general enough to accommodate any particular control flow\nconstruct, but provide no guidance on the details, let alone applications.\n  We address this new problem by appeal to a classic idea: macro-expression of\ncommonly-used programming constructs in terms of control operators. We give an\neffect system for a subset of Racket's tagged delimited control operators, as a\nlifting of an effect system for a language without direct control operators.\nThis gives the first account of sequential effects in the presence of general\ncontrol operators. Using this system, we also re-derive the sequential effect\nsystem rules for control flow constructs previously shown sound directly, and\nderive sequential effect rules for new constructs not previously studied in the\ncontext of source-level sequential effect systems. This offers a way to\ndirectly extend source-level support for sequential effect systems to real\nprogramming languages.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 16:20:59 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 17:18:37 GMT"}, {"version": "v3", "created": "Wed, 13 May 2020 02:03:07 GMT"}, {"version": "v4", "created": "Fri, 15 May 2020 14:07:25 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Gordon", "Colin S.", ""]]}, {"id": "1811.12515", "submitter": "Quentin Bouillaguet", "authors": "Bouillaguet Quentin, Bobot Fran\\c{c}ois, Sighireanu Mihaela and\n  Yakobowski Boris", "title": "Exploiting Pointer Analysis in Memory Models for Deductive Verification", "comments": "VMCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperation between verification methods is crucial to tackle the challenging\nproblem of software verification. The paper focuses on the verification of C\nprograms using pointers and it formalizes a cooperation between static\nanalyzers doing pointer analysis and a deductive verification tool based on\nfirst order logic. We propose a framework based on memory models that captures\nthe partitioning of memory inferred by pointer analyses, and complies with the\nmemory models used to generate verification conditions. The framework guided us\nto propose a pointer analysis that accommodates to various low-level operations\non pointers while providing precise information about memory partitioning to\nthe deductive verification. We implemented this cooperation inside the Frama-C\nplatform and we show its effectiveness in reducing the task of deductive\nverification on a complex case study.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 22:14:46 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Quentin", "Bouillaguet", ""], ["Fran\u00e7ois", "Bobot", ""], ["Mihaela", "Sighireanu", ""], ["Boris", "Yakobowski", ""]]}, {"id": "1811.12874", "submitter": "Tom Mens", "authors": "Ahmed Zerouali, Tom Mens, Gregorio Robles, Jesus Gonzalez-Barahona", "title": "On The Relation Between Outdated Docker Containers, Severity\n  Vulnerabilities and Bugs", "comments": "Preprint. Paper accepted for publication at the 26th IEEE\n  International Conference on Software Analysis, Evolution and Reengineering\n  (SANER 2019), Hangzhou, China. A replication package of our analysis is\n  available on https://doi.org/10.5281/zenodo.1250314", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Packaging software into containers is becoming a common practice when\ndeploying services in cloud and other environments. Docker images are one of\nthe most popular container technologies for building and deploying containers.\nA container image usually includes a collection of software packages, that can\nhave bugs and security vulnerabilities that affect the container health. Our\ngoal is to support container deployers by analysing the relation between\noutdated containers and vulnerable and buggy packages installed in them. We use\nthe concept of technical lag of a container as the difference between a given\ncontainer and the most up-to-date container that is possible with the most\nrecent releases of the same collection of packages. For 7,380 official and\ncommunity Docker images that are based on the Debian Linux distribution, we\nidentify which software packages are installed in them and measure their\ntechnical lag in terms of version updates, security vulnerabilities and bugs.\nWe have found, among others, that no release is devoid of vulnerabilities, so\ndeployers cannot avoid vulnerabilities even if they deploy the most recent\npackages. We offer some lessons learned for container developers in regard to\nthe strategies they can follow to minimize the number of vulnerabilities. We\nargue that Docker container scan and security management tools should improve\ntheir platforms by adding data about other kinds of bugs and include the\nmeasurement of technical lag to offer deployers information of when to update.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 16:24:04 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Zerouali", "Ahmed", ""], ["Mens", "Tom", ""], ["Robles", "Gregorio", ""], ["Gonzalez-Barahona", "Jesus", ""]]}]