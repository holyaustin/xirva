[{"id": "1212.0582", "submitter": "Eric Mjolsness", "authors": "Eric Mjolsness", "title": "Compositional Stochastic Modeling and Probabilistic Programming", "comments": "Extended Abstract for the Neural Information Processing Systems\n  (NIPS) Workshop on Probabilistic Programming, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic programming is related to a compositional approach to\nstochastic modeling by switching from discrete to continuous time dynamics. In\ncontinuous time, an operator-algebra semantics is available in which processes\nproceeding in parallel (and possibly interacting) have summed time-evolution\noperators. From this foundation, algorithms for simulation, inference and model\nreduction may be systematically derived. The useful consequences are\npotentially far-reaching in computational science, machine learning and beyond.\nHybrid compositional stochastic modeling/probabilistic programming approaches\nmay also be possible.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 23:05:30 GMT"}], "update_date": "2012-12-05", "authors_parsed": [["Mjolsness", "Eric", ""]]}, {"id": "1212.2341", "submitter": "Damien Cassou", "authors": "St\\'ephane Ducasse (INRIA Lille - Nord Europe), Nicolas Petton (INRIA\n  Lille - Nord Europe), Guillermo Polito (INRIA Lille - Nord Europe), Damien\n  Cassou (INRIA Lille - Nord Europe)", "title": "Semantics and Security Issues in JavaScript", "comments": "Deliverable Resilience FUI 12: 7.3.2.1 Failles de s\\'ecurit\\'e en\n  JavaScript / JavaScript security issues", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a plethora of research articles describing the deep semantics of\nJavaScript. Nevertheless, such articles are often difficult to grasp for\nreaders not familiar with formal semantics. In this report, we propose a digest\nof the semantics of JavaScript centered around security concerns. This document\nproposes an overview of the JavaScript language and the misleading semantic\npoints in its design. The first part of the document describes the main\ncharacteristics of the language itself. The second part presents how those\ncharacteristics can lead to problems. It finishes by showing some coding\npatterns to avoid certain traps and presents some ECMAScript 5 new features.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 09:04:39 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Ducasse", "St\u00e9phane", "", "INRIA Lille - Nord Europe"], ["Petton", "Nicolas", "", "INRIA\n  Lille - Nord Europe"], ["Polito", "Guillermo", "", "INRIA Lille - Nord Europe"], ["Cassou", "Damien", "", "INRIA Lille - Nord Europe"]]}, {"id": "1212.3458", "submitter": "EPTCS", "authors": "Marco Carbone, Ivan Lanese, Alexandra Silva, Ana Sokolova", "title": "Proceedings Fifth Interaction and Concurrency Experience", "comments": "EPTCS 104, 2012", "journal-ref": null, "doi": "10.4204/EPTCS.104", "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of ICE'12, the 5th Interaction and\nConcurrency Experience workshop, which was held in Stockholm, Sweden on the\n16th of June 2012 as a satellite event of DisCoTec'12. The topic of ICE'12 was\nDistributed Coordination, Execution Models, and Resilient Interaction. The ICE\nprocedure for paper selection allows for PC members to interact, anonymously,\nwith authors. During the review phase, each submitted paper is published on a\nWiki and associated with a discussion forum whose access is restricted to the\nauthors and to all the PC members not declaring a conflict of interests. The PC\nmembers post comments and questions that the authors reply to. Each paper was\nreviewed by four PC members, and altogether 8 papers were accepted for\npublication. We were proud to host two invited talks, Marcello Bonsangue and\nIchiro Hasuo, whose abstracts are included in this volume together with the\nregular papers.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2012 12:50:44 GMT"}], "update_date": "2012-12-17", "authors_parsed": [["Carbone", "Marco", ""], ["Lanese", "Ivan", ""], ["Silva", "Alexandra", ""], ["Sokolova", "Ana", ""]]}, {"id": "1212.3806", "submitter": "Pierre-Evariste Dagand", "authors": "Pierre-Evariste Dagand and Conor McBride", "title": "A Categorical Treatment of Ornaments", "comments": "32 pages, technical report, extends paper to appear in LICS'13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ornaments aim at taming the multiplication of special-purpose datatype in\ndependently-typed theory. In its original form, the definition of ornaments is\ntied to a particular universe of datatypes. Being a type theoretic object,\nconstructions on ornaments are typically explained through an operational\nnarrative. This overbearing concreteness calls for an abstract model of\nornaments.\n  In this paper, we give a categorical model of ornaments. As a necessary first\nstep, we abstract the universe of datatypes using the theory of polynomial\nfunctors. We are then able to characterize ornaments as cartesian morphisms\nbetween polynomial functors. We thus gain access to powerful mathematical tools\nthat shall help us understand and develop ornaments.\n  We shall also illustrate the adequacy of our model. Firstly, we rephrase the\nstandard ornamental constructions into our framework. Thanks to its\nconciseness, this process gives us a deeper understanding of the structures at\nplay. Secondly, we develop new ornamental constructions, by translating\ncategorical structures into type theoretic artifacts.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2012 17:08:56 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2012 19:09:09 GMT"}, {"version": "v3", "created": "Sun, 21 Apr 2013 16:36:23 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Dagand", "Pierre-Evariste", ""], ["McBride", "Conor", ""]]}, {"id": "1212.3874", "submitter": "EPTCS", "authors": "Andr\\'es Aristiz\\'abal (CNRS/DGA and LIX \\'Ecole Polytechnique de\n  Paris), Filippo Bonchi (ENS Lyon, Universit\\'e de Lyon, LIP), Luis Pino\n  (INRIA/DGA and LIX \\'Ecole Polytechnique de Paris), Frank Valencia (CNRS and\n  LIX \\'Ecole Polytechnique de Paris)", "title": "Reducing Weak to Strong Bisimilarity in CCP", "comments": "In Proceedings ICE 2012, arXiv:1212.3458. arXiv admin note: text\n  overlap with arXiv:1212.1548", "journal-ref": "EPTCS 104, 2012, pp. 2-16", "doi": "10.4204/EPTCS.104.2", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concurrent constraint programming (ccp) is a well-established model for\nconcurrency that singles out the fundamental aspects of asynchronous systems\nwhose agents (or processes) evolve by posting and querying (partial)\ninformation in a global medium. Bisimilarity is a standard behavioural\nequivalence in concurrency theory. However, only recently a well-behaved notion\nof bisimilarity for ccp, and a ccp partition refinement algorithm for deciding\nthe strong version of this equivalence have been proposed. Weak bisimiliarity\nis a central behavioural equivalence in process calculi and it is obtained from\nthe strong case by taking into account only the actions that are observable in\nthe system. Typically, the standard partition refinement can also be used for\ndeciding weak bisimilarity simply by using Milner's reduction from weak to\nstrong bisimilarity; a technique referred to as saturation. In this paper we\ndemonstrate that, because of its involved labeled transitions, the\nabove-mentioned saturation technique does not work for ccp. We give an\nalternative reduction from weak ccp bisimilarity to the strong one that allows\nus to use the ccp partition refinement algorithm for deciding this equivalence.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 03:42:03 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Aristiz\u00e1bal", "Andr\u00e9s", "", "CNRS/DGA and LIX \u00c9cole Polytechnique de\n  Paris"], ["Bonchi", "Filippo", "", "ENS Lyon, Universit\u00e9 de Lyon, LIP"], ["Pino", "Luis", "", "INRIA/DGA and LIX \u00c9cole Polytechnique de Paris"], ["Valencia", "Frank", "", "CNRS and\n  LIX \u00c9cole Polytechnique de Paris"]]}, {"id": "1212.3875", "submitter": "EPTCS", "authors": "\\'Etienne Lozes (Universit\\\"at Kassel, Germany), Jules Villard\n  (University College London, UK)", "title": "Shared Contract-Obedient Endpoints", "comments": "In Proceedings ICE 2012, arXiv:1212.3458", "journal-ref": "EPTCS 104, 2012, pp. 17-31", "doi": "10.4204/EPTCS.104.3", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing verification techniques for message-passing programs\nsuppose either that channel endpoints are used in a linear fashion, where at\nmost one thread may send or receive from an endpoint at any given time, or that\nendpoints may be used arbitrarily by any number of threads. The former approach\nusually forbids the sharing of channels while the latter limits what is\nprovable about programs. In this paper we propose a midpoint between these\ntechniques by extending a proof system based on separation logic to allow\nsharing of endpoints. We identify two independent mechanisms for supporting\nsharing: an extension of fractional shares to endpoints, and a new technique\nbased on what we call reflexive ownership transfer. We demonstrate on a number\nof examples that a linear treatment of sharing is possible.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 03:42:11 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Lozes", "\u00c9tienne", "", "Universit\u00e4t Kassel, Germany"], ["Villard", "Jules", "", "University College London, UK"]]}, {"id": "1212.3877", "submitter": "EPTCS", "authors": "Simon Bliudze (Ecole Polytechnique F\\'ed\\'erale de Lausanne)", "title": "Towards a Theory of Glue", "comments": "In Proceedings ICE 2012, arXiv:1212.3458", "journal-ref": "EPTCS 104, 2012, pp. 48-66", "doi": "10.4204/EPTCS.104.6", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study the notions of behaviour type and composition operator\nmaking a first step towards the definition of a formal framework for studying\nbehaviour composition in a setting sufficiently general to provide insight into\nhow the component-based systems should be modelled and compared. We illustrate\nthe proposed notions on classical examples (Traces, Labelled Transition Systems\nand Coalgebras). Finally, the definition of memoryless glue operators, takes us\none step closer to a formal understanding of the separation of concerns\nprinciple stipulating that computational aspects of a system should be\nlocalised within its atomic components, whereas coordination layer responsible\nfor managing concurrency should be realised by memoryless glue operators.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 03:42:23 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Bliudze", "Simon", "", "Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne"]]}, {"id": "1212.3878", "submitter": "EPTCS", "authors": "Dan R. Ghica (University of Birmingham), Zaid Al-Zobaidi (University\n  of Birmingham)", "title": "Coherent Minimisation: Towards efficient tamper-proof compilation", "comments": "In Proceedings ICE 2012, arXiv:1212.3458", "journal-ref": "EPTCS 104, 2012, pp. 83-98", "doi": "10.4204/EPTCS.104.8", "report-no": null, "categories": "cs.PL cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automata representing game-semantic models of programs are meant to operate\nin environments whose input-output behaviour is constrained by the rules of a\ngame. This can lead to a notion of equivalence between states which is weaker\nthan the conventional notion of bisimulation, since not all actions are\navailable to the environment. An environment which attempts to break the rules\nof the game is, effectively, mounting a low-level attack against a system. In\nthis paper we show how (and why) to enforce game rules in games-based hardware\nsynthesis and how to use this weaker notion of equivalence, called coherent\nequivalence, to aggressively minimise automata.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 03:42:40 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Ghica", "Dan R.", "", "University of Birmingham"], ["Al-Zobaidi", "Zaid", "", "University\n  of Birmingham"]]}, {"id": "1212.3879", "submitter": "EPTCS", "authors": "Jurriaan Rot (LIACS - Leiden University, The Netherlands), Irina\n  M\\u{a}riuca As\\u{a}voae (Alexandru Ioan Cuza University, Romania), Frank de\n  Boer (Centrum Wiskunde en Informatica (CWI), The Netherlands), Marcello M.\n  Bonsangue (LIACS - Leiden University, The Netherlands), Dorel Lucanu\n  (Alexandru Ioan Cuza University, Romania)", "title": "Interacting via the Heap in the Presence of Recursion", "comments": "In Proceedings ICE 2012, arXiv:1212.3458", "journal-ref": "EPTCS 104, 2012, pp. 99-113", "doi": "10.4204/EPTCS.104.9", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Almost all modern imperative programming languages include operations for\ndynamically manipulating the heap, for example by allocating and deallocating\nobjects, and by updating reference fields. In the presence of recursive\nprocedures and local variables the interactions of a program with the heap can\nbecome rather complex, as an unbounded number of objects can be allocated\neither on the call stack using local variables, or, anonymously, on the heap\nusing reference fields. As such a static analysis is, in general, undecidable.\n  In this paper we study the verification of recursive programs with unbounded\nallocation of objects, in a simple imperative language for heap manipulation.\nWe present an improved semantics for this language, using an abstraction that\nis precise. For any program with a bounded visible heap, meaning that the\nnumber of objects reachable from variables at any point of execution is\nbounded, this abstraction is a finitary representation of its behaviour, even\nthough an unbounded number of objects can appear in the state. As a\nconsequence, for such programs model checking is decidable.\n  Finally we introduce a specification language for temporal properties of the\nheap, and discuss model checking these properties against heap-manipulating\nprograms.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 03:42:47 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Rot", "Jurriaan", "", "LIACS - Leiden University, The Netherlands"], ["As\u0103voae", "Irina M\u0103riuca", "", "Alexandru Ioan Cuza University, Romania"], ["de Boer", "Frank", "", "Centrum Wiskunde en Informatica"], ["Bonsangue", "Marcello M.", "", "LIACS - Leiden University, The Netherlands"], ["Lucanu", "Dorel", "", "Alexandru Ioan Cuza University, Romania"]]}, {"id": "1212.4446", "submitter": "Vadim Zaytsev", "authors": "Vadim Zaytsev", "title": "The Grammar Hammer of 2012", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.PL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This document is a case study in aggressive self-archiving. It collects all\ninitiatives undertaken by its author in 2012, including unpublished ones,\nexplains their relevance and relation with one another. Discussed topics\ninclude guided convergence of formal grammars in a broad sense, programmable\ngrammar transformation operator suites, metasyntactic specifications and\nmethods of their manipulation, tolerant (soft computing) methods in parsing\ntheory, megamodelling as modelling linguistic architecture of software systems,\nrepositories of grammatical knowledge, open notebook computer science, as well\nas the number of minor topics (new parsing algorithms, visualisation\ntechniques, etc). A brief overview of involved venues is also included in the\nreport.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2012 19:42:16 GMT"}], "update_date": "2012-12-19", "authors_parsed": [["Zaytsev", "Vadim", ""]]}, {"id": "1212.5210", "submitter": "Luca Saiu", "authors": "Luca Saiu", "title": "GNU epsilon - an extensible programming language", "comments": "172 pages, PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reductionism is a viable strategy for designing and implementing practical\nprogramming languages, leading to solutions which are easier to extend,\nexperiment with and formally analyze. We formally specify and implement an\nextensible programming language, based on a minimalistic first-order imperative\ncore language plus strong abstraction mechanisms, reflection and\nself-modification features. The language can be extended to very high levels:\nby using Lisp-style macros and code-to-code transforms which automatically\nrewrite high-level expressions into core forms, we define closures and\nfirst-class continuations on top of the core. Non-self-modifying programs can\nbe analyzed and formally reasoned upon, thanks to the language simple\nsemantics. We formally develop a static analysis and prove a soundness property\nwith respect to the dynamic semantics. We develop a parallel garbage collector\nsuitable to multi-core machines to permit efficient execution of parallel\nprograms.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2012 19:56:38 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2012 14:53:12 GMT"}, {"version": "v3", "created": "Fri, 11 Jan 2013 15:13:35 GMT"}, {"version": "v4", "created": "Mon, 11 Mar 2013 12:27:10 GMT"}, {"version": "v5", "created": "Sun, 31 Mar 2013 15:52:33 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Saiu", "Luca", ""]]}, {"id": "1212.5692", "submitter": "Vivek Nigam", "authors": "Nick Benton, Martin Hofmann, Vivek Nigam", "title": "Abstract Effects and Proof-Relevant Logical Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel variant of logical relations that maps types not merely\nto partial equivalence relations on values, as is commonly done, but rather to\na proof-relevant generalisation thereof, namely setoids. The objects of a\nsetoid establish that values inhabit semantic types, whilst its morphisms are\nunderstood as proofs of semantic equivalence. The transition to proof-relevance\nsolves two well-known problems caused by the use of existential quantification\nover future worlds in traditional Kripke logical relations: failure of\nadmissibility, and spurious functional dependencies. We illustrate the novel\nformat with two applications: a direct-style validation of Pitts and Stark's\nequivalences for \"new\" and a denotational semantics for a region-based effect\nsystem that supports type abstraction in the sense that only externally visible\neffects need to be tracked; non-observable internal modifications, such as the\nreorganisation of a search tree or lazy initialisation, can count as `pure' or\n`read only'. This `fictional purity' allows clients of a module soundly to\nvalidate more effect-based program equivalences than would be possible with\ntraditional effect systems.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2012 13:54:02 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Benton", "Nick", ""], ["Hofmann", "Martin", ""], ["Nigam", "Vivek", ""]]}, {"id": "1212.6542", "submitter": "Dirk Beyer", "authors": "Dirk Beyer and Stefan L\\\"owe", "title": "Explicit-Value Analysis Based on CEGAR and Interpolation", "comments": "12 pages, 5 figures, 3 tables, 4 algorithms", "journal-ref": null, "doi": null, "report-no": "MIP-1205", "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstraction, counterexample-guided refinement, and interpolation are\ntechniques that are essential to the success of predicate-based program\nanalysis. These techniques have not yet been applied together to explicit-value\nprogram analysis. We present an approach that integrates abstraction and\ninterpolation-based refinement into an explicit-value analysis, i.e., a program\nanalysis that tracks explicit values for a specified set of variables (the\nprecision). The algorithm uses an abstract reachability graph as central data\nstructure and a path-sensitive dynamic approach for precision adjustment. We\nevaluate our algorithm on the benchmark set of the Competition on Software\nVerification 2012 (SV-COMP'12) to show that our new approach is highly\ncompetitive. In addition, we show that combining our new approach with an\nauxiliary predicate analysis scores significantly higher than the SV-COMP'12\nwinner.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2012 17:47:10 GMT"}], "update_date": "2013-01-01", "authors_parsed": [["Beyer", "Dirk", ""], ["L\u00f6we", "Stefan", ""]]}, {"id": "1212.6844", "submitter": "Mi-Young  Park", "authors": "Keehang Kwon, Sungwoo Hur, and Mi-Young Park", "title": "Improving Robustness via Disjunctive Statements in Imperative\n  Programming", "comments": null, "journal-ref": "IEICE transaction on information and system, vol.E96-D, no. 9,\n  Sep, 2013", "doi": "10.1587/transinf.E96.D.2036", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To deal with failures as simply as possible, we propose a new foun- dation\nfor the core (untyped) C, which is based on a new logic called task logic or\nimperative logic. We then introduce a sequential-disjunctive statement of the\nform S : R. This statement has the following semantics: execute S and R\nsequentially. It is considered a success if at least one of S;R is a success.\nThis statement is useful for dealing with inessential errors without explicitly\ncatching them.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2012 09:20:05 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Kwon", "Keehang", ""], ["Hur", "Sungwoo", ""], ["Park", "Mi-Young", ""]]}]