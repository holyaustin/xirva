[{"id": "1908.00093", "submitter": "Jingmei Hu", "authors": "David A. Holland and Jingmei Hu and Ming Kawaguchi and Eric Lu and\n  Stephen Chong and Margo I. Seltzer", "title": "Aquarium: Cassiopea and Alewife Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report describes two of the domain-specific languages used in\nthe Aquarium kernel code synthesis project. It presents the language cores in\nterms of abstract syntax. Cassiopea is a machine description language for\ndescribing the semantics of processor instruction sets. Alewife is a\nspecification language that can be used to write machine-independent\nspecifications for assembly-level instruction blocks. An Alewife specification\ncan be used to verify and synthesize code for any machine described in\nCassiopea, given a machine-specific translation for abstractions used in the\nspecification. This article does not include an introduction to either the\nAquarium system or the use of the languages. In addition to this version of the\narticle being a draft, the Aquarium project and the languages are work in\nprogress. This article cannot currently be considered either final or complete.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 20:50:04 GMT"}, {"version": "v2", "created": "Sat, 5 Oct 2019 00:57:50 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2019 22:24:27 GMT"}, {"version": "v4", "created": "Thu, 14 May 2020 15:42:09 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Holland", "David A.", ""], ["Hu", "Jingmei", ""], ["Kawaguchi", "Ming", ""], ["Lu", "Eric", ""], ["Chong", "Stephen", ""], ["Seltzer", "Margo I.", ""]]}, {"id": "1908.00104", "submitter": "Joaqu\\'in Arias M.Sc.", "authors": "Joaquin Arias and Manuel Carro", "title": "Evaluation of the Implementation of an Abstract Interpretation Algorithm\n  using Tabled CLP", "comments": "Paper presented at the 35th International Conference on Logic\n  Programming (ICLP 2019), Las Cruces, New Mexico, USA, 20-25 September 2019,\n  16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CiaoPP is an analyzer and optimizer for logic programs, part of the Ciao\nProlog system. It includes PLAI, a fixpoint algorithm for the abstract\ninterpretation of logic programs which we adapt to use tabled constraint logic\nprogramming. In this adaptation, the tabling engine drives the fixpoint\ncomputation, while the constraint solver handles the LUB of the abstract\nsubstitutions of different clauses. That simplifies the code and improves\nperformance, since termination, dependencies, and some crucial operations\n(e.g., branch switching and resumption) are directly handled by the tabling\nengine. Determining whether the fixpoint has been reached uses semantic\nequivalence, which can decide that two syntactically different abstract\nsubstitutions represent the same element in the abstract domain. Therefore, the\ntabling analyzer can reuse answers in more cases than an analyzer using\nsyntactical equality. This helps achieve better performance, even taking into\naccount the additional cost associated to these checks. Our implementation is\nbased on the TCLP framework available in Ciao Prolog and is one-third the size\nof the initial fixpoint implementation in CiaoPP. Its performance has been\nevaluated by analyzing several programs using different abstract domains. This\npaper is under consideration for publication in Theory and Practice of Logic\nProgramming (TPLP).\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 21:19:21 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Arias", "Joaquin", ""], ["Carro", "Manuel", ""]]}, {"id": "1908.00441", "submitter": "Bernardo Toninho", "authors": "Lu\\'is Caires and Bernardo Toninho", "title": "Refinement Kinds: Type-safe Programming with Practical Type-level\n  Computation (Extended Version)", "comments": "Extended version with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces the novel concept of kind refinement, which we develop\nin the context of an explicitly polymorphic ML-like language with type-level\ncomputation. Just as type refinements embed rich specifications by means of\ncomprehension principles expressed by predicates over values in the type\ndomain, kind refinements provide rich kind specifications by means of\npredicates over types in the kind domain. By leveraging our powerful refinement\nkind discipline, types in our language are not just used to statically classify\nprogram expressions and values, but also conveniently manipulated as tree-like\ndata structures, with their kinds refined by logical constraints on such\nstructures. Remarkably, the resulting typing and kinding disciplines allow for\npowerful forms of type reflection, ad-hoc polymorphism and type\nmeta-programming, which are often found in modern software development, but not\ntypically expressible in a type-safe manner in general purpose languages. We\nvalidate our approach both formally and pragmatically by establishing the\nstandard meta-theoretical results of type safety and via a prototype\nimplementation of a kind checker, type checker and interpreter for our\nlanguage.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 14:47:50 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Caires", "Lu\u00eds", ""], ["Toninho", "Bernardo", ""]]}, {"id": "1908.00898", "submitter": "Roly Perera", "authors": "Roly Perera", "title": "The meaning of a program change is a change to the program's meaning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming is the activity of modifying a program in order to bring about\nspecific changes in its behaviour. Yet programming language theory almost\nexclusively focuses on the meaning of programs. We motivate a \"change-oriented\"\nviewpoint from which the meaning of a program change is a change to the\nprogram's meaning.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 15:01:07 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Perera", "Roly", ""]]}, {"id": "1908.01057", "submitter": "Asma Balamane", "authors": "Asma Balamane and Zina Taklit", "title": "Proposition d'un mod\\`ele pour l'optimisation automatique de boucles\n  dans le compilateur Tiramisu : cas d'optimisation de d\\'eroulage", "comments": "in french", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer architectures become more and more complex. It requires more effort\nto develop techniques that improve the programs of performance and allow to\nexploit material resources efficiently. As a result, many transformations are\napplied on various levels of code abstraction. The first level is the high\nlevel, where the representation is close to the high level language. The second\none is the low level, where the presentation is close to the machine code.\nThose transformations are called code optimizations. Optimizing programs\nrequires deep expertise. On one hand, it is a tedious task, because it requires\na lot of tests to find out the best combination of optimizations to apply with\ntheir best factors. On the other hand, this task is critical, because it may\ndegrade the performance of the program instead of improving it. The\nautomatization of this task can deal with this problem and permit to obtain\ngood results. Our end of study project consists on proposing a novel approach\nbased on neural networks to automatically optimize loops in Tiramisu. Tiramisu\nis a new language to create a code of high performance. It allows to separate\nbetween the algorithm and its optimizations. We have chosen loop unrolling as a\nstudy case. Our contribution aims to automate the choice of the best loop\nunrolling factor for a program written in Tiramisu.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 23:13:32 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Balamane", "Asma", ""], ["Taklit", "Zina", ""]]}, {"id": "1908.01324", "submitter": "Thomas Melham", "authors": "Andreas Tiemeyer, Tom Melham, Daniel Kroening, John O'Leary", "title": "CREST: Hardware Formal Verification with ANSI-C Reference Specifications", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents CREST, a prototype front-end tool intended as an add-on\nto commercial EDA formal verifcation environments. CREST is an adaptation of\nthe CBMC bounded model checker for C, an academic tool widely used in industry\nfor software analysis and property verification. It leverages the capabilities\nof CBMC to process hardware datapath specifications written in arbitrary\nANSI-C, without limiting restrictions to a synthesizable subset. We briefly\nsketch the architecture of our tool and show its use in a range of verification\ncase studies.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 12:18:50 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Tiemeyer", "Andreas", ""], ["Melham", "Tom", ""], ["Kroening", "Daniel", ""], ["O'Leary", "John", ""]]}, {"id": "1908.01572", "submitter": "EPTCS", "authors": "Patrik Jansson (Chalmers University of Technology), S\\'olr\\'un Halla\n  Einarsd\\'ottir (Chalmers University of Technology), Cezar Ionescu (University\n  of Oxford)", "title": "Examples and Results from a BSc-level Course on Domain Specific\n  Languages of Mathematics", "comments": "In Proceedings TFPIE 2018, arXiv:1906.10757", "journal-ref": "EPTCS 295, 2019, pp. 79-90", "doi": "10.4204/EPTCS.295.6", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the workshop on Trends in Functional Programming in Education (TFPIE) in\n2015 Ionescu and Jansson presented the approach underlying the \"Domain Specific\nLanguages of Mathematics\" (DSLsofMath) course even before the first course\ninstance. We were then encouraged to come back to present our experience and\nthe student results. Now, three years later, we have seen three groups of\nlearners attend the course, and the first two groups have also continued on to\ntake challenging courses in the subsequent year. In this paper we present three\nexamples from the course material to set the scene, and we present an\nevaluation of the student results showing improvements in the pass rates and\ngrades in later courses.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 03:34:10 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Jansson", "Patrik", "", "Chalmers University of Technology"], ["Einarsd\u00f3ttir", "S\u00f3lr\u00fan Halla", "", "Chalmers University of Technology"], ["Ionescu", "Cezar", "", "University\n  of Oxford"]]}, {"id": "1908.01909", "submitter": "Farzaneh Derakhshan", "authors": "Farzaneh Derakhshan, Frank Pfenning", "title": "Circular Proofs as Session-Typed Processes: A Local Validity Condition", "comments": "The revised version, 48 pages, submitted to Logical Methods in\n  Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proof theory provides a foundation for studying and reasoning about\nprogramming languages, most directly based on the well-known Curry-Howard\nisomorphism between intuitionistic logic and the typed lambda-calculus. More\nrecently, a correspondence between intuitionistic linear logic and the\nsession-typed pi-calculus has been discovered. In this paper, we establish an\nextension of the latter correspondence for a fragment of substructural logic\nwith least and greatest fixed points. We describe the computational\ninterpretation of the resulting infinitary proof system as session-typed\nprocesses, and provide an effectively decidable local criterion to recognize\nmutually recursive processes corresponding to valid circular proofs as\nintroduced by Fortier and Santocanale. We show that our algorithm imposes a\nstricter requirement than Fortier and Santocanale's guard condition, but is\nlocal and compositional and therefore more suitable as the basis for a\nprogramming language.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 00:07:04 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 02:32:01 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Derakhshan", "Farzaneh", ""], ["Pfenning", "Frank", ""]]}, {"id": "1908.02035", "submitter": "Akira Kawata", "authors": "Akira Kawata and Atsushi Igarashi", "title": "A Dependently Typed Multi-Stage Calculus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study a dependently typed extension of a multi-stage programming language\n\\`a la MetaOCaml, which supports quasi-quotation and cross-stage persistence\nfor manipulation of code fragments as first-class values and an evaluation\nconstruct for execution of programs dynamically generated by this code\nmanipulation. Dependent types are expected to bring to multi-stage programming\nenforcement of strong invariant---beyond simple type safety---on the behavior\nof dynamically generated code. An extension is, however, not trivial because\nsuch a type system would have to take stages of types ---roughly speaking, the\nnumber of surrounding quotations---into account.\n  To rigorously study properties of such an extension, we develop\n$\\lambda^{MD}$, which is an extension of Hanada and Igarashi's typed calculus\n$\\lambda^{\\triangleright\\%} $ with dependent types, and prove its properties\nincluding preservation, confluence, strong normalization for full reduction,\nand progress for staged reduction. Motivated by code generators that generate\ncode whose type depends on a value from outside of the quotations, we argue the\nsignificance of cross-stage persistence in dependently typed multi-stage\nprogramming and certain type equivalences that are not directly derived from\nreduction rules.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 09:19:45 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 09:29:50 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Kawata", "Akira", ""], ["Igarashi", "Atsushi", ""]]}, {"id": "1908.02078", "submitter": "Enrique Martin-Martin", "authors": "Elvira Albert, Samir Genaim, Ra\\'ul Guti\\'errez, Enrique Martin-Martin", "title": "A Transformational Approach to Resource Analysis with Typed-norms\n  Inference", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to automatically infer the resource consumption of programs,\nanalyzers track how data sizes change along program's execution. Typically,\nanalyzers measure the sizes of data by applying norms which are mappings from\ndata to natural numbers that represent the sizes of the corresponding data.\nWhen norms are defined by taking type information into account, they are named\ntyped-norms. This article presents a transformational approach to resource\nanalysis with typed-norms that are inferred by a data-flow analysis. The\nanalysis is based on a transformation of the program into an intermediate\nabstract program in which each variable is abstracted with respect to all\nconsidered norms which are valid for its type. We also present the data-flow\nanalysis to automatically infer the required, useful, typed-norms from\nprograms. Our analysis is formalized on a simple rule-based representation to\nwhich programs written in different programming paradigms (e.g., functional,\nlogic, imperative) can be automatically translated. Experimental results on\nstandard benchmarks used by other type-based analyzers show that our approach\nis both efficient and accurate in practice.\n  Under consideration in Theory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 11:13:12 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Albert", "Elvira", ""], ["Genaim", "Samir", ""], ["Guti\u00e9rrez", "Ra\u00fal", ""], ["Martin-Martin", "Enrique", ""]]}, {"id": "1908.02414", "submitter": "Yuya Tsuda", "authors": "Yuya Tsuda, Atsushi Igarashi, Tomoya Tabuchi", "title": "Space-Efficient Gradual Typing in Coercion-Passing Style", "comments": null, "journal-ref": "34th European Conference on Object-Oriented Programming, ECOOP\n  2020, volume 166 of LIPIcs, pages 8:1--8:29", "doi": "10.4230/LIPIcs.ECOOP.2020.8", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Herman et al. pointed out that the insertion of run-time checks into a\ngradually typed program could hamper tail-call optimization and, as a result,\nworsen the space complexity of the program. To address the problem, they\nproposed a space-efficient coercion calculus, which was subsequently improved\nby Siek et al. The semantics of these calculi involves eager composition of\nrun-time checks expressed by coercions to prevent the size of a term from\ngrowing. However, it relies also on a nonstandard reduction rule, which does\nnot seem easy to implement. In fact, no compiler implementation of gradually\ntyped languages fully supports the space-efficient semantics faithfully.\n  In this paper, we study coercion-passing style, which Herman et al. have\nalready mentioned, as a technique for straightforward space-efficient\nimplementation of gradually typed languages. A program in coercion-passing\nstyle passes \"the rest of the run-time checks\" around---just like\ncontinuation-passing style (CPS), in which \"the rest of the computation\" is\npassed around---and (unlike CPS) composes coercions eagerly. We give a formal\ncoercion-passing translation from $\\lambda$S by Siek et al. to $\\lambda$S$_1$,\nwhich is a new calculus of first-class coercions tailored for coercion-passing\nstyle, and prove correctness of the translation. We also implement our\ncoercion-passing style transformation for the Grift compiler developed by\nKuhlenschmidt et al. An experimental result shows stack overflow can be\nprevented properly at the cost of up to 3 times slower execution for most\npartially typed practical programs.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 01:40:24 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 14:31:33 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Tsuda", "Yuya", ""], ["Igarashi", "Atsushi", ""], ["Tabuchi", "Tomoya", ""]]}, {"id": "1908.02644", "submitter": "Matthew Amy", "authors": "Matthew Amy", "title": "Sized Types for low-level Quantum Metaprogramming", "comments": "Presented at Reversible Computation 2019. Final authenticated\n  publication is available online at\n  https://doi.org/10.1007/978-3-030-21500-2_6", "journal-ref": "Thomsen M., Soeken M. (eds) Reversible Computation. RC 2019.\n  Lecture Notes in Computer Science, vol 11497. Springer, Cham", "doi": "10.1007/978-3-030-21500-2_6", "report-no": null, "categories": "quant-ph cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most fundamental aspects of quantum circuit design is the concept\nof families of circuits parametrized by an instance size. As in classical\nprogramming, metaprogramming allows the programmer to write entire families of\ncircuits simultaneously, an ability which is of particular importance in the\ncontext of quantum computing as algorithms frequently use arithmetic over\nnon-standard word lengths. In this work, we introduce metaQASM, a typed\nextension of the openQASM language supporting the metaprogramming of circuit\nfamilies. Our language and type system, built around a lightweight\nimplementation of sized types, supports subtyping over register sizes and is\nmoreover type-safe. In particular, we prove that our system is strongly\nnormalizing, and as such any well-typed metaQASM program can be statically\nunrolled into a finite circuit.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 14:02:37 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Amy", "Matthew", ""]]}, {"id": "1908.02709", "submitter": "Massimo Bartoletti", "authors": "Massimo Bartoletti and Letterio Galletta and Maurizio Murgia", "title": "A minimal core calculus for Solidity contracts", "comments": "arXiv admin note: substantial text overlap with arXiv:1905.04366", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ethereum platform supports the decentralized execution of smart\ncontracts, i.e. computer programs that transfer digital assets between users.\nThe most common language used to develop these contracts is Solidity, a\nJavascript-like language which compiles into EVM bytecode, the language\nactually executed by Ethereum nodes. While much research has addressed the\nformalisation of the semantics of EVM bytecode, relatively little attention has\nbeen devoted to that of Solidity. In this paper we propose a minimal calculus\nfor Solidity contracts, which extends an imperative core with a single\nprimitive to transfer currency and invoke contract procedures. We build upon\nthis formalisation to give semantics to the Ethereum blockchain. We show our\ncalculus expressive enough to reason about some typical quirks of Solidity,\nlike e.g. re-entrancy.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 08:30:55 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Bartoletti", "Massimo", ""], ["Galletta", "Letterio", ""], ["Murgia", "Maurizio", ""]]}, {"id": "1908.02940", "submitter": "Peter Thiemann", "authors": "Peter Thiemann", "title": "Intrinsically-Typed Mechanized Semantics for Session Types", "comments": "Appears in PPDP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session types have emerged as a powerful paradigm for structuring\ncommunication-based programs. They guarantee type soundness and session\nfidelity for concurrent programs with sophisticated communication protocols. As\ntype soundness proofs for languages with session types are tedious and\ntechnically involved, it is rare to see mechanized soundness proofs for these\nsystems.\n  We present an executable intrinsically typed small-step semantics for a\nrealistic functional session type calculus. The calculus includes linearity,\nrecursion, and recursive sessions with subtyping. Asynchronous communication is\nmodeled with an encoding.\n  The semantics is implemented in Agda as an intrinsically typed, interruptible\nCEK machine. This implementation proves type preservation and a particular\nnotion of progress by construction.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 05:52:36 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Thiemann", "Peter", ""]]}, {"id": "1908.03010", "submitter": "Yuki Nishida", "authors": "Yuki Nishida and Atsushi Igarashi", "title": "Manifest Contracts with Intersection Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a manifest contract system PCFv$\\Delta$H with intersection types.\nA manifest contract system is a typed functional calculus in which software\ncontracts are integrated into a refinement type system and consistency of\ncontracts is checked by combination of compile- and run-time type checking.\nIntersection types naturally arise when a contract is expressed by a\nconjunction of smaller contracts. Run-time contract checking for conjunctive\nhigher-order contracts in an untyped language has been studied but our typed\nsetting poses an additional challenge due to the fact that an expression of an\nintersection type $\\tau_1 \\wedge \\tau_2$ may have to perform different run-time\nchecking whether it is used as $\\tau_1$ or $\\tau_2$. We build PCFv$\\Delta$H on\ntop of the $\\Delta$-calculus, a Church-style intersection type system by\nLiquori and Stolze. In the $\\Delta$-calculus, a canonical expression of an\nintersection type is a strong pair, whose elements are the same expressions\nexcept for type annotations. To address the challenge above, we relax strong\npairs so that expressions in a pair are the same except for type annotations\nand casts, which are a construct for run-time checking. We give a formal\ndefinition of PCFv$\\Delta$H and show its basic properties as a manifest\ncontract system: preservation, progress, and value inversion. Furthermore, we\nshow that run-time checking does not affect essential computation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 10:48:33 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 09:25:43 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Nishida", "Yuki", ""], ["Igarashi", "Atsushi", ""]]}, {"id": "1908.03316", "submitter": "Qiaochu Chen", "authors": "Qiaochu Chen, Xinyu Wang, Xi Ye, Greg Durrett, Isil Dillig", "title": "Multi-modal Synthesis of Regular Expressions", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a multi-modal synthesis technique for automatically\nconstructing regular expressions (regexes) from a combination of examples and\nnatural language. Using multiple modalities is useful in this context because\nnatural language alone is often highly ambiguous, whereas examples in isolation\nare often not sufficient for conveying user intent. Our proposed technique\nfirst parses the English description into a so-called hierarchical sketch that\nguides our programming-by-example (PBE) engine. Since the hierarchical sketch\ncaptures crucial hints, the PBE engine can leverage this information to both\nprioritize the search as well as make useful deductions for pruning the search\nspace. We have implemented the proposed technique in a tool called Regel and\nevaluate it on over three hundred regexes. Our evaluation shows that Regel\nachieves 80% accuracy whereas the NLP-only and PBE-only baselines achieve 43%\nand 26% respectively. We also compare our proposed PBE engine against an\nadaptation of AlphaRegex, a state-of-the-art regex synthesis tool, and show\nthat our proposed PBE engine is an order of magnitude faster, even if we adapt\nthe search algorithm of AlphaRegex to leverage the sketch. Finally, we conduct\na user study involving 20 participants and show that users are twice as likely\nto successfully come up with the desired regex using Regel compared to without\nit.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 05:20:17 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 08:59:56 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 04:46:42 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Chen", "Qiaochu", ""], ["Wang", "Xinyu", ""], ["Ye", "Xi", ""], ["Durrett", "Greg", ""], ["Dillig", "Isil", ""]]}, {"id": "1908.03619", "submitter": "Gabriel Scherer", "authors": "Ulysse G\\'erard, Dale Miller, Gabriel Scherer", "title": "Functional programming with lambda-tree syntax", "comments": "PPDP 2019", "journal-ref": null, "doi": "10.1145/3354166.3354177", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present the design of a new functional programming language, MLTS, that\nuses the lambda-tree syntax approach to encoding bindings appearing within data\nstructures. In this approach, bindings never become free nor escape their\nscope: instead, binders in data structures are permitted to move to binders\nwithin programs. The design of MLTS includes additional sites within programs\nthat directly support this movement of bindings. In order to formally define\nthe language's operational semantics, we present an abstract syntax for MLTS\nand a natural semantics for its evaluation. We shall view such natural\nsemantics as a logical theory within a rich logic that includes both *nominal\nabstraction* and the *nabla-quantifier*: as a result, the natural semantics\nspecification of MLTS can be given a succinct and elegant presentation. We\npresent a typing discipline that naturally extends the typing of core ML\nprograms and we illustrate the features of MLTS by presenting several examples.\nAn on-line interpreter for MLTS is briefly described.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 20:38:05 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["G\u00e9rard", "Ulysse", ""], ["Miller", "Dale", ""], ["Scherer", "Gabriel", ""]]}, {"id": "1908.03719", "submitter": "Esra Erdem", "authors": "Esra Erdem, Andrea Formisano, German Vidal, and Fangkai Yang", "title": "Introduction to the 35th International Conference on Logic Programming\n  Special Issue", "comments": "The 35th International Conference on Logic Programming (ICLP 2019),\n  Las Cruces, New Mexico, USA, September 20--25, 2019. 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are proud to introduce this special issue of Theory and Practice of Logic\nProgramming (TPLP), dedicated to the regular papers accepted for the 35th\nInternational Conference on Logic Programming (ICLP). The ICLP meetings started\nin Marseille in 1982 and since then constitute the main venue for presenting\nand discussing work in the area of logic programming. Under consideration for\nacceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 09:25:01 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Erdem", "Esra", ""], ["Formisano", "Andrea", ""], ["Vidal", "German", ""], ["Yang", "Fangkai", ""]]}, {"id": "1908.04265", "submitter": "Namolaru Mircea", "authors": "Mircea Namolaru and Thierry Goubier", "title": "Recursion, Probability, Convolution and Classification for Computations", "comments": "17 pages, revised version of a NIPS 2019 submission (with the same\n  title), combined with a revised (rejected) PLDI 2019 submission (with the\n  title \"Rubik's cube and Bayesian Networks\")", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main motivation of this work was practical, to offer computationally and\ntheoretical scalable ways to structuring large classes of computation. It\nstarted from attempts to optimize R code for machine learning/artificial\nintelligence algorithms for huge data sets, that due to their size, should be\nhandled into an incremental (online) fashion.\n  Our target are large classes of relational (attribute based), mathematical\n(index based) or graph computations. We wanted to use powerful computation\nrepresentations that emerged in AI (artificial intelligence)/ML (machine\nlearning) as BN (Bayesian networks) and CNN (convolution neural networks). For\nthe classes of computation addressed by us, and for our HPC (high performance\ncomputing) needs, the current solutions for translating computations into such\nrepresentation need to be extended.\n  Our results show that the classes of computation targeted by us, could be\ntree-structured, and a probability distribution (defining a DBN, i.e. Dynamic\nBayesian Network) associated with it. More ever, this DBN may be viewed as a\nrecursive CNN (Convolution Neural Network). Within this tree-like structure,\nclassification in classes with size bounded (by a parameterizable may be\nperformed.\n  These results are at the core of very powerful, yet highly practically\nalgorithms for restructuring and parallelizing the computations. The\nmathematical background required for an in depth presentation and exposing the\nfull generality of our approach) is the subject of a subsequent paper. In this\npaper, we work in an limited (but important) framework that could be understood\nwith rudiments of linear algebra and graph theory. The focus is in\napplicability, most of this paper discuss the usefulness of our approach for\nsolving hard compilation problems related to automatic parallelism.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 11:07:55 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Namolaru", "Mircea", ""], ["Goubier", "Thierry", ""]]}, {"id": "1908.04291", "submitter": "Dan Ghica", "authors": "Dan R. Ghica", "title": "The far side of the cube", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game-semantic models usually start from the core model of the prototypical\nlanguage PCF, which is characterised by a range of combinatorial constraints on\nthe shape of plays. Relaxing each such constraint usually corresponds to the\nintroduction of a new language operation, a feature of game semantics commonly\nknown as the `Abramsky Cube'. In this presentation we relax all such\ncombinatorial constraints, resulting in the most general game model, in which\nall the other game models live. This is perhaps the simplest set up in which to\nunderstand game semantics, so it should serve as a portal to the other, more\ncomplex, game models in the literature. It might also be interesting in its own\nright, as an extremal instance of the game-semantic paradigm.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 09:22:58 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Ghica", "Dan R.", ""]]}, {"id": "1908.04509", "submitter": "Constantin Enea", "authors": "Ranadeep Biswas and Constantin Enea", "title": "On the Complexity of Checking Transactional Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transactions simplify concurrent programming by enabling computations on\nshared data that are isolated from other concurrent computations and are\nresilient to failures. Modern databases provide different consistency models\nfor transactions corresponding to different tradeoffs between consistency and\navailability. In this work, we investigate the problem of checking whether a\ngiven execution of a transactional database adheres to some consistency model.\nWe show that consistency models like read committed, read atomic, and causal\nconsistency are polynomial time checkable while prefix consistency and snapshot\nisolation are NP-complete in general. These results complement a previous\nNP-completeness result concerning serializability. Moreover, in the context of\nNP-complete consistency models, we devise algorithms that are polynomial time\nassuming that certain parameters in the input executions, e.g., the number of\nsessions, are fixed. We evaluate the scalability of these algorithms in the\ncontext of several production databases.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 06:19:26 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Biswas", "Ranadeep", ""], ["Enea", "Constantin", ""]]}, {"id": "1908.04546", "submitter": "Bruce Collie", "authors": "Bruce Collie, Philip Ginsbach, Michael F.P. O'Boyle", "title": "Type-Directed Program Synthesis and Constraint Generation for Library\n  Portability", "comments": "Accepted to PACT 2019", "journal-ref": null, "doi": "10.1109/PACT.2019.00013", "report-no": null, "categories": "cs.PL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast numerical libraries have been a cornerstone of scientific computing for\ndecades, but this comes at a price. Programs may be tied to vendor specific\nsoftware ecosystems resulting in polluted, non-portable code. As we enter an\nera of heterogeneous computing, there is an explosion in the number of\naccelerator libraries required to harness specialized hardware. We need a\nsystem that allows developers to exploit ever-changing accelerator libraries,\nwithout over-specializing their code.\n  As we cannot know the behavior of future libraries ahead of time, this paper\ndevelops a scheme that assists developers in matching their code to new\nlibraries, without requiring the source code for these libraries.\n  Furthermore, it can recover equivalent code from programs that use existing\nlibraries and automatically port them to new interfaces. It first uses program\nsynthesis to determine the meaning of a library, then maps the synthesized\ndescription into generalized constraints which are used to search the program\nfor replacement opportunities to present to the developer.\n  We applied this approach to existing large applications from the scientific\ncomputing and deep learning domains. Using our approach, we show speedups\nranging from 1.1$\\times$ to over 10$\\times$ on end to end performance when\nusing accelerator libraries.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 08:58:44 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 15:32:33 GMT"}, {"version": "v3", "created": "Thu, 22 Aug 2019 17:06:48 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Collie", "Bruce", ""], ["Ginsbach", "Philip", ""], ["O'Boyle", "Michael F. P.", ""]]}, {"id": "1908.04921", "submitter": "EPTCS", "authors": "L\\^e Th\\`anh D\\~ung Nguyen (LIPN, Universit\\'e Paris 13)", "title": "On the Elementary Affine Lambda-Calculus with and Without Fixed Points", "comments": "In Proceedings DICE-FOPARA 2019, arXiv:1908.04478", "journal-ref": "EPTCS 298, 2019, pp. 15-29", "doi": "10.4204/EPTCS.298.2", "report-no": null, "categories": "cs.LO cs.CC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The elementary affine lambda-calculus was introduced as a polyvalent setting\nfor implicit computational complexity, allowing for characterizations of\npolynomial time and hyperexponential time predicates. But these results rely on\ntype fixpoints (a.k.a. recursive types), and it was unknown whether this\nfeature of the type system was really necessary. We give a positive answer by\nshowing that without type fixpoints, we get a characterization of regular\nlanguages instead of polynomial time. The proof uses the semantic evaluation\nmethod. We also propose an aesthetic improvement on the characterization of the\nfunction classes FP and k-FEXPTIME in the presence of recursive types.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 01:57:27 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Nguyen", "L\u00ea Th\u00e0nh D\u0169ng", "", "LIPN, Universit\u00e9 Paris 13"]]}, {"id": "1908.04922", "submitter": "EPTCS", "authors": "Paulin Jacob\\'e de Naurois (CNRS/universit\\'e paris13)", "title": "Pointers in Recursion: Exploring the Tropics", "comments": "In Proceedings DICE-FOPARA 2019, arXiv:1908.04478", "journal-ref": "EPTCS 298, 2019, pp. 31-45", "doi": "10.4204/EPTCS.298.3", "report-no": null, "categories": "cs.CC cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We translate the usual class of partial/primitive recursive functions to a\npointer recursion framework, accessing actual input values via a pointer\nreading unit-cost function. These pointer recursive functions classes are\nproven equivalent to the usual partial/primitive recursive functions.\nComplexity-wise, this framework captures in a streamlined way most of the\nrelevant sub-polynomial classes. Pointer recursion with the safe/normal tiering\ndiscipline of Bellantoni and Cook corresponds to polylogtime computation. We\nintroduce a new, non-size increasing tiering discipline, called tropical\ntiering. Tropical tiering and pointer recursion, used with some of the most\ncommon recursion schemes, capture the classes logspace, logspace/polylogtime,\nptime, and NC. Finally, in a fashion reminiscent of the safe recursive\nfunctions, tropical tiering is expressed directly in the syntax of the function\nalgebras, yielding the tropical recursive function algebras.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 01:57:44 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["de Naurois", "Paulin Jacob\u00e9", "", "CNRS/universit\u00e9 paris13"]]}, {"id": "1908.04923", "submitter": "EPTCS", "authors": "Bruce M. Kapron (University of Victoria), Florian Steinberg (INRIA\n  Saclay)", "title": "Type-two Iteration with Bounded Query Revision", "comments": "In Proceedings DICE-FOPARA 2019, arXiv:1908.04478", "journal-ref": "EPTCS 298, 2019, pp. 61-73", "doi": "10.4204/EPTCS.298.5", "report-no": null, "categories": "cs.CC cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent results of Kapron and Steinberg (LICS 2018) we introduce\nnew forms of iteration on length in the setting of applied lambda-calculi for\nhigher-type poly-time computability. In particular, in a type-two setting, we\nconsider functionals which capture iteration on input length which bound\ninteraction with the type-one input parameter, by restricting to a constant\neither the number of times the function parameter may return a value of\nincreasing size, or the number of times the function parameter may be applied\nto an argument of increasing size. We prove that for any constant bound, the\niterators obtained are equivalent, with respect to lambda-definability over\ntype-one poly-time functions, to the recursor of Cook and Urquhart which\ncaptures Cobham's notion of limited recursion on notation in this setting.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 01:58:18 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Kapron", "Bruce M.", "", "University of Victoria"], ["Steinberg", "Florian", "", "INRIA\n  Saclay"]]}, {"id": "1908.05294", "submitter": "Jason Hu", "authors": "Jason Hu, Ond\\v{r}ej Lhot\\'ak", "title": "Undecidability of $D_{<:}$ and Its Decidable Fragments", "comments": null, "journal-ref": null, "doi": "10.1145/3371077", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dependent Object Types (DOT) is a calculus with path dependent types,\nintersection types, and object self-references, which serves as the core\ncalculus of Scala 3. Although the calculus has been proven sound, it remains\nopen whether type checking in DOT is decidable. In this paper, we establish\nundecidability proofs of type checking and subtyping of $D_{<:}$, a syntactic\nsubset of DOT. It turns out that even for $D_{<:}$, undecidability is\nsurprisingly difficult to show, as evidenced by counterexamples for past\nattempts. To prove undecidability, we discover an equivalent definition of the\n$D_{<:}$ subtyping rules in normal form. Besides being easier to reason about,\nthis definition makes the phenomenon of bad bounds explicit as a single\ninference rule. After removing this rule, we discover two decidable fragments\nof $D_{<:}$ subtyping and identify algorithms to decide them. We prove\nsoundness and completeness of the algorithms with respect to the fragments, and\nwe prove that the algorithms terminate. Our proofs are mechanized in a\ncombination of Coq and Agda.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 18:15:47 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Hu", "Jason", ""], ["Lhot\u00e1k", "Ond\u0159ej", ""]]}, {"id": "1908.05535", "submitter": "Brandon Bohrer", "authors": "Brandon Bohrer and Andr\\'e Platzer", "title": "Toward Structured Proofs for Dynamic Logics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Kaisar, a structured interactive proof language for differential\ndynamic logic (dL), for safety-critical cyber-physical systems (CPS). The\ndefining feature of Kaisar is *nominal terms*, which simplify CPS proofs by\nmaking the frequently needed historical references to past program states\nfirst-class. To support nominals, we extend the notion of structured proof with\na first-class notion of *structured symbolic execution* of CPS models. We\nimplement Kaisar in the theorem prover KeYmaera X and reproduce an example on\nthe safe operation of a parachute and a case study on ground robot control. We\nshow how nominals simplify common CPS reasoning tasks when combined with other\nfeatures of structured proof. We develop an extensive metatheory for Kaisar. In\naddition to soundness and completeness, we show a formal specification for\nKaisar's nominals and relate Kaisar to a nominal variant of dL.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 13:46:13 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Bohrer", "Brandon", ""], ["Platzer", "Andr\u00e9", ""]]}, {"id": "1908.05647", "submitter": "Sebastian Ullrich", "authors": "Sebastian Ullrich and Leonardo de Moura", "title": "Counting Immutable Beans: Reference Counting Optimized for Purely\n  Functional Programming", "comments": null, "journal-ref": "IFL 2019", "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most functional languages rely on some garbage collection for automatic\nmemory management. They usually eschew reference counting in favor of a tracing\ngarbage collector, which has less bookkeeping overhead at runtime. On the other\nhand, having an exact reference count of each value can enable optimizations,\nsuch as destructive updates. We explore these optimization opportunities in the\ncontext of an eager, purely functional programming language. We propose a new\nmechanism for efficiently reclaiming memory used by nonshared values, reducing\nstress on the global memory allocator. We describe an approach for minimizing\nthe number of reference counts updates using borrowed references and a\nheuristic for automatically inferring borrow annotations. We implemented all\nthese techniques in a new compiler for an eager and purely functional\nprogramming language with support for multi-threading. Our preliminary\nexperimental results demonstrate our approach is competitive and often\noutperforms state-of-the-art compilers.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 17:28:01 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 18:28:43 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2020 17:03:14 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Ullrich", "Sebastian", ""], ["de Moura", "Leonardo", ""]]}, {"id": "1908.05655", "submitter": "Kiarash Rahmani", "authors": "Kia Rahmani, Kartik Nagar, Benjamin Delaware, Suresh Jagannathan", "title": "CLOTHO: Directed Test Generation for Weakly Consistent Database Systems", "comments": "Conditionally accepted to OOPSLA'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational database applications are notoriously difficult to test and debug.\nConcurrent execution of database transactions may violate complex structural\ninvariants that constraint how changes to the contents of one (shared) table\naffect the contents of another. Simplifying the underlying concurrency model is\none way to ameliorate the difficulty of understanding how concurrent accesses\nand updates can affect database state with respect to these sophisticated\nproperties. Enforcing serializable execution of all transactions achieves this\nsimplification, but it comes at a significant price in performance, especially\nat scale, where database state is often replicated to improve latency and\navailability. To address these challenges, this paper presents a novel testing\nframework for detecting serializability violations in (SQL) database-backed\nJava applications executing on weakly-consistent storage systems. We manifest\nour approach in a tool named CLOTHO, that combines a static analyzer and a\nmodel checker to generate abstract executions, discover serializability\nviolations in these executions, and translate them back into concrete test\ninputs suitable for deployment in a test environment. To the best of our\nknowledge, CLOTHO is the first automated test generation facility for\nidentifying serializability anomalies of Java applications intended to operate\nin geo-replicated distributed environments. An experimental evaluation on a set\nof industry-standard benchmarks demonstrates the utility of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 17:57:37 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Rahmani", "Kia", ""], ["Nagar", "Kartik", ""], ["Delaware", "Benjamin", ""], ["Jagannathan", "Suresh", ""]]}, {"id": "1908.05799", "submitter": "Arshavir Ter-Gabrielyan", "authors": "Arshavir Ter-Gabrielyan and Alexander J. Summers and Peter M\\\"uller", "title": "Modular Verification of Heap Reachability Properties in Separation Logic", "comments": "OOPSLA-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The correctness of many algorithms and data structures depends on\nreachability properties, that is, on the existence of chains of references\nbetween objects in the heap. Reasoning about reachability is difficult for two\nmain reasons. First, any heap modification may affect an unbounded number of\nreference chains, which complicates modular verification, in particular,\nframing. Second, general graph reachability is not supported by SMT solvers,\nwhich impedes automatic verification. In this paper, we present a modular\nspecification and verification technique for reachability properties in\nseparation logic. For each method, we specify reachability only locally within\nthe fragment of the heap on which the method operates. A novel form of\nreachability framing for relatively convex subheaps allows one to extend\nreachability properties from the heap fragment of a callee to the larger\nfragment of its caller, enabling precise procedure-modular reasoning. Our\ntechnique supports practically important heap structures, namely acyclic graphs\nwith a bounded outdegree as well as (potentially cyclic) graphs with at most\none path (modulo cycles) between each pair of nodes. The integration into\nseparation logic allows us to reason about reachability and other properties in\na uniform way, to verify concurrent programs, and to automate our technique via\nexisting separation logic verifiers. We demonstrate that our verification\ntechnique is amenable to SMT-based verification by encoding a number of\nbenchmark examples into the Viper verification infrastructure.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 00:18:43 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Ter-Gabrielyan", "Arshavir", ""], ["Summers", "Alexander J.", ""], ["M\u00fcller", "Peter", ""]]}, {"id": "1908.05839", "submitter": "Jana Dunfield", "authors": "Jana Dunfield and Neel Krishnaswami", "title": "Bidirectional Typing", "comments": "37 pages; submitted to ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bidirectional typing combines two modes of typing: type checking, which\nchecks that a program satisfies a known type, and type synthesis, which\ndetermines a type from the program. Using checking enables bidirectional typing\nto support features for which inference is undecidable; using synthesis enables\nbidirectional typing to avoid the large annotation burden of explicitly typed\nlanguages. In addition, bidirectional typing improves error locality. We\nhighlight the design principles that underlie bidirectional type systems,\nsurvey the development of bidirectional typing from the prehistoric period\nbefore Pierce and Turner's local type inference to the present day, and provide\nguidance for future investigations.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 04:22:23 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 18:26:29 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Dunfield", "Jana", ""], ["Krishnaswami", "Neel", ""]]}, {"id": "1908.05845", "submitter": "Matthias Springer", "authors": "Matthias Springer", "title": "Memory-Efficient Object-Oriented Programming on GPUs", "comments": "Ph.D. Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object-oriented programming is often regarded as too inefficient for\nhigh-performance computing (HPC), despite the fact that many important HPC\nproblems have an inherent object structure. Our goal is to bring efficient,\nobject-oriented programming to massively parallel SIMD architectures,\nespecially GPUs.\n  In this thesis, we develop various techniques for optimizing object-oriented\nGPU code. Most notably, we identify the object-oriented Single-Method\nMultiple-Objects (SMMO) programming model. We first develop an embedded C++\nStructure of Arrays (SOA) data layout DSL for SMMO applications. We then design\na lock-free, dynamic memory allocator that stores allocations in SOA layout.\nFinally, we show how to further optimize the memory access of SMMO applications\nwith memory defragmentation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 04:50:29 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Springer", "Matthias", ""]]}, {"id": "1908.05940", "submitter": "Tim Soethout", "authors": "Tim Soethout (ING Bank / Centrum Wiskunde & Informatica (CWI),\n  Netherlands), Tijs van der Storm (Centrum Wiskunde & Informatica (CWI) /\n  Universiteit Groningen, Netherlands), Jurgen Vinju (Centrum Wiskunde &\n  Informatica (CWI) / TU Eindhoven, Netherlands)", "title": "Path-Sensitive Atomic Commit: Local Coordination Avoidance for\n  Distributed Transactions", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2021, Vol. 5,\n  Issue 1, Article 3", "doi": "10.22152/programming-journal.org/2021/5/3", "report-no": null, "categories": "cs.DC cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Concurrent objects with asynchronous messaging are an increasingly\npopular way to structure highly available, high performance, large-scale\nsoftware systems. To ensure data-consistency and support synchronization\nbetween objects such systems often use distributed transactions with Two-Phase\nLocking (2PL) for concurrency control and Two-Phase commit (2PC) as atomic\ncommitment protocol. Inquiry In highly available, high-throughput systems, such\nas large banking infrastructure, however, 2PL becomes a bottleneck when objects\nare highly contended, when an object is queuing a lot of messages because of\nlocking.\n  Approach: In this paper we introduce Path-Sensitive Atomic Commit (PSAC) to\naddress this situation. We start from message handlers (or methods), which are\ndecorated with pre- and post-conditions, describing their guards and effect.\n  Knowledge: This allows the PSAC lock mechanism to check whether the effect of\ntwo incoming messages at the same time are independent, and to avoid locking if\nthis is the case. As a result, more messages are directly accepted or rejected,\nand higher overall throughput is obtained.\n  Grounding: We have implemented PSAC for a state machine-based DSL called\nRebel, on top of a runtime based on the Akka actor framework. Our performance\nevaluation shows that PSAC exhibits the same scalability and latency\ncharacteristics as standard 2PL/2PC, and obtains up to 1.8 times median higher\nthroughput in congested scenarios.\n  Importance: We believe PSAC is a step towards enabling organizations to build\nscalable distributed applications, even if their consistency requirements are\nnot embarrassingly parallel.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 11:52:05 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 18:30:12 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Soethout", "Tim", "", "ING Bank / Centrum Wiskunde & Informatica"], ["van der Storm", "Tijs", "", "Centrum Wiskunde & Informatica"], ["Vinju", "Jurgen", "", "Centrum Wiskunde &\n  Informatica"]]}, {"id": "1908.05979", "submitter": "Chuangjie Xu", "authors": "Chuangjie Xu", "title": "A Gentzen-style monadic translation of G\\\"odel's System T", "comments": "17 pages. Changes: (1) remove the restriction of satisfying the monad\n  laws in the definition of nuclei, (2) add a unified theorem of logical\n  relation. This paper will appear in FSCD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a syntactic translation of Goedel's System T parametrized by a\nweak notion of a monad, and prove a corresponding fundamental theorem of\nlogical relation. Our translation structurally corresponds to Gentzen's\nnegative translation of classical logic. By instantiating the monad and the\nlogical relation, we reveal the well-known properties and structures of\nT-definable functionals including majorizability, continuity and bar recursion.\nOur development has been formalized in the Agda proof assistant.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 14:04:35 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 15:45:49 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Xu", "Chuangjie", ""]]}, {"id": "1908.06223", "submitter": "Aditya Thakur", "authors": "Matthew Sotoudeh and Aditya V. Thakur", "title": "A Symbolic Neural Network Representation and its Application to\n  Understanding, Verifying, and Patching Networks", "comments": "Code is available at https://github.com/95616ARG/SyReNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis and manipulation of trained neural networks is a challenging and\nimportant problem. We propose a symbolic representation for piecewise-linear\nneural networks and discuss its efficient computation. With this\nrepresentation, one can translate the problem of analyzing a complex neural\nnetwork into that of analyzing a finite set of affine functions. We demonstrate\nthe use of this representation for three applications. First, we apply the\nsymbolic representation to computing weakest preconditions on network inputs,\nwhich we use to exactly visualize the advisories made by a network meant to\noperate an aircraft collision avoidance system. Second, we use the symbolic\nrepresentation to compute strongest postconditions on the network outputs,\nwhich we use to perform bounded model checking on standard neural network\ncontrollers. Finally, we show how the symbolic representation can be combined\nwith a new form of neural network to perform patching; i.e., correct\nuser-specified behavior of the network.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2019 01:48:50 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 03:22:57 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Sotoudeh", "Matthew", ""], ["Thakur", "Aditya V.", ""]]}, {"id": "1908.06478", "submitter": "EPTCS", "authors": "Franz Siglm\\\"uller (Ludwig Maximilian University of Munich)", "title": "Type-Based Resource Analysis on Haskell", "comments": "In Proceedings DICE-FOPARA 2019, arXiv:1908.04478", "journal-ref": "EPTCS 298, 2019, pp. 47-60", "doi": "10.4204/EPTCS.298.4", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an amortized analysis that approximates the resource usage of a\nHaskell expression. Using the plugin API of GHC, we convert the Haskell code\ninto a simplified representation called GHC Core. We then apply a type-based\nsystem which derives linear upper bounds on the resource usage. This setup\nallows us to analyze actual Haskell code, whereas previous implementations of\nsimilar analyses do not support any commonly used lazy functional programming\nlanguages.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 01:57:59 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Siglm\u00fcller", "Franz", "", "Ludwig Maximilian University of Munich"]]}, {"id": "1908.06601", "submitter": "Mike Ji", "authors": "Mike H. Ji", "title": "Implicit Recursive Characteristics of STOP", "comments": "5 pages. A proof that STOP itself is a recursive process.\n  STOP$_{\\alpha X} = \\mu$ X. nil $\\rightarrow$ X", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most important notations of Communicating Sequential Process(CSP) are the\nprocess and the prefix (event)$\\rightarrow$(process) operator. While we can\nformally apply the $\\rightarrow$ operator to define a live process's behavior,\nthe STOP process, which usually resulted from deadlock, starving or livelock,\nis lack of formal description, defined by most literatures as \"doing nothing\nbut halt\". In this paper, we argue that the STOP process should not be\nconsidered as a black box, it should follow the prefix $\\rightarrow$ schema and\nthe same inference rules so that a unified and consistent process algebra model\ncan be established. In order to achieve this goal, we introduce a special event\ncalled \"nil\" that any process can take. This nil event will do nothing\nmeaningful and leave nothing on a process's observable record. With the nil\nevent and its well-defined rules, we can successfully use the $\\rightarrow$\noperator to formally describe a process's complete behavior in its whole life\ncircle. More interestingly, we can use prefix $\\rightarrow$ and nil event to\nfully describe the STOP process's internal behavior and conclude that the\nSTOP's formal equation can be given as simple as STOP$_{\\alpha X} = \\mu$ X. nil\n$\\rightarrow$ X.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 05:43:13 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 03:22:01 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Ji", "Mike H.", ""]]}, {"id": "1908.06723", "submitter": "EPTCS", "authors": "Alexei Lisitsa (The University of Liverpool), Andrei Nemytykh (Program\n  Systems Institute of RAS)", "title": "Proceedings Seventh International Workshop on Verification and Program\n  Transformation", "comments": null, "journal-ref": "EPTCS 299, 2019", "doi": "10.4204/EPTCS.299", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains a final and revised selection of papers presented at the\nSeventh International Workshop on Verification and Program Transformation (VPT\n2019), which took place in Genova, Italy, on April 2nd, 2019, affiliated with\nProgramming 2019.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 05:53:26 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Lisitsa", "Alexei", "", "The University of Liverpool"], ["Nemytykh", "Andrei", "", "Program\n  Systems Institute of RAS"]]}, {"id": "1908.07188", "submitter": "EPTCS", "authors": "Emanuele De Angelis (DEC, University \"G. d'Annunzio\" of\n  Chieti-Pescara, Italy), Fabio Fioravanti (DEC, University \"G. d'Annunzio\" of\n  Chieti-Pescara, Italy), Alberto Pettorossi (DICII, University of Roma Tor\n  Vergata, Italy), Maurizio Proietti (CNR-IASI, Rome, Italy)", "title": "Lemma Generation for Horn Clause Satisfiability: A Preliminary Study", "comments": "In Proceedings VPT 2019, arXiv:1908.06723", "journal-ref": "EPTCS 299, 2019, pp. 4-18", "doi": "10.4204/EPTCS.299.4", "report-no": null, "categories": "cs.LO cs.PL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the verification of imperative, functional, and logic\nprograms can be reduced to the satisfiability of constrained Horn clauses\n(CHCs), and this satisfiability check can be performed by using CHC solvers,\nsuch as Eldarica and Z3. These solvers perform well when they act on simple\nconstraint theories, such as Linear Integer Arithmetic and the theory of\nBooleans, but their efficacy is very much reduced when the clauses refer to\nconstraints on inductively defined structures, such as lists or trees.\nRecently, we have presented a transformation technique for eliminating those\ninductively defined data structures, and hence avoiding the need for\nincorporating induction principles into CHC solvers. However, this technique\nmay fail when the transformation requires the use of lemmata whose generation\nneeds ingenuity. In this paper we show, through an example, how during the\nprocess of transforming CHCs for eliminating inductively defined structures one\ncan introduce suitable predicates, called difference predicates, whose\ndefinitions correspond to the lemmata to be introduced. Through a second\nexample, we show that, whenever difference predicates cannot be introduced, we\ncan introduce, instead, auxiliary queries which also correspond to lemmata, and\nthe proof of these lemmata can be done by showing the satisfiability of those\nqueries.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 06:35:19 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["De Angelis", "Emanuele", "", "DEC, University \"G. d'Annunzio\" of\n  Chieti-Pescara, Italy"], ["Fioravanti", "Fabio", "", "DEC, University \"G. d'Annunzio\" of\n  Chieti-Pescara, Italy"], ["Pettorossi", "Alberto", "", "DICII, University of Roma Tor\n  Vergata, Italy"], ["Proietti", "Maurizio", "", "CNR-IASI, Rome, Italy"]]}, {"id": "1908.07189", "submitter": "EPTCS", "authors": "John P. Gallagher (Roskilde University and IMDEA Software Institute)", "title": "Polyvariant Program Specialisation with Property-based Abstraction", "comments": "In Proceedings VPT 2019, arXiv:1908.06723", "journal-ref": "EPTCS 299, 2019, pp. 34-48", "doi": "10.4204/EPTCS.299.6", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that property-based abstraction, an established\ntechnique originating in software model checking, is a flexible method of\ncontrolling polyvariance in program specialisation in a standard online\nspecialisation algorithm. Specialisation is a program transformation that\ntransforms a program with respect to given constraints that restrict its\nbehaviour. Polyvariant specialisation refers to the generation of two or more\nspecialised versions of the same program code. The same program point can be\nreached more than once during a computation, with different constraints\napplying in each case, and polyvariant specialisation allows different\nspecialisations to be realised. A property-based abstraction uses a finite set\nof properties to define a finite set of abstract versions of predicates,\nensuring that only a finite number of specialised versions is generated. The\nparticular choice of properties is critical for polyvariance; too few versions\ncan result in insufficient specialisation, while too many can result in an\nincrease of code size with no corresponding efficiency gains. Using examples,\nwe show the flexibility of specialisation with property-based abstraction and\ndiscuss its application in control flow refinement, verification, termination\nanalysis and dimension-based specialisation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 06:36:01 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Gallagher", "John P.", "", "Roskilde University and IMDEA Software Institute"]]}, {"id": "1908.07563", "submitter": "Louis Mandel", "authors": "Guillaume Baudart, Louis Mandel, Eric Atkinson, Benjamin Sherman, Marc\n  Pouzet, Michael Carbin", "title": "Reactive Probabilistic Programming", "comments": "Version with appendices of the PLDI 2020 paper \"Reactive\n  Probabilistic Programming\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronous modeling is at the heart of programming languages like Lustre,\nEsterel, or Scade used routinely for implementing safety critical control\nsoftware, e.g., fly-by-wire and engine control in planes. However, to date\nthese languages have had limited modern support for modeling uncertainty --\nprobabilistic aspects of software's environment or behavior -- even though\nmodeling uncertainty is a primary activity when designing a control system.\n  In this paper we present ProbZelus the first synchronous probabilistic\nprogramming language. ProbZelus conservatively provides the facilities of a\nsynchronous language to write control software, with probabilistic constructs\nto model uncertainties and perform inference-in-the-loop.\n  We present the design and implementation of the language. We propose a\nmeasure-theoretic semantics of probabilistic stream functions and a simple type\ndiscipline to separate deterministic and probabilistic expressions. We\ndemonstrate a semantics-preserving compilation into a first-order functional\nlanguage that lends itself to a simple presentation of inference algorithms for\nstreaming models. We also redesign the delayed sampling inference algorithm to\nprovide efficient streaming inference. Together with an evaluation on several\nreactive applications, our results demonstrate that ProbZelus enables the\ndesign of reactive probabilistic applications and efficient, bounded memory\ninference.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 18:37:44 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 21:13:41 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Baudart", "Guillaume", ""], ["Mandel", "Louis", ""], ["Atkinson", "Eric", ""], ["Sherman", "Benjamin", ""], ["Pouzet", "Marc", ""], ["Carbin", "Michael", ""]]}, {"id": "1908.07776", "submitter": "Janis Voigtl\\\"ander", "authors": "Janis Voigtl\\\"ander", "title": "Free Theorems Simply, via Dinaturality", "comments": "Part of DECLARE 19 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Free theorems are a popular tool in reasoning about parametrically\npolymorphic code. They are also of instructive use in teaching. Their\nderivation, though, can be tedious, as it involves unfolding a lot of\ndefinitions, then hoping to be able to simplify the resulting logical formula\nto something nice and short. Even in a mechanised generator it is not easy to\nget the right heuristics in place to achieve good outcomes. Dinaturality is a\ncategorical abstraction that captures many instances of free theorems.\nArguably, its origins are more conceptually involved to explain, though, and\ngenerating useful statements from it also has its pitfalls. We present a simple\napproach for obtaining dinaturality-related free theorems from the standard\nformulation of relational parametricity in a rather direct way. It is\nconceptually appealing and easy to control and implement, as the provided\nHaskell code shows.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 10:16:16 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Voigtl\u00e4nder", "Janis", ""]]}, {"id": "1908.07883", "submitter": "Heather Miller", "authors": "Filip K\\v{r}ikava, Heather Miller, Jan Vitek", "title": "Scala Implicits are Everywhere: A large-scale study of the use of\n  Implicits in the wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Scala programming language offers two distinctive language features\nimplicit parameters and implicit conversions, often referred together as\nimplicits. Announced without fanfare in 2004, implicits have quickly grown to\nbecome a widely and pervasively used feature of the language. They provide a\nway to reduce the boilerplate code in Scala programs. They are also used to\nimplement certain language features without having to modify the compiler. We\nreport on a large-scale study of the use of implicits in the wild. For this, we\nanalyzed 7,280 Scala projects hosted on GitHub, spanning over 8.1M call sites\ninvolving implicits and 370.7K implicit declarations across 18.7M lines of\nScala code.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 14:12:10 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 12:56:15 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 18:19:08 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["K\u0159ikava", "Filip", ""], ["Miller", "Heather", ""], ["Vitek", "Jan", ""]]}, {"id": "1908.08213", "submitter": "EPTCS", "authors": "Jorge A. P\\'erez (University of Groningen), Jurriaan Rot (UCL and\n  Radboud University)", "title": "Proceedings Combined 26th International Workshop on Expressiveness in\n  Concurrency and 16th Workshop on Structural Operational Semantics", "comments": null, "journal-ref": "EPTCS 300, 2019", "doi": "10.4204/EPTCS.300", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of EXPRESS/SOS 2019: the Combined 26th\nInternational Workshop on Expressiveness in Concurrency and the 16th Workshop\non Structural Operational Semantics, which was held on August 26, 2019, in\nAmsterdam (The Netherlands), as an affiliated workshop of CONCUR 2019, the 30th\nInternational Conference on Concurrency Theory.\n  The EXPRESS/SOS workshop series aims at bringing together researchers\ninterested in the formal semantics of systems and programming concepts, and in\nthe expressiveness of computational models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 06:06:57 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["P\u00e9rez", "Jorge A.", "", "University of Groningen"], ["Rot", "Jurriaan", "", "UCL and\n  Radboud University"]]}, {"id": "1908.08963", "submitter": "Yunong Shi", "authors": "Yunong Shi, Runzhou Tao, Xupeng Li, Ali Javadi-Abhari, Andrew W.\n  Cross, Frederic T. Chong, Ronghui Gu", "title": "CertiQ: A Mostly-automated Verification of a Realistic Quantum Compiler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.ET cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CertiQ, a verification framework for writing and verifying\ncompiler passes of Qiskit, the most widely-used quantum compiler. To our\nknowledge, CertiQ is the first effort enabling the verification of real-world\nquantum compiler passes in a mostly-automated manner. Compiler passes written\nin the CertiQ interface with annotations can be used to generate verification\nconditions, as well as the executable code that can be integrated into Qiskit.\nCertiQ introduces the quantum circuit calculus to enable the efficient checking\nof equivalence of quantum circuits by encoding such a checking procedure into\nan SMT problem. CertiQ also provides a verified library of widely-used data\nstructures, transformation functions for circuits, and conversion functions for\ndifferent quantum data representations. This verified library not only enables\nmodular verification but also sheds light on future quantum compiler design. We\nhave re-implemented and verified 26 (out of 30) Qiskit compiler passes in\nCertiQ, during which three bugs are detected in the Qiskit implementation. Our\nverified compiler pass implementations passed all of Qiskit's regression tests\nwithout showing noticeable performance loss.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 18:02:17 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 02:59:04 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2019 19:38:44 GMT"}, {"version": "v4", "created": "Tue, 24 Nov 2020 04:26:32 GMT"}, {"version": "v5", "created": "Thu, 26 Nov 2020 14:09:35 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Shi", "Yunong", ""], ["Tao", "Runzhou", ""], ["Li", "Xupeng", ""], ["Javadi-Abhari", "Ali", ""], ["Cross", "Andrew W.", ""], ["Chong", "Frederic T.", ""], ["Gu", "Ronghui", ""]]}, {"id": "1908.09123", "submitter": "Gabriel Scherer", "authors": "Pierre-\\'Evariste Dagand, Lionel Rieg, Gabriel Scherer", "title": "Dependent Pearl: Normalization by realizability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  For those of us who generally live in the world of syntax, semantic proof\ntechniques such as reducibility, realizability or logical relations seem\nsomewhat magical despite -- or perhaps due to -- their seemingly unreasonable\neffectiveness. Why do they work? At which point in the proof is \"the real work\"\ndone?\n  Hoping to build a programming intuition of these proofs, we implement a\nnormalization argument for the simply-typed lambda-calculus with sums: instead\nof a proof, it is described as a program in a dependently-typed meta-language.\n  The semantic technique we set out to study is Krivine's classical\nrealizability, which amounts to a proof-relevant presentation of reducibility\narguments -- unary logical relations. Reducibility assigns a predicate to each\ntype, realizability assigns a set of realizers, which are abstract machines\nthat extend lambda-terms with a first-class notion of contexts. Normalization\nis a direct consequence of an adequacy theorem or \"fundamental lemma\", which\nstates that any well-typed term translates to a realizer of its type.\n  We show that the adequacy theorem, when written as a dependent program,\ncorresponds to an evaluation procedure. In particular, a weak normalization\nproof precisely computes a series of reduction from the input term to a normal\nform. Interestingly, the choices that we make when we define the reducibility\npredicates -- truth and falsity witnesses for each connective -- determine the\nevaluation order of the proof, with each datatype constructor behaving in a\nlazy or strict fashion.\n  While most of the ideas in this presentation are folklore among specialists,\nour dependently-typed functional program provides an accessible presentation to\na wider audience. In particular, our work provides a gentle introduction to\nabstract machine calculi which have recently been used as an effective research\nvehicle.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 10:53:27 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 12:56:02 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Dagand", "Pierre-\u00c9variste", ""], ["Rieg", "Lionel", ""], ["Scherer", "Gabriel", ""]]}, {"id": "1908.09302", "submitter": "EPTCS", "authors": "Davide Ancona (DIBRIS, University of Genova, Italy), Gordon Pace\n  (Department of Computer Science, University of Malta)", "title": "Proceedings of the Second Workshop on Verification of Objects at RunTime\n  EXecution", "comments": null, "journal-ref": "EPTCS 302, 2019", "doi": "10.4204/EPTCS.302", "report-no": null, "categories": "cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the post-proceedings of the second Workshop on\nVerification of Objects at RunTime EXecution (VORTEX 2018) that was held in\nAmsterdam, co-located with the European Conference on Object-Oriented\nProgramming (ECOOP 2018) and the ACM SIGSOFT International Symposium on\nSoftware Testing and Analysis (ISSTA 2018).\n  Runtime verification is an approach to software verification which is\nconcerned with monitoring and analysis of software and hardware system\nexecutions. Recently, it has gained more consensus as an effective and\npromising approach to ensure software reliability, bridging a gap between\nformal verification, and conventional testing; monitoring a system during\nruntime execution offers additional opportunities for addressing error\nrecovery, self-adaptation, and other issues that go beyond software\nreliability. The goal of VORTEX is to bring together researchers working on\nruntime verification for topics covering either theoretical, or practical\naspects, or, preferably, both, with emphasis on object-oriented languages, and\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 11:23:42 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Ancona", "Davide", "", "DIBRIS, University of Genova, Italy"], ["Pace", "Gordon", "", "Department of Computer Science, University of Malta"]]}, {"id": "1908.09681", "submitter": "Gabriel Radanne", "authors": "Gabriel Radanne, Hannes Saffrich and Peter Thiemann", "title": "Kindly Bent to Free Us", "comments": "ICFP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Systems programming often requires the manipulation of resources like file\nhandles, network connections, or dynamically allocated memory. Programmers need\nto follow certain protocols to handle these resources correctly. Violating\nthese protocols causes bugs ranging from type mismatches over data races to\nuse-after-free errors and memory leaks. These bugs often lead to security\nvulnerabilities.\n  While statically typed programming languages guarantee type soundness and\nmemory safety by design, most of them do not address issues arising from\nimproper handling of resources. An important step towards handling resources is\nthe adoption of linear and affine types that enforce single-threaded resource\nusage. However, the few languages supporting such types require heavy type\nannotations.\n  We present Affe, an extension of ML that manages linearity and affinity\nproperties using kinds and constrained types. In addition Affe supports the\nexclusive and shared borrowing of affine resources, inspired by features of\nRust. Moreover, Affe retains the defining features of the ML family: it is an\nimpure, strict, functional expression language with complete principal type\ninference and type abstraction. Affe does not require any linearity annotations\nin expressions and supports common functional programming idioms.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 13:40:51 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 09:38:34 GMT"}, {"version": "v3", "created": "Wed, 18 Mar 2020 14:03:24 GMT"}, {"version": "v4", "created": "Thu, 25 Jun 2020 13:30:58 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Radanne", "Gabriel", ""], ["Saffrich", "Hannes", ""], ["Thiemann", "Peter", ""]]}, {"id": "1908.09758", "submitter": "Ton Chanh Le", "authors": "Wei-Ngan Chin, Ton Chanh Le, Shengchao Qin", "title": "Automated Verification of CountDownLatch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CountDownLatch (CDL) is a versatile concurrency mechanism that was first\nintroduced in Java 5, and is also being adopted into C++ and C#. Its usage\nallows one or more threads to exchange resources and synchronize by waiting for\nsome tasks to be completed before others can proceed. In this paper, we propose\na new framework for verifying the correctness of concurrent applications that\nuse CDLs. Our framework is built on top of two existing mechanisms, concurrent\nabstract predicate and fictional separation logic, with some enhancements such\nas borrowed heap and thread local abstraction. In addition, we propose a new\ninconsistency detection mechanism based on waits-for relation to guarantee\ndeadlock freedom. Prior concurrency verification works have mostly focused on\ndata-race freedom. As a practical proof of concept, we have implemented this\nnew specification and verification mechanism for CDL in a new tool, called\nHIPCAP, on top of an existing HIP verifier. We have used this new tool to\nsuccessfully verify various use cases for CDL.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 15:58:27 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Chin", "Wei-Ngan", ""], ["Le", "Ton Chanh", ""], ["Qin", "Shengchao", ""]]}, {"id": "1908.10041", "submitter": "EPTCS", "authors": "Eduardo Geraldo (NOVA LINCS - Faculdade de Ci\\^encias e Tecnologia da\n  Universidade Nova de Lisboa), Jo\\~ao Costa Seco (NOVA LINCS - Faculdade de\n  Ci\\^encias e Tecnologia da Universidade Nova de Lisboa)", "title": "SNITCH: Dynamic Dependent Information Flow Analysis for Independent Java\n  Bytecode", "comments": "In Proceedings VORTEX 2018, arXiv:1908.09302", "journal-ref": "EPTCS 302, 2019, pp. 16-31", "doi": "10.4204/EPTCS.302.2", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software testing is the most commonly used technique in the industry to\ncertify the correctness of software systems. This includes security properties\nlike access control and data confidentiality. However, information flow control\nand the detection of information leaks using tests is a demanding task without\nthe use of specialized monitoring and assessment tools.\n  In this paper, we tackle the challenge of dynamically tracking information\nflow in third-party Java-based applications using dependent information flow\ncontrol. Dependent security labels increase the expressiveness of traditional\ninformation flow control techniques by allowing to parametrize labels with\ncontext-related information and allowing for the specification of more detailed\nand fine-grained policies. Instead of the fixed security lattice used in\ntraditional approaches that defines a fixed set of security compartments,\ndependent security labels allow for a dynamic lattice that can be extended at\nruntime, allowing for new security compartments to be defined using context\nvalues.\n  We present a specification and instrumentation approach for rewriting JVM\ncompiled code with in-lined reference monitors. To illustrate the proposed\napproach we use an example and a working prototype, SNITCH. SNITCH operates\nover the static single assignment language Shimple, an intermediate\nrepresentation for Java bytecode used in the SOOT framework.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 06:19:59 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Geraldo", "Eduardo", "", "NOVA LINCS - Faculdade de Ci\u00eancias e Tecnologia da\n  Universidade Nova de Lisboa"], ["Seco", "Jo\u00e3o Costa", "", "NOVA LINCS - Faculdade de\n  Ci\u00eancias e Tecnologia da Universidade Nova de Lisboa"]]}, {"id": "1908.10051", "submitter": "Hong Long Pham", "authors": "Long H. Pham, Jun Sun, and Quang Loc Le", "title": "Compositional Verification of Heap-Manipulating Programs through\n  Property-Guided Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing and verifying heap-manipulating programs automatically is\nchallenging. A key for fighting the complexity is to develop compositional\nmethods. For instance, many existing verifiers for heap-manipulating programs\nrequire user-provided specification for each function in the program in order\nto decompose the verification problem. The requirement, however, often hinders\nthe users from applying such tools. To overcome the issue, we propose to\nautomatically learn heap-related program invariants in a property-guided way\nfor each function call. The invariants are learned based on the memory graphs\nobserved during test execution and improved through memory graph mutation. We\nimplemented a prototype of our approach and integrated it with two existing\nprogram verifiers. The experimental results show that our approach enhances\nexisting verifiers effectively in automatically verifying complex\nheap-manipulating programs with multiple function calls.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 07:01:43 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Pham", "Long H.", ""], ["Sun", "Jun", ""], ["Le", "Quang Loc", ""]]}, {"id": "1908.10264", "submitter": "Jan C. Dagef\\\"orde", "authors": "Jan C. Dagef\\\"orde and Finn Teegen", "title": "Structured Traversal of Search Trees in Constraint-logic Object-oriented\n  Programming", "comments": "Part of DECLARE 19 proceedings", "journal-ref": null, "doi": "10.1007/978-3-030-46714-2_13", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an explicit, non-strict representation of search\ntrees in constraint-logic object-oriented programming. Our search tree\nrepresentation includes both the non-deterministic and deterministic behaviour\nduring execution of an application. Introducing such a representation\nfacilitates the use of various search strategies. In order to demonstrate the\napplicability of our approach, we incorporate explicit search trees into the\nvirtual machine of the constraint-logic object-oriented programming language\nMuli. We then exemplarily implement three search algorithms that traverse the\nsearch tree on-demand: depth-first search, breadth-first search, and iterative\ndeepening depth-first search. In particular, the last two strategies allow for\na complete search, which is novel in constraint-logic object-oriented\nprogramming and highlights our main contribution. Finally, we compare the\nimplemented strategies using several benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 15:15:30 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Dagef\u00f6rde", "Jan C.", ""], ["Teegen", "Finn", ""]]}, {"id": "1908.10273", "submitter": "Jonathan DiLorenzo", "authors": "Jonathan DiLorenzo, Katie Mancini, Kathleen Fisher, Nate Foster", "title": "TxForest: A DSL for Concurrent Filestores", "comments": "Tech Report, 29 pages, 12 figures, to be published in APLAS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many systems use ad hoc collections of files and directories to store\npersistent data. For consumers of this data, the process of properly parsing,\nusing, and updating these filestores using conventional APIs is cumbersome and\nerror-prone. Making matters worse, most filestores are too big to fit in\nmemory, so applications must process the data incrementally while managing\nconcurrent accesses by multiple users. This paper presents Transactional Forest\n(TxForest), which builds on earlier work on Forest to provide a simpler, more\npowerful API for managing filestores, including a mechanism for managing\nconcurrent accesses using serializable transactions. Under the hood, TxForest\nimplements an optimistic concurrency control scheme using Huet's zippers to\ntrack the data associated with filestores. We formalize TxForest in a core\ncalculus, develop a proof of serializability, and describe our OCaml prototype,\nwhich we have used to build several practical applications.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 15:29:30 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["DiLorenzo", "Jonathan", ""], ["Mancini", "Katie", ""], ["Fisher", "Kathleen", ""], ["Foster", "Nate", ""]]}, {"id": "1908.10416", "submitter": "Youkichi Hosoi", "authors": "Youkichi Hosoi, Naoki Kobayashi, Takeshi Tsukada", "title": "A Type-Based HFL Model Checking Algorithm", "comments": "A longer version of APLAS 2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher-order modal fixpoint logic (HFL) is a higher-order extension of the\nmodal mu-calculus, and strictly more expressive than the modal mu-calculus. It\nhas recently been shown that various program verification problems can\nnaturally be reduced to HFL model checking: the problem of whether a given\nfinite state system satisfies a given HFL formula. In this paper, we propose a\nnovel algorithm for HFL model checking: it is the first practical algorithm in\nthat it runs fast for typical inputs, despite the hyper-exponential worst-case\ncomplexity of the HFL model checking problem. Our algorithm is based on\nKobayashi et al.'s type-based characterization of HFL model checking, and was\ninspired by a saturation-based algorithm for HORS model checking, another\nhigher-order extension of model checking. We prove the correctness of the\nalgorithm and report on an implementation and experimental results.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 19:14:27 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Hosoi", "Youkichi", ""], ["Kobayashi", "Naoki", ""], ["Tsukada", "Takeshi", ""]]}, {"id": "1908.10481", "submitter": "Md Rafiqul Islam Rabin", "authors": "Md Rafiqul Islam Rabin, Mohammad Amin Alipour", "title": "K-CONFIG: Using Failing Test Cases to Generate Test Cases in GCC\n  Compilers", "comments": "ASE 2019 Late Breaking Results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The correctness of compilers is instrumental in the safety and reliability of\nother software systems, as bugs in compilers can produce programs that do not\nreflect the intents of programmers. Compilers are complex software systems due\nto the complexity of optimization. GCC is an optimizing C compiler that has\nbeen used in building operating systems and many other system software. In this\npaper, we describe K-CONFIG, an approach that uses the bugs reported in the GCC\nrepository to generate new test inputs. Our main insight is that the features\nappearing in the bug reports are likely to reappear in the future bugs, as the\nbugfixes can be incomplete or those features may be inherently challenging to\nimplement hence more prone to errors. Our approach first clusters the failing\ntest input extracted from the bug reports into clusters of similar test inputs.\nIt then uses these clusters to create configurations for Csmith, the most\npopular test generator for C compilers. In our experiments on two versions of\nGCC, our approach could trigger up to 36 miscompilation failures, and 179\ncrashes, while Csmith with the default configuration did not trigger any\nfailures. This work signifies the benefits of analyzing and using the reported\nbugs in the generation of new test inputs.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 22:09:00 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Rabin", "Md Rafiqul Islam", ""], ["Alipour", "Mohammad Amin", ""]]}, {"id": "1908.10607", "submitter": "Michael Hanus", "authors": "Michael Hanus, Finn Teegen", "title": "Adding Data to Curry", "comments": "Part of DECLARE 19 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional logic languages can solve equations over user-defined data and\nfunctions. Thus, the definition of an appropriate meaning of equality has a\nlong history in these languages, ranging from reflexive equality in early\nequational logic languages to strict equality in contemporary functional logic\nlanguages like Curry. With the introduction of type classes, where the equality\noperation \"==\" is overloaded and user-defined, the meaning became more complex.\nMoreover, logic variables appearing in equations require a different typing\nthan pattern variables, since the latter might be instantiated with functional\nvalues or non-terminating operations. In this paper, we present a solution to\nthese problems by introducing a new type class \"Data\" which is associated with\nspecific algebraic data types, logic variables, and strict equality. We discuss\nthe ideas of this class and its implications on various concepts of Curry, like\nunification, functional patterns, and program optimization.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 09:34:50 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Hanus", "Michael", ""], ["Teegen", "Finn", ""]]}, {"id": "1908.10711", "submitter": "Md Rafiqul Islam Rabin", "authors": "Md Rafiqul Islam Rabin, Ke Wang, Mohammad Amin Alipour", "title": "Testing Neural Program Analyzers", "comments": "ASE 2019 Late Breaking Results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been increasingly used in software engineering and\nprogram analysis tasks. They usually take a program and make some predictions\nabout it, e.g., bug prediction. We call these models neural program analyzers.\nThe reliability of neural programs can impact the reliability of the\nencompassing analyses. In this paper, we describe our ongoing efforts to\ndevelop effective techniques for testing neural programs. We discuss the\nchallenges involved in developing such tools and our future plans. In our\npreliminary experiment on a neural model recently proposed in the literature,\nwe found that the model is very brittle, and simple perturbations in the input\ncan cause the model to make mistakes in its prediction.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 04:55:22 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 22:27:27 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Rabin", "Md Rafiqul Islam", ""], ["Wang", "Ke", ""], ["Alipour", "Mohammad Amin", ""]]}, {"id": "1908.10743", "submitter": "EPTCS", "authors": "Giorgio Audrito (University of Turin, Italy), Ferruccio Damiani\n  (University of Turin, Italy), Volker Stolz (Western Norway University of\n  Applied Sciences, Norway), Mirko Viroli (University of Bologna, Italy)", "title": "On Distributed Runtime Verification by Aggregate Computing", "comments": "In Proceedings VORTEX 2018, arXiv:1908.09302", "journal-ref": "EPTCS 302, 2019, pp. 47-61", "doi": "10.4204/EPTCS.302.4", "report-no": null, "categories": "cs.SE cs.DC cs.MA cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Runtime verification is a computing analysis paradigm based on observing a\nsystem at runtime (to check its expected behaviour) by means of monitors\ngenerated from formal specifications. Distributed runtime verification is\nruntime verification in connection with distributed systems: it comprises both\nmonitoring of distributed systems and using distributed systems for monitoring.\nAggregate computing is a programming paradigm based on a reference computing\nmachine that is the aggregate collection of devices that cooperatively carry\nout a computational process: the details of behaviour, position and number of\ndevices are largely abstracted away, to be replaced with a space-filling\ncomputational environment. In this position paper we argue, by means of simple\nexamples, that aggregate computing is particularly well suited for implementing\ndistributed monitors. Our aim is to foster further research on how to generate\naggregate computing monitors from suitable formal specifications.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 06:20:41 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Audrito", "Giorgio", "", "University of Turin, Italy"], ["Damiani", "Ferruccio", "", "University of Turin, Italy"], ["Stolz", "Volker", "", "Western Norway University of\n  Applied Sciences, Norway"], ["Viroli", "Mirko", "", "University of Bologna, Italy"]]}, {"id": "1908.10926", "submitter": "V\\'it \\v{S}efl", "authors": "V\\'it \\v{S}efl", "title": "Performance Analysis of Zippers", "comments": "Part of DECLARE 19 proceedings, 15 pages, 9 listings, 6 figures,\n  source files available at https://github.com/vituscze/performance-zippers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A zipper is a powerful technique of representing a purely functional data\nstructure in a way that allows fast access to a specific element. It is often\nused in cases where the imperative data structures would use a mutable pointer.\nHowever, the efficiency of zippers as a replacement for mutable pointers is not\nsufficiently explored. We attempt to address this issue by comparing the\nperformance of zippers and mutable pointers in two common scenarios and three\ndifferent languages: C++, C#, and Haskell.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 19:48:10 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["\u0160efl", "V\u00edt", ""]]}, {"id": "1908.11101", "submitter": "Michael Hanus", "authors": "Sergio Antoy, Michael Hanus, Andy Jost, Steven Libby", "title": "ICurry", "comments": "Part of DECLARE 19 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FlatCurry is a well-established intermediate representation of Curry programs\nused in compilers that translate Curry code into Prolog and Haskell code. Some\nFlatCurry constructs have no direct translation into imperative code. These\nconstructs must be each handled differently when translating Curry code into C,\nC++ and Python code. We introduce a new representation of Curry programs,\ncalled ICurry, and derive a translation from all FlatCurry constructs into\nICurry. We present the syntax of ICurry and the translation from FlatCurry to\nICurry. We present a model of functional logic computations as graph rewriting,\nshow how this model can be implemented in a low-level imperative language, and\ndescribe the translation from ICurry to this model.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 08:47:52 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Antoy", "Sergio", ""], ["Hanus", "Michael", ""], ["Jost", "Andy", ""], ["Libby", "Steven", ""]]}, {"id": "1908.11105", "submitter": "Juan Carlos Saenz-Carrasco", "authors": "Juan Carlos Saenz-Carrasco", "title": "FunSeqSet: Towards a Purely Functional Data Structure for the\n  Linearisation Case of Dynamic Trees Problem", "comments": "Part of DECLARE 19 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic trees, originally described by Sleator and Tarjan, have been studied\ndeeply for non persistent structures providing $\\mathcal{O}(\\log n)$ time for\nupdate and lookup operations as shown in theory and practice by Werneck.\nHowever, discussions on how the most common dynamic trees operations (i.e. link\nand cut) are computed over a purely functional data structure have not been\nstudied. Even more, asking whether vertices $u$ and $v$ are connected (i.e.\nwithin the same forest) assumes that corresponding indices or locations for $u$\nand $v$ are taken for granted in most of the literature, and not performed as\npart of the whole computation for such a question. We present FunSeqSet, based\non the primitive version of finger trees, i.e. the de facto sequence data\nstructure for the purely functional programming language Haskell, augmented\nwith variants of the collection (i.e. sets) data structures in order to manage\nefficiently $k$-ary trees for the linearisation case of the dynamic trees\nproblem. Different implementations are discussed, and the performance is\nmeasured.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 08:49:29 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Saenz-Carrasco", "Juan Carlos", ""]]}, {"id": "1908.11142", "submitter": "Baltasar Tranc\\'on Y Widemann", "authors": "Baltasar Tranc\\'on y Widemann and Markus Lepper", "title": "Improving the Performance of the Paisley Pattern-Matching EDSL by Staged\n  Combinatorial Compilation", "comments": "Part of DECLARE 19 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paisley is a declarative lightweight embedded domain-specific language for\nexpressive, non-deterministic, non-invasive pattern matching on arbitrary data\nstructures in Java applications. As such, it comes as a pure Java library of\npattern-matching combinators and corresponding programming idioms. While the\ncombinators support a basic form of self-optimization based on heuristic\nmetadata, overall performance is limited by the distributed and compositional\nimplementation that impedes non-local code optimization. In this paper, we\ndescribe a technique for improving the performance of Paisley transparently,\nwithout compromising the flexible and extensible combinatorial design. By means\nof distributed bytecode generation, dynamic class loading and just-in-time\ncompilation of patterns, the run-time overhead of the combinatorial approach\ncan be reduced significantly, without requiring any technology other than a\nstandard Java virtual machine and our LLJava bytecode framework. We evaluate\nthe impact by comparison to earlier benchmarking results on interpreted\nPaisley. The key ideas of our compilation technique are fairly general, and\napply in principle to any kind of combinator language running on any\njit-compiling host.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 10:25:31 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Widemann", "Baltasar Tranc\u00f3n y", ""], ["Lepper", "Markus", ""]]}, {"id": "1908.11169", "submitter": "EPTCS", "authors": "Tom Hirschowitz (Univ. Grenoble Alpes, Univ. Savoie Mont Blanc, CNRS,\n  LAMA, 73000 Chamb\\'ery, France)", "title": "Cellular Monads from Positive GSOS Specifications", "comments": "In Proceedings EXPRESS/SOS 2019, arXiv:1908.08213", "journal-ref": "EPTCS 300, 2019, pp. 1-18", "doi": "10.4204/EPTCS.300.1", "report-no": null, "categories": "cs.LO cs.PL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a leisurely introduction to our abstract framework for operational\nsemantics based on cellular monads on transition categories. Furthermore, we\nrelate it for the first time to an existing format, by showing that all\nPositive GSOS specifications generate cellular monads whose free algebras are\nall compositional. As a consequence, we recover the known result that\nbisimilarity is a congruence in the generated labelled transition system.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 12:11:23 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Hirschowitz", "Tom", "", "Univ. Grenoble Alpes, Univ. Savoie Mont Blanc, CNRS,\n  LAMA, 73000 Chamb\u00e9ry, France"]]}, {"id": "1908.11227", "submitter": "Sunbeom So", "authors": "Sunbeom So, Myungho Lee, Jisu Park, Heejo Lee, Hakjoo Oh", "title": "VeriSmart: A Highly Precise Safety Verifier for Ethereum Smart Contracts", "comments": "To appear in the IEEE Symposium on Security & Privacy, May 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present VeriSmart, a highly precise verifier for ensuring arithmetic\nsafety of Ethereum smart contracts. Writing safe smart contracts without\nunintended behavior is critically important because smart contracts are\nimmutable and even a single flaw can cause huge financial damage. In\nparticular, ensuring that arithmetic operations are safe is one of the most\nimportant and common security concerns of Ethereum smart contracts nowadays. In\nresponse, several safety analyzers have been proposed over the past few years,\nbut state-of-the-art is still unsatisfactory; no existing tools achieve high\nprecision and recall at the same time, inherently limited to producing annoying\nfalse alarms or missing critical bugs. By contrast, VeriSmart aims for an\nuncompromising analyzer that performs exhaustive verification without\ncompromising precision or scalability, thereby greatly reducing the burden of\nmanually checking undiscovered or incorrectly-reported issues. To achieve this\ngoal, we present a new domain-specific algorithm for verifying smart contracts,\nwhich is able to automatically discover and leverage transaction invariants\nthat are essential for precisely analyzing smart contracts. Evaluation with\nreal-world smart contracts shows that VeriSmart can detect all arithmetic bugs\nwith a negligible number of false alarms, far outperforming existing analyzers.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 13:51:34 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 10:46:35 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["So", "Sunbeom", ""], ["Lee", "Myungho", ""], ["Park", "Jisu", ""], ["Lee", "Heejo", ""], ["Oh", "Hakjoo", ""]]}, {"id": "1908.11343", "submitter": "Michael Schaper", "authors": "Martin Avanzini, Michael Schaper and Georg Moser", "title": "Modular Runtime Complexity Analysis of Probabilistic While Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with the average case runtime complexity analysis of a\nprototypical imperative language endowed with primitives for sampling and\nprobabilistic choice. Taking inspiration from known approaches from to the\nmodular resource analysis of non-probabilistic programs, we investigate how a\nmodular runtime analysis is obtained for probabilistic programs.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 14:38:36 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Avanzini", "Martin", ""], ["Schaper", "Michael", ""], ["Moser", "Georg", ""]]}, {"id": "1908.11685", "submitter": "Brendon Boldt", "authors": "Brendon Boldt", "title": "Using LSTMs to Model the Java Programming Language", "comments": "9 pages, 2 figures", "journal-ref": "Artificial Neural Networks and Machine Learning -- ICANN 2017.\n  ICANN 2017. Lecture Notes in Computer Science, vol 10614", "doi": "10.1007/978-3-319-68612-7_31", "report-no": null, "categories": "cs.SE cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs), specifically long-short term memory\nnetworks (LSTMs), can model natural language effectively. This research\ninvestigates the ability for these same LSTMs to perform next \"word\" prediction\non the Java programming language. Java source code from four different\nrepositories undergoes a transformation that preserves the logical structure of\nthe source code and removes the code's various specificities such as variable\nnames and literal values. Such datasets and an additional English language\ncorpus are used to train and test standard LSTMs' ability to predict the next\nelement in a sequence. Results suggest that LSTMs can effectively model Java\ncode achieving perplexities under 22 and accuracies above 0.47, which is an\nimprovement over LSTM's performance on the English language which demonstrated\na perplexity of 85 and an accuracy of 0.27. This research can have\napplicability in other areas such as syntactic template suggestion and\nautomated bug patching.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 00:43:32 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Boldt", "Brendon", ""]]}, {"id": "1908.11781", "submitter": "Yuke Wang", "authors": "Yuke Wang, Boyuan Feng, Gushu Li, Lei Deng, Yuan Xie, Yufei Ding", "title": "AccD: A Compiler-based Framework for Accelerating Distance-related\n  Algorithms on CPU-FPGA Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a promising solution to boost the performance of distance-related\nalgorithms (e.g., K-means and KNN), FPGA-based acceleration attracts lots of\nattention, but also comes with numerous challenges. In this work, we propose\nAccD, a compiler-based framework for accelerating distance-related algorithms\non CPU-FPGA platforms. Specifically, AccD provides a Domain-specific Language\nto unify distance-related algorithms effectively, and an optimizing compiler to\nreconcile the benefits from both the algorithmic optimization on the CPU and\nthe hardware acceleration on the FPGA. The output of AccD is a high-performance\nand power-efficient design that can be easily synthesized and deployed on\nmainstream CPU-FPGA platforms. Intensive experiments show that AccD designs\nachieve 31.42x speedup and 99.63x better energy efficiency on average over\nstandard CPU-based implementations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2019 19:15:26 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Wang", "Yuke", ""], ["Feng", "Boyuan", ""], ["Li", "Gushu", ""], ["Deng", "Lei", ""], ["Xie", "Yuan", ""], ["Ding", "Yufei", ""]]}, {"id": "1908.11850", "submitter": "Swapnil Haria", "authors": "Swapnil Haria, Mark D. Hill, Michael M. Swift", "title": "MOD: Minimally Ordered Durable Datastructures for Persistent Memory", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent Memory (PM) makes possible recoverable applications that can\npreserve application progress across system reboots and power failures. Actual\nrecoverability requires careful ordering of cacheline flushes, currently done\nin two extreme ways. On one hand, expert programmers have reasoned deeply about\nconsistency and durability to create applications centered on a single\ncustom-crafted durable datastructure. On the other hand, less-expert\nprogrammers have used software transaction memory (STM) to make atomic one or\nmore updates, albeit at a significant performance cost due largely to ordered\nlog updates.\n  In this work, we propose the middle ground of composable persistent\ndatastructures called Minimally Ordered Durable (MOD) datastructures. MOD is a\nC++ library of several datastructures---currently, map, set, stack, queue and\nvector--- that often perform better than STM and yet are relatively easy to\nuse. They allow multiple updates to one or more datastructures to be atomic\nwith respect to failure. Moreover, we provide a recipe to create more\nrecoverable datastructures.\n  MOD is motivated by our analysis of real Intel Optane PM hardware showing\nthat allowing unordered, overlapping flushes significantly improves\nperformance. MOD reduces ordering by adapting existing techniques for\nout-of-place updates (like shadow paging) with space-reducing structural\nsharing (from functional programming). MOD exposes a Basic interface for single\nupdates and a Composition interface for atomically performing multiple updates.\nRelative to the state-of-the-art Intel PMDK v1.5 STM, MOD improves map, set,\nstack, queue microbenchmark performance by 40%, and speeds up application\nbenchmark performance by 38%.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 15:11:31 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Haria", "Swapnil", ""], ["Hill", "Mark D.", ""], ["Swift", "Michael M.", ""]]}]