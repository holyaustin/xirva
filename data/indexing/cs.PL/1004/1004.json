[{"id": "1004.0202", "submitter": "Alexandre Chapoutot", "authors": "Alexandre Chapoutot (LIP6)", "title": "Interval Slopes as Numerical Abstract Domain for Floating-Point\n  Variables", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-15769-1_12", "report-no": null, "categories": "cs.PL cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of embedded control systems is mainly done with model-based tools\nsuch as Matlab/Simulink. Numerical simulation is the central technique of\ndevelopment and verification of such tools. Floating-point arithmetic, that is\nwell-known to only provide approximated results, is omnipresent in this\nactivity. In order to validate the behaviors of numerical simulations using\nabstract interpretation-based static analysis, we present, theoretically and\nwith experiments, a new partially relational abstract domain dedicated to\nfloating-point variables. It comes from interval expansion of non-linear\nfunctions using slopes and it is able to mimic all the behaviors of the\nfloating-point arithmetic. Hence it is adapted to prove the absence of run-time\nerrors or to analyze the numerical precision of embedded control systems.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2010 18:43:10 GMT"}, {"version": "v2", "created": "Mon, 31 May 2010 09:12:36 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2010 09:17:40 GMT"}, {"version": "v4", "created": "Fri, 18 Jun 2010 07:50:43 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Chapoutot", "Alexandre", "", "LIP6"]]}, {"id": "1004.0944", "submitter": "Roberto Bagnara", "authors": "Roberto Bagnara, Fred Mesnard, Andrea Pescetti, Enea Zaffanella", "title": "The Automatic Synthesis of Linear Ranking Functions: The Complete\n  Unabridged Version", "comments": "47 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical technique for proving termination of a generic sequential\ncomputer program involves the synthesis of a ranking function for each loop of\nthe program. Linear ranking functions are particularly interesting because many\nterminating loops admit one and algorithms exist to automatically synthesize\nit. In this paper we present two such algorithms: one based on work dated 1991\nby Sohn and Van Gelder; the other, due to Podelski and Rybalchenko, dated 2004.\nRemarkably, while the two algorithms will synthesize a linear ranking function\nunder exactly the same set of conditions, the former is mostly unknown to the\ncommunity of termination analysis and its general applicability has never been\nput forward before the present paper. In this paper we thoroughly justify both\nalgorithms, we prove their correctness, we compare their worst-case complexity\nand experimentally evaluate their efficiency, and we present an open-source\nimplementation of them that will make it very easy to include\ntermination-analysis capabilities in automatic program verifiers.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2010 19:54:20 GMT"}, {"version": "v2", "created": "Sun, 1 Apr 2012 07:14:21 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Bagnara", "Roberto", ""], ["Mesnard", "Fred", ""], ["Pescetti", "Andrea", ""], ["Zaffanella", "Enea", ""]]}, {"id": "1004.1211", "submitter": "Avik Chaudhuri", "authors": "Avik Chaudhuri", "title": "Liberalizing Dependency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dependency core calculus (DCC), a simple extension of the computational\nlambda calculus, captures a common notion of dependency that arises in many\nprogramming language settings. This notion of dependency is closely related to\nthe notion of information flow in security; it is sensitive not only to data\ndependencies that cause explicit flows, but also to control dependencies that\ncause implicit flows. In this paper, we study variants of DCC in which the data\nand control dependencies are decoupled. This allows us to consider settings\nwhere a weaker notion of dependency---one that restricts only explicit\nflows---may usefully coexist with DCC's stronger notion of dependency. In\nparticular, we show how strong, noninterference-based security may be\nreconciled with weak, trace-based security within the same system, enhancing\nsoundness of the latter and completeness of the former.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2010 23:53:50 GMT"}], "update_date": "2010-04-09", "authors_parsed": [["Chaudhuri", "Avik", ""]]}, {"id": "1004.2697", "submitter": "Vishwanath Raman", "authors": "Krishnendu Chatterjee and Vishwanath Raman", "title": "Assume-Guarantee Synthesis for Digital Contract Signing", "comments": "40 pages, 1 figure, 3 tables and 3 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.CR cs.GT cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the automatic synthesis of fair non-repudiation protocols, a class\nof fair exchange protocols, used for digital contract signing. First, we show\nhow to specify the objectives of the participating agents and the trusted third\nparty (TTP) as path formulas in LTL and prove that the satisfaction of these\nobjectives imply fairness; a property required of fair exchange protocols. We\nthen show that weak (co-operative) co-synthesis and classical (strictly\ncompetitive) co-synthesis fail, whereas assume-guarantee synthesis (AGS)\nsucceeds. We demonstrate the success of assume-guarantee synthesis as follows:\n(a) any solution of assume-guarantee synthesis is attack-free; no subset of\nparticipants can violate the objectives of the other participants; (b) the\nAsokan-Shoup-Waidner (ASW) certified mail protocol that has known\nvulnerabilities is not a solution of AGS; (c) the Kremer-Markowitch (KM)\nnon-repudiation protocol is a solution of AGS; and (d) AGS presents a new and\nsymmetric fair non-repudiation protocol that is attack-free. To our knowledge\nthis is the first application of synthesis to fair non-repudiation protocols,\nand our results show how synthesis can both automatically discover\nvulnerabilities in protocols and generate correct protocols. The solution to\nassume-guarantee synthesis can be computed efficiently as the secure\nequilibrium solution of three-player graph games.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2010 19:41:24 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2010 07:35:32 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2010 04:58:22 GMT"}, {"version": "v4", "created": "Sun, 13 Nov 2011 00:16:54 GMT"}], "update_date": "2011-11-15", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Raman", "Vishwanath", ""]]}, {"id": "1004.2884", "submitter": "Andrey Rybalchenko", "authors": "Ranjit Jhala and Rupak Majumdar and Andrey Rybalchenko", "title": "HMC: Verifying Functional Programs Using Abstract Interpreters", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Hindley-Milner-Cousots (HMC), an algorithm that allows any\ninterprocedural analysis for first-order imperative programs to be used to\nverify safety properties of typed higher-order functional programs. HMC works\nas follows. First, it uses the type structure of the functional program to\ngenerate a set of logical refinement constraints whose satisfaction implies the\nsafety of the source program. Next, it transforms the logical refinement\nconstraints into a simple first-order imperative program that is safe iff the\nconstraints are satisfiable. Thus, in one swoop, HMC makes tools for invariant\ngeneration, e.g., based on abstract domains, predicate abstraction,\ncounterexample-guided refinement, and Craig interpolation be directly\napplicable to verify safety properties of modern functional languages in a\nfully automatic manner. We have implemented HMC and describe preliminary\nexperimental results using two imperative checkers -- ARMC and InterProc -- to\nverify OCaml programs. Thus, by composing type-based reasoning grounded in\nprogram syntax and state-based reasoning grounded in abstract interpretation,\nHMC opens the door to automatic verification of programs written in modern\nprogramming languages.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2010 17:01:27 GMT"}, {"version": "v2", "created": "Thu, 30 Dec 2010 14:41:52 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Jhala", "Ranjit", ""], ["Majumdar", "Rupak", ""], ["Rybalchenko", "Andrey", ""]]}, {"id": "1004.3241", "submitter": "James Cheney", "authors": "James Cheney", "title": "Causality and the semantics of provenance", "comments": "Workshop submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Provenance, or information about the sources, derivation, custody or history\nof data, has been studied recently in a number of contexts, including\ndatabases, scientific workflows and the Semantic Web. Many provenance\nmechanisms have been developed, motivated by informal notions such as\ninfluence, dependence, explanation and causality. However, there has been\nlittle study of whether these mechanisms formally satisfy appropriate policies\nor even how to formalize relevant motivating concepts such as causality. We\ncontend that mathematical models of these concepts are needed to justify and\ncompare provenance techniques. In this paper we review a theory of causality\nbased on structural models that has been developed in artificial intelligence,\nand describe work in progress on a causal semantics for provenance graphs.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2010 16:14:13 GMT"}], "update_date": "2010-04-20", "authors_parsed": [["Cheney", "James", ""]]}, {"id": "1004.3808", "submitter": "Chris Hawblitzel", "authors": "Chris Hawblitzel (Microsoft), Erez Petrank (Technion)", "title": "Automated Verification of Practical Garbage Collectors", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 6, Issue 3 (August 18,\n  2010) lmcs:1039", "doi": "10.2168/LMCS-6(3:6)2010", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Garbage collectors are notoriously hard to verify, due to their low-level\ninteraction with the underlying system and the general difficulty in reasoning\nabout reachability in graphs. Several papers have presented verified\ncollectors, but either the proofs were hand-written or the collectors were too\nsimplistic to use on practical applications. In this work, we present two\nmechanically verified garbage collectors, both practical enough to use for\nreal-world C# benchmarks. The collectors and their associated allocators\nconsist of x86 assembly language instructions and macro instructions, annotated\nwith preconditions, postconditions, invariants, and assertions. We used the\nBoogie verification generator and the Z3 automated theorem prover to verify\nthis assembly language code mechanically. We provide measurements comparing the\nperformance of the verified collector with that of the standard Bartok\ncollectors on off-the-shelf C# benchmarks, demonstrating their competitiveness.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2010 21:14:34 GMT"}, {"version": "v2", "created": "Wed, 18 Aug 2010 08:11:21 GMT"}, {"version": "v3", "created": "Mon, 18 Oct 2010 11:57:59 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Hawblitzel", "Chris", "", "Microsoft"], ["Petrank", "Erez", "", "Technion"]]}, {"id": "1004.4109", "submitter": "Pavel Ruzankin", "authors": "Pavel Ruzankin", "title": "Operator-oriented programming: a new paradigm for implementing window\n  interfaces and parallel algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new programming paradigm which can be useful, in particular, for\nimplementing window interfaces and parallel algorithms. This paradigm allows a\nuser to define operators which can contain nested operators. The new paradigm\nis called operator-oriented. One of the goals of this paradigm is to escape the\ncomplexity of objects definitions inherent in many object-oriented languages\nand to move to transparent algorithms definitions.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2010 12:02:51 GMT"}], "update_date": "2010-04-26", "authors_parsed": [["Ruzankin", "Pavel", ""]]}, {"id": "1004.4656", "submitter": "Krzysztof R. Apt", "authors": "Krzysztof R. Apt, Frank S. de Boer, Ernst-Ruediger Olderog and Stijn\n  de Gouw", "title": "Verification of Object-Oriented Programs: a Transformational Approach", "comments": "49 pages. To appear in Journal of Computer and System Sciences. Stijn\n  de Gouw is now a new author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that verification of object-oriented programs by means of the\nassertional method can be achieved in a simple way by exploiting a\nsyntax-directed transformation from object-oriented programs to recursive\nprograms. This transformation suggests natural proofs rules and its correctness\nhelps us to establish soundness and relative completeness of the proposed proof\nsystem. One of the difficulties is how to properly deal in the assertion\nlanguage with the instance variables and aliasing. The discussed programming\nlanguage supports arrays, instance variables, failures and recursive methods\nwith parameters.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2010 21:02:10 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2011 06:57:28 GMT"}, {"version": "v3", "created": "Tue, 8 Nov 2011 07:41:30 GMT"}], "update_date": "2011-11-09", "authors_parsed": [["Apt", "Krzysztof R.", ""], ["de Boer", "Frank S.", ""], ["Olderog", "Ernst-Ruediger", ""], ["de Gouw", "Stijn", ""]]}, {"id": "1004.4796", "submitter": "Henning Thielemann", "authors": "Henning Thielemann", "title": "Compiling Signal Processing Code embedded in Haskell via LLVM", "comments": "8 pages, 1 figure, 3 listings, 1 table, accepted by Linux Audio\n  Conference LAC2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a programming language for real-time audio signal processing that\nis embedded in the functional language Haskell and uses the Low-Level Virtual\nMachine as back-end. With that framework we can code with the comfort and type\nsafety of Haskell while achieving maximum efficiency of fast inner loops and\nfull vectorisation. This way Haskell becomes a valuable alternative to special\npurpose signal processing languages.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2010 13:31:41 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2011 09:14:23 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Thielemann", "Henning", ""]]}, {"id": "1004.5534", "submitter": "David A. Wheeler", "authors": "David A. Wheeler", "title": "Fully Countering Trusting Trust through Diverse Double-Compiling", "comments": "PhD dissertation. Accepted by George Mason University, Fairfax,\n  Virginia, USA's Volgenau School of Information Technology and Engineering in\n  2009. 199 single-side printed pages.", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An Air Force evaluation of Multics, and Ken Thompson's Turing award lecture\n(\"Reflections on Trusting Trust\"), showed that compilers can be subverted to\ninsert malicious Trojan horses into critical software, including themselves. If\nthis \"trusting trust\" attack goes undetected, even complete analysis of a\nsystem's source code will not find the malicious code that is running.\nPreviously-known countermeasures have been grossly inadequate. If this attack\ncannot be countered, attackers can quietly subvert entire classes of computer\nsystems, gaining complete control over financial, infrastructure, military,\nand/or business systems worldwide. This dissertation's thesis is that the\ntrusting trust attack can be detected and effectively countered using the\n\"Diverse Double-Compiling\" (DDC) technique, as demonstrated by (1) a formal\nproof that DDC can determine if source code and generated executable code\ncorrespond, (2) a demonstration of DDC with four compilers (a small C compiler,\na small Lisp compiler, a small maliciously corrupted Lisp compiler, and a large\nindustrial-strength C compiler, GCC), and (3) a description of approaches for\napplying DDC in various real-world scenarios. In the DDC technique, source code\nis compiled twice: the source code of the compiler's parent is compiled using a\ntrusted compiler, and then the putative compiler source code is compiled using\nthe result of the first compilation. If the DDC result is bit-for-bit identical\nwith the original compiler-under-test's executable, and certain other\nassumptions hold, then the compiler-under-test's executable corresponds with\nits putative source code.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2010 14:01:18 GMT"}], "update_date": "2010-05-03", "authors_parsed": [["Wheeler", "David A.", ""]]}, {"id": "1004.5548", "submitter": "David A. Wheeler", "authors": "David A. Wheeler", "title": "Countering Trusting Trust through Diverse Double-Compiling", "comments": "13 pages.", "journal-ref": "Proc. of the 21st Annual Computer Security Applications Conference\n  (ACSAC), December 5-9, 2005, Tucson, Arizona, pp. 28-40, Los Alamitos: IEEE\n  Computer Society, ISBN 0-7695-2461-3, ISSN 1063-9527, IEEE Computer Society\n  Order Number P2461.", "doi": "10.1109/CSAC.2005.17", "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An Air Force evaluation of Multics, and Ken Thompson's famous Turing award\nlecture \"Reflections on Trusting Trust,\" showed that compilers can be subverted\nto insert malicious Trojan horses into critical software, including themselves.\nIf this attack goes undetected, even complete analysis of a system's source\ncode will not find the malicious code that is running, and methods for\ndetecting this particular attack are not widely known. This paper describes a\npractical technique, termed diverse double-compiling (DDC), that detects this\nattack and some compiler defects as well. Simply recompile the source code\ntwice: once with a second (trusted) compiler, and again using the result of the\nfirst compilation. If the result is bit-for-bit identical with the untrusted\nbinary, then the source code accurately represents the binary. This technique\nhas been mentioned informally, but its issues and ramifications have not been\nidentified or discussed in a peer-reviewed work, nor has a public demonstration\nbeen made. This paper describes the technique, justifies it, describes how to\novercome practical challenges, and demonstrates it.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2010 15:01:21 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Wheeler", "David A.", ""]]}]