[{"id": "2001.00059", "submitter": "Petros Maniatis", "authors": "Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, Kensen Shi", "title": "Learning and Evaluating Contextual Embedding of Source Code", "comments": "Published in ICML 2020. This version (v.3) is the final camera-ready\n  version of the paper. It contains the re-computed results, based on the\n  open-sourced datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has achieved impressive results on understanding and\nimproving source code by building up on machine-learning techniques developed\nfor natural languages. A significant advancement in natural-language\nunderstanding has come with the development of pre-trained contextual\nembeddings, such as BERT, which can be fine-tuned for downstream tasks with\nless labeled data and training budget, while achieving better accuracies.\nHowever, there is no attempt yet to obtain a high-quality contextual embedding\nof source code, and to evaluate it on multiple program-understanding tasks\nsimultaneously; that is the gap that this paper aims to mitigate. Specifically,\nfirst, we curate a massive, deduplicated corpus of 7.4M Python files from\nGitHub, which we use to pre-train CuBERT, an open-sourced code-understanding\nBERT model; and, second, we create an open-sourced benchmark that comprises\nfive classification tasks and one program-repair task, akin to\ncode-understanding tasks proposed in the literature before. We fine-tune CuBERT\non our benchmark tasks, and compare the resulting models to different variants\nof Word2Vec token embeddings, BiLSTM and Transformer models, as well as\npublished state-of-the-art models, showing that CuBERT outperforms them all,\neven with shorter training, and with fewer labeled examples. Future work on\nsource-code embedding can benefit from reusing our benchmark, and from\ncomparing against CuBERT models as a strong baseline.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 05:05:22 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 22:06:21 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2020 21:40:59 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Kanade", "Aditya", ""], ["Maniatis", "Petros", ""], ["Balakrishnan", "Gogul", ""], ["Shi", "Kensen", ""]]}, {"id": "2001.00532", "submitter": "Ryan Senanayake", "authors": "Ryan Senanayake, Fredrik Kjolstad, Changwan Hong, Shoaib Kamil, and\n  Saman Amarasinghe", "title": "A Unified Iteration Space Transformation Framework for Sparse and Dense\n  Tensor Algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of optimizing mixed sparse and dense tensor algebra in\na compiler. We show that standard loop transformations, such as strip-mining,\ntiling, collapsing, parallelization and vectorization, can be applied to\nirregular loops over sparse iteration spaces. We also show how these\ntransformations can be applied to the contiguous value arrays of sparse tensor\ndata structures, which we call their position space, to unlock load-balanced\ntiling and parallelism.\n  We have prototyped these concepts in the open-source TACO system, where they\nare exposed as a scheduling API similar to the Halide domain-specific language\nfor dense computations. Using this scheduling API, we show how to optimize\nmixed sparse/dense tensor algebra expressions, how to generate load-balanced\ncode by scheduling sparse tensor algebra in position space, and how to generate\nsparse tensor algebra GPU code. Our evaluation shows that our transformations\nlet us generate good code that is competitive with many hand-optimized\nimplementations from the literature.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 23:17:48 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Senanayake", "Ryan", ""], ["Kjolstad", "Fredrik", ""], ["Hong", "Changwan", ""], ["Kamil", "Shoaib", ""], ["Amarasinghe", "Saman", ""]]}, {"id": "2001.00806", "submitter": "Mihhail Aizatulin", "authors": "Mihhail Aizatulin", "title": "Verifying Cryptographic Security Implementations in C Using Automated\n  Model Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis presents an automated method for verifying security properties of\nprotocol implementations written in the C language. We assume that each\nsuccessful run of a protocol follows the same path through the C code,\njustified by the fact that typical security protocols have linear structure. We\nthen perform symbolic execution of that path to extract a model expressed in a\nprocess calculus similar to the one used by the CryptoVerif tool. The symbolic\nexecution uses a novel algorithm that allows symbolic variables to represent\nbitstrings of potentially unknown length to model incoming protocol messages.\n  The extracted models do not use pointer-addressed memory, but they may still\ncontain low-level details concerning message formats. In the next step we\nreplace the message formatting expressions by abstract tupling and projection\noperators. The properties of these operators, such as the projection operation\nbeing the inverse of the tupling operation, are typically only satisfied with\nrespect to inputs of correct types. Therefore we typecheck the model to ensure\nthat all type-safety constraints are satisfied. The resulting model can then be\nverified with CryptoVerif to obtain a computational security result directly,\nor with ProVerif, to obtain a computational security result by invoking a\ncomputational soundness theorem.\n  Our method achieves high automation and does not require user input beyond\nwhat is necessary to specify the properties of the cryptographic primitives and\nthe desired security goals. We evaluated the method on several protocol\nimplementations, totalling over 3000 lines of code. The biggest case study was\na 1000-line implementation that was independently written without verification\nin mind. We found several flaws that were acknowledged and fixed by the\nauthors, and were able to verify the fixed code without any further\nmodifications to it.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 12:52:11 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Aizatulin", "Mihhail", ""]]}, {"id": "2001.01337", "submitter": "Francesco Gavazzo", "authors": "Ugo Dal Lago and Francesco Gavazzo", "title": "A Diagrammatic Calculus for Algebraic Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce a new diagrammatic notation for representing the result of\n(algebraic) effectful computations. Our notation explicitly separates the\neffects produced during a computation from the possible values returned, this\nway simplifying the extension of definitions and results on pure computations\nto an effectful setting. Additionally, we show a number of algebraic and\norder-theoretic laws on diagrams, this way laying the foundations for a\ndiagrammatic calculus of algebraic effects. We give a formal foundation for\nsuch a calculus in terms of Lawvere theories and generic effects.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 23:48:41 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 08:45:23 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Lago", "Ugo Dal", ""], ["Gavazzo", "Francesco", ""]]}, {"id": "2001.01516", "submitter": "Florian Frohn", "authors": "Florian Frohn", "title": "A Calculus for Modular Loop Acceleration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loop acceleration can be used to prove safety, reachability, runtime bounds,\nand (non-)termination of programs operating on integers. To this end, a variety\nof acceleration techniques has been proposed. However, all of them are\nmonolithic: Either they accelerate a loop successfully or they fail completely.\nIn contrast, we present a calculus that allows for combining acceleration\ntechniques in a modular way and we show how to integrate many existing\nacceleration techniques into our calculus. Moreover, we propose two novel\nacceleration techniques that can be incorporated into our calculus seamlessly.\nAn empirical evaluation demonstrates the applicability of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 12:22:14 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 10:30:09 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 09:15:14 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Frohn", "Florian", ""]]}, {"id": "2001.02031", "submitter": "Zirun Zhu", "authors": "Zirun Zhu, Zhixuan Yang, Hsiang-Shang Ko, Zhenjiang Hu", "title": "Retentive Lenses", "comments": "34 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on Foster et al.'s lenses, various bidirectional programming languages\nand systems have been developed for helping the user to write correct data\nsynchronisers. The two well-behavedness laws of lenses, namely Correctness and\nHippocraticness, are usually adopted as the guarantee of these systems. While\nlenses are designed to retain information in the source when the view is\nmodified, well-behavedness says very little about the retaining of information:\nHippocraticness only requires that the source be unchanged if the view is not\nmodified, and nothing about information retention is guaranteed when the view\nis changed. To address the problem, we propose an extension of the original\nlenses, called retentive lenses, which satisfy a new Retentiveness law\nguaranteeing that if parts of the view are unchanged, then the corresponding\nparts of the source are retained as well. As a concrete example of retentive\nlenses, we present a domain-specific language for writing tree transformations;\nwe prove that the pair of get and put functions generated from a program in our\nDSL forms a retentive lens. We demonstrate the practical use of retentive\nlenses and the DSL by presenting case studies on code refactoring, Pombrio and\nKrishnamurthi's resugaring, and XML synchronisation.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 13:52:29 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 06:54:39 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Zhu", "Zirun", ""], ["Yang", "Zhixuan", ""], ["Ko", "Hsiang-Shang", ""], ["Hu", "Zhenjiang", ""]]}, {"id": "2001.02209", "submitter": "Matthijs V\\'ak\\'ar", "authors": "Mathieu Huot, Sam Staton, Matthijs V\\'ak\\'ar", "title": "Correctness of Automatic Differentiation via Diffeologies and\n  Categorical Gluing", "comments": "Proceedings of FoSSaCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present semantic correctness proofs of Automatic Differentiation (AD). We\nconsider a forward-mode AD method on a higher order language with algebraic\ndata types, and we characterise it as the unique structure preserving macro\ngiven a choice of derivatives for basic operations. We describe a rich\nsemantics for differentiable programming, based on diffeological spaces. We\nshow that it interprets our language, and we phrase what it means for the AD\nmethod to be correct with respect to this semantics. We show that our\ncharacterisation of AD gives rise to an elegant semantic proof of its\ncorrectness based on a gluing construction on diffeological spaces. We explain\nhow this is, in essence, a logical relations argument. Finally, we sketch how\nthe analysis extends to other AD methods by considering a continuation-based\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 18:21:52 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 07:56:36 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2020 08:05:26 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Huot", "Mathieu", ""], ["Staton", "Sam", ""], ["V\u00e1k\u00e1r", "Matthijs", ""]]}, {"id": "2001.02545", "submitter": "George Fourtounis", "authors": "George Fourtounis and Yannis Smaragdakis", "title": "Deep Static Modeling of invokedynamic", "comments": null, "journal-ref": "ECOOP 2019", "doi": "10.4230/LIPIcs.ECOOP.2019.15", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Java 7 introduced programmable dynamic linking in the form of the\ninvokedynamic framework. Static analysis of code containing programmable\ndynamic linking has often been cited as a significant source of unsoundness in\nthe analysis of Java programs. For example, Java lambdas, introduced in Java 8,\nare a very popular feature, which is, however, resistant to static analysis,\nsince it mixes invokedynamic with dynamic code generation. These techniques\ninvalidate static analysis assumptions: programmable linking breaks reasoning\nabout method resolution while dynamically generated code is, by definition, not\navailable statically. In this paper, we show that a static analysis can\npredictively model uses of invokedynamic while also cooperating with extra\nrules to handle the runtime code generation of lambdas. Our approach plugs into\nan existing static analysis and helps eliminate all unsoundness in the handling\nof lambdas (including associated features such as method references) and\ngeneric invokedynamic uses. We evaluate our technique on a benchmark suite of\nour own and on third-party benchmarks, uncovering all code previously\nunreachable due to unsoundness, highly efficiently.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 14:27:36 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Fourtounis", "George", ""], ["Smaragdakis", "Yannis", ""]]}, {"id": "2001.02609", "submitter": "Stephen Chou", "authors": "Stephen Chou, Fredrik Kjolstad, Saman Amarasinghe", "title": "Automatic Generation of Efficient Sparse Tensor Format Conversion\n  Routines", "comments": "Presented at PLDI 2020", "journal-ref": null, "doi": "10.1145/3385412.3385963", "report-no": null, "categories": "cs.MS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows how to generate code that efficiently converts sparse\ntensors between disparate storage formats (data layouts) such as CSR, DIA, ELL,\nand many others. We decompose sparse tensor conversion into three logical\nphases: coordinate remapping, analysis, and assembly. We then develop a\nlanguage that precisely describes how different formats group together and\norder a tensor's nonzeros in memory. This lets a compiler emit code that\nperforms complex remappings of nonzeros when converting between formats. We\nalso develop a query language that can extract statistics about sparse tensors,\nand we show how to emit efficient analysis code that computes such queries.\nFinally, we define an abstract interface that captures how data structures for\nstoring a tensor can be efficiently assembled given specific statistics about\nthe tensor. Disparate formats can implement this common interface, thus letting\na compiler emit optimized sparse tensor conversion code for arbitrary\ncombinations of many formats without hard-coding for any specific combination.\n  Our evaluation shows that the technique generates sparse tensor conversion\nroutines with performance between 1.00 and 2.01$\\times$ that of hand-optimized\nversions in SPARSKIT and Intel MKL, two popular sparse linear algebra\nlibraries. And by emitting code that avoids materializing temporaries, which\nboth libraries need for many combinations of source and target formats, our\ntechnique outperforms those libraries by 1.78 to 4.01$\\times$ for CSC/COO to\nDIA/ELL conversion.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 16:43:35 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 22:51:30 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 02:09:40 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Chou", "Stephen", ""], ["Kjolstad", "Fredrik", ""], ["Amarasinghe", "Saman", ""]]}, {"id": "2001.02630", "submitter": "Bruno Bernardo", "authors": "Bruno Bernardo, Rapha\\\"el Cauderlier, Basile Pesin, and Julien Tesson", "title": "Albert, an intermediate smart-contract language for the Tezos blockchain", "comments": "15 pages. arXiv admin note: text overlap with arXiv:1909.08671", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tezos is a smart-contract blockchain. Tezos smart contracts are written in a\nlow-level stack-based language called Michelson. In this article we present\nAlbert, an intermediate language for Tezos smart contracts which abstracts\nMichelson stacks as linearly typed records. We also describe its compiler to\nMichelson, written in Coq, that targets Mi-Cho-Coq, a formal specification of\nMichelson implemented in Coq.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 13:07:22 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Bernardo", "Bruno", ""], ["Cauderlier", "Rapha\u00ebl", ""], ["Pesin", "Basile", ""], ["Tesson", "Julien", ""]]}, {"id": "2001.02656", "submitter": "David Tolpin", "authors": "David Tolpin, Tomer Dobkin", "title": "Stochastic Probabilistic Programs", "comments": "7 pages main body, 4 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of a stochastic probabilistic program and present a\nreference implementation of a probabilistic programming facility supporting\nspecification of stochastic probabilistic programs and inference in them.\nStochastic probabilistic programs allow straightforward specification and\nefficient inference in models with nuisance parameters, noise, and\nnondeterminism. We give several examples of stochastic probabilistic programs,\nand compare the programs with corresponding deterministic probabilistic\nprograms in terms of model specification and inference. We conclude with\ndiscussion of open research topics and related work.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 17:54:40 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 07:14:20 GMT"}, {"version": "v3", "created": "Wed, 22 Jan 2020 16:02:56 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Tolpin", "David", ""], ["Dobkin", "Tomer", ""]]}, {"id": "2001.02659", "submitter": "Paul He", "authors": "Yannick Zakowski, Paul He, Chung-Kil Hur, Steve Zdancewic", "title": "An Equational Theory for Weak Bisimulation via Generalized Parameterized\n  Coinduction", "comments": "To be published in CPP 2020", "journal-ref": null, "doi": "10.1145/3372885.3373813", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coinductive reasoning about infinitary structures such as streams is widely\napplicable. However, practical frameworks for developing coinductive proofs and\nfinding reasoning principles that help structure such proofs remain a\nchallenge, especially in the context of machine-checked formalization.\n  This paper gives a novel presentation of an equational theory for reasoning\nabout structures up to weak bisimulation. The theory is both compositional,\nmaking it suitable for defining general-purpose lemmas, and also incremental,\nmeaning that the bisimulation can be created interactively. To prove the\ntheory's soundness, this paper also introduces generalized parameterized\ncoinduction, which addresses expressivity problems of earlier works and\nprovides a practical framework for coinductive reasoning. The paper presents\nthe resulting equational theory for streams, but the technique applies to other\nstructures too.\n  All of the results in this paper have been proved in Coq, and the generalized\nparameterized coinduction framework is available as a Coq library.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 18:03:10 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Zakowski", "Yannick", ""], ["He", "Paul", ""], ["Hur", "Chung-Kil", ""], ["Zdancewic", "Steve", ""]]}, {"id": "2001.02828", "submitter": "Christopher Jenkins", "authors": "Christopher Jenkins and Aaron Stump", "title": "Monotone recursive types and recursive data representations in Cedille", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guided by Tarksi's fixpoint theorem in order theory, we show how to derive\nmonotone recursive types with constant-time roll and unroll operations within\nCedille, an impredicative, constructive, and logically consistent pure typed\nlambda calculus. This derivation takes place within the preorder on Cedille\ntypes induced by type inclusions, a notion which is expressible within the\ntheory itself. As applications, we use monotone recursive types to generically\nderive two recursive representations of data in lambda calculus, the Parigot\nand Scott encoding. For both encodings, we prove induction and examine the\ncomputational and extensional properties of their destructor, iterator, and\nprimitive recursor in Cedille. For our Scott encoding in particular, we\ntranslate into Cedille a construction due to Lepigre and Raffalli that equips\nScott naturals with primitive recursion, then extend this construction to\nderive a generic induction principle. This allows us to give efficient and\nprovably unique (up to function extensionality) solutions for the iteration and\nprimitive recursion schemes for Scott-encoded data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 04:04:27 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 01:46:55 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Jenkins", "Christopher", ""], ["Stump", "Aaron", ""]]}, {"id": "2001.02981", "submitter": "Laura Titolo", "authors": "Laura Titolo, Mariano Moscato, Cesar A. Mu\\~noz", "title": "Automatic generation and verification of test-stable floating-point code", "comments": "32 pages. arXiv admin note: text overlap with arXiv:1808.04289", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Test instability in a floating-point program occurs when the control flow of\nthe program diverges from its ideal execution assuming real arithmetic. This\nphenomenon is caused by the presence of round-off errors that affect the\nevaluation of arithmetic expressions occurring in conditional statements.\nUnstable tests may lead to significant errors in safety-critical applications\nthat depend on numerical computations. Writing programs that take into\nconsideration test instability is a difficult task that requires expertise on\nfinite precision computations and rounding errors. This paper presents a\ntoolchain to automatically generate and verify a provably correct test-stable\nfloating-point program from a functional specification in real arithmetic. The\ninput is a real-valued program written in the Prototype Verification System\n(PVS) specification language and the output is a transformed floating-point C\nprogram annotated with ANSI/ISO C Specification Language (ACSL) contracts.\nThese contracts relate the floating-point program to its functional\nspecification in real arithmetic. The transformed program detects if unstable\ntests may occur and, in these cases, issues a warning and terminate. An\napproach that combines the Frama-C analyzer, the PRECiSA round-off error\nestimator, and PVS is proposed to automatically verify that the generated\nprogram code is correct in the sense that, if the program terminates without a\nwarning, it follows the same computational path as its real-valued functional\nspecification.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 19:46:42 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Titolo", "Laura", ""], ["Moscato", "Mariano", ""], ["Mu\u00f1oz", "Cesar A.", ""]]}, {"id": "2001.03256", "submitter": "\\'Akos Hajdu", "authors": "\\'Akos Hajdu, Dejan Jovanovi\\'c", "title": "SMT-Friendly Formalization of the Solidity Memory Model", "comments": "Authors' manuscript. Published in P. M\\\"uller (Ed.): ESOP 2020, LNCS\n  12075, 2020. The final publication is available at Springer via\n  https://doi.org/10.1007/978-3-030-44914-8_9", "journal-ref": null, "doi": "10.1007/978-3-030-44914-8_9", "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Solidity is the dominant programming language for Ethereum smart contracts.\nThis paper presents a high-level formalization of the Solidity language with a\nfocus on the memory model. The presented formalization covers all features of\nthe language related to managing state and memory. In addition, the\nformalization we provide is effective: all but few features can be encoded in\nthe quantifier-free fragment of standard SMT theories. This enables precise and\nefficient reasoning about the state of smart contracts written in Solidity. The\nformalization is implemented in the solc-verify verifier and we provide an\nextensive set of tests that covers the breadth of the required semantics. We\nalso provide an evaluation on the test set that validates the semantics and\nshows the novelty of the approach compared to other Solidity-level contract\nanalysis tools.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 23:29:19 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 13:09:17 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Hajdu", "\u00c1kos", ""], ["Jovanovi\u0107", "Dejan", ""]]}, {"id": "2001.03541", "submitter": "Amir Shaikhha", "authors": "Amir Shaikhha, Maximilian Schleich, Alexandru Ghita, Dan Olteanu", "title": "Multi-layer Optimizations for End-to-End Data Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of training machine learning models over\nmulti-relational data. The mainstream approach is to first construct the\ntraining dataset using a feature extraction query over input database and then\nuse a statistical software package of choice to train the model. In this paper\nwe introduce Iterative Functional Aggregate Queries (IFAQ), a framework that\nrealizes an alternative approach. IFAQ treats the feature extraction query and\nthe learning task as one program given in the IFAQ's domain-specific language,\nwhich captures a subset of Python commonly used in Jupyter notebooks for rapid\nprototyping of machine learning applications. The program is subject to several\nlayers of IFAQ optimizations, such as algebraic transformations, loop\ntransformations, schema specialization, data layout optimizations, and finally\ncompilation into efficient low-level C++ code specialized for the given\nworkload and data.\n  We show that a Scala implementation of IFAQ can outperform mlpack, Scikit,\nand TensorFlow by several orders of magnitude for linear regression and\nregression tree models over several relational datasets.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 16:14:44 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Shaikhha", "Amir", ""], ["Schleich", "Maximilian", ""], ["Ghita", "Alexandru", ""], ["Olteanu", "Dan", ""]]}, {"id": "2001.04301", "submitter": "Daniel Selsam", "authors": "Daniel Selsam, Sebastian Ullrich, Leonardo de Moura", "title": "Tabled Typeclass Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typeclasses provide an elegant and effective way of managing ad-hoc\npolymorphism in both programming languages and interactive proof assistants.\nHowever, the increasingly sophisticated uses of typeclasses within proof\nassistants, especially within Lean's burgeoning mathematics library, mathlib,\nhave elevated once-theoretical limitations of existing typeclass resolution\nprocedures into major impediments to ongoing progress. The two most devastating\nlimitations of existing procedures are exponential running times in the\npresence of diamonds and divergence in the presence of cycles. We present a new\nprocedure, tabled typeclass resolution, that solves both problems by tabling,\nwhich is a generalization of memoizing originally introduced to address similar\nlimitations of early logic programming systems. We have implemented our\nprocedure for the upcoming version (v4) of Lean, and have confirmed empirically\nthat our implementation is exponentially faster than existing systems in the\npresence of diamonds. Although tabling is notoriously difficult to implement,\nour procedure is notably lightweight and could easily be implemented in other\nsystems. We hope our new procedure facilitates even more sophisticated uses of\ntypeclasses in both software development and interactive theorem proving.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 14:40:59 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 15:09:43 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Selsam", "Daniel", ""], ["Ullrich", "Sebastian", ""], ["de Moura", "Leonardo", ""]]}, {"id": "2001.04439", "submitter": "Ankush Das", "authors": "Ankush Das and Frank Pfenning", "title": "Session Types with Arithmetic Refinements and Their Application to Work\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session types statically prescribe bidirectional communication protocols for\nmessage-passing processes and are in a Curry-Howard correspondence with linear\nlogic propositions. However, simple session types cannot specify properties\nbeyond the type of exchanged messages. In this paper we extend the type system\nby using index refinements from linear arithmetic capturing intrinsic\nattributes of data structures and algorithms so that we can express and verify\namortized cost of programs using ergometric types. We show that, despite the\ndecidability of Presburger arithmetic, type equality and therefore also type\nchecking are now undecidable, which stands in contrast to analogous dependent\nrefinement type systems from functional languages. We also present a practical\nincomplete algorithm for type equality and an algorithm for type checking which\nis complete relative to an oracle for type equality. Process expressions in\nthis explicit language are rather verbose, so we also introduce an implicit\nform and a sound and complete algorithm for reconstructing explicit programs,\nborrowing ideas from the proof-theoretic technique of focusing. We conclude by\nillustrating our systems and algorithms with a variety of examples that have\nbeen verified in our implementation.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 18:20:50 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 14:49:47 GMT"}, {"version": "v3", "created": "Thu, 23 Jan 2020 23:19:49 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Das", "Ankush", ""], ["Pfenning", "Frank", ""]]}, {"id": "2001.04729", "submitter": "Kuize Zhang", "authors": "Kuize Zhang", "title": "A unified method to decentralized state inference and fault\n  diagnosis/prediction of discrete-event systems", "comments": "30 pages,12 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.PL cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state inference problem and fault diagnosis/prediction problem are\nfundamental topics in many areas. In this paper, we consider discrete-event\nsystems (DESs) modeled by finite-state automata (FSAs). There exist results for\ndecentralized versions of the latter problem but there is almost no result for\na decentralized version of the former problem. We propose a decentralized\nversion of strong detectability called co-detectability which implies that once\na system satisfies this property, for each generated infinite-length event\nsequence, at least one local observer can determine the current and subsequent\nstates after a common observation time delay. We prove that the problem of\nverifying co-detectability of FSAs is coNP-hard. Moreover, we use a unified\nconcurrent-composition method to give PSPACE verification algorithms for\nco-detectability, co-diagnosability, and co-predictability of FSAs, without any\nassumption or modifying the FSAs under consideration, where co-diagnosability\nis firstly studied by [Debouk & Lafortune & Teneketzis 2000], while\nco-predictability is firstly studied by [Kumar \\& Takai 2010]. By our proposed\nunified method, one can see that in order to verify co-detectability, more\ntechnical difficulties will be met compared to verifying the other two\nproperties, because in co-detectability, generated outputs are counted, but in\nthe latter two properties, only occurrences of events are counted. For example,\nwhen one output was generated, any number of unobservable events could have\noccurred. The PSPACE-hardness of verifying co-diagnosability is already known\nin the literature. In this paper, we prove the PSPACE-hardness of verifying\nco-predictability.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 12:01:59 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 15:08:55 GMT"}, {"version": "v3", "created": "Thu, 23 Jan 2020 18:40:28 GMT"}, {"version": "v4", "created": "Wed, 5 Feb 2020 13:11:50 GMT"}, {"version": "v5", "created": "Thu, 13 Feb 2020 17:35:40 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Zhang", "Kuize", ""]]}, {"id": "2001.04961", "submitter": "Umang Mathur", "authors": "Umang Mathur, Mahesh Viswanathan", "title": "Atomicity Checking in Linear Time using Vector Clocks", "comments": null, "journal-ref": null, "doi": "10.1145/3373376.3378475", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-threaded programs are challenging to write. Developers often need to\nreason about a prohibitively large number of thread interleavings to reason\nabout the behavior of software. A non-interference property like atomicity can\nreduce this interleaving space by ensuring that any execution is equivalent to\nan execution where all atomic blocks are executed serially. We consider the\nwell studied notion of conflict serializability for dynamically checking\natomicity. Existing algorithms detect violations of conflict serializability by\ndetecting cycles in a graph of transactions observed in a given execution. The\nnumber of edges in such a graph can grow quadratically with the length of the\ntrace making the analysis not scalable. In this paper, we present AeroDrome, a\nnovel single pass linear time algorithm that uses vector clocks to detect\nviolations of conflict serializability in an online setting. Experiments show\nthat AeroDrome scales to traces with a large number of events with significant\nspeedup.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 18:47:31 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 04:23:52 GMT"}, {"version": "v3", "created": "Sat, 17 Oct 2020 18:33:26 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Mathur", "Umang", ""], ["Viswanathan", "Mahesh", ""]]}, {"id": "2001.05059", "submitter": "Petar Maksimovi\\'c", "authors": "Jos\\'e Fragoso Santos, Petar Maksimovi\\'c, Sacha-\\'Elie Ayoun,\n  Philippa Gardner", "title": "Gillian: Compositional Symbolic Execution for All", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Gillian, a language-independent framework for the development of\ncompositional symbolic analysis tools. Gillian supports three flavours of\nanalysis: whole-program symbolic testing, full verification, and bi-abduction.\nIt comes with fully parametric meta-theoretical results and a modular\nimplementation, designed to minimise the instantiation effort required of the\nuser. We evaluate Gillian by instantiating it to JavaScript and C, and perform\nits analyses on a set of data-structure libraries, obtaining results that\nindicate that Gillian is robust enough to reason about real-world programming\nlanguages.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 22:03:48 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Santos", "Jos\u00e9 Fragoso", ""], ["Maksimovi\u0107", "Petar", ""], ["Ayoun", "Sacha-\u00c9lie", ""], ["Gardner", "Philippa", ""]]}, {"id": "2001.05132", "submitter": "Farzaneh Derakhshan", "authors": "Farzaneh Derakhshan, Frank Pfenning", "title": "Strong Progress for Session-Typed Processes in a Linear Metalogic with\n  Circular Proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce an infinitary first order linear logic with least and greatest\nfixed points. To ensure cut elimination, we impose a validity condition on\ninfinite derivations. Our calculus is designed to reason about rich signatures\nof mutually defined inductive and coinductive linear predicates. In a major\ncase study we use it to prove the strong progress property for binary\nsession-typed processes under an asynchronous communication semantics. As far\nas we are aware, this is the first proof of this property.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 04:47:39 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 17:33:54 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Derakhshan", "Farzaneh", ""], ["Pfenning", "Frank", ""]]}, {"id": "2001.06163", "submitter": "EPTCS", "authors": "Alessandro Aldini (University of Urbino), Herbert Wiklicky (Imperial\n  College London)", "title": "Proceedings 16th Workshop on Quantitative Aspects of Programming\n  Languages and Systems", "comments": null, "journal-ref": "EPTCS 312, 2020", "doi": "10.4204/EPTCS.312", "report-no": null, "categories": "cs.PL cs.GT cs.IT cs.LO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This EPTCS volume contains the proceedings of the 16th Workshop on\nQuantitative Aspects of Programming Languages and Systems (QAPL 2019) held in\nPrague, Czech Republic, on Sunday 7 April 2019. QAPL 2019 was a satellite event\nof the European Joint Conferences on Theory and Practice of Software (ETAPS\n2019).\n  QAPL focuses on quantitative aspects of computations, which may refer to the\nuse of physical quantities (time, bandwidth, etc.) as well as mathematical\nquantities (e.g., probabilities) for the characterisation of the behaviour and\nfor determining the properties of systems. Such quantities play a central role\nin defining both the model of systems (architecture, language design,\nsemantics) and the methodologies and tools for the analysis and verification of\nsystem properties. The aim of the QAPL workshop series is to discuss the\nexplicit use of time and probability and general quantities either directly in\nthe model or as a tool for the analysis or synthesis of systems.\n  The 16th edition of QAPL also focuses on discussing the developments,\nchallenges and results in this area covered by our workshop in its nearly\n20-year history.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 05:38:50 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Aldini", "Alessandro", "", "University of Urbino"], ["Wiklicky", "Herbert", "", "Imperial\n  College London"]]}, {"id": "2001.06942", "submitter": "EPTCS", "authors": "Alessandro Aldini (University of Urbino)", "title": "Quantitative Aspects of Programming Languages and Systems over the past\n  $2^4$ years and beyond", "comments": "In Proceedings QAPL 2019, arXiv:2001.06163", "journal-ref": "EPTCS 312, 2020, pp. 1-19", "doi": "10.4204/EPTCS.312.1", "report-no": null, "categories": "cs.PL cs.IT cs.LO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative aspects of computation are related to the use of both physical\nand mathematical quantities, including time, performance metrics, probability,\nand measures for reliability and security. They are essential in characterizing\nthe behaviour of many critical systems and in estimating their properties.\nHence, they need to be integrated both at the level of system modeling and\nwithin the verification methodologies and tools. Along the last two decades a\nvariety of theoretical achievements and automated techniques have contributed\nto make quantitative modeling and verification mainstream in the research\ncommunity. In the same period, they represented the central theme of the series\nof workshops entitled Quantitative Aspects of Programming Languages and Systems\n(QAPL) and born in 2001. The aim of this survey is to revisit such achievements\nand results from the standpoint of QAPL and its community.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 02:17:03 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Aldini", "Alessandro", "", "University of Urbino"]]}, {"id": "2001.06943", "submitter": "EPTCS", "authors": "Maja Hanne Kirkeby (Computer Science, Roskilde University, Denmark)", "title": "Probabilistic Output Analyses for Deterministic Programs --- Reusing\n  Existing Non-probabilistic Analyses", "comments": "In Proceedings QAPL 2019, arXiv:2001.06163", "journal-ref": "EPTCS 312, 2020, pp. 43-57", "doi": "10.4204/EPTCS.312.4", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider reusing established non-probabilistic output analyses (either\nforward or backwards) that yield over-approximations of a program's pre-image\nor image relation, e.g., interval analyses. We assume a probability measure\nover the program input and present two techniques (one for forward and one for\nbackward analyses) that both derive upper and lower probability bounds for the\noutput events. We demonstrate the most involved technique, namely the forward\ntechnique, for two examples and compare their results to a cutting-edge\nprobabilistic output analysis.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 02:18:04 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Kirkeby", "Maja Hanne", "", "Computer Science, Roskilde University, Denmark"]]}, {"id": "2001.06952", "submitter": "Xiaokang Qiu", "authors": "Xiaokang Qiu", "title": "Streaming Transformations of Infinite Ordered-Data Words", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we define streaming register transducer (SRT), a one-way,\nletter-to-letter, transductional machine model for transformations of infinite\ndata words whose data domain forms a linear group. Comparing with existing data\nword transducers, SRT are able to perform two extra operations on the\nregisters: a linear-order-based comparison and an additive update. We consider\nthe transformations that can be defined by SRT and several subclasses of SRT.\nWe investigate the expressiveness of these languages and several decision\nproblems. Our main results include: 1) SRT are closed under union and\nintersection, and add-free SRT are also closed under composition; 2)\nSRT-definable transformations can be defined in monadic second-order (MSO)\nlogic, but are not comparable with first-order (FO) definable transformations;\n3) the functionality problem is decidable for add-free SRT, the reactivity\nproblem and inclusion problem are decidable for deterministic add-free SRT, but\nnone of these problems is decidable in general for SRT.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 02:53:35 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Qiu", "Xiaokang", ""]]}, {"id": "2001.07063", "submitter": "Jean-Marie Madiot", "authors": "Jean-Marie Madiot, Damien Pous, Davide Sangiorgi", "title": "Modular coinduction up-to for higher-order languages via first-order\n  transition systems", "comments": "This paper is an improved and extended version of a paper appeared in\n  Proc. CONCUR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The bisimulation proof method can be enhanced by employing `bisimulations\nup-to' techniques. A comprehensive theory of such enhancements has been\ndeveloped for first-order (i.e., CCS-like) labelled transition systems (LTSs)\nand bisimilarity, based on abstract fixed-point theory and compatible\nfunctions.\n  We transport this theory onto languages whose bisimilarity and LTS go beyond\nthose of first-order models. The approach consists in exhibiting fully abstract\ntranslations of the more sophisticated LTSs and bisimilarities onto the\nfirst-order ones. This allows us to reuse directly the large corpus of up-to\ntechniques that are available on first-order LTSs. The only ingredient that has\nto be manually supplied is the compatibility of basic up-to techniques that are\nspecific to the new languages. We investigate the method on the pi-calculus,\nthe lambda-calculus, and a (call-by-value) lambda-calculus with references.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 11:32:03 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 11:14:32 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 12:52:11 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Madiot", "Jean-Marie", ""], ["Pous", "Damien", ""], ["Sangiorgi", "Davide", ""]]}, {"id": "2001.07488", "submitter": "Mario Rom\\'an", "authors": "Bryce Clarke, Derek Elkins, Jeremy Gibbons, Fosco Loregian, Bartosz\n  Milewski, Emily Pillmore and Mario Rom\\'an", "title": "Profunctor optics, a categorical update", "comments": "42 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Profunctor optics are bidirectional data accessors that capture data\ntransformation patterns such as accessing subfields or iterating over\ncontainers. They are modular, meaning that we can construct accessors for\ncomplex structures by combining simpler ones. Profunctor optics have been\nstudied only using $\\mathbf{Sets}$ as the enriching category and in the\nnon-mixed case. However, functional programming languages are arguably better\ndescribed by enriched categories and we have found that some structures in the\nliterature are actually mixed optics. Our work generalizes a classic result by\nPastro and Street on Tambara theory and uses it to describe mixed V-enriched\nprofunctor optics and to endow them with V-category structure. We provide some\noriginal families of optics and derivations, including an elementary one for\ntraversals that solves an open problem posed by Milewski. Finally, we discuss a\nHaskell implementation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 12:53:00 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Clarke", "Bryce", ""], ["Elkins", "Derek", ""], ["Gibbons", "Jeremy", ""], ["Loregian", "Fosco", ""], ["Milewski", "Bartosz", ""], ["Pillmore", "Emily", ""], ["Rom\u00e1n", "Mario", ""]]}, {"id": "2001.08040", "submitter": "Thorsten Wissmann", "authors": "Mario Bravetti", "title": "Axiomatizing Maximal Progress and Discrete Time", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 17, Issue 1 (January\n  21, 2021) lmcs:7116", "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Milner's complete proof system for observational congruence is crucially\nbased on the possibility to equate $\\tau$ divergent expressions to\nnon-divergent ones by means of the axiom $recX. (\\tau.X + E) = recX. \\tau. E$.\nIn the presence of a notion of priority, where, e.g., actions of type $\\delta$\nhave a lower priority than silent $\\tau$ actions, this axiom is no longer\nsound. Such a form of priority is, however, common in timed process algebra,\nwhere, due to the interpretation of $\\delta$ as a time delay, it naturally\narises from the maximal progress assumption. We here present our solution,\nbased on introducing an auxiliary operator $pri(E)$ defining a \"priority\nscope\", to the long time open problem of axiomatizing priority using standard\nobservational congruence: we provide a complete axiomatization for a basic\nprocess algebra with priority and (unguarded) recursion. We also show that,\nwhen the setting is extended by considering static operators of a discrete time\ncalculus, an axiomatization that is complete over (a characterization of)\nfinite-state terms can be developed by re-using techniques devised in the\ncontext of a cooperation with Prof. Jos Baeten.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 14:51:13 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2020 17:39:39 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 18:06:10 GMT"}, {"version": "v4", "created": "Wed, 20 Jan 2021 16:16:45 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Bravetti", "Mario", ""]]}, {"id": "2001.08045", "submitter": "Mario Rom\\'an", "authors": "Mario Rom\\'an", "title": "Profunctor optics and traversals", "comments": "Submitted as a thesis for MSc Mathematics and Foundations of Computer\n  Science, University of Oxford 2019. The work on the article \"Profunctor\n  optics, a categorical update\" started in this MSc dissertation. 82 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optics are bidirectional accessors of data structures; they provide a\npowerful abstraction of many common data transformations. This abstraction is\ncompositional thanks to a representation in terms of profunctors endowed with\nan algebraic structure called Tambara module. There exists a general definition\nof optic in terms of coends that, after some elementary application of the\nYoneda lemma, particularizes in each one of the basic optics. Traversals used\nto be the exception; we show an elementary derivation of traversals and discuss\nsome other new derivations for optics. We relate our characterization of\ntraversals to the previous ones showing that the coalgebras of a comonad that\nrepresents and split into shape and contents are traversable functors. The\nrepresentation of optics in terms of profunctors has many different proofs in\nthe literature; we discuss two ways of proving it, generalizing both to the\ncase of mixed optics for an arbitrary action. Categories of optics can be seen\nas Eilenberg-Moore categories for a monad described by Pastro and Street. This\ngives us two different approaches to composition between profunctor optics of\ndifferent families: using distributive laws between the monads defining them,\nand using coproducts of monads. The second one is the one implicitly used in\nHaskell programming; but we show that a refinement of the notion of optic is\nrequired in order to model it faithfully. We provide experimental\nimplementations of a library of optics in Haskell and partial Agda\nformalizations of the profunctor representation theorem.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 15:03:59 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Rom\u00e1n", "Mario", ""]]}, {"id": "2001.08133", "submitter": "Johan Bos", "authors": "Johan Bos", "title": "Drawing Prolog Search Trees: A Manual for Teachers and Students of Logic\n  Programming", "comments": "20 pages, 8 listings, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Programming in Prolog is hard for programmers that are used to procedural\ncoding. In this manual the method of drawing search trees is introduced with\nthe aim to get a better understanding of how Prolog works. After giving a first\nexample of a Prolog database, query and search tree, the art of drawing search\ntrees is systematically introduced giving guidelines for queries with\nvariables, conjunction, disjunction, and negation. Further examples are\nprovided by giving the complete search trees that are shown in Learn Prolog\nNow!\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 16:31:45 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Bos", "Johan", ""]]}, {"id": "2001.09258", "submitter": "Naoki Shibata", "authors": "Naoki Shibata and Francesco Petrogalli", "title": "SLEEF: A Portable Vectorized Library of C Standard Mathematical\n  Functions", "comments": "in IEEE Transactions on Parallel and Distributed Systems. This is a\n  version with all appendices included in a PDF. Accompanying software can be\n  accessed at https://sleef.org or https://codeocean.com/capsule/6861013", "journal-ref": null, "doi": "10.1109/TPDS.2019.2960333", "report-no": null, "categories": "cs.MS cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present techniques used to implement our portable\nvectorized library of C standard mathematical functions written entirely in C\nlanguage. In order to make the library portable while maintaining good\nperformance, intrinsic functions of vector extensions are abstracted by inline\nfunctions or preprocessor macros. We implemented the functions so that they can\nuse sub-features of vector extensions such as fused multiply-add, mask\nregisters and extraction of mantissa. In order to make computation with SIMD\ninstructions efficient, the library only uses a small number of conditional\nbranches, and all the computation paths are vectorized. We devised a variation\nof the Payne-Hanek argument reduction for trigonometric functions and a\nfloating point remainder, both of which are suitable for vector computation. We\ncompare the performance of our library to Intel SVML.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 03:05:52 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Shibata", "Naoki", ""], ["Petrogalli", "Francesco", ""]]}, {"id": "2001.09649", "submitter": "Stefan Ciobaca", "authors": "\\c{S}tefan Ciob\\^ac\\u{a}, Dorel Lucanu and Andrei Sebastian\n  Buruian\\u{a}", "title": "Operationally-based Program Equivalence Proofs using LCTRSs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an operationally-based deductive proof method for program\nequivalence. It is based on encoding the language semantics as logically\nconstrained term rewriting systems (LCTRSs) and the two programs as terms. The\nmain feature of our method is its flexibility. We illustrate this flexibility\nin two applications, which are novel. For the first application, we show how to\nencode low-level details such as stack size in the language semantics and how\nto prove equivalence between two programs operating at different levels of\nabstraction. For our running example, we show how our method can prove\nequivalence between a recursive function operating with an unbounded stack and\nits tail-recursive optimized version operating with a bounded stack. This type\nof equivalence checking can be used to ensure that new, undesirable behavior is\nnot introduced by a more concrete level of abstraction. For the second\napplication, we show how to formalize read-sets and write-sets of symbolic\nexpressions and statements by extending the operational semantics in a\nconservative way. This enables the relational verification of program schemas,\nwhich we exploit to prove correctness of compiler optimizations, some of which\ncannot be proven by existing tools. Our method requires an extension of\nstandard LCTRSs with axiomatized symbols. We also present a prototype\nimplementation that proves the feasibility of both applications that we\npropose.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 09:47:07 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Ciob\u00e2c\u0103", "\u015etefan", ""], ["Lucanu", "Dorel", ""], ["Buruian\u0103", "Andrei Sebastian", ""]]}, {"id": "2001.09995", "submitter": "Richard Uhrie", "authors": "Richard Uhrie, Chaitali Chakrabarti, John Brunhaver", "title": "Automated Parallel Kernel Extraction from Dynamic Application Traces", "comments": "14 pages, 16 figures. Submitted to IEEE Transactions on Parallel and\n  Distributed Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern program runtime is dominated by segments of repeating code called\nkernels. Kernels are accelerated by increasing memory locality, increasing\ndata-parallelism, and exploiting producer-consumer parallelism among kernels -\nwhich requires hardware specialized for a particular class of kernels.\nProgramming this hardware can be difficult, requiring that the kernels be\nidentified and annotated in the code or translated to a domain-specific\nlanguage. This paper describes a technique to automatically localize parallel\nkernels from a dynamic application trace, facilitating further code\noptimization.\n  Dynamic trace collection is fast and compact. With optimization, it only\nincurs a time-dilation of a factor on nine and file-size of one megabyte per\nsecond, addressing a significant criticism of this approach. Kernel extraction\nis accurate and performed in linear time within logarithmic memory, detecting a\nwide range of kernels. This approach was validated across 16 libraries,\ncomprised of 10,507 kernels instances. To validate the accuracy of our detected\nkernels, five test programs were written that spans traditional kernel\ndefinitions and were certified to contain all the kernels that were expected.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 14:51:08 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Uhrie", "Richard", ""], ["Chakrabarti", "Chaitali", ""], ["Brunhaver", "John", ""]]}, {"id": "2001.10052", "submitter": "Venkatesh-Prasad Ranganath", "authors": "Joydeep Mitra, Venkatesh-Prasad Ranganath, Torben Amtoft, Mike Higgins", "title": "SeMA: Extending and Analyzing Storyboards to Develop Secure Android Apps", "comments": "Updates based on reviews MobileSoft, FSE, and Onward reviews. Added\n  content about usability evaluation. Added formal syntax and semantics of the\n  DSL and info flow analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile apps provide various critical services, such as banking,\ncommunication, and healthcare. To this end, they have access to our personal\ninformation and have the ability to perform actions on our behalf. Hence,\nsecuring mobile apps is crucial to ensuring the privacy and safety of its\nusers.\n  Recent research efforts have focused on developing solutions to secure mobile\necosystems (i.e., app platforms, apps, and app stores), specifically in the\ncontext of detecting vulnerabilities in Android apps. Despite this attention,\nknown vulnerabilities are often found in mobile apps, which can be exploited by\nmalicious apps to harm the user. Further, fixing vulnerabilities after\ndeveloping an app has downsides in terms of time, resources, user\ninconvenience, and information loss.\n  In an attempt to address this concern, we have developed SeMA, a mobile app\ndevelopment methodology that builds on existing mobile app design artifacts\nsuch as storyboards. With SeMA, security is a first-class citizen in an app's\ndesign -- app designers and developers can collaborate to specify and reason\nabout the security properties of an app at an abstract level without being\ndistracted by implementation level details. Our realization of SeMA using\nAndroid Studio tooling demonstrates the methodology is complementary to\nexisting design and development practices. An evaluation of the effectiveness\nof SeMA shows the methodology can detect and help prevent 49 vulnerabilities\nknown to occur in Android apps. Further, a usability study of the methodology\ninvolving ten real-world developers shows the methodology is likely to reduce\nthe development time and help developers uncover and prevent known\nvulnerabilities while designing apps.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 20:10:52 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 19:41:13 GMT"}, {"version": "v3", "created": "Mon, 20 Jul 2020 21:32:10 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Mitra", "Joydeep", ""], ["Ranganath", "Venkatesh-Prasad", ""], ["Amtoft", "Torben", ""], ["Higgins", "Mike", ""]]}, {"id": "2001.10150", "submitter": "Di Wang", "authors": "Di Wang, Jan Hoffmann, Thomas Reps", "title": "Central Moment Analysis for Cost Accumulators in Probabilistic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For probabilistic programs, it is usually not possible to automatically\nderive exact information about their properties, such as the distribution of\nstates at a given program point. Instead, one can attempt to derive\napproximations, such as upper bounds on tail probabilities. Such bounds can be\nobtained via concentration inequalities, which rely on the moments of a\ndistribution, such as the expectation (the first raw moment) or the variance\n(the second central moment). Tail bounds obtained using central moments are\noften tighter than the ones obtained using raw moments, but automatically\nanalyzing central moments is more challenging.\n  This paper presents an analysis for probabilistic programs that automatically\nderives symbolic upper and lower bounds on variances, as well as higher central\nmoments, of cost accumulators. To overcome the challenges of higher-moment\nanalysis, it generalizes analyses for expectations with an algebraic\nabstraction that simultaneously analyzes different moments, utilizing relations\nbetween them. A key innovation is the notion of moment-polymorphic recursion,\nand a practical derivation system that handles recursive functions.\n  The analysis has been implemented using a template-based technique that\nreduces the inference of polynomial bounds to linear programming. Experiments\nwith our prototype central-moment analyzer show that, despite the analyzer's\nupper/lower bounds on various quantities, it obtains tighter tail bounds than\nan existing system that uses only raw moments, such as expectations.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 03:01:52 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 06:46:06 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 08:14:51 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Wang", "Di", ""], ["Hoffmann", "Jan", ""], ["Reps", "Thomas", ""]]}, {"id": "2001.10274", "submitter": "EPTCS", "authors": "Dominic Orchard (University of Kent), Philip Wadler (University of\n  Edinburgh), Harley Eades III (Augusta University)", "title": "Unifying graded and parameterised monads", "comments": "In Proceedings MSFP 2020, arXiv:2004.14735", "journal-ref": "EPTCS 317, 2020, pp. 18-38", "doi": "10.4204/EPTCS.317.2", "report-no": null, "categories": "cs.PL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monads are a useful tool for structuring effectful features of computation\nsuch as state, non-determinism, and continuations. In the last decade, several\ngeneralisations of monads have been suggested which provide a more fine-grained\nmodel of effects by replacing the single type constructor of a monad with an\nindexed family of constructors. Most notably, graded monads (indexed by a\nmonoid) model effect systems and parameterised monads (indexed by pairs of pre-\nand post-conditions) model program logics. This paper studies the relationship\nbetween these two generalisations of monads via a third generalisation. This\nthird generalisation, which we call category-graded monads, arises by\ngeneralising a view of monads as a particular special case of lax functors. A\ncategory-graded monad provides a family of functors T f indexed by morphisms f\nof some other category. This allows certain compositions of effects to be ruled\nout (in the style of a program logic) as well as an abstract description of\neffects (in the style of an effect system). Using this as a basis, we show how\ngraded and parameterised monads can be unified, studying their similarities and\ndifferences along the way.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 11:41:39 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 03:41:57 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Orchard", "Dominic", "", "University of Kent"], ["Wadler", "Philip", "", "University of\n  Edinburgh"], ["Eades", "Harley", "III", "Augusta University"]]}, {"id": "2001.10328", "submitter": "Inzemamul Haque", "authors": "Inzemamul Haque, Deepak D'Souza, Habeeb P, Arnab Kundu and Ganesh Babu", "title": "Verification of a Generative Separation Kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a formal verification of the functional correctness of the Muen\nSeparation Kernel. Muen is representative of the class of modern separation\nkernels that leverage hardware virtualization support, and are generative in\nnature in that they generate a specialized kernel for each system\nconfiguration. These features pose substantial challenges to existing\nverification techniques. We propose a verification framework called conditional\nparametric refinement which allows us to formally reason about generative\nsystems. We use this framework to carry out a conditional refinement-based\nproof of correctness of the Muen kernel generator. Our analysis of several\nsystem configurations shows that our technique is effective in producing\nmechanized proofs of correctness, and also in identifying issues that may\ncompromise the separation property.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 06:17:25 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 06:36:40 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Haque", "Inzemamul", ""], ["D'Souza", "Deepak", ""], ["P", "Habeeb", ""], ["Kundu", "Arnab", ""], ["Babu", "Ganesh", ""]]}, {"id": "2001.10490", "submitter": "Sebastian Ullrich", "authors": "Sebastian Ullrich and Leonardo de Moura", "title": "Beyond Notations: Hygienic Macro Expansion for Theorem Proving Languages", "comments": "accepted to IJCAR 2020 (v3)", "journal-ref": "IJCAR 2020", "doi": "10.1007/978-3-030-51054-1_10", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In interactive theorem provers (ITPs), extensible syntax is not only crucial\nto lower the cognitive burden of manipulating complex mathematical objects, but\nplays a critical role in developing reusable abstractions in libraries. Most\nITPs support such extensions in the form of restrictive \"syntax sugar\"\nsubstitutions and other ad hoc mechanisms, which are too rudimentary to support\nmany desirable abstractions. As a result, libraries are littered with\nunnecessary redundancy. Tactic languages in these systems are plagued by a\nseemingly unrelated issue: accidental name capture, which often produces\nunexpected and counterintuitive behavior. We take ideas from the Scheme family\nof programming languages and solve these two problems simultaneously by\nproposing a novel hygienic macro system custom-built for ITPs. We further\ndescribe how our approach can be extended to cover type-directed macro\nexpansion resulting in a single, uniform system offering multiple abstraction\nlevels that range from supporting simplest syntax sugars to elaboration of\nformerly baked-in syntax. We have implemented our new macro system and\nintegrated it into the new version of the Lean theorem prover, Lean 4. Despite\nits expressivity, the macro system is simple enough that it can easily be\nintegrated into other systems.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 17:49:29 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 23:03:20 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2020 09:10:16 GMT"}, {"version": "v4", "created": "Thu, 29 Apr 2021 13:12:46 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Ullrich", "Sebastian", ""], ["de Moura", "Leonardo", ""]]}, {"id": "2001.10594", "submitter": "Robert Y. Lewis", "authors": "Robert Y. Lewis and Paul-Nicolas Madelaine", "title": "Simplifying Casts and Coercions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces norm_cast, a toolbox of tactics for the Lean proof\nassistant designed to manipulate expressions containing coercions and casts.\nThese expressions can be frustrating for beginning and expert users alike; the\npresence of coercions can cause seemingly identical expressions to fail to\nunify and rewrites to fail. The norm_cast tactics aim to make reasoning with\nsuch expressions as transparent as possible. They are used extensively to\neliminate boilerplate arguments in the Lean mathematical library and in\nexternal developments.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 21:21:44 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 15:33:59 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Lewis", "Robert Y.", ""], ["Madelaine", "Paul-Nicolas", ""]]}, {"id": "2001.10630", "submitter": "Ethan Cecchetti", "authors": "Andrew K. Hirsch, Pedro H. Azevedo de Amorim, Ethan Cecchetti, Ross\n  Tate, Owen Arden", "title": "First-Order Logic for Flow-Limited Authorization", "comments": "Coq code can be found at https://github.com/FLAFOL/flafol-coq", "journal-ref": null, "doi": "10.1109/CSF49147.2020.00017", "report-no": null, "categories": "cs.CR cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Flow-Limited Authorization First-Order Logic (FLAFOL), a logic\nfor reasoning about authorization decisions in the presence of information-flow\npolicies. We formalize the FLAFOL proof system, characterize its\nproof-theoretic properties, and develop its security guarantees. In particular,\nFLAFOL is the first logic to provide a non-interference guarantee while\nsupporting all connectives of first-order logic. Furthermore, this guarantee is\nthe first to combine the notions of non-interference from both authorization\nlogic and information-flow systems. All theorems in this paper are proven in\nCoq.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 23:01:05 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Hirsch", "Andrew K.", ""], ["de Amorim", "Pedro H. Azevedo", ""], ["Cecchetti", "Ethan", ""], ["Tate", "Ross", ""], ["Arden", "Owen", ""]]}, {"id": "2001.10723", "submitter": "Ilya Sergey", "authors": "Andreea Costea, Amy Zhu, Nadia Polikarpova, Ilya Sergey", "title": "Concise Read-Only Specifications for Better Synthesis of Programs with\n  Pointers -- Extended Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In program synthesis there is a well-known trade-off between concise and\nstrong specifications: if a specification is too verbose, it might be harder to\nwrite than the program; if it is too weak, the synthesised program might not\nmatch the user's intent. In this work we explore the use of annotations for\nrestricting memory access permissions in program synthesis, and show that they\ncan make specifications much stronger while remaining surprisingly concise.\nSpecifically, we enhance Synthetic Separation Logic (SSL), a framework for\nsynthesis of heap-manipulating programs, with the logical mechanism of\nread-only borrows. We observe that this minimalistic and conservative SSL\nextension benefits the synthesis in several ways, making it more (a) expressive\n(stronger correctness guarantees are achieved with a modest annotation\noverhead), (b) effective (it produces more concise and easier-to-read\nprograms), (c) efficient (faster synthesis), and (d) robust (synthesis\nefficiency is less affected by the choice of the search heuristic). We explain\nthe intuition and provide formal treatment for read-only borrows. We\nsubstantiate the claims (a)--(d) by describing our quantitative evaluation of\nthe borrowing-aware synthesis implementation on a series of standard benchmark\nspecifications for various heap-manipulating programs.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 08:35:41 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Costea", "Andreea", ""], ["Zhu", "Amy", ""], ["Polikarpova", "Nadia", ""], ["Sergey", "Ilya", ""]]}, {"id": "2001.10799", "submitter": "Rustam Tagiew", "authors": "Rustam Tagiew", "title": "Business Negotiation Definition Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.FL cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The target of this paper is to present an industry-ready prototype software\nfor general game playing. This software can also be used as the central element\nfor experimental economics research, interfacing of game-theoretic libraries,\nAI-driven software testing, algorithmic trade, human behavior mining and\nsimulation of (strategic) interactions. The software is based on a\ndomain-specific language for electronic business to business negotiations --\nSIDL3.0. The paper also contains many examples to prove the power of this\nlanguage.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 11:07:00 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Tagiew", "Rustam", ""]]}, {"id": "2001.10834", "submitter": "Yutaka Nagashima", "authors": "Yutaka Nagashima", "title": "Smart Induction for Isabelle/HOL (System Description)", "comments": "Under submission at IJCAR2020 as a System Description", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.PL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proof assistants offer tactics to facilitate inductive proofs. However, it\nstill requires human ingenuity to decide what arguments to pass to those\ninduction tactics. To automate this process, we present smart_induct for\nIsabelle/HOL. Given an inductive problem in any problem domain, smart_induct\nlists promising arguments for the induct tactic without relying on a search.\nOur evaluation demonstrated smart_induct produces valuable recommendations\nacross problem domains.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 15:29:34 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Nagashima", "Yutaka", ""]]}, {"id": "2001.11001", "submitter": "Guillaume Allais", "authors": "Guillaume Allais and Robert Atkey and James Chapman and Conor McBride\n  and James McKinna", "title": "A Type and Scope Safe Universe of Syntaxes with Binding: Their Semantics\n  and Proofs", "comments": "Extended version of the ICFP 18 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Almost every programming language's syntax includes a notion of binder and\ncorresponding bound occurrences, along with the accompanying notions of\n$\\alpha$-equivalence, capture-avoiding substitution, typing contexts, runtime\nenvironments, and so on. In the past, implementing and reasoning about\nprogramming languages required careful handling to maintain the correct\nbehaviour of bound variables. Modern programming languages include features\nthat enable constraints like scope safety to be expressed in types.\nNevertheless, the programmer is still forced to write the same boilerplate over\nagain for each new implementation of a scope safe operation (e.g., renaming,\nsubstitution, desugaring, printing, etc.), and then again for correctness\nproofs.\n  We present an expressive universe of syntaxes with binding and demonstrate\nhow to (1) implement scope safe traversals once and for all by generic\nprogramming; and (2) how to derive properties of these traversals by generic\nproving. Our universe description, generic traversals and proofs, and our\nexamples have all been formalised in Agda and are available in the accompanying\nmaterial available online at https://github.com/gallais/generic-syntax.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 18:16:54 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Allais", "Guillaume", ""], ["Atkey", "Robert", ""], ["Chapman", "James", ""], ["McBride", "Conor", ""], ["McKinna", "James", ""]]}, {"id": "2001.11560", "submitter": "Jeremy Siek", "authors": "Jeremy G. Siek", "title": "Parameterized Cast Calculi and Reusable Meta-theory for Gradually Typed\n  Lambda Calculi", "comments": "In submission to Journal of Functional Programming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research on gradual typing has led to many variations on the Gradually\nTyped Lambda Calculus (GTLC) of Siek and Taha (2006) and its underlying cast\ncalculus. For example, Wadler and Findler (2009) added blame tracking, Siek et\nal. (2009) investigated alternate cast evaluation strategies, and Herman et al.\n(2010) replaced casts with coercions for space-efficiency. The meta-theory for\nthe GTLC has also expanded beyond type safety to include blame safety\n(Tobin-Hochstadt and Felleisen 2006), space consumption (Herman et al. 2010),\nand the gradual guarantees (Siek et al. 2015). These results have been proven\nfor some variations of the GTLC but not others. Furthermore, researchers\ncontinue to develop variations on the GTLC but establishing all of the\nmeta-theory for new variations is time consuming.\n  This article identifies abstractions that capture similarities between many\ncast calculi in the form of two parameterized cast calculi, one for the\npurposes of language specification and the other to guide space-efficient\nimplementations. The article then develops reusable meta-theory for these two\ncalculi, proving type safety, blame safety, the gradual guarantees, and space\nconsumption. Finally, the article instantiates this meta-theory for eight cast\ncalculi including five from the literature and three new calculi. All of these\ndefinitions and theorems, including the two parameterized calculi, the reusable\nmeta-theory, and the eight instantiations, are mechanized in Agda making\nextensive use of module parameters and dependent records to define the\nabstractions.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 20:38:23 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 16:05:00 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Siek", "Jeremy G.", ""]]}, {"id": "2001.11604", "submitter": "Qi Wu", "authors": "Qi Wu (1), Tyson Neuroth (1), Oleg Igouchkine (1), Konduri Aditya (2),\n  Jacqueline H. Chen (3), Kwan-Liu Ma (1) ((1) UC Davis, (2) Indian Institute\n  of Science, (3) Sandia National Laboratories)", "title": "Diva: A Declarative and Reactive Language for In-Situ Visualization", "comments": "11 pages, 5 figures, 6 listings, 1 table, to be published in LDAV\n  2020. The article has gone through 2 major revisions: Emphasized\n  contributions, features and examples. Addressed connections between DIVA and\n  FRP. In sec. 3, we fixed a design flaw and addressed it in sec. 3.3-3.4.\n  Re-designed sec. 5 with a more concrete example and benchmark results.\n  Simplified the syntax of DIVA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of adaptive workflow management for in situ visualization and\nanalysis has been a growing trend in large-scale scientific simulations.\nHowever, coordinating adaptive workflows with traditional procedural\nprogramming languages can be difficult because system flow is determined by\nunpredictable scientific phenomena, which often appear in an unknown order and\ncan evade event handling. This makes the implementation of adaptive workflows\ntedious and error-prone. Recently, reactive and declarative programming\nparadigms have been recognized as well-suited solutions to similar problems in\nother domains. However, there is a dearth of research on adapting these\napproaches to in situ visualization and analysis. With this paper, we present a\nlanguage design and runtime system for developing adaptive systems through a\ndeclarative and reactive programming paradigm. We illustrate how an adaptive\nworkflow programming system is implemented using our approach and demonstrate\nit with a use case from a combustion simulation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 23:28:20 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 19:38:21 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Wu", "Qi", ""], ["Neuroth", "Tyson", ""], ["Igouchkine", "Oleg", ""], ["Aditya", "Konduri", ""], ["Chen", "Jacqueline H.", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2001.11816", "submitter": "Guillaume Boisseau", "authors": "Guillaume Boisseau", "title": "Understanding Profunctor Optics: a representation theorem", "comments": "Submitted as a thesis for the degree of MSc in Computer Science at\n  the University of Oxford in 2017. 65 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optics, aka functional references, are classes of tools that allow composable\naccess into compound data structures. Usually defined as programming language\nlibraries, they provide combinators to manipulate different shapes of data such\nas sums, products and collections, that can be composed to operate on larger\nstructures. Together they form a powerful language to describe transformations\nof data. Among the different approaches to describing optics, one particular\ntype of optics, called profunctor optics, stands out. It describes alternative\nbut equivalent representations of most of the common combinators, and enhances\nthem with elegant composability properties via a higher-order encoding.\nNotably, it enables easy composition across different optic families.\nUnfortunately, profunctor optics are difficult to reason about, and linking\nusual optics with an equivalent profunctor representation has so far been done\non a case-by-case basis, with definitions that sometimes seem very ad hoc. This\nmakes it hard both to analyse properties of existing profunctor optics and to\ndefine new ones. This thesis presents an equivalent representation of\nprofunctor optics, called isomorphism optics, that is both closer to intuition\nand easier to reason about. This tool enables powerful theorems to be derived\ngenerically about profunctor optics. Finally, this thesis develops a framework\nto ease deriving new profunctor encodings from concrete optic families.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 18:21:20 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Boisseau", "Guillaume", ""]]}, {"id": "2001.11819", "submitter": "Dan Piponi", "authors": "Dan Piponi, Dave Moore, Joshua V. Dillon", "title": "Joint Distributions for TensorFlow Probability", "comments": "Based on extended abstract submitted to PROBPROG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central tenet of probabilistic programming is that a model is specified\nexactly once in a canonical representation which is usable by inference\nalgorithms. We describe JointDistributions, a family of declarative\nrepresentations of directed graphical models in TensorFlow Probability.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 01:00:35 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Piponi", "Dan", ""], ["Moore", "Dave", ""], ["Dillon", "Joshua V.", ""]]}]