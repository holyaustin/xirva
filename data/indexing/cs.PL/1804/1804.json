[{"id": "1804.00036", "submitter": "Scott Pakin", "authors": "Scott Pakin", "title": "Performing Fully Parallel Constraint Logic Programming on a Quantum\n  Annealer", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": null, "doi": null, "report-no": "LA-UR-17-22721", "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A quantum annealer exploits quantum effects to solve a particular type of\noptimization problem. The advantage of this specialized hardware is that it\neffectively considers all possible solutions in parallel, thereby potentially\noutperforming classical computing systems. However, despite quantum annealers\nhaving recently become commercially available, there are relatively few\nhigh-level programming models that target these devices.\n  In this article, we show how to compile a subset of Prolog enhanced with\nsupport for constraint logic programming into a 2-local Ising-model Hamiltonian\nsuitable for execution on a quantum annealer. In particular, we describe the\nseries of transformations one can apply to convert constraint logic programs\nexpressed in Prolog into an executable form that bears virtually no resemblance\nto a classical machine model yet that evaluates the specified constraints in a\nfully parallel manner. We evaluate our efforts on a 1095-qubit D-Wave 2X\nquantum annealer and describe the approach's associated capabilities and\nshortcomings.\n  Under consideration in Theory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 19:14:08 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Pakin", "Scott", ""]]}, {"id": "1804.00119", "submitter": "Gerg\\H{o} \\'Erdi", "authors": "Gerg\\H{o} \\'Erdi", "title": "Generic Description of Well-Scoped, Well-Typed Syntaxes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We adapt the technique of type-generic programming via descriptions pointing\ninto a universe to the domain of typed languages with binders and variables,\nimplementing a notion of \"syntax-generic programming\" in a dependently typed\nprogramming language. We present an Agda library implementation of\ntype-preserving renaming and substitution (including proofs about their\nbehaviour) \"once and for all\" over all applicable languages using our\ntechnique.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 04:43:31 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["\u00c9rdi", "Gerg\u0151", ""]]}, {"id": "1804.00218", "submitter": "Lazar Valkov", "authors": "Lazar Valkov, Dipak Chaudhari, Akash Srivastava, Charles Sutton,\n  Swarat Chaudhuri", "title": "HOUDINI: Lifelong Learning as Program Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neurosymbolic framework for the lifelong learning of algorithmic\ntasks that mix perception and procedural reasoning. Reusing high-level concepts\nacross domains and learning complex procedures are key challenges in lifelong\nlearning. We show that a program synthesis approach that combines gradient\ndescent with combinatorial search over programs can be a more effective\nresponse to these challenges than purely neural methods. Our framework, called\nHOUDINI, represents neural networks as strongly typed, differentiable\nfunctional programs that use symbolic higher-order combinators to compose a\nlibrary of neural functions. Our learning algorithm consists of: (1) a symbolic\nprogram synthesizer that performs a type-directed search over parameterized\nprograms, and decides on the library functions to reuse, and the architectures\nto combine them, while learning a sequence of tasks; and (2) a neural module\nthat trains these programs using stochastic gradient descent. We evaluate\nHOUDINI on three benchmarks that combine perception with the algorithmic tasks\nof counting, summing, and shortest-path computation. Our experiments show that\nHOUDINI transfers high-level concepts more effectively than traditional\ntransfer learning and progressive neural networks, and that the typed\nrepresentation of networks significantly accelerates the search.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 21:34:50 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 15:59:35 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Valkov", "Lazar", ""], ["Chaudhari", "Dipak", ""], ["Srivastava", "Akash", ""], ["Sutton", "Charles", ""], ["Chaudhuri", "Swarat", ""]]}, {"id": "1804.00485", "submitter": "Didier Verna", "authors": "Didier Verna (EPITA Research and Development Laboratory (LRDE),\n  France)", "title": "Lisp, Jazz, Aikido -- Three Expressions of a Single Essence", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2018, Vol. 2,\n  Issue 3, Article 10", "doi": "10.22152/programming-journal.org/2018/2/10", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relation between Science (what we can explain) and Art (what we can't)\nhas long been acknowledged and while every science contains an artistic part,\nevery art form also needs a bit of science. Among all scientific disciplines,\nprogramming holds a special place for two reasons. First, the artistic part is\nnot only undeniable but also essential. Second, and much like in a purely\nartistic discipline, the act of programming is driven partly by the notion of\naesthetics: the pleasure we have in creating beautiful things. Even though the\nimportance of aesthetics in the act of programming is now unquestioned, more\ncould still be written on the subject. The field called \"psychology of\nprogramming\" focuses on the cognitive aspects of the activity, with the goal of\nimproving the productivity of programmers. While many scientists have\nemphasized their concern for aesthetics and the impact it has on their\nactivity, few computer scientists have actually written about their thought\nprocess while programming. What makes us like or dislike such and such language\nor paradigm? Why do we shape our programs the way we do? By answering these\nquestions from the angle of aesthetics, we may be able to shed some new light\non the art of programming. Starting from the assumption that aesthetics is an\ninherently transversal dimension, it should be possible for every programmer to\nfind the same aesthetic driving force in every creative activity they\nundertake, not just programming, and in doing so, get deeper insight on why and\nhow they do things the way they do. On the other hand, because our aesthetic\nsensitivities are so personal, all we can really do is relate our own\nexperiences and share it with others, in the hope that it will inspire them to\ndo the same. My personal life has been revolving around three major creative\nactivities, of equal importance: programming in Lisp, playing Jazz music, and\npracticing Aikido. But why so many of them, why so different ones, and why\nthese specifically? By introspecting my personal aesthetic sensitivities, I\neventually realized that my tastes in the scientific, artistic, and physical\ndomains are all motivated by the same driving forces, hence unifying Lisp,\nJazz, and Aikido as three expressions of a single essence, not so different\nafter all. Lisp, Jazz, and Aikido are governed by a limited set of rules which\nremain simple and unobtrusive. Conforming to them is a pleasure. Because Lisp,\nJazz, and Aikido are inherently introspective disciplines, they also invite you\nto transgress the rules in order to find your own. Breaking the rules is fun.\nFinally, if Lisp, Jazz, and Aikido unify so many paradigms, styles, or\ntechniques, it is not by mere accumulation but because they live at the\nmeta-level and let you reinvent them. Working at the meta-level is an\nenlightening experience. Understand your aesthetic sensitivities and you may\ngain considerable insight on your own psychology of programming. Mine is\nperhaps common to most lispers. Perhaps also common to other programming\ncommunities, but that, is for the reader to decide...\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 17:56:08 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Verna", "Didier", "", "EPITA Research and Development Laboratory"]]}, {"id": "1804.00489", "submitter": "Marco Patrignani", "authors": "Marco Patrignani, Deepak Garg", "title": "Robustly Safe Compilation or, Efficient, Provably Secure Compilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secure compilers generate compiled code that withstands many target-level\nattacks such as alteration of control flow, data leaks or memory corruption.\nMany existing secure compilers are proven to be fully abstract, meaning that\nthey reflect and preserve observational equivalence. Fully abstract compilation\nis a strong and useful property that, in certain cases, comes at the cost of\nrequiring expensive runtime constructs in compiled code. These constructs may\nhave no relevance for security, but are needed to accommodate differences\nbetween the source language and the target language that fully abstract\ncompilation necessarily regards. As an alternative to fully abstract\ncompilation, this paper explores a different criterion for secure compilation\ncalled robustly safe compilation or RSC. Briefly, this criterion means that the\ncompiled code preserves relevant safety properties of the source program\nagainst all adversarial contexts interacting with said program. We show that\nRSC can be attained easily and results in code that is more efficient than that\ngenerated by fully abstract compilers. We also develop three illustrative\nrobustly-safe compilers and, through them, develop two different proof\ntechniques for establishing that a compiler attains RSC. Through these, we also\nestablish that proving RSC is simpler than proving fully abstraction.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 13:42:06 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 09:21:59 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 19:21:39 GMT"}, {"version": "v4", "created": "Mon, 16 Dec 2019 20:17:09 GMT"}, {"version": "v5", "created": "Fri, 27 Nov 2020 14:26:49 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Patrignani", "Marco", ""], ["Garg", "Deepak", ""]]}, {"id": "1804.00746", "submitter": "Conal Elliott", "authors": "Conal Elliott", "title": "The simple essence of automatic differentiation", "comments": "37 pages with proof appendices and 15 figures. Extended version of a\n  paper appearing at ICFP 2018. More info at\n  http://conal.net/papers/essence-of-ad/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic differentiation (AD) in reverse mode (RAD) is a central component\nof deep learning and other uses of large-scale optimization. Commonly used RAD\nalgorithms such as backpropagation, however, are complex and stateful,\nhindering deep understanding, improvement, and parallel execution. This paper\ndevelops a simple, generalized AD algorithm calculated from a simple, natural\nspecification. The general algorithm is then specialized by varying the\nrepresentation of derivatives. In particular, applying well-known constructions\nto a naive representation yields two RAD algorithms that are far simpler than\npreviously known. In contrast to commonly used RAD implementations, the\nalgorithms defined here involve no graphs, tapes, variables, partial\nderivatives, or mutation. They are inherently parallel-friendly, correct by\nconstruction, and usable directly from an existing programming language with no\nneed for new data types or programming style, thanks to use of an AD-agnostic\ncompiler plugin.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 22:03:52 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 15:05:32 GMT"}, {"version": "v3", "created": "Thu, 12 Jul 2018 06:52:41 GMT"}, {"version": "v4", "created": "Tue, 2 Oct 2018 16:44:30 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Elliott", "Conal", ""]]}, {"id": "1804.00987", "submitter": "Kyle Richardson", "authors": "Kyle Richardson", "title": "A Language for Function Signature Representations", "comments": "short note", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work by (Richardson and Kuhn, 2017a,b; Richardson et al., 2018) looks\nat semantic parser induction and question answering in the domain of source\ncode libraries and APIs. In this brief note, we formalize the representations\nbeing learned in these studies and introduce a simple domain specific language\nand a systematic translation from this language to first-order logic. By\nrecasting the target representations in terms of classical logic, we aim to\nbroaden the applicability of existing code datasets for investigating more\ncomplex natural language understanding and reasoning problems in the software\ndomain.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 13:01:29 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 13:23:03 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Richardson", "Kyle", ""]]}, {"id": "1804.01186", "submitter": "Oleksandr Polozov", "authors": "Ashwin Kalyan, Abhishek Mohta, Oleksandr Polozov, Dhruv Batra, Prateek\n  Jain, Sumit Gulwani", "title": "Neural-Guided Deductive Search for Real-Time Program Synthesis from\n  Examples", "comments": "Published in ICLR 2018, International Conference on Learning\n  Representations (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing user-intended programs from a small number of input-output\nexamples is a challenging problem with several important applications like\nspreadsheet manipulation, data wrangling and code refactoring. Existing\nsynthesis systems either completely rely on deductive logic techniques that are\nextensively hand-engineered or on purely statistical models that need massive\namounts of data, and in general fail to provide real-time synthesis on\nchallenging benchmarks. In this work, we propose Neural Guided Deductive Search\n(NGDS), a hybrid synthesis technique that combines the best of both symbolic\nlogic techniques and statistical models. Thus, it produces programs that\nsatisfy the provided specifications by construction and generalize well on\nunseen examples, similar to data-driven systems. Our technique effectively\nutilizes the deductive search framework to reduce the learning problem of the\nneural component to a simple supervised learning setup. Further, this allows us\nto both train on sparingly available real-world data and still leverage\npowerful recurrent neural network encoders. We demonstrate the effectiveness of\nour method by evaluating on real-world customer scenarios by synthesizing\naccurate programs with up to 12x speed-up compared to state-of-the-art systems.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 22:37:08 GMT"}, {"version": "v2", "created": "Sun, 9 Sep 2018 21:32:49 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Kalyan", "Ashwin", ""], ["Mohta", "Abhishek", ""], ["Polozov", "Oleksandr", ""], ["Batra", "Dhruv", ""], ["Jain", "Prateek", ""], ["Gulwani", "Sumit", ""]]}, {"id": "1804.01295", "submitter": "Jiao Jiao", "authors": "Jiao Jiao, Shuanglong Kan, Shang-Wei Lin, David Sanan, Yang Liu and\n  Jun Sun", "title": "Executable Operational Semantics of Solidity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin has attracted everyone's attention and interest recently. Ethereum\n(ETH), a second generation cryptocurrency, extends Bitcoin's design by offering\na Turing-complete programming language called Solidity to develop smart\ncontracts. Smart contracts allow creditable execution of contracts on EVM\n(Ethereum Virtual Machine) without third parties. Developing correct smart\ncontracts is challenging due to its decentralized computation nature. Buggy\nsmart contracts may lead to huge financial loss. Furthermore, smart contracts\nare very hard, if not impossible, to patch once they are deployed. Thus, there\nis a recent surge of interest on analyzing/verifying smart contracts. While\nexisting work focuses on EVM opcode, we argue that it is equally important to\nunderstand and define the semantics of Solidity since programmers program and\nreason about smart contracts at the level of source code. In this work, we\ndevelop the structural operational semantics for Solidity, which allows us to\nidentify multiple design issues which underlines many problematic smart\ncontracts. Furthermore, our semantics is executable in the K framework, which\nallows us to verify/falsify contracts automatically.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 08:37:05 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Jiao", "Jiao", ""], ["Kan", "Shuanglong", ""], ["Lin", "Shang-Wei", ""], ["Sanan", "David", ""], ["Liu", "Yang", ""], ["Sun", "Jun", ""]]}, {"id": "1804.01468", "submitter": "Ali Kheradmand", "authors": "Ali Kheradmand and Grigore Rosu", "title": "P4K: A Formal Semantics of P4 and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programmable packet processors and P4 as a programming language for such\ndevices have gained significant interest, because their flexibility enables\nrapid development of a diverse set of applications that work at line rate.\nHowever, this flexibility, combined with the complexity of devices and\nnetworks, increases the chance of introducing subtle bugs that are hard to\ndiscover manually. Worse, this is a domain where bugs can have catastrophic\nconsequences, yet formal analysis tools for P4 programs / networks are missing.\n  We argue that formal analysis tools must be based on a formal semantics of\nthe target language, rather than on its informal specification. To this end, we\nprovide an executable formal semantics of the P4 language in the K framework.\nBased on this semantics, K provides an interpreter and various analysis tools\nincluding a symbolic model checker and a deductive program verifier for P4.\n  This paper overviews our formal K semantics of P4, as well as several P4\nlanguage design issues that we found during our formalization process. We also\ndiscuss some applications resulting from the tools provided by K for P4\nprogrammers and network administrators as well as language designers and\ncompiler developers, such as detection of unportable code, state space\nexploration of P4 programs and of networks, bug finding using symbolic\nexecution, data plane verification, program verification, and translation\nvalidation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 15:29:31 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Kheradmand", "Ali", ""], ["Rosu", "Grigore", ""]]}, {"id": "1804.01836", "submitter": "Nikos Tzevelekos", "authors": "Yu-Yang Lin and Nikos Tzevelekos", "title": "Higher-Order Bounded Model Checking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bounded Model Checking technique for higher-order programs. The\nvehicle of our study is a higher-order calculus with general references. Our\ntechnique is a symbolic state syntactical translation based on SMT solvers,\nadapted to a setting where the values passed and stored during computation can\nbe functions of arbitrary order. We prove that our algorithm is sound, and\ndevise an optimisation based on points-to analysis to improve scalability. We\nmoreover provide a prototype implementation of the algorithm with experimental\nresults showcasing its performance.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 13:21:10 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Lin", "Yu-Yang", ""], ["Tzevelekos", "Nikos", ""]]}, {"id": "1804.01839", "submitter": "Isabel Garcia-Contreras", "authors": "Isabel Garcia-Contreras, Jose F. Morales, Manuel V. Hermenegildo", "title": "Incremental and Modular Context-sensitive Analysis", "comments": "56 pages, 27 figures. To be published in Theory and Practice of Logic\n  Programming. v3 corresponds to the extended version of the ICLP2018 Technical\n  Communication. v4 is the revised version submitted to Theory and Practice of\n  Logic Programming. v5 (this one) is the final author version to be published\n  in TPLP", "journal-ref": "Theory and Practice of Logic Programming 21 (2021) 196-243", "doi": "10.1017/S1471068420000496", "report-no": "CLIP-2/2018 version 4 (July 2019)", "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context-sensitive global analysis of large code bases can be expensive, which\ncan make its use impractical during software development. However, there are\nmany situations in which modifications are small and isolated within a few\ncomponents, and it is desirable to reuse as much as possible previous analysis\nresults. This has been achieved to date through incremental global analysis\nfixpoint algorithms that achieve cost reductions at fine levels of granularity,\nsuch as changes in program lines. However, these fine-grained techniques are\nnot directly applicable to modular programs, nor are they designed to take\nadvantage of modular structures. This paper describes, implements, and\nevaluates an algorithm that performs efficient context-sensitive analysis\nincrementally on modular partitions of programs. The experimental results show\nthat the proposed modular algorithm shows significant improvements, in both\ntime and memory consumption, when compared to existing non-modular, fine-grain\nincremental analysis techniques. Furthermore, thanks to the proposed\ninter-modular propagation of analysis information, our algorithm also\noutperforms traditional modular analysis even when analyzing from scratch.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 13:28:10 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 08:58:56 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2018 15:26:49 GMT"}, {"version": "v4", "created": "Sat, 20 Jul 2019 22:56:07 GMT"}, {"version": "v5", "created": "Fri, 18 Dec 2020 17:30:33 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Garcia-Contreras", "Isabel", ""], ["Morales", "Jose F.", ""], ["Hermenegildo", "Manuel V.", ""]]}, {"id": "1804.02074", "submitter": "Mat\\'u\\v{s} Sul\\'ir", "authors": "Mat\\'u\\v{s} Sul\\'ir and Michaela Ba\\v{c}\\'ikov\\'a and Sergej Chodarev\n  and Jaroslav Porub\\\"an", "title": "Visual augmentation of source code editors: A systematic mapping study", "comments": null, "journal-ref": null, "doi": "10.1016/j.jvlc.2018.10.001", "report-no": "Journal of Visual Languages and Computing (JVLC), Vol. 49, 2018, pp.\n  46-59, Elsevier", "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source code written in textual programming languages is typically edited in\nintegrated development environments or specialized code editors. These tools\noften display various visual items, such as icons, color highlights or more\nadvanced graphical overlays directly in the main editable source code view. We\ncall such visualizations source code editor augmentation.\n  In this paper, we present a first systematic mapping study of source code\neditor augmentation tools and approaches. We manually reviewed the metadata of\n5,553 articles published during the last twenty years in two phases -- keyword\nsearch and references search. The result is a list of 103 relevant articles and\na taxonomy of source code editor augmentation tools with seven dimensions,\nwhich we used to categorize the resulting list of the surveyed articles.\n  We also provide the definition of the term source code editor augmentation,\nalong with a brief overview of historical development and augmentations\navailable in current industrial IDEs.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 22:34:32 GMT"}, {"version": "v2", "created": "Sat, 10 Nov 2018 21:25:54 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Sul\u00edr", "Mat\u00fa\u0161", ""], ["Ba\u010d\u00edkov\u00e1", "Michaela", ""], ["Chodarev", "Sergej", ""], ["Porub\u00e4n", "Jaroslav", ""]]}, {"id": "1804.02380", "submitter": "Nataliia Stulova", "authors": "Maximiliano Klemen, Nataliia Stulova, Pedro Lopez-Garcia, Jos\\'e F.\n  Morales, Manuel V. Hermenegildo", "title": "An Approach to Static Performance Guarantees for Programs with Run-time\n  Checks", "comments": "15 pages, 3 tables; submitted to ICLP'18, accepted as technical\n  communication", "journal-ref": null, "doi": null, "report-no": "CLIP-1/2018.0", "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumenting programs for performing run-time checking of properties, such\nas regular shapes, is a common and useful technique that helps programmers\ndetect incorrect program behaviors. This is specially true in dynamic languages\nsuch as Prolog. However, such run-time checks inevitably introduce run-time\noverhead (in execution time, memory, energy, etc.). Several approaches have\nbeen proposed for reducing such overhead, such as eliminating the checks that\ncan statically be proved to always succeed, and/or optimizing the way in which\nthe (remaining) checks are performed. However, there are cases in which it is\nnot possible to remove all checks statically (e.g., open libraries which must\ncheck their interfaces, complex properties, unknown code, etc.) and in which,\neven after optimizations, these remaining checks still may introduce an\nunacceptable level of overhead. It is thus important for programmers to be able\nto determine the additional cost due to the run-time checks and compare it to\nsome notion of admissible cost. The common practice used for estimating\nrun-time checking overhead is profiling, which is not exhaustive by nature.\nInstead, we propose a method that uses static analysis to estimate such\noverhead, with the advantage that the estimations are functions parameterized\nby input data sizes. Unlike profiling, this approach can provide guarantees for\nall possible execution traces, and allows assessing how the overhead grows as\nthe size of the input grows. Our method also extends an existing assertion\nverification framework to express \"admissible\" overheads, and statically and\nautomatically checks whether the instrumented program conforms with such\nspecifications. Finally, we present an experimental evaluation of our approach\nthat suggests that our method is feasible and promising.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 17:53:29 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Klemen", "Maximiliano", ""], ["Stulova", "Nataliia", ""], ["Lopez-Garcia", "Pedro", ""], ["Morales", "Jos\u00e9 F.", ""], ["Hermenegildo", "Manuel V.", ""]]}, {"id": "1804.02452", "submitter": "Roberto Casta\\~neda Lozano", "authors": "Roberto Casta\\~neda Lozano, Mats Carlsson, Gabriel Hjort Blindell, and\n  Christian Schulte", "title": "Combinatorial Register Allocation and Instruction Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a combinatorial optimization approach to register\nallocation and instruction scheduling, two central compiler problems.\nCombinatorial optimization has the potential to solve these problems optimally\nand to exploit processor-specific features readily. Our approach is the first\nto leverage this potential in practice: it captures the complete set of program\ntransformations used in state-of-the-art compilers, scales to medium-sized\nfunctions of up to 1000 instructions, and generates executable code. This level\nof practicality is reached by using constraint programming, a particularly\nsuitable combinatorial optimization technique. Unison, the implementation of\nour approach, is open source, used in industry, and integrated with the LLVM\ntoolchain.\n  An extensive evaluation confirms that Unison generates better code than LLVM\nwhile scaling to medium-sized functions. The evaluation uses systematically\nselected benchmarks from MediaBench and SPEC CPU2006 and different processor\narchitectures (Hexagon, ARM, MIPS). Mean estimated speedup ranges from 1.1% to\n10% and mean code size reduction ranges from 1.3% to 3.8% for the different\narchitectures. A significant part of this improvement is due to the integrated\nnature of the approach. Executing the generated code on Hexagon confirms that\nthe estimated speedup results in actual speedup. Given a fixed time limit,\nUnison solves optimally functions of up to 946 instructions, nearly an order of\nmagnitude larger than previous approaches.\n  The results show that our combinatorial approach can be applied in practice\nto trade compilation time for code quality beyond the usual compiler\noptimization levels, identify improvement opportunities in heuristic\nalgorithms, and fully exploit processor-specific features.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 20:47:21 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 12:54:27 GMT"}, {"version": "v3", "created": "Wed, 30 Jan 2019 23:34:51 GMT"}, {"version": "v4", "created": "Thu, 18 Apr 2019 12:11:18 GMT"}, {"version": "v5", "created": "Thu, 20 Jun 2019 10:58:16 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Lozano", "Roberto Casta\u00f1eda", ""], ["Carlsson", "Mats", ""], ["Blindell", "Gabriel Hjort", ""], ["Schulte", "Christian", ""]]}, {"id": "1804.02477", "submitter": "Abhinav Verma", "authors": "Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli,\n  Swarat Chaudhuri", "title": "Programmatically Interpretable Reinforcement Learning", "comments": "Published at The 35th International Conference on Machine Learning\n  (ICML 2018)", "journal-ref": "PMLR 80:5045-5054", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a reinforcement learning framework, called Programmatically\nInterpretable Reinforcement Learning (PIRL), that is designed to generate\ninterpretable and verifiable agent policies. Unlike the popular Deep\nReinforcement Learning (DRL) paradigm, which represents policies by neural\nnetworks, PIRL represents policies using a high-level, domain-specific\nprogramming language. Such programmatic policies have the benefits of being\nmore easily interpreted than neural networks, and being amenable to\nverification by symbolic methods. We propose a new method, called Neurally\nDirected Program Search (NDPS), for solving the challenging nonsmooth\noptimization problem of finding a programmatic policy with maximal reward. NDPS\nworks by first learning a neural policy network using DRL, and then performing\na local search over programmatic policies that seeks to minimize a distance\nfrom this neural \"oracle\". We evaluate NDPS on the task of learning to drive a\nsimulated car in the TORCS car-racing environment. We demonstrate that NDPS is\nable to discover human-readable policies that pass some significant performance\nbars. We also show that PIRL policies can have smoother trajectories, and can\nbe more easily transferred to environments not encountered during training,\nthan corresponding policies discovered by DRL.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 22:17:18 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 02:27:26 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 09:09:46 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Verma", "Abhinav", ""], ["Murali", "Vijayaraghavan", ""], ["Singh", "Rishabh", ""], ["Kohli", "Pushmeet", ""], ["Chaudhuri", "Swarat", ""]]}, {"id": "1804.02503", "submitter": "Kostas Ferles", "authors": "Kostas Ferles, Jacob Van Geffen, Isil Dillig, Yannis Smaragdakis", "title": "Symbolic Reasoning for Automatic Signal Placement (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explicit signaling between threads is a perennial cause of bugs in concurrent\nprograms. While there are several run-time techniques to automatically notify\nthreads upon the availability of some shared resource, such techniques are not\nwidely-adopted due to their run-time overhead. This paper proposes a new\nsolution based on static analysis for automatically generating a performant\nexplicit-signal program from its corresponding implicit-signal implementation.\nThe key idea is to generate verification conditions that allow us to minimize\nthe number of required signals and unnecessary context switches, while\nguaranteeing semantic equivalence between the source and target programs. We\nhave implemented our method in a tool called Expresso and evaluate it on\nchallenging benchmarks from prior papers and open-source software.\nExpresso-generated code significantly outperforms past automatic signaling\nmechanisms (avg. 1.56x speedup) and closely matches the performance of\nhand-optimized explicit-signal code.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 03:52:01 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Ferles", "Kostas", ""], ["Van Geffen", "Jacob", ""], ["Dillig", "Isil", ""], ["Smaragdakis", "Yannis", ""]]}, {"id": "1804.03140", "submitter": "Satoshi Egi", "authors": "Satoshi Egi", "title": "Symbolical Index Reduction and Completion Rules for Importing Tensor\n  Index Notation into Programming Languages", "comments": "13 pages. arXiv admin note: substantial text overlap with\n  arXiv:1702.06343", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mathematics, many notations have been invented for the concise\nrepresentation of mathematical formulae. Tensor index notation is one of such\nnotations and has been playing a crucial role in describing formulae in\nmathematical physics. This paper shows a programming language that can deal\nwith symbolical tensor indices by introducing a set of tensor index rules that\nis compatible with two types of parameters, i.e., scalar and tensor parameters.\nWhen a tensor parameter obtains a tensor as an argument, the function treats\nthe tensor argument as a whole. In contrast, when a scalar parameter obtains a\ntensor as an argument, the function is applied to each component of the tensor.\nOn a language with scalar and tensor parameters, we can design a set of index\nreduction rules that allows users to use tensor index notation for arbitrary\nuser-defined functions without requiring additional description. Furthermore,\nwe can also design index completion rules that allow users to define the\noperators concisely for differential forms such as the wedge product, exterior\nderivative, and Hodge star operator. In our proposal, all these tensor\noperators are user-defined functions and can be passed as arguments of\nhigh-order functions.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 21:42:01 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 01:31:03 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Egi", "Satoshi", ""]]}, {"id": "1804.03460", "submitter": "Dylan McDermott", "authors": "Ohad Kammar, Dylan McDermott", "title": "Factorisation systems for logical relations and monadic lifting in\n  type-and-effect system semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Type-and-effect systems incorporate information about the computational\neffects, e.g., state mutation, probabilistic choice, or I/O, a program phrase\nmay invoke alongside its return value. A semantics for type-and-effect systems\ninvolves a parameterised family of monads whose size is exponential in the\nnumber of effects. We derive such refined semantics from a single monad over a\ncategory, a choice of algebraic operations for this monad, and a suitable\nfactorisation system over this category. We relate the derived semantics to the\noriginal semantics using fibrations for logical relations. Our proof uses a\nfolklore technique for lifting monads with operations.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 11:25:06 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Kammar", "Ohad", ""], ["McDermott", "Dylan", ""]]}, {"id": "1804.03523", "submitter": "Bradley Gram-Hansen", "authors": "Bradley Gram-Hansen, Yuan Zhou, Tobias Kohn, Tom Rainforth, Hongseok\n  Yang, Frank Wood", "title": "Hamiltonian Monte Carlo for Probabilistic Programs with Discontinuities", "comments": "4 pages, 2 figures", "journal-ref": "Inaugural Conference on Probabilistic Programming, 2018", "doi": null, "report-no": null, "categories": "stat.CO cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) is arguably the dominant statistical inference\nalgorithm used in most popular \"first-order differentiable\" Probabilistic\nProgramming Languages (PPLs). However, the fact that HMC uses derivative\ninformation causes complications when the target distribution is\nnon-differentiable with respect to one or more of the latent variables. In this\npaper, we show how to use extensions to HMC to perform inference in\nprobabilistic programs that contain discontinuities. To do this, we design a\nSimple first-order Probabilistic Programming Language (SPPL) that contains a\nsufficient set of language restrictions together with a compilation scheme.\nThis enables us to preserve both the statistical and syntactic interpretation\nof if-else statements in the probabilistic program, within the scope of\nfirst-order PPLs. We also provide a corresponding mathematical formalism that\nensures any joint density denoted in such a language has a suitably low measure\nof discontinuities.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 09:22:56 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 16:32:10 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Gram-Hansen", "Bradley", ""], ["Zhou", "Yuan", ""], ["Kohn", "Tobias", ""], ["Rainforth", "Tom", ""], ["Yang", "Hongseok", ""], ["Wood", "Frank", ""]]}, {"id": "1804.04052", "submitter": "Justin Hsu", "authors": "Aws Albarghouthi and Justin Hsu", "title": "Constraint-Based Synthesis of Coupling Proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proof by coupling is a classical technique for proving properties about pairs\nof randomized algorithms by carefully relating (or coupling) two probabilistic\nexecutions. In this paper, we show how to automatically construct such proofs\nfor probabilistic programs. First, we present f-coupled postconditions, an\nabstraction describing two correlated program executions. Second, we show how\nproperties of f-coupled postconditions can imply various probabilistic\nproperties of the original programs. Third, we demonstrate how to reduce the\nproof-search problem to a purely logical synthesis problem of the form $\\exists\nf\\ldotp \\forall X\\ldotp \\phi$, making probabilistic reasoning unnecessary. We\ndevelop a prototype implementation to automatically build coupling proofs for\nprobabilistic properties, including uniformity and independence of program\nexpressions.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 15:34:42 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Albarghouthi", "Aws", ""], ["Hsu", "Justin", ""]]}, {"id": "1804.04091", "submitter": "J\\'er\\^ome Dohrau", "authors": "J\\'er\\^ome Dohrau, Alexander J. Summers, Caterina Urban, Severin\n  M\\\"unger, and Peter M\\\"uller", "title": "Permission Inference for Array Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information about the memory locations accessed by a program is, for\ninstance, required for program parallelisation and program verification.\nExisting inference techniques for this information provide only partial\nsolutions for the important class of array-manipulating programs. In this\npaper, we present a static analysis that infers the memory footprint of an\narray program in terms of permission pre- and postconditions as used, for\nexample, in separation logic. This formulation allows our analysis to handle\nconcurrent programs and produces specifications that can be used by\nverification tools. Our analysis expresses the permissions required by a loop\nvia maximum expressions over the individual loop iterations. These maximum\nexpressions are then solved by a novel maximum elimination algorithm, in the\nspirit of quantifier elimination. Our approach is sound and is implemented; an\nevaluation on existing benchmarks for memory safety of array programs\ndemonstrates accurate results, even for programs with complex access patterns\nand nested loops.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 16:54:52 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Dohrau", "J\u00e9r\u00f4me", ""], ["Summers", "Alexander J.", ""], ["Urban", "Caterina", ""], ["M\u00fcnger", "Severin", ""], ["M\u00fcller", "Peter", ""]]}, {"id": "1804.04152", "submitter": "Xinyu Wang", "authors": "Xinyu Wang, Greg Anderson, Isil Dillig, K. L. McMillan", "title": "Learning Abstractions for Program Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many example-guided program synthesis techniques use abstractions to prune\nthe search space. While abstraction-based synthesis has proven to be very\npowerful, a domain expert needs to provide a suitable abstract domain, together\nwith the abstract transformers of each DSL construct. However, coming up with\nuseful abstractions can be non-trivial, as it requires both domain expertise\nand knowledge about the synthesizer. In this paper, we propose a new technique\nfor learning abstractions that are useful for instantiating a general synthesis\nframework in a new domain. Given a DSL and a small set of training problems,\nour method uses tree interpolation to infer reusable predicate templates that\nspeed up synthesis in a given domain. Our method also learns suitable abstract\ntransformers by solving a certain kind of second-order constraint solving\nproblem in a data-driven way. We have implemented the proposed method in a tool\ncalled ATLAS and evaluate it in the context of the BLAZE meta-synthesizer. Our\nevaluation shows that (a) ATLAS can learn useful abstract domains and\ntransformers from few training problems, and (b) the abstractions learned by\nATLAS allow BLAZE to achieve significantly better results compared to\nmanually-crafted abstractions.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 18:14:51 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Wang", "Xinyu", ""], ["Anderson", "Greg", ""], ["Dillig", "Isil", ""], ["McMillan", "K. L.", ""]]}, {"id": "1804.04214", "submitter": "Ryan Kavanagh", "authors": "Ryan Kavanagh and Stephen Brookes", "title": "A denotational account of C11-style memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a denotational semantic framework for shared-memory concurrent\nprograms in a C11-style memory model. This denotational approach is an\nalternative to techniques based on \"execution graphs\" and axiomatizations, and\nit allows for compositional reasoning. Our semantics generalizes from traces\n(sequences of actions) to pomsets (partial orders of actions): instead of\ntraces and interleaving, we embrace \"true\" concurrency. We build on techniques\nfrom our prior work that gives a denotational semantics to SPARC TSO. We add\nsupport for C11's wider range of memory orderings, e.g., acquire-release and\nrelaxed, and support for local variables and various synchronization\nprimitives, while eliminating significant amounts of technical bookkeeping. Our\napproach features two main components. We first give programs a syntax-directed\ndenotation in terms of sets of pomsets of memory actions. We then give a\nrace-detecting executional interpretation of pomsets using footprints and a\nlocal view of state.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 20:44:25 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Kavanagh", "Ryan", ""], ["Brookes", "Stephen", ""]]}, {"id": "1804.04766", "submitter": "Peizun Liu", "authors": "Peizun Liu and Thomas Wahl", "title": "CUBA: Interprocedural Context-UnBounded Analysis of Concurrent Programs\n  (Extended Manuscript)", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classical result by Ramalingam about synchronization-sensitive\ninterprocedural program analysis implies that reachability for concurrent\nthreads running recursive procedures is undecidable. A technique proposed by\nQadeer and Rehof, to bound the number of context switches allowed between the\nthreads, leads to an incomplete solution that is, however, believed to catch\n\"most bugs\" in practice. The question whether the technique can also prove the\nabsence of bugs at least in some cases has remained largely open.\n  In this paper we introduce a broad verification methodology for\nresource-parameterized programs that observes how changes to the resource\nparameter affect the behavior of the program. Applied to the context-unbounded\nanalysis problem (CUBA), the methodology results in partial verification\ntechniques for procedural concurrent programs. Our solutions may not terminate,\nbut are able to both refute and prove context-unbounded safety for concurrent\nrecursive threads. We demonstrate the effectiveness of our method using a\nvariety of examples, the safe of which cannot be proved safe by earlier,\ncontext-bounded methods.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 01:57:35 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Liu", "Peizun", ""], ["Wahl", "Thomas", ""]]}, {"id": "1804.04812", "submitter": "Gregory Duck", "authors": "Gregory J. Duck and Roland H. C. Yap", "title": "An Extended Low Fat Allocator API and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary function of memory allocators is to allocate and deallocate\nchunks of memory primarily through the malloc API. Many memory allocators also\nimplement other API extensions, such as deriving the size of an allocated\nobject from the object's pointer, or calculating the base address of an\nallocation from an interior pointer. In this paper, we propose a general\npurpose extended allocator API built around these common extensions. We argue\nthat such extended APIs have many applications and demonstrate several use\ncases, such as (manual) memory error detection, meta data storage, typed\npointers and compact data-structures. Because most existing allocators were not\ndesigned for the extended API, traditional implementations are expensive or not\npossible.\n  Recently, the LowFat allocator for heap and stack objects has been developed.\nThe LowFat allocator is an implementation of the idea of low-fat pointers,\nwhere object bounds information (size and base) are encoded into the native\nmachine pointer representation itself. The \"killer app\" for low-fat pointers is\nautomated bounds check instrumentation for program hardening and bug detection.\nHowever, the LowFat allocator can also be used to implement highly optimized\nversion of the extended allocator API, which makes the new applications (listed\nabove) possible. In this paper, we implement and evaluate several applications\nbased efficient memory allocator API extensions using low-fat pointers. We also\nextend the LowFat allocator to cover global objects for the first time.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 07:38:35 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Duck", "Gregory J.", ""], ["Yap", "Roland H. C.", ""]]}, {"id": "1804.05097", "submitter": "Martin Holm Cservenka M.Sc.", "authors": "Martin Holm Cservenka", "title": "Design and Implementation of Dynamic Memory Management in a Reversible\n  Object-Oriented Programming Language", "comments": "Master's Thesis, 231 pages, 63 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reversible object-oriented programming language (ROOPL) was presented in\nlate 2016 and proved that object-oriented programming paradigms works in the\nreversible setting. The language featured simple statically scoped objects\nwhich made non-trivial programs tedious, if not impossible to write using the\nlimited tools provided. We introduce an extension to ROOPL in form the new\nlanguage ROOPL++, featuring dynamic memory management and fixed-sized arrays\nfor increased language expressiveness. The language is a superset of ROOPL and\nhas formally been defined by its language semantics, type system and\ncomputational universality. Considerations for reversible memory manager\nlayouts are discussed and ultimately lead to the selection of the Buddy Memory\nlayout. Translations of the extensions added in ROOPL++ to the reversible\nassembly language PISA are presented to provide garbage-free computations. The\ndynamic memory management extension successfully increases the expressiveness\nof ROOPL and as a result, shows that non-trivial reversible data structures,\nsuch as binary trees and doubly-linked lists, are feasible and do not\ncontradict the reversible computing paradigm.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 00:23:21 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Cservenka", "Martin Holm", ""]]}, {"id": "1804.05655", "submitter": "Ishan Rastogi", "authors": "Ishan Rastogi, Aditya Kanade and Shirish Shevade", "title": "Active Learning for Efficient Testing of Student Programs", "comments": "14 pages, 6 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an automated method to identify semantic bugs in\nstudent programs, called ATAS, which builds upon the recent advances in both\nsymbolic execution and active learning. Symbolic execution is a program\nanalysis technique which can generate test cases through symbolic constraint\nsolving. Our method makes use of a reference implementation of the task as its\nsole input. We compare our method with a symbolic execution-based baseline on 6\nprogramming tasks retrieved from CodeForces comprising a total of 23K student\nsubmissions. We show an average improvement of over 2.5x over the baseline in\nterms of runtime (thus making it more suitable for online evaluation), without\na significant degradation in evaluation accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 06:53:00 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Rastogi", "Ishan", ""], ["Kanade", "Aditya", ""], ["Shevade", "Shirish", ""]]}, {"id": "1804.05880", "submitter": "Andr\\'e Platzer", "authors": "Andr\\'e Platzer", "title": "Uniform Substitution for Differential Game Logic", "comments": null, "journal-ref": "Automated Reasoning, 9th International Joint Conference, IJCAR\n  2018", "doi": "10.1007/978-3-319-94205-6_15", "report-no": null, "categories": "cs.LO cs.GT cs.PL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a uniform substitution calculus for differential game\nlogic (dGL). Church's uniform substitutions substitute a term or formula for a\nfunction or predicate symbol everywhere. After generalizing them to\ndifferential game logic and allowing for the substitution of hybrid games for\ngame symbols, uniform substitutions make it possible to only use axioms instead\nof axiom schemata, thereby substantially simplifying implementations. Instead\nof subtle schema variables and soundness-critical side conditions on the\noccurrence patterns of logical variables to restrict infinitely many axiom\nschema instances to sound ones, the resulting axiomatization adopts only a\nfinite number of ordinary dGL formulas as axioms, which uniform substitutions\ninstantiate soundly. This paper proves soundness and completeness of uniform\nsubstitutions for the monotone modal logic dGL. The resulting axiomatization\nadmits a straightforward modular implementation of dGL in theorem provers.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 18:22:55 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Platzer", "Andr\u00e9", ""]]}, {"id": "1804.06013", "submitter": "Ankush Das", "authors": "Ankush Das, Jan Hoffmann and Frank Pfenning", "title": "Parallel Complexity Analysis with Temporal Session Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of parametric parallel complexity analysis of\nconcurrent, message-passing programs. To make the analysis local and\ncompositional, it is based on a conservative extension of binary session types,\nwhich structure the type and direction of communication between processes and\nstand in a Curry-Howard correspondence with intuitionistic linear logic. The\nmain innovation is to enrich session types with the temporal modalities next\n($\\bigcirc A$), always ($\\Box A$), and eventually ($\\Diamond A$), to\nadditionally prescribe the timing of the exchanged messages in a way that is\nprecise yet flexible. The resulting temporal session types uniformly express\nproperties such as the message rate of a stream, the latency of a pipeline, the\nresponse time of a concurrent queue, or the span of a fork/join parallel\nprogram. The analysis is parametric in the cost model and the presentation\nfocuses on communication cost as a concrete example. The soundness of the\nanalysis is established by proofs of progress and type preservation using a\ntimed multiset rewriting semantics. Representative examples illustrate the\nscope and usability of the approach.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 01:58:16 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Das", "Ankush", ""], ["Hoffmann", "Jan", ""], ["Pfenning", "Frank", ""]]}, {"id": "1804.06458", "submitter": "Martin Hirzel", "authors": "Guillaume Baudart, Martin Hirzel, Louis Mandel", "title": "Deep Probabilistic Programming Languages: A Qualitative Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep probabilistic programming languages try to combine the advantages of\ndeep learning with those of probabilistic programming languages. If successful,\nthis would be a big step forward in machine learning and programming languages.\nUnfortunately, as of now, this new crop of languages is hard to use and\nunderstand. This paper addresses this problem directly by explaining deep\nprobabilistic programming languages and indirectly by characterizing their\ncurrent strengths and weaknesses.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 20:03:25 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Baudart", "Guillaume", ""], ["Hirzel", "Martin", ""], ["Mandel", "Louis", ""]]}, {"id": "1804.06612", "submitter": "Constantin Enea", "authors": "Ahmed Bouajjani, Constantin Enea, Kailiang Ji, Shaz Qadeer", "title": "On the Completeness of Verifying Message Passing Programs under Bounded\n  Asynchrony", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of verifying message passing programs, defined as a\nset of parallel processes communicating through unbounded FIFO buffers. We\nintroduce a bounded analysis that explores a special type of computations,\ncalled k-synchronous. These computations can be viewed as (unbounded) sequences\nof interaction phases, each phase allowing at most k send actions (by different\nprocesses), followed by a sequence of receives corresponding to sends in the\nsame phase. We give a procedure for deciding k-synchronizability of a program,\ni.e., whether every computation is equivalent (has the same happens-before\nrelation) to one of its k-synchronous computations. We also show that\nreachability over k-synchronous computations and checking k-synchronizability\nare both PSPACE-complete. Furthermore, we introduce a class of programs called\n{\\em flow-bounded} for which the problem of deciding whether there exists a k>0\nfor which the program is k-synchronizable, is decidable.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 09:11:10 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Bouajjani", "Ahmed", ""], ["Enea", "Constantin", ""], ["Ji", "Kailiang", ""], ["Qadeer", "Shaz", ""]]}, {"id": "1804.07078", "submitter": "Cezara Dr\\u{a}goi", "authors": "Andrei Damien, Cezara Dragoi, Alexandru Militaru, and Josef Widder", "title": "Reducing asynchrony to synchronized rounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synchronous computation models simplify the design and the verification of\nfault-tolerant distributed systems. For efficiency reasons such systems are\ndesigned and implemented using an asynchronous semantics. In this paper, we\nbridge the gap between these two worlds. We introduce a (synchronous)\nround-based computational model and we prove a reduction for a class of\nasynchronous protocols to our new model. The reduction is based on properties\nof the code that can be checked with sequential methods. We apply the reduction\nto state machine replication systems, such as, Paxos, Zab, and Viewstamped\nReplication.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 10:50:11 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2019 13:42:46 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Damien", "Andrei", ""], ["Dragoi", "Cezara", ""], ["Militaru", "Alexandru", ""], ["Widder", "Josef", ""]]}, {"id": "1804.07133", "submitter": "Lukas Diekmann", "authors": "Lukas Diekmann and Laurence Tratt", "title": "Don't Panic! Better, Fewer, Syntax Errors for LR Parsers", "comments": "32 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Syntax errors are generally easy to fix for humans, but not for parsers in\ngeneral nor LR parsers in particular. Traditional 'panic mode' error recovery,\nthough easy to implement and applicable to any grammar, often leads to a\ncascading chain of errors that drown out the original. More advanced error\nrecovery techniques suffer less from this problem but have seen little\npractical use because their typical performance was seen as poor, their worst\ncase unbounded, and the repairs they reported arbitrary. In this paper we\nintroduce the CPCT+ algorithm, and an implementation of that algorithm, that\naddress these issues. First, CPCT+ reports the complete set of minimum cost\nrepair sequences for a given location, allowing programmers to select the one\nthat best fits their intention. Second, on a corpus of 200,000 real-world\nsyntactically invalid Java programs, CPCT+ is able to repair 98.37% of files\nwithin a timeout of 0.5s. Finally, CPCT+ uses the complete set of minimum cost\nrepair sequences to reduce the cascading error problem, where incorrect error\nrecovery causes further spurious syntax errors to be identified. Across the\ntest corpus, CPCT+ reports 435,812 error locations to the user, reducing the\ncascading error problem substantially relative to the 981,628 error locations\nreported by panic mode.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 13:11:57 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2019 17:29:45 GMT"}, {"version": "v3", "created": "Thu, 9 Jan 2020 11:43:10 GMT"}, {"version": "v4", "created": "Fri, 3 Jul 2020 12:34:46 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Diekmann", "Lukas", ""], ["Tratt", "Laurence", ""]]}, {"id": "1804.07271", "submitter": "Tony Clark", "authors": "Tony Clark", "title": "EBG: A Lazy Functional Programming Language Implemented on the Java\n  Virtual Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report describes the implementation of a lazy functional\nprogramming language on the Java VM.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 16:17:23 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Clark", "Tony", ""]]}, {"id": "1804.07608", "submitter": "Shuanglong Kan", "authors": "Shuanglong Kan, Zhe Chen, David Sanan, Shang-Wei Lin, Yang Liu", "title": "An Executable Operational Semantics for Rust with the Formalization of\n  Ownership and Borrowing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rust is an emergent systems programming language highlighting memory safety\nby its Ownership and Borrowing System (OBS). The existing formal semantics for\nRust only covers limited subsets of the major language features of Rust.\nMoreover, they formalize OBS as type systems at the language-level, which can\nonly be used to conservatively analyze programs against the OBS invariants at\ncompile-time. That is, they are not executable, and thus cannot be used for\nautomated verification of runtime behavior.\n  In this paper, we propose RustSEM, a new executable operational semantics for\nRust. RustSEM covers a much larger subset of the major language features than\nexisting semantics. Moreover, RustSEM provides an operational semantics for OBS\nat the memory-level, which can be used to verify the runtime behavior of Rust\nprograms against the OBS invariants. We have implemented RustSEM in the\nexecutable semantics modeling tool K-Framework. We have evaluated the semantics\ncorrectness of RustSEM wrt. the Rust compiler using around 700 tests. In\nparticular, we have proposed a new technique for testing semantic consistency\nto ensure the absence of semantic ambiguities on all possible execution\nselections. We have also evaluated the potential applications of RustSEM in\nautomated runtime and formal verification for both functional and memory\nproperties. Experimental results show that RustSEM can enhance the memory\nsafety mechanism of Rust, as it is more powerful than OBS in detecting memory\nerrors.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 02:01:43 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 20:20:58 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Kan", "Shuanglong", ""], ["Chen", "Zhe", ""], ["Sanan", "David", ""], ["Lin", "Shang-Wei", ""], ["Liu", "Yang", ""]]}, {"id": "1804.08091", "submitter": "Luca Di Stefano", "authors": "Rocco De Nicola, Luca Di Stefano, Omar Inverso", "title": "Towards formal models and languages for verifiable Multi-Robot Systems", "comments": "Changed formatting", "journal-ref": null, "doi": "10.3389/frobt.2018.00094", "report-no": null, "categories": "cs.PL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorrect operations of a Multi-Robot System (MRS) may not only lead to\nunsatisfactory results, but can also cause economic losses and threats to\nsafety. These threats may not always be apparent, since they may arise as\nunforeseen consequences of the interactions between elements of the system.\nThis call for tools and techniques that can help in providing guarantees about\nMRSs behaviour. We think that, whenever possible, these guarantees should be\nbacked up by formal proofs to complement traditional approaches based on\ntesting and simulation.\n  We believe that tailored linguistic support to specify MRSs is a major step\ntowards this goal. In particular, reducing the gap between typical features of\nan MRS and the level of abstraction of the linguistic primitives would simplify\nboth the specification of these systems and the verification of their\nproperties. In this work, we review different agent-oriented languages and\ntheir features; we then consider a selection of case studies of interest and\nimplement them useing the surveyed languages. We also evaluate and compare\neffectiveness of the proposed solution, considering, in particular, easiness of\nexpressing non-trivial behaviour.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 10:14:37 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 12:56:07 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["De Nicola", "Rocco", ""], ["Di Stefano", "Luca", ""], ["Inverso", "Omar", ""]]}, {"id": "1804.08335", "submitter": "Ioanna Symeonidou", "authors": "Angelos Charalambidis, Panos Rondogiannis and Ioanna Symeonidou", "title": "Approximation Fixpoint Theory and the Well-Founded Semantics of\n  Higher-Order Logic Programs", "comments": "Paper presented at the 34nd International Conference on Logic\n  Programming (ICLP 2018), Oxford, UK, July 14 to July 17, 2018 31 pages, LaTeX", "journal-ref": "Theory and Practice of Logic Programming 18 (2018) 421-437", "doi": "10.1017/S1471068418000108", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a novel, extensional, three-valued semantics for higher-order logic\nprograms with negation. The new semantics is based on interpreting the types of\nthe source language as three-valued Fitting-monotonic functions at all levels\nof the type hierarchy. We prove that there exists a bijection between such\nFitting-monotonic functions and pairs of two-valued-result functions where the\nfirst member of the pair is monotone-antimonotone and the second member is\nantimonotone-monotone. By deriving an extension of consistent approximation\nfixpoint theory (Denecker et al. 2004) and utilizing the above bijection, we\ndefine an iterative procedure that produces for any given higher-order logic\nprogram a distinguished extensional model. We demonstrate that this model is\nactually a minimal one. Moreover, we prove that our construction generalizes\nthe familiar well-founded semantics for classical logic programs, making in\nthis way our proposal an appealing formulation for capturing the well-founded\nsemantics for higher-order logic programs. This paper is under consideration\nfor acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 11:13:44 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Charalambidis", "Angelos", ""], ["Rondogiannis", "Panos", ""], ["Symeonidou", "Ioanna", ""]]}, {"id": "1804.08470", "submitter": "Sean Heelan", "authors": "Sean Heelan, Tom Melham, Daniel Kroening", "title": "Automatic Heap Layout Manipulation for Exploitation", "comments": null, "journal-ref": "USENIX Security Symposium 2018: 763-779", "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heap layout manipulation is integral to exploiting heap-based memory\ncorruption vulnerabilities. In this paper we present the first automatic\napproach to the problem, based on pseudo-random black-box search. Our approach\nsearches for the inputs required to place the source of a heap-based buffer\noverflow or underflow next to heap-allocated objects that an exploit developer,\nor automatic exploit generation system, wishes to read or corrupt. We present a\nframework for benchmarking heap layout manipulation algorithms, and use it to\nevaluate our approach on several real-world allocators, showing that\npseudo-random black box search can be highly effective. We then present SHRIKE,\na novel system that can perform automatic heap layout manipulation on the PHP\ninterpreter and can be used in the construction of control-flow hijacking\nexploits. Starting from PHP's regression tests, SHRIKE discovers fragments of\nPHP code that interact with the interpreter's heap in useful ways, such as\nmaking allocations and deallocations of particular sizes, or allocating objects\ncontaining sensitive data, such as pointers. SHRIKE then uses our search\nalgorithm to piece together these fragments into programs, searching for one\nthat achieves a desired heap layout. SHRIKE allows an exploit developer to\nfocus on the higher level concepts in an exploit, and to defer the resolution\nof heap layout constraints to SHRIKE. We demonstrate this by using SHRIKE in\nthe construction of a control-flow hijacking exploit for the PHP interpreter.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 14:32:45 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 20:19:28 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Heelan", "Sean", ""], ["Melham", "Tom", ""], ["Kroening", "Daniel", ""]]}, {"id": "1804.08733", "submitter": "Charith Mendis", "authors": "Charith Mendis and Saman Amarasinghe", "title": "goSLP: Globally Optimized Superword Level Parallelism Framework", "comments": "Published at OOPSLA 2018", "journal-ref": "OOPSLA 2018", "doi": "10.1145/3276480", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern microprocessors are equipped with single instruction multiple data\n(SIMD) or vector instruction sets which allow compilers to exploit superword\nlevel parallelism (SLP), a type of fine-grained parallelism. Current SLP\nauto-vectorization techniques use heuristics to discover vectorization\nopportunities in high-level language code. These heuristics are fragile, local\nand typically only present one vectorization strategy that is either accepted\nor rejected by a cost model. We present goSLP, a novel SLP auto-vectorization\nframework which solves the statement packing problem in a pairwise optimal\nmanner. Using an integer linear programming (ILP) solver, goSLP searches the\nentire space of statement packing opportunities for a whole function at a time,\nwhile limiting total compilation time to a few minutes. Furthermore, goSLP\noptimally solves the vector permutation selection problem using dynamic\nprogramming. We implemented goSLP in the LLVM compiler infrastructure,\nachieving a geometric mean speedup of 7.58% on SPEC2017fp, 2.42% on SPEC2006fp\nand 4.07% on NAS benchmarks compared to LLVM's existing SLP auto-vectorizer.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 20:59:12 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 14:23:11 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Mendis", "Charith", ""], ["Amarasinghe", "Saman", ""]]}, {"id": "1804.08976", "submitter": "Lu\\'is Cruz-Filipe", "authors": "Farhad Arbab and Lu\\'is Cruz-Filipe and Sung-Shik Jongmans and\n  Fabrizio Montesi", "title": "Connectors meet Choreographies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Cho-Reo-graphies (CR), a new language model that unites two\npowerful programming paradigms for concurrent software based on communicating\nprocesses: Choreographic Programming and Exogenous Coordination. In CR,\nprogrammers specify the desired communications among processes using a\nchoreography, and define how communications should be concretely animated by\nconnectors given as constraint automata (e.g., synchronous barriers and\nasynchronous multi-casts). CR is the first choreography calculus where\ndifferent communication semantics (determined by connectors) can be freely\nmixed; since connectors are user-defined, CR also supports many communication\nsemantics that were previously unavailable for choreographies. We develop a\nstatic analysis that guarantees that a choreography in CR and its user-defined\nconnectors are compatible, define a compiler from choreographies to a process\ncalculus based on connectors, and prove that compatibility guarantees\ndeadlock-freedom of the compiled process implementations.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 12:10:05 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Arbab", "Farhad", ""], ["Cruz-Filipe", "Lu\u00eds", ""], ["Jongmans", "Sung-Shik", ""], ["Montesi", "Fabrizio", ""]]}, {"id": "1804.08984", "submitter": "Amir Kafshdar Goharshady", "authors": "Krishnendu Chatterjee, Hongfei Fu, Amir Kafshdar Goharshady, Nastaran\n  Okati", "title": "Computational Approaches for Stochastic Shortest Path on Succinct MDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the stochastic shortest path (SSP) problem for succinct Markov\ndecision processes (MDPs), where the MDP consists of a set of variables, and a\nset of nondeterministic rules that update the variables. First, we show that\nseveral examples from the AI literature can be modeled as succinct MDPs. Then\nwe present computational approaches for upper and lower bounds for the SSP\nproblem: (a)~for computing upper bounds, our method is polynomial-time in the\nimplicit description of the MDP; (b)~for lower bounds, we present a\npolynomial-time (in the size of the implicit description) reduction to\nquadratic programming. Our approach is applicable even to infinite-state MDPs.\nFinally, we present experimental results to demonstrate the effectiveness of\nour approach on several classical examples from the AI literature.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 12:26:37 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 10:37:41 GMT"}, {"version": "v3", "created": "Tue, 17 Jul 2018 09:27:59 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Fu", "Hongfei", ""], ["Goharshady", "Amir Kafshdar", ""], ["Okati", "Nastaran", ""]]}, {"id": "1804.09007", "submitter": "Emanuele De Angelis", "authors": "Emanuele De Angelis (1), Fabio Fioravanti (1), Alberto Pettorossi (2),\n  Maurizio Proietti (3) ((1) DEC, University G. D'Annunzio of Chieti-Pescara,\n  Pescara, Italy, (2) DICII, University of Rome Tor Vergata, Roma, Italy, (3)\n  CNR-IASI, Roma, Italy)", "title": "Solving Horn Clauses on Inductive Data Types Without Induction", "comments": "Paper presented at the 34nd International Conference on Logic\n  Programming (ICLP 2018), Oxford, UK, July 14 to July 17, 2018. 22 pages,\n  LaTeX", "journal-ref": "Theory and Practice of Logic Programming, 18(3-4), 2018, 452-469", "doi": "10.1017/S1471068418000157", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of verifying the satisfiability of Constrained Horn\nClauses (CHCs) based on theories of inductively defined data structures, such\nas lists and trees. We propose a transformation technique whose objective is\nthe removal of these data structures from CHCs, hence reducing their\nsatisfiability to a satisfiability problem for CHCs on integers and booleans.\nWe propose a transformation algorithm and identify a class of clauses where it\nalways succeeds. We also consider an extension of that algorithm, which\ncombines clause transformation with reasoning on integer constraints. Via an\nexperimental evaluation we show that our technique greatly improves the\neffectiveness of applying the Z3 solver to CHCs. We also show that our\nverification technique based on CHC transformation followed by CHC solving, is\ncompetitive with respect to CHC solvers extended with induction. This paper is\nunder consideration for acceptance in TPLP.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 13:20:25 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["De Angelis", "Emanuele", ""], ["Fioravanti", "Fabio", ""], ["Pettorossi", "Alberto", ""], ["Proietti", "Maurizio", ""]]}, {"id": "1804.09352", "submitter": "Gregory Duck", "authors": "Gregory J. Duck and Joxan Jaffar and Roland H. C. Yap", "title": "Shape Neutral Analysis of Graph-based Data-structures", "comments": "Paper presented at the 34nd International Conference on Logic\n  Programming (ICLP 2018), Oxford, UK, July 14 to July 17, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malformed data-structures can lead to runtime errors such as arbitrary memory\naccess or corruption. Despite this, reasoning over data-structure properties\nfor low-level heap manipulating programs remains challenging. In this paper we\npresent a constraint-based program analysis that checks data-structure\nintegrity, w.r.t. given target data-structure properties, as the heap is\nmanipulated by the program. Our approach is to automatically generate a solver\nfor properties using the type definitions from the target program. The\ngenerated solver is implemented using a Constraint Handling Rules (CHR)\nextension of built-in heap, integer and equality solvers. A key property of our\nprogram analysis is that the target data-structure properties are shape\nneutral, i.e., the analysis does not check for properties relating to a given\ndata-structure graph shape, such as doubly-linked-lists versus trees.\nNevertheless, the analysis can detect errors in a wide range of data-structure\nmanipulating programs, including those that use lists, trees, DAGs, graphs,\netc. We present an implementation that uses the Satisfiability Modulo\nConstraint Handling Rules (SMCHR) system. Experimental results show that our\napproach works well for real-world C programs.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 05:13:21 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 02:58:39 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Duck", "Gregory J.", ""], ["Jaffar", "Joxan", ""], ["Yap", "Roland H. C.", ""]]}, {"id": "1804.09822", "submitter": "Vladimir Zamdzhiev", "authors": "Bert Lindenhovius, Michael Mislove, Vladimir Zamdzhiev", "title": "Enriching a Linear/Non-linear Lambda Calculus: A Programming Language\n  for String Diagrams", "comments": "To appear in LICS 2018", "journal-ref": null, "doi": "10.1145/3209108.3209196", "report-no": null, "categories": "cs.LO cs.PL math.CT quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear/non-linear (LNL) models, as described by Benton, soundly model a LNL\nterm calculus and LNL logic closely related to intuitionistic linear logic.\nEvery such model induces a canonical enrichment that we show soundly models a\nLNL lambda calculus for string diagrams, introduced by Rios and Selinger (with\nprimary application in quantum computing). Our abstract treatment of this\nlanguage leads to simpler concrete models compared to those presented so far.\nWe also extend the language with general recursion and prove soundness.\nFinally, we present an adequacy result for the diagram-free fragment of the\nlanguage which corresponds to a modified version of Benton and Wadler's adjoint\ncalculus with recursion.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 22:34:26 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Lindenhovius", "Bert", ""], ["Mislove", "Michael", ""], ["Zamdzhiev", "Vladimir", ""]]}, {"id": "1804.09946", "submitter": "Florian Rademacher", "authors": "Florian Rademacher, Sabine Sachweh, Albert Z\\\"undorf", "title": "Analysis of Service-oriented Modeling Approaches for Viewpoint-specific\n  Model-driven Development of Microservice Architecture", "comments": "8 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.ET cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microservice Architecture (MSA) is a novel service-based architectural style\nfor distributed software systems. Compared to Service-oriented Architecture\n(SOA), MSA puts a stronger focus on self-containment of services. Each\nmicroservice is responsible for realizing exactly one business or technological\ncapability that is distinct from other services' capabilities. Additionally, on\nthe implementation and operation level, microservices are self-contained in\nthat they are developed, tested, deployed and operated independently from each\nother. Next to these characteristics that distinguish MSA from SOA, both\narchitectural styles rely on services as building blocks of distributed\nsoftware architecture and hence face similar challenges regarding, e.g.,\nservice identification, composition and provisioning. However, in contrast to\nMSA, SOA may rely on an extensive body of knowledge to tackle these challenges.\nThus, due to both architectural styles being service-based, the question arises\nto what degree MSA might draw on existing findings of SOA research and\npractice. In this paper we address this question in the field of Model-driven\nDevelopment (MDD) for design and operation of service-based architectures.\nTherefore, we present an analysis of existing MDD approaches to SOA, which\ncomprises the identification and semantic clustering of modeling concepts for\nSOA design and operation. For each concept cluster, the analysis assesses its\napplicability to MDD of MSA (MSA-MDD) and assigns it to a specific modeling\nviewpoint. The goal of the presented analysis is to provide a conceptual\nfoundation for an MSA-MDD metamodel.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 08:57:38 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Rademacher", "Florian", ""], ["Sachweh", "Sabine", ""], ["Z\u00fcndorf", "Albert", ""]]}, {"id": "1804.09948", "submitter": "Florian Rademacher", "authors": "Florian Rademacher, Jonas Sorgalla, Sabine Sachweh, Albert Z\\\"undorf", "title": "Towards a Viewpoint-specific Metamodel for Model-driven Development of\n  Microservice Architecture", "comments": "8 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microservice Architecture (MSA) is a service-based architectural style with a\nstrong emphasis on high cohesion and loose coupling. It is commonly regarded as\na descendant of Service-oriented Architecture (SOA) and thus might draw on\nexisting findings of SOA research. This paper presents a metamodel for\nModel-driven Development (MDD) of MSA, which is deduced from existing SOA\nmodeling approaches, but also incorporates MSA-specific modeling concepts. It\nis divided into the three viewpoints Data, Service and Operation, each of which\nencapsulates concepts related to a certain aspect of MSA. The metamodel aims to\nsupport DevOps-based MSA development and automatic transformation of metamodel\ninstances into MSA implementations.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 08:57:56 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Rademacher", "Florian", ""], ["Sorgalla", "Jonas", ""], ["Sachweh", "Sabine", ""], ["Z\u00fcndorf", "Albert", ""]]}, {"id": "1804.10112", "submitter": "Stephen Chou", "authors": "Stephen Chou, Fredrik Kjolstad, Saman Amarasinghe", "title": "Format Abstraction for Sparse Tensor Algebra Compilers", "comments": "Presented at OOPSLA 2018", "journal-ref": "Proc. ACM Program. Lang. 2, OOPSLA, Article 123 (November 2018)", "doi": "10.1145/3276493", "report-no": null, "categories": "cs.MS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows how to build a sparse tensor algebra compiler that is\nagnostic to tensor formats (data layouts). We develop an interface that\ndescribes formats in terms of their capabilities and properties, and show how\nto build a modular code generator where new formats can be added as plugins. We\nthen describe six implementations of the interface that compose to form the\ndense, CSR/CSF, COO, DIA, ELL, and HASH tensor formats and countless variants\nthereof. With these implementations at hand, our code generator can generate\ncode to compute any tensor algebra expression on any combination of the\naforementioned formats.\n  To demonstrate our technique, we have implemented it in the taco tensor\nalgebra compiler. Our modular code generator design makes it simple to add\nsupport for new tensor formats, and the performance of the generated code is\ncompetitive with hand-optimized implementations. Furthermore, by extending taco\nto support a wider range of formats specialized for different application and\ndata characteristics, we can improve end-user application performance. For\nexample, if input data is provided in the COO format, our technique allows\ncomputing a single matrix-vector multiplication directly with the data in COO,\nwhich is up to 3.6$\\times$ faster than by first converting the data to CSR.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 20:57:59 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 02:16:20 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Chou", "Stephen", ""], ["Kjolstad", "Fredrik", ""], ["Amarasinghe", "Saman", ""]]}, {"id": "1804.10540", "submitter": "Noam Zeilberger", "authors": "Noam Zeilberger", "title": "A theory of linear typings as flows on 3-valent graphs", "comments": "To appear in LICS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building on recently established enumerative connections between lambda\ncalculus and the theory of embedded graphs (or \"maps\"), this paper develops an\nanalogy between typing (of lambda terms) and coloring (of maps). Our starting\npoint is the classical notion of an abelian group-valued \"flow\" on an abstract\ngraph (Tutte, 1954). Typing a linear lambda term may be naturally seen as\nconstructing a flow (on an embedded 3-valent graph with boundary) valued in a\nmore general algebraic structure consisting of a preordered set equipped with\nan \"implication\" operation and unit satisfying composition, identity, and unit\nlaws. Interesting questions and results from the theory of flows (such as the\nexistence of nowhere-zero flows) may then be re-examined from the standpoint of\nlambda calculus and logic. For example, we give a characterization of when the\nlocal flow relations (across vertices) may be categorically lifted to a global\nflow relation (across the boundary), proving that this holds just in case the\nunderlying map has the orientation of a lambda term. We also develop a basic\ntheory of rewriting of flows that suggests topological meanings for classical\ncompleteness results in combinatory logic, and introduce a polarized notion of\nflow, which draws connections to the theory of proof-nets in linear logic and\nto bidirectional typing.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 14:52:35 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Zeilberger", "Noam", ""]]}, {"id": "1804.10565", "submitter": "Stefania Dumbrava", "authors": "Angela Bonifati, Stefania Dumbrava, and Emilio Jesus Gallego Arias", "title": "Certified Graph View Maintenance with Regular Datalog", "comments": "Paper presented at the 34nd International Conference on Logic\n  Programming (ICLP 2018), Oxford, UK, July 14 to July 17, 2018. 18 pages,\n  LaTeX, (arXiv:YYMM.NNNNN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We employ the Coq proof assistant to develop a mechanically-certified\nframework for evaluating graph queries and incrementally maintaining\nmaterialized graph instances, also called views. The language we use for\ndefining queries and views is Regular Datalog (RD) -- a notable fragment of\nnon-recursive Datalog that can express complex navigational queries, with\ntransitive closure as native operator. We first design and encode the theory of\nRD and then mechanize a RD-specific evaluation algorithm capable of\nfine-grained, incremental graph view computation, which we prove sound with\nrespect to the declarative RD semantics. By using the Coq extraction mechanism,\nwe test an Ocaml version of the verified engine on a set of preliminary\nbenchmarks. Our development is particularly focused on leveraging existing\nverification and notational techniques to: a) define mechanized properties that\ncan be easily understood by logicians and database researchers and b) attain\nformal verification with limited effort. Our work is the first step towards a\nunified, machine-verified, formal framework for dynamic graph query languages\nand their evaluation engines. This paper is under consideration for acceptance\nin TPLP.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 15:42:36 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Bonifati", "Angela", ""], ["Dumbrava", "Stefania", ""], ["Arias", "Emilio Jesus Gallego", ""]]}, {"id": "1804.10694", "submitter": "R. Baghdadi", "authors": "Riyadh Baghdadi, Jessica Ray, Malek Ben Romdhane, Emanuele Del Sozzo,\n  Abdurrahman Akkas, Yunming Zhang, Patricia Suriana, Shoaib Kamil, Saman\n  Amarasinghe", "title": "Tiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code", "comments": "arXiv admin note: substantial text overlap with arXiv:1803.00419", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.MS cs.NE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Tiramisu, a polyhedral framework designed to generate\nhigh performance code for multiple platforms including multicores, GPUs, and\ndistributed machines. Tiramisu introduces a scheduling language with novel\nextensions to explicitly manage the complexities that arise when targeting\nthese systems. The framework is designed for the areas of image processing,\nstencils, linear algebra and deep learning. Tiramisu has two main features: it\nrelies on a flexible representation based on the polyhedral model and it has a\nrich scheduling language allowing fine-grained control of optimizations.\nTiramisu uses a four-level intermediate representation that allows full\nseparation between the algorithms, loop transformations, data layouts, and\ncommunication. This separation simplifies targeting multiple hardware\narchitectures with the same algorithm. We evaluate Tiramisu by writing a set of\nimage processing, deep learning, and linear algebra benchmarks and compare them\nwith state-of-the-art compilers and hand-tuned libraries. We show that Tiramisu\nmatches or outperforms existing compilers and libraries on different hardware\narchitectures, including multicore CPUs, GPUs, and distributed machines.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 21:28:44 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 19:58:57 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 21:24:44 GMT"}, {"version": "v4", "created": "Tue, 18 Dec 2018 02:41:00 GMT"}, {"version": "v5", "created": "Thu, 20 Dec 2018 16:25:40 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Baghdadi", "Riyadh", ""], ["Ray", "Jessica", ""], ["Romdhane", "Malek Ben", ""], ["Del Sozzo", "Emanuele", ""], ["Akkas", "Abdurrahman", ""], ["Zhang", "Yunming", ""], ["Suriana", "Patricia", ""], ["Kamil", "Shoaib", ""], ["Amarasinghe", "Saman", ""]]}, {"id": "1804.10806", "submitter": "Feng Wang", "authors": "Feng Wang, Fu Song, Min Zhang, Xiaoran Zhu and Jun Zhang", "title": "KRust: A Formal Executable Semantics of Rust", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rust is a new and promising high-level system programming language. It\nprovides both memory safety and thread safety through its novel mechanisms such\nas ownership, moves and borrows. Ownership system ensures that at any point\nthere is only one owner of any given resource. The ownership of a resource can\nbe moved or borrowed according to the lifetimes. The ownership system\nestablishes a clear lifetime for each value and hence does not necessarily need\ngarbage collection. These novel features bring Rust high performance, fine\nlow-level control of C and C++, and unnecessity in garbage collection, which\ndiffer Rust from other existing prevalent languages. For formal analysis of\nRust programs and helping programmers learn its new mechanisms and features, a\nformal semantics of Rust is desired and useful as a fundament for developing\nrelated tools. In this paper, we present a formal executable operational\nsemantics of a realistic subset of Rust, called KRust. The semantics is defined\nin K, a rewriting-based executable semantic framework for programming\nlanguages. The executable semantics yields automatically a formal interpreter\nand verification tools for Rust programs. KRust has been thoroughly validated\nby testing with hundreds of tests, including the official Rust test suite.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 14:12:11 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Wang", "Feng", ""], ["Song", "Fu", ""], ["Zhang", "Min", ""], ["Zhu", "Xiaoran", ""], ["Zhang", "Jun", ""]]}, {"id": "1804.11162", "submitter": "Joaqu\\'in Arias M.Sc.", "authors": "Joaqu\\'in Arias, Manuel Carro, Elmer Salazar, Kyle Marple and Gopal\n  Gupta", "title": "Constraint Answer Set Programming without Grounding", "comments": "Paper presented at the 34nd International Conference on Logic\n  Programming (ICLP 2018), Oxford, UK, July 14 to July 17, 2018 18 pages, LaTeX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending ASP with constraints (CASP) enhances its expressiveness and\nperformance. This extension is not straightforward as the grounding phase,\npresent in most ASP systems, removes variables and the links among them, and\nalso causes a combinatorial explosion in the size of the program. Several\nmethods to overcome this issue have been devised: restricting the constraint\ndomains (e.g., discrete instead of dense), or the type (or number) of models\nthat can be returned. In this paper we propose to incorporate constraints into\ns(ASP), a goal-directed, top-down execution model which implements ASP while\nretaining logical variables both during execution and in the answer sets. The\nresulting model, s(CASP), can constrain variables that, as in CLP, are kept\nduring the execution and in the answer sets. s(CASP) inherits and generalizes\nthe execution model of s(ASP) and is parametric w.r.t. the constraint solver.\nWe describe this novel execution model and show through several examples the\nenhanced expressiveness of s(CASP) w.r.t. ASP, CLP, and other CASP systems. We\nalso report improved performance w.r.t. other very mature, highly optimized ASP\nsystems in some benchmarks. This paper is under consideration for publication\nin Theory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 12:50:28 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 15:33:53 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Arias", "Joaqu\u00edn", ""], ["Carro", "Manuel", ""], ["Salazar", "Elmer", ""], ["Marple", "Kyle", ""], ["Gupta", "Gopal", ""]]}]