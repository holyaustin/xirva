[{"id": "2101.00909", "submitter": "Caterina Urban", "authors": "Francesco Ranzato, Caterina Urban, Marco Zanella", "title": "Fair Training of Decision Tree Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of formally verifying individual fairness of decision\ntree ensembles, as well as training tree models which maximize both accuracy\nand individual fairness. In our approach, fairness verification and\nfairness-aware training both rely on a notion of stability of a classification\nmodel, which is a variant of standard robustness under input perturbations used\nin adversarial machine learning. Our verification and training methods leverage\nabstract interpretation, a well established technique for static program\nanalysis which is able to automatically infer assertions about stability\nproperties of decision trees. By relying on a tool for adversarial training of\ndecision trees, our fairness-aware learning method has been implemented and\nexperimentally evaluated on the reference datasets used to assess fairness\nproperties. The experimental results show that our approach is able to train\ntree models exhibiting a high degree of individual fairness w.r.t. the natural\nstate-of-the-art CART trees and random forests. Moreover, as a by-product,\nthese fair decision trees turn out to be significantly compact, thus enhancing\nthe interpretability of their fairness properties.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 12:04:22 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Ranzato", "Francesco", ""], ["Urban", "Caterina", ""], ["Zanella", "Marco", ""]]}, {"id": "2101.00930", "submitter": "Heiko Becker", "authors": "Heiko Becker, Nathaniel Bos, Ivan Gavran, Eva Darulova, Rupak Majumdar", "title": "Lassie: HOL4 Tactics by Example", "comments": null, "journal-ref": null, "doi": "10.1145/3437992.3439925", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Proof engineering efforts using interactive theorem proving have yielded\nseveral impressive projects in software systems and mathematics. A key obstacle\nto such efforts is the requirement that the domain expert is also an expert in\nthe low-level details in constructing the proof in a theorem prover. In\nparticular, the user needs to select a sequence of tactics that lead to a\nsuccessful proof, a task that in general requires knowledge of the exact names\nand use of a large set of tactics.\n  We present Lassie, a tactic framework for the HOL4 theorem prover that allows\nindividual users to define their own tactic language by example and give\nfrequently used tactics or tactic combinations easier-to-remember names. The\ncore of Lassie is an extensible semantic parser, which allows the user to\ninteractively extend the tactic language through a process of definitional\ngeneralization. Defining tactics in Lassie thus does not require any knowledge\nin implementing custom tactics, while proofs written in Lassie retain the\ncorrectness guarantees provided by the HOL4 system. We show through case\nstudies how Lassie can be used in small and larger proofs by novice and more\nexperienced interactive theorem prover users, and how we envision it to ease\nthe learning curve in a HOL4 tutorial.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 12:50:36 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Becker", "Heiko", ""], ["Bos", "Nathaniel", ""], ["Gavran", "Ivan", ""], ["Darulova", "Eva", ""], ["Majumdar", "Rupak", ""]]}, {"id": "2101.00961", "submitter": "Justin Hsu", "authors": "Subhajit Roy, Justin Hsu, Aws Albarghouthi", "title": "Learning Differentially Private Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a formal, mathematical definition of data privacy\nthat has gained traction in academia, industry, and government. The task of\ncorrectly constructing differentially private algorithms is non-trivial, and\nmistakes have been made in foundational algorithms. Currently, there is no\nautomated support for converting an existing, non-private program into a\ndifferentially private version. In this paper, we propose a technique for\nautomatically learning an accurate and differentially private version of a\ngiven non-private program. We show how to solve this difficult program\nsynthesis problem via a combination of techniques: carefully picking\nrepresentative example inputs, reducing the problem to continuous optimization,\nand mapping the results back to symbolic expressions. We demonstrate that our\napproach is able to learn foundational algorithms from the differential privacy\nliterature and significantly outperforms natural program synthesis baselines.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 13:33:57 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Roy", "Subhajit", ""], ["Hsu", "Justin", ""], ["Albarghouthi", "Aws", ""]]}, {"id": "2101.01159", "submitter": "Joseph Hellerstein", "authors": "Alvin Cheung, Natacha Crooks, Joseph M. Hellerstein and Matthew Milano", "title": "New Directions in Cloud Programming", "comments": null, "journal-ref": "CIDR 2021", "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.OS cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nearly twenty years after the launch of AWS, it remains difficult for most\ndevelopers to harness the enormous potential of the cloud. In this paper we lay\nout an agenda for a new generation of cloud programming research aimed at\nbringing research ideas to programmers in an evolutionary fashion. Key to our\napproach is a separation of distributed programs into a PACT of four facets:\nProgram semantics, Availablity, Consistency and Targets of optimization. We\npropose to migrate developers gradually to PACT programming by lifting familiar\ncode into our more declarative level of abstraction. We then propose a\nmulti-stage compiler that emits human-readable code at each stage that can be\nhand-tuned by developers seeking more control. Our agenda raises numerous\nresearch challenges across multiple areas including language design, query\noptimization, transactions, distributed consistency, compilers and program\nsynthesis.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:42:54 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Cheung", "Alvin", ""], ["Crooks", "Natacha", ""], ["Hellerstein", "Joseph M.", ""], ["Milano", "Matthew", ""]]}, {"id": "2101.01312", "submitter": "Caleb Voss", "authors": "Caleb Voss, Vivek Sarkar", "title": "An Ownership Policy and Deadlock Detector for Promises", "comments": null, "journal-ref": "Principles and Practice of Parallel Programming, 2021, ACM, pp.\n  348-361", "doi": "10.1145/3437801.3441616", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-parallel programs often enjoy deadlock freedom under certain\nrestrictions, such as the use of structured join operations, as in Cilk and\nX10, or the use of asynchronous task futures together with deadlock-avoiding\npolicies such as Known Joins or Transitive Joins. However, the promise, a\npopular synchronization primitive for parallel tasks, does not enjoy\ndeadlock-freedom guarantees. Promises can exhibit deadlock-like bugs; however,\nthe concept of a deadlock is not currently well-defined for promises.\n  To address these challenges, we propose an ownership semantics in which each\npromise is associated to the task which currently intends to fulfill it.\nOwnership immediately enables the identification of bugs in which a task fails\nto fulfill a promise for which it is responsible. Ownership further enables the\ndiscussion of deadlock cycles among tasks and promises and allows us to\nintroduce a robust definition of deadlock-like bugs for promises.\n  Cycle detection in this context is non-trivial because it is concurrent with\nchanges in promise ownership. We provide a lock-free algorithm for precise\nruntime deadlock detection. We show how to obtain the memory consistency\ncriteria required for the correctness of our algorithm under TSO and the Java\nand C++ memory models. An evaluation compares the execution time and memory\nusage overheads of our detection algorithm on benchmark programs relative to an\nunverified baseline. Our detector exhibits a 12% (1.12$\\times$) geometric mean\ntime overhead and a 6% (1.06$\\times$) geometric mean memory overhead, which are\nsmaller overheads than in past approaches to deadlock cycle detection.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 01:51:23 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Voss", "Caleb", ""], ["Sarkar", "Vivek", ""]]}, {"id": "2101.01502", "submitter": "Ichiro Hasuo", "authors": "Ichiro Hasuo, Yuichiro Oyabu, Clovis Eberhart, Kohei Suenaga, Kenta\n  Cho, Shin-ya Katsumata", "title": "Control-Data Separation and Logical Condition Propagation for Efficient\n  Inference on Probabilistic Programs", "comments": "11 pages with appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel sampling algorithm for Bayesian inference on imperative\nprobabilistic programs. It features a hierarchical architecture that separates\ncontrol flows from data: the top-level samples a control flow, and the bottom\nlevel samples data values along the control flow picked by the top level. This\nseparation allows us to plug various language-based analysis techniques in\nprobabilistic program sampling; specifically, we use logical backward\npropagation of observations for sampling efficiency. We implemented our\nalgorithm on top of Anglican. The experimental results demonstrate our\nalgorithm's efficiency, especially for programs with while loops and rare\nobservations.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 13:40:59 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 14:43:18 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Hasuo", "Ichiro", ""], ["Oyabu", "Yuichiro", ""], ["Eberhart", "Clovis", ""], ["Suenaga", "Kohei", ""], ["Cho", "Kenta", ""], ["Katsumata", "Shin-ya", ""]]}, {"id": "2101.02522", "submitter": "Vincent Aranega", "authors": "Ronie Salgado, Marcus Denker (RMOD), St\\'ephane Ducasse (RMOD), Anne\n  Etien (RMOD), Vincent Aranega (RMOD)", "title": "Towards a Smart Data Processing and Storage Model", "comments": null, "journal-ref": "IWST20: International Workshop on Smalltalk Technologies, Sep\n  2020, Novi Sad, Serbia", "doi": null, "report-no": null, "categories": "cs.CL cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several domains it is crucial to store and manipulate data whose origin\nneeds to be completely traceable to guarantee the consistency, trustworthiness\nand reliability on the data itself typically for ethical and legal reasons. It\nis also important to guarantee that such properties are also carried further\nwhen such data is composed and processed into new data. In this article we\npresent the main requirements and theorethical problems that arise by the\ndesign of a system supporting data with such capabilities. We present an\narchitecture for implementing a system as well as a prototype developed in\nPharo.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 12:52:11 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Salgado", "Ronie", "", "RMOD"], ["Denker", "Marcus", "", "RMOD"], ["Ducasse", "St\u00e9phane", "", "RMOD"], ["Etien", "Anne", "", "RMOD"], ["Aranega", "Vincent", "", "RMOD"]]}, {"id": "2101.02690", "submitter": "Narciso Mart\\'i-Oliet", "authors": "Joseph A. Goguen", "title": "Theorem Proving and Algebra", "comments": "427+ xviii pages, 38 figures, Unfinished book by Joseph A. Goguen,\n  Edited by Kokichi Futatsugi, Narciso Mart\\'i-Oliet and Jos\\'e Meseguer;\n  revised version corrects some strange characters in page xv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This book can be seen either as a text on theorem proving that uses\ntechniques from general algebra, or else as a text on general algebra\nillustrated and made concrete by practical exercises in theorem proving. The\nbook considers several different logical systems, including first-order logic,\nHorn clause logic, equational logic, and first-order logic with equality.\nSimilarly, several different proof paradigms are considered. However, we do\nemphasize equational logic, and for simplicity we use only the OBJ3 software\nsystem, though it is used in a rather flexible manner. We do not pursue the\nlofty goal of mechanizing proofs like those of which mathematicians are justly\nso proud; instead, we seek to take steps towards providing mechanical\nassistance for proofs that are useful for computer scientists in developing\nsoftware and hardware. This more modest goal has the advantage of both being\nachievable and having practical benefits.\n  The following topics are covered: many-sorted signature, algebra and\nhomomorphism; term algebra and substitution; equation and satisfaction;\nconditional equations; equational deduction and its completeness; deduction for\nconditional equations; the theorem of constants; interpretation and equivalence\nof theories; term rewriting, termination, confluence and normal form; abstract\nrewrite systems; standard models, abstract data types, initiality, and\ninduction; rewriting and deduction modulo equations; first-order logic, models,\nand proof planning; second-order algebra; order-sorted algebra and rewriting;\nmodules; unification and completion; and hidden algebra. In parallel with these\nare a gradual introduction to OBJ3, applications to group theory, various\nabstract data types (such as number systems, lists, and stacks), propositional\ncalculus, hardware verification, the {\\lambda}-calculus, correctness of\nfunctional programs, and other topics.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 18:52:08 GMT"}, {"version": "v2", "created": "Sat, 16 Jan 2021 22:29:37 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Goguen", "Joseph A.", ""]]}, {"id": "2101.02993", "submitter": "Aritra Sarkar", "authors": "A. M. Krol, A. Sarkar, I. Ashraf, Z. Al-Ars, K. Bertels", "title": "Efficient decomposition of unitary matrices in quantum circuit compilers", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Unitary decomposition is a widely used method to map quantum algorithms to an\narbitrary set of quantum gates. Efficient implementation of this decomposition\nallows for translation of bigger unitary gates into elementary quantum\noperations, which is key to executing these algorithms on existing quantum\ncomputers. The decomposition can be used as an aggressive optimization method\nfor the whole circuit, as well as to test part of an algorithm on a quantum\naccelerator. For selection and implementation of the decomposition algorithm,\nperfect qubits are assumed. We base our decomposition technique on Quantum\nShannon Decomposition which generates O((3/4)*4^n) controlled-not gates for an\nn-qubit input gate. The resulting circuits are up to 10 times shorter than\nother methods in the field. When comparing our implementation to Qubiter, we\nshow that our implementation generates circuits with half the number of CNOT\ngates and a third of the total circuit length. In addition to that, it is also\nup to 10 times as fast. Further optimizations are proposed to take advantage of\npotential underlying structure in the input or intermediate matrices, as well\nas to minimize the execution time of the decomposition.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 12:54:27 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Krol", "A. M.", ""], ["Sarkar", "A.", ""], ["Ashraf", "I.", ""], ["Al-Ars", "Z.", ""], ["Bertels", "K.", ""]]}, {"id": "2101.03238", "submitter": "Yichen Yang", "authors": "Jeevana Priya Inala, Yichen Yang, James Paulos, Yewen Pu, Osbert\n  Bastani, Vijay Kumar, Martin Rinard, Armando Solar-Lezama", "title": "Neurosymbolic Transformers for Multi-Agent Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of inferring communication structures that can solve\ncooperative multi-agent planning problems while minimizing the amount of\ncommunication. We quantify the amount of communication as the maximum degree of\nthe communication graph; this metric captures settings where agents have\nlimited bandwidth. Minimizing communication is challenging due to the\ncombinatorial nature of both the decision space and the objective; for\ninstance, we cannot solve this problem by training neural networks using\ngradient descent. We propose a novel algorithm that synthesizes a control\npolicy that combines a programmatic communication policy used to generate the\ncommunication graph with a transformer policy network used to choose actions.\nOur algorithm first trains the transformer policy, which implicitly generates a\n\"soft\" communication graph; then, it synthesizes a programmatic communication\npolicy that \"hardens\" this graph, forming a neurosymbolic transformer. Our\nexperiments demonstrate how our approach can synthesize policies that generate\nlow-degree communication graphs while maintaining near-optimal performance.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 04:13:57 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Inala", "Jeevana Priya", ""], ["Yang", "Yichen", ""], ["Paulos", "James", ""], ["Pu", "Yewen", ""], ["Bastani", "Osbert", ""], ["Kumar", "Vijay", ""], ["Rinard", "Martin", ""], ["Solar-Lezama", "Armando", ""]]}, {"id": "2101.03263", "submitter": "Matthew Sotoudeh", "authors": "Matthew Sotoudeh and Aditya V. Thakur", "title": "SyReNN: A Tool for Analyzing Deep Neural Networks", "comments": "Accepted paper at TACAS 2021. Tool is available at\n  https://github.com/95616ARG/SyReNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are rapidly gaining popularity in a variety of\nimportant domains. Formally, DNNs are complicated vector-valued functions which\ncome in a variety of sizes and applications. Unfortunately, modern DNNs have\nbeen shown to be vulnerable to a variety of attacks and buggy behavior. This\nhas motivated recent work in formally analyzing the properties of such DNNs.\nThis paper introduces SyReNN, a tool for understanding and analyzing a DNN by\ncomputing its symbolic representation. The key insight is to decompose the DNN\ninto linear functions. Our tool is designed for analyses using low-dimensional\nsubsets of the input space, a unique design point in the space of DNN analysis\ntools. We describe the tool and the underlying theory, then evaluate its use\nand performance on three case studies: computing Integrated Gradients,\nvisualizing a DNN's decision boundaries, and patching a DNN.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 00:27:23 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Sotoudeh", "Matthew", ""], ["Thakur", "Aditya V.", ""]]}, {"id": "2101.03391", "submitter": "Jules Jacobs", "authors": "Jules Jacobs", "title": "Paradoxes of Probabilistic Programming", "comments": null, "journal-ref": null, "doi": "10.1145/3434339", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic programming languages allow programmers to write down\nconditional probability distributions that represent statistical and machine\nlearning models as programs that use observe statements. These programs are run\nby accumulating likelihood at each observe statement, and using the likelihood\nto steer random choices and weigh results with inference algorithms such as\nimportance sampling or MCMC. We argue that naive likelihood accumulation does\nnot give desirable semantics and leads to paradoxes when an observe statement\nis used to condition on a measure-zero event, particularly when the observe\nstatement is executed conditionally on random data. We show that the paradoxes\ndisappear if we explicitly model measure-zero events as a limit of positive\nmeasure events, and that we can execute these type of probabilistic programs by\naccumulating infinitesimal probabilities rather than probability densities. Our\nextension improves probabilistic programming languages as an executable\nnotation for probability distributions by making it more well-behaved and more\nexpressive, by allowing the programmer to be explicit about which limit is\nintended when conditioning on an event of measure zero.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 16:58:55 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 13:09:13 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Jacobs", "Jules", ""]]}, {"id": "2101.04102", "submitter": "Wilmer Ricciotti", "authors": "Wilmer Ricciotti and James Cheney", "title": "Query Lifting: Language-integrated query for heterogeneous nested\n  collections", "comments": "Full version of ESOP 2021 conference paper", "journal-ref": null, "doi": "10.1007/978-3-030-72019-3_21", "report-no": null, "categories": "cs.PL cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Language-integrated query based on comprehension syntax is a powerful\ntechnique for safe database programming, and provides a basis for advanced\ntechniques such as query shredding or query flattening that allow efficient\nprogramming with complex nested collections. However, the foundations of these\ntechniques are lacking: although SQL, the most widely-used database query\nlanguage, supports heterogeneous queries that mix set and multiset semantics,\nthese important capabilities are not supported by known correctness results or\nimplementations that assume homogeneous collections. In this paper we study\nlanguage-integrated query for a heterogeneous query language\n$NRC_\\lambda(Set,Bag)$ that combines set and multiset constructs. We show how\nto normalize and translate queries to SQL, and develop a novel approach to\nquerying heterogeneous nested collections, based on the insight that ``local''\nquery subexpressions that calculate nested subcollections can be ``lifted'' to\nthe top level analogously to lambda-lifting for local function definitions.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 18:48:26 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 14:55:34 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Ricciotti", "Wilmer", ""], ["Cheney", "James", ""]]}, {"id": "2101.04395", "submitter": "Michael Witterauf", "authors": "Michael Witterauf and Dominik Walter and Frank Hannig and J\\\"urgen\n  Teich", "title": "Symbolic Loop Compilation for Tightly Coupled Processor Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loop compilation for Tightly Coupled Processor Arrays (TCPAs), a class of\nmassively parallel loop accelerators, entails solving NP-hard problems, yet\ndepends on the loop bounds and number of available processing elements (PEs),\nparameters known only at runtime because of dynamic resource management and\ninput sizes. Therefore, this article proposes a two-phase approach called\nsymbolic loop compilation: At compile time, the necessary NP-complete problems\nare solved and the solutions compiled into a space-efficient symbolic\nconfiguration. At runtime, a concrete configuration is generated from the\nsymbolic configuration according to the parameters values. We show that the\nlatter phase, called instantiation, runs in polynomial time with its most\ncomplex step, program instantiation, not depending on the number of PEs. As\nvalidation, we performed symbolic loop compilation on real-world loops and\nmeasured time and space requirements. Our experiments confirm that a symbolic\nconfiguration is space-efficient and suited for systems with little memory --\noften, a symbolic configuration is smaller than a single concrete configuration\n-- and that program instantiation scales well with the number of PEs -- for\nexample, when instantiating a symbolic configuration of a matrix-matrix\nmultiplication, the execution time is similar for $4\\times 4$ and $32\\times 32$\nPEs.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 10:38:39 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Witterauf", "Michael", ""], ["Walter", "Dominik", ""], ["Hannig", "Frank", ""], ["Teich", "J\u00fcrgen", ""]]}, {"id": "2101.04470", "submitter": "Amir M. Mir", "authors": "Amir M. Mir, Evaldas Latoskinas, Sebastian Proksch, Georgios Gousios", "title": "Type4Py: Deep Similarity Learning-Based Type Inference for Python", "comments": "Type4Py's source code and dataset can be retrieved here:\n  https://github.com/mir-am/type4py-paper The second version of the paper is\n  published in Jul. 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Dynamic languages, such as Python and Javascript, trade static typing for\ndeveloper flexibility and productivity. Lack of static typing can cause\nrun-time exceptions and is a major factor for weak IDE support. To alleviate\nthese issues, PEP 484 introduced optional type annotations for Python. As\nretrofitting types to existing codebases is error-prone and laborious,\nlearning-based approaches have been proposed to enable automatic type\nannotations based on existing, partially annotated codebases. However, it is\nstill quite challenging for learning-based approaches to give a relevant\nprediction in the first suggestion or the first few ones. In this paper, we\npresent Type4Py, a deep similarity learning-based hierarchical neural network\nmodel that learns to discriminate between types of the same kind and dissimilar\ntypes in a high-dimensional space, which results in clusters of types. Nearest\nneighbor search suggests a list of likely types for arguments, variables, and\nfunctions' return. The results of the quantitative and qualitative evaluation\nindicate that Type4Py significantly outperforms state-of-the-art approaches at\nthe type prediction task. Considering the Top-1 prediction, Type4Py obtains a\nMean Reciprocal Rank of 72.5%, which is 10.87% and 16.45% higher than that of\nTypilus and TypeWriter, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 13:32:53 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 16:10:37 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Mir", "Amir M.", ""], ["Latoskinas", "Evaldas", ""], ["Proksch", "Sebastian", ""], ["Gousios", "Georgios", ""]]}, {"id": "2101.04622", "submitter": "Fangyi Zhou", "authors": "Anson Miu (1 and 2), Francisco Ferreira (1), Nobuko Yoshida (1),\n  Fangyi Zhou (1) ((1) Imperial College London, (2) Bloomberg)", "title": "Communication-Safe Web Programming in TypeScript with Routed Multiparty\n  Session Types", "comments": "Long version for the paper accepted at CC '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern web programming involves coordinating interactions between browser\nclients and a server. Typically, the interactions in web-based distributed\nsystems are informally described, making it hard to ensure correctness,\nespecially communication safety, i.e. all endpoints progress without type\nerrors or deadlocks, conforming to a specified protocol.\n  We present STScript, a toolchain that generates TypeScript APIs for\ncommunication-safe web development over WebSockets, and RouST, a new session\ntype theory that supports multiparty communications with routing mechanisms.\nSTScript provides developers with TypeScript APIs generated from a\ncommunication protocol specification based on RouST. The generated APIs build\nupon TypeScript concurrency practices, complement the event-driven style of\nprogramming in full-stack web development, and are compatible with the Node.js\nruntime for server-side endpoints and the React.js framework for browser-side\nendpoints.\n  RouST can express multiparty interactions routed via an intermediate\nparticipant. It supports peer-to-peer communication between browser-side\nendpoints by routing communication via the server in a way that avoids\nexcessive serialisation. RouST guarantees communication safety for endpoint web\napplications written using STScript APIs.\n  We evaluate the expressiveness of STScript for modern web programming using\nseveral production-ready case studies deployed as web applications.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 17:22:39 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Miu", "Anson", "", "1 and 2"], ["Ferreira", "Francisco", "", "Imperial College London"], ["Yoshida", "Nobuko", "", "Imperial College London"], ["Zhou", "Fangyi", "", "Imperial College London"]]}, {"id": "2101.04718", "submitter": "Kristopher Micinski", "authors": "Yihao Sun, Jeffrey Ching, Kristopher Micinski", "title": "Declarative Demand-Driven Reverse Engineering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary reverse engineering is a challenging task because it often\nnecessitates reasoning using both domain-specific knowledge (e.g.,\nunderstanding entrypoint idioms common to an ABI) and logical inference (e.g.,\nreconstructing interprocedural control flow). To help perform these tasks,\nreverse engineers often use toolkits (such as IDA Pro or Ghidra) that allow\nthem to interactively explicate properties of binaries. We argue that deductive\ndatabases serve as a natural abstraction for interfacing between\nvisualization-based binary analysis tools and high-performance logical\ninference engines that compute facts about binaries. In this paper, we present\na vision for the future in which reverse engineers use a visualization-based\ntool to understand binaries while simultaneously querying a logical-inference\nengine to perform arbitrarily-complex deductive inference tasks. We call our\nvision declarative demand-driven reverse engineering (D^3RE for short), and\nsketch a formal semantics whose goal is to mediate interaction between a\nlogical-inference engine (such Souffle) and a reverse engineering tool. We\ndescribe aprototype tool, d3re, which are using to explore the D^3RE vision.\nWhile still a prototype, we have used d3re to reimplement several common\nquerying tasks on binaries. Our evaluation demonstrates that d3re enables both\nbetter performance and more succinct implementation of these common RE tasks.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 19:41:45 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Sun", "Yihao", ""], ["Ching", "Jeffrey", ""], ["Micinski", "Kristopher", ""]]}, {"id": "2101.04742", "submitter": "Eric Atkinson", "authors": "Eric Atkinson and Michael Carbin", "title": "Programming and Reasoning with Partial Observability", "comments": null, "journal-ref": "Proc. ACM Program. Lang. 4, OOPSLA, Article 200 (November 2020),\n  28 pages", "doi": "10.1145/3428268", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computer programs are increasingly being deployed in partially-observable\nenvironments. A partially observable environment is an environment whose state\nis not completely visible to the program, but from which the program receives\npartial observations. Developers typically deal with partial observability by\nwriting a state estimator that, given observations, attempts to deduce the\nhidden state of the environment. In safety-critical domains, to formally verify\nsafety properties developers may write an environment model. The model captures\nthe relationship between observations and hidden states and is used to prove\nthe software correct.\n  In this paper, we present a new methodology for writing and verifying\nprograms in partially observable environments. We present belief programming, a\nprogramming methodology where developers write an environment model that the\nprogram runtime automatically uses to perform state estimation. A belief\nprogram dynamically updates and queries a belief state that captures the\npossible states the environment could be in. To enable verification, we present\nEpistemic Hoare Logic that reasons about the possible belief states of a belief\nprogram the same way that classical Hoare logic reasons about the possible\nstates of a program. We develop these concepts by defining a semantics and a\nprogram logic for a simple core language called BLIMP. In a case study, we show\nhow belief programming could be used to write and verify a controller for the\nMars Polar Lander in BLIMP. We present an implementation of BLIMP called CBLIMP\nand evaluate it to determine the feasibility of belief programming.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 20:26:40 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Atkinson", "Eric", ""], ["Carbin", "Michael", ""]]}, {"id": "2101.04808", "submitter": "Mircea Trofin", "authors": "Mircea Trofin (1), Yundi Qian (1), Eugene Brevdo (1), Zinan Lin (2),\n  Krzysztof Choromanski (1), David Li (1) ((1) Google, Inc., (2) Carnegie\n  Mellon University)", "title": "MLGO: a Machine Learning Guided Compiler Optimizations Framework", "comments": "First two authors are equal contributors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Leveraging machine-learning (ML) techniques for compiler optimizations has\nbeen widely studied and explored in academia. However, the adoption of ML in\ngeneral-purpose, industry strength compilers has yet to happen. We propose\nMLGO, a framework for integrating ML techniques systematically in an industrial\ncompiler -- LLVM. As a case study, we present the details and results of\nreplacing the heuristics-based inlining-for-size optimization in LLVM with\nmachine learned models. To the best of our knowledge, this work is the first\nfull integration of ML in a complex compiler pass in a real-world setting. It\nis available in the main LLVM repository. We use two different ML algorithms:\nPolicy Gradient and Evolution Strategies, to train the inlining-for-size model,\nand achieve up to 7\\% size reduction, when compared to state of the art LLVM\n-Oz. The same model, trained on one corpus, generalizes well to a diversity of\nreal-world targets, as well as to the same set of targets after months of\nactive development. This property of the trained models is beneficial to deploy\nML techniques in real-world settings.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 00:02:49 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Trofin", "Mircea", ""], ["Qian", "Yundi", ""], ["Brevdo", "Eugene", ""], ["Lin", "Zinan", ""], ["Choromanski", "Krzysztof", ""], ["Li", "David", ""]]}, {"id": "2101.05702", "submitter": "Albert Benveniste", "authors": "Albert Benveniste (HYCOMES), Beno\\^it Caillaud (HYCOMES), Mathias\n  Malandain (HYCOMES)", "title": "Structural Analysis of Multimode DAE Systems: summary of results", "comments": "arXiv admin note: substantial text overlap with arXiv:2008.05166", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern modeling languages for general physical systems, such as Modelica,\nAmesim, or Simscape, rely on Differential Algebraic Equations (DAEs), i.e.,\nconstraints of the form f(\\dot{x},x,u)=0. This drastically facilitates modeling\nfrom first principles of the physics, as well as model reuse. In recent works\n[RR-9334], we presented the mathematical theory needed to establish the\ndevelopment of compilers and tools for DAE-based physical modeling languages on\nsolid mathematical grounds.At the core of this analysis sits the so-called\n*structural analysis*, whose purpose, at compile time, is to either identify\nunder- and over-specified subsystems (if any), or to rewrite the model in a\nform amenable of existing DAE solvers, including the handling of mode change\nevents. The notion of \"structure\" collects, for each mode and mode change\nevent, the variables and equations involved, as well as the *latent equations*\n(additional equations redundant with the system), needed to prepare the code\nsubmitted to the solver. The notion of DAE *index* (the minimal number of times\nany equation has to be possibly differentiated) is part of this structural\nanalysis. This report complements [RR-9334] by collecting all the needed\nbackground on structural analysis. The body of knowledge on structural analysis\nis large and scattered, which also motivated us to collect it in a single\nreport.We first explain the primary meaning of structural analysis of systems\nof equations, namely the study of their regularity or singularity in some\ngeneric sense. We then briefly review the body of graph theory used in this\ncontext. We develop some extensions, for which we are not aware of any\nreference, namely the structural analysis of systems of equations with\nexistential quantifiers. For the structural analysis of DAE systems, we focus\non John Pryce's Sigma-method, that we both summarize and extend to non-square\nsystems. The uses of these tools and methods in [RR9334] are highlighted in\nthis report.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 16:26:25 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 09:21:39 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Benveniste", "Albert", "", "HYCOMES"], ["Caillaud", "Beno\u00eet", "", "HYCOMES"], ["Malandain", "Mathias", "", "HYCOMES"]]}, {"id": "2101.06039", "submitter": "Albert Cohen", "authors": "Son Tuan Vu, Albert Cohen, Karine Heydemann, Arnaud de Grandmaison,\n  Christophe Guillon", "title": "Secure Optimization Through Opaque Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Secure applications implement software protections against side-channel and\nphysical attacks. Such protections are meaningful at machine code or\nmicro-architectural level, but they typically do not carry observable semantics\nat source level. To prevent optimizing compilers from altering the protection,\nsecurity engineers embed input/output side-effects into the protection. These\nside-effects are error-prone and compiler-dependent, and the current practice\ninvolves analyzing the generated machine code to make sure security or privacy\nproperties are still enforced. Vu et al. recently demonstrated how to automate\nthe insertion of volatile side-effects in a compiler [52], but these may be too\nexpensive in fined-grained protections such as control-flow integrity. We\nintroduce observations of the program state that are intrinsic to the correct\nexecution of security protections, along with means to specify and preserve\nobservations across the compilation flow. Such observations complement the\ntraditional input/output-preservation contract of compilers. We show how to\nguarantee their preservation without modifying compilation passes and with as\nlittle performance impact as possible. We validate our approach on a range of\nbenchmarks, expressing the secure compilation of these applications in terms of\nobservations to be made at specific program points.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 10:02:18 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Vu", "Son Tuan", ""], ["Cohen", "Albert", ""], ["Heydemann", "Karine", ""], ["de Grandmaison", "Arnaud", ""], ["Guillon", "Christophe", ""]]}, {"id": "2101.06087", "submitter": "Christian Lidstr\\\"om", "authors": "Christian Lidstr\\\"om and Dilian Gurov (KTH Royal Institute of\n  Technology, Stockholm, Sweden)", "title": "An Abstract Contract Theory for Programs with Procedures", "comments": "24 pages. This is the full version of the paper An Abstract Contract\n  Theory for Programs with Procedures, published in Proceedings of the 24th\n  International Conference on Fundamental Approaches to Software Engineering\n  (FASE 2021), which includes the proofs of all theorems and additional\n  examples. The conference version should always be cited", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  When developing complex software and systems, contracts provide a means for\ncontrolling the complexity by dividing the responsibilities among the\ncomponents of the system in a hierarchical fashion. In specific application\nareas, dedicated contract theories formalise the notion of contract and the\noperations on contracts in a manner that supports best the development of\nsystems in that area. At the other end, contract meta-theories attempt to\nprovide a systematic view on the various contract theories by axiomatising\ntheir desired properties. However, there exists a noticeable gap between the\nmost well-known contract meta-theory of Benveniste et al., which focuses on the\ndesign of embedded and cyber-physical systems, and the established way of using\ncontracts when developing general software, following Meyer's\ndesign-by-contract methodology. At the core of this gap appears to be the\nnotion of procedure: while it is a central unit of composition in software\ndevelopment, the meta-theory does not suggest an obvious way of treating\nprocedures as components.\n  In this paper, we provide a first step towards a contract theory that takes\nprocedures as the basic building block, and is at the same time an\ninstantiation of the meta-theory. To this end, we propose an abstract contract\ntheory for sequential programming languages with procedures, based on\ndenotational semantics. We show that, on the one hand, the specification of\ncontracts of procedures in Hoare logic, and their procedure-modular\nverification, can be cast naturally in the framework of our abstract contract\ntheory. On the other hand, we also show our contract theory to fulfil the\naxioms of the meta-theory. In this way, we give further evidence for the\nutility of the meta-theory, and prepare the ground for combining our\ninstantiation with other, already existing instantiations.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 13:02:57 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 08:16:29 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Lidstr\u00f6m", "Christian", "", "KTH Royal Institute of\n  Technology, Stockholm, Sweden"], ["Gurov", "Dilian", "", "KTH Royal Institute of\n  Technology, Stockholm, Sweden"]]}, {"id": "2101.06249", "submitter": "Chuta Sano", "authors": "Chuta Sano, Stephanie Balzer, and Frank Pfenning", "title": "Manifestly Phased Communication via Shared Session Types", "comments": "Technical report; 63 pages (30 from Appendix) and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Session types denote message protocols between concurrent processes, allowing\na type-safe formalization of inter-process communication. Although previous\nworks demonstrate a well-defined notion of subtyping where processes have\ndifferent perceptions of the protocol, these formulations were limited to\nlinear session types where each channel of communication has a unique provider\nand client. In our work, we extend these previous formulations into the shared\nsession type setting where channels can now have multiple clients instead of a\nsingle client. We demonstrate that this allows shared sessions to be released\nat a different type, allowing the encoding of phases in a shared protocol to be\nmanifest in the session type.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 18:17:50 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Sano", "Chuta", ""], ["Balzer", "Stephanie", ""], ["Pfenning", "Frank", ""]]}, {"id": "2101.06542", "submitter": "Chandra Maddila", "authors": "Chandra Maddila, Nachiappan Nagappan, Christian Bird, Georgios\n  Gousios, Arie van Deursen", "title": "ConE: A Concurrent Edit Detection Tool for Large ScaleSoftware\n  Development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern, complex software systems are being continuously extended and\nadjusted. The developers responsible for this may come from different teams or\norganizations, and may be distributed over the world. This may make it\ndifficult to keep track of what other developers are doing, which may result in\nmultiple developers concurrently editing the same code areas. This, in turn,\nmay lead to hard-to-merge changes or even merge conflicts, logical bugs that\nare difficult to detect, duplication of work, and wasted developer\nproductivity. To address this, we explore the extent of this problem in the\npull request based software development model. We study half a year of changes\nmade to six large repositories in Microsoft in which at least 1,000 pull\nrequests are created each month. We find that files concurrently edited in\ndifferent pull requests are more likely to introduce bugs. Motivated by these\nfindings, we design, implement, and deploy a service named ConE (Concurrent\nEdit Detector) that proactively detects pull requests containing concurrent\nedits, to help mitigate the problems caused by them. ConE has been designed to\nscale, and to minimize false alarms while still flagging relevant concurrently\nedited files. Key concepts of ConE include the detection of the Extent of\nOverlap between pull requests, and the identification of Rarely Concurrently\nEdited Files. To evaluate ConE, we report on its operational deployment on 234\nrepositories inside Microsoft. ConE assessed 26,000 pull requests and made 775\nrecommendations about conflicting changes, which were rated as useful in over\n70% (554) of the cases. From interviews with 48 users we learned that they\nbelieved ConE would save time in conflict resolution and avoiding duplicate\nwork, and that over 90% intend to keep using the service on a daily basis.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 22:55:44 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 22:05:52 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Maddila", "Chandra", ""], ["Nagappan", "Nachiappan", ""], ["Bird", "Christian", ""], ["Gousios", "Georgios", ""], ["van Deursen", "Arie", ""]]}, {"id": "2101.06757", "submitter": "Matthijs V\\'ak\\'ar", "authors": "Mathieu Huot, Sam Staton, Matthijs V\\'ak\\'ar", "title": "Higher Order Automatic Differentiation of Higher Order Functions", "comments": "arXiv admin note: substantial text overlap with arXiv:2001.02209", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present semantic correctness proofs of automatic differentiation (AD). We\nconsider a forward-mode AD method on a higher order language with algebraic\ndata types, and we characterise it as the unique structure preserving macro\ngiven a choice of derivatives for basic operations. We describe a rich\nsemantics for differentiable programming, based on diffeological spaces. We\nshow that it interprets our language, and we phrase what it means for the AD\nmethod to be correct with respect to this semantics. We show that our\ncharacterisation of AD gives rise to an elegant semantic proof of its\ncorrectness based on a gluing construction on diffeological spaces. We explain\nhow this is, in essence, a logical relations argument. Throughout, we show how\nthe analysis extends to AD methods for computing higher order derivatives using\na Taylor approximation.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 19:24:46 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Huot", "Mathieu", ""], ["Staton", "Sam", ""], ["V\u00e1k\u00e1r", "Matthijs", ""]]}, {"id": "2101.06759", "submitter": "Michael D. Adams", "authors": "Baptiste Saleil and Michael D. Adams", "title": "Proceedings of the 2020 Scheme and Functional Programming Workshop", "comments": "85 pages; 30 figures; workshop website at\n  https://icfp20.sigplan.org/home/scheme-2020", "journal-ref": null, "doi": null, "report-no": "CSE-TR-001-21", "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report aggregates the papers presented at the twenty-first annual Scheme\nand Functional Programming Workshop, hosted on August 28th, 2020, online and\nco-located with the twenty-fifth International Conference on Functional\nProgramming. The Scheme and Functional Programming Workshop is held every year\nto provide an opportunity for researchers and practitioners using Scheme and\nrelated functional programming languages like Racket, Clojure, and Lisp, to\nshare research findings and discuss the future of the Scheme programming\nlanguage.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2021 19:38:54 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Saleil", "Baptiste", ""], ["Adams", "Michael D.", ""]]}, {"id": "2101.07888", "submitter": "EPTCS", "authors": "David I. Spivak (Massachusetts Institute of Technology), Jamie Vicary\n  (University of Cambridge)", "title": "Proceedings of the 3rd Annual International Applied Category Theory\n  Conference 2020", "comments": null, "journal-ref": "EPTCS 333, 2021", "doi": "10.4204/EPTCS.333", "report-no": null, "categories": "cs.DM cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The third annual International Applied Category Theory Conference (ACT2020)\nwas planned to take place at MIT in Cambridge, Massachusetts USA. However, the\nglobal COVID-19 pandemic made the prospect of holding a large in-person meeting\nimpossible, and the event was thus held completely online. Holding the talks\nonline had the new benefits of reducing carbon footprint, being inclusive of\npeople from more parts of the world, and producing higher-quality video talks,\nwhich have been posted online for posterity.\n  The ACT2020 contributions spanned a broad spectrum of application areas,\nincluding databases, dynamical systems, functional programming, game theory,\nlenses, neuroscience, probabilistic programming, natural language processing,\nquantum mechanics, and cyberphysical systems. Papers featured a broad range of\ncategorical techniques.\n  Papers in this Proceedings volume represents about half of the talks\npresented at ACT2020. Being included in the proceedings vs. not is not an\nindication of talk quality, but instead almost exclusively the choice of the\nauthors, e.g. to present work already published elsewhere.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 22:46:00 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Spivak", "David I.", "", "Massachusetts Institute of Technology"], ["Vicary", "Jamie", "", "University of Cambridge"]]}, {"id": "2101.08095", "submitter": "Jesse Sigal", "authors": "Jesse Sigal", "title": "Automatic Differentiation via Effects and Handlers: An Implementation in\n  Frank", "comments": "Appeared as short paper in PEPM'21, see\n  https://www.youtube.com/watch?v=BmBSJFkfL2M for associated talk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic differentiation (AD) is an important family of algorithms which\nenables derivative based optimization. We show that AD can be simply\nimplemented with effects and handlers by doing so in the Frank language. By\nconsidering how our implementation behaves in Frank's operational semantics, we\nshow how our code performs the dynamic creation of programs during evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 12:34:25 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Sigal", "Jesse", ""]]}, {"id": "2101.08116", "submitter": "Francisco Ortin", "authors": "Javier Escalada (1), Ted Scully (2), Francisco Ortin (1 and 2) ((1)\n  University of Oviedo, (2) Cork Institute of Technology)", "title": "Improving type information inferred by decompilers with supervised\n  machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In software reverse engineering, decompilation is the process of recovering\nsource code from binary files. Decompilers are used when it is necessary to\nunderstand or analyze software for which the source code is not available.\nAlthough existing decompilers commonly obtain source code with the same\nbehavior as the binaries, that source code is usually hard to interpret and\ncertainly differs from the original code written by the programmer. Massive\ncodebases could be used to build supervised machine learning models aimed at\nimproving existing decompilers. In this article, we build different\nclassification models capable of inferring the high-level type returned by\nfunctions, with significantly higher accuracy than existing decompilers. We\nautomatically instrument C source code to allow the association of binary\npatterns with their corresponding high-level constructs. A dataset is created\nwith a collection of real open-source applications plus a huge number of\nsynthetic programs. Our system is able to predict function return types with a\n79.1% F1-measure, whereas the best decompiler obtains a 30% F1-measure.\nMoreover, we document the binary patterns used by our classifier to allow their\naddition in the implementation of existing decompilers.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 11:45:46 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 11:01:27 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Escalada", "Javier", "", "1 and 2"], ["Scully", "Ted", "", "1 and 2"], ["Ortin", "Francisco", "", "1 and 2"]]}, {"id": "2101.08181", "submitter": "Julien Lange", "authors": "Mario Bravetti, Julien Lange, Gianluigi Zavattaro", "title": "Fair Refinement for Asynchronous Session Types (extended version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Session types are widely used as abstractions of asynchronous message passing\nsystems. Refinement for such abstractions is crucial as it allows improvements\nof a given component without compromising its compatibility with the rest of\nthe system. In the context of session types, the most general notion of\nrefinement is the asynchronous session subtyping, which allows to anticipate\nmessage emissions but only under certain conditions. In particular,\nasynchronous session subtyping rules out candidates subtypes that occur\nnaturally in communication protocols where, e.g., two parties simultaneously\nsend each other a finite but unspecified amount of messages before removing\nthem from their respective buffers. To address this shortcoming, we study fair\ncompliance over asynchronous session types and fair refinement as the relation\nthat preserves it. This allows us to propose a novel variant of session\nsubtyping that leverages the notion of controllability from service contract\ntheory and that is a sound characterisation of fair refinement. In addition, we\nshow that both fair refinement and our novel subtyping are undecidable. We also\npresent a sound algorithm, and its implementation, which deals with examples\nthat feature potentially unbounded buffering.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 15:29:27 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Bravetti", "Mario", ""], ["Lange", "Julien", ""], ["Zavattaro", "Gianluigi", ""]]}, {"id": "2101.08458", "submitter": "Jian Weng", "authors": "Jian Weng, Animesh Jain, Jie Wang, Leyuan Wang, Yida Wang, and Tony\n  Nowatzki", "title": "UNIT: Unifying Tensorized Instruction Compilation", "comments": "13 pages, 13 figures, and 1 table", "journal-ref": "2021 IEEE/ACM International Symposium on Code Generation and\n  Optimization (CGO), Seoul, Korea (South), 2021, pp. 77-89", "doi": "10.1109/CGO51591.2021.9370330", "report-no": null, "categories": "cs.PL cs.AR cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Because of the increasing demand for computation in DNN, researchers develope\nboth hardware and software mechanisms to reduce the compute and memory burden.\nA widely adopted approach is to use mixed precision data types. However, it is\nhard to leverage mixed precision without hardware support because of the\noverhead of data casting. Hardware vendors offer tensorized instructions for\nmixed-precision tensor operations, like Intel VNNI, Tensor Core, and ARM-DOT.\nThese instructions involve a computing idiom that reduces multiple low\nprecision elements into one high precision element. The lack of compilation\ntechniques for this makes it hard to utilize these instructions: Using\nvendor-provided libraries for computationally-intensive kernels is inflexible\nand prevents further optimizations, and manually writing hardware intrinsics is\nerror-prone and difficult for programmers. Some prior works address this\nproblem by creating compilers for each instruction. This requires excessive\neffort when it comes to many tensorized instructions. In this work, we develop\na compiler framework to unify the compilation for these instructions -- a\nunified semantics abstraction eases the integration of new instructions, and\nreuses the analysis and transformations. Tensorized instructions from different\nplatforms can be compiled via UNIT with moderate effort for favorable\nperformance. Given a tensorized instruction and a tensor operation, UNIT\nautomatically detects the applicability, transforms the loop organization of\nthe operation,and rewrites the loop body to leverage the tensorized\ninstruction. According to our evaluation, UNIT can target various mainstream\nhardware platforms. The generated end-to-end inference model achieves 1.3x\nspeedup over Intel oneDNN on an x86 CPU, 1.75x speedup over Nvidia cuDNN on an\nNvidiaGPU, and 1.13x speedup over a carefully tuned TVM solution for ARM DOT on\nan ARM CPU.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 06:22:58 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 03:36:45 GMT"}, {"version": "v3", "created": "Sun, 28 Mar 2021 04:11:38 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Weng", "Jian", ""], ["Jain", "Animesh", ""], ["Wang", "Jie", ""], ["Wang", "Leyuan", ""], ["Wang", "Yida", ""], ["Nowatzki", "Tony", ""]]}, {"id": "2101.08491", "submitter": "Guilhem Jaber", "authors": "Guilhem Jaber (GALLINETTE, LS2N), Andrzej S. Murawski", "title": "Complete trace models of state and control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a hierarchy of four typed call-by-value languages with either\nhigher-order or ground-type references and with either callcc or no control\noperator.Our first result is a fully abstract trace model for the most\nexpressive setting, featuring both higher-order references and callcc,\nconstructed in the spirit of operational game semantics. Next we examine the\nimpact of suppressing higher-order references and callcc in contexts and\nprovide an operational explanation for the game-semantic conditions known as\nvisibility and bracketing respectively.This allows us to refine the original\nmodel to provide fully abstract trace models of interaction with contexts that\nneed not use higher-order references or callcc. Along the way, we discuss the\nrelationship between error- and termination-based contextual testing in each\ncase, and relate the two to trace and complete trace equivalence\nrespectively.Overall, the paper provides a systematic development of\noperational game semantics for all four cases, which represent the state-based\nface of the so-called semantic cube.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 08:11:08 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Jaber", "Guilhem", "", "GALLINETTE, LS2N"], ["Murawski", "Andrzej S.", ""]]}, {"id": "2101.08611", "submitter": "Ramanathan Srinivasan Thinniyam", "authors": "Rupak Majumdar, Ramanathan S. Thinniyam, Georg Zetzsche", "title": "General Decidability Results for Asynchronous Shared-Memory Programs:\n  Higher-Order and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The model of asynchronous programming arises in many contexts, from low-level\nsystems software to high-level web programming. We take a language-theoretic\nperspective and show general decidability and undecidability results for\nasynchronous programs that capture all known results as well as show\ndecidability of new and important classes. As a main consequence, we show\ndecidability of safety, termination and boundedness verification for\nhigher-order asynchronous programs -- such as OCaml programs using Lwt -- and\nundecidability of liveness verification already for order-2 asynchronous\nprograms. We show that under mild assumptions, surprisingly, safety and\ntermination verification of asynchronous programs with handlers from a language\nclass are decidable iff emptiness is decidable for the underlying language\nclass. Moreover, we show that configuration reachability and liveness (fair\ntermination) verification are equivalent, and decidability of these problems\nimplies decidability of the well-known \"equal-letters\" problem on languages.\nOur results close the decidability frontier for asynchronous programs.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 13:57:22 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Majumdar", "Rupak", ""], ["Thinniyam", "Ramanathan S.", ""], ["Zetzsche", "Georg", ""]]}, {"id": "2101.08720", "submitter": "Alex Dixon", "authors": "Alex Dixon, Ranko Lazi\\'c, Andrzej S. Murawski and Igor Walukiewicz", "title": "Leafy Automata for Higher-Order Concurrency", "comments": "18 pages plus appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finitary Idealized Concurrent Algol (FICA) is a prototypical programming\nlanguage combining functional, imperative, and concurrent computation. There\nexists a fully abstract game model of FICA, which in principle can be used to\nprove equivalence and safety of FICA programs. Unfortunately, the problems are\nundecidable for the whole language, and only very rudimentary decidable\nsub-languages are known. We propose leafy automata as a dedicated\nautomata-theoretic formalism for representing the game semantics of FICA. The\nautomata use an infinite alphabet with a tree structure. We show that the game\nsemantics of any FICA term can be represented by traces of a leafy automaton.\nConversely, the traces of any leafy automaton can be represented by a FICA\nterm. Because of the close match with FICA, we view leafy automata as a\npromising starting point for finding decidable subclasses of the language and,\nmore generally, to provide a new perspective on models of higher-order\nconcurrent computation. Moreover, we identify a fragment of FICA that is\namenable to verification by translation into a particular class of leafy\nautomata. Using a locality property of the latter class, where communication\nbetween levels is restricted and every other level is bounded, we show that\ntheir emptiness problem is decidable by reduction to Petri net reachability.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 17:00:50 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Dixon", "Alex", ""], ["Lazi\u0107", "Ranko", ""], ["Murawski", "Andrzej S.", ""], ["Walukiewicz", "Igor", ""]]}, {"id": "2101.08733", "submitter": "Rosa Abbasi Boroujeni", "authors": "Rosa Abbasi Boroujeni, Jonas Schiffl, Eva Darulova, Mattias Ulbrich,\n  Wolfgang Ahrendt", "title": "Deductive Verification of Floating-Point Java Programs in KeY", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deductive verification has been successful in verifying interesting\nproperties of real-world programs. One notable gap is the limited support for\nfloating-point reasoning. This is unfortunate, as floating-point arithmetic is\nparticularly unintuitive to reason about due to rounding as well as the\npresence of the special values infinity and `Not a Number' (NaN). In this\npaper, we present the first floating-point support in a deductive verification\ntool for the Java programming language. Our support in the KeY verifier handles\narithmetic via floating-point decision procedures inside SMT solvers and\ntranscendental functions via axiomatization. We evaluate this integration on\nnew benchmarks, and show that this approach is powerful enough to prove the\nabsence of floating-point special values -- often a prerequisite for further\nreasoning about numerical computations -- as well as certain functional\nproperties for realistic benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 17:18:01 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Boroujeni", "Rosa Abbasi", ""], ["Schiffl", "Jonas", ""], ["Darulova", "Eva", ""], ["Ulbrich", "Mattias", ""], ["Ahrendt", "Wolfgang", ""]]}, {"id": "2101.08809", "submitter": "Daiyi Peng", "authors": "Daiyi Peng, Xuanyi Dong, Esteban Real, Mingxing Tan, Yifeng Lu,\n  Hanxiao Liu, Gabriel Bender, Adam Kraft, Chen Liang, Quoc V. Le", "title": "PyGlove: Symbolic Programming for Automated Machine Learning", "comments": "NeurIPS 2020 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are sensitive to hyper-parameter and architecture choices.\nAutomated Machine Learning (AutoML) is a promising paradigm for automating\nthese choices. Current ML software libraries, however, are quite limited in\nhandling the dynamic interactions among the components of AutoML. For example,\nefficientNAS algorithms, such as ENAS and DARTS, typically require an\nimplementation coupling between the search space and search algorithm, the two\nkey components in AutoML. Furthermore, implementing a complex search flow, such\nas searching architectures within a loop of searching hardware configurations,\nis difficult. To summarize, changing the search space, search algorithm, or\nsearch flow in current ML libraries usually requires a significant change in\nthe program logic. In this paper, we introduce a new way of programming AutoML\nbased on symbolic programming. Under this paradigm, ML programs are mutable,\nthus can be manipulated easily by another program. As a result, AutoML can be\nreformulated as an automated process of symbolic manipulation. With this\nformulation, we decouple the triangle of the search algorithm, the search space\nand the child program. This decoupling makes it easy to change the search space\nand search algorithm (without and with weight sharing), as well as to add\nsearch capabilities to existing code and implement complex search flows. We\nthen introduce PyGlove, a new Python library that implements this paradigm.\nThrough case studies on ImageNet and NAS-Bench-101, we show that with PyGlove\nusers can easily convert a static program into a search space, quickly iterate\non the search spaces and search algorithms, and craft complex search flows to\nachieve better results.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 19:05:44 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Peng", "Daiyi", ""], ["Dong", "Xuanyi", ""], ["Real", "Esteban", ""], ["Tan", "Mingxing", ""], ["Lu", "Yifeng", ""], ["Liu", "Hanxiao", ""], ["Bender", "Gabriel", ""], ["Kraft", "Adam", ""], ["Liang", "Chen", ""], ["Le", "Quoc V.", ""]]}, {"id": "2101.08939", "submitter": "Kartik Singhal", "authors": "Robert Rand, Aarthi Sundaram, Kartik Singhal, Brad Lackey", "title": "Static Analysis of Quantum Programs via Gottesman Types", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.ET cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Heisenberg representation of quantum operators provides a powerful\ntechnique for reasoning about quantum circuits, albeit those restricted to the\ncommon (non-universal) Clifford set $H$, $S$ and $CNOT$. The Gottesman-Knill\ntheorem showed that we can use this representation to efficiently simulate\nClifford circuits. We show that Gottesman's semantics for quantum programs can\nbe treated as a type system, allowing us to efficiently characterize a common\nsubset of quantum programs. We apply this primarily towards tracking\nentanglement in programs, showing how superdense coding and GHZ circuits\nentangle and disentangle qubits and how to safely dispose of ancillae. We\ndemonstrate the efficiency of our typechecking algorithm both for simple\ndeductions and those involving entanglement and measurement.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 04:07:12 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Rand", "Robert", ""], ["Sundaram", "Aarthi", ""], ["Singhal", "Kartik", ""], ["Lackey", "Brad", ""]]}, {"id": "2101.09031", "submitter": "Gokhan Unel", "authors": "G. Unel, S. Sekmen, A. M. Toon, B. Gokturk, B. Orgen, A. Paul, N.\n  Ravel, J. Setpal", "title": "CutLang V2: towards a unified Analysis Description Language", "comments": "published version", "journal-ref": "Front. Big Data 4:659986, 2021", "doi": "10.3389/fdata.2021.659986", "report-no": null, "categories": "hep-ph cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We will present the latest developments in CutLang, the runtime interpreter\nof a recently-developed analysis description language (ADL) for collider data\nanalysis. ADL is a domain-specific, declarative language that describes the\ncontents of an analysis in a standard and unambiguous way, independent of any\ncomputing framework. In ADL, analyses are written in human-readable plain text\nfiles, separating object, variable and event selection definitions in blocks,\nwith a syntax that includes mathematical and logical operations, comparison and\noptimisation operators, reducers, four-vector algebra and commonly used\nfunctions. Adopting ADLs would bring numerous benefits to the LHC experimental\nand phenomenological communities, ranging from analysis preservation beyond the\nlifetimes of experiments or analysis software to facilitating the abstraction,\ndesign, visualization, validation, combination, reproduction, interpretation\nand overall communication of the analysis contents. Since their initial\nrelease, ADL and CutLang have been used for implementing and running numerous\nLHC analyses. In this process, the original syntax from CutLang v1 has been\nmodified for better ADL compatibility, and the interpreter has been adapted to\nwork with that syntax, resulting in the current release v2. Furthermore,\nCutLang has been enhanced to handle object combinatorics, to include tables and\nweights, to save events at any analysis stage, to benefit from\nmulti-core/multi-CPU hardware among other improvements. In this contribution,\nthese and other enhancements are discussed in details. In addition, real life\nexamples from LHC analyses are presented together with a user manual.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 10:14:40 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 10:00:50 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Unel", "G.", ""], ["Sekmen", "S.", ""], ["Toon", "A. M.", ""], ["Gokturk", "B.", ""], ["Orgen", "B.", ""], ["Paul", "A.", ""], ["Ravel", "N.", ""], ["Setpal", "J.", ""]]}, {"id": "2101.09032", "submitter": "Sidi Mohamed Beillahi", "authors": "Sidi Mohamed Beillahi, Ahmed Bouajjani, and Constantin Enea", "title": "Checking Robustness Between Weak Transactional Consistency Models", "comments": "38 pages, 7 figures, 2 tables, extended version of ESOP 2021\n  conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Concurrent accesses to databases are typically encapsulated in transactions\nin order to enable isolation from other concurrent computations and resilience\nto failures. Modern databases provide transactions with various semantics\ncorresponding to different trade-offs between consistency and availability.\nSince a weaker consistency model provides better performance, an important\nissue is investigating the weakest level of consistency needed by a given\nprogram (to satisfy its specification). As a way of dealing with this issue, we\ninvestigate the problem of checking whether a given program has the same set of\nbehaviors when replacing a consistency model with a weaker one. This property\nknown as robustness generally implies that any specification of the program is\npreserved when weakening the consistency. We focus on the robustness problem\nfor consistency models which are weaker than standard serializability, namely,\ncausal consistency, prefix consistency, and snapshot isolation. We show that\nchecking robustness between these models is polynomial time reducible to a\nstate reachability problem under serializability. We use this reduction to also\nderive a pragmatic proof technique based on Lipton's reduction theory that\nallows to prove programs robust. We have applied our techniques to several\nchallenging applications drawn from the literature of distributed systems and\ndatabases.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 10:17:42 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Beillahi", "Sidi Mohamed", ""], ["Bouajjani", "Ahmed", ""], ["Enea", "Constantin", ""]]}, {"id": "2101.09038", "submitter": "Bas van den Heuvel", "authors": "Bas van den Heuvel and Jorge A. P\\'erez", "title": "A Decentralized Analysis of Multiparty Protocols", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Protocols provide the unifying glue in concurrent and distributed software\ntoday; verifying that message-passing programs conform to such governing\nprotocols is important but difficult. Static approaches based on multiparty\nsession types (MPST) use protocols as types to avoid protocol violations and\ndeadlocks in programs. An elusive problem for MPST is to ensure both protocol\nconformance and deadlock freedom for implementations with interleaved and\ndelegated protocols.\n  We propose a decentralized analysis of multiparty protocols, specified as\nglobal types and implemented as interacting processes in an asynchronous\n$\\pi$-calculus. Our solution rests upon two novel notions: router processes and\nrelative types. While router processes use the global type to enable the\ncomposition of participant implementations in arbitrary process networks,\nrelative types extract from the global type the intended interactions and\ndependencies between pairs of participants. In our analysis, processes are\ntyped using APCP, a type system that ensures protocol conformance and deadlock\nfreedom with respect to binary protocols, developed in prior work. Our\ndecentralized, router-based analysis enables the sound and complete\ntransference of protocol conformance and deadlock freedom from APCP to\nmultiparty protocols.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 10:24:11 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 13:58:59 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Heuvel", "Bas van den", ""], ["P\u00e9rez", "Jorge A.", ""]]}, {"id": "2101.09042", "submitter": "Marie-Christine Jakobs", "authors": "Marie-Christine Jakobs", "title": "PEQcheck: Localized and Context-aware Checking of Functional Equivalence\n  (Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Refactorings must not alter the program's functionality. However, not all\nrefactorings fulfill this requirement. Hence, one must explicitly check that a\nrefactoring does not alter the functionality. Since one rarely has a formal\nspecification of the program's behavior, we utilize the original program as\nfunctional specification. Then, we check whether the original and refactored\nprogram are functionally equivalent. To this end, we apply a common idea and\nreduce equivalence checking to program verification. To increase efficiency,\nour equivalence checker PEQcheck constructs one verification task per\nrefactored code segment instead of one per function as typically done by prior\nwork. In addition, PEQcheck considers the context of the code segments. For\ninstance, only variables that are modified and live are required to be\nequivalent and read-only variables may be shared between original and\nrefactored code segments. We show that PEQcheck is sound.Moreover, our\nevaluation testifies that the localized and context-aware checking performed by\n\\peqcheck can indeed be beneficial.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 10:29:41 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 21:22:55 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Jakobs", "Marie-Christine", ""]]}, {"id": "2101.09408", "submitter": "Shin-Cheng Mu", "authors": "Shin-Cheng Mu", "title": "Equational reasoning for non-determinism monad: the case of Spark\n  aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": "TR-IIS-19-002,Institute of Information Science, Academia Sinica", "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  As part of the author's studies on equational reasoning for monadic programs,\nthis report focus on non-determinism monad.\n  We discuss what properties this monad should satisfy, what additional\noperators and notations can be introduced to facilitate equational reasoning\nabout non-determinism, and put them to the test by proving a number of\nproperties in our example problem inspired by the author's previous work on\nproving properties of Spark aggregation.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 03:17:31 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Mu", "Shin-Cheng", ""]]}, {"id": "2101.09409", "submitter": "Shin-Cheng Mu", "authors": "Shin-Cheng Mu", "title": "Calculating a backtracking algorithm: an exercise in monadic program\n  derivation", "comments": null, "journal-ref": null, "doi": null, "report-no": "TR-IIS-19-003, Institute of Information Science, Academia Sinica", "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Equational reasoning is among the most important tools that functional\nprogramming provides us. Curiously, relatively less attention has been paid to\nreasoning about monadic programs.\n  In this report we derive a backtracking algorithm for problem specifications\nthat use a monadic unfold to generate possible solutions, which are filtered\nusing a $\\mathit{scanl}$-like predicate. We develop theorems that convert a\nvariation of $\\mathit{scanl}$ to a $\\mathit{foldr}$ that uses the state monad,\nas well as theorems constructing hylomorphism. The algorithm is used to solve\nthe $n$-queens puzzle, our running example. The aim is to develop theorems and\npatterns useful for the derivation of monadic programs, focusing on the\nintricate interaction between state and non-determinism.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 03:27:20 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Mu", "Shin-Cheng", ""]]}, {"id": "2101.09619", "submitter": "Pascual Juli\\'an-Iranzo", "authors": "Pascual Juli\\'an-Iranzo and Fernando S\\'aenz-P\\'erez", "title": "Implementing WordNet Measures of Lexical Semantic Similarity in a Fuzzy\n  Logic Programming System", "comments": "To be published in the Journal of Theory and Practice of Logic\n  Programming, 17 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper introduces techniques to integrate WordNet into a Fuzzy Logic\nProgramming system. Since WordNet relates words but does not give graded\ninformation on the relation between them, we have implemented standard\nsimilarity measures and new directives allowing the proximity equations linking\ntwo words to be generated with an approximation degree. Proximity equations are\nthe key syntactic structures which, in addition to a weak unification\nalgorithm, make a flexible query-answering process possible in this kind of\nprogramming language. This addition widens the scope of Fuzzy Logic\nProgramming, allowing certain forms of lexical reasoning, and reinforcing\nNatural Language Processing applications.\n  [Under consideration in Theory and Practice of Logic Programming (TPLP)]\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 01:15:13 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Juli\u00e1n-Iranzo", "Pascual", ""], ["S\u00e1enz-P\u00e9rez", "Fernando", ""]]}, {"id": "2101.09699", "submitter": "Shin-Cheng Mu", "authors": "Shin-Cheng Mu, Tsung-Ju Chiang", "title": "Longest segment of balanced parentheses -- an exercise in program\n  inversion in a segment problem (Functional Pearl)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Given a string of parentheses, the task is to find a longest consecutive\nsegment that is properly bracketed. We find it an interesting problem because\nit involves two techniques: the usual approach for solving segment problems,\nand the converse-of-a-function theorem -- through which we derived an instance\nof shift-reduce parsing.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 12:20:27 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Mu", "Shin-Cheng", ""], ["Chiang", "Tsung-Ju", ""]]}, {"id": "2101.09700", "submitter": "Shin-Cheng Mu", "authors": "Richard Bird, Shin-Cheng Mu", "title": "A greedy algorithm for dropping digits (Functional Pearl)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Consider the puzzle: given a number, remove $k$ digits such that the\nresulting number is as large as possible. Various techniques were employed to\nderive a linear-time solution to the puzzle: predicate logic was used to\njustify the structure of a greedy algorithm, a dependently-typed proof\nassistant was used to give a constructive proof of the greedy condition, and\nequational reasoning was used to calculate the greedy step as well as the\nfinal, linear-time optimisation.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 12:20:59 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Bird", "Richard", ""], ["Mu", "Shin-Cheng", ""]]}, {"id": "2101.09783", "submitter": "Shaowei Zhu", "authors": "Shaowei Zhu, Zachary Kincaid", "title": "Termination Analysis Without the Tears", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining whether a given program terminates is the quintessential\nundecidable problem. Algorithms for termination analysis are divided into two\ngroups: (1) algorithms with strong behavioral guarantees that work in limited\ncircumstances (e.g., complete synthesis of linear ranking functions for\npolyhedral loops [Podelski and Rybalchenko, 2004]), and (2) algorithms that are\nwidely applicable, but have weak behavioral guarantees (e.g., Terminator [Cook\net al., 2006]). This paper investigates the space in between: how can we design\npractical termination analyzers with useful behavioral guarantees?\n  This paper presents a termination analysis that is both compositional (the\nresult of analyzing a composite program is a function of the analysis results\nof its components) and monotone (\"more information into the analysis yields\nmore information out\"). The paper has two key contributions. The first is an\nextension of Tarjan's method for solving path problems in graphs to solve\ninfinite path problems. This provides a foundation upon which to build\ncompositional termination analyses. The second is a collection of monotone\nconditional termination analyses based on this framework. We demonstrate that\nour tool ComPACT (Compositional and Predictable Analysis for Conditional\nTermination) is competitive with state-of-the-art termination tools while\nproviding stronger behavioral guarantees.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 19:53:16 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Zhu", "Shaowei", ""], ["Kincaid", "Zachary", ""]]}, {"id": "2101.10233", "submitter": "Raghavan Komondoor", "authors": "Snigdha Athaiya, Raghavan Komondoor, K Narayan Kumar", "title": "Data Flow Analysis of Asynchronous Systems using Infinite Abstract\n  Domains", "comments": "61 pages, 4 figures, 3 Tables, European Symposium on Programming 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous message-passing systems are employed frequently to implement\ndistributed mechanisms, protocols, and processes. This paper addresses the\nproblem of precise data flow analysis for such systems. To obtain good\nprecision, data flow analysis needs to somehow skip execution paths that read\nmore messages than the number of messages sent so far in the path, as such\npaths are infeasible at run time. Existing data flow analysis techniques do\nelide a subset of such infeasible paths, but have the restriction that they\nadmit only finite abstract analysis domains. In this paper we propose a\ngeneralization of these approaches to admit infinite abstract analysis domains,\nas such domains are commonly used in practice to obtain high precision. We have\nimplemented our approach, and have analyzed its performance on a set of 14\nbenchmarks. On these benchmarks our tool obtains significantly higher precision\ncompared to a baseline approach that does not elide any infeasible paths and to\nanother baseline that elides infeasible paths but admits only finite abstract\ndomains.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 16:49:17 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Athaiya", "Snigdha", ""], ["Komondoor", "Raghavan", ""], ["Kumar", "K Narayan", ""]]}, {"id": "2101.10479", "submitter": "EPTCS", "authors": "Swaraj Dash (University of Oxford), Sam Staton (University of Oxford)", "title": "A Monad for Probabilistic Point Processes", "comments": "In Proceedings ACT 2020, arXiv:2101.07888", "journal-ref": "EPTCS 333, 2021, pp. 19-32", "doi": "10.4204/EPTCS.333.2", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A point process on a space is a random bag of elements of that space. In this\npaper we explore programming with point processes in a monadic style. To this\nend we identify point processes on a space X with probability measures of bags\nof elements in X. We describe this view of point processes using the\ncomposition of the Giry and bag monads on the category of measurable spaces and\nfunctions and prove that this composition also forms a monad using a\ndistributive law for monads. Finally, we define a morphism from a point process\nto its intensity measure, and show that this is a monad morphism. A special\ncase of this monad morphism gives us Wald's Lemma, an identity used to\ncalculate the expected value of the sum of a random number of random variables.\nUsing our monad we define a range of point processes and point process\noperations and compositionally compute their corresponding intensity measures\nusing the monad morphism.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 00:01:21 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Dash", "Swaraj", "", "University of Oxford"], ["Staton", "Sam", "", "University of Oxford"]]}, {"id": "2101.10491", "submitter": "EPTCS", "authors": "Geoffrey Cruttwell, Jonathan Gallagher, Dorette Pronk", "title": "Categorical semantics of a simple differential programming language", "comments": "In Proceedings ACT 2020, arXiv:2101.07888", "journal-ref": "EPTCS 333, 2021, pp. 289-310", "doi": "10.4204/EPTCS.333.20", "report-no": null, "categories": "math.CT cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased interest in machine learning, and deep learning in\nparticular, the use of automatic differentiation has become more wide-spread in\ncomputation. There have been two recent developments to provide the theoretical\nsupport for this types of structure. One approach, due to Abadi and Plotkin,\nprovides a simple differential programming language. Another approach is the\nnotion of a reverse differential category. In the present paper we bring these\ntwo approaches together. In particular, we show how an extension of reverse\nderivative categories models Abadi and Plotkin's language, and describe how\nthis categorical model allows one to consider potential improvements to the\noperational semantics of the language.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 00:08:48 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Cruttwell", "Geoffrey", ""], ["Gallagher", "Jonathan", ""], ["Pronk", "Dorette", ""]]}, {"id": "2101.10720", "submitter": "Harold Pancho Gordon Eliott", "authors": "Harold Pancho Eliott and Martin Berger", "title": "A program logic for fresh name generation", "comments": "15 core pages accepted for publication in FSEN 2021, +60 pages of\n  proofs included in appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a program logic for Pitts and Stark's {\\nu}-calculus, an extension\nof the call-by-value simply-typed {\\lambda}-calculus with a mechanism for the\ngeneration of fresh names. Names can be compared for (in)-equality, producing\nprograms with subtle observable properties. Hidden names produced by\ninteractions between generation and abstraction are captured logically with a\nsecond-order quantifier over type contexts. We illustrate usage of the logic\nthrough reasoning about well-known difficult cases from the literature.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 11:27:42 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 21:51:18 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Eliott", "Harold Pancho", ""], ["Berger", "Martin", ""]]}, {"id": "2101.11030", "submitter": "David Ittah", "authors": "David Ittah, Thomas H\\\"aner, Vadym Kliuchnikov, Torsten Hoefler", "title": "Enabling Dataflow Optimization for Quantum Programs", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.ET cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an IR for quantum computing that directly exposes quantum and\nclassical data dependencies for the purpose of optimization. Our IR consists of\ntwo dialects, one input dialect and one that is specifically tailored to enable\nquantum-classical co-optimization. While the first employs a perhaps more\nintuitive memory-semantics (quantum operations act as side-effects), the latter\nuses value-semantics (operations consume and produce states). Crucially, this\nencodes the dataflow directly in the IR, allowing for a host of optimizations\nthat leverage dataflow analysis. We discuss how to map existing quantum\nprogramming languages to the input dialect and how to lower the resulting IR to\nthe optimization dialect. We present a prototype implementation based on MLIR\nthat includes several quantum-specific optimization passes. Our benchmarks show\nthat significant improvements in resource requirements are possible even\nthrough static optimization. In contrast to circuit optimization at runtime,\nthis is achieved while incurring only a small constant overhead in compilation\ntime, making this a compelling approach for quantum program optimization at\napplication scale.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:01:12 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Ittah", "David", ""], ["H\u00e4ner", "Thomas", ""], ["Kliuchnikov", "Vadym", ""], ["Hoefler", "Torsten", ""]]}, {"id": "2101.11049", "submitter": "Joel Fuentes", "authors": "Guei-Yuan Lueh, Kaiyu Chen, Gang Chen, Joel Fuentes, Wei-Yu Chen,\n  Fangwen Fu, Hong Jiang, Hongzheng Li, Daniel Rhee", "title": "C-for-Metal: High Performance SIMD Programming on Intel GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The SIMT execution model is commonly used for general GPU development. CUDA\nand OpenCL developers write scalar code that is implicitly parallelized by\ncompiler and hardware. On Intel GPUs, however, this abstraction has profound\nperformance implications as the underlying ISA is SIMD and important hardware\ncapabilities cannot be fully utilized. To close this performance gap we\nintroduce C-For-Metal (CM), an explicit SIMD programming framework designed to\ndeliver close-to-the-metal performance on Intel GPUs. The CM programming\nlanguage and its vector/matrix types provide an intuitive interface to exploit\nthe underlying hardware features, allowing fine-grained register management,\nSIMD size control and cross-lane data sharing. Experimental results show that\nCM applications from different domains outperform the best-known SIMT-based\nOpenCL implementations, achieving up to 2.7x speedup on the latest Intel GPU.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:43:50 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Lueh", "Guei-Yuan", ""], ["Chen", "Kaiyu", ""], ["Chen", "Gang", ""], ["Fuentes", "Joel", ""], ["Chen", "Wei-Yu", ""], ["Fu", "Fangwen", ""], ["Jiang", "Hong", ""], ["Li", "Hongzheng", ""], ["Rhee", "Daniel", ""]]}, {"id": "2101.11320", "submitter": "Boro Sitnikovski", "authors": "Boro Sitnikovski", "title": "Tutorial on implementing Hoare logic for imperative programs in Haskell", "comments": "Added sample implementation for H-Consequence, H-While, and another\n  example; Added CoI section, tweaks to labels for 'boptimize'; Improved Hoare\n  logic implementation by relying on actual Propositional calculus and Number\n  theory systems, rather than toy optimization functions; improve formula\n  printer; small tweak updates. Associated files are available at\n  https://github.com/bor0/hoare-imp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the programming language Haskell, we introduce an implementation of\npropositional calculus, number theory, and a simple imperative language that\ncan evaluate arithmetic and boolean expressions. Finally, we provide an\nimplementation of Hoare's logic which will allow us to deduce facts about\nprograms without the need for a full evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 11:14:37 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 15:50:23 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 23:14:03 GMT"}, {"version": "v4", "created": "Sun, 25 Apr 2021 10:34:11 GMT"}, {"version": "v5", "created": "Wed, 28 Apr 2021 20:42:29 GMT"}, {"version": "v6", "created": "Tue, 4 May 2021 18:19:11 GMT"}, {"version": "v7", "created": "Tue, 11 May 2021 06:50:03 GMT"}, {"version": "v8", "created": "Wed, 2 Jun 2021 10:36:34 GMT"}, {"version": "v9", "created": "Sun, 6 Jun 2021 20:19:52 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Sitnikovski", "Boro", ""]]}, {"id": "2101.11351", "submitter": "Dario Stein", "authors": "Dario Stein, Sam Staton", "title": "Compositional Semantics for Probabilistic Programs with Exact\n  Conditioning", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.LO math.CT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a probabilistic programming language for Gaussian random variables\nwith a first-class exact conditioning construct. We give operational,\ndenotational and equational semantics for this language, establishing\nconvenient properties like exchangeability of conditions. Conditioning on\nequality of continuous random variables is nontrivial, as the exact observation\nmay have probability zero; this is Borel's paradox. Using categorical\nformulations of conditional probability, we show that the good properties of\nour language are not particular to Gaussians, but can be derived from universal\nproperties, thus generalizing to wider settings. We define the Cond\nconstruction, which internalizes conditioning as a morphism, providing general\ncompositional semantics for probabilistic programming with exact conditioning.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 12:31:18 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Stein", "Dario", ""], ["Staton", "Sam", ""]]}, {"id": "2101.11365", "submitter": "Alexander McCaskey", "authors": "Alexander McCaskey, Thien Nguyen", "title": "A MLIR Dialect for Quantum Assembly Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the utility of the Multi-Level Intermediate Representation\n(MLIR) for quantum computing. Specifically, we extend MLIR with a new quantum\ndialect that enables the expression and compilation of common quantum assembly\nlanguages. The true utility of this dialect is in its ability to be lowered to\nthe LLVM intermediate representation (IR) in a manner that is adherent to the\nquantum intermediate representation (QIR) specification recently proposed by\nMicrosoft. We leverage a qcor-enabled implementation of the QIR quantum runtime\nAPI to enable a retargetable (quantum hardware agnostic) compiler workflow\nmapping quantum languages to hybrid quantum-classical binary executables and\nobject code. We evaluate and demonstrate this novel compiler workflow with\nquantum programs written in OpenQASM 2.0. We provide concrete examples\ndetailing the generation of MLIR from OpenQASM source files, the lowering\nprocess from MLIR to LLVM IR, and ultimately the generation of executable\nbinaries targeting available quantum processors.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 13:00:39 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["McCaskey", "Alexander", ""], ["Nguyen", "Thien", ""]]}, {"id": "2101.11421", "submitter": "Shin-Cheng Mu", "authors": "Shin-Cheng Mu, Tsung-Ju Chiang", "title": "Deriving monadic quicksort (Declarative Pearl)", "comments": null, "journal-ref": "In Nakano K., Sagonas K. (eds) Functional and Logic Programming\n  (FLOPS 2020). LNCS 12073. pp 124-138. 2020", "doi": "10.1007/978-3-030-59025-3_8", "report-no": null, "categories": "cs.PL cs.DS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To demonstrate derivation of monadic programs, we present a specification of\nsorting using the non-determinism monad, and derive pure quicksort on lists and\nstate-monadic quicksort on arrays. In the derivation one may switch between\npoint-free and pointwise styles, and deploy techniques familiar to functional\nprogrammers such as pattern matching and induction on structures or on sizes.\nDerivation of stateful programs resembles reasoning backwards from the\npostcondition.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 14:15:46 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Mu", "Shin-Cheng", ""], ["Chiang", "Tsung-Ju", ""]]}, {"id": "2101.11501", "submitter": "Adekunle Akinrinmade", "authors": "Emmanuel Adetiba, Temitope John, Adekunle Akinrinmade, Funmilayo\n  Moninuola, Oladipupo Akintade, Joke Badejo", "title": "Evolution of artificial intelligence languages, a systematic literature\n  review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The field of Artificial Intelligence (AI) has undoubtedly received\nsignificant attention in recent years. AI is being adopted to provide solutions\nto problems in fields such as medicine, engineering, education, government and\nseveral other domains. In order to analyze the state of the art of research in\nthe field of AI, we present a systematic literature review focusing on the\nEvolution of AI programming languages. We followed the systematic literature\nreview method by searching relevant databases like SCOPUS, IEEE Xplore and\nGoogle Scholar. EndNote reference manager was used to catalog the relevant\nextracted papers. Our search returned a total of 6565 documents, whereof 69\nstudies were retained. Of the 69 retained studies, 15 documents discussed LISP\nprogramming language, another 34 discussed PROLOG programming language, the\nremaining 20 documents were spread between Logic and Object Oriented\nProgramming (LOOP), ARCHLOG, Epistemic Ontology Language with Constraints\n(EOLC), Python, C++, ADA and JAVA programming languages. This review provides\ninformation on the year of implementation, development team, capabilities,\nlimitations and applications of each of the AI programming languages discussed.\nThe information in this review could guide practitioners and researchers in AI\nto make the right choice of languages to implement their novel AI methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 15:57:04 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Adetiba", "Emmanuel", ""], ["John", "Temitope", ""], ["Akinrinmade", "Adekunle", ""], ["Moninuola", "Funmilayo", ""], ["Akintade", "Oladipupo", ""], ["Badejo", "Joke", ""]]}, {"id": "2101.11730", "submitter": "David Naumann", "authors": "Ramana Nagasamudram and David A. Naumann", "title": "Alignment Completeness for Relational Hoare Logics", "comments": "Minor revision of original. To appear in LICS 2021 but this version\n  has appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational Hoare logics (RHL) provide rules for reasoning about relations\nbetween programs. Several RHLs include a rule we call sequential product that\ninfers a relational correctness judgment from judgments of ordinary Hoare logic\n(HL). Other rules embody sensible patterns of reasoning and have been found\nuseful in practice, but sequential product is relatively complete on its own\n(with HL). As a more satisfactory way to evaluate RHLs, a notion of alignment\ncompleteness is introduced, in terms of the inductive assertion method and\nproduct automata. Alignment completeness results are given to account for\nseveral different sets of rules. The notion may serve to guide the design of\nRHLs and relational verifiers for richer programming languages and alignment\npatterns.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 22:36:28 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 03:07:22 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Nagasamudram", "Ramana", ""], ["Naumann", "David A.", ""]]}, {"id": "2101.12029", "submitter": "Georg Moser", "authors": "Martin Hofmann, Lorenz Leutgeb, Georg Moser, David Obwaller, Florian\n  Zuleger", "title": "Type-Based Analysis of Logarithmic Amortised Complexity", "comments": "35 pages. arXiv admin note: text overlap with arXiv:1807.08242", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel amortised resource analysis couched in a type-and-effect\nsystem. Our analysis is formulated in terms of the physicist's method of\namortised analysis, and is potential-based. The type system makes use of\nlogarithmic potential functions and is the first such system to exhibit\n*logarithmic amortised complexity*. With our approach we target the automated\nanalysis of self-adjusting data structures, like splay trees, which so far have\nonly manually been analysed in the literature. In particular, we have\nimplemented a semi-automated prototype, which successfully analyses the zig-zig\ncase of *splaying*, once the type annotations are fixed.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 14:47:45 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 17:45:07 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Hofmann", "Martin", ""], ["Leutgeb", "Lorenz", ""], ["Moser", "Georg", ""], ["Obwaller", "David", ""], ["Zuleger", "Florian", ""]]}, {"id": "2101.12123", "submitter": "Shankara Narayanan Krishna", "authors": "Adwait Godbole, Shankara Narayanan Krishna, Roland Meyer", "title": "Safety Verification of Parameterized Systems under Release-Acquire", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the safety verification problem for parameterized systems under the\nrelease-acquire (RA) semantics. It has been shown that the problem is\nintractable for systems with unlimited access to atomic compare-and-swap (CAS)\ninstructions. We show that, from a verification perspective where approximate\nresults help, this is overly pessimistic. We study parameterized systems\nconsisting of an unbounded number of environment threads executing identical\nbut CAS-free programs and a fixed number of distinguished threads that are\nunrestricted.\n  Our first contribution is a new semantics that considerably simplifies RA but\nis still equivalent for the above systems as far as safety verification is\nconcerned. We apply this (general) result to two subclasses of our model. We\nshow that safety verification is only \\pspace-complete for the bounded model\nchecking problem where the distinguished threads are loop-free. Interestingly,\nwe can still afford the unbounded environment. We show that the complexity\njumps to \\nexp-complete for thread-modular verification where an unrestricted\ndistinguished `ego' thread interacts with an environment of CAS-free threads\nplus loop-free distinguished threads (as in the earlier setting). Besides the\nusefulness for verification, the results are strong in that they delineate the\ntractability border for an established semantics.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 17:13:14 GMT"}], "update_date": "2021-02-13", "authors_parsed": [["Godbole", "Adwait", ""], ["Krishna", "Shankara Narayanan", ""], ["Meyer", "Roland", ""]]}, {"id": "2101.12299", "submitter": "Enrique Naudon", "authors": "Bhargav Shivkumar, Enrique Naudon and Lukasz Ziarek", "title": "Putting gradual types to work", "comments": null, "journal-ref": "Practical Aspects of Declarative Languages (2021) 54-70", "doi": "10.1007/978-3-030-67438-0", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we describe our experience incorporating gradual types in a\nstatically typed functional language with Hindley-Milner style type inference.\nWhere most gradually typed systems aim to improve static checking in a\ndynamically typed language, we approach it from the opposite perspective and\npromote dynamic checking in a statically typed language. Our approach provides\na glimpse into how languages like SML and OCaml might handle gradual typing. We\ndiscuss our implementation and challenges faced -- specifically how gradual\ntyping rules apply to our representation of composite and recursive types. We\nreview the various implementations that add dynamic typing to a statically\ntyped language in order to highlight the different ways of mixing static and\ndynamic typing and examine possible inspirations while maintaining the gradual\nnature of our type system. This paper also discusses our motivation for adding\ngradual types to our language, and the practical benefits of doing so in our\nindustrial setting.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 22:17:49 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Shivkumar", "Bhargav", ""], ["Naudon", "Enrique", ""], ["Ziarek", "Lukasz", ""]]}, {"id": "2101.12624", "submitter": "Tobias Knopp", "authors": "Tobias Knopp and Mirco Grosser", "title": "MRIReco.jl: An MRI Reconstruction Framework written in Julia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: The aim of this work is to develop a high-performance, flexible and\neasy-to-use MRI reconstruction framework using the scientific programming\nlanguage Julia.\n  Methods: Julia is a modern, general purpose programming language with strong\nfeatures in the area of signal / image processing and numerical computing. It\nhas a high-level syntax but still generates efficient machine code that is\nusually as fast as comparable C/C++ applications. In addition to the language\nfeatures itself, Julia has a sophisticated package management system that makes\nproper modularization of functionality across different packages feasible. Our\ndeveloped MRI reconstruction framework MRIReco.jl can therefore reuse existing\nfunctionality from other Julia packages and concentrate on the MRI-related\nparts. This includes common imaging operators and support for MRI raw data\nformats.\n  Results: MRIReco.jl is a simple to use framework with a high degree of\naccessibility. While providing a simple-to-use interface, many of its\ncomponents can easily be extended and customized. The performance of MRIReco.jl\nis compared to the Berkeley Advanced Reconstruction Toolbox (BART) and we show\nthat the Julia framework achieves comparable reconstruction speed as the\npopular C/C++ library.\n  Conclusion: Modern programming languages can bridge the gap between high\nperformance and accessible implementations. MRIReco.jl leverages this fact and\ncontributes a promising environment for future algorithmic development in MRI\nreconstruction.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 15:05:17 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 08:13:16 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Knopp", "Tobias", ""], ["Grosser", "Mirco", ""]]}]