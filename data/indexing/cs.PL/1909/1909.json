[{"id": "1909.00043", "submitter": "Mario Wenzel", "authors": "Mario Wenzel, Stefan Brass", "title": "Declarative Programming for Microcontrollers -- Datalog on Arduino", "comments": "Part of DECLARE 19 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe an approach to programming microcontrollers based\non the Arduino platform using Datalog as a clear and concise description\nlanguage for system behaviors. The application areas of cheap and easily\nprogrammable microcontrollers, like robotics, home automation, and IoT devices\nhold mainstream appeal and are often used as motivation in natural science and\ntechnology teaching. The choice of programming languages for microcontrollers\nis severely limited, especially with regard to rule-based declarative\nlanguages. We use an approach that is based on the Dedalus language augmented\nwith operations that allow for side-effects and takes the limited resources of\na microcontroller into account. Our compiler and runtime environment allow to\nrun Datalog programs on Arduino-based systems.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 19:21:07 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Wenzel", "Mario", ""], ["Brass", "Stefan", ""]]}, {"id": "1909.00097", "submitter": "Qinxiang Cao", "authors": "Qinshi Wang and Qinxiang Cao", "title": "VST-A: A Foundationally Sound Annotation Verifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An interactive program verification tool usually requires users to write\nformal proofs in a theorem prover like Coq and Isabelle, which is an obstacle\nfor most software engineers. In comparison, annotation verifiers can use\nassertions in source files as hints for program verification but they\nthemselves do not have a formal soundness proof.\n  In this paper, we demonstrate VST-A, a foundationally sound annotation\nverifier for sequential C programs. On one hand, users can write high order\nassertion in C programs' comments. On the other hand, separation logic proofs\nwill be generated in the backend whose proof rules are formally proved sound\nw.r.t. CompCert's Clight semantics. Residue proof goals in Coq may be generated\nif some assertion entailments cannot be verified automatically.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 00:31:41 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Wang", "Qinshi", ""], ["Cao", "Qinxiang", ""]]}, {"id": "1909.00973", "submitter": "Darius Foo", "authors": "Darius Foo, Jason Yeo, Hao Xiao, Asankhaya Sharma", "title": "The Dynamics of Software Composition Analysis", "comments": "ASE 2019, LBR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developers today use significant amounts of open source code, surfacing the\nneed for ways to automatically audit and upgrade library dependencies, and\ngiving rise to the subfield of Software Composition Analysis (SCA). SCA\nproducts are concerned with three tasks: discovering dependencies, checking the\nreachability of vulnerable code for false positive elimination, and automated\nremediation. The latter two tasks rely on call graphs of application and\nlibrary code to check whether vulnerability-specific sinks identified in\nlibraries are used by applications. However, statically-constructed call graphs\nintroduce both false positives and false negatives on real-world projects. In\nthis paper, we develop a novel, modular means of combining call graphs derived\nfrom both static and dynamic analysis to improve the performance of false\npositive elimination. Our experiments indicate significant performance\nimprovements.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 06:31:33 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 04:04:11 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Foo", "Darius", ""], ["Yeo", "Jason", ""], ["Xiao", "Hao", ""], ["Sharma", "Asankhaya", ""]]}, {"id": "1909.00989", "submitter": "Andreas Pavlogiannis", "authors": "Krishnendu Chatterjee and Andreas Pavlogiannis and Viktor Toman", "title": "Value-centric Dynamic Partial Order Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The verification of concurrent programs remains an open challenge, as thread\ninteraction has to be accounted for, which leads to state-space explosion.\nStateless model checking battles this problem by exploring traces rather than\nstates of the program. As there are exponentially many traces, dynamic\npartial-order reduction (DPOR) techniques are used to partition the trace space\ninto equivalence classes, and explore a few representatives from each class.\nThe standard equivalence that underlies most DPOR techniques is the\nhappens-before equivalence, however recent works have spawned a vivid interest\ntowards coarser equivalences. The efficiency of such approaches is a product of\ntwo parameters: (i) the size of the partitioning induced by the equivalence,\nand (ii) the time spent by the exploration algorithm in each class of the\npartitioning.\n  In this work, we present a new equivalence, called value-happens-before and\nshow that it has two appealing features. First, value-happens-before is always\nat least as coarse as the happens-before equivalence, and can be even\nexponentially coarser. Second, the value-happens-before partitioning is\nefficiently explorable when the number of threads is bounded. We present an\nalgorithm called value-centric DPOR (VCDPOR), which explores the underlying\npartitioning using polynomial time per class. Finally, we perform an\nexperimental evaluation of VCDPOR on various benchmarks, and compare it against\nother state-of-the-art approaches. Our results show that value-happens-before\ntypically induces a significant reduction in the size of the underlying\npartitioning, which leads to a considerable reduction in the running time for\nexploring the whole partitioning.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 08:06:03 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Pavlogiannis", "Andreas", ""], ["Toman", "Viktor", ""]]}, {"id": "1909.01465", "submitter": "Kiko Fernandez-Reyes", "authors": "Kiko Fernandez-Reyes and Isaac Oscar Gariano and James Noble and\n  Tobias Wrigstad", "title": "Towards Gradual Checking of Reference Capabilities", "comments": "draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concurrent and parallel programming is difficult due to the presence of\nmemory side-effects, which may introduce data races. Type qualifiers, such as\nreference capabilities, can remove data races by restricting sharing of mutable\ndata. Unfortunately, reference capability languages are an all-in or nothing\ngame, i.e., all the types must be annotated with reference capabilities. In\nthis work in progress, we propose to mix the ideas from the reference\ncapability literature with gradual typing, leading to gradual reference\ncapabilities.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 21:37:55 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 06:44:42 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Fernandez-Reyes", "Kiko", ""], ["Gariano", "Isaac Oscar", ""], ["Noble", "James", ""], ["Wrigstad", "Tobias", ""]]}, {"id": "1909.01640", "submitter": "Ramtine Tofighi-Shirazi", "authors": "Ramtine Tofighi-Shirazi (TL, IF), Irina As\\u{a}voae (TL), Philippe\n  Elbaz-Vincent (IF), Thanh-Ha Le (TL)", "title": "Defeating Opaque Predicates Statically through Machine Learning and\n  Binary Analysis", "comments": null, "journal-ref": "3rd International Workshop on Software PROtection, Nov 2019,\n  London, United Kingdom", "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.PL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach that bridges binary analysis techniques with\nmachine learning classification for the purpose of providing a static and\ngeneric evaluation technique for opaque predicates, regardless of their\nconstructions. We use this technique as a static automated deobfuscation tool\nto remove the opaque predicates introduced by obfuscation mechanisms. According\nto our experimental results, our models have up to 98% accuracy at detecting\nand deob-fuscating state-of-the-art opaque predicates patterns. By contrast,\nthe leading edge deobfuscation methods based on symbolic execution show less\naccuracy mostly due to the SMT solvers constraints and the lack of scalability\nof dynamic symbolic analyses. Our approach underlines the efficiency of hybrid\nsymbolic analysis and machine learning techniques for a static and generic\ndeobfuscation methodology.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 09:19:14 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Tofighi-Shirazi", "Ramtine", "", "TL, IF"], ["As\u0103voae", "Irina", "", "TL"], ["Elbaz-Vincent", "Philippe", "", "IF"], ["Le", "Thanh-Ha", "", "TL"]]}, {"id": "1909.01743", "submitter": "EPTCS", "authors": "Cezar-Constantin Andrici (Alexandru Ioan Cuza University), \\c{S}tefan\n  Ciob\\^ac\\u{a} (Alexandru Ioan Cuza University)", "title": "Verifying the DPLL Algorithm in Dafny", "comments": "In Proceedings FROM 2019, arXiv:1909.00584", "journal-ref": "EPTCS 303, 2019, pp. 3-15", "doi": "10.4204/EPTCS.303.1", "report-no": null, "categories": "cs.LO cs.PL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern high-performance SAT solvers quickly solve large satisfiability\ninstances that occur in practice. If the instance is satisfiable, then the SAT\nsolver can provide a witness which can be checked independently in the form of\na satisfying truth assignment. However, if the instance is unsatisfiable, the\ncertificates could be exponentially large or the SAT solver might not be able\nto output certificates. The implementation of the SAT solver should then be\ntrusted not to contain bugs. However, the data structures and algorithms\nimplemented by a typical high-performance SAT solver are complex enough to\nallow for subtle programming errors. To counter this issue, we build a verified\nSAT solver using the Dafny system. We discuss its implementation in the present\narticle.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 12:48:13 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Andrici", "Cezar-Constantin", "", "Alexandru Ioan Cuza University"], ["Ciob\u00e2c\u0103", "\u015etefan", "", "Alexandru Ioan Cuza University"]]}, {"id": "1909.01745", "submitter": "EPTCS", "authors": "Georgiana Caltais", "title": "Explaining SDN Failures via Axiomatisations", "comments": "In Proceedings FROM 2019, arXiv:1909.00584", "journal-ref": "EPTCS 303, 2019, pp. 48-60", "doi": "10.4204/EPTCS.303.4", "report-no": null, "categories": "cs.FL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a concept of explanations with respect to the violation\nof safe behaviours within software defined networks (SDNs) expressible in\nNetKAT. The latter is a network programming language that is based on a\nwell-studied mathematical structure, namely, Kleene Algebra with Tests (KAT).\nAmongst others, the mathematical foundation of NetKAT gave rise to a sound and\ncomplete equational theory. In our setting, a safe behaviour is characterised\nby a NetKAT policy which does not enable forwarding packets from ingress to an\nundesirable egress. Explanations for safety violations are derived in an\nequational fashion, based on a modification of the existing NetKAT\naxiomatisation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 12:48:54 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Caltais", "Georgiana", ""]]}, {"id": "1909.02457", "submitter": "Tiffany Mintz", "authors": "Tiffany M. Mintz, Alexander J. Mccaskey, Eugene F. Dumitrescu, Shirley\n  V. Moore, Sarah Powers, Pavel Lougovski", "title": "QCOR: A Language Extension Specification for the Heterogeneous\n  Quantum-Classical Model of Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum computing is an emerging computational paradigm that leverages the\nlaws of quantum mechanics to perform elementary logic operations. Existing\nprogramming models for quantum computing were designed with fault-tolerant\nhardware in mind, envisioning standalone applications. However, near-term\nquantum computers are susceptible to noise which limits their standalone\nutility. To better leverage limited computational strengths of noisy quantum\ndevices, hybrid algorithms have been suggested whereby quantum computers are\nused in tandem with their classical counterparts in a heterogeneous fashion.\nThis {\\it modus operandi} calls out for a programming model and a high-level\nprogramming language that natively and seamlessly supports heterogeneous\nquantum-classical hardware architectures in a single-source-code paradigm.\nMotivated by the lack of such a model, we introduce a language extension\nspecification, called QCOR, that enables single-source quantum-classical\nprogramming. Programs written using the QCOR library and directives based\nlanguage extensions can be compiled to produce functional hybrid binary\nexecutables. After defining the QCOR's programming model, memory model, and\nexecution model, we discuss how QCOR enables variational, iterative, and feed\nforward quantum computing. QCOR approaches quantum-classical computation in a\nhardware-agnostic heterogeneous fashion and strives to build on best practices\nof high performance computing (HPC). The high level of abstraction in the\ndeveloped language is intended to accelerate the adoption of quantum computing\nby researchers familiar with classical HPC.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 14:43:15 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Mintz", "Tiffany M.", ""], ["Mccaskey", "Alexander J.", ""], ["Dumitrescu", "Eugene F.", ""], ["Moore", "Shirley V.", ""], ["Powers", "Sarah", ""], ["Lougovski", "Pavel", ""]]}, {"id": "1909.02481", "submitter": "Joseph Near", "authors": "Joseph P. Near, David Darais, Chike Abuah, Tim Stevens, Pranav\n  Gaddamadugu, Lun Wang, Neel Somani, Mu Zhang, Nikhil Sharma, Alex Shan, Dawn\n  Song", "title": "Duet: An Expressive Higher-order Language and Linear Type System for\n  Statically Enforcing Differential Privacy", "comments": "Extended version of OOPSLA 2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past decade, differential privacy has become the gold standard for\nprotecting the privacy of individuals. However, verifying that a particular\nprogram provides differential privacy often remains a manual task to be\ncompleted by an expert in the field. Language-based techniques have been\nproposed for fully automating proofs of differential privacy via type system\ndesign, however these results have lagged behind advances in\ndifferentially-private algorithms, leaving a noticeable gap in programs which\ncan be automatically verified while also providing state-of-the-art bounds on\nprivacy.\n  We propose Duet, an expressive higher-order language, linear type system and\ntool for automatically verifying differential privacy of general-purpose\nhigher-order programs. In addition to general purpose programming, Duet\nsupports encoding machine learning algorithms such as stochastic gradient\ndescent, as well as common auxiliary data analysis tasks such as clipping,\nnormalization and hyperparameter tuning - each of which are particularly\nchallenging to encode in a statically verified differential privacy framework.\n  We present a core design of the Duet language and linear type system, and\ncomplete key proofs about privacy for well-typed programs. We then show how to\nextend Duet to support realistic machine learning applications and recent\nvariants of differential privacy which result in improved accuracy for many\npractical differentially private algorithms. Finally, we implement several\ndifferentially private machine learning algorithms in Duet which have never\nbefore been automatically verified by a language-based tool, and we present\nexperimental results which demonstrate the benefits of Duet's language design\nin terms of accuracy of trained machine learning models.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 15:35:07 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Near", "Joseph P.", ""], ["Darais", "David", ""], ["Abuah", "Chike", ""], ["Stevens", "Tim", ""], ["Gaddamadugu", "Pranav", ""], ["Wang", "Lun", ""], ["Somani", "Neel", ""], ["Zhang", "Mu", ""], ["Sharma", "Nikhil", ""], ["Shan", "Alex", ""], ["Song", "Dawn", ""]]}, {"id": "1909.02599", "submitter": "R. Ghosh", "authors": "Prashant Kumar and R. K. Ghosh", "title": "Formal Methods and Event Notification Systems in Mobile Computing\n  Environment", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this report, we have explored the issues associated with the specification\nof event-based systems in a mobile environment using Unity \\cite{unity}. We\nused a few constructs and concepts from Mobile UNITY which was proposed as an\nextension of UNITY by Roman and McCann \\cite{intro}. Our aim in this report is\nto show that some of the constructs proposed in Mobile UNITY are not\nunnecessary. Those constructs are overly powerful and put a hindrance on the\nmapping from UNITY specification to particular architectures, which is one of\nthe key simplicity of UNITY specification. Using an example of a message-based\nevent notification system we have shown that a system with a simple\nmodification to the structure of assign section of the UNITY programs could\nserve well in mapping and implementation at the same time preserve the small\nand compact proof logic of UNITY.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 04:31:48 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Kumar", "Prashant", ""], ["Ghosh", "R. K.", ""]]}, {"id": "1909.03110", "submitter": "Joseph Spitzer", "authors": "Joseph Spitzer, Joydeep Biswas, and Arjun Guha", "title": "Making High-Performance Robots Safe and Easy to Use for an Introduction\n  to Computing", "comments": "8 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.PL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots are a popular platform for introducing computing and artificial\nintelligence to novice programmers. However, programming state-of-the-art\nrobots is very challenging, and requires knowledge of concurrency, operation\nsafety, and software engineering skills, which can take years to teach. In this\npaper, we present an approach to introducing computing that allows students to\nsafely and easily program high-performance robots. We develop a platform for\nstudents to program RoboCup Small Size League robots using JavaScript. The\nplatform 1) ensures physical safety at several levels of abstraction, 2) allows\nstudents to program robots using the JavaScript in the browser, without the\nneed to install software, and 3) presents a simplified JavaScript semantics\nthat shields students from confusing language features. We discuss our\nexperience running a week-long workshop using this platform, and analyze over\n3,000 student-written program revisions to provide empirical evidence that our\napproach does help students.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 20:00:07 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 01:36:06 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Spitzer", "Joseph", ""], ["Biswas", "Joydeep", ""], ["Guha", "Arjun", ""]]}, {"id": "1909.03289", "submitter": "Martin Sulzmann", "authors": "Martin Sulzmann and Kai Stadtm\\\"uller", "title": "Predicting All Data Race Pairs for a Specific Schedule (extended\n  version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of data race prediction where the program's behavior\nis represented by a trace. A trace is a sequence of program events recorded\nduring the execution of the program.\n  We employ the schedulable happens-before relation to characterize all pairs\nof events that are in a race for the schedule as manifested in the trace.\n  Compared to the classic happens-before relation, the schedulable\nhappens-before relations properly takes care of write-read dependencies and\nthus avoids false positives.\n  The challenge is to efficiently identify all (schedulable) data race pairs.\n  We present a refined linear time vector clock algorithm to predict many of\nthe schedulable data race pairs.\n  We introduce a quadratic time post-processing algorithm to predict all\nremaining data race pairs.\n  This improves the state of the art in the area and our experiments show that\nour approach scales to real-world examples.\n  Thus, the user can systematically examine and fix all program locations that\nare in a race for a particular schedule.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 15:30:44 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Sulzmann", "Martin", ""], ["Stadtm\u00fcller", "Kai", ""]]}, {"id": "1909.03291", "submitter": "Torsten Grust", "authors": "Christian Duta, Denis Hirn, Torsten Grust", "title": "Compiling PL/SQL Away", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"PL/SQL functions are slow,\" is common developer wisdom that derives from the\ntension between set-oriented SQL evaluation and statement-by-statement PL/SQL\ninterpretation. We pursue the radical approach of compiling PL/SQL away,\nturning interpreted functions into regular subqueries that can then be\nefficiently evaluated together with their embracing SQL query, avoiding any\nPL/SQL to SQL context switches. Input PL/SQL functions may exhibit arbitrary\ncontrol flow. Iteration, in particular, is compiled into SQL-level recursion.\nRDBMSs across the board reward this compilation effort with significant run\ntime savings that render established developer lore questionable.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 15:42:22 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Duta", "Christian", ""], ["Hirn", "Denis", ""], ["Grust", "Torsten", ""]]}, {"id": "1909.03523", "submitter": "Michael Coblenz", "authors": "Michael Coblenz, Reed Oei, Tyler Etzel, Paulette Koronkevich, Miles\n  Baker, Yannick Bloem, Brad A. Myers, Joshua Sunshine, Jonathan Aldrich", "title": "Obsidian: Typestate and Assets for Safer Blockchain Programming", "comments": "Working draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain platforms are coming into broad use for processing critical\ntransactions among participants who have not established mutual trust. Many\nblockchains are programmable, supporting smart contracts, which maintain\npersistent state and support transactions that transform the state.\nUnfortunately, bugs in many smart contracts have been exploited by hackers.\nObsidian is a novel programming language with a type system that enables static\ndetection of bugs that are common in smart contracts today. Obsidian is based\non a core calculus, Silica, for which we proved type soundness. Obsidian uses\ntypestate to detect improper state manipulation and uses linear types to detect\nabuse of assets. We describe two case studies that evaluate Obsidian's\napplicability to the domains of parametric insurance and supply chain\nmanagement, finding that Obsidian's type system facilitates reasoning about\nhigh-level states and ownership of resources. We compared our Obsidian\nimplementation to a Solidity implementation, observing that the Solidity\nimplementation requires much boilerplate checking and tracking of state,\nwhereas Obsidian does this work statically.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 18:24:26 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Coblenz", "Michael", ""], ["Oei", "Reed", ""], ["Etzel", "Tyler", ""], ["Koronkevich", "Paulette", ""], ["Baker", "Miles", ""], ["Bloem", "Yannick", ""], ["Myers", "Brad A.", ""], ["Sunshine", "Joshua", ""], ["Aldrich", "Jonathan", ""]]}, {"id": "1909.03658", "submitter": "Lse Lse", "authors": "Thomas Dupriez (RMOD), Guillermo Polito (RMOD), Steven Costiou (RMOD),\n  Vincent Aranega (RMOD), St\\'ephane Ducasse (RMOD)", "title": "Sindarin: A Versatile Scripting API for the Pharo Debugger", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Debugging is one of the most important and time consuming activities in\nsoftware maintenance, yet mainstream debuggers are not well-adapted to several\ndebugging scenarios. This has led to the research of new techniques covering\nspecific families of complex bugs. Notably, recent research proposes to empower\ndevelopers with scripting DSLs, plugin-based and moldable debuggers. However,\nthese solutions are tailored to specific use-cases, or too costly for\none-time-use scenarios. In this paper we argue that exposing a debugging\nscripting interface in mainstream debuggers helps in solving many challenging\ndebugging scenarios. For this purpose, we present Sindarin, a scripting API\nthat eases the expression and automation of different strategies developers\npursue during their debugging sessions. Sindarin provides a GDB-like API,\naugmented with AST-bytecode-source code mappings and object-centric\ncapabilities. To demonstrate the versatility of Sindarin, we reproduce several\nadvanced breakpoints and non-trivial debugging mechanisms from the literature.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 06:59:44 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Dupriez", "Thomas", "", "RMOD"], ["Polito", "Guillermo", "", "RMOD"], ["Costiou", "Steven", "", "RMOD"], ["Aranega", "Vincent", "", "RMOD"], ["Ducasse", "St\u00e9phane", "", "RMOD"]]}, {"id": "1909.03721", "submitter": "Carla Ferreira", "authors": "Filipe Meirim and M\\'ario Pereira and Carla Ferreira", "title": "CISE3: Verifica\\c{c}\\~ao de aplica\\c{c}\\~oes com consist\\^encia fraca em\n  Why3", "comments": "Article in Portuguese, accepted in the national informatics\n  conference INForum 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we present a tool for the verification of programs built on\ntop replicated databases. The tool evaluates a sequential specification and\ndeduces which operations need to be synchronized for the program to function\nproperly in a distributed environment. Our prototype is built over the\ndeductive verification platform Why3. The Why3 Framework provides a\nsophisticated user experience, the possibility to scale to realistic case\nstudies, as well as a high degree of automation. A case study is presented and\ndiscussed, with the purpose of experimentally validating our approach.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 09:36:56 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Meirim", "Filipe", ""], ["Pereira", "M\u00e1rio", ""], ["Ferreira", "Carla", ""]]}, {"id": "1909.03741", "submitter": "Carla Ferreira", "authors": "Miguel Loureiro and Lu\\'isa Louren\\c{c}o and L\\'ucio Ferr\\~ao and\n  Carla Ferreira", "title": "An\\'alise de Seguran\\c{c}a Baseada em Roles para F\\'abricas de Software", "comments": "Article in Portuguese, accepted in the national informatics\n  conference INForum 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most software factories contain applications with sensitive information that\nneeds to be protected against breaches of confidentiality and integrity, which\ncan have serious consequences. In the context of large factories with complex\napplications, it is not feasible to manually analyze accesses to sensitive\ninformation without some form of safety mechanisms. This article presents a\nstatic analysis technique for software factories, based on role-based security\npolicies. We start by synthesising a graph representation of the relevant\nsoftware factories, based on the security policy defined by the user. Later the\ngraph model is analysed to find access information where the security policy is\nbreached, ensuring that all possible execution states are analysed. A proof of\nconcept of our technique has been developed for the analysis of OutSystems\nsoftware factories. The security reports generated by the tool allows\ndevelopers to find and prioritise security breaches in their factories. The\nprototype was evaluated using large software factories, with strong safety\nrequirements. Several security flaws were found, some serious ones that would\nbe hard to be detected without our analysis.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 10:15:48 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Loureiro", "Miguel", ""], ["Louren\u00e7o", "Lu\u00edsa", ""], ["Ferr\u00e3o", "L\u00facio", ""], ["Ferreira", "Carla", ""]]}, {"id": "1909.04160", "submitter": "Pavel Kalvoda", "authors": "Pavel Kalvoda, Tom Sydney Kerckhove", "title": "Structural and semantic pattern matching analysis in Haskell", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Haskell functions are defined as a series of clauses consisting of patterns\nthat are matched against the arguments in the order of definition. In case an\ninput is not matched by any of the clauses, an error occurs. Therefore it is\ndesirable to statically prove that the function is defined for all well-typed\ninputs. Conversely, a clause that can never be matched also indicates a likely\ndefect. Analyzing these properties is challenging due to presence of GADT and\nguards as well as due to Haskell's lazy evaluation. We implement a recently\nproposed algorithm that unifies and extends the related analyses implemented in\nversion 7 of the Glasgow Haskell Compiler. By using an SMT solver to handle the\nsemantic constraints arising from pattern guards, we achieve a further\nimprovement in precision over the existing GHC 8.0.1 implementation. We present\na tool that uses the analysis to give sound, more precise, and actionable\nwarnings about program defects.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 21:13:38 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Kalvoda", "Pavel", ""], ["Kerckhove", "Tom Sydney", ""]]}, {"id": "1909.04374", "submitter": "Gregory Stock", "authors": "Gregory Stock, Sebastian Hahn, Jan Reineke", "title": "Cache Persistence Analysis: Finally Exact", "comments": "Technical Report RTSS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cache persistence analysis is an important part of worst-case execution time\n(WCET) analysis. It has been extensively studied in the past twenty years.\nDespite these efforts, all existing persistence analyses are approximative in\nthe sense that they are not guaranteed to find all persistent memory blocks.\n  In this paper, we close this gap by introducing the first exact persistence\nanalysis for caches with least-recently-used (LRU) replacement. To this end, we\nfirst introduce an exact abstraction that exploits monotonicity properties of\nLRU to significantly reduce the information the analysis needs to maintain for\nexact persistence classifications. We show how to efficiently implement this\nabstraction using zero-suppressed binary decision diagrams (ZDDs) and introduce\nnovel techniques to deal with uncertainty that arises during the analysis of\ndata caches.\n  The experimental evaluation demonstrates that the new exact analysis is\ncompetitive with state-of-the-art inexact analyses in terms of both memory\nconsumption and analysis run time, which is somewhat surprising as we show that\npersistence analysis is NP-complete. We also observe that while prior analyses\nare not exact in theory they come close to being exact in practice.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 09:49:57 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Stock", "Gregory", ""], ["Hahn", "Sebastian", ""], ["Reineke", "Jan", ""]]}, {"id": "1909.04870", "submitter": "Sven L\\\"offler", "authors": "Salvador Abreu, Petra Hofstedt, Ulrich John, Herbert Kuchen, Dietmar\n  Seipel", "title": "Pre-proceedings of the DECLARE 2019 Conference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume constitutes the pre-proceedings of the DECLARE 2019 conference,\nheld on September 9 to 13, 2019 at the University of Technology Cottbus -\nSenftenberg (Germany).\n  Declarative programming is an advanced paradigm for the modeling and solving\nof complex problems. This method has attracted increased attention over the\nlast decades, e.g., in the domains of data and knowledge engineering,\ndatabases, artificial intelligence, natural language processing, modeling and\nprocessing combinatorial problems, and for establishing systems for the web.\n  The conference DECLARE 2019 aims at cross-fertilizing exchange of ideas and\nexperiences among researches and students from the different communities\ninterested in the foundations, implementation techniques, novel applications,\nand combinations of high-level, declarative programming and related areas. The\ntechnical program of the event included invited talks, presentations of\nrefereed papers, and system demonstrations. DECLARE 2019 consisted of the\nsub-events INAP, WFLP, and WLP:\n  INAP - 22nd International Conference on Applications of Declarative\nProgramming and Knowledge Management WFLP - 27th International Workshop on\nFunctional and (Constraint) Logic Programming WLP - 33rd Workshop on\n(Constraint) Logic Programming\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 06:40:02 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 11:15:07 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 08:24:27 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Abreu", "Salvador", ""], ["Hofstedt", "Petra", ""], ["John", "Ulrich", ""], ["Kuchen", "Herbert", ""], ["Seipel", "Dietmar", ""]]}, {"id": "1909.05027", "submitter": "\\'Eric Tanter", "authors": "Nicolas Tabareau, \\'Eric Tanter, Matthieu Sozeau", "title": "The Marriage of Univalence and Parametricity", "comments": "Journal of the ACM camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reasoning modulo equivalences is natural for everyone, including\nmathematicians. Unfortunately, in proof assistants based on type theory,\nequality is appallingly syntactic and, as a result, exploiting equivalences is\ncumbersome at best. Parametricity and univalence are two major concepts that\nhave been explored to transport programs and proofs across type equivalences,\nbut they fall short of achieving seamless, automatic transport. This work first\nclarifies the limitations of these two concepts in isolation, and then devises\na fruitful marriage between both. The resulting concept, univalent\nparametricity, is an heterogeneous extension of parametricity strengthened with\nunivalence that fully realizes programming and proving modulo equivalences. In\naddition to the theory of univalent parametricity, we present a lightweight\nframework implemented in Coq that allows the user to transparently transfer\ndefinitions and theorems for a type to an equivalent one, as if they were\nequal. For instance, this makes it possible to conveniently switch between an\neasy-to-reason-about representation and a computationally-efficient\nrepresentation, as soon as they are proven equivalent. The combination of\nparametricity and univalence supports transport \\`a la carte: basic univalent\ntransport, which stems from a type equivalence, can be complemented with\nadditional proofs of equivalences between functions over these types, in order\nto be able to lift more programs and proofs, as well as to yield more efficient\nterms. We illustrate the use of univalent parametricity on several examples,\nincluding a recent integration of native integers in Coq.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 13:04:46 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 13:37:41 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 21:16:28 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Tabareau", "Nicolas", ""], ["Tanter", "\u00c9ric", ""], ["Sozeau", "Matthieu", ""]]}, {"id": "1909.05076", "submitter": "Ryan Bernstein", "authors": "Ryan Bernstein", "title": "Static Analysis for Probabilistic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic programming is a powerful abstraction for statistical machine\nlearning. Applying static analysis methods to probabilistic programs could\nserve to optimize the learning process, automatically verify properties of\nmodels, and improve the programming interface for users. This field of static\nanalysis for probabilistic programming (SAPP) is young and unorganized,\nconsisting of a constellation of techniques with various goals and limitations.\nThe primary aim of this work is to synthesize the major contributions of the\nSAPP field within an organizing structure and context. We provide technical\nbackground for static analysis and probabilistic programming, suggest a\nfunctional taxonomy for probabilistic programming languages, and analyze the\napplicability of major ideas in the SAPP field. We conclude that, while current\nstatic analysis techniques for probabilistic programs have practical\nlimitations, there are a number of future directions with high potential to\nimprove the state of statistical machine learning.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 15:34:10 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Bernstein", "Ryan", ""]]}, {"id": "1909.05242", "submitter": "EPTCS", "authors": "Massimo Bartoletti (University of Cagliari, Italy), Ludovic Henrio\n  (CNRS, LIP, Lyon, France), Anastasia Mavridou (NASA Ames, USA), Alceste\n  Scalas (Aston University, Birmingham, UK)", "title": "Proceedings 12th Interaction and Concurrency Experience", "comments": null, "journal-ref": "EPTCS 304, 2019", "doi": "10.4204/EPTCS.304", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of ICE'19, the 12th Interaction and\nConcurrency Experience, which was held in Copenhagen, Denmark on the 20th and\n21st of June 2019, as a satellite event of DisCoTec'19. The ICE workshop series\nfeatures a distinguishing review and selection procedure, allowing PC members\nto interact anonymously with authors. As in the past 11 editions, this\ninteraction considerably improved the accuracy of the feedback from the\nreviewers and the quality of accepted papers, and offered the basis for lively\ndiscussion during the workshop. The 2019 edition of ICE included double blind\nreviewing of original research papers, in order to increase fairness and avoid\nbias in reviewing. Each paper was reviewed by three PC members, and altogether\n9 papers were accepted for publication - plus 2 oral presentations which are\nnot part of this volume. We were proud to host 4 invited talks, by Dilian\nGurov, Fritz Henglein, Sophia Knight, and Hern\\'an Melgratti. The abstracts of\nthese talks are included in this volume together with the regular papers. Final\nversions of the contributions, taking into account the discussion at the\nworkshop, are included.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 17:57:03 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Bartoletti", "Massimo", "", "University of Cagliari, Italy"], ["Henrio", "Ludovic", "", "CNRS, LIP, Lyon, France"], ["Mavridou", "Anastasia", "", "NASA Ames, USA"], ["Scalas", "Alceste", "", "Aston University, Birmingham, UK"]]}, {"id": "1909.05339", "submitter": "Karl Cronburg", "authors": "Karl Cronburg, Samuel Z. Guyer", "title": "Floorplan: Spatial Layout in Memory Management Systems", "comments": "Accepted for publication at Generative Programming: Concepts &\n  Experiences (GPCE) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern runtime systems, memory layout calculations are hand-coded in\nsystems languages. Primitives in these languages are not powerful enough to\ndescribe a rich set of layouts, leading to reliance on ad-hoc macros, numerous\ninterrelated static constants, and other boilerplate code. Memory management\npolicies must also carefully orchestrate their application of address\ncalculations in order to modify memory cooperatively, a task ill-suited to\nlow-level systems languages at hand which lack proper safety mechanisms.\n  In this paper we introduce Floorplan, a declarative language for specifying\nhigh level memory layouts. Constraints formerly implemented by describing how\nto compute locations are, in Floorplan, defined declaratively using explicit\nlayout constructs. The challenge here was to discover constructs capable of\nsufficiently enabling the automatic generation of address calculations.\nFloorplan is implemented as a compiler for generating a Rust library. In a case\nstudy of an existing implementation of the immix garbage collection algorithm,\nFloorplan eliminates 55 out of the 63 unsafe lines of code: 100\\% of unsafe\nlines pertaining to memory safety.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 20:24:18 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Cronburg", "Karl", ""], ["Guyer", "Samuel Z.", ""]]}, {"id": "1909.05464", "submitter": "Andrei Arusoaie", "authors": "Andrei Arusoaie", "title": "A Formal Semantics of Findel in Coq (Short Paper)", "comments": "Presented in FROM 2019: http://from2019.projects.uvt.ro/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first formal semantics of Findel - a DSL for specifying\nfinancial derivatives. The semantics is encoded in Coq, and we use it to prove\nproperties of several Findel contracts.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 05:52:35 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Arusoaie", "Andrei", ""]]}, {"id": "1909.05581", "submitter": "Isaac Oscar Gariano", "authors": "Isaac Oscar Gariano, Richard Roberts, Stefan Marr, Michael Homer,\n  James Noble", "title": "Which of My Transient Type Checks Are Not (Almost) Free?", "comments": null, "journal-ref": null, "doi": "10.1145/3358504.3361232", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One form of type checking used in gradually typed language is transient type\nchecking: whenever an object 'flows' through code with a type annotation, the\nobject is dynamically checked to ensure it has the methods required by the\nannotation. Just-in-time compilation and optimisation in virtual machines can\neliminate much of the overhead of run-time transient type checks. Unfortunately\nthis optimisation is not uniform: some type checks will significantly decrease,\nor even increase, a program's performance.\n  In this paper, we refine the so called \"Takikawa\" protocol, and use it to\nidentify which type annotations have the greatest effects on performance. In\nparticular, we show how graphing the performance of such benchmarks when\nvarying which type annotations are present in the source code can be used to\ndiscern potential patterns in performance. We demonstrate our approach by\ntesting the Moth virtual machine: for many of the benchmarks where Moth's\ntransient type checking impacts performance, we have been able to identify one\nor two specific type annotations that are the likely cause. Without these type\nannotations, the performance impact of transient type checking becomes\nnegligible.\n  Using our technique programmers can optimise programs by removing expensive\ntype checks, and VM engineers can identify new opportunities for compiler\noptimisation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 11:46:09 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Gariano", "Isaac Oscar", ""], ["Roberts", "Richard", ""], ["Marr", "Stefan", ""], ["Homer", "Michael", ""], ["Noble", "James", ""]]}, {"id": "1909.05951", "submitter": "Sung Kook Kim", "authors": "Sung Kook Kim, Arnaud J. Venet, Aditya V. Thakur", "title": "Deterministic Parallel Fixpoint Computation", "comments": "Published in POPL 2020. Code is available at\n  https://github.com/95616ARG/pikos_popl2020", "journal-ref": null, "doi": "10.1145/3371082", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract interpretation is a general framework for expressing static program\nanalyses. It reduces the problem of extracting properties of a program to\ncomputing an approximation of the least fixpoint of a system of equations. The\nde facto approach for computing this approximation uses a sequential algorithm\nbased on weak topological order (WTO). This paper presents a deterministic\nparallel algorithm for fixpoint computation by introducing the notion of weak\npartial order (WPO). We present an algorithm for constructing a WPO in\nalmost-linear time. Finally, we describe PIKOS, our deterministic parallel\nabstract interpreter, which extends the sequential abstract interpreter IKOS.\nWe evaluate the performance and scalability of PIKOS on a suite of 1017 C\nprograms. When using 4 cores, PIKOS achieves an average speedup of 2.06x over\nIKOS, with a maximum speedup of 3.63x. When using 16 cores, PIKOS achieves a\nmaximum speedup of 10.97x.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 21:02:58 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 02:10:03 GMT"}, {"version": "v3", "created": "Thu, 5 Dec 2019 03:47:44 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Kim", "Sung Kook", ""], ["Venet", "Arnaud J.", ""], ["Thakur", "Aditya V.", ""]]}, {"id": "1909.05964", "submitter": "Ashish Tiwari", "authors": "Sumit Gulwani and Kunal Pathak and Arjun Radhakrishna and Ashish\n  Tiwari and Abhishek Udupa", "title": "Quantitative Programming by Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming-by-Example (PBE) systems synthesize an intended program in some\n(relatively constrained) domain-specific language from a small number of\ninput-output examples provided by the user. In this paper, we motivate and\ndefine the problem of quantitative PBE (qPBE) that relates to synthesizing an\nintended program over an underlying (real world) programming language that also\nminimizes a given quantitative cost function. We present a modular approach for\nsolving qPBE that consists of three phases: intent disambiguation, global\nsearch, and local search. On two concrete objectives, namely program\nperformance and size, our qPBE procedure achieves $1.53 X$ and $1.26 X$\nimprovement respectively over the baseline FlashFill PBE system, averaged over\n$701$ benchmarks. Our detailed experiments validate the design of our procedure\nand show the value of combining global and local search for qPBE.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 21:55:00 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Gulwani", "Sumit", ""], ["Pathak", "Kunal", ""], ["Radhakrishna", "Arjun", ""], ["Tiwari", "Ashish", ""], ["Udupa", "Abhishek", ""]]}, {"id": "1909.05969", "submitter": "EPTCS", "authors": "Maurizio Murgia (Universit\\`a degli Studi di Cagliari)", "title": "A Note On Compliance Relations And Fixed Points", "comments": "In Proceedings ICE 2019, arXiv:1909.05242", "journal-ref": "EPTCS 304, 2019, pp. 38-47", "doi": "10.4204/EPTCS.304.3", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study compliance relations between behavioural contracts in a syntax\nindependent setting based on Labelled Transition Systems. We introduce a\nfix-point based family of compliance relations, and show that many compliance\nrelations appearing in literature belong to this family.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 22:23:43 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Murgia", "Maurizio", "", "Universit\u00e0 degli Studi di Cagliari"]]}, {"id": "1909.05970", "submitter": "EPTCS", "authors": "Wen Kokke (University of Edinburgh)", "title": "Rusty Variation: Deadlock-free Sessions with Failure in Rust", "comments": "In Proceedings ICE 2019, arXiv:1909.05242", "journal-ref": "EPTCS 304, 2019, pp. 48-60", "doi": "10.4204/EPTCS.304.4", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rusty Variation (RV) is a library for session-typed communication in Rust\nwhich offers strong compile-time correctness guarantees. Programs written using\nRV are guaranteed to respect a specified protocol, and are guaranteed to be\nfree from deadlocks and races.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 22:24:02 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Kokke", "Wen", "", "University of Edinburgh"]]}, {"id": "1909.05971", "submitter": "EPTCS", "authors": "Matteo Cimini (University of Massachusetts Lowell)", "title": "Towards Gradually Typed Capabilities in the Pi-Calculus", "comments": "In Proceedings ICE 2019, arXiv:1909.05242", "journal-ref": "EPTCS 304, 2019, pp. 61-76", "doi": "10.4204/EPTCS.304.5", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradual typing is an approach to integrating static and dynamic typing within\nthe same language, and puts the programmer in control of which regions of code\nare type checked at compile-time and which are type checked at run-time.\n  In this paper, we focus on the pi-calculus equipped with types for the\nmodeling of input-output capabilities of channels. We present our preliminary\nwork towards a gradually typed version of this calculus. We present a type\nsystem, a cast insertion procedure that automatically inserts run-time checks,\nand an operational semantics of a pi-calculus that handles casts on channels.\nAlthough we do not claim any theoretical results on our formulations, we\ndemonstrate our calculus with an example and discuss our future plans.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 22:24:26 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Cimini", "Matteo", "", "University of Massachusetts Lowell"]]}, {"id": "1909.06215", "submitter": "Krzysztof R. Apt", "authors": "Krzysztof R. Apt and Frank S. de Boer", "title": "Reasoning about call-by-value: a missing result in the history of\n  Hoare's logic", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a sound and relatively complete Hoare-like proof system for\nreasoning about partial correctness of recursive procedures in presence of\nlocal variables and the call-by-value parameter mechanism, and in which the\ncorrectness proofs are linear in the length of the program. We argue that in\nspite of the fact that Hoare-like proof systems for recursive procedures were\nintensively studied, no such proof system has been proposed in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 13:18:54 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Apt", "Krzysztof R.", ""], ["de Boer", "Frank S.", ""]]}, {"id": "1909.06228", "submitter": "S VenkataKeerthy", "authors": "S. VenkataKeerthy, Rohit Aggarwal, Shalini Jain, Maunendra Sankar\n  Desarkar, Ramakrishna Upadrasta and Y. N. Srikant", "title": "IR2Vec: LLVM IR based Scalable Program Embeddings", "comments": "Accepted in ACM TACO", "journal-ref": null, "doi": "10.1145/3418463", "report-no": null, "categories": "cs.PL cs.LG cs.NE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose IR2Vec, a Concise and Scalable encoding infrastructure to\nrepresent programs as a distributed embedding in continuous space. This\ndistributed embedding is obtained by combining representation learning methods\nwith flow information to capture the syntax as well as the semantics of the\ninput programs. As our infrastructure is based on the Intermediate\nRepresentation (IR) of the source code, obtained embeddings are both language\nand machine independent. The entities of the IR are modeled as relationships,\nand their representations are learned to form a seed embedding vocabulary.\nUsing this infrastructure, we propose two incremental encodings:Symbolic and\nFlow-Aware. Symbolic encodings are obtained from the seed embedding vocabulary,\nand Flow-Aware encodings are obtained by augmenting the Symbolic encodings with\nthe flow information.\n  We show the effectiveness of our methodology on two optimization tasks\n(Heterogeneous device mapping and Thread coarsening). Our way of representing\nthe programs enables us to use non-sequential models resulting in orders of\nmagnitude of faster training time. Both the encodings generated by IR2Vec\noutperform the existing methods in both the tasks, even while using simple\nmachine learning models. In particular, our results improve or match the\nstate-of-the-art speedup in 11/14 benchmark-suites in the device mapping task\nacross two platforms and 53/68 benchmarks in the Thread coarsening task across\nfour different platforms. When compared to the other methods, our embeddings\nare more scalable, is non-data-hungry, and has betterOut-Of-Vocabulary (OOV)\ncharacteristics.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 13:41:40 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 06:22:25 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2020 09:24:01 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["VenkataKeerthy", "S.", ""], ["Aggarwal", "Rohit", ""], ["Jain", "Shalini", ""], ["Desarkar", "Maunendra Sankar", ""], ["Upadrasta", "Ramakrishna", ""], ["Srikant", "Y. N.", ""]]}, {"id": "1909.06344", "submitter": "Paul Emmerich", "authors": "Paul Emmerich, Simon Ellmann, Fabian Bonk, Alex Egger, Esa\\'u Garc\\'ia\n  S\\'anchez-Torija, Thomas G\\\"unzel, Sebastian Di Luzio, Alexandru Obada,\n  Maximilian Stadlmeier, Sebastian Voit, Georg Carle", "title": "The Case for Writing Network Drivers in High-Level Programming Languages", "comments": null, "journal-ref": "ACM/IEEE Symposium on Architectures for Networking and\n  Communications Systems (ANCS 2019), 2019", "doi": null, "report-no": null, "categories": "cs.NI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drivers are written in C or restricted subsets of C++ on all production-grade\nserver, desktop, and mobile operating systems. They account for 66% of the code\nin Linux, but 39 out of 40 security bugs related to memory safety found in\nLinux in 2017 are located in drivers. These bugs could have been prevented by\nusing high-level languages for drivers. We present user space drivers for the\nIntel ixgbe 10 Gbit/s network cards implemented in Rust, Go, C#, Java, OCaml,\nHaskell, Swift, JavaScript, and Python written from scratch in idiomatic style\nfor the respective languages. We quantify costs and benefits of using these\nlanguages: High-level languages are safer (fewer bugs, more safety checks), but\nrun-time safety checks reduce throughput and garbage collection leads to\nlatency spikes. Out-of-order CPUs mitigate the cost of safety checks: Our Rust\ndriver executes 63% more instructions per packet but is only 4% slower than a\nreference C implementation. Go's garbage collector keeps latencies below 100\n$\\mu$s even under heavy load. Other languages fare worse, but their unique\nproperties make for an interesting case study.\n  All implementations are available as free and open source at\nhttps://github.com/ixy-languages/ixy-languages.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 17:41:43 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Emmerich", "Paul", ""], ["Ellmann", "Simon", ""], ["Bonk", "Fabian", ""], ["Egger", "Alex", ""], ["S\u00e1nchez-Torija", "Esa\u00fa Garc\u00eda", ""], ["G\u00fcnzel", "Thomas", ""], ["Di Luzio", "Sebastian", ""], ["Obada", "Alexandru", ""], ["Stadlmeier", "Maximilian", ""], ["Voit", "Sebastian", ""], ["Carle", "Georg", ""]]}, {"id": "1909.06353", "submitter": "Roberto Bagnara", "authors": "Roberto Bagnara", "title": "That's C, baby. C!", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardly a week goes by at BUGSENG without having to explain to someone that\nalmost any piece of C text, considered in isolation, means absolutely nothing.\nThe belief that C text has meaning in itself is so common, also among seasoned\nC practitioners, that I thought writing a short paper on the subject was a good\ntime investment. The problem is due to the fact that the semantics of the C\nprogramming language is not fully defined: non-definite behavior, predefined\nmacros, different library implementations, peculiarities of the translation\nprocess, . . . : all these contribute to the fact that no meaning can be\nassigned to source code unless full details about the build are available. The\npaper starts with an exercise that admits a solution. The existence of this\nsolution will hopefully convince anyone that, in general, unless the toolchain\nand the build procedure are fully known, no meaning can be assigned to any\nnontrivial piece of C code.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 17:56:40 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Bagnara", "Roberto", ""]]}, {"id": "1909.07036", "submitter": "Keehang Kwon", "authors": "Keehang Kwon", "title": "Towards Distributed Logic Programming based on Computability Logic", "comments": "We discuss an agent programming model with query/knowledgebase\n  duality", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  {\\em Computability logic} (CoL) is a powerful computational model which views\ncomputational problems as games played by a machine and its environment. It\nuses formulas to represent computational problems. In this paper, we show that\nCoL naturally supports multiagent programming models with distributed control.\nTo be specific, we discuss a web-based implemention of a distributed logic\nprogramming model based on CoL (CL1 to be exact).\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 07:33:22 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Kwon", "Keehang", ""]]}, {"id": "1909.07190", "submitter": "Abhinav Jangda", "authors": "Abhinav Jangda and Arjun Guha", "title": "Model-Based Warp Overlapped Tiling for Image Processing Programs on GPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain-specific languages that execute image processing pipelineson GPUs,\nsuch as Halide and Forma, operate by 1) dividing the image into overlapped\ntiles, and 2) fusing loops to improve memory locality. However, current\napproaches have limitations: 1) they require intra thread block\nsynchronization, which has a non-trivial cost, 2) they must choose between\nsmall tiles that require more overlapped computations or large tiles that\nincrease shared memory access (and lowers occupancy), and 3) their\nautoscheduling algorithms use simplified GPU models that can result in\ninefficient global memory accesses. We present a new approach for executing\nimage processing pipelines on GPUs that addresses these limitations as follows.\n1) We fuse loops to form overlapped tiles that fit in a single warp, which\nallows us to use lightweight warp synchronization. 2) We introduce hybrid\ntiling, which stores overlapped regions in a combination of thread-local\nregisters and shared memory. Thus hybrid tiling either increases occupancy by\ndecreasing shared memory usage or decreases overlapping computations using\nlarger tiles. 3) We present an automatic loop fusion algorithm that considers\nseveral factors that affect the performance of GPU kernels. We implement these\ntechniques in PolyMage-GPU, which is a new GPU backend for PolyMage. Our\napproach produces code that is faster than Halide's manual schedules: 1.65x\nfaster on an NVIDIA GTX 1080Ti and 1.33 faster on an NVIDIA Tesla V100.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 13:34:25 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 14:51:53 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Jangda", "Abhinav", ""], ["Guha", "Arjun", ""]]}, {"id": "1909.07331", "submitter": "Marat Akhin", "authors": "Daniil Stepanov, Marat Akhin, Mikhail Belyaev", "title": "ReduKtor: How We Stopped Worrying About Bugs in Kotlin Compiler", "comments": "Accepted to: 2019 34th IEEE/ACM International Conference on Automated\n  Software Engineering (ASE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bug localization is well-known to be a difficult problem in software\nengineering, and specifically in compiler development, where it is beneficial\nto reduce the input program to a minimal reproducing example; this technique is\nmore commonly known as delta debugging. What additionally contributes to the\nproblem is that every new programming language has its own unique quirks and\nfoibles, making it near impossible to reuse existing tools and approaches with\nfull efficiency. In this experience paper we tackle the delta debugging problem\nw.r.t. Kotlin, a relatively new programming language from JetBrains. Our\napproach is based on a novel combination of program slicing, hierarchical delta\ndebugging and Kotlin-specific transformations, which are synergistic to each\nother. We implemented it in a prototype called ReduKtor and did extensive\nevaluation on both synthetic and real Kotlin programs; we also compared its\nperformance with classic delta debugging techniques. The evaluation results\nsupport the practical usability of our approach to Kotlin delta debugging and\nalso shows the importance of using both language-agnostic and language-specific\ntechniques to achieve best reduction efficiency and performance.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 16:58:12 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Stepanov", "Daniil", ""], ["Akhin", "Marat", ""], ["Belyaev", "Mikhail", ""]]}, {"id": "1909.07479", "submitter": "W{\\l}odzimierz Drabent", "authors": "W{\\l}odzimierz Drabent", "title": "On correctness of an n queens program", "comments": "14 pages, 1 figure. This version: various modifications, mainly in\n  Section 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thom Fr\\\"uhwirth presented a short, elegant and efficient Prolog program for\nthe n queens problem. However the program may be seen as rather tricky and one\nmay be not convinced about its correctness. This paper explains the program in\na declarative way, and provides proofs of its correctness and completeness. The\nspecification and the proofs are declarative, i.e. they abstract from any\noperational semantics. The specification is approximate, it is unnecessary to\ndescribe the program's semantics exactly. Despite the program works on\nnon-ground terms, this work employs the standard semantics, based on logical\nconsequence and Herbrand interpretations.\n  Another purpose of the paper is to present an example of precise declarative\nreasoning about the semantics of a logic program.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 20:55:54 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 10:12:01 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 22:53:30 GMT"}, {"version": "v4", "created": "Mon, 15 Jun 2020 15:51:21 GMT"}, {"version": "v5", "created": "Fri, 17 Jul 2020 19:01:06 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Drabent", "W\u0142odzimierz", ""]]}, {"id": "1909.07646", "submitter": "EPTCS", "authors": "Bart Bogaerts (Vrije Universiteit Brussel), Esra Erdem (Sabanci\n  University), Paul Fodor (Stony Brook University), Andrea Formisano\n  (Universit\\`a di Perugia), Giovambattista Ianni (University of Calabria),\n  Daniela Inclezan (Miami University), German Vidal (Universitat Politecnica de\n  Valencia), Alicia Villanueva (Universitat Politecnica de Valencia), Marina De\n  Vos (University of Bath), Fangkai Yang (NVIDIA Corporation)", "title": "Proceedings 35th International Conference on Logic Programming\n  (Technical Communications)", "comments": null, "journal-ref": "EPTCS 306, 2019", "doi": "10.4204/EPTCS.306", "report-no": null, "categories": "cs.LO cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the first conference held in Marseille in 1982, ICLP has been the\npremier international event for presenting research in logic programming.\nContributions are sought in all areas of logic programming, including but not\nrestricted to:\n  Foundations: Semantics, Formalisms, Nonmonotonic reasoning, Knowledge\nrepresentation.\n  Languages: Concurrency, Objects, Coordination, Mobility, Higher Order, Types,\nModes, Assertions, Modules, Meta-programming, Logic-based domain-specific\nlanguages, Programming Techniques.\n  Declarative programming: Declarative program development, Analysis, Type and\nmode inference, Partial evaluation, Abstract interpretation, Transformation,\nValidation, Verification, Debugging, Profiling, Testing, Execution\nvisualization\n  Implementation: Virtual machines, Compilation, Memory management,\nParallel/distributed execution, Constraint handling rules, Tabling, Foreign\ninterfaces, User interfaces.\n  Related Paradigms and Synergies: Inductive and Co-inductive Logic\nProgramming, Constraint Logic Programming, Answer Set Programming, Interaction\nwith SAT, SMT and CSP solvers, Logic programming techniques for type inference\nand theorem proving, Argumentation, Probabilistic Logic Programming, Relations\nto object-oriented and Functional programming.\n  Applications: Databases, Big Data, Data integration and federation, Software\nengineering, Natural language processing, Web and Semantic Web, Agents,\nArtificial intelligence, Computational life sciences, Education, Cybersecurity,\nand Robotics.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 08:33:12 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Bogaerts", "Bart", "", "Vrije Universiteit Brussel"], ["Erdem", "Esra", "", "Sabanci\n  University"], ["Fodor", "Paul", "", "Stony Brook University"], ["Formisano", "Andrea", "", "Universit\u00e0 di Perugia"], ["Ianni", "Giovambattista", "", "University of Calabria"], ["Inclezan", "Daniela", "", "Miami University"], ["Vidal", "German", "", "Universitat Politecnica de\n  Valencia"], ["Villanueva", "Alicia", "", "Universitat Politecnica de Valencia"], ["De Vos", "Marina", "", "University of Bath"], ["Yang", "Fangkai", "", "NVIDIA Corporation"]]}, {"id": "1909.07814", "submitter": "Nishanth Chandran", "authors": "Nishant Kumar, Mayank Rathee, Nishanth Chandran, Divya Gupta, Aseem\n  Rastogi, and Rahul Sharma", "title": "CrypTFlow: Secure TensorFlow Inference", "comments": "To appear at 41st IEEE Symposium on Security and Privacy 2020. Code\n  available at: https://github.com/mpc-msri/EzPC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CrypTFlow, a first of its kind system that converts TensorFlow\ninference code into Secure Multi-party Computation (MPC) protocols at the push\nof a button. To do this, we build three components. Our first component, Athos,\nis an end-to-end compiler from TensorFlow to a variety of semi-honest MPC\nprotocols. The second component, Porthos, is an improved semi-honest 3-party\nprotocol that provides significant speedups for TensorFlow like applications.\nFinally, to provide malicious secure MPC protocols, our third component,\nAramis, is a novel technique that uses hardware with integrity guarantees to\nconvert any semi-honest MPC protocol into an MPC protocol that provides\nmalicious security. The malicious security of the protocols output by Aramis\nrelies on integrity of the hardware and semi-honest security of MPC. Moreover,\nour system matches the inference accuracy of plaintext TensorFlow.\n  We experimentally demonstrate the power of our system by showing the secure\ninference of real-world neural networks such as ResNet50 and DenseNet121 over\nthe ImageNet dataset with running times of about 30 seconds for semi-honest\nsecurity and under two minutes for malicious security. Prior work in the area\nof secure inference has been limited to semi-honest security of small networks\nover tiny datasets such as MNIST or CIFAR. Even on MNIST/CIFAR, CrypTFlow\noutperforms prior work.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 03:55:05 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 02:13:56 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Kumar", "Nishant", ""], ["Rathee", "Mayank", ""], ["Chandran", "Nishanth", ""], ["Gupta", "Divya", ""], ["Rastogi", "Aseem", ""], ["Sharma", "Rahul", ""]]}, {"id": "1909.07918", "submitter": "Elisabet Lobo-Vesga", "authors": "Elisabet Lobo-Vesga (1), Alejandro Russo (1), Marco Gaboardi (2) ((1)\n  Chalmers University of Technology, (2) Boston University)", "title": "A Programming Framework for Differential Privacy with Accuracy\n  Concentration Bounds", "comments": "22 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy offers a formal framework for reasoning about privacy\nand accuracy of computations on private data. It also offers a rich set of\nbuilding blocks for constructing data analyses. When carefully calibrated,\nthese analyses simultaneously guarantee privacy of the individuals contributing\ntheir data, and accuracy of their results for inferring useful properties about\nthe population. The compositional nature of differential privacy has motivated\nthe design and implementation of several programming languages aimed at helping\na data analyst in programming differentially private analyses. However, most of\nthe programming languages for differential privacy proposed so far provide\nsupport for reasoning about privacy but not for reasoning about the accuracy of\ndata analyses. To overcome this limitation, in this work we present DPella, a\nprogramming framework providing data analysts with support for reasoning about\nprivacy, accuracy and their trade-offs. The distinguishing feature of DPella is\na novel component which statically tracks the accuracy of different data\nanalyses. In order to make tighter accuracy estimations, this component\nleverages taint analysis for automatically inferring statistical independence\nof the different noise quantities added for guaranteeing privacy. We show the\nflexibility of our approach by not only implementing classical counting queries\n(e.g., CDFs) but also by analyzing hierarchical counting queries (like those\ndone by Census Bureaus), where accuracy have different constraints per level\nand data analysts should figure out the best manner to calibrate privacy to\nmeet the accuracy requirements.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 20:29:08 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Lobo-Vesga", "Elisabet", ""], ["Russo", "Alejandro", ""], ["Gaboardi", "Marco", ""]]}, {"id": "1909.08230", "submitter": "EPTCS", "authors": "Falco Nogatz (University of W\\\"urzburg, Germany), Philipp K\\\"orner\n  (University of D\\\"usseldorf, Germany), Sebastian Krings (Niederrhein\n  University of Applied Sciences, Germany)", "title": "Prolog Coding Guidelines: Status and Tool Support", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 8-21", "doi": "10.4204/EPTCS.306.8", "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of coding guidelines is generally accepted throughout\ndevelopers of every programming language. Naturally, Prolog makes no exception.\nHowever, establishing coding guidelines is fraught with obstacles: Finding\ncommon ground on kind and selection of rules is matter of debate; once found,\nadhering to or enforcing rules is complicated as well, not least because of\nProlog's flexible syntax without keywords. In this paper, we evaluate the\nstatus of coding guidelines in the Prolog community and discuss to what extent\nthey can be automatically verified. We implemented a linter for Prolog and\napplied it to several packages to get a hold on the current state of the\ncommunity.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 06:59:16 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Nogatz", "Falco", "", "University of W\u00fcrzburg, Germany"], ["K\u00f6rner", "Philipp", "", "University of D\u00fcsseldorf, Germany"], ["Krings", "Sebastian", "", "Niederrhein\n  University of Applied Sciences, Germany"]]}, {"id": "1909.08232", "submitter": "EPTCS", "authors": "Jo\\~ao Barbosa (Faculty of Science of the University of Porto),\n  M\\'ario Florido (Faculty of Science of the University of Porto), V\\'itor\n  Santos Costa (Faculty of Science of the University of Porto)", "title": "A Three-Valued Semantics for Typed Logic Programming", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 36-51", "doi": "10.4204/EPTCS.306.10", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Types in logic programming have focused on conservative approximations of\nprogram semantics by regular types, on one hand, and on type systems based on a\nprescriptive semantics defined for typed programs, on the other. In this paper,\nwe define a new semantics for logic programming, where programs evaluate to\ntrue, false, and to a new semantic value called wrong, corresponding to a\nrun-time type error. We then have a type language with a separated semantics of\ntypes. Finally, we define a type system for logic programming and prove that it\nis semantically sound with respect to a semantic relation between programs and\ntypes where, if a program has a type, then its semantics is not wrong. Our work\nfollows Milner's approach for typed functional languages where the semantics of\nprograms is independent from the semantic of types, and the type system is\nproved to be sound with respect to a relation between both semantics.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:00:16 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Barbosa", "Jo\u00e3o", "", "Faculty of Science of the University of Porto"], ["Florido", "M\u00e1rio", "", "Faculty of Science of the University of Porto"], ["Costa", "V\u00edtor Santos", "", "Faculty of Science of the University of Porto"]]}, {"id": "1909.08242", "submitter": "EPTCS", "authors": "Mateusz \\'Sla\\.zy\\'nski (AGH University of Science and Technology),\n  Salvador Abreu (University of \\'Evora and LISP), Grzegorz J. Nalepa (AGH\n  University of Science and Technology)", "title": "Generating Local Search Neighborhood with Synthesized Logic Programs", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 168-181", "doi": "10.4204/EPTCS.306.22", "report-no": null, "categories": "cs.NE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local Search meta-heuristics have been proven a viable approach to solve\ndifficult optimization problems. Their performance depends strongly on the\nsearch space landscape, as defined by a cost function and the selected\nneighborhood operators. In this paper we present a logic programming based\nframework, named Noodle, designed to generate bespoke Local Search\nneighborhoods tailored to specific discrete optimization problems. The proposed\nsystem consists of a domain specific language, which is inspired by logic\nprogramming, as well as a genetic programming solver, based on the grammar\nevolution algorithm. We complement the description with a preliminary\nexperimental evaluation, where we synthesize efficient neighborhood operators\nfor the traveling salesman problem, some of which reproduce well-known results.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:05:35 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["\u015ala\u017cy\u0144ski", "Mateusz", "", "AGH University of Science and Technology"], ["Abreu", "Salvador", "", "University of \u00c9vora and LISP"], ["Nalepa", "Grzegorz J.", "", "AGH\n  University of Science and Technology"]]}, {"id": "1909.08243", "submitter": "EPTCS", "authors": "Vincent Barichard, Igor St\\'ephan", "title": "Quantified Constraint Handling Rules", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 210-223", "doi": "10.4204/EPTCS.306.25", "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We shift the QCSP (Quantified Constraint Satisfaction Problems) framework to\nthe QCHR (Quantified Constraint Handling Rules) framework by enabling dynamic\nbinder and access to user-defined constraints. QCSP offers a natural framework\nto express PSPACE problems as finite two-players games. But to define a QCSP\nmodel, the binder must be formerly known and cannot be built dynamically even\nif the worst case won't occur. To overcome this issue, we define the new QCHR\nformalism that allows to build the binder dynamically during the solving. Our\nQCHR models exhibit state-of-the-art performances on static binder and\noutperforms previous QCSP approaches when the binder is dynamic.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:07:09 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Barichard", "Vincent", ""], ["St\u00e9phan", "Igor", ""]]}, {"id": "1909.08246", "submitter": "EPTCS", "authors": "K. Tuncay Tekle (Stony Brook University), Yanhong A. Liu (Stony Brook\n  University)", "title": "Extended Magic for Negation: Efficient Demand-Driven Evaluation of\n  Stratified Datalog with Precise Complexity Guarantees", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 241-254", "doi": "10.4204/EPTCS.306.28", "report-no": null, "categories": "cs.LO cs.AI cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of Datalog rules, facts, and a query, answers to the query can be\ninferred bottom-up starting from the facts or top-down starting from the query.\nFor efficiency, top-down evaluation is extended with memoization of inferred\nfacts, and bottom-up evaluation is performed after transformations to make\nrules driven by the demand from the query. Prior work has shown their precise\ncomplexity analysis and relationships. However, when Datalog is extended with\neven stratified negation, which has a simple and universally accepted\nsemantics, transformations to make rules demand-driven may result in\nnon-stratified negation, which has had many complex semantics and evaluation\nmethods.\n  This paper presents (1) a simple extension to demand transformation, a\ntransformation to make rules demand-driven for Datalog without negation, to\nsupport stratified negation, and (2) a simple extension to an optimal bottom-up\nevaluation method for Datalog with stratified negation, to handle\nnon-stratified negation in the resulting rules. We show that the method\nprovides precise complexity guarantees. It is also optimal in that only facts\nneeded for top-down evaluation of the query are inferred and each firing of a\nrule to infer such a fact takes worst-case constant time. We extend the precise\nrelationship between top-down evaluation and demand-driven bottom-up evaluation\nto Datalog with stratified negation. Finally, we show experimental results for\nperformance, as well as applications to previously challenging examples.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:07:42 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Tekle", "K. Tuncay", "", "Stony Brook University"], ["Liu", "Yanhong A.", "", "Stony Brook\n  University"]]}, {"id": "1909.08248", "submitter": "EPTCS", "authors": "Felicidad Aguado (IRLab, CITIC Research Center, University of A\n  Coru\\~na, Spain), Pedro Cabalar (IRLab, CITIC Research Center, University of\n  A Coru\\~na, Spain), Jorge Fandinno (University of Potsdam, Germany), Brais\n  Mu\\~niz (IRLab, CITIC Research Center, University of A Coru\\~na, Spain),\n  Gilberto P\\'erez (IRLab, CITIC Research Center, University of A Coru\\~na,\n  Spain), Francisco Su\\'arez (Digestive Service, Complexo Hospitalario\n  Universitario de A Coru\\~na (CHUAC), Instituto de Investigaci\\'on Biom\\'edica\n  de A Coru\\~na (INIBIC), Coru\\~na, Spain)", "title": "A Rule-Based System for Explainable Donor-Patient Matching in Liver\n  Transplantation", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 266-272", "doi": "10.4204/EPTCS.306.31", "report-no": null, "categories": "cs.LO cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present web-liver, a rule-based system for decision support\nin the medical domain, focusing on its application in a liver transplantation\nunit for implementing policies for donor-patient matching. The rule-based\nsystem is built on top of an interpreter for logic programs with partial\nfunctions, called lppf, that extends the paradigm of Answer Set Programming\n(ASP) adding two main features: (1) the inclusion of partial functions and (2)\nthe computation of causal explanations for the obtained solutions. The final\ngoal of web-liver is assisting the medical experts in the design of new\ndonor-patient matching policies that take into account not only the patient\nseverity but also the transplantation utility. As an example, we illustrate the\ntool behaviour with a set of rules that implement the utility index called\nSOFT.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:08:25 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Aguado", "Felicidad", "", "IRLab, CITIC Research Center, University of A\n  Coru\u00f1a, Spain"], ["Cabalar", "Pedro", "", "IRLab, CITIC Research Center, University of\n  A Coru\u00f1a, Spain"], ["Fandinno", "Jorge", "", "University of Potsdam, Germany"], ["Mu\u00f1iz", "Brais", "", "IRLab, CITIC Research Center, University of A Coru\u00f1a, Spain"], ["P\u00e9rez", "Gilberto", "", "IRLab, CITIC Research Center, University of A Coru\u00f1a,\n  Spain"], ["Su\u00e1rez", "Francisco", "", "Digestive Service, Complexo Hospitalario\n  Universitario de A Coru\u00f1a"]]}, {"id": "1909.08261", "submitter": "EPTCS", "authors": "Mateusz \\'Sla\\.zy\\'nski (AGH University of Science and Technology)", "title": "Research Report on Automatic Synthesis of Local Search Neighborhood\n  Operators", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 433-440", "doi": "10.4204/EPTCS.306.59", "report-no": null, "categories": "cs.NE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint Programming (CP) and Local Search (LS) are different paradigms for\ndealing with combinatorial search and optimization problems. Their\ncomplementary features motivated researchers to create hybrid CP/LS solutions,\nmaintaining both the modeling capabilities of CP and the computational\nadvantages of the heuristic-based LS approach. Research presented in this\nreport is focused on developing a novel method to infer an efficient LS\nneighborhood operator based on the problem structure, as modeled in the CP\nparadigm. We consider a limited formal language that we call a Neighborhood\nDefinition Language, used to specify the neighborhood operators in a\nfine-grained and declarative manner. Together with Logic Programming runtime\ncalled Noodle, it allows to automatically synthesize complex operators using a\nGrammar Evolution algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:15:41 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["\u015ala\u017cy\u0144ski", "Mateusz", "", "AGH University of Science and Technology"]]}, {"id": "1909.08557", "submitter": "Lukas Diekmann", "authors": "Lukas Diekmann and Laurence Tratt", "title": "Default Disambiguation for Online Parsers", "comments": "14 pages, 6 tables, 8 figures. Note: This reverts this paper back to\n  v1 (which was accidentally replaced with a different paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since composed grammars are often ambiguous, grammar composition requires a\nmechanism for dealing with ambiguity: either ruling it out by using delimiters\n(which are awkward to work with), or by using disambiguation operators to\nfilter a parse forest down to a single parse tree (where, in general, we cannot\nbe sure that we have covered all possible parse forests). In this paper, we\nshow that default disambiguation, which is inappropriate for batch parsing,\nworks well for online parsing, where it can be overridden by the user if\nnecessary. We extend language boxes -- a delimiter-based algorithm atop\nincremental parsing -- in such a way that default disambiguation can\nautomatically insert, remove, or resize, language boxes, leading to the\nautomatic language boxes algorithm. The nature of the problem means that\ndefault disambiguation cannot always match a user's intention. However, our\nexperimental evaluation shows that automatic language boxes behave acceptably\nin 98.8% of tests involving compositions of real-world programming languages.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 16:25:20 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 12:42:47 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 15:18:05 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Diekmann", "Lukas", ""], ["Tratt", "Laurence", ""]]}, {"id": "1909.08671", "submitter": "Bruno Bernardo", "authors": "Bruno Bernardo, Rapha\\\"el Cauderlier, Zhenlei Hu, Basile Pesin and\n  Julien Tesson", "title": "Mi-Cho-Coq, a framework for certifying Tezos Smart Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tezos is a blockchain launched in June 2018. It is written in OCaml and\nsupports smart contracts. Its smart contract language is called Michelson and\nit has been designed with formal verification in mind. In this article, we\npresent Mi-Cho-Coq, a Coq framework for verifying the functional correctness of\nMichelson smart contracts. As a case study, we detail the certification of a\nMultisig contract with the Mi-Cho-Coq framework.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 19:49:03 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Bernardo", "Bruno", ""], ["Cauderlier", "Rapha\u00ebl", ""], ["Hu", "Zhenlei", ""], ["Pesin", "Basile", ""], ["Tesson", "Julien", ""]]}, {"id": "1909.08789", "submitter": "Qinxiang Cao", "authors": "Qinxiang Cao, Shengyi Wang, Aquinas Hobor, Andrew W. Appel", "title": "Proof Pearl: Magic Wand as Frame", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Separation logic adds two connectives to assertion languages: separating\nconjunction * (\"star\") and its adjoint, separating implication -* (\"magic\nwand\"). Comparatively, separating implication is less widely used.\n  This paper demonstrates that by using magic wand to express frames that\nrelate mutable local portions of data structures to global portions, we can\nexploit its power while proofs are still easily understandable. Many useful\nseparation logic theorems about partial data structures can now be proved by\nsimple automated tactics, which were usually proved by induction. This\nmagic-wand-as-frame technique is especially useful when formalizing the proofs\nby a high order logic. We verify binary search tree insert in Coq as an example\nto demonstrate this proof technique.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 03:44:51 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Cao", "Qinxiang", ""], ["Wang", "Shengyi", ""], ["Hobor", "Aquinas", ""], ["Appel", "Andrew W.", ""]]}, {"id": "1909.08815", "submitter": "Raphael Mosaner", "authors": "Raphael Mosaner, David Leopoldseder, Manuel Rigger, Roland Schatz,\n  Hanspeter M\\\"ossenb\\\"ock", "title": "Supporting On-Stack Replacement in Unstructured Languages by Loop\n  Reconstruction and Extraction", "comments": "Accepted at MPLR 2019. This is the author's version of the work", "journal-ref": null, "doi": "10.1145/3357390.3361030", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-stack replacement (OSR) is a common technique employed by dynamic\ncompilers to reduce program warm-up time. OSR allows switching from interpreted\nto compiled code during the execution of this code. The main targets are long\nrunning loops, which need to be represented explicitly, with dedicated\ninformation about condition and body, to be optimized at run time. Bytecode\ninterpreters, however, represent control flow implicitly via unstructured jumps\nand thus do not exhibit the required high-level loop representation. To enable\nOSR also for jump-based - often called unstructured - languages, we propose the\npartial reconstruction of loops in order to explicitly represent them in a\nbytecode interpreter. Besides an outline of the general idea, we implemented\nour approach in Sulong, a bytecode interpreter for LLVM bitcode, which allows\nthe execution of C/C++. We conducted an evaluation with a set of C benchmarks,\nwhich showed speed-ups in warm-up of up to 9x for certain benchmarks. This\nfacilitates execution of programs with long-running loops in rarely called\nfunctions, which would yield significant slowdown without OSR. While shown with\na prototype implementation, the overall idea of our approach is generalizable\nfor all bytecode interpreters.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 06:03:51 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Mosaner", "Raphael", ""], ["Leopoldseder", "David", ""], ["Rigger", "Manuel", ""], ["Schatz", "Roland", ""], ["M\u00f6ssenb\u00f6ck", "Hanspeter", ""]]}, {"id": "1909.08958", "submitter": "Aviral Goel", "authors": "Aviral Goel, Jan Vitek", "title": "On the Design, Implementation, and Use of Laziness in R", "comments": "27 pages, 4 tables, 21 figures", "journal-ref": "Object-Oriented Programming, Systems, Languages & Applications.\n  October 2019, Pages 153:1--153:27", "doi": "10.1145/3360579", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The R programming language has been lazy for over twenty-five years. This\npaper presents a review of the design and implementation of call-by-need in R,\nand a data-driven study of how generations of programmers have put laziness to\nuse in their code. We analyze 16,707 packages and observe the creation of 270.9\nB promises. Our data suggests that there is little supporting evidence to\nassert that programmers use laziness to avoid unnecessary computation or to\noperate over infinite data structures. For the most part R code appears to have\nbeen written without reliance on, and in many cases even knowledge of, delayed\nargument evaluation. The only significant exception is a small number of\npackages which leverage call-by-need for meta-programming.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 13:14:19 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Goel", "Aviral", ""], ["Vitek", "Jan", ""]]}, {"id": "1909.09324", "submitter": "Asankhaya Sharma", "authors": "Asankhaya Sharma", "title": "Automated Verification of Integer Overflow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integer overflow accounts for one of the major source of bugs in software.\nVerification systems typically assume a well defined underlying semantics for\nvarious integer operations and do not explicitly check for integer overflow in\nprograms. In this paper we present a specification mechanism for expressing\ninteger overflow. We develop an automated procedure for integer overflow\nchecking during program verification. We have implemented a prototype integer\noverflow checker and tested it on a benchmark consisting of already verified\nprograms (over 14k LOC). We have found 43 bugs in these programs due to integer\noverflow.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 05:20:34 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Sharma", "Asankhaya", ""]]}, {"id": "1909.09543", "submitter": "Artem Polyvyanyy", "authors": "Artem Polyvyanyy, Arthur H. M. ter Hofstede, Marcello La Rosa, Chun\n  Ouyang, Anastasiia Pika", "title": "Process Query Language: Design, Implementation, and Evaluation", "comments": "83 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Organizations can benefit from the use of practices, techniques, and tools\nfrom the area of business process management. Through the focus on processes,\nthey create process models that require management, including support for\nversioning, refactoring and querying. Querying thus far has primarily focused\non structural properties of models rather than on exploiting behavioral\nproperties capturing aspects of model execution. While the latter is more\nchallenging, it is also more effective, especially when models are used for\nauditing or process automation. The focus of this paper is to overcome the\nchallenges associated with behavioral querying of process models in order to\nunlock its benefits. The first challenge concerns determining decidability of\nthe building blocks of the query language, which are the possible behavioral\nrelations between process tasks. The second challenge concerns achieving\nacceptable performance of query evaluation. The evaluation of a query may\nrequire expensive checks in all process models, of which there may be\nthousands. In light of these challenges, this paper proposes a special-purpose\nprogramming language, namely Process Query Language (PQL) for behavioral\nquerying of process model collections. The language relies on a set of\nbehavioral predicates between process tasks, whose usefulness has been\nempirically evaluated with a pool of process model stakeholders. This study\nresulted in a selection of the predicates to be implemented in PQL, whose\ndecidability has also been formally proven. The computational performance of\nthe language has been extensively evaluated through a set of experiments\nagainst two large process model collections.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 14:50:36 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Polyvyanyy", "Artem", ""], ["ter Hofstede", "Arthur H. M.", ""], ["La Rosa", "Marcello", ""], ["Ouyang", "Chun", ""], ["Pika", "Anastasiia", ""]]}, {"id": "1909.09562", "submitter": "Michael Hanus", "authors": "Sergio Antoy and Michael Hanus", "title": "Equivalence Checking of Non-deterministic Operations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Checking the semantic equivalence of operations is an important task in\nsoftware development. For instance, regression testing is a routine task\nperformed when software systems are developed and improved, and software\npackage managers require the equivalence of operations in different versions of\na package within the same major number version. A solid foundation is required\nto support a good automation of this process. It has been shown that the notion\nof equivalence is not obvious when non-deterministic features are present. In\nthis paper, we discuss a general notion of equivalence in functional logic\nprograms and develop a practical method to check it. Our method is integrated\nin a property-based testing tool which is used in a software package manager to\ncheck the semantic versioning of software packages.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 15:37:21 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Antoy", "Sergio", ""], ["Hanus", "Michael", ""]]}, {"id": "1909.09567", "submitter": "Thorsten Wissmann", "authors": "Cristian Ene, Laurent Mounier, Marie-Laure Potet", "title": "Output-sensitive Information flow analysis", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 17, Issue 1 (February\n  12, 2021) lmcs:7172", "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Constant-time programming is a countermeasure to prevent cache based attacks\nwhere programs should not perform memory accesses that depend on secrets. In\nsome cases this policy can be safely relaxed if one can prove that the program\ndoes not leak more information than the public outputs of the computation. We\npropose a novel approach for verifying constant-time programming based on a new\ninformation flow property, called output-sensitive noninterference.\nNoninterference states that a public observer cannot learn anything about the\nprivate data. Since real systems need to intentionally declassify some\ninformation, this property is too strong in practice. In order to take into\naccount public outputs we proceed as follows: instead of using complex explicit\ndeclassification policies, we partition variables in three sets: input, output\nand leakage variables. Then, we propose a typing system to statically check\nthat leakage variables do not leak more information about the secret inputs\nthan the public normal output. The novelty of our approach is that we track the\ndependence of leakage variables with respect not only to the initial values of\ninput variables (as in classical approaches for noninterference), but taking\nalso into account the final values of output variables. We adapted this\napproach to LLVM IR and we developed a prototype to verify LLVM\nimplementations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 15:42:41 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 16:42:03 GMT"}, {"version": "v3", "created": "Thu, 17 Dec 2020 12:10:09 GMT"}, {"version": "v4", "created": "Sun, 24 Jan 2021 17:42:48 GMT"}, {"version": "v5", "created": "Thu, 11 Feb 2021 13:34:17 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Ene", "Cristian", ""], ["Mounier", "Laurent", ""], ["Potet", "Marie-Laure", ""]]}, {"id": "1909.10493", "submitter": "Chunhui Guo", "authors": "Chunhui Guo, Zhicheng Fu, Zhenyu Zhang, Shangping Ren, Lui Sha", "title": "Formalism for Supporting the Development of Verifiably Safe Medical\n  Guidelines with Statecharts", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.FL cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving the effectiveness and safety of patient care is the ultimate\nobjective for medical cyber-physical systems. Many medical best practice\nguidelines exist, but most of the existing guidelines in handbooks are\ndifficult for medical staff to remember and apply clinically. Furthermore,\nalthough the guidelines have gone through clinical validations, validations by\nmedical professionals alone do not provide guarantees for the safety of medical\ncyber-physical systems. Hence, formal verification is also needed. The paper\npresents the formal semantics for a framework that we developed to support the\ndevelopment of verifiably safe medical guidelines.\n  The framework allows computer scientists to work together with medical\nprofessionals to transform medical best practice guidelines into executable\nstatechart models, Yakindu in particular, so that medical functionalities and\nproperties can be quickly prototyped and validated. Existing formal\nverification technologies, UPPAAL timed automata in particular, is integrated\ninto the framework to provide formal verification capabilities to verify safety\nproperties. However, some components used/built into the framework, such as the\nopen-source Yakindu statecharts as well as the transformation rules from\nstatecharts to timed automata, do not have built-in semantics. The ambiguity\nbecomes unavoidable unless formal semantics is defined for the framework, which\nis what the paper is to present.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 17:25:50 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Guo", "Chunhui", ""], ["Fu", "Zhicheng", ""], ["Zhang", "Zhenyu", ""], ["Ren", "Shangping", ""], ["Sha", "Lui", ""]]}, {"id": "1909.11206", "submitter": "Julie Newcomb", "authors": "Julie L Newcomb, Rastislav Bodik", "title": "Using human-in-the-loop synthesis to author functional reactive programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programs that respond to asynchronous events are challenging to write; they\nare difficult to reason about and tricky to test and debug. Because these\nprograms can have a huge space of possible input timings and interleaving, the\nprogrammer may easily miss corner cases. We propose applying synthesis to aid\nprogrammers in creating programs more easily and with a higher degree of\nconfidence in their correctness. We have written an efficient encoding of\nfunctional reactive programming (FRP) semantics based on functional programming\nover lists lifted in Rosette. We demonstrate that this technique is\nstate-of-the-art by first comparing its performance against two existing\nsynthesis tools that produce list manipulation programs, and then by\nsynthesizing a suite of benchmarks given complete specifications. We also\npropose an interactive tool in which a programmer provides some initial partial\nspecification in the form of input/output examples or invariants; the tool\nfinds ambiguity in the specification by synthesizing two candidate programs and\ngives the user an input that distinguishes them; the user updates the\nspecification and continues iterating until the correct program is found. As\nevaluation, we demonstrate the use of the tool on a suite of benchmarks from\nthe web programming and Internet of Things domains and walk through a sample\ninteraction on a realistic web benchmark, showing that we can converge on the\ntarget program with a tractable number of interactions. As future work, we\ndiscuss encoding additional FRP languages to in order to explore metalinguistic\nfeatures, strategies for decomposition that would allow the synthesis of larger\nprograms, and improved programmer tools such as a GUI to more easily elicit\nspecifications.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 22:06:39 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Newcomb", "Julie L", ""], ["Bodik", "Rastislav", ""]]}, {"id": "1909.12252", "submitter": "Chandrakana Nandi", "authors": "Chandrakana Nandi, Max Willsey, Adam Anderson, James R. Wilcox, Eva\n  Darulova, Dan Grossman, Zachary Tatlock", "title": "Synthesizing Structured CAD Models with Equality Saturation and Inverse\n  Transformations", "comments": "14 pages", "journal-ref": "PLDI 2020", "doi": "10.1145/3385412.3386012", "report-no": null, "categories": "cs.PL cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent program synthesis techniques help users customize CAD models(e.g., for\n3D printing) by decompiling low-level triangle meshes to Constructive Solid\nGeometry (CSG) expressions. Without loops or functions, editing CSG can require\nmany coordinated changes, and existing mesh decompilers use heuristics that can\nobfuscate high-level structure.\n  This paper proposes a second decompilation stage to robustly \"shrink\"\nunstructured CSG expressions into more editable programs with map and fold\noperators. We present Szalinski, a tool that uses Equality Saturation with\nsemantics-preserving CAD rewrites to efficiently search for smaller equivalent\nprograms. Szalinski relies on inverse transformations, a novel way for solvers\nto speculatively add equivalences to an E-graph. We qualitatively evaluate\nSzalinski in case studies, show how it composes with an existing mesh\ndecompiler, and demonstrate that Szalinski can shrink large models in seconds.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 16:58:08 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 15:05:08 GMT"}, {"version": "v3", "created": "Sun, 12 Apr 2020 18:15:51 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Nandi", "Chandrakana", ""], ["Willsey", "Max", ""], ["Anderson", "Adam", ""], ["Wilcox", "James R.", ""], ["Darulova", "Eva", ""], ["Grossman", "Dan", ""], ["Tatlock", "Zachary", ""]]}, {"id": "1909.12279", "submitter": "Ezra Zigmond", "authors": "Ezra Zigmond (Harvard University, United States), Stephen Chong\n  (Harvard University, United States), Christos Dimoulas (Northwestern\n  University, United States), Scott Moore (Galois, Inc, United States)", "title": "Fine-Grained, Language-Based Access Control for Database-Backed\n  Applications", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2020, Vol. 4,\n  Issue 2, Article 3", "doi": "10.22152/programming-journal.org/2020/4/3", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Database-backed applications often run queries with more authority\nthan necessary. Since programs can access more data than they legitimately\nneed, flaws in security checks at the application level can enable malicious or\nbuggy code to view or modify data in violation of intended access control\npolicies.\n  Inquiry: Although database management systems provide tools to control access\nto data, these tools are not well-suited for modern applications which often\nhave many users and consist of many different software components. First,\ndatabases are unaware of application users, and creating a new database user\nfor each application user is impractical for applications with many users.\nSecond, different components of the same application may require different\nauthority, which would require creating different database users for different\nsoftware components. Thus, it is difficult to use existing tools to properly\nlimit the authority an application has when executing queries. For this reason,\nwe consider a new, language-based approach to application-specific database\nsecurity.\n  Approach: Prior work has addressed the difficulty of running applications\nwith least privilege using capability-based security and software contracts,\nwhich we adapt to the setting of database-backed applications.\n  Knowledge: This paper's main contribution is the design and implementation of\nShillDB, a language for writing secure database-backed applications. ShillDB\nenables reasoning about database access at the language level through\ncapabilities, which limit which database tables a program can access, and\ncontracts, which limit what operations a program can perform on those tables.\nShillDB contracts are expressed as part of function interfaces, making it easy\nto specify different access control policies for different components.\nContracts act as executable security documentation for ShillDB programs and are\nenforced by the language runtime. Further, ShillDB provides database access\ncontrol guarantees independent of (and in addition to) the security mechanisms\nof the underlying database management system.\n  Grounding: We have implemented a prototype of ShillDB and have used it to\nimplement the backend for a lending library reservation system with contracts\nfor each endpoint to evaluate the performance and usability of ShillDB.\nFurther, we benchmark individual database operations in ShillDB to better\nunderstand the language's performance overhead.\n  Importance: Our experience indicates that ShillDB is a practical language for\nenforcing database access control policies in realistic, multi-user\napplications and has reasonable performance overhead. ShillDB allows developers\nto reason about security at the component level, safely compose components, and\nreuse third-party components with their own application-specific database\nsecurity policies.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 17:37:23 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Zigmond", "Ezra", "", "Harvard University, United States"], ["Chong", "Stephen", "", "Harvard University, United States"], ["Dimoulas", "Christos", "", "Northwestern\n  University, United States"], ["Moore", "Scott", "", "Galois, Inc, United States"]]}, {"id": "1909.12281", "submitter": "Will Crichton", "authors": "Will Crichton", "title": "Human-Centric Program Synthesis", "comments": "To appear at PLATEAU'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program synthesis techniques offer significant new capabilities in searching\nfor programs that satisfy high-level specifications. While synthesis has been\nthoroughly explored for input/output pair specifications\n(programming-by-example), this paper asks: what does program synthesis look\nlike beyond examples? What actual issues in day-to-day development would stand\nto benefit the most from synthesis? How can a human-centric perspective inform\nthe exploration of alternative specification languages for synthesis? I sketch\na human-centric vision for program synthesis where programmers explore and\nlearn languages and APIs aided by a synthesis tool.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 17:44:06 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Crichton", "Will", ""]]}, {"id": "1909.12582", "submitter": "Lionel Rieg", "authors": "G\\'erard Berry and Lionel Rieg", "title": "Towards Coq-verified Esterel Semantics and Compiling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on semantics of the Esterel synchronous programming\nlanguage. In particular, in addition to the usual behavioral (CBS) and state\n(CSS) semantics, it introduces a novel microstep semantics which does not need\nthe Can potential function. Formal proofs in Coq of the equivalence between the\nCBS and CSS semantics and of the refinement between the CSS and microstep\nsemantics are also provided.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 09:43:14 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Berry", "G\u00e9rard", ""], ["Rieg", "Lionel", ""]]}, {"id": "1909.12795", "submitter": "Hongki Lee", "authors": "Hongki Lee (Korea Advanced Institute of Science and Technology, South\n  Korea), Changhee Park (Samsung Electronics, South Korea), Sukyoung Ryu (Korea\n  Advanced Institute of Science and Technology, South Korea)", "title": "Automatically Tracing Imprecision Causes in JavaScript Static Analysis", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2020, Vol. 4,\n  Issue 2, Article 2", "doi": "10.22152/programming-journal.org/2020/4/2", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers have developed various techniques for static analysis of\nJavaScript to improve analysis precision. To develop such techniques, they\nfirst identify causes of the precision losses for unproven properties. While\nmost of the existing work has diagnosed main causes of imprecision in static\nanalysis by manual investigation, manually tracing the imprecision causes is\nchallenging because it requires detailed knowledge of analyzer internals.\nRecently, several studies proposed to localize the analysis imprecision causes\nautomatically, but these localization techniques work for only specific\nanalysis techniques.\n  In this paper, we present an automatic technique that can trace analysis\nimprecision causes of JavaScript applications starting from user-selected\nvariables. Given a set of program variables, our technique stops an analysis\nwhen any of the variables gets imprecise analysis values. It then traces the\nimprecise analysis values using intermediate analysis results back to program\npoints where the imprecision first started. Our technique shows the trace\ninformation with a new representation called tracing graphs, whose nodes and\nedges together represent traces from imprecise points to precise points. In\norder to detect major causes of analysis imprecision automatically, we present\nfour node/edge patterns in tracing graphs for common imprecision causes. We\nformalized the technique of generating tracing graphs and identifying patterns,\nand implemented them on SAFE, a state-of-the-art JavaScript static analyzer\nwith various analysis configurations, such as context-sensitivity,\nloop-sensitivity, and heap cloning. Our evaluation demonstrates that the\ntechnique can easily find 96 % of the major causes of the imprecision problems\nin 17 applications by only automatic detection in tracing graphs using the\npatterns, and selectively adopting various advanced techniques can eliminate\nthe found causes of imprecision.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 17:01:52 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Lee", "Hongki", "", "Korea Advanced Institute of Science and Technology, South\n  Korea"], ["Park", "Changhee", "", "Samsung Electronics, South Korea"], ["Ryu", "Sukyoung", "", "Korea\n  Advanced Institute of Science and Technology, South Korea"]]}, {"id": "1909.13058", "submitter": "Shubhendra Singhal Mr.", "authors": "Shubhendra Pal Singhal, Sandeep Gupta, Pierluigi Nuzzo", "title": "Profiling minisat based on user defined execution time -- GPROF", "comments": "10 figures, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the explanation of the architecture of profilers\nparticularly gprof and how to profile a program according to the user defined\ninput of execution time . Gprof is a profiler available open source in the\npackage of binutils. Gprof records the flow of the program including the callee\nand caller information and their respective execution time. This information is\nrepresented in the form of a call graph. Profilers at the time of execution\ncreates a call graph file which indicates the full flow of the program\nincluding the individual execution time as well. This paper aims at providing a\nbetter understanding of the data structure used to store the information and\nhow is a profiler(gprof) actually using this data structure to give user a\nreadable format. The next section of this paper solves one of the limitation of\ngprof i.e. edit the time of block of code without understanding the call graph.\nAny changes in the execution time of a particular block of code would affect\nthe total execution time. So if we edit the gprof in such a way that its\nconsistent and platform independent, then it can yield various results like\ntesting execution time after parallelism, before even designing it by replacing\nthe values with theoretical/emulated ones and see if the total execution time\nis getting reduced by a desired number or not? Gprof edit can help us figure\nout that what section of code can be parallelized or which part of code is\ntaking the most time and which call or part can be changed to reduce the\nexecution time. The last section of the paper walks through the application of\ngprof in minisat and how gprof helps in the hardware acceleration in minisat by\nsuggesting which part to be parallelised and how does it affect the total\npercentage.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 09:51:36 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Singhal", "Shubhendra Pal", ""], ["Gupta", "Sandeep", ""], ["Nuzzo", "Pierluigi", ""]]}, {"id": "1909.13516", "submitter": "Yao Wan", "authors": "Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao, Jian Wu and\n  Philip S. Yu", "title": "Multi-Modal Attention Network Learning for Semantic Source Code\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code retrieval techniques and tools have been playing a key role in\nfacilitating software developers to retrieve existing code fragments from\navailable open-source repositories given a user query. Despite the existing\nefforts in improving the effectiveness of code retrieval, there are still two\nmain issues hindering them from being used to accurately retrieve satisfiable\ncode fragments from large-scale repositories when answering complicated\nqueries. First, the existing approaches only consider shallow features of\nsource code such as method names and code tokens, but ignoring structured\nfeatures such as abstract syntax trees (ASTs) and control-flow graphs (CFGs) of\nsource code, which contains rich and well-defined semantics of source code.\nSecond, although the deep learning-based approach performs well on the\nrepresentation of source code, it lacks the explainability, making it hard to\ninterpret the retrieval results and almost impossible to understand which\nfeatures of source code contribute more to the final results.\n  To tackle the two aforementioned issues, this paper proposes MMAN, a novel\nMulti-Modal Attention Network for semantic source code retrieval. A\ncomprehensive multi-modal representation is developed for representing\nunstructured and structured features of source code, with one LSTM for the\nsequential tokens of code, a Tree-LSTM for the AST of code and a GGNN (Gated\nGraph Neural Network) for the CFG of code. Furthermore, a multi-modal attention\nfusion layer is applied to assign weights to different parts of each modality\nof source code and then integrate them into a single hybrid representation.\nComprehensive experiments and analysis on a large-scale real-world dataset show\nthat our proposed model can accurately retrieve code snippets and outperforms\nthe state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 08:35:04 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Wan", "Yao", ""], ["Shu", "Jingdong", ""], ["Sui", "Yulei", ""], ["Xu", "Guandong", ""], ["Zhao", "Zhou", ""], ["Wu", "Jian", ""], ["Yu", "Philip S.", ""]]}, {"id": "1909.13639", "submitter": "Ameer Haj-Ali", "authors": "Ameer Haj-Ali, Nesreen K. Ahmed, Ted Willke, Sophia Shao, Krste\n  Asanovic, Ion Stoica", "title": "NeuroVectorizer: End-to-End Vectorization with Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges arising when compilers vectorize loops for today's\nSIMD-compatible architectures is to decide if vectorization or interleaving is\nbeneficial. Then, the compiler has to determine how many instructions to pack\ntogether and how many loop iterations to interleave. Compilers are designed\ntoday to use fixed-cost models that are based on heuristics to make\nvectorization decisions on loops. However, these models are unable to capture\nthe data dependency, the computation graph, or the organization of\ninstructions. Alternatively, software engineers often hand-write the\nvectorization factors of every loop. This, however, places a huge burden on\nthem, since it requires prior experience and significantly increases the\ndevelopment time. In this work, we explore a novel approach for handling loop\nvectorization and propose an end-to-end solution using deep reinforcement\nlearning (RL). We conjecture that deep RL can capture different instructions,\ndependencies, and data structures to enable learning a sophisticated model that\ncan better predict the actual performance cost and determine the optimal\nvectorization factors. We develop an end-to-end framework, from code to\nvectorization, that integrates deep RL in the LLVM compiler. Our proposed\nframework takes benchmark codes as input and extracts the loop codes. These\nloop codes are then fed to a loop embedding generator that learns an embedding\nfor these loops. Finally, the learned embeddings are used as input to a Deep RL\nagent, which determines the vectorization factors for all the loops. We further\nextend our framework to support multiple supervised learning methods. We\nevaluate our approaches against the currently used LLVM vectorizer and loop\npolyhedral optimization techniques. Our experiments show 1.29X-4.73X\nperformance speedup compared to baseline and only 3% worse than the brute-force\nsearch on a wide range of benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 12:29:09 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2019 12:29:38 GMT"}, {"version": "v3", "created": "Sat, 14 Dec 2019 22:57:00 GMT"}, {"version": "v4", "created": "Sat, 4 Jan 2020 09:11:03 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Haj-Ali", "Ameer", ""], ["Ahmed", "Nesreen K.", ""], ["Willke", "Ted", ""], ["Shao", "Sophia", ""], ["Asanovic", "Krste", ""], ["Stoica", "Ion", ""]]}, {"id": "1909.13649", "submitter": "Emma Tosch", "authors": "Emma Tosch, Eytan Bakshy, Emery D. Berger, David D. Jensen, J. Eliot\n  B. Moss", "title": "PlanAlyzer: Assessing Threats to the Validity of Online Experiments", "comments": "30 pages, hella long", "journal-ref": "OOPSLA 2019", "doi": "10.1145/3360608", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online experiments are ubiquitous. As the scale of experiments has grown, so\nhas the complexity of their design and implementation. In response, firms have\ndeveloped software frameworks for designing and deploying online experiments.\nEnsuring that experiments in these frameworks are correctly designed and that\ntheir results are trustworthy---referred to as *internal validity*---can be\ndifficult. Currently, verifying internal validity requires manual inspection by\nsomeone with substantial expertise in experimental design.\n  We present the first approach for statically checking the internal validity\nof online experiments. Our checks are based on well-known problems that arise\nin experimental design and causal inference. Our analyses target PlanOut, a\nwidely deployed, open-source experimentation framework that uses a\ndomain-specific language to specify and run complex experiments. We have built\na tool, PlanAlyzer, that checks PlanOut programs for a variety of threats to\ninternal validity, including failures of randomization, treatment assignment,\nand causal sufficiency. PlanAlyzer uses its analyses to automatically generate\n*contrasts*, a key type of information required to perform valid statistical\nanalyses over experimental results. We demonstrate PlanAlyzer's utility on a\ncorpus of PlanOut scripts deployed in production at Facebook, and we evaluate\nits ability to identify threats to validity on a mutated subset of this corpus.\nPlanAlyzer has both precision and recall of 92% on the mutated corpus, and 82%\nof the contrasts it automatically generates match hand-specified data.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 12:49:12 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Tosch", "Emma", ""], ["Bakshy", "Eytan", ""], ["Berger", "Emery D.", ""], ["Jensen", "David D.", ""], ["Moss", "J. Eliot B.", ""]]}, {"id": "1909.13768", "submitter": "Damiano Mazza", "authors": "Alois Brunel, Damiano Mazza, Michele Pagani", "title": "Backpropagation in the Simply Typed Lambda-calculus with Linear Negation", "comments": "27 pages", "journal-ref": "Proc. ACM Program. Lang. 4, POPL, Article 64 (January 2020)", "doi": "10.1145/3371132", "report-no": null, "categories": "cs.LO cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backpropagation is a classic automatic differentiation algorithm computing\nthe gradient of functions specified by a certain class of simple, first-order\nprograms, called computational graphs. It is a fundamental tool in several\nfields, most notably machine learning, where it is the key for efficiently\ntraining (deep) neural networks. Recent years have witnessed the quick growth\nof a research field called differentiable programming, the aim of which is to\nexpress computational graphs more synthetically and modularly by resorting to\nactual programming languages endowed with control flow operators and\nhigher-order combinators, such as map and fold. In this paper, we extend the\nbackpropagation algorithm to a paradigmatic example of such a programming\nlanguage: we define a compositional program transformation from the\nsimply-typed lambda-calculus to itself augmented with a notion of linear\nnegation, and prove that this computes the gradient of the source program with\nthe same efficiency as first-order backpropagation. The transformation is\ncompletely effect-free and thus provides a purely logical understanding of the\ndynamics of backpropagation.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 15:18:49 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 13:04:53 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Brunel", "Alois", ""], ["Mazza", "Damiano", ""], ["Pagani", "Michele", ""]]}]