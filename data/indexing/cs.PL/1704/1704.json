[{"id": "1704.00032", "submitter": "Sven Karol", "authors": "Sven Karol, Tobias Nett, Jeronimo Castrillon, Ivo F. Sbalzarini", "title": "A Domain-Specific Language and Editor for Parallel Particle Methods", "comments": "Submitted to ACM Transactions on Mathematical Software on Dec. 25,\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain-specific languages (DSLs) are of increasing importance in scientific\nhigh-performance computing to reduce development costs, raise the level of\nabstraction and, thus, ease scientific programming. However, designing and\nimplementing DSLs is not an easy task, as it requires knowledge of the\napplication domain and experience in language engineering and compilers.\nConsequently, many DSLs follow a weak approach using macros or text generators,\nwhich lack many of the features that make a DSL a comfortable for programmers.\nSome of these features---e.g., syntax highlighting, type inference, error\nreporting, and code completion---are easily provided by language workbenches,\nwhich combine language engineering techniques and tools in a common ecosystem.\nIn this paper, we present the Parallel Particle-Mesh Environment (PPME), a DSL\nand development environment for numerical simulations based on particle methods\nand hybrid particle-mesh methods. PPME uses the meta programming system (MPS),\na projectional language workbench. PPME is the successor of the Parallel\nParticle-Mesh Language (PPML), a Fortran-based DSL that used conventional\nimplementation strategies. We analyze and compare both languages and\ndemonstrate how the programmer's experience can be improved using static\nanalyses and projectional editing. Furthermore, we present an explicit domain\nmodel for particle abstractions and the first formal type system for particle\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 19:39:27 GMT"}, {"version": "v2", "created": "Sun, 17 Sep 2017 13:50:08 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Karol", "Sven", ""], ["Nett", "Tobias", ""], ["Castrillon", "Jeronimo", ""], ["Sbalzarini", "Ivo F.", ""]]}, {"id": "1704.00082", "submitter": "Yanhong Annie Liu", "authors": "Yanhong A. Liu, Saksham Chand, Scott D. Stoller", "title": "Moderately Complex Paxos Made Simple: High-Level Executable\n  Specification of Distributed Algorithms", "comments": null, "journal-ref": "PPDP 2019: Proceedings of the 21st International Symposium on\n  Principles and Practice of Declarative Programming. October 2019. Article No.\n  15. Pages 1-15. ACM Press", "doi": "10.1145/3354166.3354180", "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the application of a high-level language and method in\ndeveloping simpler specifications of more complex variants of the Paxos\nalgorithm for distributed consensus. The specifications are for Multi-Paxos\nwith preemption, replicated state machine, and reconfiguration and optimized\nwith state reduction and failure detection. The language is DistAlgo. The key\nis to express complex control flows and synchronization conditions precisely at\na high level, using nondeterministic waits and message-history queries. We\nobtain complete executable specifications that are almost completely\ndeclarative---updating only a number for the protocol round besides the sets of\nmessages sent and received.\n  We show the following results: 1.English and pseudocode descriptions of\ndistributed algorithms can be captured completely and precisely at a high\nlevel, without adding, removing, or reformulating algorithm details to fit\nlower-level, more abstract, or less direct languages. 2.We created higher-level\ncontrol flows and synchronization conditions than all previous specifications,\nand obtained specifications that are much simpler and smaller, even matching or\nsmaller than abstract specifications that omit many algorithm details. 3.The\nsimpler specifications led us to easily discover useless replies, unnecessary\ndelays, and liveness violations (if messages can be lost) in previous published\nspecifications, by just following the simplified algorithm flows. 4.The\nresulting specifications can be executed directly, and we can express\noptimizations cleanly, yielding drastic performance improvement over naive\nexecution and facilitating a general method for merging processes. 5.We\nsystematically translated the resulting specifications into TLA+ and developed\nmachine-checked safety proofs, which also allowed us to detect and fix a subtle\nsafety violation in an earlier unpublished specification.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 23:22:40 GMT"}, {"version": "v2", "created": "Sun, 23 Jul 2017 13:13:00 GMT"}, {"version": "v3", "created": "Thu, 31 Jan 2019 14:17:37 GMT"}, {"version": "v4", "created": "Mon, 12 Aug 2019 06:12:40 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Liu", "Yanhong A.", ""], ["Chand", "Saksham", ""], ["Stoller", "Scott D.", ""]]}, {"id": "1704.00135", "submitter": "Vadim Markovtsev", "authors": "Vadim Markovtsev and Eiso Kant", "title": "Topic modeling of public repositories at scale using names in source\n  code", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming languages themselves have a limited number of reserved keywords\nand character based tokens that define the language specification. However,\nprogrammers have a rich use of natural language within their code through\ncomments, text literals and naming entities. The programmer defined names that\ncan be found in source code are a rich source of information to build a high\nlevel understanding of the project. The goal of this paper is to apply topic\nmodeling to names used in over 13.6 million repositories and perceive the\ninferred topics. One of the problems in such a study is the occurrence of\nduplicate repositories not officially marked as forks (obscure forks). We show\nhow to address it using the same identifiers which are extracted for topic\nmodeling.\n  We open with a discussion on naming in source code, we then elaborate on our\napproach to remove exact duplicate and fuzzy duplicate repositories using\nLocality Sensitive Hashing on the bag-of-words model and then discuss our work\non topic modeling; and finally present the results from our data analysis\ntogether with open-access to the source code, tools and datasets.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 08:16:20 GMT"}, {"version": "v2", "created": "Sat, 20 May 2017 08:29:00 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Markovtsev", "Vadim", ""], ["Kant", "Eiso", ""]]}, {"id": "1704.00617", "submitter": "James Cheney", "authors": "James Cheney and Alberto Momigliano", "title": "$\\alpha$Check: A mechanized metatheory model-checker", "comments": "Under consideration for publication in Theory and Practice of Logic\n  Programming (TPLP)", "journal-ref": "Theory and Practice of Logic Programming, 17(3), 311-352 (2017)", "doi": "10.1017/S1471068417000035", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of mechanically formalizing and proving metatheoretic properties\nof programming language calculi, type systems, operational semantics, and\nrelated formal systems has received considerable attention recently. However,\nthe dual problem of searching for errors in such formalizations has attracted\ncomparatively little attention. In this article, we present $\\alpha$Check, a\nbounded model-checker for metatheoretic properties of formal systems specified\nusing nominal logic. In contrast to the current state of the art for metatheory\nverification, our approach is fully automatic, does not require expertise in\ntheorem proving on the part of the user, and produces counterexamples in the\ncase that a flaw is detected. We present two implementations of this technique,\none based on negation-as-failure and one based on negation elimination, along\nwith experimental results showing that these techniques are fast enough to be\nused interactively to debug systems as they are developed.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 14:31:24 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Cheney", "James", ""], ["Momigliano", "Alberto", ""]]}, {"id": "1704.00778", "submitter": "Guilherme Bicalho De P\\'adua", "authors": "Guilherme B. de P\\'adua, Weiyi Shang", "title": "Studying the Prevalence of Exception Handling Anti-Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern programming languages, such as Java and C#, typically provide features\nthat handle exceptions. These features separate error-handling code from\nregular source code and are proven to enhance the practice of software\nreliability, comprehension, and maintenance. Having acknowledged the advantages\nof exception handling features, the misuse of them can still cause catastrophic\nsoftware failures, such as application crash. Prior studies suggested\nanti-patterns of exception handling; while little knowledge was shared about\nthe prevalence of these anti-patterns. In this paper, we investigate the\nprevalence of exception-handling anti-patterns. We collected a thorough list of\nexception anti-patterns from 16 open-source Java and C# libraries and\napplications using an automated exception flow analysis tool. We found that\nalthough exception handling anti- patterns widely exist in all of our subjects,\nonly a few anti- patterns (e.g. Unhandled Exceptions, Catch Generic,\nUnreachable Handler, Over-catch, and Destructive Wrapping) can be commonly\nidentified. On the other hand, we find that the prevalence of anti- patterns\nillustrates differences between C# and Java. Our results call for further\nin-depth analyses on the exception handling practices across different\nlanguages.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 19:26:03 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["de P\u00e1dua", "Guilherme B.", ""], ["Shang", "Weiyi", ""]]}, {"id": "1704.00917", "submitter": "J\\\"urgen Koslowski", "authors": "Sooraj Bhat and Johannes Borgstr\\\"om and Andrew D. Gordon and Claudio\n  Russo", "title": "Deriving Probability Density Functions from Probabilistic Functional\n  Programs", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 13, Issue 2 (July 3,\n  2017) lmcs:3758", "doi": "10.23638/LMCS-13(2:16)2017", "report-no": null, "categories": "cs.PL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The probability density function of a probability distribution is a\nfundamental concept in probability theory and a key ingredient in various\nwidely used machine learning methods. However, the necessary framework for\ncompiling probabilistic functional programs to density functions has only\nrecently been developed. In this work, we present a density compiler for a\nprobabilistic language with failure and both discrete and continuous\ndistributions, and provide a proof of its soundness. The compiler greatly\nreduces the development effort of domain experts, which we demonstrate by\nsolving inference problems from various scientific applications, such as\nmodelling the global carbon cycle, using a standard Markov chain Monte Carlo\nframework.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 08:27:21 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 19:47:20 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Bhat", "Sooraj", ""], ["Borgstr\u00f6m", "Johannes", ""], ["Gordon", "Andrew D.", ""], ["Russo", "Claudio", ""]]}, {"id": "1704.01513", "submitter": "Sabri Pllana", "authors": "Adrian Calvo Chozas, Suejb Memeti, Sabri Pllana", "title": "Using Cognitive Computing for Learning Parallel Programming: An IBM\n  Watson Solution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While modern parallel computing systems provide high performance resources,\nutilizing them to the highest extent requires advanced programming expertise.\nProgramming for parallel computing systems is much more difficult than\nprogramming for sequential systems. OpenMP is an extension of C++ programming\nlanguage that enables to express parallelism using compiler directives. While\nOpenMP alleviates parallel programming by reducing the lines of code that the\nprogrammer needs to write, deciding how and when to use these compiler\ndirectives is up to the programmer. Novice programmers may make mistakes that\nmay lead to performance degradation or unexpected program behavior. Cognitive\ncomputing has shown impressive results in various domains, such as health or\nmarketing. In this paper, we describe the use of IBM Watson cognitive system\nfor education of novice parallel programmers. Using the dialogue service of the\nIBM Watson we have developed a solution that assists the programmer in avoiding\ncommon OpenMP mistakes. To evaluate our approach we have conducted a survey\nwith a number of novice parallel programmers at the Linnaeus University, and\nobtained encouraging results with respect to usefulness of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 16:28:55 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["Chozas", "Adrian Calvo", ""], ["Memeti", "Suejb", ""], ["Pllana", "Sabri", ""]]}, {"id": "1704.01696", "submitter": "Pengcheng Yin", "authors": "Pengcheng Yin, Graham Neubig", "title": "A Syntactic Neural Model for General-Purpose Code Generation", "comments": "To appear in ACL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of parsing natural language descriptions into source\ncode written in a general-purpose programming language like Python. Existing\ndata-driven methods treat this problem as a language generation task without\nconsidering the underlying syntax of the target programming language. Informed\nby previous work in semantic parsing, in this paper we propose a novel neural\narchitecture powered by a grammar model to explicitly capture the target syntax\nas prior knowledge. Experiments find this an effective way to scale up to\ngeneration of complex programs from natural language descriptions, achieving\nstate-of-the-art results that well outperform previous code generation and\nsemantic parsing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 03:13:46 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Yin", "Pengcheng", ""], ["Neubig", "Graham", ""]]}, {"id": "1704.01814", "submitter": "Jayadev Misra", "authors": "Jayadev Misra", "title": "Bilateral Proofs of Safety and Progress Properties of Concurrent\n  Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper suggests a theomisra@utexas.edury of composable specification of\nconcurrent programs that permits: (1) verification of program code for a given\nspecification, and (2) composition of the specifications of the components to\nyield the specification of a program. The specification consists of both\nterminal properties that hold at the end of a program execution (if the\nexecution terminates) and perpetual properties that hold throughout an\nexecution. We devise (1) proof techniques for verification, and (2) composition\nrules to derive the specification of a program from those of its components. We\nemploy terminal properties of components to derive perpetual properties of a\nprogram and conversely. Hence, this proof strategy is called bilateral. The\ncompositional aspect of the theory is important in assembling a program out of\ncomponents some of whose source code may not be available, as is increasingly\nthe case with cross-vendor program integration.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 17:48:36 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Misra", "Jayadev", ""]]}, {"id": "1704.02375", "submitter": "Yanhong Annie Liu", "authors": "David S. Warren and Yanhong A. Liu", "title": "AppLP: A Dialogue on Applications of Logic Programming", "comments": "David S. Warren and Yanhong A. Liu (Editors). 33 pages. Including\n  summaries by Christopher Kane and abstracts or position papers by M. Aref, J.\n  Rosenwald, I. Cervesato, E.S.L. Lam, M. Balduccini, J. Lobo, A. Russo, E.\n  Lupu, N. Leone, F. Ricca, G. Gupta, K. Marple, E. Salazar, Z. Chen, A. Sobhi,\n  S. Srirangapalli, C.R. Ramakrishnan, N. Bj{\\o}rner, N.P. Lopes, A.\n  Rybalchenko, and P. Tarau", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes the contributions of the 2016 Applications of Logic\nProgramming Workshop (AppLP), which was held on October 17 and associated with\nthe International Conference on Logic Programming (ICLP) in Flushing, New York\nCity.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 21:10:00 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Warren", "David S.", ""], ["Liu", "Yanhong A.", ""]]}, {"id": "1704.02418", "submitter": "EPTCS", "authors": "Vasco T. Vasconcelos (University of Lisbon), Philipp Haller (KTH Royal\n  Institute of Technology)", "title": "Proceedings Tenth Workshop on Programming Language Approaches to\n  Concurrency- and Communication-cEntric Software", "comments": null, "journal-ref": "EPTCS 246, 2017", "doi": "10.4204/EPTCS.246", "report-no": null, "categories": "cs.PL cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PLACES 2017 (full title: Programming Language Approaches to Concurrency- and\nCommunication-cEntric Software) is the tenth edition of the PLACES workshop\nseries. After the first PLACES, which was affiliated to DisCoTec in 2008, the\nworkshop has been part of ETAPS every year since 2009 and is now an established\npart of the ETAPS satellite events. PLACES 2017 was held on 29th April in\nUppsala, Sweden. The workshop series was started in order to promote the\napplication of novel programming language ideas to the increasingly important\nproblem of developing software for systems in which concurrency and\ncommunication are intrinsic aspects. This includes software for both multi-core\nsystems and large-scale distributed and/or service-oriented systems. The scope\nof PLACES includes new programming language features, whole new programming\nlanguage designs, new type systems, new semantic approaches, new program\nanalysis techniques, and new implementation mechanisms. This volume consists of\nthe papers accepted for presentation at the workshop.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 01:32:06 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Vasconcelos", "Vasco T.", "", "University of Lisbon"], ["Haller", "Philipp", "", "KTH Royal\n  Institute of Technology"]]}, {"id": "1704.02432", "submitter": "Umang Mathur", "authors": "Dileep Kini, Umang Mathur, Mahesh Viswanathan", "title": "Dynamic Race Prediction in Linear Time", "comments": "22 pages, 8 figures, 1 algorithm, 1 table", "journal-ref": null, "doi": "10.1145/3140587.3062374", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writing reliable concurrent software remains a huge challenge for today's\nprogrammers. Programmers rarely reason about their code by explicitly\nconsidering different possible inter-leavings of its execution. We consider the\nproblem of detecting data races from individual executions in a sound manner.\nThe classical approach to solving this problem has been to use Lamport's\nhappens-before (HB) relation. Until now HB remains the only approach that runs\nin linear time. Previous efforts in improving over HB such as causally-precedes\n(CP) and maximal causal models fall short due to the fact that they are not\nimplementable efficiently and hence have to compromise on their race detecting\nability by limiting their techniques to bounded sized fragments of the\nexecution. We present a new relation weak-causally-precedes (WCP) that is\nprovably better than CP in terms of being able to detect more races, while\nstill remaining sound. Moreover it admits a linear time algorithm which works\non the entire execution without having to fragment it.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 03:14:13 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 15:38:53 GMT"}, {"version": "v3", "created": "Sun, 16 Apr 2017 05:55:54 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Kini", "Dileep", ""], ["Mathur", "Umang", ""], ["Viswanathan", "Mahesh", ""]]}, {"id": "1704.02996", "submitter": "Rathijit Sen", "authors": "Rathijit Sen, Jianqiao Zhu, Jignesh M. Patel, and Somesh Jha", "title": "ROSA: R Optimizations with Static Analysis", "comments": "A talk on this work will be presented at RIOT 2017 (3rd Workshop on R\n  Implementation, Optimization and Tooling)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  R is a popular language and programming environment for data scientists. It\nis increasingly co-packaged with both relational and Hadoop-based data\nplatforms and can often be the most dominant computational component in data\nanalytics pipelines. Recent work has highlighted inefficiencies in executing R\nprograms, both in terms of execution time and memory requirements, which in\npractice limit the size of data that can be analyzed by R. This paper presents\nROSA, a static analysis framework to improve the performance and space\nefficiency of R programs. ROSA analyzes input programs to determine program\nproperties such as reaching definitions, live variables, aliased variables, and\ntypes of variables. These inferred properties enable program transformations\nsuch as C++ code translation, strength reduction, vectorization, code motion,\nin addition to interpretive optimizations such as avoiding redundant object\ncopies and performing in-place evaluations. An empirical evaluation shows\nsubstantial reductions by ROSA in execution time and memory consumption over\nboth CRAN R and Microsoft R Open.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 18:08:36 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 16:54:03 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Sen", "Rathijit", ""], ["Zhu", "Jianqiao", ""], ["Patel", "Jignesh M.", ""], ["Jha", "Somesh", ""]]}, {"id": "1704.03093", "submitter": "EPTCS", "authors": "Philipp Haller, Fredrik Sommar", "title": "Towards an Empirical Study of Affine Types for Isolated Actors in Scala", "comments": "In Proceedings PLACES 2017, arXiv:1704.02418", "journal-ref": "EPTCS 246, 2017, pp. 3-9", "doi": "10.4204/EPTCS.246.3", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LaCasa is a type system and programming model to enforce the object\ncapability discipline in Scala, and to provide affine types. One important\napplication of LaCasa's type system is software isolation of concurrent\nprocesses. Isolation is important for several reasons including security and\ndata-race freedom. Moreover, LaCasa's affine references enable efficient,\nby-reference message passing while guaranteeing a \"deep-copy\" semantics. This\ndeep-copy semantics enables programmers to seamlessly port concurrent programs\nrunning on a single machine to distributed programs running on large-scale\nclusters of machines.\n  This paper presents an integration of LaCasa with actors in Scala,\nspecifically, the Akka actor-based middleware, one of the most widely-used\nactor systems in industry. The goal of this integration is to statically ensure\nthe isolation of Akka actors. Importantly, we present the results of an\nempirical study investigating the effort required to use LaCasa's type system\nin existing open-source Akka-based systems and applications.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 00:42:42 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Haller", "Philipp", ""], ["Sommar", "Fredrik", ""]]}, {"id": "1704.03094", "submitter": "EPTCS", "authors": "Elias Castegren (Uppsala University, Sweden), Tobias Wrigstad (Uppsala\n  University, Sweden)", "title": "Actors without Borders: Amnesty for Imprisoned State", "comments": "In Proceedings PLACES 2017, arXiv:1704.02418", "journal-ref": "EPTCS 246, 2017, pp. 10-20", "doi": "10.4204/EPTCS.246.4", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In concurrent systems, some form of synchronisation is typically needed to\nachieve data-race freedom, which is important for correctness and safety. In\nactor-based systems, messages are exchanged concurrently but executed\nsequentially by the receiving actor. By relying on isolation and non-sharing,\nan actor can access its own state without fear of data-races, and the internal\nbehavior of an actor can be reasoned about sequentially.\n  However, actor isolation is sometimes too strong to express useful patterns.\nFor example, letting the iterator of a data-collection alias the internal\nstructure of the collection allows a more efficient implementation than if each\naccess requires going through the interface of the collection. With full\nisolation, in order to maintain sequential reasoning the iterator must be made\npart of the collection, which bloats the interface of the collection and means\nthat a client must have access to the whole data-collection in order to use the\niterator.\n  In this paper, we propose a programming language construct that enables a\nrelaxation of isolation but without sacrificing sequential reasoning. We\nformalise the mechanism in a simple lambda calculus with actors and passive\nobjects, and show how an actor may leak parts of its internal state while\nensuring that any interaction with this data is still synchronised.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 00:43:06 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Castegren", "Elias", "", "Uppsala University, Sweden"], ["Wrigstad", "Tobias", "", "Uppsala\n  University, Sweden"]]}, {"id": "1704.03095", "submitter": "EPTCS", "authors": "Philipp Haller, Ludvig Axelsson", "title": "Quantifying and Explaining Immutability in Scala", "comments": "In Proceedings PLACES 2017, arXiv:1704.02418", "journal-ref": "EPTCS 246, 2017, pp. 21-27", "doi": "10.4204/EPTCS.246.5", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional programming typically emphasizes programming with first-class\nfunctions and immutable data. Immutable data types enable fault tolerance in\ndistributed systems, and ensure process isolation in message-passing\nconcurrency, among other applications. However, beyond the distinction between\nreassignable and non-reassignable fields, Scala's type system does not have a\nbuilt-in notion of immutability for type definitions. As a result, immutability\nis \"by-convention\" in Scala, and statistics about the use of immutability in\nreal-world Scala code are non-existent.\n  This paper reports on the results of an empirical study on the use of\nimmutability in several medium-to-large Scala open-source code bases, including\nScala's standard library and the Akka actor framework. The study investigates\nboth shallow and deep immutability, two widely-used forms of immutability in\nScala. Perhaps most interestingly, for type definitions determined to be\nmutable, explanations are provided for why neither the shallow nor the deep\nimmutability property holds; in turn, these explanations are aggregated into\nstatistics in order to determine the most common reasons for why type\ndefinitions are mutable rather than immutable.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 00:43:25 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Haller", "Philipp", ""], ["Axelsson", "Ludvig", ""]]}, {"id": "1704.03096", "submitter": "EPTCS", "authors": "Francisco Martins (LaSIGE, Faculty of Sciences, University of Lisbon),\n  Vasco Thudichum Vasconcelos (LaSIGE, Faculty of Sciences, University of\n  Lisbon), Hans H\\\"uttel (Department of Computer Science, Aalborg University)", "title": "Inferring Types for Parallel Programs", "comments": "In Proceedings PLACES 2017, arXiv:1704.02418", "journal-ref": "EPTCS 246, 2017, pp. 28-36", "doi": "10.4204/EPTCS.246.6", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Message Passing Interface (MPI) framework is widely used in implementing\nimperative pro- grams that exhibit a high degree of parallelism. The PARTYPES\napproach proposes a behavioural type discipline for MPI-like programs in which\na type describes the communication protocol followed by the entire program.\nWell-typed programs are guaranteed to be exempt from deadlocks. In this paper\nwe describe a type inference algorithm for a subset of the original system; the\nalgorithm allows to statically extract a type for an MPI program from its\nsource code.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 00:43:38 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Martins", "Francisco", "", "LaSIGE, Faculty of Sciences, University of Lisbon"], ["Vasconcelos", "Vasco Thudichum", "", "LaSIGE, Faculty of Sciences, University of\n  Lisbon"], ["H\u00fcttel", "Hans", "", "Department of Computer Science, Aalborg University"]]}, {"id": "1704.03097", "submitter": "EPTCS", "authors": "Alceste Scalas (Imperial College London), Nobuko Yoshida (Imperial\n  College London)", "title": "Multiparty Session Types, Beyond Duality (Abstract)", "comments": "In Proceedings PLACES 2017, arXiv:1704.02418", "journal-ref": "EPTCS 246, 2017, pp. 37-38", "doi": "10.4204/EPTCS.246.7", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiparty Session Types (MPST) are a well-established typing discipline for\nmessage-passing processes interacting on sessions involving two or more\nparticipants. Session typing can ensure desirable properties: absence of\ncommunication errors and deadlocks, and protocol conformance. However, existing\nMPST works provide a subject reduction result that is arguably (and sometimes,\nsurprisingly) restrictive: it only holds for typing contexts with strong\nduality constraints on the interactions between pairs of participants.\nConsequently, many \"intuitively correct\" examples cannot be typed and/or cannot\nbe proved type-safe. We illustrate some of these examples, and discuss the\nreason for these limitations. Then, we outline a novel MPST typing system that\nremoves these restrictions.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 00:43:53 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Scalas", "Alceste", "", "Imperial College London"], ["Yoshida", "Nobuko", "", "Imperial\n  College London"]]}, {"id": "1704.03098", "submitter": "EPTCS", "authors": "Hendrik Maarand, Tarmo Uustalu", "title": "Generating Representative Executions [Extended Abstract]", "comments": "In Proceedings PLACES 2017, arXiv:1704.02418", "journal-ref": "EPTCS 246, 2017, pp. 39-48", "doi": "10.4204/EPTCS.246.8", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing the behaviour of a concurrent program is made difficult by the\nnumber of possible executions. This problem can be alleviated by applying the\ntheory of Mazurkiewicz traces to focus only on the canonical representatives of\nthe equivalence classes of the possible executions of the program. This paper\npresents a generic framework that allows to specify the possible behaviours of\nthe execution environment, and generate all Foata-normal executions of a\nprogram, for that environment, by discarding abnormal executions during the\ngeneration phase. The key ingredient of Mazurkiewicz trace theory, the\ndependency relation, is used in the framework in two roles: first, as part of\nthe specification of which executions are allowed at all, and then as part of\nthe normality checking algorithm, which is used to discard the abnormal\nexecutions. The framework is instantiated to the relaxed memory models of the\nSPARC hierarchy.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 00:44:07 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Maarand", "Hendrik", ""], ["Uustalu", "Tarmo", ""]]}, {"id": "1704.03105", "submitter": "EPTCS", "authors": "Yingfu Zeng (Rice University), Ferenc Bartha (Rice University), Walid\n  Taha (Halmstad University)", "title": "Compile-Time Extensions to Hybrid ODEs", "comments": "In Proceedings SNR 2017, arXiv:1704.02421", "journal-ref": "EPTCS 247, 2017, pp. 52-70", "doi": "10.4204/EPTCS.247.5", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reachability analysis for hybrid systems is an active area of development and\nhas resulted in many promising prototype tools. Most of these tools allow users\nto express hybrid system as automata with a set of ordinary differential\nequations (ODEs) associated with each state, as well as rules for transitions\nbetween states. Significant effort goes into developing and verifying and\ncorrectly implementing those tools. As such, it is desirable to expand the\nscope of applicability tools of such as far as possible. With this goal, we\nshow how compile-time transformations can be used to extend the basic hybrid\nODE formalism traditionally supported in hybrid reachability tools such as\nSpaceEx or Flow*. The extension supports certain types of partial derivatives\nand equational constraints. These extensions allow users to express, among\nother things, the Euler-Lagrangian equation, and to capture practically\nrelevant constraints that arise naturally in mechanical systems. Achieving this\nlevel of expressiveness requires using a binding time-analysis (BTA), program\ndifferentiation, symbolic Gaussian elimination, and abstract interpretation\nusing interval analysis. Except for BTA, the other components are either\nreadily available or can be easily added to most reachability tools. The paper\ntherefore focuses on presenting both the declarative and algorithmic\nspecifications for the BTA phase, and establishes the soundness of the\nalgorithmic specifications with respect to the declarative one.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 00:57:40 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Zeng", "Yingfu", "", "Rice University"], ["Bartha", "Ferenc", "", "Rice University"], ["Taha", "Walid", "", "Halmstad University"]]}, {"id": "1704.03202", "submitter": "Laura Kovacs", "authors": "Laura Kovacs", "title": "Symbolic Computation and Automated Reasoning for Program Analysis", "comments": "Paper published at iFM 2016", "journal-ref": null, "doi": "10.1007/978-3-319-33693-0_2", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This talk describes how a combination of symbolic computation techniques with\nfirst-order theorem proving can be used for solving some challenges of\nautomating program analysis, in particular for generating and proving\nproperties about the logically complex parts of software. The talk will first\npresent how computer algebra methods, such as Groebner basis computation,\nquantifier elimination and algebraic recurrence solving, help us in inferring\nproperties of program loops with non-trivial arithmetic. Typical properties\ninferred by our work are loop invariants and expressions bounding the number of\nloop iterations. The talk will then describe our work to generate first-order\nproperties of programs with unbounded data structures, such as arrays. For\ndoing so, we use saturation-based first-order theorem proving and extend\nfirst-order provers with support for program analysis. Since program analysis\nrequires reasoning in the combination of first-order theories of data\nstructures, the talk also discusses new features in firstorder theorem proving,\nsuch as inductive reasoning and built-in boolean sort. These extensions allow\nus to express program properties directly in first-order logic and hence use\nfurther first-order theorem provers to reason about program properties.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 08:58:55 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Kovacs", "Laura", ""]]}, {"id": "1704.03324", "submitter": "Luis Veiga", "authors": "Duarte Patr\\'icio and Jos\\'e Sim\\~ao and Lu\\'is Veiga", "title": "Gang-GC: Locality-aware Parallel Data Placement Optimizations for\n  Key-Value Storages", "comments": null, "journal-ref": null, "doi": null, "report-no": "INESC-ID Tec. Rep. 5/2017, Feb 2017", "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many cloud applications rely on fast and non-relational storage to aid in the\nprocessing of large amounts of data. Managed runtimes are now widely used to\nsupport the execution of several storage solutions of the NoSQL movement,\nparticularly when dealing with big data key-value store-driven applications.\nThe benefits of these runtimes can however be limited by modern parallel\nthroughput-oriented GC algorithms, where related objects have the potential to\nbe dispersed in memory, either in the same or different generations. In the\nlong run this causes more page faults and degradation of locality on\nsystem-level memory caches.\n  We propose, Gang-CG, an extension to modern heap layouts and to a parallel GC\nalgorithm to promote locality between groups of related objects. This is done\nwithout extensive profiling of the applications and in a way that is\ntransparent to the programmer, without the need to use specialized data\nstructures. The heap layout and algorithmic extensions were implemented over\nthe Parallel Scavenge garbage collector of the HotSpot JVM\\@.\n  Using microbenchmarks that capture the architecture of several key-value\nstores databases, we show negligible overhead in frequent operations such as\nthe allocation of new objects and improvements to the access speed of data,\nsupported by lower misses in system-level memory caches. Overall, we show a 6\\%\nimprovement in the average time of read and update operations and an average\ndecrease of 12.4\\% in page faults.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 14:44:29 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Patr\u00edcio", "Duarte", ""], ["Sim\u00e3o", "Jos\u00e9", ""], ["Veiga", "Lu\u00eds", ""]]}, {"id": "1704.03394", "submitter": "Zhoulai Fu", "authors": "Zhoulai Fu, Zhendong Su", "title": "Achieving High Coverage for Floating-point Code via Unconstrained\n  Programming (Extended Version)", "comments": "Extended version of Fu and Su's PLDI'17 paper. arXiv admin note: text\n  overlap with arXiv:1610.01133", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving high code coverage is essential in testing, which gives us\nconfidence in code quality. Testing floating-point code usually requires\npainstaking efforts in handling floating-point constraints, e.g., in symbolic\nexecution. This paper turns the challenge of testing floating-point code into\nthe opportunity of applying unconstrained programming --- the mathematical\nsolution for calculating function minimum points over the entire search space.\nOur core insight is to derive a representing function from the floating-point\nprogram, any of whose minimum points is a test input guaranteed to exercise a\nnew branch of the tested program. This guarantee allows us to achieve high\ncoverage of the floating-point program by repeatedly minimizing the\nrepresenting function.\n  We have realized this approach in a tool called CoverMe and conducted an\nextensive evaluation of it on Sun's C math library. Our evaluation results show\nthat CoverMe achieves, on average, 90.8% branch coverage in 6.9 seconds,\ndrastically outperforming our compared tools: (1) Random testing, (2) AFL, a\nhighly optimized, robust fuzzer released by Google, and (3) Austin, a\nstate-of-the-art coverage-based testing tool designed to support floating-point\ncode.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 16:15:59 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Fu", "Zhoulai", ""], ["Su", "Zhendong", ""]]}, {"id": "1704.04460", "submitter": "Alexander Singh", "authors": "Alexander Singh, Konstantinos Giannakis, Theodore Andronikos", "title": "Qumin, a minimalist quantum programming language", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce Qumin, a novel quantum programming language with a\nfocus on providing an easy to use, minimalist, high-level, and easily\nextensible platform for quantum programming. Qumin's design concentrates on\nencompassing the various interactions between classical and quantum computation\nvia the use of two sublanguages: an untyped one that handles classical\npreparation and control, and one linearly typed that explicitly handles quantum\nroutines. This allows the classical part of the language to be freely used for\ngeneral programming while placing restrictions on the quantum part that enforce\nrules of quantum computing like the no-cloning of qubits.\n  We describe both the language's theoretical foundations in terms of lambda\ncalculi and linear type systems, and more practical matters such as\nimplementations of algorithms and useful programming tools like matrix and\noracle generators that streamline the interaction of the classical and quantum\nfragments of a program. Finally, we provide an experimental open-source\nimplementation of an interpreter, typechecker and related tools for the\nlanguage (which can be found in \\url{https://github.com/wintershammer/QImp}).\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 15:38:22 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Singh", "Alexander", ""], ["Giannakis", "Konstantinos", ""], ["Andronikos", "Theodore", ""]]}, {"id": "1704.04647", "submitter": "Ugo Dal Lago", "authors": "Ugo Dal Lago, Francesco Gavazzo, Paul Blain Levy", "title": "Effectful Applicative Bisimilarity: Monads, Relators, and Howe's Method\n  (Long Version)", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Abramsky's applicative bisimilarity abstractly, in the context of\ncall-by-value $\\lambda$-calculi with algebraic effects. We first of all endow a\ncomputational $\\lambda$-calculus with a monadic operational semantics. We then\nshow how the theory of relators provides precisely what is needed to generalise\napplicative bisimilarity to such a calculus, and to single out those monads and\nrelators for which applicative bisimilarity is a congruence, thus a sound\nmethodology for program equivalence. This is done by studying Howe's method in\nthe abstract.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 15:20:11 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Lago", "Ugo Dal", ""], ["Gavazzo", "Francesco", ""], ["Levy", "Paul Blain", ""]]}, {"id": "1704.05004", "submitter": "Nathan Burow", "authors": "Nathan Burow, Derrick McKee, Scott A. Carr, Mathias Payer", "title": "CUP: Comprehensive User-Space Protection for C/C++", "comments": null, "journal-ref": null, "doi": "10.1145/3196494.3196540", "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory corruption vulnerabilities in C/C++ applications enable attackers to\nexecute code, change data, and leak information. Current memory sanitizers do\nno provide comprehensive coverage of a program's data. In particular, existing\ntools focus primarily on heap allocations with limited support for stack\nallocations and globals. Additionally, existing tools focus on the main\nexecutable with limited support for system libraries. Further, they suffer from\nboth false positives and false negatives.\n  We present Comprehensive User-Space Protection for C/C++, CUP, an LLVM\nsanitizer that provides complete spatial and probabilistic temporal memory\nsafety for C/C++ program on 64-bit architectures (with a prototype\nimplementation for x86_64). CUP uses a hybrid metadata scheme that supports all\nprogram data including globals, heap, or stack and maintains the ABI. Compared\nto existing approaches with the NIST Juliet test suite, CUP reduces false\nnegatives by 10x (0.1%) compared to the state of the art LLVM sanitizers, and\nproduces no false positives. CUP instruments all user-space code, including\nlibc and other system libraries, removing them from the trusted code base.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 15:39:06 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Burow", "Nathan", ""], ["McKee", "Derrick", ""], ["Carr", "Scott A.", ""], ["Payer", "Mathias", ""]]}, {"id": "1704.05169", "submitter": "EPTCS", "authors": "Guillaume Bonfante (Universit\\'e de Lorraine, France), Georg Moser\n  (Universit\\\"at Innsbruck, Austria)", "title": "Proceedings 8th Workshop on Developments in Implicit Computational\n  Complexity and 5th Workshop on Foundational and Practical Aspects of Resource\n  Analysis", "comments": null, "journal-ref": "EPTCS 248, 2017", "doi": "10.4204/EPTCS.248", "report-no": null, "categories": "cs.LO cs.CC cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DICE workshop explores the area of Implicit Computational Complexity\n(ICC), which grew out from several proposals to use logic and formal methods to\nprovide languages for complexity-bounded computation (e.g. Ptime, Logspace\ncomputation). It aims at studying the computational complexity of programs\nwithout referring to external measuring conditions or a particular machine\nmodel, but only by considering language restrictions or logical/computational\nprinciples entailing complexity properties.\n  The FOPARA workshop serves as a forum for presenting original research\nresults that are relevant to the analysis of resource (e.g. time, space,\nenergy) consumption by computer programs. The workshop aims to bring together\nthe researchers that work on foundational issues with the researchers that\nfocus more on practical results. Therefore, both theoretical and practical\ncontributions are encouraged. We also encourage papers that combine theory and\npractice.\n  Given the complementarity and the synergy between these two communities, and\nfollowing the successful experience of co-location of DICE-FOPARA 2015 in\nLondon at ETAPS 2015, we hold these two workshops together at ETAPS 2017, which\ntakes place in Uppsala, Sweden. The provided proceedings collect the papers\naccepted at the workshop.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 01:52:49 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Bonfante", "Guillaume", "", "Universit\u00e9 de Lorraine, France"], ["Moser", "Georg", "", "Universit\u00e4t Innsbruck, Austria"]]}, {"id": "1704.05316", "submitter": "Sabri Pllana", "authors": "Suejb Memeti and Lu Li and Sabri Pllana and Joanna Kolodziej and\n  Christoph Kessler", "title": "Benchmarking OpenCL, OpenACC, OpenMP, and CUDA: programming\n  productivity, performance, and energy consumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern parallel computing systems are heterogeneous at their node level.\nSuch nodes may comprise general purpose CPUs and accelerators (such as, GPU, or\nIntel Xeon Phi) that provide high performance with suitable energy-consumption\ncharacteristics. However, exploiting the available performance of heterogeneous\narchitectures may be challenging. There are various parallel programming\nframeworks (such as, OpenMP, OpenCL, OpenACC, CUDA) and selecting the one that\nis suitable for a target context is not straightforward.\n  In this paper, we study empirically the characteristics of OpenMP, OpenACC,\nOpenCL, and CUDA with respect to programming productivity, performance, and\nenergy. To evaluate the programming productivity we use our homegrown tool\nCodeStat, which enables us to determine the percentage of code lines that was\nrequired to parallelize the code using a specific framework. We use our tool\nx-MeterPU to evaluate the energy consumption and the performance. Experiments\nare conducted using the industry-standard SPEC benchmark suite and the Rodinia\nbenchmark suite for accelerated computing on heterogeneous systems that combine\nIntel Xeon E5 Processors with a GPU accelerator or an Intel Xeon Phi\nco-processor.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 13:08:35 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Memeti", "Suejb", ""], ["Li", "Lu", ""], ["Pllana", "Sabri", ""], ["Kolodziej", "Joanna", ""], ["Kessler", "Christoph", ""]]}, {"id": "1704.05585", "submitter": "EPTCS", "authors": "Martin Avanzini (University of Innsbruck), Ugo Dal Lago (University of\n  Bologna and INRIA)", "title": "Automated Sized-Type Inference and Complexity Analysis", "comments": "In Proceedings DICE-FOPARA 2017, arXiv:1704.05169", "journal-ref": "EPTCS 248, 2017, pp. 7-16", "doi": "10.4204/EPTCS.248.5", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new methodology for the complexity analysis of\nhigher-order functional programs, which is based on three components: a\npowerful type system for size analysis and a sound type inference procedure for\nit, a ticking monadic transformation and a concrete tool for constraint\nsolving. Noticeably, the presented methodology can be fully automated, and is\nable to analyse a series of examples which cannot be handled by most competitor\nmethodologies. This is possible due to various key ingredients, and in\nparticular an abstract index language and index polymorphism at higher ranks. A\nprototype implementation is available.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 02:19:27 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Avanzini", "Martin", "", "University of Innsbruck"], ["Lago", "Ugo Dal", "", "University of\n  Bologna and INRIA"]]}, {"id": "1704.05589", "submitter": "EPTCS", "authors": "Jean-Yves Moyen (Department of Computer Science University of\n  Copenhagen (DIKU)), Thomas Rubiano (Universit\\'e Paris 13 - LIPN), Thomas\n  Seiller (Department of Computer Science University of Copenhagen (DIKU))", "title": "Loop Quasi-Invariant Chunk Motion by peeling with statement composition", "comments": "In Proceedings DICE-FOPARA 2017, arXiv:1704.05169", "journal-ref": "EPTCS 248, 2017, pp. 47-59", "doi": "10.4204/EPTCS.248.9", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several techniques for analysis and transformations are used in compilers.\nAmong them, the peeling of loops for hoisting quasi-invariants can be used to\noptimize generated code, or simply ease developers' lives. In this paper, we\nintroduce a new concept of dependency analysis borrowed from the field of\nImplicit Computational Complexity (ICC), allowing to work with composed\nstatements called Chunks to detect more quasi-invariants. Based on an\noptimization idea given on a WHILE language, we provide a transformation method\n- reusing ICC concepts and techniques - to compilers. This new analysis\ncomputes an invariance degree for each statement or chunks of statements by\nbuilding a new kind of dependency graph, finds the maximum or worst dependency\ngraph for loops, and recognizes if an entire block is Quasi-Invariant or not.\nThis block could be an inner loop, and in that case the computational\ncomplexity of the overall program can be decreased. We already implemented a\nproof of concept on a toy C parser 1 analysing and transforming the AST\nrepresentation. In this paper, we introduce the theory around this concept and\npresent a prototype analysis pass implemented on LLVM. In a very near future,\nwe will implement the corresponding transformation and provide benchmarks\ncomparisons.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 02:20:31 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Moyen", "Jean-Yves", "", "Department of Computer Science University of\n  Copenhagen"], ["Rubiano", "Thomas", "", "Universit\u00e9 Paris 13 - LIPN"], ["Seiller", "Thomas", "", "Department of Computer Science University of Copenhagen"]]}, {"id": "1704.06611", "submitter": "Jonathon Cai", "authors": "Jonathon Cai, Richard Shin, Dawn Song", "title": "Making Neural Programming Architectures Generalize via Recursion", "comments": "Published in ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirically, neural networks that attempt to learn programs from data have\nexhibited poor generalizability. Moreover, it has traditionally been difficult\nto reason about the behavior of these models beyond a certain level of input\ncomplexity. In order to address these issues, we propose augmenting neural\narchitectures with a key abstraction: recursion. As an application, we\nimplement recursion in the Neural Programmer-Interpreter framework on four\ntasks: grade-school addition, bubble sort, topological sort, and quicksort. We\ndemonstrate superior generalizability and interpretability with small amounts\nof training data. Recursion divides the problem into smaller pieces and\ndrastically reduces the domain of each neural network component, making it\ntractable to prove guarantees about the overall system's behavior. Our\nexperience suggests that in order for neural architectures to robustly learn\nprogram semantics, it is necessary to incorporate a concept like recursion.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 16:02:26 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Cai", "Jonathon", ""], ["Shin", "Richard", ""], ["Song", "Dawn", ""]]}, {"id": "1704.07004", "submitter": "Hanwen Wu", "authors": "Hanwen Wu and Hongwei Xi", "title": "Dependent Session Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Session types offer a type-based discipline for enforcing communication\nprotocols in distributed programming. We have previously formalized simple\nsession types in the setting of multi-threaded $\\lambda$-calculus with linear\ntypes. In this work, we build upon our earlier work by presenting a form of\ndependent session types (of DML-style). The type system we formulate provides\nlinearity and duality guarantees with no need for any runtime checks or special\nencodings. Our formulation of dependent session types is the first of its kind,\nand it is particularly suitable for practical implementation. As an example, we\ndescribe one implementation written in ATS that compiles to an Erlang/Elixir\nback-end.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 00:33:36 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Wu", "Hanwen", ""], ["Xi", "Hongwei", ""]]}, {"id": "1704.07234", "submitter": "Natalia Chechina", "authors": "Phil Trinder, Natalia Chechina, Nikolaos Papaspyrou, Konstantinos\n  Sagonas, Simon Thompson, Stephen Adams, Stavros Aronis, Robert Baker, Eva\n  Bihari, Olivier Boudeville, Francesco Cesarini, Maurizio Di Stefano, Sverker\n  Eriksson, Viktoria Fordos, Amir Ghaffari, Aggelos Giantsios, Rickard Green,\n  Csaba Hoch, David Klaftenegger, Huiqing Li, Kenneth Lundin, Kenneth\n  Mackenzie, Katerina Roukounaki, Yiannis Tsiouris, Kjell Winblad", "title": "Scaling Reliably: Improving the Scalability of the Erlang Distributed\n  Actor Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed actor languages are an effective means of constructing scalable\nreliable systems, and the Erlang programming language has a well-established\nand influential model. While Erlang model conceptually provides reliable\nscalability, it has some inherent scalability limits and these force developers\nto depart from the model at scale. This article establishes the scalability\nlimits of Erlang systems, and reports the work to improve the language\nscalability.\n  We systematically study the scalability limits of Erlang and address the\nissues at the virtual machine (VM), language, and tool levels. More\nspecifically: (1) We have evolved the Erlang VM so that it can work effectively\nin large scale single-host multicore and NUMA architectures. We have made\nimportant architectural improvements to the Erlang/OTP. (2) We have designed\nand implemented Scalable Distributed (SD) Erlang libraries to address\nlanguage-level scalability issues, and provided and validated a set of\nsemantics for the new language constructs. (3) To make large Erlang systems\neasier to deploy, monitor, and debug we have developed and made open source\nreleases of five complementary tools, some specific to SD Erlang.\n  Throughout the article we use two case studies to investigate the\ncapabilities of our new technologies and tools: a distributed hash table based\nOrbit calculation and Ant Colony Optimisation (ACO). Chaos Monkey experiments\nshow that two versions of ACO survive random process failure and hence that SD\nErlang preserves the Erlang reliability model. Even for programs with no global\nrecovery data to maintain, SD Erlang partitions the network to reduce network\ntraffic and hence improves performance of the Orbit and ACO benchmarks above 80\nhosts. ACO measurements show that maintaining global recovery data dramatically\nlimits scalability; however scalability is recovered by partitioning the\nrecovery data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 13:52:28 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 09:35:43 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Trinder", "Phil", ""], ["Chechina", "Natalia", ""], ["Papaspyrou", "Nikolaos", ""], ["Sagonas", "Konstantinos", ""], ["Thompson", "Simon", ""], ["Adams", "Stephen", ""], ["Aronis", "Stavros", ""], ["Baker", "Robert", ""], ["Bihari", "Eva", ""], ["Boudeville", "Olivier", ""], ["Cesarini", "Francesco", ""], ["Di Stefano", "Maurizio", ""], ["Eriksson", "Sverker", ""], ["Fordos", "Viktoria", ""], ["Ghaffari", "Amir", ""], ["Giantsios", "Aggelos", ""], ["Green", "Rickard", ""], ["Hoch", "Csaba", ""], ["Klaftenegger", "David", ""], ["Li", "Huiqing", ""], ["Lundin", "Kenneth", ""], ["Mackenzie", "Kenneth", ""], ["Roukounaki", "Katerina", ""], ["Tsiouris", "Yiannis", ""], ["Winblad", "Kjell", ""]]}, {"id": "1704.08073", "submitter": "Manuel Mazzara", "authors": "Claudio Guidi, Ivan Lanese, Manuel Mazzara, Fabrizio Montesi", "title": "Microservices: a Language-based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microservices is an emerging development paradigm where software is obtained\nby composing autonomous entities, called (micro)services. However, microservice\nsystems are currently developed using general-purpose programming languages\nthat do not provide dedicated abstractions for service composition. Instead,\ncurrent practice is focused on the deployment aspects of microservices, in\nparticular by using containerization. In this chapter, we make the case for a\nlanguage-based approach to the engineering of microservice architectures, which\nwe believe is complementary to current practice. We discuss the approach in\ngeneral, and then we instantiate it in terms of the Jolie programming language.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 12:17:20 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Guidi", "Claudio", ""], ["Lanese", "Ivan", ""], ["Mazzara", "Manuel", ""], ["Montesi", "Fabrizio", ""]]}, {"id": "1704.08909", "submitter": "Francesco Ranzato", "authors": "Francesco Ranzato", "title": "A Constructive Framework for Galois Connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract interpretation-based static analyses rely on abstract domains of\nprogram properties, such as intervals or congruences for integer variables.\nGalois connections (GCs) between posets provide the most widespread and useful\nformal tool for mathematically specifying abstract domains. Recently, Darais\nand Van Horn [2016] put forward a notion of constructive Galois connection for\nunordered sets (rather than posets), which allows to define abstract domains in\na so-called mechanized and calculational proof style and therefore enables the\nuse of proof assistants like Coq and Agda for automatically extracting verified\nalgorithms of static analysis. We show here that constructive GCs are\nisomorphic, in a precise and comprehensive meaning including sound abstract\nfunctions, to so-called partitioning GCs--an already known class of GCs which\nallows to cast standard set partitions as an abstract domain. Darais and Van\nHorn [2016] also provide a notion of constructive GC for posets, which we prove\nto be isomorphic to plain GCs and therefore lose their constructive attribute.\nDrawing on these findings, we put forward and advocate the use of purely\npartitioning GCs, a novel class of constructive abstract domains for a\nmechanized approach to abstract interpretation. We show that this class of\nabstract domains allows us to represent a set partition with more flexibility\nwhile retaining a constructive approach to Galois connections.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 12:58:01 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Ranzato", "Francesco", ""]]}, {"id": "1704.09026", "submitter": "Andr\\'es Ezequiel Viso", "authors": "Juan Edi and Andr\\'es Viso and Eduardo Bonelli", "title": "Efficient Type Checking for Path Polymorphism", "comments": null, "journal-ref": null, "doi": "10.4230/LIPIcs.TYPES.2015.6", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A type system combining type application, constants as types, union types\n(associative, commutative and idempotent) and recursive types has recently been\nproposed for statically typing path polymorphism, the ability to define\nfunctions that can operate uniformly over recursively specified applicative\ndata structures. A typical pattern such functions resort to is $x\\,y$ which\ndecomposes a compound, in other words any applicative tree structure, into its\nparts. We study type-checking for this type system in two stages. First we\npropose algorithms for checking type equivalence and subtyping based on\ncoinductive characterizations of those relations. We then formulate a\nsyntax-directed presentation and prove its equivalence with the original one.\nThis yields a type-checking algorithm which unfortunately has exponential time\ncomplexity in the worst case. A second algorithm is then proposed, based on\nautomata techniques, which yields a polynomial-time type-checking algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 17:50:24 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Edi", "Juan", ""], ["Viso", "Andr\u00e9s", ""], ["Bonelli", "Eduardo", ""]]}]