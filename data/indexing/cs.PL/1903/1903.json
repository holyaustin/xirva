[{"id": "1903.00982", "submitter": "Aaron Weiss", "authors": "Aaron Weiss and Olek Gierczak and Daniel Patterson and Nicholas D.\n  Matsakis and Amal Ahmed", "title": "Oxide: The Essence of Rust", "comments": "In submission (25 pages + technical appendices) (corrected authors in\n  metadata from v2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rust claims to advance industrial programming by bridging the gap between\nlow-level systems programming and high-level application programming. At the\nheart of the argument that this enables programmers to build more reliable and\nefficient software is the borrow checker - a novel approach to ownership that\naims to balance type system expressivity with usability. And yet, to date there\nis no core type system that captures Rust's notion of ownership and borrowing,\nand hence no foundation for research on Rust to build upon.\n  In this work, we set out to capture the essence of this model of ownership by\ndeveloping a type systems account of Rust's borrow checker. We present Oxide, a\nformalized programming language close to source-level Rust (but with\nfully-annotated types). This presentation takes a new view of lifetimes as an\napproximation of the provenances of references, and our type system is able to\nautomatically compute this information through a substructural typing judgment.\nWe provide the first syntactic proof of type safety for borrow checking using\nprogress and preservation. Oxide is a simpler formulation of borrow checking -\nincluding recent features such as non-lexical lifetimes - that we hope\nresearchers will be able to use as the basis for work on Rust.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 20:22:37 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 21:22:04 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 00:04:32 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Weiss", "Aaron", ""], ["Gierczak", "Olek", ""], ["Patterson", "Daniel", ""], ["Matsakis", "Nicholas D.", ""], ["Ahmed", "Amal", ""]]}, {"id": "1903.01237", "submitter": "Catalin Hritcu", "authors": "Kenji Maillard, Danel Ahman, Robert Atkey, Guido Martinez, Catalin\n  Hritcu, Exequiel Rivas, \\'Eric Tanter", "title": "Dijkstra Monads for All", "comments": "Long version of ICFP'19 camera ready paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a general semantic framework for verifying programs with\narbitrary monadic side-effects using Dijkstra monads, which we define as\nmonad-like structures indexed by a specification monad. We prove that any monad\nmorphism between a computational monad and a specification monad gives rise to\na Dijkstra monad, which provides great flexibility for obtaining Dijkstra\nmonads tailored to the verification task at hand. We moreover show that a large\nvariety of specification monads can be obtained by applying monad transformers\nto various base specification monads, including predicate transformers and\nHoare-style pre- and postconditions. For defining correct monad transformers,\nwe propose a language inspired by Moggi's monadic metalanguage that is\nparameterized by a dependent type theory. We also develop a notion of algebraic\noperations for Dijkstra monads, and start to investigate two ways of also\naccommodating effect handlers. We implement our framework in both Coq and F*,\nand illustrate that it supports a wide variety of verification styles for\neffects such as exceptions, nondeterminism, state, input-output, and general\nrecursion.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 13:46:38 GMT"}, {"version": "v2", "created": "Sat, 18 May 2019 14:00:39 GMT"}, {"version": "v3", "created": "Sun, 2 Jun 2019 18:43:50 GMT"}, {"version": "v4", "created": "Wed, 26 Jun 2019 16:23:55 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Maillard", "Kenji", ""], ["Ahman", "Danel", ""], ["Atkey", "Robert", ""], ["Martinez", "Guido", ""], ["Hritcu", "Catalin", ""], ["Rivas", "Exequiel", ""], ["Tanter", "\u00c9ric", ""]]}, {"id": "1903.01665", "submitter": "Unnikrishnan Cheramangalath", "authors": "Bikash Gogoi, Unnikrishnan Cheramangalath, Rupesh Nasre", "title": "Custom Code Generation for a Graph DSL", "comments": "submitted to Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph algorithms are at the heart of several applications, and achieving high\nperformance with them has become critical due to the tremendous growth of\nirregular data. However, irregular algorithms are quite challenging to\nparallelize automatically, due to access patterns influenced by the input\ngraph, which is unavailable until execution. Former research has addressed this\nissue by designing domain-specific languages (DSLs) for graph algorithms, which\nrestrict generality but allow efficient code-generation for various backends.\nSuch DSLs are, however, too rigid, and do not adapt to changes in backends or\nto input graph properties or to both. We narrate our experiences in making an\nexisting DSL, named Falcon, adaptive. The biggest challenge in the process is\nto not change the DSL code for specifying the algorithm. We illustrate the\neffectiveness of our proposal by auto-generating codes for vertex-based versus\nedge-based graph processing, synchronous versus asynchronous execution, and CPU\nversus GPU backends from the same specification.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 04:39:45 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Gogoi", "Bikash", ""], ["Cheramangalath", "Unnikrishnan", ""], ["Nasre", "Rupesh", ""]]}, {"id": "1903.01855", "submitter": "Akshay Agrawal", "authors": "Akshay Agrawal, Akshay Naresh Modi, Alexandre Passos, Allen Lavoie,\n  Ashish Agarwal, Asim Shankar, Igor Ganichev, Josh Levenberg, Mingsheng Hong,\n  Rajat Monga, Shanqing Cai", "title": "TensorFlow Eager: A Multi-Stage, Python-Embedded DSL for Machine\n  Learning", "comments": null, "journal-ref": "Proc. of the 2nd SysML Conference, 2019", "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TensorFlow Eager is a multi-stage, Python-embedded domain-specific language\nfor hardware-accelerated machine learning, suitable for both interactive\nresearch and production. TensorFlow, which TensorFlow Eager extends, requires\nusers to represent computations as dataflow graphs; this permits compiler\noptimizations and simplifies deployment but hinders rapid prototyping and\nrun-time dynamism. TensorFlow Eager eliminates these usability costs without\nsacrificing the benefits furnished by graphs: It provides an imperative\nfront-end to TensorFlow that executes operations immediately and a JIT tracer\nthat translates Python functions composed of TensorFlow operations into\nexecutable dataflow graphs. TensorFlow Eager thus offers a multi-stage\nprogramming model that makes it easy to interpolate between imperative and\nstaged execution in a single package.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 03:08:20 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Agrawal", "Akshay", ""], ["Modi", "Akshay Naresh", ""], ["Passos", "Alexandre", ""], ["Lavoie", "Allen", ""], ["Agarwal", "Ashish", ""], ["Shankar", "Asim", ""], ["Ganichev", "Igor", ""], ["Levenberg", "Josh", ""], ["Hong", "Mingsheng", ""], ["Monga", "Rajat", ""], ["Cai", "Shanqing", ""]]}, {"id": "1903.02482", "submitter": "Yuan Zhou", "authors": "Yuan Zhou, Bradley J. Gram-Hansen, Tobias Kohn, Tom Rainforth,\n  Hongseok Yang, Frank Wood", "title": "LF-PPL: A Low-Level First Order Probabilistic Programming Language for\n  Non-Differentiable Models", "comments": "Published in the proceedings of the 22nd International Conference on\n  Artificial Intelligence and Statistics (AISTATS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new Low-level, First-order Probabilistic Programming Language\n(LF-PPL) suited for models containing a mix of continuous, discrete, and/or\npiecewise-continuous variables. The key success of this language and its\ncompilation scheme is in its ability to automatically distinguish parameters\nthe density function is discontinuous with respect to, while further providing\nruntime checks for boundary crossings. This enables the introduction of new\ninference engines that are able to exploit gradient information, while\nremaining efficient for models which are not everywhere differentiable. We\ndemonstrate this ability by incorporating a discontinuous Hamiltonian Monte\nCarlo (DHMC) inference engine that is able to deliver automated and efficient\ninference for non-differentiable models. Our system is backed up by a\nmathematical formalism that ensures that any model expressed in this language\nhas a density with measure zero discontinuities to maintain the validity of the\ninference engine.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 16:29:20 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Zhou", "Yuan", ""], ["Gram-Hansen", "Bradley J.", ""], ["Kohn", "Tobias", ""], ["Rainforth", "Tom", ""], ["Yang", "Hongseok", ""], ["Wood", "Frank", ""]]}, {"id": "1903.02835", "submitter": "Chandrika Bhardwaj", "authors": "Chandrika Bhardwaj, Sanjiva Prasad", "title": "Only Connect, Securely", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lattice model proposed by Denning in her seminal work provided secure\ninformation flow analyses with an intuitive and uniform mathematical\nfoundation. Different organisations, however, may employ quite different\nsecurity lattices. In this paper, we propose a connection framework that\npermits different organisations to exchange information while maintaining both\nsecurity of information flows as well as their autonomy in formulating and\nmaintaining security policy. Our prescriptive framework is based on the\nrigorous mathematical framework of Lagois connections given by Melton, together\nwith a simple operational model for transferring object data between domains.\nThe merit of this formulation is that it is simple, minimal, adaptable and\nintuitive, and provides a formal framework for establishing secure information\nflow across autonomous interacting organisations. We show that our framework is\nsemantically sound, by proving that the connections proposed preserve standard\ncorrectness notions such as non-interference.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 11:12:16 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Bhardwaj", "Chandrika", ""], ["Prasad", "Sanjiva", ""]]}, {"id": "1903.03229", "submitter": "John Feser", "authors": "John K. Feser, Samuel Madden, Nan Tang, Armando Solar-Lezama", "title": "Deductive Optimization of Relational Data Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing the physical data storage and retrieval of data are two key\ndatabase management problems. In this paper, we propose a language that can\nexpress a wide range of physical database layouts, going well beyond the row-\nand column-based methods that are widely used in database management systems.\nWe use deductive synthesis to turn a high-level relational representation of a\ndatabase query into a highly optimized low-level implementation which operates\non a specialized layout of the dataset. We build a compiler for this language\nand conduct experiments using a popular database benchmark, which shows that\nthe performance of these specialized queries is competitive with a\nstate-of-the-art in memory compiled database system.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 00:40:21 GMT"}, {"version": "v2", "created": "Sun, 14 Jul 2019 10:30:29 GMT"}, {"version": "v3", "created": "Wed, 5 Feb 2020 22:07:28 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Feser", "John K.", ""], ["Madden", "Samuel", ""], ["Tang", "Nan", ""], ["Solar-Lezama", "Armando", ""]]}, {"id": "1903.03276", "submitter": "Prakash Murali", "authors": "Prakash Murali and Ali Javadi-Abhari and Frederic T. Chong and\n  Margaret Martonosi", "title": "Formal Constraint-based Compilation for Noisy Intermediate-Scale Quantum\n  Systems", "comments": "Invited paper in Special Issue on Quantum Computer Architecture: a\n  full-stack overview, Microprocessors and Microsystems", "journal-ref": "Microprocessors and Microsystems 2019", "doi": "10.1016/j.micpro.2019.02.005", "report-no": null, "categories": "cs.PL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy, intermediate-scale quantum (NISQ) systems are expected to have a few\nhundred qubits, minimal or no error correction, limited connectivity and limits\non the number of gates that can be performed within the short coherence window\nof the machine. The past decade's research on quantum programming languages and\ncompilers is directed towards large systems with thousands of qubits. For near\nterm quantum systems, it is crucial to design tool flows which make efficient\nuse of the hardware resources without sacrificing the ease and portability of a\nhigh-level programming environment. In this paper, we present a compiler for\nthe Scaffold quantum programming language in which aggressive optimization\nspecifically targets NISQ machines with hundreds of qubits. Our compiler\nextracts gates from a Scaffold program, and formulates a constrained\noptimization problem which considers both program characteristics and machine\nconstraints. Using the Z3 SMT solver, the compiler maps program qubits to\nhardware qubits, schedules gates, and inserts CNOT routing operations while\noptimizing the overall execution time. The output of the optimization is used\nto produce target code in the OpenQASM language, which can be executed on\nexisting quantum hardware such as the 16-qubit IBM machine. Using real and\nsynthetic benchmarks, we show that it is feasible to synthesize near-optimal\ncompiled code for current and small NISQ systems. For large programs and\nmachine sizes, the SMT optimization approach can be used to synthesize compiled\ncode that is guaranteed to finish within the coherence window of the machine.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 04:13:58 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Murali", "Prakash", ""], ["Javadi-Abhari", "Ali", ""], ["Chong", "Frederic T.", ""], ["Martonosi", "Margaret", ""]]}, {"id": "1903.03501", "submitter": "Aurojit Panda", "authors": "Aurojit Panda", "title": "Certifying Safety when Implementing Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Ensuring the correctness of distributed system implementations remains a\nchallenging and largely unaddressed problem. In this paper we present a\nprotocol that can be used to certify the safety of consensus implementations.\nOur proposed protocol is efficient both in terms of the number of additional\nmessages sent and their size, and is designed to operate correctly in the\npresence of $n-1$ nodes failing in an $n$ node distributed system (assuming\nfail-stop failures). We also comment on how our construction might be\ngeneralized to certify other protocols and invariants.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 15:28:44 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Panda", "Aurojit", ""]]}, {"id": "1903.03671", "submitter": "Brendan Fong", "authors": "Brendan Fong and Michael Johnson", "title": "Lenses and Learners", "comments": "14 pages", "journal-ref": "In: J. Cheney, H-S. Ko (eds.): Proceedings of the Eighth\n  International Workshop on Bidirectional Transformations (Bx 2019),\n  Philadelphia, PA, USA, June 4, 2019, published at http://ceur-ws.org", "doi": null, "report-no": null, "categories": "cs.LG cs.PL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lenses are a well-established structure for modelling bidirectional\ntransformations, such as the interactions between a database and a view of it.\nLenses may be symmetric or asymmetric, and may be composed, forming the\nmorphisms of a monoidal category. More recently, the notion of a learner has\nbeen proposed: these provide a compositional way of modelling supervised\nlearning algorithms, and again form the morphisms of a monoidal category. In\nthis paper, we show that the two concepts are tightly linked. We show both that\nthere is a faithful, identity-on-objects symmetric monoidal functor embedding a\ncategory of asymmetric lenses into the category of learners, and furthermore\nthere is such a functor embedding the category of learners into a category of\nsymmetric lenses.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 17:57:59 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 18:07:27 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Fong", "Brendan", ""], ["Johnson", "Michael", ""]]}, {"id": "1903.05126", "submitter": "Moez AbdelGawad", "authors": "Moez A. AbdelGawad", "title": "Induction, Coinduction, and Fixed Points in PL Type Theory", "comments": "12 pages, formerly part of arXiv:1812.10026", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently we presented a concise survey of the formulation of the induction\nand coinduction principles, and some concepts related to them, in programming\nlanguages type theory and four other mathematical disciplines. The presentation\nin type theory involved the separate formulation of these concepts, first, in\nthe theory of types of functional programming languages and, next, in the\ntheory of types of object-oriented programming languages. In this article we\nshow that separating these two formulations helps demonstrate some of the\nfundamental differences between structural subtyping, predominant in functional\nprogramming languages, and nominal subtyping, predominant in object-oriented\nprogramming languages---including differences concerning type negation and\nconcerning the existence of inductive types, of coinductive types, and of\napproximations thereof. In the article we also motivate mutual coinduction and\nmutual coinductive types, and their approximations, and we discuss in brief the\npotential relevance of these concepts to object-oriented programming (OOP) type\ntheory.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 16:44:11 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["AbdelGawad", "Moez A.", ""]]}, {"id": "1903.05333", "submitter": "Yingzhou Zhang", "authors": "Yingzhou Zhang", "title": "SymPas: Symbolic Program Slicing", "comments": "29 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program slicing is a technique for simplifying programs by focusing on\nselected aspects of their behaviour. Current mainstream static slicing methods\noperate on the PDG (program dependence graph) or SDG (system dependence graph),\nbut these friendly graph representations may be expensive and error-prone for\nsome users. We attempt in this paper to study a light-weight approach of static\nprogram slicing, called Symbolic Program Slicing (SymPas), which works as a\ndataflow analysis on LLVM (Low-Level Virtual Machine). In our SymPas approach,\nslices are stored symbolically rather than procedure being re-analysed (cf.\nprocedure summaries). Instead of re-analysing a procedure multiple times to\nfind its slices for each callling context, SymPas calculates a single symbolic\n(or parameterized) slice which can be instantiated at call sites avoiding\nre-analysis; it is implemented in LLVM to perform slicing on its intermediate\nrepresentation (IR). For comparison, we systematically adapt IFDS\n(Interprocedural Finite Distributive Subset) analysis and the SDG-based slicing\nmethod (SDG-IFDS) to statically IR slice programs. Evaluated on open-source and\nbenchmark programs, our backward SymPas shows a factor-of-6 reduction in time\ncost and a factor-of-4 reduction in space cost, compared to backward SDG-IFDS,\nthus being more efficient. In addition, the result shows that after studying\nslices from 66 programs, ranging up to 336,800 IR instructions in size, SymPas\nis highly size-scalable.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 06:31:12 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Zhang", "Yingzhou", ""]]}, {"id": "1903.05879", "submitter": "Christian Graulund", "authors": "Patrick Bahr, Christian Graulund, Rasmus M{\\o}gelberg", "title": "Simply RaTT: A Fitch-style Modal Calculus for Reactive Programming\n  without Space Leaks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional reactive programming (FRP) is a paradigm for programming with\nsignals and events, allowing the user to describe reactive programs on a high\nlevel of abstraction. For this to make sense, an FRP language must ensure that\nall programs are causal, and can be implemented without introducing space leaks\nand time leaks. To this end, some FRP languages do not give direct access to\nsignals, but just to signal functions.\n  Recently, modal types have been suggested as an alternative approach to\nensuring causality in FRP languages in the synchronous case, giving direct\naccess to the signal and event abstractions. This paper presents Simply RaTT, a\nnew modal calculus for reactive programming. Unlike prior calculi, Simply RaTT\nuses a Fitch-style approach to modal types, which simplifies the type system\nand makes programs more concise. Echoing a previous result by Krishnaswami for\na different language, we devise an operational semantics that safely executes\nSimply RaTT programs without space leaks.\n  We also identify a source of time leaks present in other modal FRP languages:\nThe unfolding of fixed points in delayed computations. These time leaks are\neliminated by the Simply RaTT type system.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 09:50:15 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 13:23:14 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Bahr", "Patrick", ""], ["Graulund", "Christian", ""], ["M\u00f8gelberg", "Rasmus", ""]]}, {"id": "1903.06119", "submitter": "Roberto Bagnara", "authors": "Roberto Bagnara, Abramo Bagnara, Fabio Biselli, Michele Chiari,\n  Roberta Gori", "title": "Correct Approximation of IEEE 754 Floating-Point Arithmetic for Program\n  Verification", "comments": "80 pages, 19 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification of programs using floating-point arithmetic is challenging on\nseveral accounts. One of the difficulties of reasoning about such programs is\ndue to the peculiarities of floating-point arithmetic: rounding errors,\ninfinities, non-numeric objects (NaNs), signed zeroes, denormal numbers,\ndifferent rounding modes.... One possibility to reason about floating-point\narithmetic is to model a program computation path by means of a set of ternary\nconstraints of the form z = x op y and use constraint propagation techniques to\ninfer new information on the variables' possible values. In this setting, we\ndefine and prove the correctness of algorithms to precisely bound the value of\none of the variables x, y or z, starting from the bounds known for the other\ntwo. We do this for each of the operations and for each rounding mode defined\nby the IEEE 754 binary floating-point standard, even in the case the rounding\nmode in effect is only partially known. This is the first time that such\nso-called filtering algorithms are defined and their correctness is formally\nproved. This is an important slab for paving the way to formal verification of\nprograms that use floating-point arithmetics.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 18:31:49 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Bagnara", "Roberto", ""], ["Bagnara", "Abramo", ""], ["Biselli", "Fabio", ""], ["Chiari", "Michele", ""], ["Gori", "Roberta", ""]]}, {"id": "1903.06407", "submitter": "Fr\\'ed\\'eric Recoules", "authors": "Fr\\'ed\\'eric Recoules, S\\'ebastien Bardin, Richard Bonichon, Laurent\n  Mounier, Marie-Laure Potet", "title": "Get rid of inline assembly through verification-oriented lifting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal methods for software development have made great strides in the last\ntwo decades, to the point that their application in safety-critical embedded\nsoftware is an undeniable success. Their extension to non-critical software is\none of the notable forthcoming challenges. For example, C programmers regularly\nuse inline assembly for low-level optimizations and system primitives. This\nusually results in driving state-of-the-art formal analyzers developed for C\nineffective. We thus propose TInA, an automated, generic, trustable and\nverification-oriented lifting technique turning inline assembly into\nsemantically equivalent C code, in order to take advantage of existing C\nanalyzers. Extensive experiments on real-world C code with inline assembly\n(including GMP and ffmpeg) show the feasibility and benefits of TInA.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 08:39:27 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 05:49:20 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Recoules", "Fr\u00e9d\u00e9ric", ""], ["Bardin", "S\u00e9bastien", ""], ["Bonichon", "Richard", ""], ["Mounier", "Laurent", ""], ["Potet", "Marie-Laure", ""]]}, {"id": "1903.06514", "submitter": "Moez AbdelGawad", "authors": "Moez A. AbdelGawad", "title": "Mutual Coinduction", "comments": "22 pages (updated to include a proof of the sufficiency of\n  monotonicity)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present mutual coinduction as a dual of mutual induction and\nalso as a generalization of standard coinduction. In particular, we present a\nprecise formal definition of mutual induction and mutual coinduction. In the\nprocess we present the associated mutual induction and mutual coinduction proof\nprinciples, and we present the conditions under which these principles hold.\n  In spite of some mention of mutual (co)induction in research literature, but\nthe formal definition of mutual (co)induction and the proof of the mutual\n(co)induction proof principles we present here seem to be the first such\ndefinition and proof. As such, it seems our work is the first to point out that\nmonotonicity of mutual generators seems not sufficient for guaranteeing the\nexistence of least and greatest simultaneous fixed points in complete lattices,\nand that continuity on the other hand is sufficient for guaranteeing their\nexistence. [The paper has been responsively updated so as to not require the\ncontinuity of generators but only require their monotonicity, and it now\nincludes a proof of the sufficiency of monotonicity. A full revision of the\npaper to reflect the relaxed requirement is currently underway.]\n  In the course of our presentation of mutual coinduction we also discuss some\nconcepts related to standard (also called direct) induction and standard\ncoinduction, as well as ones related to mutual (also called simultaneous or\nindirect) induction. During the presentation we purposely discuss particular\nstandard concepts so as to help motivate the definitions of their more general\ncounterparts for mutual/ simultaneous/ indirect (co)induction. Greatest\nsimultaneous post-fixed points, in particular, will be abstractions and models\nof mathematical objects (e.g., points, sets, types, predicates, etc.) that are\ndefined mutually-coinductively.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 12:45:53 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 10:32:01 GMT"}, {"version": "v3", "created": "Thu, 21 Mar 2019 16:58:50 GMT"}, {"version": "v4", "created": "Mon, 8 Apr 2019 15:19:23 GMT"}, {"version": "v5", "created": "Mon, 29 Jul 2019 10:48:46 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["AbdelGawad", "Moez A.", ""]]}, {"id": "1903.06560", "submitter": "Constantin Enea", "authors": "Constantin Enea, Suha Orhun Mutluergil, Gustavo Petri, and Chao Wang", "title": "Replication-Aware Linearizability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geo-distributed systems often replicate data at multiple locations to achieve\navailability and performance despite network partitions. These systems must\naccept updates at any replica and propagate these updates asynchronously to\nevery other replica. Conflict-Free Replicated Data Types (CRDTs) provide a\nprincipled approach to the problem of ensuring that replicas are eventually\nconsistent despite the asynchronous delivery of updates.\n  We address the problem of specifying and verifying CRDTs, introducing a new\ncorrectness criterion called Replication-Aware Linearizability. This criterion\nis inspired by linearizability, the de-facto correctness criterion for\n(shared-memory) concurrent data structures. We argue that this criterion is\nboth simple to understand, and it fits most known implementations of CRDTs. We\nprovide a proof methodology to show that a CRDT satisfies replication-aware\nlinearizability which we apply on a wide range of implementations. Finally, we\nshow that our criterion can be leveraged to reason modularly about the\ncomposition of CRDTs.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 14:03:53 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Enea", "Constantin", ""], ["Mutluergil", "Suha Orhun", ""], ["Petri", "Gustavo", ""], ["Wang", "Chao", ""]]}, {"id": "1903.07038", "submitter": "Marcelo Pereira Novaes", "authors": "Marcelo Novaes, Vin\\'icius Petrucci, Abdoulaye Gamati\\'e and Fernando\n  Quint\\~ao", "title": "Compiler-assisted Adaptive Program Scheduling in big.LITTLE Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy-aware architectures provide applications with a mix of low (LITTLE)\nand high (big) frequency cores. Choosing the best hardware configuration for a\nprogram running on such an architecture is difficult, because program parts\nbenefit differently from the same hardware configuration. State-of-the-art\ntechniques to solve this problem adapt the program's execution to dynamic\ncharacteristics of the runtime environment, such as energy consumption and\nthroughput. We claim that these purely dynamic techniques can be improved if\nthey are aware of the program's syntactic structure. To support this claim, we\nshow how to use the compiler to partition source code into program phases:\nregions whose syntactic characteristics lead to similar runtime behavior. We\nuse reinforcement learning to map pairs formed by a program phase and a\nhardware state to the configuration that best fit this setup. To demonstrate\nthe effectiveness of our ideas, we have implemented the Astro system. Astro\nuses Q-learning to associate syntactic features of programs with hardware\nconfigurations. As a proof of concept, we provide evidence that Astro\noutperforms GTS, the ARM-based Linux scheduler tailored for heterogeneous\narchitectures, on the parallel benchmarks from Rodinia and Parsec.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2019 07:31:02 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Novaes", "Marcelo", ""], ["Petrucci", "Vin\u00edcius", ""], ["Gamati\u00e9", "Abdoulaye", ""], ["Quint\u00e3o", "Fernando", ""]]}, {"id": "1903.07213", "submitter": "Eric Koskinen", "authors": "Timos Antonopoulos and Eric Koskinen and Ton-Chanh Le", "title": "Specification and Inference of Trace Refinement Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern software is constantly changing. Researchers and practitioners are\nincreasingly aware that verification tools can be impactful if they embrace\nchange through analyses that are compositional and span program versions.\nReasoning about similarities and differences between programs goes back to\nBenton, who introduced state-based refinement relations, which were extended by\nYang and others. However, to our knowledge, refinement relations have not been\nexplored for traces.\n  We present a novel theory that allows one to perform compositional reasoning\nabout the similarities/differences between how fragments of two different\nprograms behave over time through the use of what we call trace-refinement\nrelations. We take a reactive view of programs and found Kleene Algebra with\nTests (KAT) [Kozen] to be a natural choice to describe traces since it permits\nalgebraic reasoning and has built-in composition. Our theory involves a\ntwo-step semantic abstraction from programs to KAT, and then our trace\nrefinement relations correlate behaviors by (i) categorizing program behaviors\ninto trace classes through KAT intersection and (ii) correlating atomic\nevents/conditions across programs with KAT hypotheses. We next describe a\nsynthesis algorithm that iteratively constructs trace-refinement relations\nbetween two programs by exploring sub-partitions of their traces, iteratively\nabstracting them as KAT expressions, discovering relationships through a custom\nedit-distance algorithm, and applying strategies (i) and (ii) above. We have\nimplemented this algorithm as {\\sc knotical}, the first tool capable of\nsynthesizing trace-refinement relations. It built from the ground up in Ocaml,\nusing InterProc and SymKAT. We have demonstrated that useful relations can be\nefficiently generated across a suite of 37 benchmarks that include changing\nfragments of array programs, systems code, and web servers.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 00:47:21 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Antonopoulos", "Timos", ""], ["Koskinen", "Eric", ""], ["Le", "Ton-Chanh", ""]]}, {"id": "1903.07962", "submitter": "Saverio Giallorenzo", "authors": "Maurizio Gabbrielli and Saverio Giallorenzo and Ivan Lanese and\n  Fabrizio Montesi and Marco Peressotti and Stefano Pio Zingaro", "title": "No more, no less - A formal model for serverless computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Serverless computing, also known as Functions-as-a-Service, is a recent\nparadigm aimed at simplifying the programming of cloud applications. The idea\nis that developers design applications in terms of functions, which are then\ndeployed on a cloud infrastructure. The infrastructure takes care of executing\nthe functions whenever requested by remote clients, dealing automatically with\ndistribution and scaling with respect to inbound traffic.\n  While vendors already support a variety of programming languages for\nserverless computing (e.g. Go, Java, Javascript, Python), as far as we know\nthere is no reference model yet to formally reason on this paradigm. In this\npaper, we propose the first formal programming model for serverless computing,\nwhich combines ideas from both the $\\lambda$-calculus (for functions) and the\n$\\pi$-calculus (for communication). To illustrate our proposal, we model a\nreal-world serverless system. Thanks to our model, we are also able to capture\nand pinpoint the limitations of current vendor technologies, proposing possible\namendments.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 12:26:10 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 07:11:36 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Gabbrielli", "Maurizio", ""], ["Giallorenzo", "Saverio", ""], ["Lanese", "Ivan", ""], ["Montesi", "Fabrizio", ""], ["Peressotti", "Marco", ""], ["Zingaro", "Stefano Pio", ""]]}, {"id": "1903.08233", "submitter": "Christopher Jenkins", "authors": "Christopher Jenkins, Colin McDonald, Aaron Stump", "title": "Elaborating Inductive Definitions and Course-of-Values Induction in\n  Cedille", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Calculus of Dependent Lambda Eliminations (CDLE), a pure Curry-style\ntype theory, it is possible to generically {\\lambda}-encode inductive datatypes\nwhich support course-of-values (CoV) induction. We present a datatype subsystem\nfor Cedille (an implementation of CDLE) that provides this feature to\nprogrammers through convenient notation for declaring datatypes and for\ndefining functions over them by case analysis and fixpoint-style recursion\nguarded by a type-based termination checker. We demonstrate that this does not\nrequire extending CDLE by showing how datatypes and functions over them\nelaborate to {\\lambda}-encodings, and proving that this elaboration is type-\nand value-preserving. This datatype subsystem and elaborator are implemented in\nCedille, establishing for the first time a complete translation of inductive\ndefinitions to a small pure typed {\\lambda}-calculus.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 19:40:32 GMT"}, {"version": "v2", "created": "Sun, 11 Aug 2019 05:56:11 GMT"}, {"version": "v3", "created": "Sat, 2 Nov 2019 22:18:51 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Jenkins", "Christopher", ""], ["McDonald", "Colin", ""], ["Stump", "Aaron", ""]]}, {"id": "1903.09477", "submitter": "Gregor Ulm", "authors": "Gregor Ulm, Simon Smith, Adrian Nilsson, Emil Gustavsson, Mats\n  Jirstrand", "title": "Facilitating Rapid Prototyping in the OODIDA Data Analytics Platform via\n  Active-Code Replacement", "comments": "24 pages, 4 figures, 3 code listings, 1 table", "journal-ref": "Array Vol. 8, December 2020 (100043)", "doi": "10.1016/j.array.2020.100043", "report-no": null, "categories": "cs.DC cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OODIDA (On-board/Off-board Distributed Data Analytics) is a platform for\ndistributed real-time analytics, targeting fleets of reference vehicles in the\nautomotive industry. Its users are data analysts. The bulk of the data\nanalytics tasks are performed by clients (on-board), while a central cloud\nserver performs supplementary tasks (off-board). OODIDA can be automatically\npackaged and deployed, which necessitates restarting parts of the system, or\nall of it. As this is potentially disruptive, we added the ability to execute\nuser-defined Python modules on clients as well as the server. These modules can\nbe replaced without restarting any part of the system; they can even be\nreplaced between iterations of an ongoing assignment. This feature is referred\nto as active-code replacement. It facilitates use cases such as iterative A/B\ntesting of machine learning algorithms or modifying experimental algorithms\non-the-fly. Consistency of results is achieved by majority vote, which prevents\ntainted state. Active-code replacement can be done in less than a second in an\nidealized setting whereas a standard deployment takes many orders of magnitude\nmore time. The main contribution of this paper is the description of a\nrelatively straightforward approach to active-code replacement that is very\nuser-friendly. It enables a data analyst to quickly execute custom code on the\ncloud server as well as on client devices. Sensible safeguards and design\ndecisions ensure that this feature can be used by non-specialists who are not\nfamiliar with the implementation of OODIDA in general or this feature in\nparticular. As a consequence of adding the active-code replacement feature,\nOODIDA is now very well-suited for rapid prototyping.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 12:46:34 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 13:26:01 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 10:36:43 GMT"}, {"version": "v4", "created": "Wed, 30 Dec 2020 10:34:47 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Ulm", "Gregor", ""], ["Smith", "Simon", ""], ["Nilsson", "Adrian", ""], ["Gustavsson", "Emil", ""], ["Jirstrand", "Mats", ""]]}, {"id": "1903.09713", "submitter": "ThanhVu Nguyen", "authors": "Ton Chanh Le, Guolong Zheng, and ThanhVu Nguyen", "title": "SLING: Using Dynamic Analysis to Infer Program Invariants in Separation\n  Logic", "comments": "In PLDI 2019, add footnote/clarification to comparison results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new dynamic analysis technique to discover invariants in\nseparation logic for heap-manipulating programs. First, we use a debugger to\nobtain rich program execution traces at locations of interest on sample inputs.\nThese traces consist of heap and stack information of variables that point to\ndynamically allocated data structures. Next, we iteratively analyze separate\nmemory regions related to each pointer variable and search for a formula over\npredefined heap predicates in separation logic to model these regions. Finally,\nwe combine the computed formulae into an invariant that describes the shape of\nexplored memory regions.\n  We present SLING, a tool that implements these ideas to automatically\ngenerate invariants in separation logic at arbitrary locations in C programs,\ne.g., program pre and postconditions and loop invariants. Preliminary results\non existing benchmarks show that SLING can efficiently generate correct and\nuseful invariants for programs that manipulate a wide variety of complex data\nstructures.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 21:43:01 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2019 18:09:47 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Le", "Ton Chanh", ""], ["Zheng", "Guolong", ""], ["Nguyen", "ThanhVu", ""]]}, {"id": "1903.10267", "submitter": "Aleksandar Prokopec", "authors": "Aleksandar Prokopec, Andrea Ros\\`a, David Leopoldseder, Gilles\n  Duboscq, Petr T\\r{u}ma, Martin Studener, Lubom\\'ir Bulej, Yudi Zheng, Alex\n  Villaz\\'on, Doug Simon, Thomas Wuerthinger, Walter Binder", "title": "On Evaluating the Renaissance Benchmarking Suite: Variety, Performance,\n  and Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed Renaissance suite is composed of modern, real-world,\nconcurrent, and object-oriented workloads that exercise various concurrency\nprimitives of the JVM. Renaissance was used to compare performance of two\nstateof-the-art, production-quality JIT compilers (HotSpot C2 and Graal), and\nto show that the performance differences are more significant than on existing\nsuites such as DaCapo and SPECjvm2008.\n  In this technical report, we give an overview of the experimental setup that\nwe used to assess the variety and complexity of the Renaissance suite, as well\nas its amenability to new compiler optimizations. We then present the obtained\nmeasurements in detail.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 12:22:01 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Prokopec", "Aleksandar", ""], ["Ros\u00e0", "Andrea", ""], ["Leopoldseder", "David", ""], ["Duboscq", "Gilles", ""], ["T\u016fma", "Petr", ""], ["Studener", "Martin", ""], ["Bulej", "Lubom\u00edr", ""], ["Zheng", "Yudi", ""], ["Villaz\u00f3n", "Alex", ""], ["Simon", "Doug", ""], ["Wuerthinger", "Thomas", ""], ["Binder", "Walter", ""]]}, {"id": "1903.10556", "submitter": "Xin Zhang", "authors": "Zenna Tavares, Xin Zhang, Edgar Minaysan, Javier Burroni, Rajesh\n  Ranganath, Armando Solar Lezama", "title": "The Random Conditional Distribution for Higher-Order Probabilistic\n  Inference", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to condition distributional properties such as expectation,\nvariance, and entropy arises in algorithmic fairness, model simplification,\nrobustness and many other areas. At face value however, distributional\nproperties are not random variables, and hence conditioning them is a semantic\nerror and type error in probabilistic programming languages. On the other hand,\ndistributional properties are contingent on other variables in the model,\nchange in value when we observe more information, and hence in a precise sense\nare random variables too. In order to capture the uncertain over distributional\nproperties, we introduce a probability construct -- the random conditional\ndistribution -- and incorporate it into a probabilistic programming language\nOmega. A random conditional distribution is a higher-order random variable\nwhose realizations are themselves conditional random variables. In Omega we\nextend distributional properties of random variables to random conditional\ndistributions, such that for example while the expectation a real valued random\nvariable is a real value, the expectation of a random conditional distribution\nis a distribution over expectations. As a consequence, it requires minimal\nsyntax to encode inference problems over distributional properties, which so\nfar have evaded treatment within probabilistic programming systems and\nprobabilistic modeling in general. We demonstrate our approach case studies in\nalgorithmic fairness and robustness.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 19:16:23 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Tavares", "Zenna", ""], ["Zhang", "Xin", ""], ["Minaysan", "Edgar", ""], ["Burroni", "Javier", ""], ["Ranganath", "Rajesh", ""], ["Lezama", "Armando Solar", ""]]}, {"id": "1903.10677", "submitter": "Conal Elliott", "authors": "Conal Elliott", "title": "Generalized Convolution and Efficient Language Recognition", "comments": "43 pages, one figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolution is a broadly useful operation with applications including signal\nprocessing, machine learning, probability, optics, polynomial multiplication,\nand efficient parsing. Usually, however, this operation is understood and\nimplemented in more specialized forms, hiding commonalities and limiting\nusefulness. This paper formulates convolution in the common algebraic framework\nof semirings and semimodules and populates that framework with various\nrepresentation types. One of those types is the grand abstract template and\nitself generalizes to the free semimodule monad. Other representations serve\nvaried uses and performance trade-offs, with implementations calculated from\nsimple and regular specifications.\n  Of particular interest is Brzozowski's method for regular expression\nmatching. Uncovering the method's essence frees it from syntactic\nmanipulations, while generalizing from boolean to weighted membership (such as\nmultisets and probability distributions) and from sets to n-ary relations. The\nclassic trie data structure then provides an elegant and efficient alternative\nto syntax.\n  Pleasantly, polynomial arithmetic requires no additional implementation\neffort, works correctly with a variety of representations, and handles\nmultivariate polynomials and power series with ease. Image convolution also\nfalls out as a special case.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 05:15:47 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Elliott", "Conal", ""]]}, {"id": "1903.11397", "submitter": "Kyriakos Georgiou", "authors": "Kyriakos Georgiou, Zbigniew Chamski, Andres Amaya Garcia, David May,\n  Kerstin Eder", "title": "Lost in translation: Exposing hidden compiler optimization opportunities", "comments": "31 pages, 7 figures, 2 table. arXiv admin note: text overlap with\n  arXiv:1802.09845", "journal-ref": "The Computer Journal (2020)", "doi": "10.1093/comjnl/bxaa103", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing iterative compilation and machine-learning-based optimization\ntechniques have been proven very successful in achieving better optimizations\nthan the standard optimization levels of a compiler. However, they were not\nengineered to support the tuning of a compiler's optimizer as part of the\ncompiler's daily development cycle. In this paper, we first establish the\nrequired properties which a technique must exhibit to enable such tuning. We\nthen introduce an enhancement to the classic nightly routine testing of\ncompilers which exhibits all the required properties, and thus, is capable of\ndriving the improvement and tuning of the compiler's common optimizer. This is\nachieved by leveraging resource usage and compilation information collected\nwhile systematically exploiting prefixes of the transformations applied at\nstandard optimization levels. Experimental evaluation using the LLVM v6.0.1\ncompiler demonstrated that the new approach was able to reveal hidden\ncross-architecture and architecture-dependent potential optimizations on two\npopular processors: the Intel i5-6300U and the Arm Cortex-A53-based Broadcom\nBCM2837 used in the Raspberry Pi 3B+. As a case study, we demonstrate how the\ninsights from our approach enabled us to identify and remove a significant\nshortcoming of the CFG simplification pass of the LLVM v6.0.1 compiler.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 23:34:53 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 10:59:54 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 22:46:38 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Georgiou", "Kyriakos", ""], ["Chamski", "Zbigniew", ""], ["Garcia", "Andres Amaya", ""], ["May", "David", ""], ["Eder", "Kerstin", ""]]}, {"id": "1903.11765", "submitter": "ThanhVu Nguyen", "authors": "ThanhVu Nguyen and Westley Weimer and Deepak Kapur and Stephanie\n  Forrest", "title": "Connecting Program Synthesis and Reachability: Automatic Program Repair\n  using Test-Input Generation", "comments": null, "journal-ref": "Tools and Algorithms for the Construction and Analysis of Systems\n  (TACAS), pages 301--318. Springer, 2017", "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that certain formulations of program synthesis and reachability are\nequivalent. Specifically, our constructive proof shows the reductions between\nthe template-based synthesis problem, which generates a program in a\npre-specified form, and the reachability problem, which decides the\nreachability of a program location. This establishes a link between the two\nresearch fields and allows for the transfer of techniques and results between\nthem.\n  To demonstrate the equivalence, we develop a program repair prototype using\nreachability tools. We transform a buggy program and its required specification\ninto a specific program containing a location reachable only when the original\nprogram can be repaired, and then apply an off-the-shelf test-input generation\ntool on the transformed program to find test values to reach the desired\nlocation. Those test values correspond to repairs for the original programm.\nPreliminary results suggest that our approach compares favorably to other\nrepair methods.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 02:44:04 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Nguyen", "ThanhVu", ""], ["Weimer", "Westley", ""], ["Kapur", "Deepak", ""], ["Forrest", "Stephanie", ""]]}, {"id": "1903.12254", "submitter": "Yuxin Wang", "authors": "Yuxin Wang, Zeyu Ding, Guanhong Wang, Daniel Kifer, Danfeng Zhang", "title": "Proving Differential Privacy with Shadow Execution", "comments": "23 pages, 12 figures, PLDI'19", "journal-ref": "PLDI 2019 Proceedings of the 40th ACM SIGPLAN Conference on\n  Programming Language Design and Implementation. Pages 655-669", "doi": "10.1145/3314221.3314619", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on formal verification of differential privacy shows a trend\ntoward usability and expressiveness -- generating a correctness proof of\nsophisticated algorithm while minimizing the annotation burden on programmers.\nSometimes, combining those two requires substantial changes to program logics:\none recent paper is able to verify Report Noisy Max automatically, but it\ninvolves a complex verification system using customized program logics and\nverifiers.\n  In this paper, we propose a new proof technique, called shadow execution, and\nembed it into a language called ShadowDP. ShadowDP uses shadow execution to\ngenerate proofs of differential privacy with very few programmer annotations\nand without relying on customized logics and verifiers. In addition to\nverifying Report Noisy Max, we show that it can verify a new variant of Sparse\nVector that reports the gap between some noisy query answers and the noisy\nthreshold. Moreover, ShadowDP reduces the complexity of verification: for all\nof the algorithms we have evaluated, type checking and verification in total\ntakes at most 3 seconds, while prior work takes minutes on the same algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 20:39:54 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 02:46:34 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Wang", "Yuxin", ""], ["Ding", "Zeyu", ""], ["Wang", "Guanhong", ""], ["Kifer", "Daniel", ""], ["Zhang", "Danfeng", ""]]}, {"id": "1903.12366", "submitter": "Zehra Sura", "authors": "Zehra Sura, Tong Chen, and Hyojin Sung", "title": "Using Structured Input and Modularity for Improved Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for utilizing the known structure of input data to make\nlearning more efficient. Our work is in the domain of programming languages,\nand we use deep neural networks to do program analysis. Computer programs\ninclude a lot of structural information (such as loop nests, conditional\nblocks, and data scopes), which is pertinent to program analysis. In this case,\nthe neural network has to learn to recognize the structure, and also learn the\ntarget function for the problem. However, the structural information in this\ndomain is readily accessible to software with the availability of compiler\ntools and parsers for well-defined programming languages.\n  Our method for utilizing the known structure of input data includes: (1)\npre-processing the input data to expose relevant structures, and (2)\nconstructing neural networks by incorporating the structure of the input data\nas an integral part of the network design. The method has the effect of\nmodularizing the neural network which helps break down complexity, and results\nin more efficient training of the overall network. We apply this method to an\nexample code analysis problem, and show that it can achieve higher accuracy\nwith a smaller network size and fewer training examples. Further, the method is\nrobust, performing equally well on input data with different distributions.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 06:30:32 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Sura", "Zehra", ""], ["Chen", "Tong", ""], ["Sung", "Hyojin", ""]]}, {"id": "1903.12519", "submitter": "Matthew Mirman", "authors": "Matthew Mirman, Gagandeep Singh, Martin Vechev", "title": "A Provable Defense for Deep Residual Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a training system, which can provably defend significantly larger\nneural networks than previously possible, including ResNet-34 and DenseNet-100.\nOur approach is based on differentiable abstract interpretation and introduces\ntwo novel concepts: (i) abstract layers for fine-tuning the precision and\nscalability of the abstraction, (ii) a flexible domain specific language (DSL)\nfor describing training objectives that combine abstract and concrete losses\nwith arbitrary specifications. Our training method is implemented in the DiffAI\nsystem.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 13:35:31 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 14:50:42 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Mirman", "Matthew", ""], ["Singh", "Gagandeep", ""], ["Vechev", "Martin", ""]]}]