[{"id": "1706.00206", "submitter": "Bhargava Shastry", "authors": "Bhargava Shastry and Federico Maggi and Fabian Yamaguchi and Konrad\n  Rieck and Jean-Pierre Seifert", "title": "Static Exploration of Taint-Style Vulnerabilities Found by Fuzzing", "comments": "10 pages excl. bibliography", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taint-style vulnerabilities comprise a majority of fuzzer discovered program\nfaults. These vulnerabilities usually manifest as memory access violations\ncaused by tainted program input. Although fuzzers have helped uncover a\nmajority of taint-style vulnerabilities in software to date, they are limited\nby (i) extent of test coverage; and (ii) the availability of fuzzable test\ncases. Therefore, fuzzing alone cannot provide a high assurance that all\ntaint-style vulnerabilities have been uncovered. In this paper, we use static\ntemplate matching to find recurrences of fuzzer-discovered vulnerabilities. To\ncompensate for the inherent incompleteness of template matching, we implement a\nsimple yet effective match-ranking algorithm that uses test coverage data to\nfocus attention on those matches that comprise untested code. We prototype our\napproach using the Clang/LLVM compiler toolchain and use it in conjunction with\nafl-fuzz, a modern coverage-guided fuzzer. Using a case study carried out on\nthe Open vSwitch codebase, we show that our prototype uncovers corner cases in\nmodules that lack a fuzzable test harness. Our work demonstrates that static\nanalysis can effectively complement fuzz testing, and is a useful addition to\nthe security assessment tool-set. Furthermore, our techniques hold promise for\nincreasing the effectiveness of program analysis and testing, and serve as a\nbuilding block for a hybrid vulnerability discovery framework.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 08:40:37 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Shastry", "Bhargava", ""], ["Maggi", "Federico", ""], ["Yamaguchi", "Fabian", ""], ["Rieck", "Konrad", ""], ["Seifert", "Jean-Pierre", ""]]}, {"id": "1706.00231", "submitter": "Samer Abdallah", "authors": "Samer Abdallah", "title": "Automatic Differentiation using Constraint Handling Rules in Prolog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.PL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic differentiation is a technique which allows a programmer to define\na numerical computation via compositions of a broad range of numeric and\ncomputational primitives and have the underlying system support the computation\nof partial derivatives of the result with respect to any of its inputs, without\nmaking any finite difference approximations, and without manipulating large\nsymbolic expressions representing the computation. This note describes a novel\napproach to reverse mode automatic differentiation using constraint logic\nprogrammming, specifically, the constraint handling rules (CHR) library of SWI\nProlog, resulting in a very small (50 lines of code) implementation. When\napplied to a differentiation-based implementation of the inside-outside\nalgorithm for parameter learning in probabilistic grammars, the CHR based\nimplementations outperformed two well-known frameworks for optimising\ndifferentiable functions, Theano and TensorFlow, by a large margin.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 09:55:10 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Abdallah", "Samer", ""]]}, {"id": "1706.00274", "submitter": "Moez AbdelGawad", "authors": "Moez A. AbdelGawad", "title": "Towards a Java Subtyping Operad", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subtyping relation in Java exhibits self-similarity. The self-similarity\nin Java subtyping is interesting and intricate due to the existence of wildcard\ntypes and, accordingly, the existence of three subtyping rules for generic\ntypes: covariant subtyping, contravariant subtyping and invariant subtyping.\nSupporting bounded type variables also adds to the complexity of the subtyping\nrelation in Java and in other generic nominally-typed OO languages such as C#\nand Scala. In this paper we explore defining an operad to model the\nconstruction of the subtyping relation in Java and in similar generic\nnominally-typed OO programming languages. Operads, from category theory, are\nfrequently used to model self-similar phenomena. The Java subtyping operad, we\nhope, will shed more light on understanding the type systems of generic\nnominally-typed OO languages.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 16:11:29 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 17:44:08 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["AbdelGawad", "Moez A.", ""]]}, {"id": "1706.00363", "submitter": "Stefan Marr", "authors": "Stefan Marr, Carmen Torres Lopez, Dominik Aumayr, Elisa Gonzalez Boix,\n  Hanspeter M\\\"ossenb\\\"ock", "title": "A Concurrency-Agnostic Protocol for Multi-Paradigm Concurrent Debugging\n  Tools", "comments": "International Symposium on Dynamic Languages", "journal-ref": null, "doi": "10.1145/3133841.3133842", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's complex software systems combine high-level concurrency models. Each\nmodel is used to solve a specific set of problems. Unfortunately, debuggers\nsupport only the low-level notions of threads and shared memory, forcing\ndevelopers to reason about these notions instead of the high-level concurrency\nmodels they chose.\n  This paper proposes a concurrency-agnostic debugger protocol that decouples\nthe debugger from the concurrency models employed by the target application. As\na result, the underlying language runtime can define custom breakpoints,\nstepping operations, and execution events for each concurrency model it\nsupports, and a debugger can expose them without having to be specifically\nadapted.\n  We evaluated the generality of the protocol by applying it to SOMns, a\nNewspeak implementation, which supports a diversity of concurrency models\nincluding communicating sequential processes, communicating event loops,\nthreads and locks, fork/join parallelism, and software transactional memory. We\nimplemented 21 breakpoints and 20 stepping operations for these concurrency\nmodels. For none of these, the debugger needed to be changed. Furthermore, we\nvisualize all concurrent interactions independently of a specific concurrency\nmodel. To show that tooling for a specific concurrency model is possible, we\nvisualize actor turns and message sends separately.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 16:05:14 GMT"}, {"version": "v2", "created": "Sun, 29 Oct 2017 16:31:18 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Marr", "Stefan", ""], ["Lopez", "Carmen Torres", ""], ["Aumayr", "Dominik", ""], ["Boix", "Elisa Gonzalez", ""], ["M\u00f6ssenb\u00f6ck", "Hanspeter", ""]]}, {"id": "1706.00648", "submitter": "Michael Bukatin", "authors": "Michael Bukatin, Jon Anthony", "title": "Dataflow Matrix Machines as a Model of Computations with Linear Streams", "comments": "6 pages, accepted for presentation at LearnAut 2017: Learning and\n  Automata workshop at LICS (Logic in Computer Science) 2017 conference.\n  Preprint original version: April 9, 2017; minor correction: May 1, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We overview dataflow matrix machines as a Turing complete generalization of\nrecurrent neural networks and as a programming platform. We describe vector\nspace of finite prefix trees with numerical leaves which allows us to combine\nexpressive power of dataflow matrix machines with simplicity of traditional\nrecurrent neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 13:46:05 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Bukatin", "Michael", ""], ["Anthony", "Jon", ""]]}, {"id": "1706.00767", "submitter": "Swarnendu Biswas", "authors": "Swarnendu Biswas, Yan Pei, Donald S. Fussell and Keshav Pingali", "title": "Capri: A Control System for Approximate Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate computing trades off accuracy of results for resources such as\nenergy or computing time. There is a large and rapidly growing literature on\napproximate computing that has focused mostly on showing the benefits of\napproximation. However, we know relatively little about how to control\napproximation in a disciplined way.\n  This document briefly describes our published work of controlling\napproximation for non-streaming programs that have a set of \"knobs\" that can be\ndialed up or down to control the level of approximation of different components\nin the program. The proposed system, Capri, solves this control problem as a\nconstrained optimization problem. Capri uses machine learning to learn cost and\nerror models for the program, and uses these models to determine, for a desired\nlevel of approximation, knob settings that optimize metrics such as running\ntime or energy usage. Experimental results with complex benchmarks from\ndifferent problem domains demonstrate the effectiveness of this approach.\n  This report outlines improvements and extensions to the existing Capri system\nto address its limitations, including a complete rewrite of the software, and\ndiscusses directions for follow up work. The document also includes\ninstructions and guidelines for using the new Capri infrastructure.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 17:29:40 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Biswas", "Swarnendu", ""], ["Pei", "Yan", ""], ["Fussell", "Donald S.", ""], ["Pingali", "Keshav", ""]]}, {"id": "1706.00862", "submitter": "Brenton Chapin", "authors": "Brenton Chapin", "title": "Efficient Textual Representation of Structure", "comments": "submitted to 10th ACM SIGPLAN International Conference on Software\n  Language Engineering (SLE), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper attempts a more formal approach to the legibility of text based\nprogramming languages, presenting, with proof, minimum possible ways of\nrepresenting structure in text interleaved with information. This presumes that\na minimalist approach is best for purposes of human readability, data storage\nand transmission, and machine evaluation.\n  Several proposals are given for improving the expression of interleaved\nhierarchical structure. For instance, a single colon can replace a pair of\nbrackets, and bracket types do not need to be repeated in both opening and\nclosing symbols or words. Historic and customary uses of punctuation symbols\nguided the chosen form and nature of the improvements.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 21:49:53 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Chapin", "Brenton", ""]]}, {"id": "1706.01284", "submitter": "Xinyun Chen", "authors": "Xinyun Chen, Chang Liu, Dawn Song", "title": "Towards Synthesizing Complex Programs from Input-Output Examples", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning techniques have been developed to improve the\nperformance of program synthesis from input-output examples. Albeit its\nsignificant progress, the programs that can be synthesized by state-of-the-art\napproaches are still simple in terms of their complexity. In this work, we move\na significant step forward along this direction by proposing a new class of\nchallenging tasks in the domain of program synthesis from input-output\nexamples: learning a context-free parser from pairs of input programs and their\nparse trees. We show that this class of tasks are much more challenging than\npreviously studied tasks, and the test accuracy of existing approaches is\nalmost 0%.\n  We tackle the challenges by developing three novel techniques inspired by\nthree novel observations, which reveal the key ingredients of using deep\nlearning to synthesize a complex program. First, the use of a\nnon-differentiable machine is the key to effectively restrict the search space.\nThus our proposed approach learns a neural program operating a domain-specific\nnon-differentiable machine. Second, recursion is the key to achieve\ngeneralizability. Thus, we bake-in the notion of recursion in the design of our\nnon-differentiable machine. Third, reinforcement learning is the key to learn\nhow to operate the non-differentiable machine, but it is also hard to train the\nmodel effectively with existing reinforcement learning algorithms from a cold\nboot. We develop a novel two-phase reinforcement learning-based search\nalgorithm to overcome this issue. In our evaluation, we show that using our\nnovel approach, neural parsing programs can be learned to achieve 100% test\naccuracy on test inputs that are 500x longer than the training samples.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 11:44:35 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 04:54:32 GMT"}, {"version": "v3", "created": "Sun, 11 Feb 2018 04:33:30 GMT"}, {"version": "v4", "created": "Thu, 8 Mar 2018 00:22:59 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Chen", "Xinyun", ""], ["Liu", "Chang", ""], ["Song", "Dawn", ""]]}, {"id": "1706.01407", "submitter": "Danfeng Zhang", "authors": "Peixuan Li and Danfeng Zhang", "title": "Towards a Flow- and Path-Sensitive Information Flow Analysis: Technical\n  Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a flow- and path-sensitive static information flow\nanalysis. Compared with security type systems with fixed labels, it has been\nshown that flow-sensitive type systems accept more secure programs. We show\nthat an information flow analysis with fixed labels can be both flow- and\npath-sensitive. The novel analysis has two major components: 1) a\ngeneral-purpose program transformation that removes false dataflow dependencies\nin a program that confuse a fixed-label type system, and 2) a fixed-label type\nsystem that allows security types to depend on path conditions. We formally\nprove that the proposed analysis enforces a rigorous security property:\nnoninterference. Moreover, we show that the analysis is strictly more precise\nthan a classic flow-sensitive type system, and it allows sound control of\ninformation flow in the presence of mutable variables without resorting to\nrun-time mechanisms.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 16:22:05 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 21:58:43 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Li", "Peixuan", ""], ["Zhang", "Danfeng", ""]]}, {"id": "1706.01755", "submitter": "Nada Sharaf Ms.", "authors": "Nada Sharaf, Slim Abdennadher, Thom Fr\\\"uhwirth", "title": "Visualization of Constraint Handling Rules: Semantics and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work in the paper presents an animation extension ($CHR^{vis}$) to\nConstraint Handling Rules (CHR). Visualizations have always helped programmers\nunderstand data and debug programs. A picture is worth a thousand words. It can\nhelp identify where a problem is or show how something works. It can even\nillustrate a relation that was not clear otherwise. $CHR^{vis}$ aims at\nembedding animation and visualization features into CHR programs. It thus\nenables users, while executing programs, to have such executions animated. The\npaper aims at providing the operational semantics for $CHR^{vis}$. The\ncorrectness of $CHR^{vis}$ programs is also discussed. Some applications of the\nnew extension are also introduced.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 23:08:12 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Sharaf", "Nada", ""], ["Abdennadher", "Slim", ""], ["Fr\u00fchwirth", "Thom", ""]]}, {"id": "1706.02136", "submitter": "Mikhail Gadelha", "authors": "Mikhail Y. R. Gadelha and Lucas C. Cordeiro and Denis A. Nicole", "title": "Counterexample-Guided k-Induction Verification for Fast Bug Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the k-induction algorithm has proven to be a successful approach\nfor both finding bugs and proving correctness. However, since the algorithm is\nan incremental approach, it might waste resources trying to prove incorrect\nprograms. In this paper, we propose to extend the k-induction algorithm in\norder to shorten the number of steps required to find a property violation. We\nconvert the algorithm into a meet-in-the-middle bidirectional search algorithm,\nusing the counterexample produced from over-approximating the program. The\npreliminary results show that the number of steps required to find a property\nviolation is reduced to $\\lfloor\\frac{k}{2} + 1\\rfloor$ and the verification\ntime for programs with large state space is reduced considerably.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 11:23:58 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 22:59:17 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Gadelha", "Mikhail Y. R.", ""], ["Cordeiro", "Lucas C.", ""], ["Nicole", "Denis A.", ""]]}, {"id": "1706.02400", "submitter": "Beta Ziliani", "authors": "Mallku Soldevila and Beta Ziliani and Bruno Silvestre and Daniel\n  Fridlender and Fabio Mascarenhas", "title": "Decoding Lua: Formal Semantics for the Developer and the Semanticist", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide formal semantics for a large subset of the Lua programming\nlanguage, in its version 5.2. We validate our model by mechanizing it and\ntesting it against the test suite of the reference interpreter of Lua,\nconfirming that our model accurately represents the language. In addition, we\nset us an ambitious goal: to target both a PL semanticist ---not necessarily\nversed in Lua---, and a Lua developer ---not necessarily versed in semantic\nframeworks. To the former, we present the peculiarities of the language, and\nhow we model them in a traditional small-step operational semantics, embedded\nwithin Felleisen-Hieb's reduction semantics with evaluation contexts. The\nmechanization is, naturally, performed in PLT Redex, the de facto tool for\nmechanizing reduction semantics.\n  To the reader unfamiliar with such concepts, we provide, to our best possible\nwithin the space limitations, a gentle introduction of the model. It is our\nhope that developers of the different Lua implementations and dialects\nunderstand the model and consider it both for testing their work and for\nexperimenting with new language features.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 22:30:58 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Soldevila", "Mallku", ""], ["Ziliani", "Beta", ""], ["Silvestre", "Bruno", ""], ["Fridlender", "Daniel", ""], ["Mascarenhas", "Fabio", ""]]}, {"id": "1706.02630", "submitter": "EPTCS", "authors": "Francisco Rios (Dalhousie University, Halifax, Canada), Peter Selinger\n  (Dalhousie University, Halifax, Canada)", "title": "A Categorical Model for a Quantum Circuit Description Language (Extended\n  Abstract)", "comments": "In Proceedings QPL 2017, arXiv:1802.09737", "journal-ref": "EPTCS 266, 2018, pp. 164-178", "doi": "10.4204/EPTCS.266.11", "report-no": null, "categories": "quant-ph cs.PL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quipper is a practical programming language for describing families of\nquantum circuits. In this paper, we formalize a small, but useful fragment of\nQuipper called Proto-Quipper-M. Unlike its parent Quipper, this language is\ntype-safe and has a formal denotational and operational semantics.\nProto-Quipper-M is also more general than Quipper, in that it can describe\nfamilies of morphisms in any symmetric monoidal category, of which quantum\ncircuits are but one example. We design Proto-Quipper-M from the ground up, by\nfirst giving a general categorical model of parameters and state. The\ndistinction between parameters and state is also known from hardware\ndescription languages. A parameter is a value that is known at circuit\ngeneration time, whereas a state is a value that is known at circuit execution\ntime. After finding some interesting categorical structures in the model, we\nthen define the programming language to fit the model. We cement the connection\nbetween the language and the model by proving type safety, soundness, and\nadequacy properties.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 15:14:59 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 03:48:39 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Rios", "Francisco", "", "Dalhousie University, Halifax, Canada"], ["Selinger", "Peter", "", "Dalhousie University, Halifax, Canada"]]}, {"id": "1706.02769", "submitter": "Vineeth Kashyap", "authors": "Vineeth Kashyap, David Bingham Brown, Ben Liblit, David Melski, Thomas\n  Reps", "title": "Source Forager: A Search Engine for Similar Source Code", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.IR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developers spend a significant amount of time searching for code: e.g., to\nunderstand how to complete, correct, or adapt their own code for a new context.\nUnfortunately, the state of the art in code search has not evolved much beyond\ntext search over tokenized source. Code has much richer structure and semantics\nthan normal text, and this property can be exploited to specialize the\ncode-search process for better querying, searching, and ranking of code-search\nresults.\n  We present a new code-search engine named Source Forager. Given a query in\nthe form of a C/C++ function, Source Forager searches a pre-populated code\ndatabase for similar C/C++ functions. Source Forager preprocesses the database\nto extract a variety of simple code features that capture different aspects of\ncode. A search returns the $k$ functions in the database that are most similar\nto the query, based on the various extracted code features.\n  We tested the usefulness of Source Forager using a variety of code-search\nqueries from two domains. Our experiments show that the ranked results returned\nby Source Forager are accurate, and that query-relevant functions can be\nreliably retrieved even when searching through a large code database that\ncontains very few query-relevant functions.\n  We believe that Source Forager is a first step towards much-needed tools that\nprovide a better code-search experience.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 20:57:20 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Kashyap", "Vineeth", ""], ["Brown", "David Bingham", ""], ["Liblit", "Ben", ""], ["Melski", "David", ""], ["Reps", "Thomas", ""]]}, {"id": "1706.03272", "submitter": "Hasan Jamil", "authors": "Hasan M. Jamil", "title": "Computational Thinking in Patch", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the future likely to see even more pervasive computation, computational\nthinking (problem-solving skills incorporating computing knowledge) is now\nbeing recognized as a fundamental skill needed by all students. Computational\nthinking is conceptualizing as opposed to programming, promotes natural human\nthinking style than algorithmic reasoning, complements and combines\nmathematical and engineering thinking, and it emphasizes ideas, not artifacts.\nIn this paper, we outline a new visual language, called Patch, using which\nstudents are able to express their solutions to eScience computational problems\nin abstract visual tools. Patch is closer to high level procedural languages\nsuch as C++ or Java than Scratch or Snap! but similar to them in ease of use\nand combines simplicity and expressive power in one single platform.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 19:00:51 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Jamil", "Hasan M.", ""]]}, {"id": "1706.03814", "submitter": "Marianna Rapoport", "authors": "Marianna Rapoport, Ifaz Kabir, Paul He, Ond\\v{r}ej Lhot\\'ak", "title": "A Simple Soundness Proof for Dependent Object Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dependent Object Types (DOT) is intended to be a core calculus for modelling\nScala. Its distinguishing feature is abstract type members, fields in objects\nthat hold types rather than values. Proving soundness of DOT has been\nsurprisingly challenging, and existing proofs are complicated, and reason about\nmultiple concepts at the same time (e.g. types, values, evaluation). To serve\nas a core calculus for Scala, DOT should be easy to experiment with and extend,\nand therefore its soundness proof needs to be easy to modify.\n  This paper presents a simple and modular proof strategy for reasoning in DOT.\nThe strategy separates reasoning about types from other concerns. It is centred\naround a theorem that connects the full DOT type system to a restricted variant\nin which the challenges and paradoxes caused by abstract type members are\neliminated. Almost all reasoning in the proof is done in the intuitive world of\nthis restricted type system. Once we have the necessary results about types, we\nobserve that the other aspects of DOT are mostly standard and can be\nincorporated into a soundness proof using familiar techniques known from other\ncalculi.\n  Our paper comes with a machine-verified version of the proof in Coq.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 19:06:54 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Rapoport", "Marianna", ""], ["Kabir", "Ifaz", ""], ["He", "Paul", ""], ["Lhot\u00e1k", "Ond\u0159ej", ""]]}, {"id": "1706.04468", "submitter": "Valentin W\\\"ustholz", "authors": "Kostas Ferles, Valentin W\\\"ustholz, Maria Christakis, Isil Dillig", "title": "Failure-Directed Program Trimming (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new program simplification technique called program\ntrimming that aims to improve the scalability and precision of safety checking\ntools. Given a program ${\\mathcal P}$, program trimming generates a new program\n${\\mathcal P}'$ such that ${\\mathcal P}$ and ${\\mathcal P}'$ are equi-safe\n(i.e., ${\\mathcal P}'$ has a bug if and only if ${\\mathcal P}$ has a bug), but\n${\\mathcal P}'$ has fewer execution paths than ${\\mathcal P}$. Since many\nprogram analyzers are sensitive to the number of execution paths, program\ntrimming has the potential to improve the effectiveness of safety checking\ntools.\n  In addition to introducing the concept of program trimming, this paper also\npresents a lightweight static analysis that can be used as a pre-processing\nstep to remove program paths while retaining equi-safety. We have implemented\nthe proposed technique in a tool called Trimmer and evaluate it in the context\nof two program analysis techniques, namely abstract interpretation and dynamic\nsymbolic execution. Our experiments show that program trimming significantly\nimproves the effectiveness of both techniques.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 13:10:18 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Ferles", "Kostas", ""], ["W\u00fcstholz", "Valentin", ""], ["Christakis", "Maria", ""], ["Dillig", "Isil", ""]]}, {"id": "1706.04567", "submitter": "Yue Li", "authors": "Yue Li, Tian Tan, Jingling Xue", "title": "Understanding and Analyzing Java Reflection", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Java reflection has been increasingly used in a wide range of software. It\nallows a software system to inspect and/or modify the behaviour of its classes,\ninterfaces, methods and fields at runtime, enabling the software to adapt to\ndynamically changing runtime environments. However, this dynamic language\nfeature imposes significant challenges to static analysis, because the\nbehaviour of reflection-rich software is logically complex and statically hard\nto predict, especially when manipulated frequently by statically unknown string\nvalues. As a result, existing static analysis tools either ignore reflection or\nhandle it partially, resulting in missed, important behaviours, i.e., unsound\nresults. Therefore, improving or even achieving soundness in (static)\nreflection analysis -- an analysis that infers statically the behaviour of\nreflective code -- will provide significant benefits to many analysis clients,\nsuch as bug detectors, security analyzers and program verifiers. This paper\nmakes two contributions: we provide a comprehensive understanding of Java\nreflection through examining its underlying concept, API and real-world usage,\nand, building on this, we introduce a new static approach to resolving Java\nreflection effectively in practice. We have implemented our reflection analysis\nin an open-source tool, called SOLAR, and evaluated its effectiveness\nextensively with large Java programs and libraries. Our experimental results\ndemonstrate that SOLAR is able to (1) resolve reflection more soundly than the\nstate-of-the-art reflection analysis; (2) automatically and accurately identify\nthe parts of the program where reflection is resolved unsoundly or imprecisely;\nand (3) guide users to iteratively refine the analysis results by using\nlightweight annotations until their specific requirements are satisfied.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 16:17:03 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Li", "Yue", ""], ["Tan", "Tian", ""], ["Xue", "Jingling", ""]]}, {"id": "1706.05271", "submitter": "Youssef El Bakouny", "authors": "Youssef El Bakouny, Tristan Crolard, Dani Mezher", "title": "A Coq-based synthesis of Scala programs which are\n  correct-by-construction", "comments": "2 pages, accepted version of the paper as submitted to FTfJP 2017\n  (Formal Techniques for Java-like Programs), June 18-23, 2017, Barcelona ,\n  Spain", "journal-ref": null, "doi": "10.1145/3103111.3104041", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper introduces Scala-of-Coq, a new compiler that allows a\nCoq-based synthesis of Scala programs which are \"correct-by-construction\". A\ntypical workflow features a user implementing a Coq functional program, proving\nthis program's correctness with regards to its specification and making use of\nScala-of-Coq to synthesize a Scala program that can seamlessly be integrated\ninto an existing industrial Scala or Java application.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 13:33:45 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Bakouny", "Youssef El", ""], ["Crolard", "Tristan", ""], ["Mezher", "Dani", ""]]}, {"id": "1706.05851", "submitter": "Julia Belyakova", "authors": "Julia Belyakova", "title": "Generic Approach to Certified Static Checking of Module-like Constructs", "comments": null, "journal-ref": null, "doi": "10.1145/3103111.3104045", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of certified static checking of\nmodule-like constructs of programming languages. We argue that there are\nalgorithms and properties related to modules that can be defined and proven in\nan abstract way. We advocate the design of a generic Coq library, which is\naimed to provide three building blocks for each checking mechanism:\npropositional, computable, and correctness proofs. Implemented part of the\nlibrary is justified by applying it to a certified static checker of an\nextension of STLC.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 09:39:01 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Belyakova", "Julia", ""]]}, {"id": "1706.06462", "submitter": "Kohei Suenaga", "authors": "Taro Sekiyama, Akifumi Imanishi, and Kohei Suenaga", "title": "Towards Proof Synthesis Guided by Neural Machine Translation for\n  Intuitionistic Propositional Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the recent evolution of deep neural networks (DNNs) in machine\nlearning, we explore their application to PL-related topics. This paper is the\nfirst step towards this goal; we propose a proof-synthesis method for the\nnegation-free propositional logic in which we use a DNN to obtain a guide of\nproof search. The idea is to view the proof-synthesis problem as a translation\nfrom a proposition to its proof. We train seq2seq, which is a popular network\nin neural machine translation, so that it generates a proof encoded as a\n$\\lambda$-term of a given proposition. We implement the whole framework and\nempirically observe that a generated proof term is close to a correct proof in\nterms of the tree edit distance of AST. This observation justifies using the\noutput from a trained seq2seq model as a guide for proof search.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 14:22:29 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Sekiyama", "Taro", ""], ["Imanishi", "Akifumi", ""], ["Suenaga", "Kohei", ""]]}, {"id": "1706.06497", "submitter": "H\\\"armel Nestra", "authors": "H\\\"armel Nestra", "title": "Alignment Elimination from Adams' Grammars", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adams' extension of parsing expression grammars enables specifying\nindentation sensitivity using two non-standard grammar constructs ---\nindentation by a binary relation and alignment. This paper proposes a\nstep-by-step transformation of well-formed Adams' grammars for elimination of\nthe alignment construct from the grammar. The idea that alignment could be\navoided was suggested by Adams but no process for achieving this aim has been\ndescribed before.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 14:50:03 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Nestra", "H\u00e4rmel", ""]]}, {"id": "1706.07372", "submitter": "Carmen Torres Lopez", "authors": "Carmen Torres Lopez, Stefan Marr, Hanspeter M\\\"ossenb\\\"ock, Elisa\n  Gonzalez Boix", "title": "A Study of Concurrency Bugs and Advanced Development Support for\n  Actor-based Programs", "comments": "- Submitted for review - Removed section 6 \"Research Roadmap for\n  Debuggers\", its content was summarized in the Future Work section - Added\n  references for section 1, section 3, section 4.3 and section 5.1 - Updated\n  citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The actor model is an attractive foundation for developing concurrent\napplications because actors are isolated concurrent entities that communicate\nthrough asynchronous messages and do not share state. Thereby, they avoid\nconcurrency bugs such as data races, but are not immune to concurrency bugs in\ngeneral. This study taxonomizes concurrency bugs in actor-based programs\nreported in literature. Furthermore, it analyzes the bugs to identify the\npatterns causing them as well as their observable behavior. Based on this\ntaxonomy, we further analyze the literature and find that current approaches to\nstatic analysis and testing focus on communication deadlocks and message\nprotocol violations. However, they do not provide solutions to identify\nlivelocks and behavioral deadlocks. The insights obtained in this study can be\nused to improve debugging support for actor-based programs with new debugging\ntechniques to identify the root cause of complex concurrency bugs.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 15:31:53 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 16:35:24 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2018 08:41:27 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Lopez", "Carmen Torres", ""], ["Marr", "Stefan", ""], ["M\u00f6ssenb\u00f6ck", "Hanspeter", ""], ["Boix", "Elisa Gonzalez", ""]]}, {"id": "1706.07946", "submitter": "Thom Fruehwirth", "authors": "Thom Fruehwirth", "title": "Justifications in Constraint Handling Rules for Logical Retraction in\n  Dynamic Algorithms", "comments": "Pre-proceedings paper presented at the 27th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2017), Namur,\n  Belgium, 10-12 October 2017 (arXiv:1708.07854)", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2017/23", "categories": "cs.AI cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a straightforward source-to-source transformation that introduces\njustifications for user-defined constraints into the CHR programming language.\nThen a scheme of two rules suffices to allow for logical retraction (deletion,\nremoval) of constraints during computation. Without the need to recompute from\nscratch, these rules remove not only the constraint but also undo all\nconsequences of the rule applications that involved the constraint. We prove a\nconfluence result concerning the rule scheme and show its correctness. When\nalgorithms are written in CHR, constraints represent both data and operations.\nCHR is already incremental by nature, i.e. constraints can be added at runtime.\nLogical retraction adds decrementality. Hence any algorithm written in CHR with\njustifications will become fully dynamic. Operations can be undone and data can\nbe removed at any point in the computation without compromising the correctness\nof the result. We present two classical examples of dynamic algorithms, written\nin our prototype implementation of CHR with justifications that is available\nonline: maintaining the minimum of a changing set of numbers and shortest paths\nin a graph whose edges change.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 12:43:50 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 13:07:20 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Fruehwirth", "Thom", ""]]}, {"id": "1706.08007", "submitter": "Benjamin Cosman", "authors": "Benjamin Cosman, Ranjit Jhala", "title": "Local Refinement Typing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the Fusion algorithm for local refinement type inference,\nyielding a new SMT-based method for verifying programs with polymorphic data\ntypes and higher-order functions. Fusion is concise as the programmer need only\nwrite signatures for (externally exported) top-level functions and places with\ncyclic (recursive) dependencies, after which Fusion can predictably synthesize\nthe most precise refinement types for all intermediate terms (expressible in\nthe decidable refinement logic), thereby checking the program without false\nalarms. We have implemented Fusion and evaluated it on the benchmarks from the\nLiquidHaskell suite totalling about 12KLOC. Fusion checks an existing safety\nbenchmark suite using about half as many templates as previously required and\nnearly 2x faster. In a new set of theorem proving benchmarks Fusion is both 10\n- 50x faster and, by synthesizing the most precise types, avoids false alarms\nto make verification possible.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 22:06:23 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Cosman", "Benjamin", ""], ["Jhala", "Ranjit", ""]]}, {"id": "1706.09715", "submitter": "J. Garrett Morris", "authors": "J. Garrett Morris and Richard Eisenberg", "title": "Constrained Type Families", "comments": "Originally presented at ICFP 2017; extended edition", "journal-ref": null, "doi": "10.1145/3110286", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to support partiality in type-level computation\nwithout compromising expressiveness or type safety. Existing frameworks for\ntype-level computation either require totality or implicitly assume it. For\nexample, type families in Haskell provide a powerful, modular means of defining\ntype-level computation. However, their current design implicitly assumes that\ntype families are total, introducing nonsensical types and significantly\ncomplicating the metatheory of type families and their extensions. We propose\nan alternative design, using qualified types to pair type-level computations\nwith predicates that capture their domains. Our approach naturally captures the\nintuitive partiality of type families, simplifying their metatheory. As\nevidence, we present the first complete proof of consistency for a language\nwith closed type families.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 12:29:58 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Morris", "J. Garrett", ""], ["Eisenberg", "Richard", ""]]}]