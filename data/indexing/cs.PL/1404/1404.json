[{"id": "1404.0073", "submitter": "EPTCS", "authors": "Abeer S. Al-Humaimeedy, Maribel Fern\\'andez", "title": "General dynamic recovery for compensating CSP", "comments": "In Proceedings DCM 2012, arXiv:1403.7579", "journal-ref": "EPTCS 143, 2014, pp. 3-16", "doi": "10.4204/EPTCS.143.1", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compensation is a technique to roll-back a system to a consistent state in\ncase of failure. Recovery mechanisms for compensating calculi specify the order\nof execution of compensation sequences. Dynamic recovery means that the order\nof execution is determined at runtime. In this paper, we define an extension of\nCompensating CSP, called DEcCSP, with general dynamic recovery. We provide a\nformal, operational semantics for the calculus, and illustrate its expressive\npower with a case study. In contrast with previous versions of Compensating\nCSP, DEcCSP provides mechanisms to replace or discard compensations at runtime.\nAdditionally, we bring back to DEcCSP standard CSP operators that are not\navailable in other compensating CSP calculi, and introduce channel\ncommunication.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 00:36:54 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Al-Humaimeedy", "Abeer S.", ""], ["Fern\u00e1ndez", "Maribel", ""]]}, {"id": "1404.0076", "submitter": "EPTCS", "authors": "Eugen Jiresch (Technische Universit\\\"at Wien)", "title": "Towards a GPU-based implementation of interaction nets", "comments": "In Proceedings DCM 2012, arXiv:1403.7579", "journal-ref": "EPTCS 143, 2014, pp. 41-53", "doi": "10.4204/EPTCS.143.4", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ingpu, a GPU-based evaluator for interaction nets that heavily\nutilizes their potential for parallel evaluation. We discuss advantages and\nchallenges of the ongoing implementation of ingpu and compare its performance\nto existing interaction nets evaluators.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 00:37:24 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Jiresch", "Eugen", "", "Technische Universit\u00e4t Wien"]]}, {"id": "1404.0084", "submitter": "EPTCS", "authors": "Adriana Compagnoni (Stevens Institute of Technology), Paola Giannini\n  (Universit\\`a del Piemonte Orientale), Catherine Kim (Stevens Institute of\n  Technology), Matthew Milideo (Stevens Institute of Technology), Vishakha\n  Sharma (Stevens Institute of Technology)", "title": "A Calculus of Located Entities", "comments": "In Proceedings DCM 2013, arXiv:1403.7685", "journal-ref": "EPTCS 144, 2014, pp. 41-56", "doi": "10.4204/EPTCS.144.4", "report-no": null, "categories": "cs.PL cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define BioScapeL, a stochastic pi-calculus in 3D-space. A novel aspect of\nBioScapeL is that entities have programmable locations. The programmer can\nspecify a particular location where to place an entity, or a location relative\nto the current location of the entity. The motivation for the extension comes\nfrom the need to describe the evolution of populations of biochemical species\nin space, while keeping a sufficiently high level description, so that\nphenomena like diffusion, collision, and confinement can remain part of the\nsemantics of the calculus. Combined with the random diffusion movement\ninherited from BioScape, programmable locations allow us to capture the\nassemblies of configurations of polymers, oligomers, and complexes such as\nmicrotubules or actin filaments.\n  Further new aspects of BioScapeL include random translation and scaling.\nRandom translation is instrumental in describing the location of new entities\nrelative to the old ones. For example, when a cell secretes a hydronium ion,\nthe ion should be placed at a given distance from the originating cell, but in\na random direction. Additionally, scaling allows us to capture at a high level\nevents such as division and growth; for example, daughter cells after mitosis\nhave half the size of the mother cell.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 00:39:01 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Compagnoni", "Adriana", "", "Stevens Institute of Technology"], ["Giannini", "Paola", "", "Universit\u00e0 del Piemonte Orientale"], ["Kim", "Catherine", "", "Stevens Institute of\n  Technology"], ["Milideo", "Matthew", "", "Stevens Institute of Technology"], ["Sharma", "Vishakha", "", "Stevens Institute of Technology"]]}, {"id": "1404.0085", "submitter": "EPTCS", "authors": "Carlos Alberto Ram\\'irez Restrepo, Jorge A. P\\'erez, Jes\\'us Aranda,\n  Juan Francisco D\\'iaz-Frias", "title": "Towards Formal Interaction-Based Models of Grid Computing\n  Infrastructures", "comments": "In Proceedings DCM 2013, arXiv:1403.7685", "journal-ref": "EPTCS 144, 2014, pp. 57-72", "doi": "10.4204/EPTCS.144.5", "report-no": null, "categories": "cs.PL cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid computing (GC) systems are large-scale virtual machines, built upon a\nmassive pool of resources (processing time, storage, software) that often span\nmultiple distributed domains. Concurrent users interact with the grid by adding\nnew tasks; the grid is expected to assign resources to tasks in a fair,\ntrustworthy way. These distinctive features of GC systems make their\nspecification and verification a challenging issue. Although prior works have\nproposed formal approaches to the specification of GC systems, a precise\naccount of the interaction model which underlies resource sharing has not been\nyet proposed. In this paper, we describe ongoing work aimed at filling in this\ngap. Our approach relies on (higher-order) process calculi: these core\nlanguages for concurrency offer a compositional framework in which GC systems\ncan be precisely described and potentially reasoned about.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 00:39:11 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Restrepo", "Carlos Alberto Ram\u00edrez", ""], ["P\u00e9rez", "Jorge A.", ""], ["Aranda", "Jes\u00fas", ""], ["D\u00edaz-Frias", "Juan Francisco", ""]]}, {"id": "1404.0089", "submitter": "EPTCS", "authors": "Mladen Skelin, Marc Geilen, Francky Catthoor, Sverre Hendseth", "title": "Worst-case Throughput Analysis for Parametric Rate and Parametric Actor\n  Execution Time Scenario-Aware Dataflow Graphs", "comments": "In Proceedings SynCoP 2014, arXiv:1403.7841", "journal-ref": "EPTCS 145, 2014, pp. 65-79", "doi": "10.4204/EPTCS.145.7", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scenario-aware dataflow (SADF) is a prominent tool for modeling and analysis\nof dynamic embedded dataflow applications. In SADF the application is\nrepresented as a finite collection of synchronous dataflow (SDF) graphs, each\nof which represents one possible application behaviour or scenario. A finite\nstate machine (FSM) specifies the possible orders of scenario occurrences. The\nSADF model renders the tightest possible performance guarantees, but is limited\nby its finiteness. This means that from a practical point of view, it can only\nhandle dynamic dataflow applications that are characterized by a reasonably\nsized set of possible behaviours or scenarios. In this paper we remove this\nlimitation for a class of SADF graphs by means of SADF model parametrization in\nterms of graph port rates and actor execution times. First, we formally define\nthe semantics of the model relevant for throughput analysis based on (max,+)\nlinear system theory and (max,+) automata. Second, by generalizing some of the\nexisting results, we give the algorithms for worst-case throughput analysis of\nparametric rate and parametric actor execution time acyclic SADF graphs with a\nfully connected, possibly infinite state transition system. Third, we\ndemonstrate our approach on a few realistic applications from digital signal\nprocessing (DSP) domain mapped onto an embedded multi-processor architecture.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 00:40:11 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Skelin", "Mladen", ""], ["Geilen", "Marc", ""], ["Catthoor", "Francky", ""], ["Hendseth", "Sverre", ""]]}, {"id": "1404.0099", "submitter": "Vikash Mansinghka", "authors": "Vikash Mansinghka and Daniel Selsam and Yura Perov", "title": "Venture: a higher-order probabilistic programming platform with\n  programmable inference", "comments": "78 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Venture, an interactive virtual machine for probabilistic\nprogramming that aims to be sufficiently expressive, extensible, and efficient\nfor general-purpose use. Like Church, probabilistic models and inference\nproblems in Venture are specified via a Turing-complete, higher-order\nprobabilistic language descended from Lisp. Unlike Church, Venture also\nprovides a compositional language for custom inference strategies built out of\nscalable exact and approximate techniques. We also describe four key aspects of\nVenture's implementation that build on ideas from probabilistic graphical\nmodels. First, we describe the stochastic procedure interface (SPI) that\nspecifies and encapsulates primitive random variables. The SPI supports custom\ncontrol flow, higher-order probabilistic procedures, partially exchangeable\nsequences and ``likelihood-free'' stochastic simulators. It also supports\nexternal models that do inference over latent variables hidden from Venture.\nSecond, we describe probabilistic execution traces (PETs), which represent\nexecution histories of Venture programs. PETs capture conditional dependencies,\nexistential dependencies and exchangeable coupling. Third, we describe\npartitions of execution histories called scaffolds that factor global inference\nproblems into coherent sub-problems. Finally, we describe a family of\nstochastic regeneration algorithms for efficiently modifying PET fragments\ncontained within scaffolds. Stochastic regeneration linear runtime scaling in\ncases where many previous approaches scaled quadratically. We show how to use\nstochastic regeneration and the SPI to implement general-purpose inference\nstrategies such as Metropolis-Hastings, Gibbs sampling, and blocked proposals\nbased on particle Markov chain Monte Carlo and mean-field variational inference\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 01:44:05 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Mansinghka", "Vikash", ""], ["Selsam", "Daniel", ""], ["Perov", "Yura", ""]]}, {"id": "1404.1830", "submitter": "Christoph Kirsch", "authors": "Silviu S. Craciunas, Christoph M. Kirsch, Hannes Payer, Harald R\\\"ock,\n  Ana Sokolova", "title": "Concurrency and Scalability versus Fragmentation and Compaction with\n  Compact-fit", "comments": null, "journal-ref": null, "doi": null, "report-no": "University of Salzburg, Department of Computer Sciences, Technical\n  Report 2009-02", "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study, formally and experimentally, the trade-off in temporal and spatial\noverhead when managing contiguous blocks of memory using the explicit, dynamic\nand real-time heap management system Compact-fit (CF). The key property of CF\nis that temporal and spatial overhead can be bounded, related, and predicted in\nconstant time through the notion of partial and incremental compaction. Partial\ncompaction determines the maximally tolerated degree of memory fragmentation.\nIncremental compaction of objects, introduced here, determines the maximal\namount of memory involved in any, logically atomic, portion of a compaction\noperation. We explore CF's potential application space on (1) multiprocessor\nand multicore systems as well as on (2) memory-constrained uniprocessor\nsystems. For (1), we argue that little or no compaction is likely to avoid the\nworst case in temporal as well as spatial overhead but also observe that\nscalability only improves by a constant factor. Scalability can be further\nimproved significantly by reducing overall data sharing through separate\ninstances of Compact-fit. For (2), we observe that incremental compaction can\neffectively trade-off throughput and memory fragmentation for lower latency.\n", "versions": [{"version": "v1", "created": "Mon, 7 Apr 2014 16:07:54 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Craciunas", "Silviu S.", ""], ["Kirsch", "Christoph M.", ""], ["Payer", "Hannes", ""], ["R\u00f6ck", "Harald", ""], ["Sokolova", "Ana", ""]]}, {"id": "1404.2163", "submitter": "Timur Mirzoev", "authors": "Dr. Timur Mirzoev, Lawton Sack", "title": "Webpage Load Speed: ASP.NET vs. PHP", "comments": null, "journal-ref": "i-managers Journal on Information Technology, Vol. 2, No. 2, March\n  May 2013", "doi": null, "report-no": null, "categories": "cs.PL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data transmission speeds over the Internet continue to increase, it is\nnecessary to research and identify technologies that can take advantage of the\nincreased speeds by enhancing the loading speed of webpages. One area of\nconsideration is found in the type of framework that is utilized for a website.\nThere are numerous frameworks that can be chosen from to be used to support a\nwebsite, each with their distinctive advantages. There are many different\nopinions that have been tested on which framework should be used to fully\nrealize the optimization of page load speed. This manuscript examines and\nimplements testing methods for two popular frameworks, Active Server Pages .net\nand PHP, to make a final determination of which framework is most beneficial\nfor webpage load speeds.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 14:51:22 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Mirzoev", "Dr. Timur", ""], ["Sack", "Lawton", ""]]}, {"id": "1404.3407", "submitter": "Ruslan Shevchenko", "authors": "Ruslan Shevchenko", "title": "Annotated imports", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presented simple extensions to scala language related to import statements:\nexported imports, which provide ability to reuse sequence of import clauses in\ncomposable form and default rewriters, which provide mechanism for pluggable\nmacro-based AST transformation of overall compilation unit, activated by import\nof library object. Using these facilities not only allows more compact code, it\nprevents application programmer from producing certain type of errors too and\nallows to implement local language extension as libraries on top of standard\ncompiler. Part of discussed extensions is submitted to scala language committee\nas pre-sip \\cite{ai-presip} and can be used as first step for refining imports\nsemantics in the future version of scala language.\n", "versions": [{"version": "v1", "created": "Sun, 13 Apr 2014 18:20:40 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Shevchenko", "Ruslan", ""]]}, {"id": "1404.3875", "submitter": "Pierre Lescanne", "authors": "Pierre Lescanne (LIP)", "title": "Boltzmann samplers for random generation of lambda terms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomly generating structured objects is important in testing and optimizing\nfunctional programs, whereas generating random $'l$-terms is more specifically\nneeded for testing and optimizing compilers. For that a tool called QuickCheck\nhas been proposed, but in this tool the control of the random generation is\nleft to the programmer. Ten years ago, a method called Boltzmann samplers has\nbeen proposed to generate combinatorial structures. In this paper, we show how\nBoltzmann samplers can be developed to generate lambda-terms, but also other\ndata structures like trees. These samplers rely on a critical value which\nparameters the main random selector and which is exhibited here with\nexplanations on how it is computed. Haskell programs are proposed to show how\nsamplers are actually implemented.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 11:40:45 GMT"}, {"version": "v2", "created": "Mon, 28 Apr 2014 14:06:56 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Lescanne", "Pierre", "", "LIP"]]}, {"id": "1404.4246", "submitter": "Nataliia Stulova", "authors": "Nataliia Stulova, Jos\\'e F. Morales and Manuel V. Hermenegildo", "title": "An Approach to Assertion-based Debugging of Higher-Order (C)LP Programs", "comments": "24 pages, 1 figure. A 2-page extended abstract to be published as a\n  technical communication in the on-line addendum of the special issue(s) of\n  the TPLP journal for ICLP14", "journal-ref": null, "doi": null, "report-no": "CLIP-1/2014.0", "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher-order constructs extend the expressiveness of first-order (Constraint)\nLogic Programming ((C)LP) both syntactically and semantically. At the same time\nassertions have been in use for some time in (C)LP systems helping programmers\ndetect errors and validate programs. However, these assertion-based extensions\nto (C)LP have not been integrated well with higher-order to date. This paper\ncontributes to filling this gap by extending the assertion-based approach to\nerror detection and program validation to the higher-order context within\n(C)LP. We propose an extension of properties and assertions as used in (C)LP in\norder to be able to fully describe arguments that are predicates. The extension\nmakes the full power of the assertion language available when describing\nhigher-order arguments. We provide syntax and semantics for (higher-order)\nproperties and assertions, as well as for programs which contain such\nassertions, including the notions of error and partial correctness and provide\nsome formal results. We also discuss several alternatives for performing\nrun-time checking of such programs.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 13:53:02 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Stulova", "Nataliia", ""], ["Morales", "Jos\u00e9 F.", ""], ["Hermenegildo", "Manuel V.", ""]]}, {"id": "1404.4666", "submitter": "Edward Givelberg", "authors": "Edward Givelberg", "title": "Object-Oriented Parallel Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an object-oriented framework for parallel programming, which is\nbased on the observation that programming objects can be naturally interpreted\nas processes. A parallel program consists of a collection of persistent\nprocesses that communicate by executing remote methods. We discuss code\nparallelization and process persistence, and explain the main ideas in the\ncontext of computations with very large data objects.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 22:36:44 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Givelberg", "Edward", ""]]}, {"id": "1404.4821", "submitter": "Sergey Kovalchuk", "authors": "Sergey V. Kovalchuk, Artem V. Zakharchuk, Jiaqi Liao, Sergey V.\n  Ivanov, Alexander V. Boukhanovsky", "title": "A Technology for BigData Analysis Task Description using Domain-Specific\n  Languages", "comments": "To appear in Proceedings of the International Conference on\n  Computational Science (ICCS) 2014", "journal-ref": null, "doi": "10.1016/j.procs.2014.05.044", "report-no": null, "categories": "cs.DC cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article presents a technology for dynamic knowledge-based building of\nDomain-Specific Languages (DSL) to describe data-intensive scientific discovery\ntasks using BigData technology. The proposed technology supports high level\nabstract definition of analytic and simulation parts of the task as well as\nintegration into the composite scientific solutions. Automatic translation of\nthe abstract task definition enables seamless integration of various data\nsources within single solution.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 15:44:29 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["Kovalchuk", "Sergey V.", ""], ["Zakharchuk", "Artem V.", ""], ["Liao", "Jiaqi", ""], ["Ivanov", "Sergey V.", ""], ["Boukhanovsky", "Alexander V.", ""]]}, {"id": "1404.5770", "submitter": "Sebastian Erdweg", "authors": "Sebastian Erdweg, Tijs van der Storm, Yi Dai", "title": "Capture-Avoiding and Hygienic Program Transformations (incl. Proofs)", "comments": "In Proceedings of European Conference on Object-Oriented Programming\n  (ECOOP) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program transformations in terms of abstract syntax trees compromise\nreferential integrity by introducing variable capture. Variable capture occurs\nwhen in the generated program a variable declaration accidentally shadows the\nintended target of a variable reference. Existing transformation systems either\ndo not guarantee the avoidance of variable capture or impair the implementation\nof transformations.\n  We present an algorithm called name-fix that automatically eliminates\nvariable capture from a generated program by systematically renaming variables.\nname-fix is guided by a graph representation of the binding structure of a\nprogram, and requires name-resolution algorithms for the source language and\nthe target language of a transformation. name-fix is generic and works for\narbitrary transformations in any transformation system that supports origin\ntracking for names. We verify the correctness of name-fix and identify an\ninteresting class of transformations for which name-fix provides hygiene. We\ndemonstrate the applicability of name-fix for implementing capture-avoiding\nsubstitution, inlining, lambda lifting, and compilers for two domain-specific\nlanguages.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 10:08:11 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Erdweg", "Sebastian", ""], ["van der Storm", "Tijs", ""], ["Dai", "Yi", ""]]}, {"id": "1404.5785", "submitter": "EPTCS", "authors": "Catherine Dubois (ENSIIE - CEDRIC), Dimitra Giannakopoulou (NASA -\n  Ames), Dominique M\\'ery (Universit\\'e de Lorraine - LORIA)", "title": "Proceedings 1st Workshop on Formal Integrated Development Environment", "comments": null, "journal-ref": "EPTCS 149, 2014", "doi": "10.4204/EPTCS.149", "report-no": null, "categories": "cs.SE cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of F-IDE 2014, the first international\nworkshop on Formal Integrated Development Environment, which was held as an\nETAPS 2014 satellite event, on April 6, 2014, in Grenoble (France). High levels\nof safety, security and also privacy standards require the use of formal\nmethods to specify and develop compliant software (sub)systems. Any standard\ncomes with an assessment process, which requires a complete documentation of\nthe application in order to ease the justification of design choices and the\nreview of code and proofs. Thus tools are needed for handling specifications,\nprogram constructs and verification artifacts. The aim of the F-IDE workshop is\nto provide a forum for presenting and discussing research efforts as well as\nexperience returns on design, development and usage of formal IDE aiming at\nmaking formal methods \"easier\" for both specialists and non-specialists.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 11:09:18 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Dubois", "Catherine", "", "ENSIIE - CEDRIC"], ["Giannakopoulou", "Dimitra", "", "NASA -\n  Ames"], ["M\u00e9ry", "Dominique", "", "Universit\u00e9 de Lorraine - LORIA"]]}, {"id": "1404.6218", "submitter": "Ashkan Tousimojarad Mr", "authors": "Ashkan Tousimojarad and Wim Vanderbauwhede", "title": "A Parallel Task-based Approach to Linear Algebra", "comments": "Final version as appeared in \"dx.doi.org/10.1109/ISPDC.2014.11\"", "journal-ref": "Tousimojarad, A., Vanderbauwhede, W.: A parallel task-based\n  approach to linear algebra. In: Parallel and Distributed Computing (ISPDC),\n  2014 IEEE 13th International Symposium on. pp. 59-66. IEEE (2014)", "doi": "10.1109/ISPDC.2014.11", "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processors with large numbers of cores are becoming commonplace. In order to\ntake advantage of the available resources in these systems, the programming\nparadigm has to move towards increased parallelism. However, increasing the\nlevel of concurrency in the program does not necessarily lead to better\nperformance. Parallel programming models have to provide flexible ways of\ndefining parallel tasks and at the same time, efficiently managing the created\ntasks. OpenMP is a widely accepted programming model for shared-memory\narchitectures. In this paper we highlight some of the drawbacks in the OpenMP\ntasking approach, and propose an alternative model based on the Glasgow\nParallel Reduction Machine (GPRM) programming framework. As the main focus of\nthis study, we deploy our model to solve a fundamental linear algebra problem,\nLU factorisation of sparse matrices. We have used the SparseLU benchmark from\nthe BOTS benchmark suite, and compared the results obtained from our model to\nthose of the OpenMP tasking approach. The TILEPro64 system has been used to run\nthe experiments. The results are very promising, not only because of the\nperformance improvement for this particular problem, but also because they\nverify the task management efficiency, stability, and flexibility of our model,\nwhich can be applied to solve problems in future many-core systems.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 18:39:30 GMT"}, {"version": "v2", "created": "Fri, 3 Oct 2014 14:53:58 GMT"}, {"version": "v3", "created": "Mon, 6 Oct 2014 15:46:24 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Tousimojarad", "Ashkan", ""], ["Vanderbauwhede", "Wim", ""]]}, {"id": "1404.6383", "submitter": "Pierre de Buyl", "authors": "Valentin Haenel", "title": "Bloscpack: a compressed lightweight serialization format for numerical\n  data", "comments": "Part of the Proceedings of the 6th European Conference on Python in\n  Science (EuroSciPy 2013), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2013-02", "categories": "cs.MS cs.PL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper introduces the Bloscpack file format and the accompanying Python\nreference implementation. Bloscpack is a lightweight, compressed binary\nfile-format based on the Blosc codec and is designed for lightweight, fast\nserialization of numerical data. This article presents the features of the\nfile-format and some some API aspects of the reference implementation, in\nparticular the ability to handle Numpy ndarrays. Furthermore, in order to\ndemonstrate its utility, the format is compared both feature- and\nperformance-wise to a few alternative lightweight serialization solutions for\nNumpy ndarrays. The performance comparisons take the form of some comprehensive\nbenchmarks over a range of different artificial datasets with varying size and\ncomplexity, the results of which are presented as the last section of this\narticle.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 10:53:23 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 14:16:55 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Haenel", "Valentin", ""]]}, {"id": "1404.6385", "submitter": "Pierre de Buyl", "authors": "Fabrice Salvaire", "title": "High-Content Digital Microscopy with Python", "comments": "Part of the Proceedings of the 6th European Conference on Python in\n  Science (EuroSciPy 2013), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2013-05", "categories": "cs.CE cs.PL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  High-Content Digital Microscopy enhances user comfort, data storage and\nanalysis throughput, paving the way to new researches and medical diagnostics.\nA digital microscopy platform aims at capturing an image of a cover slip, at\nstoring information on a file server and a database, at visualising the image\nand analysing its content. We will discuss how the Python ecosystem can provide\nsuch software framework efficiently. Moreover this paper will give an\nillustration of the data chunking approach to manage the huge amount of data.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 10:54:26 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 14:19:35 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Salvaire", "Fabrice", ""]]}, {"id": "1404.6387", "submitter": "Pierre de Buyl", "authors": "Kelsey D'Souza", "title": "PySTEMM: Executable Concept Modeling for K-12 STEM Learning", "comments": "Part of the Proceedings of the 6th European Conference on Python in\n  Science (EuroSciPy 2013), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2013-06", "categories": "cs.CY cs.PL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Modeling should play a central role in K-12 STEM education, where it could\nmake classes much more engaging. A model underlies every scientific theory, and\nmodels are central to all the STEM disciplines (Science, Technology,\nEngineering, Math). This paper describes executable concept modeling of STEM\nconcepts using immutable objects and pure functions in Python. I present\nexamples in math, physics, chemistry, and engineering, built using a\nproof-of-concept tool called PySTEMM . The approach applies to all STEM areas\nand supports learning with pictures, narrative, animation, and graph plots.\nModels can extend each other, simplifying getting started. The\nfunctional-programming style reduces incidental complexity and code debugging.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 10:55:44 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["D'Souza", "Kelsey", ""]]}, {"id": "1404.6388", "submitter": "Pierre de Buyl", "authors": "Riccardo Murri", "title": "Performance of Python runtimes on a non-numeric scientific code", "comments": "Part of the Proceedings of the 6th European Conference on Python in\n  Science (EuroSciPy 2013), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2013-07", "categories": "cs.MS cs.PL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The Python library FatGHol FatGHoL used in Murri2012 to reckon the rational\nhomology of the moduli space of Riemann surfaces is an example of a non-numeric\nscientific code: most of the processing it does is generating graphs\n(represented by complex Python objects) and computing their isomorphisms (a\ntriple of Python lists; again a nested data structure). These operations are\nrepeated many times over: for example, the spaces and are triangulated by\n4'583'322 and 747'664 graphs, respectively. This is an opportunity for every\nPython runtime to prove its strength in optimization. The purpose of this\nexperiment was to assess the maturity of alternative Python runtimes, in terms\nof: compatibility with the language as implemented in CPython 2.7, and\nperformance speedup. This paper compares the results and experiences from\nrunning FatGHol with different Python runtimes: CPython 2.7.5, PyPy 2.1, Cython\n0.19, Numba 0.11, Nuitka 0.4.4 and Falcon.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 10:55:48 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 14:21:46 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Murri", "Riccardo", ""]]}, {"id": "1404.6390", "submitter": "Pierre de Buyl", "authors": "Stefan Richthofer", "title": "JyNI - Using native CPython-Extensions in Jython", "comments": "Part of the Proceedings of the 6th European Conference on Python in\n  Science (EuroSciPy 2013), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2013-09", "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Jython is a Java based Python implementation and the most seamless way to\nintegrate Python and Java. However, it does not support native extensions\nwritten for CPython like NumPy or SciPy. Since most scientific Python code\nfundamentally depends on exactly such native extensions directly or indirectly,\nit usually cannot be run with Jython. JyNI (Jython Native Interface) aims to\nclose this gap. It is a layer that enables Jython users to load native CPython\nextensions and access them from Jython the same way as they would do in\nCPython. In order to leverage the JyNI functionality, you just have to put it\non the Java classpath when Jython is launched. It neither requires you to\nrecompile the extension code, nor to build a customized Jython fork. That\nmeans, it is binary compatible with existing extension builds. At the time of\nwriting, JyNI does not fully implement the Python C-API and it is only capable\nof loading simple examples that only involve most basic built-in types. The\nconcept is rather complete though and our goal is to provide the C-API needed\nto load NumPy as soon as possible. After that we will focus on SciPy and\nothers. We expect that our work will also enable Java developers to use CPython\nextensions like NumPy in their Java code.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 10:56:33 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 14:23:37 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Richthofer", "Stefan", ""]]}, {"id": "1404.6391", "submitter": "Pierre de Buyl", "authors": "Robert Cimrman", "title": "SfePy - Write Your Own FE Application", "comments": "Part of the Proceedings of the 6th European Conference on Python in\n  Science (EuroSciPy 2013), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2013-10", "categories": "cs.CE cs.PL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  SfePy (Simple Finite Elements in Python) is a framework for solving various\nkinds of problems (mechanics, physics, biology, ...) described by partial\ndifferential equations in two or three space dimensions by the finite element\nmethod. The paper illustrates its use in an interactive environment or as a\nframework for building custom finite-element based solvers.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 10:56:35 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 14:28:34 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Cimrman", "Robert", ""]]}, {"id": "1404.6602", "submitter": "EPTCS", "authors": "K. Rustan M. Leino (Microsoft Research, Redmond, WA, USA), Valentin\n  W\\\"ustholz (ETH Zurich, Department of Computer Science, Switzerland)", "title": "The Dafny Integrated Development Environment", "comments": "In Proceedings F-IDE 2014, arXiv:1404.5785", "journal-ref": "EPTCS 149, 2014, pp. 3-15", "doi": "10.4204/EPTCS.149.2", "report-no": null, "categories": "cs.PL cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, program verifiers and interactive theorem provers have\nbecome more powerful and more suitable for verifying large programs or proofs.\nThis has demonstrated the need for improving the user experience of these tools\nto increase productivity and to make them more accessible to non-experts. This\npaper presents an integrated development environment for Dafny-a programming\nlanguage, verifier, and proof assistant-that addresses issues present in most\nstate-of-the-art verifiers: low responsiveness and lack of support for\nunderstanding non-obvious verification failures. The paper demonstrates several\nnew features that move the state-of-the-art closer towards a verification\nenvironment that can provide verification feedback as the user types and can\npresent more helpful information about the program or failed verifications in a\ndemand-driven and unobtrusive way.\n", "versions": [{"version": "v1", "created": "Sat, 26 Apr 2014 05:32:07 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Leino", "K. Rustan M.", "", "Microsoft Research, Redmond, WA, USA"], ["W\u00fcstholz", "Valentin", "", "ETH Zurich, Department of Computer Science, Switzerland"]]}, {"id": "1404.6605", "submitter": "EPTCS", "authors": "David R. Cok, Scott C. Johnson", "title": "SPEEDY: An Eclipse-based IDE for invariant inference", "comments": "In Proceedings F-IDE 2014, arXiv:1404.5785", "journal-ref": "EPTCS 149, 2014, pp. 44-57", "doi": "10.4204/EPTCS.149.5", "report-no": null, "categories": "cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SPEEDY is an Eclipse-based IDE for exploring techniques that assist users in\ngenerating correct specifications, particularly including invariant inference\nalgorithms and tools. It integrates with several back-end tools that propose\ninvariants and will incorporate published algorithms for inferring object and\nloop invariants. Though the architecture is language-neutral, current SPEEDY\ntargets C programs. Building and using SPEEDY has confirmed earlier experience\ndemonstrating the importance of showing and editing specifications in the IDEs\nthat developers customarily use, automating as much of the production and\nchecking of specifications as possible, and showing counterexample information\ndirectly in the source code editing environment. As in previous work,\nautomation of specification checking is provided by back-end SMT solvers.\nHowever, reducing the effort demanded of software developers using formal\nmethods also requires a GUI design that guides users in writing, reviewing, and\ncorrecting specifications and automates specification inference.\n", "versions": [{"version": "v1", "created": "Sat, 26 Apr 2014 05:32:34 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Cok", "David R.", ""], ["Johnson", "Scott C.", ""]]}, {"id": "1404.6607", "submitter": "EPTCS", "authors": "Fran\\c{c}ois Pessaux (ENSTA ParisTech)", "title": "FoCaLiZe: Inside an F-IDE", "comments": "In Proceedings F-IDE 2014, arXiv:1404.5785", "journal-ref": "EPTCS 149, 2014, pp. 64-78", "doi": "10.4204/EPTCS.149.7", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For years, Integrated Development Environments have demonstrated their\nusefulness in order to ease the development of software. High-level security or\nsafety systems require proofs of compliance to standards, based on analyses\nsuch as code review and, increasingly nowadays, formal proofs of conformance to\nspecifications. This implies mixing computational and logical aspects all along\nthe development, which naturally raises the need for a notion of Formal IDE.\nThis paper examines the FoCaLiZe environment and explores the implementation\nissues raised by the decision to provide a single language to express\nspecification properties, source code and machine-checked proofs while allowing\nincremental development and code reusability. Such features create strong\ndependencies between functions, properties and proofs, and impose an particular\ncompilation scheme, which is described here. The compilation results are\nrunnable OCaml code and a checkable Coq term. All these points are illustrated\nthrough a running example.\n", "versions": [{"version": "v1", "created": "Sat, 26 Apr 2014 05:32:57 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Pessaux", "Fran\u00e7ois", "", "ENSTA ParisTech"]]}, {"id": "1404.6608", "submitter": "EPTCS", "authors": "David R. Cok (GrammaTech, Inc.)", "title": "OpenJML: Software verification for Java 7 using JML, OpenJDK, and\n  Eclipse", "comments": "In Proceedings F-IDE 2014, arXiv:1404.5785", "journal-ref": "EPTCS 149, 2014, pp. 79-92", "doi": "10.4204/EPTCS.149.8", "report-no": null, "categories": "cs.SE cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OpenJML is a tool for checking code and specifications of Java programs. We\ndescribe our experience building the tool on the foundation of JML, OpenJDK and\nEclipse, as well as on many advances in specification-based software\nverification. The implementation demonstrates the value of integrating\nspecification tools directly in the software development IDE and in automating\nas many tasks as possible. The tool, though still in progress, has now been\nused for several college-level courses on software specification and\nverification and for small-scale studies on existing Java programs.\n", "versions": [{"version": "v1", "created": "Sat, 26 Apr 2014 05:33:03 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Cok", "David R.", "", "GrammaTech, Inc."]]}, {"id": "1404.6609", "submitter": "EPTCS", "authors": "John Witulski (Heinrich-Heine Universit\\\"at D\\\"usseldorf), Michael\n  Leuschel (Heinrich-Heine Universit\\\"at D\\\"usseldorf)", "title": "Checking Computations of Formal Method Tools - A Secondary Toolchain for\n  ProB", "comments": "In Proceedings F-IDE 2014, arXiv:1404.5785", "journal-ref": "EPTCS 149, 2014, pp. 93-105", "doi": "10.4204/EPTCS.149.9", "report-no": null, "categories": "cs.SE cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the implementation of pyB, a predicate - and expression - checker\nfor the B language. The tool is to be used for a secondary tool chain for data\nvalidation and data generation, with ProB being used in the primary tool chain.\nIndeed, pyB is an independent cleanroom-implementation which is used to\ndouble-check solutions generated by ProB, an animator and model-checker for B\nspecifications. One of the major goals is to use ProB together with pyB to\ngenerate reliable outputs for high-integrity safety critical applications.\nAlthough pyB is still work in progress, the ProB/pyB toolchain has already been\nsuccessfully tested on various industrial B machines and data validation tasks.\n", "versions": [{"version": "v1", "created": "Sat, 26 Apr 2014 05:33:17 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Witulski", "John", "", "Heinrich-Heine Universit\u00e4t D\u00fcsseldorf"], ["Leuschel", "Michael", "", "Heinrich-Heine Universit\u00e4t D\u00fcsseldorf"]]}, {"id": "1404.6838", "submitter": "Mathieu Acher", "authors": "Mathieu Acher and Benoit Combemale and Philippe Collet", "title": "Metamorphic Domain-Specific Languages: A Journey Into the Shapes of a\n  Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  External or internal domain-specific languages (DSLs) or (fluent) APIs?\nWhoever you are -- a developer or a user of a DSL -- you usually have to choose\nyour side; you should not! What about metamorphic DSLs that change their shape\naccording to your needs? We report on our 4-years journey of providing the\n\"right\" support (in the domain of feature modeling), leading us to develop an\nexternal DSL, different shapes of an internal API, and maintain all these\nlanguages. A key insight is that there is no one-size-fits-all solution or no\nclear superiority of a solution compared to another. On the contrary, we found\nthat it does make sense to continue the maintenance of an external and internal\nDSL. The vision that we foresee for the future of software languages is their\nability to be self-adaptable to the most appropriate shape (including the\ncorresponding integrated development environment) according to a particular\nusage or task. We call metamorphic DSL such a language, able to change from one\nshape to another shape.\n", "versions": [{"version": "v1", "created": "Sun, 27 Apr 2014 22:51:24 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Acher", "Mathieu", ""], ["Combemale", "Benoit", ""], ["Collet", "Philippe", ""]]}, {"id": "1404.6966", "submitter": "Pierre de Buyl", "authors": "Ant\\`onia Tugores, Pere Colet", "title": "Mining online social networks with Python to study urban mobility", "comments": "Part of the Proceedings of the 6th European Conference on Python in\n  Science (EuroSciPy 2013), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2013-04", "categories": "cs.SI cs.PL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  On-line social networks have grown quickly over the last few years and\nnowadays many people use them frequently. Furthermore the emergence of\nsmartphones allows to access these networks any time from any physical\nlocation. Among the social networks, Twitter offers a particularly large set of\ndata publicly available. Here we discuss the procedure to mine this data and\nstore it in distributed databases using Python scripts. We also illustrate how\ngeolocated tweets can be used to study the mobility of people in urban areas.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 10:54:23 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Tugores", "Ant\u00f2nia", ""], ["Colet", "Pere", ""]]}, {"id": "1404.7078", "submitter": "James Cheney", "authors": "James Cheney and Sam Lindley and Philip Wadler", "title": "Query shredding: Efficient relational evaluation of queries over nested\n  multisets (extended version)", "comments": "Extended version of SIGMOD 2014 conference paper", "journal-ref": null, "doi": "10.1145/2588555.2612186", "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nested relational query languages have been explored extensively, and\nunderlie industrial language-integrated query systems such as Microsoft's LINQ.\nHowever, relational databases do not natively support nested collections in\nquery results. This can lead to major performance problems: if programmers\nwrite queries that yield nested results, then such systems typically either\nfail or generate a large number of queries. We present a new approach to query\nshredding, which converts a query returning nested data to a fixed number of\nSQL queries. Our approach, in contrast to prior work, handles multiset\nsemantics, and generates an idiomatic SQL:1999 query directly from a normal\nform for nested queries. We provide a detailed description of our translation\nand present experiments showing that it offers comparable or better performance\nthan a recent alternative approach on a range of examples.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 18:13:49 GMT"}, {"version": "v2", "created": "Fri, 2 May 2014 12:03:44 GMT"}], "update_date": "2014-05-05", "authors_parsed": [["Cheney", "James", ""], ["Lindley", "Sam", ""], ["Wadler", "Philip", ""]]}, {"id": "1404.7335", "submitter": "Florent Jacquemard", "authors": "Florent Jacquemard (Inria Paris-Rocquencourt, STMS), Cl\\'ement\n  Poncelet Sanchez (Inria Paris-Rocquencourt, STMS)", "title": "Antescofo Intermediate Representation", "comments": "RR-8520 (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an intermediate language designed as a medium-level internal\nrepresentation of programs of the interactive music system Antescofo. This\nrepresentation is independent both of the Antescofo source language and of the\narchitecture of the execution platform. It is used in tasks such as\nverification of timings, model-based conformance testing, static control-flow\nanalysis or simulation. This language is essentially a flat representation of\nAntescofo's code, as a finite state machine extended with local and global\nvariables, with delays and with concurrent threads creation. It features a\nsmall number of simple instructions which are either blocking (wait for\nexternal event, signal or duration) or not (variable assignment, message\nemission and control).\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 12:30:36 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Jacquemard", "Florent", "", "Inria Paris-Rocquencourt, STMS"], ["Sanchez", "Cl\u00e9ment Poncelet", "", "Inria Paris-Rocquencourt, STMS"]]}]