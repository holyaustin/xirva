[{"id": "1509.00040", "submitter": "Kentaro Sano", "authors": "Kentaro Sano", "title": "DSL-based Design Space Exploration for Temporal and Spatial Parallelism\n  of Custom Stream Computing", "comments": "Presented at Second International Workshop on FPGAs for Software\n  Programmers (FSP 2015) (arXiv:1508.06320)", "journal-ref": null, "doi": null, "report-no": "FSP/2015/06", "categories": "cs.AR cs.CE cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream computation is one of the approaches suitable for FPGA-based custom\ncomputing due to its high throughput capability brought by pipelining with\nregular memory access. To increase performance of iterative stream computation,\nwe can exploit both temporal and spatial parallelism by deepening and\nduplicating pipelines, respectively. However, the performance is constrained by\nseveral factors including available hardware resources on FPGA, an external\nmemory bandwidth, and utilization of pipeline stages, and therefore we need to\nfind the best mix of the different parallelism to achieve the highest\nperformance per power. In this paper, we present a domain-specific language\n(DSL) based design space exploration for temporally and/or spatially parallel\nstream computation with FPGA. We define a DSL where we can easily design a\nhierarchical structure of parallel stream computation with abstract description\nof computation. For iterative stream computation of fluid dynamics simulation,\nwe design hardware structures with a different mix of the temporal and spatial\nparallelism. By measuring the performance and the power consumption, we find\nthe best among them.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 12:23:57 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Sano", "Kentaro", ""]]}, {"id": "1509.00413", "submitter": "Amey Karkare", "authors": "Aditya Desai, Sumit Gulwani, Vineet Hingorani, Nidhi Jain, Amey\n  Karkare, Mark Marron, Sailesh R, Subhajit Roy", "title": "Program Synthesis using Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interacting with computers is a ubiquitous activity for millions of people.\nRepetitive or specialized tasks often require creation of small, often one-off,\nprograms. End-users struggle with learning and using the myriad of\ndomain-specific languages (DSLs) to effectively accomplish these tasks.\n  We present a general framework for constructing program synthesizers that\ntake natural language (NL) inputs and produce expressions in a target DSL. The\nframework takes as input a DSL definition and training data consisting of\nNL/DSL pairs. From these it constructs a synthesizer by learning optimal\nweights and classifiers (using NLP features) that rank the outputs of a\nkeyword-programming based translation. We applied our framework to three\ndomains: repetitive text editing, an intelligent tutoring system, and flight\ninformation queries. On 1200+ English descriptions, the respective synthesizers\nrank the desired program as the top-1 and top-3 for 80% and 90% descriptions\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 17:42:49 GMT"}], "update_date": "2015-09-02", "authors_parsed": [["Desai", "Aditya", ""], ["Gulwani", "Sumit", ""], ["Hingorani", "Vineet", ""], ["Jain", "Nidhi", ""], ["Karkare", "Amey", ""], ["Marron", "Mark", ""], ["R", "Sailesh", ""], ["Roy", "Subhajit", ""]]}, {"id": "1509.00996", "submitter": "Pablo Barenbaum", "authors": "Beniamino Accattoli, Pablo Barenbaum, Damiano Mazza", "title": "A Strong Distillery", "comments": "Accepted at APLAS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract machines for the strong evaluation of lambda-terms (that is, under\nabstractions) are a mostly neglected topic, despite their use in the\nimplementation of proof assistants and higher-order logic programming\nlanguages. This paper introduces a machine for the simplest form of strong\nevaluation, leftmost-outermost (call-by-name) evaluation to normal form,\nproving it correct, complete, and bounding its overhead. Such a machine, deemed\nStrong Milner Abstract Machine, is a variant of the KAM computing normal forms\nand using just one global environment. Its properties are studied via a special\nform of decoding, called a distillation, into the Linear Substitution Calculus,\nneatly reformulating the machine as a standard micro-step strategy for explicit\nsubstitutions, namely linear leftmost-outermost reduction, i.e., the extension\nto normal form of linear head reduction. Additionally, the overhead of the\nmachine is shown to be linear both in the number of steps and in the size of\nthe initial term, validating its design. The study highlights two distinguished\nfeatures of strong machines, namely backtracking phases and their interactions\nwith abstractions and environments.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 09:10:28 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 21:15:59 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Accattoli", "Beniamino", ""], ["Barenbaum", "Pablo", ""], ["Mazza", "Damiano", ""]]}, {"id": "1509.01763", "submitter": "Marina Blanton", "authors": "Yihua Zhang, Marina Blanton, and Ghada Almashaqbeh", "title": "Implementing Support for Pointers to Private Data in a General-Purpose\n  Secure Multi-Party Compiler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent compilers allow a general-purpose program (written in a conventional\nprogramming language) that handles private data to be translated into secure\ndistributed implementation of the corresponding functionality. The resulting\nprogram is then guaranteed to provably protect private data using secure\nmulti-party computation techniques. The goals of such compilers are generality,\nusability, and efficiency, but the complete set of features of a modern\nprogramming language has not been supported to date by the existing compilers.\nIn particular, recent compilers PICCO and the two-party ANSI C compiler strive\nto translate any C program into its secure multi-party implementation, but\ncurrently lack support for pointers and dynamic memory allocation, which are\nimportant components of many C programs. In this work, we mitigate the\nlimitation and add support for pointers to private data and consequently\ndynamic memory allocation to the PICCO compiler, enabling it to handle a more\ndiverse set of programs over private data. Because doing so opens up a new\ndesign space, we investigate the use of pointers to private data (with known as\nwell as private locations stored in them) in programs and report our findings.\nBesides dynamic memory allocation, we examine other important topics associated\nwith common pointer use such as reference by pointer/address, casting, and\nbuilding various data structures in the context of secure multi-party\ncomputation. This results in enabling the compiler to automatically translate a\nuser program that uses pointers to private data into its distributed\nimplementation that provably protects private data throughout the computation.\nWe empirically evaluate the constructions and report on performance of\nrepresentative programs.\n", "versions": [{"version": "v1", "created": "Sun, 6 Sep 2015 03:22:29 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 19:11:29 GMT"}, {"version": "v3", "created": "Tue, 27 Dec 2016 16:53:08 GMT"}, {"version": "v4", "created": "Fri, 16 Jun 2017 18:10:28 GMT"}, {"version": "v5", "created": "Fri, 30 Jun 2017 18:13:37 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Zhang", "Yihua", ""], ["Blanton", "Marina", ""], ["Almashaqbeh", "Ghada", ""]]}, {"id": "1509.02151", "submitter": "Daniel Ritchie", "authors": "Daniel Ritchie, Andreas Stuhlm\\\"uller, Noah D. Goodman", "title": "C3: Lightweight Incrementalized MCMC for Probabilistic Programs using\n  Continuations and Callsite Caching", "comments": "Fix typo in author name", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lightweight, source-to-source transformation approaches to implementing MCMC\nfor probabilistic programming languages are popular for their simplicity,\nsupport of existing deterministic code, and ability to execute on existing fast\nruntimes. However, they are also slow, requiring a complete re-execution of the\nprogram on every Metropolis Hastings proposal. We present a new extension to\nthe lightweight approach, C3, which enables efficient, incrementalized\nre-execution of MH proposals. C3 is based on two core ideas: transforming\nprobabilistic programs into continuation passing style (CPS), and caching the\nresults of function calls. We show that on several common models, C3 reduces\nproposal runtime by 20-100x, in some cases reducing runtime complexity from\nlinear in model size to constant. We also demonstrate nearly an order of\nmagnitude speedup on a complex inverse procedural modeling application.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 19:35:42 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2015 17:53:35 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Ritchie", "Daniel", ""], ["Stuhlm\u00fcller", "Andreas", ""], ["Goodman", "Noah D.", ""]]}, {"id": "1509.02439", "submitter": "Nicolas Laurent", "authors": "Nicolas Laurent, Kim Mens", "title": "Parsing Expression Grammars Made Practical", "comments": "\"Proceedings of the International Conference on Software Language\n  Engineering (SLE 2015)\" - 167-172 (ISBN : 978-1-4503-3686-4)", "journal-ref": null, "doi": "10.1145/2814251.2814265", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parsing Expression Grammars (PEGs) define languages by specifying\nrecursive-descent parser that recognises them. The PEG formalism exhibits\ndesirable properties, such as closure under composition, built-in\ndisambiguation, unification of syntactic and lexical concerns, and closely\nmatching programmer intuition. Unfortunately, state of the art PEG parsers\nstruggle with left-recursive grammar rules, which are not supported by the\noriginal definition of the formalism and can lead to infinite recursion under\nnaive implementations. Likewise, support for associativity and explicit\nprecedence is spotty. To remedy these issues, we introduce Autumn, a general\npurpose PEG library that supports left-recursion, left and right associativity\nand precedence rules, and does so efficiently. Furthermore, we identify infix\nand postfix operators as a major source of inefficiency in left-recursive PEG\nparsers and show how to tackle this problem. We also explore the extensibility\nof the PEG paradigm by showing how one can easily introduce new parsing\noperators and how our parser accommodates custom memoization and error handling\nstrategies. We compare our parser to both state of the art and battle-tested\nPEG and CFG parsers, such as Rats!, Parboiled and ANTLR.\n", "versions": [{"version": "v1", "created": "Tue, 8 Sep 2015 16:43:53 GMT"}, {"version": "v2", "created": "Sat, 17 Sep 2016 17:08:03 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Laurent", "Nicolas", ""], ["Mens", "Kim", ""]]}, {"id": "1509.03013", "submitter": "EPTCS", "authors": "Angelos Charalambidis, Panos Rondogiannis, Ioanna Symeonidou", "title": "Equivalence of two Fixed-Point Semantics for Definitional Higher-Order\n  Logic Programs", "comments": "In Proceedings FICS 2015, arXiv:1509.02826", "journal-ref": "EPTCS 191, 2015, pp. 18-32", "doi": "10.4204/EPTCS.191.4", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two distinct research approaches have been proposed for assigning a purely\nextensional semantics to higher-order logic programming. The former approach\nuses classical domain theoretic tools while the latter builds on a fixed-point\nconstruction defined on a syntactic instantiation of the source program. The\nrelationships between these two approaches had not been investigated until now.\nIn this paper we demonstrate that for a very broad class of programs, namely\nthe class of definitional programs introduced by W. W. Wadge, the two\napproaches coincide (with respect to ground atoms that involve symbols of the\nprogram). On the other hand, we argue that if existential higher-order\nvariables are allowed to appear in the bodies of program rules, the two\napproaches are in general different. The results of the paper contribute to a\nbetter understanding of the semantics of higher-order logic programming.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 05:31:21 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Charalambidis", "Angelos", ""], ["Rondogiannis", "Panos", ""], ["Symeonidou", "Ioanna", ""]]}, {"id": "1509.03014", "submitter": "EPTCS", "authors": "Naohi Eguchi", "title": "Formalizing Termination Proofs under Polynomial Quasi-interpretations", "comments": "In Proceedings FICS 2015, arXiv:1509.02826", "journal-ref": "EPTCS 191, 2015, pp. 33-47", "doi": "10.4204/EPTCS.191.5", "report-no": null, "categories": "cs.LO cs.CC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usual termination proofs for a functional program require to check all the\npossible reduction paths. Due to an exponential gap between the height and size\nof such the reduction tree, no naive formalization of termination proofs yields\na connection to the polynomial complexity of the given program. We solve this\nproblem employing the notion of minimal function graph, a set of pairs of a\nterm and its normal form, which is defined as the least fixed point of a\nmonotone operator. We show that termination proofs for programs reducing under\nlexicographic path orders (LPOs for short) and polynomially quasi-interpretable\ncan be optimally performed in a weak fragment of Peano arithmetic. This yields\nan alternative proof of the fact that every function computed by an\nLPO-terminating, polynomially quasi-interpretable program is computable in\npolynomial space. The formalization is indeed optimal since every\npolynomial-space computable function can be computed by such a program. The\ncrucial observation is that inductive definitions of minimal function graphs\nunder LPO-terminating programs can be approximated with transfinite induction\nalong LPOs.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 05:31:31 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Eguchi", "Naohi", ""]]}, {"id": "1509.03020", "submitter": "EPTCS", "authors": "Etienne Lozes (ENS Cachan, CNRS)", "title": "A Type-Directed Negation Elimination", "comments": "In Proceedings FICS 2015, arXiv:1509.02826", "journal-ref": "EPTCS 191, 2015, pp. 132-142", "doi": "10.4204/EPTCS.191.12", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the modal mu-calculus, a formula is well-formed if each recursive variable\noccurs underneath an even number of negations. By means of De Morgan's laws, it\nis easy to transform any well-formed formula into an equivalent formula without\nnegations -- its negation normal form. Moreover, if the formula is of size n,\nits negation normal form of is of the same size O(n). The full modal\nmu-calculus and the negation normal form fragment are thus equally expressive\nand concise.\n  In this paper we extend this result to the higher-order modal fixed point\nlogic (HFL), an extension of the modal mu-calculus with higher-order recursive\npredicate transformers. We present a procedure that converts a formula into an\nequivalent formula without negations of quadratic size in the worst case and of\nlinear size when the number of variables of the formula is fixed.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 05:32:29 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Lozes", "Etienne", "", "ENS Cachan, CNRS"]]}, {"id": "1509.03339", "submitter": "Robbert Krebbers", "authors": "Robbert Krebbers", "title": "A Formal C Memory Model for Separation Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The core of a formal semantics of an imperative programming language is a\nmemory model that describes the behavior of operations on the memory. Defining\na memory model that matches the description of C in the C11 standard is\nchallenging because C allows both high-level (by means of typed expressions)\nand low-level (by means of bit manipulation) memory accesses. The C11 standard\nhas restricted the interaction between these two levels to make more effective\ncompiler optimizations possible, on the expense of making the memory model\ncomplicated.\n  We describe a formal memory model of the (non-concurrent part of the) C11\nstandard that incorporates these restrictions, and at the same time describes\nlow-level memory operations. This formal memory model includes a rich\npermission model to make it usable in separation logic and supports reasoning\nabout program transformations. The memory model and essential properties of it\nhave been fully formalized using the Coq proof assistant.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 21:16:41 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Krebbers", "Robbert", ""]]}, {"id": "1509.03424", "submitter": "George Karpenkov", "authors": "George Karpenkov, David Monniaux and Philipp Wendler", "title": "Program Analysis with Local Policy Iteration", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-662-49122-5_6", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for deriving numerical invariants that combines\nthe precision of max-policy iteration with the flexibility and scalability of\nconventional Kleene iterations. It is defined in the Configurable Program\nAnalysis (CPA) framework, thus allowing inter-analysis communication.\n  It uses adjustable-block encoding in order to traverse loop-free program\nsections, possibly containing branching, without introducing extra abstraction.\nOur technique operates over any template linear constraint domain, including\nthe interval and octagon domains; templates can also be derived from the\nprogram source.\n  The implementation is evaluated on a set of benchmarks from the Software\nVerification Competition (SV-Comp). It competes favorably with state-of-the-art\nanalyzers.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 09:02:31 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 15:39:54 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2015 10:54:03 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Karpenkov", "George", ""], ["Monniaux", "David", ""], ["Wendler", "Philipp", ""]]}, {"id": "1509.03476", "submitter": "Justin Hsu", "authors": "Gilles Barthe, Thomas Espitau, Benjamin Gr\\'egoire, Justin Hsu, L\\'eo\n  Stefanesco, Pierre-Yves Strub", "title": "Relational reasoning via probabilistic coupling", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-662-48899-7_27", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic coupling is a powerful tool for analyzing pairs of\nprobabilistic processes. Roughly, coupling two processes requires finding an\nappropriate witness process that models both processes in the same probability\nspace. Couplings are powerful tools proving properties about the relation\nbetween two processes, include reasoning about convergence of distributions and\nstochastic dominance---a probabilistic version of a monotonicity property.\n  While the mathematical definition of coupling looks rather complex and\ncumbersome to manipulate, we show that the relational program logic pRHL---the\nlogic underlying the EasyCrypt cryptographic proof assistant---already\ninternalizes a generalization of probabilistic coupling. With this insight,\nconstructing couplings is no harder than constructing logical proofs. We\ndemonstrate how to express and verify classic examples of couplings in pRHL,\nand we mechanically verify several couplings in EasyCrypt.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 12:29:04 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2015 21:26:37 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Barthe", "Gilles", ""], ["Espitau", "Thomas", ""], ["Gr\u00e9goire", "Benjamin", ""], ["Hsu", "Justin", ""], ["Stefanesco", "L\u00e9o", ""], ["Strub", "Pierre-Yves", ""]]}, {"id": "1509.03705", "submitter": "Yuting Wang", "authors": "Yuting Wang and Gopalan Nadathur", "title": "A Higher-Order Abstract Syntax Approach to Verified Transformations on\n  Functional Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an approach to the verified implementation of transformations on\nfunctional programs that exploits the higher-order representation of syntax. In\nthis approach, transformations are specified using the logic of hereditary\nHarrop formulas. On the one hand, these specifications serve directly as\nimplementations, being programs in the language Lambda Prolog. On the other\nhand, they can be used as input to the Abella system which allows us to prove\nproperties about them and thereby about the implementations. We argue that this\napproach is especially effective in realizing transformations that analyze\nbinding structure. We do this by describing concise encodings in Lambda Prolog\nfor transformations like typed closure conversion and code hoisting that are\nsensitive to such structure and by showing how to prove their correctness using\nAbella.\n", "versions": [{"version": "v1", "created": "Sat, 12 Sep 2015 05:22:52 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 21:48:25 GMT"}, {"version": "v3", "created": "Sat, 23 Jan 2016 21:12:03 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Wang", "Yuting", ""], ["Nadathur", "Gopalan", ""]]}, {"id": "1509.04040", "submitter": "J{\\o}rgen Steensgaard-Madsen (Retired)", "authors": "J{\\o}rgen Steensgaard-Madsen", "title": "Programs as proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Curry-Howard correspondence is about a relationship between types and\nprograms on the one hand and propositions and proofs on the other. The\nimplications for programming language design and program verification is an\nactive field of research.\n  Transformer-like semantics of internal definitions that combine a defining\ncomputation and an application will be presented. By specialisation for a given\ndefining computation one can derive inference rules for applications of defined\noperations.\n  With semantics of that kind for every operation, each application identifies\nan axiom in a logic defined by the programming language, so a language can be\nconsidered a theory.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 11:37:06 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Steensgaard-Madsen", "J\u00f8rgen", ""]]}, {"id": "1509.04207", "submitter": "Lse Lse", "authors": "Mart\\'in Dias (RMOD), Guillermo Polito (RMOD), Damien Cassou (RMOD),\n  St\\'ephane Ducasse (RMOD)", "title": "DeltaImpactFinder: Assessing Semantic Merge Conflicts with Dependency\n  Analysis", "comments": "International Workshop on Smalltalk Technologies 2015, Jul 2015,\n  Brescia, Italy", "journal-ref": null, "doi": "10.1145/2811237.2811299", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In software development, version control systems (VCS) provide branching and\nmerging support tools. Such tools are popular among developers to concurrently\nchange a code-base in separate lines and reconcile their changes automatically\nafterwards. However, two changes that are correct independently can introduce\nbugs when merged together. We call semantic merge conflicts this kind of bugs.\nChange impact analysis (CIA) aims at estimating the effects of a change in a\ncodebase. In this paper, we propose to detect semantic merge conflicts using\nCIA. On a merge, DELTAIMPACTFINDER analyzes and compares the impact of a change\nin its origin and destination branches. We call the difference between these\ntwo impacts the delta-impact. If the delta-impact is empty, then there is no\nindicator of a semantic merge conflict and the merge can continue\nautomatically. Otherwise, the delta-impact contains what are the sources of\npossible conflicts.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 17:10:45 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Dias", "Mart\u00edn", "", "RMOD"], ["Polito", "Guillermo", "", "RMOD"], ["Cassou", "Damien", "", "RMOD"], ["Ducasse", "St\u00e9phane", "", "RMOD"]]}, {"id": "1509.04315", "submitter": "Robert Webb", "authors": "Robert Webb", "title": "Implementing a teleo-reactive programming system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This thesis explores the teleo-reactive programming paradigm for controlling\nautonomous agents, such as robots. Teleo-reactive programming provides a\nrobust, opportunistic method for goal-directed programming that continuously\nreacts to the sensed environment. In particular, the TR and TeleoR systems are\ninvestigated. They influence the design of a teleo-reactive system programming\nin Python, for controlling autonomous agents via the Pedro communications\narchitecture. To demonstrate the system, it is used as a controller in a simple\ngame.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 20:46:49 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Webb", "Robert", ""]]}, {"id": "1509.05100", "submitter": "Arjun Guha", "authors": "Rian Shambaugh, Aaron Weiss and Arjun Guha", "title": "Rehearsal: A Configuration Verification Tool for Puppet", "comments": "In proceedings of ACM SIGPLAN Conference on Programming Language\n  Design and Implementation (PLDI) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale data centers and cloud computing have turned system configuration\ninto a challenging problem. Several widely-publicized outages have been blamed\nnot on software bugs, but on configuration bugs. To cope, thousands of\norganizations use system configuration languages to manage their computing\ninfrastructure. Of these, Puppet is the most widely used with thousands of\npaying customers and many more open-source users. The heart of Puppet is a\ndomain-specific language that describes the state of a system. Puppet already\nperforms some basic static checks, but they only prevent a narrow range of\nerrors. Furthermore, testing is ineffective because many errors are only\ntriggered under specific machine states that are difficult to predict and\nreproduce. With several examples, we show that a key problem with Puppet is\nthat configurations can be non-deterministic.\n  This paper presents Rehearsal, a verification tool for Puppet configurations.\nRehearsal implements a sound, complete, and scalable determinacy analysis for\nPuppet. To develop it, we (1) present a formal semantics for Puppet, (2) use\nseveral analyses to shrink our models to a tractable size, and (3) frame\ndeterminism-checking as decidable formulas for an SMT solver. Rehearsal then\nleverages the determinacy analysis to check other important properties, such as\nidempotency. Finally, we apply Rehearsal to several real-world Puppet\nconfigurations.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 01:53:49 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2015 18:35:19 GMT"}, {"version": "v3", "created": "Tue, 17 May 2016 14:30:48 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Shambaugh", "Rian", ""], ["Weiss", "Aaron", ""], ["Guha", "Arjun", ""]]}, {"id": "1509.05659", "submitter": "Ferruccio Damiani", "authors": "Ferruccio Damiani (University of Torino), Mirko Viroli (University of\n  Bologna)", "title": "Type-based Self-stabilisation for Computational Fields", "comments": "Logical Methods in Computer Science accepted paper, 53 pages", "journal-ref": "Logical Methods in Computer Science, Volume 11, Issue 4 (December\n  31, 2015) lmcs:1622", "doi": "10.2168/LMCS-11(4:21)2015", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging network scenarios require the development of solid large-scale\nsituated systems. Unfortunately, the diffusion/aggregation computational\nprocesses therein often introduce a source of complexity that hampers\npredictability of the overall system behaviour. Computational fields have been\nintroduced to help engineering such systems: they are spatially distributed\ndata structures designed to adapt their shape to the topology of the underlying\n(mobile) network and to the events occurring in it, with notable applications\nto pervasive computing, sensor networks, and mobile robots. To assure\nbehavioural correctness, namely, correspondence of micro-level specification\n(single device behaviour) with macro-level behaviour (resulting global spatial\npattern), we investigate the issue of self-stabilisation for computational\nfields. We present a tiny, expressive, and type-sound calculus of computational\nfields, and define sufficient conditions for self-stabilisation, defined as the\nability to react to changes in the environment finding a new stable state in\nfinite time. A type-based approach is used to provide a correct checking\nprocedure for self-stabilisation.\n", "versions": [{"version": "v1", "created": "Fri, 18 Sep 2015 15:19:59 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2015 21:43:19 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Damiani", "Ferruccio", "", "University of Torino"], ["Viroli", "Mirko", "", "University of\n  Bologna"]]}, {"id": "1509.06079", "submitter": "EPTCS", "authors": "Sol Swords (Centaur Technology, Inc.), Jared Davis (Centaur\n  Technology, Inc.)", "title": "Fix Your Types", "comments": "In Proceedings ACL2 2015, arXiv:1509.05526", "journal-ref": "EPTCS 192, 2015, pp. 3-16", "doi": "10.4204/EPTCS.192.2", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When using existing ACL2 datatype frameworks, many theorems require type\nhypotheses. These hypotheses slow down the theorem prover, are tedious to\nwrite, and are easy to forget. We describe a principled approach to types that\nprovides strong type safety and execution efficiency while avoiding type\nhypotheses, and we present a library that automates this approach. Using this\napproach, types help you catch programming errors and then get out of the way\nof theorem proving.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 00:34:37 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Swords", "Sol", "", "Centaur Technology, Inc."], ["Davis", "Jared", "", "Centaur\n  Technology, Inc."]]}, {"id": "1509.06082", "submitter": "EPTCS", "authors": "Yan Peng (University of British Columbia), Mark Greenstreet\n  (University of British Columbia)", "title": "Extending ACL2 with SMT Solvers", "comments": "In Proceedings ACL2 2015, arXiv:1509.05526", "journal-ref": "EPTCS 192, 2015, pp. 61-77", "doi": "10.4204/EPTCS.192.6", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our extension of ACL2 with Satisfiability Modulo Theories (SMT)\nsolvers using ACL2's trusted clause processor mechanism. We are particularly\ninterested in the verification of physical systems including Analog and\nMixed-Signal (AMS) designs. ACL2 offers strong induction abilities for\nreasoning about sequences and SMT complements deduction methods like ACL2 with\nfast nonlinear arithmetic solving procedures. While SAT solvers have been\nintegrated into ACL2 in previous work, SMT methods raise new issues because of\ntheir support for a broader range of domains including real numbers and\nuninterpreted functions. This paper presents Smtlink, our clause processor for\nintegrating SMT solvers into ACL2. We describe key design and implementation\nissues and describe our experience with its use.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 00:35:21 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Peng", "Yan", "", "University of British Columbia"], ["Greenstreet", "Mark", "", "University of British Columbia"]]}, {"id": "1509.06083", "submitter": "EPTCS", "authors": "David S. Hardin (Rockwell Collins)", "title": "Reasoning About LLVM Code Using Codewalker", "comments": "In Proceedings ACL2 2015, arXiv:1509.05526", "journal-ref": "EPTCS 192, 2015, pp. 79-92", "doi": "10.4204/EPTCS.192.7", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on initial experiments using J Moore's Codewalker to\nreason about programs compiled to the Low-Level Virtual Machine (LLVM)\nintermediate form. Previously, we reported on a translator from LLVM to the\napplicative subset of Common Lisp accepted by the ACL2 theorem prover,\nproducing executable ACL2 formal models, and allowing us to both prove theorems\nabout the translated models as well as validate those models by testing. That\ntranslator provided many of the benefits of a pure decompilation into logic\napproach, but had the disadvantage of not being verified. The availability of\nCodewalker as of ACL2 7.0 has provided an opportunity to revisit this idea, and\nemploy a more trustworthy decompilation into logic tool. Thus, we have employed\nthe Codewalker method to create an interpreter for a subset of the LLVM\ninstruction set, and have used Codewalker to analyze some simple array-based C\nprograms compiled to LLVM form. We discuss advantages and limitations of the\nCodewalker-based method compared to the previous method, and provide some\nchallenge problems for future Codewalker development.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 00:35:28 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Hardin", "David S.", "", "Rockwell Collins"]]}, {"id": "1509.06085", "submitter": "EPTCS", "authors": "Mitesh Jain (Northeastern University), Panagiotis Manolios\n  (Northeastern University)", "title": "Proving Skipping Refinement with ACL2s", "comments": "In Proceedings ACL2 2015, arXiv:1509.05526. arXiv admin note: text\n  overlap with arXiv:1502.02942", "journal-ref": "EPTCS 192, 2015, pp. 111-127", "doi": "10.4204/EPTCS.192.9", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe three case studies illustrating the use of ACL2s to prove the\ncorrectness of optimized reactive systems using skipping refinement. Reasoning\nabout reactive systems using refinement involves defining an abstract,\nhigh-level specification system and a concrete, low-level system. Next, one\nshows that the behaviors of the implementation system are allowed by the\nspecification system. Skipping refinement allows us to reason about\nimplementation systems that can \"skip\" specification states due to\noptimizations that allow the implementation system to take several\nspecification steps at once. Skipping refinement also allows implementation\nsystems to, i.e., to take several steps before completing a specification step.\nWe show how ACL2s can be used to prove skipping refinement theorems by modeling\nand proving the correctness of three systems: a JVM-inspired stack machine, a\nsimple memory controller, and a scalar to vector compiler transformation.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 00:35:48 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Jain", "Mitesh", "", "Northeastern University"], ["Manolios", "Panagiotis", "", "Northeastern University"]]}, {"id": "1509.06220", "submitter": "Ilya Sergey", "authors": "Ilya Sergey, Aleksandar Nanevski, Anindya Banerjee, German Andres\n  Delbianco", "title": "Hoare-style Specifications as Correctness Conditions for\n  Non-linearizable Concurrent Objects", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing scalable concurrent objects, which can be efficiently used on\nmulticore processors, often requires one to abandon standard specification\ntechniques, such as linearizability, in favor of more relaxed consistency\nrequirements. However, the variety of alternative correctness conditions makes\nit difficult to choose which one to employ in a particular case, and to compose\nthem when using objects whose behaviors are specified via different criteria.\nThe lack of syntactic verification methods for most of these criteria poses\nchallenges in their systematic adoption and application.\n  In this paper, we argue for using Hoare-style program logics as an\nalternative and uniform approach for specification and compositional formal\nverification of safety properties for concurrent objects and their client\nprograms. Through a series of case studies, we demonstrate how an existing\nprogram logic for concurrency can be employed off-the-shelf to capture\nimportant state and history invariants, allowing one to explicitly quantify\nover interference of environment threads and provide intuitive and expressive\nHoare-style specifications for several non-linearizable concurrent objects that\nwere previously specified only via dedicated correctness criteria. We\nillustrate the adequacy of our specifications by verifying a number of\nconcurrent client scenarios, that make use of the previously specified\nconcurrent objects, capturing the essence of such correctness conditions as\nconcurrency-aware linearizability, quiescent, and quantitative quiescent\nconsistency. All examples described in this paper are verified mechanically in\nCoq.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 13:33:33 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2016 13:23:15 GMT"}, {"version": "v3", "created": "Thu, 21 Jul 2016 12:37:44 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Sergey", "Ilya", ""], ["Nanevski", "Aleksandar", ""], ["Banerjee", "Anindya", ""], ["Delbianco", "German Andres", ""]]}, {"id": "1509.06503", "submitter": "Catalin Hritcu", "authors": "Arthur Azevedo de Amorim, Nathan Collins, Andr\\'e DeHon, Delphine\n  Demange, Catalin Hritcu, David Pichardie, Benjamin C. Pierce, Randy Pollack,\n  Andrew Tolmach", "title": "A Verified Information-Flow Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SAFE is a clean-slate design for a highly secure computer system, with\npervasive mechanisms for tracking and limiting information flows. At the lowest\nlevel, the SAFE hardware supports fine-grained programmable tags, with\nefficient and flexible propagation and combination of tags as instructions are\nexecuted. The operating system virtualizes these generic facilities to present\nan information-flow abstract machine that allows user programs to label\nsensitive data with rich confidentiality policies. We present a formal,\nmachine-checked model of the key hardware and software mechanisms used to\ndynamically control information flow in SAFE and an end-to-end proof of\nnoninterference for this model.\n  We use a refinement proof methodology to propagate the noninterference\nproperty of the abstract machine down to the concrete machine level. We use an\nintermediate layer in the refinement chain that factors out the details of the\ninformation-flow control policy and devise a code generator for compiling such\ninformation-flow policies into low-level monitor code. Finally, we verify the\ncorrectness of this generator using a dedicated Hoare logic that abstracts from\nlow-level machine instructions into a reusable set of verified structured code\ngenerators.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 08:38:20 GMT"}, {"version": "v2", "created": "Sun, 6 Mar 2016 10:36:23 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["de Amorim", "Arthur Azevedo", ""], ["Collins", "Nathan", ""], ["DeHon", "Andr\u00e9", ""], ["Demange", "Delphine", ""], ["Hritcu", "Catalin", ""], ["Pichardie", "David", ""], ["Pierce", "Benjamin C.", ""], ["Pollack", "Randy", ""], ["Tolmach", "Andrew", ""]]}, {"id": "1509.07036", "submitter": "David Rogers", "authors": "David M. Rogers", "title": "Towards a Direct, By-Need Evaluator for Dependently Typed Languages", "comments": "Submitted Version, 8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a C-language implementation of the lambda-pi calculus by extending\nthe (call-by-need) stack machine of Ariola, Chang and Felleisen to hold types,\nusing a typeless- tagless- final interpreter strategy. It has the advantage of\nexpressing all operations as folds over terms, including by-need evaluation,\nrecovery of the initial syntax-tree encoding for any term, and eliminating most\ngarbage-collection tasks. These are made possible by a disciplined approach to\nhandling the spine of each term, along with a robust stack-based API. Type\ninference is not covered in this work, but also derives several advantages from\nthe present stack transformation. Timing and maximum stack space usage results\nfor executing benchmark problems are presented. We discuss how the design\nchoices for this interpreter allow the language to be used as a high-level\nscripting language for automatic distributed parallel execution of common\nscientific computing workflows.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 15:41:56 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Rogers", "David M.", ""]]}, {"id": "1509.07238", "submitter": "David Pritchard", "authors": "David Pritchard", "title": "Frequency Distribution of Error Messages", "comments": "To appear at PLATEAU 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CY cs.PL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Which programming error messages are the most common? We investigate this\nquestion, motivated by writing error explanations for novices. We consider\nlarge data sets in Python and Java that include both syntax and run-time\nerrors. In both data sets, after grouping essentially identical messages, the\nerror message frequencies empirically resemble Zipf-Mandelbrot distributions.\nWe use a maximum-likelihood approach to fit the distribution parameters. This\ngives one possible way to contrast languages or compilers quantitatively.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 05:22:59 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Pritchard", "David", ""]]}, {"id": "1509.07326", "submitter": "Pascal Potvin", "authors": "Pascal Potvin, Mario Bonja, Gordon Bailey, Pierre Busnel", "title": "An IMS DSL Developed at Ericsson", "comments": "19 pages, 2 figures, 1 table, oral presentation at SDL 2013:\n  Model-Driven Dependability Engineering conference", "journal-ref": "SDL 2013: Model-Driven Dependability Engineering Volume 7916 of\n  the series Lecture Notes in Computer Science pp 144-162", "doi": "10.1007/978-3-642-38911-5", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present how we created a Domain Specific Language (DSL)\ndedicated to IP Multimedia Subsystem (IMS) at Ericsson. First, we introduce IMS\nand how developers are burdened by its complexity when integrating it in their\napplication. Then we describe the principles we followed to create our new IMS\nDSL from its core in the Scala language to its syntax. We then present how we\nintegrated it in two existing projects and show how it can save time for\ndevelopers and how readable the syntax of the IMS DSL is.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 11:46:20 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Potvin", "Pascal", ""], ["Bonja", "Mario", ""], ["Bailey", "Gordon", ""], ["Busnel", "Pierre", ""]]}, {"id": "1509.08068", "submitter": "Abhinav Jangda", "authors": "Abhinav Jangda", "title": "Block-Level Parallelism in Parsing Block Structured Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Softwares source code is becoming large and complex. Compilation of large\nbase code is a time consuming process. Parallel compilation of code will help\nin reducing the time complexity. Parsing is one of the phases in compiler in\nwhich significant amount of time of compilation is spent. Techniques have\nalready been developed to extract the parallelism available in parser. Current\nLR(k) parallel parsing techniques either face difficulty in creating Abstract\nSyntax Tree or requires modification in the grammar or are specific to less\nexpressive grammars. Most of the programming languages like C, ALGOL are\nblock-structured, and in most languages grammars the grammar of different\nblocks is independent, allowing different blocks to be parsed in parallel. We\nare proposing a block level parallel parser derived from Incremental Jump Shift\nReduce Parser by [13]. Block Parallelized Parser (BPP) can even work as a block\nparallel incremental parser. We define a set of Incremental Categories and\ncreate the partitions of a grammar based on a rule. When parser reaches the\nstart of the block symbol it will check whether the current block is related to\nany incremental category. If block parallel parser find the incremental\ncategory for it, parser will parse the block in parallel. Block parallel parser\nis developed for LR(1) grammar. Without making major changes in Shift Reduce\n(SR) LR(1) parsing algorithm, block parallel parser can create an Abstract\nSyntax tree easily. We believe this parser can be easily extended to LR (k)\ngrammars and also be converted to an LALR (1) parser. We implemented BPP and SR\nLR(1) parsing algorithm for C Programming Language. We evaluated performance of\nboth techniques by parsing 10 random files from Linux Kernel source. BPP showed\n28% and 52% improvement in the case of including header files and excluding\nheader files respectively.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 08:38:37 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Jangda", "Abhinav", ""]]}, {"id": "1509.08169", "submitter": "EPTCS", "authors": "Nathalie Bertrand (Inria Rennes, France), Mirco Tribastone (IMT -\n  Institute for Advanced Studies Lucca, Italy)", "title": "Proceedings Thirteenth Workshop on Quantitative Aspects of Programming\n  Languages and Systems", "comments": null, "journal-ref": "EPTCS 194, 2015", "doi": "10.4204/EPTCS.194", "report-no": null, "categories": "cs.LO cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of the Thirteenth Workshop on\nQuantitative Aspects of Programming Languages and Systems (QAPL 2015), held in\nLondon, UK, on 11 and 12 April, 2015. QAPL 2015 was a satellite event of the\nEuropean Joint Conferences on Theory and Practice of Software (ETAPS) focussing\non quantitative aspects of computation. The Program Committee of QAPL 2015\nselected 8 regular papers and 2 presentation-only papers. The workshop\nprogramme included two QAPL keynote presentations by Catuscia Palamidessi\n(Inria/LIX, France) on \"Quantitative Aspects of Privacy and Information Flow,\"\nand Holger Hermanns (Saarland University, Germany) on \"Optimal Continuous Time\nMarkov Decisions.\"\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 01:23:36 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Bertrand", "Nathalie", "", "Inria Rennes, France"], ["Tribastone", "Mirco", "", "IMT -\n  Institute for Advanced Studies Lucca, Italy"]]}, {"id": "1509.08560", "submitter": "EPTCS", "authors": "Luca Bortolussi (Saarland University, University of Trieste,\n  ISTI-CNR), Rocco De Nicola (IMT Lucca), Vashti Galpin (University of\n  Edinburgh), Stephen Gilmore (University of Edinburgh), Jane Hillston\n  (University of Edinburgh), Diego Latella (ISTI-CNR), Michele Loreti\n  (Universit\\`a di Firenze, IMT Lucca), Mieke Massink (ISTI-CNR)", "title": "CARMA: Collective Adaptive Resource-sharing Markovian Agents", "comments": "In Proceedings QAPL 2015, arXiv:1509.08169", "journal-ref": "EPTCS 194, 2015, pp. 16-31", "doi": "10.4204/EPTCS.194.2", "report-no": null, "categories": "cs.PL cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present CARMA, a language recently defined to support\nspecification and analysis of collective adaptive systems. CARMA is a\nstochastic process algebra equipped with linguistic constructs specifically\ndeveloped for modelling and programming systems that can operate in open-ended\nand unpredictable environments. This class of systems is typically composed of\na huge number of interacting agents that dynamically adjust and combine their\nbehaviour to achieve specific goals. A CARMA model, termed a collective,\nconsists of a set of components, each of which exhibits a set of attributes. To\nmodel dynamic aggregations, which are sometimes referred to as ensembles, CARMA\nprovides communication primitives that are based on predicates over the\nexhibited attributes. These predicates are used to select the participants in a\ncommunication. Two communication mechanisms are provided in the CARMA language:\nmulticast-based and unicast-based. In this paper, we first introduce the basic\nprinciples of CARMA and then we show how our language can be used to support\nspecification with a simple but illustrative example of a socio-technical\ncollective adaptive system.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 02:10:19 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Bortolussi", "Luca", "", "Saarland University, University of Trieste,\n  ISTI-CNR"], ["De Nicola", "Rocco", "", "IMT Lucca"], ["Galpin", "Vashti", "", "University of\n  Edinburgh"], ["Gilmore", "Stephen", "", "University of Edinburgh"], ["Hillston", "Jane", "", "University of Edinburgh"], ["Latella", "Diego", "", "ISTI-CNR"], ["Loreti", "Michele", "", "Universit\u00e0 di Firenze, IMT Lucca"], ["Massink", "Mieke", "", "ISTI-CNR"]]}, {"id": "1509.08562", "submitter": "EPTCS", "authors": "Yusuke Kawamoto, Thomas Given-Wilson", "title": "Quantitative Information Flow for Scheduler-Dependent Systems", "comments": "In Proceedings QAPL 2015, arXiv:1509.08169", "journal-ref": "EPTCS 194, 2015, pp. 48-62", "doi": "10.4204/EPTCS.194.4", "report-no": null, "categories": "cs.CR cs.IT cs.PL math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative information flow analyses measure how much information on\nsecrets is leaked by publicly observable outputs. One area of interest is to\nquantify and estimate the information leakage of composed systems. Prior work\nhas focused on running disjoint component systems in parallel and reasoning\nabout the leakage compositionally, but has not explored how the component\nsystems are run in parallel or how the leakage of composed systems can be\nminimised. In this paper we consider the manner in which parallel systems can\nbe combined or scheduled. This considers the effects of scheduling channels\nwhere resources may be shared, or whether the outputs may be incrementally\nobserved. We also generalise the attacker's capability, of observing outputs of\nthe system, to consider attackers who may be imperfect in their observations,\ne.g. when outputs may be confused with one another, or when assessing the time\ntaken for an output to appear. Our main contribution is to present how\nscheduling and observation effect information leakage properties. In\nparticular, that scheduling can hide some leaked information from perfect\nobservers, while some scheduling may reveal secret information that is hidden\nto imperfect observers. In addition we present an algorithm to construct a\nscheduler that minimises the min-entropy leakage and min-capacity in the\npresence of any observer.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 02:10:40 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Kawamoto", "Yusuke", ""], ["Given-Wilson", "Thomas", ""]]}, {"id": "1509.08565", "submitter": "EPTCS", "authors": "Fabio Martinelli (IIT-CNR), Ilaria Matteucci (IIT-CNR), Francesco\n  Santini (IIT-CNR)", "title": "Semiring-based Specification Approaches for Quantitative Security", "comments": "In Proceedings QAPL 2015, arXiv:1509.08169", "journal-ref": "EPTCS 194, 2015, pp. 95-109", "doi": "10.4204/EPTCS.194.7", "report-no": null, "categories": "cs.LO cs.CR cs.FL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to provide different semiring-based formal tools for the\nspecification of security requirements: we quantitatively enhance the\nopen-system approach, according to which a system is partially specified.\nTherefore, we suppose the existence of an unknown and possibly malicious agent\nthat interacts in parallel with the system. Two specification frameworks are\ndesigned along two different (but still related) lines. First, by comparing the\nbehaviour of a system with the expected one, or by checking if such system\nsatisfies some security requirements: we investigate a novel approximate\nbehavioural-equivalence for comparing processes behaviour, thus extending the\nGeneralised Non Deducibility on Composition (GNDC) approach with scores. As a\nsecond result, we equip a modal logic with semiring values with the purpose to\nhave a weight related to the satisfaction of a formula that specifies some\nrequested property. Finally, we generalise the classical partial model-checking\nfunction, and we name it as quantitative partial model-checking in such a way\nto point out the necessary and sufficient conditions that a system has to\nsatisfy in order to be considered as secure, with respect to a fixed\nsecurity/functionality threshold-value.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 02:11:07 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Martinelli", "Fabio", "", "IIT-CNR"], ["Matteucci", "Ilaria", "", "IIT-CNR"], ["Santini", "Francesco", "", "IIT-CNR"]]}, {"id": "1509.08566", "submitter": "EPTCS", "authors": "Mads Rosendahl (Roskilde University, Denmark), Maja H. Kirkeby\n  (Roskilde University, Denmark)", "title": "Probabilistic Output Analysis by Program Manipulation", "comments": "In Proceedings QAPL 2015, arXiv:1509.08169", "journal-ref": "EPTCS 194, 2015, pp. 110-124", "doi": "10.4204/EPTCS.194.8", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of a probabilistic output analysis is to derive a probability\ndistribution of possible output values for a program from a probability\ndistribution of its input. We present a method for performing static output\nanalysis, based on program transformation techniques. It generates a\nprobability function as a possibly uncomputable expression in an intermediate\nlanguage. This program is then analyzed, transformed, and approximated. The\nresult is a closed form expression that computes an over approximation of the\noutput probability distribution for the program. We focus on programs where the\npossible input follows a known probability distribution. Tests in programs are\nnot assumed to satisfy the Markov property of having fixed branching\nprobabilities independently of previous history.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 02:11:11 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Rosendahl", "Mads", "", "Roskilde University, Denmark"], ["Kirkeby", "Maja H.", "", "Roskilde University, Denmark"]]}, {"id": "1509.08605", "submitter": "Bj\\\"orn Engelmann", "authors": "Bj\\\"orn Engelmann and Ernst-R\\\"udiger Olderog", "title": "A Sound and Complete Hoare Logic for Dynamically-Typed, Object-Oriented\n  Programs -- Extended Version --", "comments": "Extended Version -- contains all proofs, proof rules and additional\n  information; new version -- elaborated explanations in section 7, added\n  reference, minor visual improvements; new version -- incorporated reviews &\n  improved formalizations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple dynamically-typed, (purely) object-oriented language is defined. A\nstructural operational semantics as well as a Hoare-style program logic for\nreasoning about programs in the language in multiple notions of correctness are\ngiven. The Hoare logic is proved to be both sound and (relative) complete and\nis -- to the best of our knowledge -- the first such logic presented for a\ndynamically-typed language.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 06:38:40 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 22:10:59 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2016 23:06:08 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Engelmann", "Bj\u00f6rn", ""], ["Olderog", "Ernst-R\u00fcdiger", ""]]}, {"id": "1509.09092", "submitter": "David Monniaux", "authors": "David Monniaux (VERIMAG - IMAG), Laure Gonnord (LIP)", "title": "An encoding of array verification problems into array-free Horn clauses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically verifying safety properties of programs is hard, and it is even\nharder if the program acts upon arrays or other forms of maps. Many approaches\nexist for verifying programs operating upon Boolean and integer values (e.g.\nabstract interpretation, counterexample-guided abstraction refinement using\ninterpolants), but transposing them to array properties has been fraught with\ndifficulties.In contrast to most preceding approaches, we do not introduce a\nnew abstract domain or a new interpolation procedure for arrays. Instead, we\ngenerate an abstraction as a scalar problem and feed it to a preexisting\nsolver, with tunable precision.Our transformed problem is expressed using Horn\nclauses, a common format with clear and unambiguous logical semantics for\nverification problems. An important characteristic of our encoding is that it\ncreates a nonlinear Horn problem, with tree unfoldings, even though following\n\"flatly\" the control-graph structure ordinarily yields a linear Horn problem,\nwith linear unfoldings. That is, our encoding cannot be expressed by an\nencoding into another control-flow graph problem, and truly leverages the\ncapacity of the Horn clause format.We illustrate our approach with a completely\nautomated proof of the functional correctness of selection sort.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 09:29:09 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Monniaux", "David", "", "VERIMAG - IMAG"], ["Gonnord", "Laure", "", "LIP"]]}]