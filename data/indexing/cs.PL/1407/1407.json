[{"id": "1407.0292", "submitter": "Ajay Kulkarni", "authors": "Ajay Kulkarni, Saurabh Kulkarni", "title": "An Open Source P2P Encrypted VoIP Application", "comments": "International Journal of Advanced Computer Science and\n  Applications(IJACSA), 5 pages, 6 figures,\n  http://thesai.org/Publications/ViewPaper?Volume=5&Issue=6&Code=IJACSA&SerialNo=1", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications(IJACSA), 5(6), 2014", "doi": "10.14569/IJACSA.2014.050601", "report-no": null, "categories": "cs.PL cs.CR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Open source is the future of technology. This community is growing by the\nday; developing and improving existing frameworks and software for free. Open\nsource replacements are coming up for almost all proprietary software nowadays.\nThis paper proposes an open source application which could replace Skype, a\npopular VoIP soft phone. The performance features of the developed software is\nanalyzed and compared with Skype so that we can conclude that it can be an\nefficient replacement. This application is developed in pure Java using various\nAPIs and package and boasts features like voice calling, chatting, file sharing\netc. The target audience for this software will initially only be organizations\n(for internal communication) and later will be released on a larger scale.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jul 2014 16:01:30 GMT"}], "update_date": "2014-07-02", "authors_parsed": [["Kulkarni", "Ajay", ""], ["Kulkarni", "Saurabh", ""]]}, {"id": "1407.0549", "submitter": "Antonio Barresi", "authors": "Mathias Payer, Antonio Barresi, Thomas R. Gross", "title": "Lockdown: Dynamic Control-Flow Integrity", "comments": "ETH Technical Report", "journal-ref": null, "doi": "10.3929/ethz-a-010171214", "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications written in low-level languages without type or memory safety are\nespecially prone to memory corruption. Attackers gain code execution\ncapabilities through such applications despite all currently deployed defenses\nby exploiting memory corruption vulnerabilities. Control-Flow Integrity (CFI)\nis a promising defense mechanism that restricts open control-flow transfers to\na static set of well-known locations. We present Lockdown, an approach to\ndynamic CFI that protects legacy, binary-only executables and libraries.\nLockdown adaptively learns the control-flow graph of a running process using\ninformation from a trusted dynamic loader. The sandbox component of Lockdown\nrestricts interactions between different shared objects to imported and\nexported functions by enforcing fine-grained CFI checks. Our prototype\nimplementation shows that dynamic CFI results in low performance overhead.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 13:10:37 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Payer", "Mathias", ""], ["Barresi", "Antonio", ""], ["Gross", "Thomas R.", ""]]}, {"id": "1407.0729", "submitter": "Satoshi Egi", "authors": "Satoshi Egi", "title": "Non-Linear Pattern-Matching against Unfree Data Types with Lexical\n  Scoping", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a pattern-matching system that enables non-linear\npattern-matching against unfree data types. The system allows multiple\noccurrences of the same variables in a pattern, multiple results of\npattern-matching and modularization of the way of pattern-matching for each\ndata type at the same time. It enables us to represent pattern-matching against\nnot only algebraic data types but also unfree data types such as sets, graphs\nand any other data types whose data have no canonical form and multiple ways of\ndecomposition. I have realized that with a rule that pattern-matching is\nexecuted from the left side of a pattern and a rule that a binding to a\nvariable in a pattern can be referred to in its right side of the pattern.\nFurthermore, I have realized modularization of these patterns with lexical\nscoping. In my system, a pattern is not a first class object, but a\npattern-function that obtains only patterns and returns a pattern is a first\nclass object. This restriction simplifies the non-linear pattern-matching\nsystem with lexical scoping. I have already implemented the pattern-matching\nsystem in the Egison programming language.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jul 2014 21:42:02 GMT"}, {"version": "v2", "created": "Wed, 9 Jul 2014 01:29:10 GMT"}, {"version": "v3", "created": "Tue, 2 Sep 2014 10:20:54 GMT"}, {"version": "v4", "created": "Thu, 11 Sep 2014 15:07:25 GMT"}, {"version": "v5", "created": "Thu, 16 Oct 2014 14:01:45 GMT"}, {"version": "v6", "created": "Fri, 17 Oct 2014 16:36:46 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Egi", "Satoshi", ""]]}, {"id": "1407.0970", "submitter": "Saverio Giallorenzo", "authors": "Mila Dalla Preda, Maurizio Gabbrielli, Saverio Giallorenzo, Ivan\n  Lanese, and Jacopo Mauro", "title": "Dynamic Choreographies - Safe Runtime Updates of Distributed\n  Applications", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming distributed applications free from communication deadlocks and\nraces is complex. Preserving these properties when applications are updated at\nruntime is even harder. We present DIOC, a language for programming distributed\napplications that are free from deadlocks and races by construction. A DIOC\nprogram describes a whole distributed application as a unique entity\n(choreography). DIOC allows the programmer to specify which parts of the\napplication can be updated. At runtime, these parts may be replaced by new DIOC\nfragments from outside the application. DIOC programs are compiled, generating\ncode for each site, in a lower-level language called DPOC. We formalise both\nDIOC and DPOC semantics as labelled transition systems and prove the\ncorrectness of the compilation as a trace equivalence result. As corollaries,\nDPOC applications are free from communication deadlocks and races, even in\npresence of runtime updates.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 16:12:12 GMT"}, {"version": "v2", "created": "Mon, 19 Jan 2015 08:46:47 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2015 13:07:40 GMT"}, {"version": "v4", "created": "Tue, 31 Mar 2015 06:48:05 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Preda", "Mila Dalla", ""], ["Gabbrielli", "Maurizio", ""], ["Giallorenzo", "Saverio", ""], ["Lanese", "Ivan", ""], ["Mauro", "Jacopo", ""]]}, {"id": "1407.0975", "submitter": "Saverio Giallorenzo", "authors": "Mila Dalla Preda, Saverio Giallorenzo, Ivan Lanese, Jacopo Mauro, and\n  Maurizio Gabbrielli", "title": "AIOCJ: A Choreographic Framework for Safe Adaptive Distributed\n  Applications", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present AIOCJ, a framework for programming distributed adaptive\napplications. Applications are programmed using AIOC, a choreographic language\nsuited for expressing patterns of interaction from a global point of view. AIOC\nallows the programmer to specify which parts of the application can be adapted.\nAdaptation takes place at runtime by means of rules, which can change during\nthe execution to tackle possibly unforeseen adaptation needs. AIOCJ relies on a\nsolid theory that ensures applications to be deadlock-free by construction also\nafter adaptation. We describe the architecture of AIOCJ, the design of the AIOC\nlanguage, and an empirical validation of the framework.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jul 2014 16:14:57 GMT"}, {"version": "v2", "created": "Fri, 4 Jul 2014 07:11:19 GMT"}, {"version": "v3", "created": "Thu, 10 Jul 2014 09:48:30 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Preda", "Mila Dalla", ""], ["Giallorenzo", "Saverio", ""], ["Lanese", "Ivan", ""], ["Mauro", "Jacopo", ""], ["Gabbrielli", "Maurizio", ""]]}, {"id": "1407.1545", "submitter": "Mary Southern", "authors": "Mary Southern and Gopalan Nadathur", "title": "A Lambda Prolog Based Animation of Twelf Specifications", "comments": "15 pages, accepted for presentation at the International Colloquium\n  on Implementation of Constraint and Logic Programming Systems (CICLOPS) in\n  Vienna", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specifications in the Twelf system are based on a logic programming\ninterpretation of the Edinburgh Logical Framework or LF. We consider an\napproach to animating such specifications using a Lambda Prolog implementation.\nThis approach is based on a lossy translation of the dependently typed LF\nexpressions into the simply typed lambda calculus (STLC) terms of Lambda Prolog\nand a subsequent encoding of lost dependency information in predicates that are\ndefined by suitable clauses. To use this idea in an implementation of logic\nprogramming a la Twelf, it is also necessary to translate the results found for\nLambda Prolog queries back into LF expressions. We describe such an inverse\ntranslation and show that it has the necessary properties to facilitate an\nemulation of Twelf behavior through our translation of LF specifications into\nLambda Prolog programs. A characteristic of Twelf is that it permits queries to\nconsist of types which have unspecified parts represented by meta-variables for\nwhich values are to be found through computation. We show that this capability\ncan be supported within our translation based approach to animating Twelf\nspecifications.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jul 2014 20:56:24 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Southern", "Mary", ""], ["Nadathur", "Gopalan", ""]]}, {"id": "1407.1873", "submitter": "Antoine Genitrini", "authors": "Olivier Bodini, Antoine Genitrini and Fr\\'ed\\'eric Peschanski", "title": "A Quantitative Study of Pure Parallel Processes", "comments": null, "journal-ref": "Electronic Journal of Combinatorics, 23, 1, (2016), P1.11", "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the interleaving -- or pure merge -- operator that\nmost often characterizes parallelism in concurrency theory. This operator is a\nprincipal cause of the so-called combinatorial explosion that makes very hard -\nat least from the point of view of computational complexity - the analysis of\nprocess behaviours e.g. by model-checking. The originality of our approach is\nto study this combinatorial explosion phenomenon on average, relying on\nadvanced analytic combinatorics techniques. We study various measures that\ncontribute to a better understanding of the process behaviours represented as\nplane rooted trees: the number of runs (corresponding to the width of the\ntrees), the expected total size of the trees as well as their overall shape.\nTwo practical outcomes of our quantitative study are also presented: (1) a\nlinear-time algorithm to compute the probability of a concurrent run prefix,\nand (2) an efficient algorithm for uniform random sampling of concurrent runs.\nThese provide interesting responses to the combinatorial explosion problem.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2014 19:43:55 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Bodini", "Olivier", ""], ["Genitrini", "Antoine", ""], ["Peschanski", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1407.2041", "submitter": "Mohamed El-Zawawy Dr.", "authors": "Mohamed A. El-Zawawy, Adel I. AlSalem", "title": "ImpNet: Programming Software-Defied Networks Using Imperative Techniques", "comments": "12 pages, 12 figures, extended and revised version of reference [22]\n  in the paper. arXiv admin note: substantial text overlap with arXiv:1403.8028", "journal-ref": "WSEAS Transactions on Computers, ISSN / E-ISSN: 1109-2750 /\n  2224-2872, Volume 13, 2014, Art. #35, pp. 402-413", "doi": null, "report-no": null, "categories": "cs.PL cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software and hardware components are basic parts of modern networks. However\nthe software compo- nent is typical sealed and function-oriented. Therefore it\nis very difficult to modify these components. This badly affected networking\ninnovations. Moreover, this resulted in network policies having complex\ninterfaces that are not user-friendly and hence resulted in huge and\ncomplicated flow tables on physical switches of networks. This greatly degrades\nthe network performance in many cases. Software-Defined Networks (SDNs) is a\nmodern architecture of networks to overcome issues mentioned above. The idea of\nSDN is to add to the network a controller device that manages all the other\ndevices on the network including physical switches of the network. One of the\nmain tasks of the managing process is switch learning; achieved via programming\nphysical switches of the network by adding or removing rules for\npacket-processing to/from switches, more specifically to/from their flow\ntables. A high-level imperative network programming language, called ImpNet, is\npresented in this paper. ImpNet enables writing efficient, yet simple, and\npowerful programs to run on the controller to control all other network devices\nincluding switches. ImpNet is compositional, simply-structured, expressive, and\nmore importantly imperative. The syntax of ImpNet together two types of\noperational semantics to contracts of ImpNet are presented in the paper. The\nproposed semantics are of the static and dynamic types. Two modern application\nprogrammed using ImpNet are shown in the paper as well. The semantics of the\napplications are shown in the paper also.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jul 2014 11:34:27 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["El-Zawawy", "Mohamed A.", ""], ["AlSalem", "Adel I.", ""]]}, {"id": "1407.2190", "submitter": "Shahid Alam", "authors": "Shahid Alam", "title": "Is Fortran Still Relevant? Comparing Fortran with Java and C++", "comments": null, "journal-ref": "International Journal of Software Engineering & Application, pages\n  25-45, Volume 5, No 3, 2014", "doi": "10.5121/ijsea.2014.5303", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper presents a comparative study to evaluate and compare Fortran with\nthe two most popular programming languages Java and C++. Fortran has gone\nthrough major and minor extensions in the years 2003 and 2008. (1) How much\nhave these extensions made Fortran comparable to Java and C++? (2) What are the\ndifferences and similarities, in supporting features like: Templates, object\nconstructors and destructors, abstract data types and dynamic binding? These\nare the main questions we are trying to answer in this study. An\nobject-oriented ray tracing application is implemented in these three languages\nto compare them. By using only one program we ensured there was only one set of\nrequirements thus making the comparison homogeneous. Based on our literature\nsurvey this is the first study carried out to compare these languages by\napplying software metrics to the ray tracing application and comparing these\nresults with the similarities and differences found in practice. We motivate\nthe language implementers and compiler developers, by providing binary analysis\nand profiling of the application, to improve Fortran object handling and\nprocessing, and hence making it more prolific and general. This study\nfacilitates and encourages the reader to further explore, study and use these\nlanguages more effectively and productively, especially Fortran.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 16:40:55 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Alam", "Shahid", ""]]}, {"id": "1407.3681", "submitter": "Thorsten Tarrach", "authors": "Pavol \\v{C}ern\\'y, Thomas A. Henzinger, Arjun Radhakrishna, Leonid\n  Ryzhyk, and Thorsten Tarrach", "title": "Regression-free Synthesis for Concurrency", "comments": "for source code see https://github.com/thorstent/ConRepair", "journal-ref": "Computer Aided Verification, Lecture Notes in Computer Science\n  Volume 8559, 2014, pp 568-584", "doi": "10.1007/978-3-319-08867-9_38", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While fixing concurrency bugs, program repair algorithms may introduce new\nconcurrency bugs. We present an algorithm that avoids such regressions. The\nsolution space is given by a set of program transformations we consider in for\nrepair process. These include reordering of instructions within a thread and\ninserting atomic sections. The new algorithm learns a constraint on the space\nof candidate solutions, from both positive examples (error-free traces) and\ncounterexamples (error traces). From each counterexample, the algorithm learns\na constraint necessary to remove the errors. From each positive examples, it\nlearns a constraint that is necessary in order to prevent the repair from\nturning the trace into an error trace. We implemented the algorithm and\nevaluated it on simplified Linux device drivers with known bugs.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 14:54:58 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["\u010cern\u00fd", "Pavol", ""], ["Henzinger", "Thomas A.", ""], ["Radhakrishna", "Arjun", ""], ["Ryzhyk", "Leonid", ""], ["Tarrach", "Thorsten", ""]]}, {"id": "1407.3845", "submitter": "Jiahao Chen", "authors": "Jeff Bezanson, Jiahao Chen, Stefan Karpinski, Viral Shah, Alan Edelman", "title": "Array operators using multiple dispatch: a design methodology for array\n  implementations in dynamic languages", "comments": "6 pages, 2 figures, workshop paper for the ARRAY '14 workshop, June\n  11, 2014, Edinburgh, United Kingdom", "journal-ref": null, "doi": "10.1145/2627373.2627383", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arrays are such a rich and fundamental data type that they tend to be built\ninto a language, either in the compiler or in a large low-level library.\nDefining this functionality at the user level instead provides greater\nflexibility for application domains not envisioned by the language designer.\nOnly a few languages, such as C++ and Haskell, provide the necessary power to\ndefine $n$-dimensional arrays, but these systems rely on compile-time\nabstraction, sacrificing some flexibility. In contrast, dynamic languages make\nit straightforward for the user to define any behavior they might want, but at\nthe possible expense of performance.\n  As part of the Julia language project, we have developed an approach that\nyields a novel trade-off between flexibility and compile-time analysis. The\ncore abstraction we use is multiple dispatch. We have come to believe that\nwhile multiple dispatch has not been especially popular in most kinds of\nprogramming, technical computing is its killer application. By expressing key\nfunctions such as array indexing using multi-method signatures, a surprising\nrange of behaviors can be obtained, in a way that is both relatively easy to\nwrite and amenable to compiler analysis. The compact factoring of concerns\nprovided by these methods makes it easier for user-defined types to behave\nconsistently with types in the standard library.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 23:13:17 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Bezanson", "Jeff", ""], ["Chen", "Jiahao", ""], ["Karpinski", "Stefan", ""], ["Shah", "Viral", ""], ["Edelman", "Alan", ""]]}, {"id": "1407.3892", "submitter": "EPTCS", "authors": "Asankhaya Sharma", "title": "Verified Subtyping with Traits and Mixins", "comments": "In Proceedings FSFMA 2014, arXiv:1407.1952", "journal-ref": "EPTCS 156, 2014, pp. 45-51", "doi": "10.4204/EPTCS.156.8", "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traits allow decomposing programs into smaller parts and mixins are a form of\ncomposition that resemble multiple inheritance. Unfortunately, in the presence\nof traits, programming languages like Scala give up on subtyping relation\nbetween objects. In this paper, we present a method to check subtyping between\nobjects based on entailment in separation logic. We implement our method as a\ndomain specific language in Scala and apply it on the Scala standard library.\nWe have verified that 67% of mixins used in the Scala standard library do\nindeed conform to subtyping between the traits that are used to build them.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 06:40:59 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Sharma", "Asankhaya", ""]]}, {"id": "1407.4075", "submitter": "Grigori Fursin", "authors": "Lianjie Luo and Yang Chen and Chengyong Wu and Shun Long and Grigori\n  Fursin", "title": "Finding representative sets of optimizations for adaptive\n  multiversioning applications", "comments": "3rd Workshop on Statistical and Machine Learning Approaches Applied\n  to Architectures and Compilation (SMART'09), co-located with HiPEAC'09\n  conference, Paphos, Cyprus, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative compilation is a widely adopted technique to optimize programs for\ndifferent constraints such as performance, code size and power consumption in\nrapidly evolving hardware and software environments. However, in case of\nstatically compiled programs, it is often restricted to optimizations for a\nspecific dataset and may not be applicable to applications that exhibit\ndifferent run-time behavior across program phases, multiple datasets or when\nexecuted in heterogeneous, reconfigurable and virtual environments. Several\nframeworks have been recently introduced to tackle these problems and enable\nrun-time optimization and adaptation for statically compiled programs based on\nstatic function multiversioning and monitoring of online program behavior. In\nthis article, we present a novel technique to select a minimal set of\nrepresentative optimization variants (function versions) for such frameworks\nwhile avoiding performance loss across available datasets and code-size\nexplosion. We developed a novel mapping mechanism using popular decision tree\nor rule induction based machine learning techniques to rapidly select best code\nversions at run-time based on dataset features and minimize selection overhead.\nThese techniques enable creation of self-tuning static binaries or libraries\nadaptable to changing behavior and environments at run-time using staged\ncompilation that do not require complex recompilation frameworks while\neffectively outperforming traditional single-version non-adaptable code.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jul 2014 17:55:07 GMT"}], "update_date": "2014-07-16", "authors_parsed": [["Luo", "Lianjie", ""], ["Chen", "Yang", ""], ["Wu", "Chengyong", ""], ["Long", "Shun", ""], ["Fursin", "Grigori", ""]]}, {"id": "1407.4378", "submitter": "Cameron Mura", "authors": "Marcin Cieslik and Cameron Mura", "title": "PaPy: Parallel and Distributed Data-processing Pipelines in Python", "comments": "7 pages, 5 figures, 2 tables, some use-cases; more at\n  http://muralab.org/PaPy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PaPy, which stands for parallel pipelines in Python, is a highly flexible\nframework that enables the construction of robust, scalable workflows for\neither generating or processing voluminous datasets. A workflow is created from\nuser-written Python functions (nodes) connected by 'pipes' (edges) into a\ndirected acyclic graph. These functions are arbitrarily definable, and can make\nuse of any Python modules or external binaries. Given a user-defined topology\nand collection of input data, functions are composed into nested higher-order\nmaps, which are transparently and robustly evaluated in parallel on a single\ncomputer or on remote hosts. Local and remote computational resources can be\nflexibly pooled and assigned to functional nodes, thereby allowing facile\nload-balancing and pipeline optimization to maximize computational throughput.\nInput items are processed by nodes in parallel, and traverse the graph in\nbatches of adjustable size -- a trade-off between lazy-evaluation, parallelism,\nand memory consumption. The processing of a single item can be parallelized in\na scatter/gather scheme. The simplicity and flexibility of distributed\nworkflows using PaPy bridges the gap between desktop -> grid, enabling this new\ncomputing paradigm to be leveraged in the processing of large scientific\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jul 2014 03:13:00 GMT"}], "update_date": "2014-07-17", "authors_parsed": [["Cieslik", "Marcin", ""], ["Mura", "Cameron", ""]]}, {"id": "1407.4917", "submitter": "Shrawan Kumar", "authors": "Shrawan Kumar, Amitabha Sanyal, Uday Khedker", "title": "Sliced Slices: Separating Data and Control Influences", "comments": "10 pages, 5 figures, two algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backward slicing has been used extensively in program understanding,\ndebugging and scaling up of program analysis. For large programs, the size of\nthe conventional backward slice is about 25% of the program size. This may be\ntoo large to be useful. Our investigations reveal that in general, the size of\na slice is influenced more by computations governing the control flow reaching\nthe slicing criterion than by the computations governing the values relevant to\nthe slicing criterion. We distinguish between the two by defining data slices\nand control slices both of which are smaller than the conventional slices which\ncan be obtained by combining the two. This is useful because for many\napplications, the individual data or control slices are sufficient.\n  Our experiments show that for more than 50% of cases, the data slice is\nsmaller than 10% of the program in size. Besides, the time to compute data or\ncontrol slice is comparable to that for computing the conventional slice.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jul 2014 08:54:19 GMT"}], "update_date": "2014-07-21", "authors_parsed": [["Kumar", "Shrawan", ""], ["Sanyal", "Amitabha", ""], ["Khedker", "Uday", ""]]}, {"id": "1407.5393", "submitter": "EPTCS", "authors": "Herbert Wiklicky (Imperial College London)", "title": "Program Synthesis and Linear Operator Semantics", "comments": "In Proceedings SYNT 2014, arXiv:1407.4937", "journal-ref": "EPTCS 157, 2014, pp. 17-33", "doi": "10.4204/EPTCS.157.6", "report-no": null, "categories": "cs.PL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For deterministic and probabilistic programs we investigate the problem of\nprogram synthesis and program optimisation (with respect to non-functional\nproperties) in the general setting of global optimisation. This approach is\nbased on the representation of the semantics of programs and program fragments\nin terms of linear operators, i.e. as matrices. We exploit in particular the\nfact that we can automatically generate the representation of the semantics of\nelementary blocks. These can then can be used in order to compositionally\nassemble the semantics of a whole program, i.e. the generator of the\ncorresponding Discrete Time Markov Chain (DTMC). We also utilise a generalised\nversion of Abstract Interpretation suitable for this linear algebraic or\nfunctional analytical framework in order to formulate semantical constraints\n(invariants) and optimisation objectives (for example performance\nrequirements).\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 07:28:08 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Wiklicky", "Herbert", "", "Imperial College London"]]}, {"id": "1407.5397", "submitter": "EPTCS", "authors": "Susmit Jha (Strategic CAD Labs, Intel), Sanjit A. Seshia (EECS, UC\n  Berkeley)", "title": "Are There Good Mistakes? A Theoretical Analysis of CEGIS", "comments": "In Proceedings SYNT 2014, arXiv:1407.4937", "journal-ref": "EPTCS 157, 2014, pp. 84-99", "doi": "10.4204/EPTCS.157.10", "report-no": null, "categories": "cs.LO cs.AI cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterexample-guided inductive synthesis CEGIS is used to synthesize\nprograms from a candidate space of programs. The technique is guaranteed to\nterminate and synthesize the correct program if the space of candidate programs\nis finite. But the technique may or may not terminate with the correct program\nif the candidate space of programs is infinite. In this paper, we perform a\ntheoretical analysis of counterexample-guided inductive synthesis technique. We\ninvestigate whether the set of candidate spaces for which the correct program\ncan be synthesized using CEGIS depends on the counterexamples used in inductive\nsynthesis, that is, whether there are good mistakes which would increase the\nsynthesis power. We investigate whether the use of minimal counterexamples\ninstead of arbitrary counterexamples expands the set of candidate spaces of\nprograms for which inductive synthesis can successfully synthesize a correct\nprogram. We consider two kinds of counterexamples: minimal counterexamples and\nhistory bounded counterexamples. The history bounded counterexample used in any\niteration of CEGIS is bounded by the examples used in previous iterations of\ninductive synthesis. We examine the relative change in power of inductive\nsynthesis in both cases. We show that the synthesis technique using minimal\ncounterexamples MinCEGIS has the same synthesis power as CEGIS but the\nsynthesis technique using history bounded counterexamples HCEGIS has different\npower than that of CEGIS, but none dominates the other.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 07:28:49 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Jha", "Susmit", "", "Strategic CAD Labs, Intel"], ["Seshia", "Sanjit A.", "", "EECS, UC\n  Berkeley"]]}, {"id": "1407.5524", "submitter": "Edward Givelberg", "authors": "Edward Givelberg", "title": "Process-Oriented Parallel Programming with an Application to\n  Data-Intensive Computing", "comments": "20 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce process-oriented programming as a natural extension of\nobject-oriented programming for parallel computing. It is based on the\nobservation that every class of an object-oriented language can be instantiated\nas a process, accessible via a remote pointer. The introduction of process\npointers requires no syntax extension, identifies processes with programming\nobjects, and enables processes to exchange information simply by executing\nremote methods. Process-oriented programming is a high-level language\nalternative to multithreading, MPI and many other languages, environments and\ntools currently used for parallel computations. It implements natural\nobject-based parallelism using only minimal syntax extension of existing\nlanguages, such as C++ and Python, and has therefore the potential to lead to\nwidespread adoption of parallel programming. We implemented a prototype system\nfor running processes using C++ with MPI and used it to compute a large\nthree-dimensional Fourier transform on a computer cluster built of commodity\nhardware components. Three-dimensional Fourier transform is a prototype of a\ndata-intensive application with a complex data-access pattern. The\nprocess-oriented code is only a few hundred lines long, and attains very high\ndata throughput by achieving massive parallelism and maximizing hardware\nutilization.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 15:16:37 GMT"}], "update_date": "2014-07-22", "authors_parsed": [["Givelberg", "Edward", ""]]}, {"id": "1407.5670", "submitter": "Raphael kena Poss", "authors": "Raphael Poss", "title": "Rust for functional programmers", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides an introduction to Rust, a systems language by Mozilla,\nto programmers already familiar with Haskell, OCaml or other functional\nlanguages.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jul 2014 21:20:31 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Poss", "Raphael", ""]]}, {"id": "1407.6124", "submitter": "Duc Hiep Chu", "authors": "Duc-Hiep Chu, Joxan Jaffar, Minh-Thai Trinh", "title": "Automating Proofs of Data-Structure Properties in Imperative Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of automated reasoning about dynamically manipulated\ndata structures. The state-of-the-art methods are limited to the\nunfold-and-match (U+M) paradigm, where predicates are transformed via\n(un)folding operations induced from their definitions before being treated as\nuninterpreted. However, proof obligations from verifying programs with\niterative loops and multiple function calls often do not succumb to this\nparadigm. Our contribution is a proof method which -- beyond U+M -- performs\nautomatic formula re-writing by treating previously encountered obligations in\neach proof path as possible induction hypotheses. This enables us, for the\nfirst time, to systematically reason about a wide range of obligations, arising\nfrom practical program verification. We demonstrate the power of our proof\nrules on commonly used lemmas, thereby close the remaining gaps in existing\nstate-of-the-art systems. Another impact, probably more important, is that our\nmethod regains the power of compositional reasoning, and shows that the usage\nof user-provided lemmas is no longer needed for the existing set of benchmarks.\nThis not only removes the burden of coming up with the appropriate lemmas, but\nalso significantly boosts up the verification process, since lemma\napplications, coupled with unfolding, often induce very large search space.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 08:06:53 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Chu", "Duc-Hiep", ""], ["Jaffar", "Joxan", ""], ["Trinh", "Minh-Thai", ""]]}, {"id": "1407.6845", "submitter": "Justin Hsu", "authors": "Gilles Barthe, Marco Gaboardi, Emilio Jes\\'us Gallego Arias, Justin\n  Hsu, Aaron Roth, Pierre-Yves Strub", "title": "Higher-Order Approximate Relational Refinement Types for Mechanism\n  Design and Differential Privacy", "comments": null, "journal-ref": null, "doi": "10.1145/2676726.2677000", "report-no": null, "categories": "cs.PL cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mechanism design is the study of algorithm design in which the inputs to the\nalgorithm are controlled by strategic agents, who must be incentivized to\nfaithfully report them. Unlike typical programmatic properties, it is not\nsufficient for algorithms to merely satisfy the property---incentive properties\nare only useful if the strategic agents also believe this fact.\n  Verification is an attractive way to convince agents that the incentive\nproperties actually hold, but mechanism design poses several unique challenges:\ninteresting properties can be sophisticated relational properties of\nprobabilistic computations involving expected values, and mechanisms may rely\non other probabilistic properties, like differential privacy, to achieve their\ngoals.\n  We introduce a relational refinement type system, called $\\mathsf{HOARe}^2$,\nfor verifying mechanism design and differential privacy. We show that\n$\\mathsf{HOARe}^2$ is sound w.r.t. a denotational semantics, and correctly\nmodels $(\\epsilon,\\delta)$-differential privacy; moreover, we show that it\nsubsumes DFuzz, an existing linear dependent type system for differential\nprivacy. Finally, we develop an SMT-based implementation of $\\mathsf{HOARe}^2$\nand use it to verify challenging examples of mechanism design, including\nauctions and aggregative games, and new proposed examples from differential\nprivacy.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jul 2014 10:53:19 GMT"}, {"version": "v2", "created": "Wed, 29 Oct 2014 21:43:39 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Barthe", "Gilles", ""], ["Gaboardi", "Marco", ""], ["Arias", "Emilio Jes\u00fas Gallego", ""], ["Hsu", "Justin", ""], ["Roth", "Aaron", ""], ["Strub", "Pierre-Yves", ""]]}, {"id": "1407.6968", "submitter": "Mark Moir", "authors": "Dave Dice, Timothy L. Harris, Alex Kogan, Yossi Lev, Mark Moir", "title": "Hardware extensions to make lazy subscription safe", "comments": "6 pages, extended version of WTTM2014 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transactional Lock Elision (TLE) uses Hardware Transactional Memory (HTM) to\nexecute unmodified critical sections concurrently, even if they are protected\nby the same lock. To ensure correctness, the transactions used to execute these\ncritical sections \"subscribe\" to the lock by reading it and checking that it is\navailable. A recent paper proposed using the tempting \"lazy subscription\"\noptimization for a similar technique in a different context, namely\ntransactional systems that use a single global lock (SGL) to protect all\ntransactional data. We identify several pitfalls that show that lazy\nsubscription \\emph{is not safe} for TLE because unmodified critical sections\nexecuting before subscribing to the lock may behave incorrectly in a number of\nsubtle ways. We also show that recently proposed compiler support for modifying\ntransaction code to ensure subscription occurs before any incorrect behavior\ncould manifest is not sufficient to avoid all of the pitfalls we identify. We\nfurther argue that extending such compiler support to avoid all pitfalls would\nadd substantial complexity and would usually limit the extent to which\nsubscription can be deferred, undermining the effectiveness of the\noptimization. Hardware extensions suggested in the recent proposal also do not\naddress all of the pitfalls we identify. In this extended version of our WTTM\n2014 paper, we describe hardware extensions that make lazy subscription safe,\nboth for SGL-based transactional systems and for TLE, without the need for\nspecial compiler support. We also explain how nontransactional loads can be\nexploited, if available, to further enhance the effectiveness of lazy\nsubscription.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2014 22:01:59 GMT"}], "update_date": "2014-07-28", "authors_parsed": [["Dice", "Dave", ""], ["Harris", "Timothy L.", ""], ["Kogan", "Alex", ""], ["Lev", "Yossi", ""], ["Moir", "Mark", ""]]}, {"id": "1407.7932", "submitter": "EPTCS", "authors": "William Mansky (University of Illinois at Urbana-Champaign), Dennis\n  Griffith (University of Illinois at Urbana-Champaign), Elsa L. Gunter\n  (University of Illinois at Urbana-Champaign)", "title": "Specifying and Executing Optimizations for Parallel Programs", "comments": "In Proceedings GRAPHITE 2014, arXiv:1407.7671", "journal-ref": "EPTCS 159, 2014, pp. 58-70", "doi": "10.4204/EPTCS.159.6", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compiler optimizations, usually expressed as rewrites on program graphs, are\na core part of all modern compilers. However, even production compilers have\nbugs, and these bugs are difficult to detect and resolve. The problem only\nbecomes more complex when compiling parallel programs; from the choice of graph\nrepresentation to the possibility of race conditions, optimization designers\nhave a range of factors to consider that do not appear when dealing with\nsingle-threaded programs. In this paper we present PTRANS, a domain-specific\nlanguage for formal specification of compiler transformations, and describe its\nexecutable semantics. The fundamental approach of PTRANS is to describe program\ntransformations as rewrites on control flow graphs with temporal logic side\nconditions. The syntax of PTRANS allows cleaner, more comprehensible\nspecification of program optimizations; its executable semantics allows these\nspecifications to act as prototypes for the optimizations themselves, so that\ncandidate optimizations can be tested and refined before going on to include\nthem in a compiler. We demonstrate the use of PTRANS to state, test, and refine\nthe specification of a redundant store elimination optimization on parallel\nprograms.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2014 03:23:14 GMT"}], "update_date": "2014-07-31", "authors_parsed": [["Mansky", "William", "", "University of Illinois at Urbana-Champaign"], ["Griffith", "Dennis", "", "University of Illinois at Urbana-Champaign"], ["Gunter", "Elsa L.", "", "University of Illinois at Urbana-Champaign"]]}]