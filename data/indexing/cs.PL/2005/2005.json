[{"id": "2005.00197", "submitter": "EPTCS", "authors": "Anne Baanen (Vrije Universiteit Amsterdam), Wouter Swierstra (Utrecht\n  Univeristy)", "title": "Combining predicate transformer semantics for effects: a case study in\n  parsing regular languages", "comments": "In Proceedings MSFP 2020, arXiv:2004.14735", "journal-ref": "EPTCS 317, 2020, pp. 39-56", "doi": "10.4204/EPTCS.317.3", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes how to verify a parser for regular expressions in a\nfunctional programming language using predicate transformer semantics for a\nvariety of effects. Where our previous work in this area focused on the\nsemantics for a single effect, parsing requires a combination of effects:\nnon-determinism, general recursion and mutable state. Reasoning about such\ncombinations of effects is notoriously difficult, yet our approach using\npredicate transformers enables the careful separation of program syntax,\ncorrectness proofs and termination proofs.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 03:42:21 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Baanen", "Anne", "", "Vrije Universiteit Amsterdam"], ["Swierstra", "Wouter", "", "Utrecht\n  Univeristy"]]}, {"id": "2005.00198", "submitter": "EPTCS", "authors": "Artjoms {\\v{S}}inkarovs (Heriot-Watt University)", "title": "Multi-dimensional Arrays with Levels", "comments": "In Proceedings MSFP 2020, arXiv:2004.14735", "journal-ref": "EPTCS 317, 2020, pp. 57-71", "doi": "10.4204/EPTCS.317.4", "report-no": null, "categories": "cs.DS cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a data structure that generalises rectangular multi-dimensional\narrays. The shape of an n-dimensional array is typically given by a tuple of n\nnatural numbers. Each element in that tuple defines the length of the\ncorresponding axis. If we treat this tuple as an array, the shape of that array\nis described by the single natural number n. A natural number itself can be\nalso treated as an array with the shape described by the natural number 1 (or\nthe element of any singleton set). This observation gives rise to the hierarchy\nof array types where the shape of an array of level l+1 is a level-l array of\nnatural numbers. Such a hierarchy occurs naturally when treating arrays as\ncontainers, which makes it possible to define both rank- and level-polymorphic\noperations. The former can be found in most array languages, whereas the latter\ngives rise to partial selections on a large set of hyperplanes, which is often\nuseful in practice. In this paper we present an Agda formalisation of arrays\nwith levels. We show that the proposed formalism supports standard\nrank-polymorphic array operations, while type system gives static guarantees\nthat indexing is within bounds. We generalise the notion of ranked operator so\nthat it becomes applicable on arrays of arbitrary levels and we show why this\nmay be useful in practice.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 03:42:41 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["{\u0160}inkarovs", "Artjoms", "", "Heriot-Watt University"]]}, {"id": "2005.00199", "submitter": "EPTCS", "authors": "Christopher Jenkins, Aaron Stump, Larry Diehl", "title": "Efficient lambda encodings for Mendler-style coinductive types in\n  Cedille", "comments": "In Proceedings MSFP 2020, arXiv:2004.14735", "journal-ref": "EPTCS 317, 2020, pp. 72-97", "doi": "10.4204/EPTCS.317.5", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the calculus of dependent lambda eliminations (CDLE), it is possible to\ndefine inductive datatypes via lambda encodings that feature constant-time\ndestructors and a course-of-values induction scheme. This paper begins to\naddress the missing derivations for the dual, coinductive types. Our derivation\nutilizes new methods within CDLE, as there are seemingly fundamental\ndifficulties in adapting previous known approaches for deriving inductive\ntypes. The lambda encodings we present implementing coinductive types feature\nconstant-time constructors and a course-of-values corecursion scheme.\nCoinductive type families are also supported, enabling proofs for many standard\ncoinductive properties such as stream bisimulation. All work is mechanically\nverified by the Cedille tool, an implementation of CDLE.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 03:42:59 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Jenkins", "Christopher", ""], ["Stump", "Aaron", ""], ["Diehl", "Larry", ""]]}, {"id": "2005.01018", "submitter": "Dmitry Rozplokhas", "authors": "Dmitry Rozplokhas, Andrey Vyatkin, Dmitry Boulytchev", "title": "Certified Semantics for Relational Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a formal study of semantics for the relational programming\nlanguage miniKanren. First, we formulate a denotational semantics which\ncorresponds to the minimal Herbrand model for definite logic programs. Second,\nwe present operational semantics which models interleaving, the distinctive\nfeature of miniKanren implementation, and prove its soundness and completeness\nw.r.t. the denotational semantics. Our development is supported by a Coq\nspecification, from which a reference interpreter can be extracted. We also\nderive from our main result a certified semantics (and a reference interpreter)\nfor SLD resolution with cut and prove its soundness.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 08:25:11 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 22:35:02 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Rozplokhas", "Dmitry", ""], ["Vyatkin", "Andrey", ""], ["Boulytchev", "Dmitry", ""]]}, {"id": "2005.01977", "submitter": "Johan Bay", "authors": "Johan Bay and Aslan Askarov", "title": "Reconciling progress-insensitive noninterference and declassification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practitioners of secure information flow often face a design challenge: what\nis the right semantic treatment of leaks via termination? On the one hand, the\npotential harm of untrusted code calls for strong progress-sensitive security.\nOn the other hand, when the code is trusted to not aggressively exploit\ntermination channels, practical concerns, such as permissiveness of the\nenforcement, make a case for settling for weaker, progress-insensitive\nsecurity. This binary situation, however, provides no suitable middle point for\nsystems that mix trusted and untrusted code. This paper connects the two\nextremes by reframing progress-insensitivity as a particular form of\ndeclassification. Our novel semantic condition reconciles progress-insensitive\nsecurity as a declassification bound on the so-called progress knowledge in an\notherwise progress or timing sensitive setting. We show how the new condition\ncan be soundly enforced using a mostly standard information-flow monitor. We\nbelieve that the connection established in this work will enable other\napplications of ideas from the literature on declassification to progress\ninsensitivity.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 07:28:57 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 22:26:30 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Bay", "Johan", ""], ["Askarov", "Aslan", ""]]}, {"id": "2005.02161", "submitter": "Jiayi Wei", "authors": "Jiayi Wei, Maruth Goyal, Greg Durrett, Isil Dillig", "title": "LambdaNet: Probabilistic Type Inference using Graph Neural Networks", "comments": "Accepted as a poster at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As gradual typing becomes increasingly popular in languages like Python and\nTypeScript, there is a growing need to infer type annotations automatically.\nWhile type annotations help with tasks like code completion and static error\ncatching, these annotations cannot be fully determined by compilers and are\ntedious to annotate by hand. This paper proposes a probabilistic type inference\nscheme for TypeScript based on a graph neural network. Our approach first uses\nlightweight source code analysis to generate a program abstraction called a\ntype dependency graph, which links type variables with logical constraints as\nwell as name and usage information. Given this program abstraction, we then use\na graph neural network to propagate information between related type variables\nand eventually make type predictions. Our neural architecture can predict both\nstandard types, like number or string, as well as user-defined types that have\nnot been encountered during training. Our experimental results show that our\napproach outperforms prior work in this space by $14\\%$ (absolute) on library\ntypes, while having the ability to make type predictions that are out of scope\nfor existing techniques.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 17:48:40 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Wei", "Jiayi", ""], ["Goyal", "Maruth", ""], ["Durrett", "Greg", ""], ["Dillig", "Isil", ""]]}, {"id": "2005.02247", "submitter": "James Wood", "authors": "James Wood, Robert Atkey", "title": "A Linear Algebra Approach to Linear Metatheory", "comments": "12 pages, 4 figures, submitted to Linearity/TLLA post-proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Linear typed $\\lambda$-calculi are more delicate than their simply typed\nsiblings when it comes to metatheoretic results like preservation of typing\nunder renaming and substitution. Tracking the usage of variables in contexts\nplaces more constraints on how variables may be renamed or substituted. We\npresent a methodology based on linear algebra over semirings, extending\nMcBride's kits and traversals approach for the metatheory of syntax with\nbinding to linear usage-annotated terms. Our approach is readily formalisable,\nand we have done so in Agda.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 14:46:34 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 16:40:02 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Wood", "James", ""], ["Atkey", "Robert", ""]]}, {"id": "2005.03292", "submitter": "Peter Hillmann", "authors": "Mario Golling, Robert Koch, Peter Hillmann, Rick Hofstede, Frank\n  Tietze", "title": "YANG2UML: Bijective Transformation and Simplification of YANG to UML", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.DC cs.NI cs.PL cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software Defined Networking is currently revolutionizing computer networking\nby decoupling the network control (control plane) from the forwarding functions\n(data plane) enabling the network control to become directly programmable and\nthe underlying infrastructure to be abstracted for applications and network\nservices. Next to the well-known OpenFlow protocol, the XML-based NETCONF\nprotocol is also an important means for exchanging configuration information\nfrom a management platform and is nowadays even part of OpenFlow. In\ncombination with NETCONF, YANG is the corresponding protocol that defines the\nassociated data structures supporting virtually all network configuration\nprotocols. YANG itself is a semantically rich language, which -- in order to\nfacilitate familiarization with the relevant subject -- is often visualized to\ninvolve other experts or developers and to support them by their daily work\n(writing applications which make use of YANG). In order to support this\nprocess, this paper presents an novel approach to optimize and simplify YANG\ndata models to assist further discussions with the management and\nimplementations (especially of interfaces) to reduce complexity. Therefore, we\nhave defined a bidirectional mapping of YANG to UML and developed a tool that\nrenders the created UML diagrams. This combines the benefits to use the formal\nlanguage YANG with automatically maintained UML diagrams to involve other\nexperts or developers, closing the gap between technically improved data models\nand their human readability.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 07:29:49 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Golling", "Mario", ""], ["Koch", "Robert", ""], ["Hillmann", "Peter", ""], ["Hofstede", "Rick", ""], ["Tietze", "Frank", ""]]}, {"id": "2005.03764", "submitter": "Cecilia Romaro Miss", "authors": "Cecilia Romaro, Fernando Araujo Najman, William W Lytton, Antonio C\n  Roque, Salvador Dura-Bernal", "title": "NetPyNE implementation and rescaling of the Potjans-Diesmanncortical\n  microcircuit model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL math.DS nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Potjans-Diesmann cortical microcircuit model is a widely used model\noriginallyimplemented in NEST. Here, we re-implemented the model using NetPyNE,\na high-level Python interface to the NEURON simulator, and reproduced the\nfindings of theoriginal publication. We also implemented a method for rescaling\nthe network sizewhich preserves first and second order statistics, building on\nexisting work on networktheory. The new implementation enables using more\ndetailed neuron models with mul-ticompartment morphologies and multiple\nbiophysically realistic channels. This opensthe model to new research,\nincluding the study of dendritic processing, the influenceof individual channel\nparameters, and generally multiscale interactions in the network.The rescaling\nmethod provides flexibility to increase or decrease the network size ifrequired\nwhen running these more realistic simulations. Finally, NetPyNE\nfacilitatesmodifying or extending the model using its declarative language;\noptimizing modelparameters; running efficient large-scale parallelized\nsimulations; and analyzing themodel through built-in methods, including local\nfield potential calculation and informa-tion flow measures.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 21:16:58 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Romaro", "Cecilia", ""], ["Najman", "Fernando Araujo", ""], ["Lytton", "William W", ""], ["Roque", "Antonio C", ""], ["Dura-Bernal", "Salvador", ""]]}, {"id": "2005.04094", "submitter": "Zheng Wang", "authors": "Jianbin Fang, Chun Huang, Tao Tang, Zheng Wang", "title": "Parallel Programming Models for Heterogeneous Many-Cores : A Survey", "comments": "Accepted to be published at CCF Transactions on High Performance\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Heterogeneous many-cores are now an integral part of modern computing systems\nranging from embedding systems to supercomputers. While heterogeneous many-core\ndesign offers the potential for energy-efficient high-performance, such\npotential can only be unlocked if the application programs are suitably\nparallel and can be made to match the underlying heterogeneous platform. In\nthis article, we provide a comprehensive survey for parallel programming models\nfor heterogeneous many-core architectures and review the compiling techniques\nof improving programmability and portability. We examine various software\noptimization techniques for minimizing the communicating overhead between\nheterogeneous computing devices. We provide a road map for a wide variety of\ndifferent research areas. We conclude with a discussion on open issues in the\narea and potential research directions. This article provides both an\naccessible introduction to the fast-moving area of heterogeneous programming\nand a detailed bibliography of its main achievements.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 13:39:05 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Fang", "Jianbin", ""], ["Huang", "Chun", ""], ["Tang", "Tao", ""], ["Wang", "Zheng", ""]]}, {"id": "2005.04453", "submitter": "L\\'eo Stefanesco", "authors": "Paul-Andr\\'e Melli\\`es and L\\'eo Stefanesco", "title": "Concurrent Separation Logic Meets Template Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An old dream of concurrency theory and programming language semantics has\nbeen to uncover the fundamental synchronization mechanisms which regulate\nsituations as different as game semantics for higher-order programs, and Hoare\nlogic for concurrent programs with shared memory and locks. In this paper, we\nestablish a deep and unexpected connection between two recent lines of work on\nconcurrent separation logic (CSL) and on template game semantics for\ndifferential linear logic (DiLL). Thanks to this connection, we reformulate in\nthe purely conceptual style of template games for DiLL the asynchronous and\ninteractive interpretation of CSL designed by Melli\\`es and Stefanesco. We\nbelieve that the analysis reveals something important about the secret anatomy\nof CSL, and more specifically about the subtle interplay, of a categorical\nnature, between sequential composition, parallel product, errors and locks.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 14:33:49 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Melli\u00e8s", "Paul-Andr\u00e9", ""], ["Stefanesco", "L\u00e9o", ""]]}, {"id": "2005.04722", "submitter": "Catalin Hritcu", "authors": "Maximilian Algehed, Jean-Philippe Bernardy, Catalin Hritcu", "title": "Dynamic IFC Theorems for Free!", "comments": "CSF 2021 final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that noninterference and transparency, the key soundness theorems for\ndynamic IFC libraries, can be obtained \"for free\", as direct consequences of\nthe more general parametricity theorem of type abstraction. This allows us to\ngive very short soundness proofs for dynamic IFC libraries such as faceted\nvalues and LIO. Our proofs stay short even when fully mechanized for Agda\nimplementations of the libraries in terms of type abstraction.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 17:29:15 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 13:56:57 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Algehed", "Maximilian", ""], ["Bernardy", "Jean-Philippe", ""], ["Hritcu", "Catalin", ""]]}, {"id": "2005.04831", "submitter": "Micah Halter", "authors": "Micah Halter, Evan Patterson, Andrew Baas, James Fairbanks", "title": "Compositional Scientific Computing with Catlab and SemanticModels", "comments": "3 pages, 6 figures, Applied Category Theory 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific computing is currently performed by writing domain specific\nmodeling frameworks for solving special classes of mathematical problems. Since\napplied category theory provides abstract reasoning machinery for describing\nand analyzing diverse areas of math, it is a natural platform for building\ngeneric and reusable software components for scientific computing. We present\nCatlab.jl, which provides the category-theoretic infrastructure for this\nproject, together with SemanticModels.jl, which leverages this infrastructure\nfor particular modeling tasks. This approach enhances and automates scientific\ncomputing workflows by applying recent advances in mathematical modeling of\ninterconnected systems as cospan algebras.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 01:54:11 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 20:13:24 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Halter", "Micah", ""], ["Patterson", "Evan", ""], ["Baas", "Andrew", ""], ["Fairbanks", "James", ""]]}, {"id": "2005.05927", "submitter": "Ruiqi Zhong", "authors": "Ruiqi Zhong, Mitchell Stern, Dan Klein", "title": "Semantic Scaffolds for Pseudocode-to-Code Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for program generation based on semantic scaffolds,\nlightweight structures representing the high-level semantic and syntactic\ncomposition of a program. By first searching over plausible scaffolds then\nusing these as constraints for a beam search over programs, we achieve better\ncoverage of the search space when compared with existing techniques. We apply\nour hierarchical search method to the SPoC dataset for pseudocode-to-code\ngeneration, in which we are given line-level natural language pseudocode\nannotations and aim to produce a program satisfying execution-based test cases.\nBy using semantic scaffolds during inference, we achieve a 10% absolute\nimprovement in top-100 accuracy over the previous state-of-the-art.\nAdditionally, we require only 11 candidates to reach the top-3000 performance\nof the previous best approach when tested against unseen problems,\ndemonstrating a substantial improvement in efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 17:10:13 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Zhong", "Ruiqi", ""], ["Stern", "Mitchell", ""], ["Klein", "Dan", ""]]}, {"id": "2005.05944", "submitter": "Akram El-Korashy", "authors": "Akram El-Korashy, Stelios Tsampas, Marco Patrignani, Dominique\n  Devriese, Deepak Garg, Frank Piessens", "title": "CapablePtrs: Securely Compiling Partial Programs Using the\n  Pointers-as-Capabilities Principle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capability machines such as CHERI provide memory capabilities that can be\nused by compilers to provide security benefits for compiled code (e.g., memory\nsafety). The existing C to CHERI compiler, for example, achieves memory safety\nby following a principle called \"pointers as capabilities\" (PAC). Informally,\nPAC says that a compiler should represent a source language pointer as a\nmachine code capability. But the security properties of PAC compilers are not\nyet well understood. We show that memory safety is only one aspect, and that\nPAC compilers can provide significant additional security guarantees for\npartial programs: the compiler can provide security guarantees for a\ncompilation unit, even if that compilation unit is later linked to\nattacker-provided machine code.\n  As such, this paper is the first to study the security of PAC compilers for\npartial programs formally. We prove for a model of such a compiler that it is\nfully abstract. The proof uses a novel proof technique (dubbed TrICL, read\ntrickle), which should be of broad interest because it reuses the whole-program\ncompiler correctness relation for full abstraction, thus saving work. We also\nimplement our scheme for C on CHERI, show that we can compile legacy C code\nwith minimal changes, and show that the performance overhead of compiled code\nis roughly proportional to the number of cross-compilation-unit function calls.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 17:43:19 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 05:35:18 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["El-Korashy", "Akram", ""], ["Tsampas", "Stelios", ""], ["Patrignani", "Marco", ""], ["Devriese", "Dominique", ""], ["Garg", "Deepak", ""], ["Piessens", "Frank", ""]]}, {"id": "2005.05970", "submitter": "Ankush Das", "authors": "Ankush Das and Frank Pfenning", "title": "Session Types with Arithmetic Refinements", "comments": "14 pages. arXiv admin note: text overlap with arXiv:2001.04439", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session types statically prescribe bidirectional communication protocols for\nmessage-passing processes. However, simple session types cannot specify\nproperties beyond the type of exchanged messages. In this paper we extend the\ntype system by using index refinements from linear arithmetic capturing\nintrinsic attributes of data structures and algorithms. We show that, despite\nthe decidability of Presburger arithmetic, type equality and therefore also\nsubtyping and type checking are now undecidable, which stands in contrast to\nanalogous dependent refinement type systems from functional languages. We also\npresent a practical, but incomplete algorithm for type equality, which we have\nused in our implementation of Rast, a concurrent session-typed language with\narithmetic index refinements as well as ergometric and temporal types.\nMoreover, if necessary, the programmer can propose additional type\nbisimulations that are smoothly integrated into the type equality algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 13:41:22 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Das", "Ankush", ""], ["Pfenning", "Frank", ""]]}, {"id": "2005.06227", "submitter": "Clara Schneidewind", "authors": "Clara Schneidewind, Ilya Grishchenko, Markus Scherer, Matteo Maffei", "title": "eThor: Practical and Provably Sound Static Analysis of Ethereum Smart\n  Contracts", "comments": "Accepted for CCS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ethereum has emerged as the most popular smart contract development platform,\nwith hundreds of thousands of contracts stored on the blockchain and covering a\nvariety of application scenarios, such as auctions, trading platforms, and so\non. Given their financial nature, security vulnerabilities may lead to\ncatastrophic consequences and, even worse, they can be hardly fixed as data\nstored on the blockchain, including the smart contract code itself, are\nimmutable. An automated security analysis of these contracts is thus of utmost\ninterest, but at the same time technically challenging for a variety of\nreasons, such as the specific transaction-oriented programming mechanisms,\nwhich feature a subtle semantics, and the fact that the blockchain data which\nthe contract under analysis interacts with, including the code of callers and\ncallees, are not statically known.\n  In this work, we present eThor, the first sound and automated static analyzer\nfor EVM bytecode, which is based on an abstraction of the EVM bytecode\nsemantics based on Horn clauses. In particular, our static analysis supports\nreachability properties, which we show to be sufficient for capturing\ninteresting security properties for smart contracts (e.g., single-entrancy) as\nwell as contract-specific functional properties. Our analysis is proven sound\nagainst a complete semantics of EVM bytecode and an experimental large-scale\nevaluation on real-world contracts demonstrates that eThor is practical and\noutperforms the state-of-the-art static analyzers: specifically, eThor is the\nonly one to provide soundness guarantees, terminates on 95% of a representative\nset of real-world contracts, and achieves an F-measure (which combines\nsensitivity and specificity) of 89%.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 09:40:11 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Schneidewind", "Clara", ""], ["Grishchenko", "Ilya", ""], ["Scherer", "Markus", ""], ["Maffei", "Matteo", ""]]}, {"id": "2005.06261", "submitter": "Ehud Shapiro", "authors": "Luca Cardelli, Liav Orgad, Gal Shahaf, Ehud Shapiro and Nimrod Talmon", "title": "Digital Social Contracts: A Foundation for an Egalitarian and Just\n  Digital Society", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.MA cs.PL cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Almost two centuries ago Pierre-Joseph Proudhon proposed social contracts --\nvoluntary agreements among free people -- as a foundation from which an\negalitarian and just society can emerge. A \\emph{digital social contract} is\nthe novel incarnation of this concept for the digital age: a voluntary\nagreement between people that is specified, undertaken, and fulfilled in the\ndigital realm. It embodies the notion of \"code-is-law\" in its purest form, in\nthat a digital social contract is in fact a program -- code in a social\ncontracts programming language, which specifies the digital actions parties to\nthe social contract may take; and the parties to the contract are entrusted,\nequally, with the task of ensuring that each party abides by the contract.\nParties to a social contract are identified via their public keys, and the one\nand only type of action a party to a digital social contract may take is a\n\"digital speech act\" -- signing an utterance with her private key and sending\nit to the other parties to the contract. Here, we present a formal definition\nof a digital social contract as agents that communicate asynchronously via\ncrypto-speech acts, where the output of each agent is the input of all the\nother agents. We outline an abstract design for a social contracts programming\nlanguage and show, via programming examples, that key application areas,\nincluding social community; simple sharing-economy applications; egalitarian\ncurrency networks; and democratic community governance, can all be expressed\nelegantly and efficiently as digital social contracts.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 11:45:49 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 09:40:06 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2020 09:34:56 GMT"}, {"version": "v4", "created": "Wed, 1 Jul 2020 22:33:03 GMT"}, {"version": "v5", "created": "Sun, 19 Jul 2020 18:42:36 GMT"}, {"version": "v6", "created": "Thu, 17 Sep 2020 08:39:43 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Cardelli", "Luca", ""], ["Orgad", "Liav", ""], ["Shahaf", "Gal", ""], ["Shapiro", "Ehud", ""], ["Talmon", "Nimrod", ""]]}, {"id": "2005.06333", "submitter": "Keigo Imai", "authors": "Keigo Imai, Rumyana Neykova, Nobuko Yoshida and Shoji Yuen", "title": "Multiparty Session Programming with Global Protocol Combinators", "comments": "ECOOP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiparty Session Types (MPST) is a typing discipline for communication\nprotocols. It ensures the absence of communication errors and deadlocks for\nwell-typed communicating processes. The state-of-the-art implementations of the\nMPST theory rely on (1) runtime linearity checks to ensure correct usage of\ncommunication channels and (2) external domain-specific languages for\nspecifying and verifying multiparty protocols. To overcome these limitations,\nwe propose a library for programming with global combinators -- a set of\nfunctions for writing and verifying multiparty protocols in OCaml. Local\nbehaviours for all processes in a protocol are inferred at once from a global\ncombinator. We formalise global combinators and prove a sound realisability of\nglobal combinators -- a well-typed global combinator derives a set of local\ntypes, by which typed endpoint programs can ensure type and communication\nsafety. Our approach enables fully-static verification and implementation of\nthe whole protocol, from the protocol specification to the process\nimplementations, to happen in the same language. We compare our implementation\nto untyped and continuation-passing style implementations, and demonstrate its\nexpressiveness by implementing a plethora of protocols. We show our library can\ninteroperate with existing libraries and services, implementing DNS (Domain\nName Service) protocol and the OAuth (Open Authentication) protocol.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 14:16:00 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 03:13:11 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Imai", "Keigo", ""], ["Neykova", "Rumyana", ""], ["Yoshida", "Nobuko", ""], ["Yuen", "Shoji", ""]]}, {"id": "2005.06334", "submitter": "Stefan Lenz", "authors": "Stefan Lenz, Maren Hackenberg, Harald Binder", "title": "The JuliaConnectoR: a functionally oriented interface for integrating\n  Julia in R", "comments": "23 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG cs.PL stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like many groups considering the new programming language Julia, we faced the\nchallenge of accessing the algorithms that we develop in Julia from R.\nTherefore, we developed the R package JuliaConnectoR, available from the CRAN\nrepository and GitHub (https://github.com/stefan-m-lenz/JuliaConnectoR), in\nparticular for making advanced deep learning tools available. For\nmaintainability and stability, we decided to base communication between R and\nJulia on TCP, using an optimized binary format for exchanging data. Our package\nalso specifically contains features that allow for a convenient interactive use\nin R. This makes it easy to develop R extensions with Julia or to simply call\nfunctionality from Julia packages in R. Interacting with Julia objects and\ncalling Julia functions becomes user-friendly, as Julia functions and variables\nare made directly available as objects in the R workspace. We illustrate the\nfurther features of our package with code examples, and also discuss advantages\nover the two alternative packages JuliaCall and XRJulia. Finally, we\ndemonstrate the usage of the package with a more extensive example for\nemploying neural ordinary differential equations, a recent deep learning\ntechnique that has received much attention. This example also provides more\ngeneral guidance for integrating deep learning techniques from Julia into R.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 14:18:34 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 19:14:29 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Lenz", "Stefan", ""], ["Hackenberg", "Maren", ""], ["Binder", "Harald", ""]]}, {"id": "2005.06444", "submitter": "Luke A. D. Hutchison", "authors": "Luke A. D. Hutchison", "title": "Pika parsing: reformulating packrat parsing as a dynamic programming\n  algorithm solves the left recursion and error recovery problems", "comments": "Submitted to ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recursive descent parser is built from a set of mutually-recursive\nfunctions, where each function directly implements one of the nonterminals of a\ngrammar. A packrat parser uses memoization to reduce the time complexity for\nrecursive descent parsing from exponential to linear in the length of the\ninput. Recursive descent parsers are extremely simple to write, but suffer from\ntwo significant problems: (i) left-recursive grammars cause the parser to get\nstuck in infinite recursion, and (ii) it can be difficult or impossible to\noptimally recover the parse state and continue parsing after a syntax error.\nBoth problems are solved by the pika parser, a novel reformulation of packrat\nparsing as a dynamic programming algorithm, which requires parsing the input in\nreverse: bottom-up and right to left, rather than top-down and left to right.\nThis reversed parsing order enables pika parsers to handle grammars that use\neither direct or indirect left recursion to achieve left associativity,\nsimplifying grammar writing, and also enables optimal recovery from syntax\nerrors, which is a crucial property for IDEs and compilers. Pika parsing\nmaintains the linear-time performance characteristics of packrat parsing as a\nfunction of input length. The pika parser was benchmarked against the\nwidely-used Parboiled2 and ANTLR4 parsing libraries. The pika parser performed\nsignificantly better than the other parsers for an expression grammar, although\nfor a complex grammar implementing the Java language specification, a large\nconstant performance impact was incurred per input character. Therefore, if\nperformance is important, pika parsing is best applied to simple to\nmoderate-sized grammars, or to very large inputs, if other parsing alternatives\ndo not scale linearly in the length of the input. Several new insights into\nprecedence, associativity, and left recursion are presented.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 17:38:47 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 07:04:15 GMT"}, {"version": "v3", "created": "Sun, 31 May 2020 07:47:44 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2020 00:16:12 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Hutchison", "Luke A. D.", ""]]}, {"id": "2005.06496", "submitter": "Ana Milanova", "authors": "Ana Milanova", "title": "FlowCFL: A Framework for Type-based Reachability Analysis in the\n  Presence of Mutable Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reachability analysis is a fundamental program analysis with a wide variety\nof applications. We present FlowCFL, a framework for type-based reachability\nanalysis in the presence of mutable data. Interestingly, the underlying\nsemantics of FlowCFL is CFL-reachability.\n  We make three contributions. First, we define a dynamic semantics that\ncaptures the notion of flow commonly used in reachability analysis. Second, we\nestablish correctness of CFL-reachability over graphs with inverse edges\n(inverse edges are necessary for the handling of mutable heap data). Our\napproach combines CFL-reachability with reference immutability to avoid the\naddition of certain infeasible inverse edges and we demonstrate empirically\nthat avoiding those edges results in precision improvement. Our formal account\nof correctness extends to this case as well. Third, we present a type-based\nreachability analysis and establish equivalence between a certain\nCFL-reachability analysis and the type-based analysis, thus proving correctness\nof the type-based analysis.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 18:07:25 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Milanova", "Ana", ""]]}, {"id": "2005.06645", "submitter": "Michael Vaughn", "authors": "Michael Vaughn and Thomas Reps", "title": "A Generating-Extension-Generator for Machine Code", "comments": "21 pages, 8 Figures Fixed inclusion of LaTeX macro in plaintext\n  abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of \"debloating\" programs for security and performance purposes\nhas begun to see increased attention. Of particular interest in many\nenvironments is debloating commodity off-the-shelf (COTS) software, which is\nmost commonly made available to end users as stripped binaries (i.e., neither\nsource code nor symbol-table/debugging information is available). Toward this\nend, we created a system, called GenXGen[MC], that specializes stripped\nbinaries.\n  Many aspects of the debloating problem can be addressed via techniques from\nthe literature on partial evaluation. However, applying such techniques to\nreal-world programs, particularly stripped binaries, involves non-trivial\nstate-management manipulations that have never been addressed in a completely\nsatisfactory manner in previous systems. In particular, a partial evaluator\nneeds to be able to (i) save and restore arbitrary program states, and (ii)\ndetermine whether a program state is equal to one that arose earlier. Moreover,\nto specialize stripped binaries, the system must also be able to handle program\nstates consisting of memory that is undifferentiated beyond the standard coarse\ndivision into regions for the stack, the heap, and global data.\n  This paper presents a new approach to state management in a program\nspecializer. The technique has been incorporated into GenXGen[MC], a novel tool\nfor producing machine-code generating extensions. Our experiments show that our\nsolution to issue (i) significantly decreases the space required to represent\nprogram states, and our solution to issue (ii) drastically improves the time\nfor producing a specialized program (as much as 13,000x speedup).\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 22:19:04 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 00:53:30 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Vaughn", "Michael", ""], ["Reps", "Thomas", ""]]}, {"id": "2005.06688", "submitter": "Daniel Schemmel", "authors": "Daniel Schemmel, Julian B\\\"uning, C\\'esar Rodr\\'iguez, David Laprell,\n  Klaus Wehrle", "title": "Symbolic Partial-Order Execution for Testing Multi-Threaded Programs", "comments": "Extended version of a paper presented at CAV'20", "journal-ref": null, "doi": "10.1007/978-3-030-53288-8_18", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a technique for systematic testing of multi-threaded programs. We\ncombine Quasi-Optimal Partial-Order Reduction, a state-of-the-art technique\nthat tackles path explosion due to interleaving non-determinism, with symbolic\nexecution to handle data non-determinism. Our technique iteratively and\nexhaustively finds all executions of the program. It represents program\nexecutions using partial orders and finds the next execution using an\nunderlying unfolding semantics. We avoid the exploration of redundant program\ntraces using cutoff events. We implemented our technique as an extension of\nKLEE and evaluated it on a set of large multi-threaded C programs. Our\nexperiments found several previously undiscovered bugs and undefined behaviors\nin memcached and GNU sort, showing that the new method is capable of finding\nbugs in industrial-size benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 02:08:18 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 17:14:00 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Schemmel", "Daniel", ""], ["B\u00fcning", "Julian", ""], ["Rodr\u00edguez", "C\u00e9sar", ""], ["Laprell", "David", ""], ["Wehrle", "Klaus", ""]]}, {"id": "2005.06875", "submitter": "Dragan Ahmetovic", "authors": "Sergio Mascetti and Mattia Ducci and Niccol\\'o Cant\\`u and Paolo Pecis\n  and Dragan Ahmetovic", "title": "Developing Accessible Mobile Applications with Cross-Platform\n  Development Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This contribution investigates how cross-platform development frameworks\n(CPDF) support the creation of mobile applications that are accessible to\npeople with visual impairments through screen readers. We first systematically\nanalyze screen-reader APIs available in native iOS and Android, and we examine\nwhether and at what level the same functionalities are available in two popular\nCPDF: Xamarin and React Native. This analysis unveils that there are many\nfunctionalities shared between native iOS and Android APIs, but most of them\nare not available in React Native or Xamarin. In particular, not even all basic\nAPIs are exposed by the examined CPDF.\n  Accessing the unavailable APIs is still possible, but it requires an\nadditional effort by the developers who need to know native APIs and to write\nplatform specific code, hence partially negating the advantages of CPDF. To\naddress this problem, we consider a representative set of native APIs that\ncannot be directly accessed from React Native and Xamarin and show sample\nimplementations for accessing them.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 11:19:37 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Mascetti", "Sergio", ""], ["Ducci", "Mattia", ""], ["Cant\u00f9", "Niccol\u00f3", ""], ["Pecis", "Paolo", ""], ["Ahmetovic", "Dragan", ""]]}, {"id": "2005.06980", "submitter": "Rajarshi Haldar", "authors": "Rajarshi Haldar, Lingfei Wu, Jinjun Xiong and Julia Hockenmaier", "title": "A Multi-Perspective Architecture for Semantic Code Search", "comments": "ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.LG cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to match pieces of code to their corresponding natural language\ndescriptions and vice versa is fundamental for natural language search\ninterfaces to software repositories. In this paper, we propose a novel\nmulti-perspective cross-lingual neural framework for code--text matching,\ninspired in part by a previous model for monolingual text-to-text matching, to\ncapture both global and local similarities. Our experiments on the CoNaLa\ndataset show that our proposed model yields better performance on this\ncross-lingual text-to-code matching task than previous approaches that map code\nand text to a single joint embedding space.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 04:46:11 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Haldar", "Rajarshi", ""], ["Wu", "Lingfei", ""], ["Xiong", "Jinjun", ""], ["Hockenmaier", "Julia", ""]]}, {"id": "2005.07173", "submitter": "Daniel Fremont", "authors": "Daniel J. Fremont, Johnathan Chiu, Dragos D. Margineantu, Denis\n  Osipychev, and Sanjit A. Seshia", "title": "Formal Analysis and Redesign of a Neural Network-Based Aircraft Taxiing\n  System with VerifAI", "comments": "Full version of a CAV 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL cs.SY eess.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a unified approach to rigorous design of safety-critical\nautonomous systems using the VerifAI toolkit for formal analysis of AI-based\nsystems. VerifAI provides an integrated toolchain for tasks spanning the design\nprocess, including modeling, falsification, debugging, and ML component\nretraining. We evaluate all of these applications in an industrial case study\non an experimental autonomous aircraft taxiing system developed by Boeing,\nwhich uses a neural network to track the centerline of a runway. We define\nrunway scenarios using the Scenic probabilistic programming language, and use\nthem to drive tests in the X-Plane flight simulator. We first perform\nfalsification, automatically finding environment conditions causing the system\nto violate its specification by deviating significantly from the centerline (or\neven leaving the runway entirely). Next, we use counterexample analysis to\nidentify distinct failure cases, and confirm their root causes with specialized\ntesting. Finally, we use the results of falsification and debugging to retrain\nthe network, eliminating several failure cases and improving the overall\nperformance of the closed-loop system.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 17:42:14 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Fremont", "Daniel J.", ""], ["Chiu", "Johnathan", ""], ["Margineantu", "Dragos D.", ""], ["Osipychev", "Denis", ""], ["Seshia", "Sanjit A.", ""]]}, {"id": "2005.07464", "submitter": "Joel Colloc", "authors": "Jo\\\"el Colloc (IDEES), Danielle Boulanger", "title": "An Object Model for the Representation of Empirical Knowledge", "comments": "in French. Colloque International ICO'89, Jun 1989, Quebec, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are currently designing an object oriented model which describes static\nand dynamical knowledge in diff{\\'e}rent domains. It provides a twin conceptual\nlevel. The internal level proposes: the object structure composed of\nsub-objects hierarchy, structure evolution with dynamical functions, same type\nobjects comparison with evaluation functions. It uses multiple upward\ninheritance from sub-objects properties to the Object. The external level\ndescribes: object environment, it enforces object types and uses external\nsimple inheritance from the type to the sub-types.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 10:45:58 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Colloc", "Jo\u00ebl", "", "IDEES"], ["Boulanger", "Danielle", ""]]}, {"id": "2005.08063", "submitter": "Prantik Chatterjee", "authors": "Prantik Chatterjee, Subhajit Roy, Bui Phi Diep, Akash Lal", "title": "Distributed Bounded Model Checking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program verification is a resource-hungry task. This paper looks at the\nproblem of parallelizing SMT-based automated program verification, specifically\nbounded model-checking, so that it can be distributed and executed on a cluster\nof machines. We present an algorithm that dynamically unfolds the call graph of\nthe program and frequently splits it to create sub-tasks that can be solved in\nparallel. The algorithm is adaptive, controlling the splitting rate according\nto available resources, and also leverages information from the SMT solver to\nsplit where most complexity lies in the search. We implemented our algorithm by\nmodifying CORRAL, the verifier used by Microsoft's Static Driver Verifier\n(SDV), and evaluate it on a series of hard SDV benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 18:33:48 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Chatterjee", "Prantik", ""], ["Roy", "Subhajit", ""], ["Diep", "Bui Phi", ""], ["Lal", "Akash", ""]]}, {"id": "2005.08211", "submitter": "Sarah McDaid PhD", "authors": "Edward McDaid and Sarah McDaid", "title": "Quantifying the Impact on Software Complexity of Composable Inductive\n  Programming using Zoea", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Composable inductive programming as implemented in the Zoea programming\nlanguage is a simple declarative approach to software development. At the\nlanguage level it is evident that Zoea is significantly simpler than all\nmainstream languages. However, until now we have only had anecdotal evidence\nthat software produced with Zoea is also simpler than equivalent software\nproduced with conventional languages. This paper presents the results of a\nquantitative comparison of the software complexity of equivalent code\nimplemented in Zoea and also in a conventional programming language. The study\nuses a varied set of programming tasks from a popular programming language\nchrestomathy. Results are presented for relative program complexity using two\nestablished metrics and also for relative program size. It was found that Zoea\nprograms are approximately 50% the complexity of equivalent programs in a\nconventional language and on average equal in size. The results suggest that\ncurrent programming languages (as opposed to software requirements) are the\nlargest contributor to software complexity and that significant complexity\ncould be avoided through an inductive programming approach.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 10:44:39 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["McDaid", "Edward", ""], ["McDaid", "Sarah", ""]]}, {"id": "2005.08384", "submitter": "Christian Anti\\'c", "authors": "Christian Anti\\'c", "title": "Fixed Point Semantics for Stream Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning over streams of input data is an essential part of human\nintelligence. During the last decade {\\em stream reasoning} has emerged as a\nresearch area within the AI-community with many potential applications. In\nfact, the increased availability of streaming data via services like Google and\nFacebook has raised the need for reasoning engines coping with data that\nchanges at high rate. Recently, the rule-based formalism {\\em LARS} for\nnon-monotonic stream reasoning under the answer set semantics has been\nintroduced. Syntactically, LARS programs are logic programs with negation\nincorporating operators for temporal reasoning, most notably {\\em window\noperators} for selecting relevant time points. Unfortunately, by preselecting\n{\\em fixed} intervals for the semantic evaluation of programs, the rigid\nsemantics of LARS programs is not flexible enough to {\\em constructively} cope\nwith rapidly changing data dependencies. Moreover, we show that defining the\nanswer set semantics of LARS in terms of FLP reducts leads to undesirable\ncircular justifications similar to other ASP extensions. This paper fixes all\nof the aforementioned shortcomings of LARS. More precisely, we contribute to\nthe foundations of stream reasoning by providing an operational fixed point\nsemantics for a fully flexible variant of LARS and we show that our semantics\nis sound and constructive in the sense that answer sets are derivable bottom-up\nand free of circular justifications.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 22:25:24 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Anti\u0107", "Christian", ""]]}, {"id": "2005.08396", "submitter": "Peng Fu", "authors": "Peng Fu, Kohei Kishida, Neil J. Ross, Peter Selinger", "title": "A tutorial introduction to quantum circuit programming in dependently\n  typed Proto-Quipper", "comments": "Added a section on related work and a paragraph explaining qubit\n  initialization and termination", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce dependently typed Proto-Quipper, or Proto-Quipper-D for short,\nan experimental quantum circuit programming language with linear dependent\ntypes. We give several examples to illustrate how linear dependent types can\nhelp in the construction of correct quantum circuits. Specifically, we show how\ndependent types enable programming families of circuits, and how dependent\ntypes solve the problem of type-safe uncomputation of garbage qubits. We also\ndiscuss other language features along the way.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 23:31:23 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 18:31:19 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Fu", "Peng", ""], ["Kishida", "Kohei", ""], ["Ross", "Neil J.", ""], ["Selinger", "Peter", ""]]}, {"id": "2005.09013", "submitter": "Marcin Szymczak", "authors": "Marcin Szymczak and Joost-Pieter Katoen", "title": "Weakest Preexpectation Semantics for Bayesian Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a semantics of a probabilistic while-language with soft\nconditioning and continuous distributions which handles programs diverging with\npositive probability. To this end, we extend the probabilistic guarded command\nlanguage (pGCL) with draws from continuous distributions and a score operator.\nThe main contribution is an extension of the standard weakest preexpectation\nsemantics to support these constructs. As a sanity check of our semantics, we\ndefine an alternative trace-based semantics of the language, and show that the\ntwo semantics are equivalent. Various examples illustrate the applicability of\nthe semantics.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 18:22:13 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Szymczak", "Marcin", ""], ["Katoen", "Joost-Pieter", ""]]}, {"id": "2005.09028", "submitter": "Rajan Walia", "authors": "Rajan Walia (Indiana University, USA), Chung-chieh Shan (Indiana\n  University, USA), Sam Tobin-Hochstadt (Indiana University, USA)", "title": "Sham: A DSL for Fast DSLs", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2022, Vol. 6,\n  Issue 1, Article 4", "doi": "10.22152/programming-journal.org/2022/6/4", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain-specific languages (DSLs) are touted as both easy to embed in programs\nand easy to optimize. Yet these goals are often in tension. Embedded or\ninternal DSLs fit naturally with a host language, while inheriting the host's\nperformance characteristics. External DSLs can use external optimizers and\nlanguages but sit apart from the host. We present Sham, a toolkit designed to\nenable internal DSLs with high performance. Sham provides a domain-specific\nlanguage (embedded in Racket) for implementing other high-performance DSLs,\nwith transparent compilation to assembly code at runtime. Sham is well suited\nas both a compilation target for other embedded DSLs and for transparently\nreplacing DSL support code with faster versions. Sham provides seamless\ninter-operation with its host language without requiring any additional effort\nfrom its users. Sham also provides a framework for defining language syntax\nwhich implements Sham's own language interface as well. We validate Sham's\ndesign on a series of case studies, ranging from Krishnamurthi's classic\nautomata DSL to a sound synthesis DSL and a probabilistic programming language.\nAll of these are existing DSLs where we replaced the backend using Sham,\nresulting in major performance gains. We present an example-driven description\nof how Sham can smoothly enhance an existing DSL into a high-performance one.\nWhen compared to existing approaches for implementing high-performance DSLs,\nSham's design aims for both simplicity and programmer control. This makes it\neasier to port our techniques to other languages and frameworks, or borrow\nSham's innovations \"\\`a la carte\" without adopting the whole approach. Sham\nbuilds a sophisticated and powerful DSL construction toolkit atop fundamental\nlanguage features including higher-order functions, data structures, and a\nforeign-function interface (FFI), all readily available in other languages.\nFurthermore, Sham's approach allows DSL developers to simply write functions,\neither using Sham or generating Sham, without needing to work through complex\nstaging or partial evaluation systems.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 18:49:58 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 13:35:43 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Walia", "Rajan", "", "Indiana University, USA"], ["Shan", "Chung-chieh", "", "Indiana\n  University, USA"], ["Tobin-Hochstadt", "Sam", "", "Indiana University, USA"]]}, {"id": "2005.09089", "submitter": "Steven Holtzen", "authors": "Steven Holtzen and Guy Van den Broeck and Todd Millstein", "title": "Scaling Exact Inference for Discrete Probabilistic Programs", "comments": null, "journal-ref": "Proc. ACM Program. Lang. 4, OOPSLA (2020)", "doi": "10.1145/3428208", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic programming languages (PPLs) are an expressive means of\nrepresenting and reasoning about probabilistic models. The computational\nchallenge of probabilistic inference remains the primary roadblock for applying\nPPLs in practice. Inference is fundamentally hard, so there is no one-size-fits\nall solution. In this work, we target scalable inference for an important class\nof probabilistic programs: those whose probability distributions are discrete.\nDiscrete distributions are common in many fields, including text analysis,\nnetwork verification, artificial intelligence, and graph analysis, but they\nprove to be challenging for existing PPLs.\n  We develop a domain-specific probabilistic programming language called Dice\nthat features a new approach to exact discrete probabilistic program inference.\nDice exploits program structure in order to factorize inference, enabling us to\nperform exact inference on probabilistic programs with hundreds of thousands of\nrandom variables. Our key technical contribution is a new reduction from\ndiscrete probabilistic programs to weighted model counting (WMC). This\nreduction separates the structure of the distribution from its parameters,\nenabling logical reasoning tools to exploit that structure for probabilistic\ninference. We (1) show how to compositionally reduce Dice inference to WMC, (2)\nprove this compilation correct with respect to a denotational semantics, (3)\nempirically demonstrate the performance benefits over prior approaches, and (4)\nanalyze the types of structure that allow Dice to scale to large probabilistic\nprograms.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 21:02:52 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 20:51:44 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 22:01:27 GMT"}, {"version": "v4", "created": "Fri, 16 Oct 2020 16:49:20 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Holtzen", "Steven", ""], ["Broeck", "Guy Van den", ""], ["Millstein", "Todd", ""]]}, {"id": "2005.09452", "submitter": "Boro Sitnikovski", "authors": "Boro Sitnikovski, Biljana Stojcevska, Lidija Goracinova-Ilieva, Irena\n  Stojmenovska", "title": "PubSub implementation in Haskell with formal verification in Coq", "comments": "4 pages, accepted for presentation at the CIIT 2020, 17th\n  International Conference on Informatics and Information Technologies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the cloud, the technology is used on-demand without the need to install\nanything on the desktop. Software as a Service is one of the many cloud\narchitectures. The PubSub messaging pattern is a cloud-based Software as a\nService solution used in complex systems, especially in the notifications part\nwhere there is a need to send a message from one unit to another single unit or\nmultiple units. Haskell is a generic typed programming language which has\npioneered several advanced programming language features. Based on the lambda\ncalculus system, it belongs to the family of functional programming languages.\nCoq, also based on a stricter version of lambda calculus, is a programming\nlanguage that has a more advanced type system than Haskell and is mainly used\nfor theorem proving i.e. proving software correctness. This paper aims to show\nhow PubSub can be used in conjunction with cloud computing (Software as a\nService), as well as to present an example implementation in Haskell and proof\nof correctness in Coq.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 09:45:24 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Sitnikovski", "Boro", ""], ["Stojcevska", "Biljana", ""], ["Goracinova-Ilieva", "Lidija", ""], ["Stojmenovska", "Irena", ""]]}, {"id": "2005.09478", "submitter": "Kacper Topolnicki", "authors": "Kacper Topolnicki", "title": "Monads and \"do\" notation in the Wolfram Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a categorical interpretation of the Wolfram Language and\nintroduces a simple implementation of monadic types and the \"do\" notation. The\nmonadic style of programming combined with the many built in functions of the\nWolfram Language has potential to be a powerful tool in writing Wolfram\nLanguage code. Additionally, using pure functions and the \"do\" notation can\nresult in programs that are very predictable and easy to parallelize.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 09:32:48 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Topolnicki", "Kacper", ""]]}, {"id": "2005.09516", "submitter": "Tristan Bruns", "authors": "S\\\"oren Tempel (University of Bremen), Tristan Bruns (University of\n  Bremen)", "title": "RIOT-POLICE: An implementation of spatial memory safety for the RIOT\n  operating system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an integration of a safe C dialect, Checked C, for the Internet of\nThings operating system RIOT. We utilize this integration to convert parts of\nthe RIOT network stack to Checked C, thereby achieving spatial memory safety in\nthese code parts. Similar to prior research done on IoT operating systems and\nsafe C dialects, our integration of Checked C remains entirely optional, i.e.\ncompilation with a standard C compiler not supporting the Checked C language\nextension is still possible. We believe this to be the first proposed\nintegration of a safe C dialect for the RIOT operating system. We present an\nincremental process for converting RIOT modules to Checked C, evaluate the\noverhead introduced by the conversions, and discuss our general experience with\nutilizing Checked C in the Internet of Things domain.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 15:22:59 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Tempel", "S\u00f6ren", "", "University of Bremen"], ["Bruns", "Tristan", "", "University of\n  Bremen"]]}, {"id": "2005.09520", "submitter": "Marco Peressotti", "authors": "Saverio Giallorenzo, Fabrizio Montesi, Marco Peressotti", "title": "Choreographies as Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Choral, the first language for programming choreographies\n(multiparty protocols) that builds on top of mainstream programming\nabstractions: in Choral, choreographies are objects. Given a choreography that\ndefines interactions among some roles (Alice, Bob, etc.), an implementation for\neach role in the choreography is automatically generated by a compiler. These\nimplementations are libraries in pure Java, which developers can modularly\ncompose in their own programs to participate correctly in choreographies.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 15:27:22 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 17:44:32 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Giallorenzo", "Saverio", ""], ["Montesi", "Fabrizio", ""], ["Peressotti", "Marco", ""]]}, {"id": "2005.09997", "submitter": "Yu Wang", "authors": "Yu Wang, Fengjuan Gao, Linzhang Wang, Ke Wang", "title": "Learning Semantic Program Embeddings with Graph Interval Neural Network", "comments": "The abstract is simplified, for full abstract, please refer to the\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning distributed representations of source code has been a challenging\ntask for machine learning models. Earlier works treated programs as text so\nthat natural language methods can be readily applied. Unfortunately, such\napproaches do not capitalize on the rich structural information possessed by\nsource code. Of late, Graph Neural Network (GNN) was proposed to learn\nembeddings of programs from their graph representations. Due to the homogeneous\nand expensive message-passing procedure, GNN can suffer from precision issues,\nespecially when dealing with programs rendered into large graphs. In this\npaper, we present a new graph neural architecture, called Graph Interval Neural\nNetwork (GINN), to tackle the weaknesses of the existing GNN. Unlike the\nstandard GNN, GINN generalizes from a curated graph representation obtained\nthrough an abstraction method designed to aid models to learn. In particular,\nGINN focuses exclusively on intervals for mining the feature representation of\na program, furthermore, GINN operates on a hierarchy of intervals for scaling\nthe learning to large graphs. We evaluate GINN for two popular downstream\napplications: variable misuse prediction and method name prediction. Results\nshow in both cases GINN outperforms the state-of-the-art models by a\ncomfortable margin. We have also created a neural bug detector based on GINN to\ncatch null pointer deference bugs in Java code. While learning from the same\n9,000 methods extracted from 64 projects, GINN-based bug detector significantly\noutperforms GNN-based bug detector on 13 unseen test projects. Next, we deploy\nour trained GINN-based bug detector and Facebook Infer to scan the codebase of\n20 highly starred projects on GitHub. Through our manual inspection, we confirm\n38 bugs out of 102 warnings raised by GINN-based bug detector compared to 34\nbugs out of 129 warnings for Facebook Infer.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 02:38:34 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 02:11:25 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Wang", "Yu", ""], ["Gao", "Fengjuan", ""], ["Wang", "Linzhang", ""], ["Wang", "Ke", ""]]}, {"id": "2005.10382", "submitter": "\\'Akos Hajdu", "authors": "\\'Akos Hajdu, Dejan Jovanovi\\'c, Gabriela Ciocarlie", "title": "Formal Specification and Verification of Solidity Contracts with Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Events in the Solidity language provide a means of communication between the\non-chain services of decentralized applications and the users of those\nservices. Events are commonly used as an abstraction of contract execution that\nis relevant from the users' perspective. Users must, therefore, be able to\nunderstand the meaning and trust the validity of the emitted events. This paper\npresents a source-level approach for the formal specification and verification\nof Solidity contracts with the primary focus on events. Our approach allows\nspecification of events in terms of the on-chain data that they track, and\npredicates that define the correspondence between the blockchain state and the\nabstract view provided by the events. The approach is implemented in\nsolc-verify, a modular verifier for Solidity, and we demonstrate its\napplicability with various examples.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 22:48:25 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Hajdu", "\u00c1kos", ""], ["Jovanovi\u0107", "Dejan", ""], ["Ciocarlie", "Gabriela", ""]]}, {"id": "2005.10554", "submitter": "Conrad Watt", "authors": "Conrad Watt, Christopher Pulte, Anton Podkopaev, Guillaume Barbier,\n  Stephen Dolan, Shaked Flur, Jean Pichon-Pharabod, Shu-yu Guo", "title": "Repairing and Mechanising the JavaScript Relaxed Memory Model", "comments": "16 pages, 13 figiures", "journal-ref": null, "doi": "10.1145/3385412.3385973", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern JavaScript includes the SharedArrayBuffer feature, which provides\naccess to true shared memory concurrency. SharedArrayBuffers are simple linear\nbuffers of bytes, and the JavaScript specification defines an axiomatic relaxed\nmemory model to describe their behaviour. While this model is heavily based on\nthe C/C++11 model, it diverges in some key areas. JavaScript chooses to give a\nwell-defined semantics to data-races, unlike the \"undefined behaviour\" of\nC/C++11. Moreover, the JavaScript model is mixed-size. This means that its\naccesses are not to discrete locations, but to (possibly overlapping) ranges of\nbytes.\n  We show that the model, in violation of the design intention, does not\nsupport a compilation scheme to ARMv8 which is used in practice. We propose a\ncorrection, which also incorporates a previously proposed fix for a failure of\nthe model to provide Sequential Consistency of Data-Race-Free programs\n(SC-DRF), an important correctness condition. We use model checking, in Alloy,\nto generate small counter-examples for these deficiencies, and investigate our\ncorrection. To accomplish this, we also develop a mixed-size extension to the\nexisting ARMv8 axiomatic model.\n  Guided by our Alloy experimentation, we mechanise (in Coq) the JavaScript\nmodel (corrected and uncorrected), our ARMv8 model, and, for the corrected\nJavaScript model, a \"model-internal\" SC-DRF proof and a compilation scheme\ncorrectness proof to ARMv8. In addition, we investigate a non-mixed-size subset\nof the corrected JavaScript model, and give proofs of compilation correctness\nfor this subset to x86-TSO, Power, RISC-V, ARMv7, and (again) ARMv8, via the\nIntermediate Memory Model (IMM).\n  As a result of our work, the JavaScript standards body (ECMA TC39) will\ninclude fixes for both issues in an upcoming edition of the specification.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 10:27:24 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 01:37:14 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Watt", "Conrad", ""], ["Pulte", "Christopher", ""], ["Podkopaev", "Anton", ""], ["Barbier", "Guillaume", ""], ["Dolan", "Stephen", ""], ["Flur", "Shaked", ""], ["Pichon-Pharabod", "Jean", ""], ["Guo", "Shu-yu", ""]]}, {"id": "2005.10636", "submitter": "Michihiro Yasunaga", "authors": "Michihiro Yasunaga, Percy Liang", "title": "Graph-based, Self-Supervised Program Repair from Diagnostic Feedback", "comments": "ICML 2020. Code & data available at\n  https://github.com/michiyasunaga/DrRepair", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CL cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning to repair programs from diagnostic\nfeedback (e.g., compiler error messages). Program repair is challenging for two\nreasons: First, it requires reasoning and tracking symbols across source code\nand diagnostic feedback. Second, labeled datasets available for program repair\nare relatively small. In this work, we propose novel solutions to these two\nchallenges. First, we introduce a program-feedback graph, which connects\nsymbols relevant to program repair in source code and diagnostic feedback, and\nthen apply a graph neural network on top to model the reasoning process.\nSecond, we present a self-supervised learning paradigm for program repair that\nleverages unlabeled programs available online to create a large amount of extra\nprogram repair examples, which we use to pre-train our models. We evaluate our\nproposed approach on two applications: correcting introductory programming\nassignments (DeepFix dataset) and correcting the outputs of program synthesis\n(SPoC dataset). Our final system, DrRepair, significantly outperforms prior\nwork, achieving 68.2% full repair rate on DeepFix (+22.9% over the prior best),\nand 48.4% synthesis success rate on SPoC (+3.7% over the prior best).\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 07:24:28 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 05:30:33 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Yasunaga", "Michihiro", ""], ["Liang", "Percy", ""]]}, {"id": "2005.11023", "submitter": "Wenjun Shi", "authors": "Wenjun Shi, Qinxiang Cao, Yuxin Deng, Hanru Jiang and Yuan Feng", "title": "Symbolic Reasoning about Quantum Circuits in Coq", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A quantum circuit is a computational unit that transforms an input quantum\nstate to an output one. A natural way to reason about its behavior is to\ncompute explicitly the unitary matrix implemented by it. However, when the\nnumber of qubits increases, the matrix dimension grows exponentially and the\ncomputation becomes intractable.\n  In this paper, we propose a symbolic approach to reasoning about quantum\ncircuits. It is based on a small set of laws involving some basic manipulations\non vectors and matrices. This symbolic reasoning scales better than the\nexplicit one and is well suited to be automated in Coq, as demonstrated with\nsome typical examples.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 06:27:52 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 08:36:41 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 08:00:35 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Shi", "Wenjun", ""], ["Cao", "Qinxiang", ""], ["Deng", "Yuxin", ""], ["Jiang", "Hanru", ""], ["Feng", "Yuan", ""]]}, {"id": "2005.11444", "submitter": "Colin Gordon", "authors": "Colin S. Gordon", "title": "Designing with Static Capabilities and Effects: Use, Mention, and\n  Invariants", "comments": "Preprint of ECOOP 2020 paper", "journal-ref": null, "doi": "10.4230/LIPIcs.ECOOP.2020.10", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capabilities (whether object or reference capabilities) are fundamentally\ntools to restrict effects. Thus static capabilities (object or reference) and\neffect systems take different technical machinery to the same core problem of\nstatically restricting or reasoning about effects in programs. Any time two\napproaches can in principle address the same sets of problems, it becomes\nimportant to understand the trade-offs between the approaches, how these\ntrade-offs might interact with the problem at hand.\n  Experts who have worked in these areas tend to find the trade-offs somewhat\nobvious, having considered them in context before. However, this kind of design\ndiscussion is often written down only implicitly as comparison between two\napproaches for a specific program reasoning problem, rather than as a\ndiscussion of general trade-offs between general classes of techniques. As a\nresult, it is not uncommon to set out to solve a problem with one technique,\nonly to find the other better-suited.\n  We discuss the trade-offs between static capabilities (specifically reference\ncapabilities) and effect systems, articulating the challenges each approach\ntends to have in isolation, and how these are sometimes mitigated. We also put\nour discussion in context, by appealing to examples of how these trade-offs\nwere considered in the course of developing prior systems in the area. Along\nthe way, we highlight how seemingly-minor aspects of type systems --\nweakening/framing and the mere existence of type contexts -- play a subtle role\nin the efficacy of these systems.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 01:52:03 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 17:32:07 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Gordon", "Colin S.", ""]]}, {"id": "2005.11644", "submitter": "Brandon T. Willard", "authors": "Brandon T. Willard", "title": "miniKanren as a Tool for Symbolic Computation in Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we give a brief overview of the current state and future\npotential of symbolic computation within the Python statistical modeling and\nmachine learning community. We detail the use of miniKanren as an underlying\nframework for term rewriting and symbolic mathematics, as well as its ability\nto orchestrate the use of existing Python libraries. We also discuss the\nrelevance and potential of relational programming for implementing more robust,\nportable, domain-specific \"math-level\" optimizations--with a slight focus on\nBayesian modeling. Finally, we describe the work going forward and raise some\nquestions regarding potential cross-overs between statistical modeling and\nprogramming language theory.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 03:09:08 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 18:58:39 GMT"}, {"version": "v3", "created": "Thu, 28 May 2020 20:41:06 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Willard", "Brandon T.", ""]]}, {"id": "2005.11710", "submitter": "Julien Lange", "authors": "Robert Griesemer, Raymond Hu, Wen Kokke, Julien Lange, Ian Lance\n  Taylor, Bernardo Toninho, Philip Wadler and Nobuko Yoshida", "title": "Featherweight Go", "comments": "Full version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a design for generics in Go inspired by previous work on\nFeatherweight Java by Igarashi, Pierce, and Wadler. Whereas subtyping in Java\nis nominal, in Go it is structural, and whereas generics in Java are defined\nvia erasure, in Go we use monomorphisation. Although monomorphisation is widely\nused, we are one of the first to formalise it. Our design also supports a\nsolution to The Expression Problem.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 10:31:39 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 18:34:23 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 12:49:29 GMT"}, {"version": "v4", "created": "Mon, 19 Oct 2020 17:04:10 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Griesemer", "Robert", ""], ["Hu", "Raymond", ""], ["Kokke", "Wen", ""], ["Lange", "Julien", ""], ["Taylor", "Ian Lance", ""], ["Toninho", "Bernardo", ""], ["Wadler", "Philip", ""], ["Yoshida", "Nobuko", ""]]}, {"id": "2005.11821", "submitter": "P\\'eter Bereczky", "authors": "P\\'eter Bereczky, D\\'aniel Horp\\'acsi, Simon Thompson", "title": "A Proof Assistant Based Formalisation of Core Erlang", "comments": "21st International Symposium on Trends in Functional Programming", "journal-ref": "In: Byrski A., Hughes J. (eds) Trends in Functional Programming.\n  TFP 2020. Lecture Notes in Computer Science, vol 12222. Springer, Cham", "doi": "10.1007/978-3-030-57761-2_7", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our research is part of a wider project that aims to investigate and reason\nabout the correctness of scheme-based source code transformations of Erlang\nprograms. In order to formally reason about the definition of a programming\nlanguage and the software built using it, we need a mathematically rigorous\ndescription of that language.\n  In this paper, we present our proof-assistant-based formalisation of a subset\nof Erlang, intended to serve as a base for proving refactorings correct. After\ndiscussing how we reused concepts from related work, we show the syntax and\nsemantics of our formal description, including the abstractions involved (e.g.\nclosures). We also present essential properties of the formalisation (e.g.\ndeterminism) along with their machine-checked proofs. Finally, we prove the\ncorrectness of some simple refactoring strategies.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 18:35:16 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 11:31:45 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Bereczky", "P\u00e9ter", ""], ["Horp\u00e1csi", "D\u00e1niel", ""], ["Thompson", "Simon", ""]]}, {"id": "2005.11839", "submitter": "Jo\\~ao Reis", "authors": "Jo\\~ao Santos Reis and Paul Crocker and Sim\\~ao Melo de Sousa", "title": "Tezla, an Intermediate Representation for Static Analysis of Michelson\n  Smart Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Tezla, an intermediate representation of Michelson\nsmart contracts that eases the design of static smart contract analysers. This\nintermediate representation uses a store and preserves the semantics, ow and\nresource usage of the original smart contract. This enables properties like gas\nconsumption to be statically verified. We provide an automated decompiler of\nMichelson smart contracts to Tezla. In order to support our claim about the\nadequacy of Tezla, we develop a static analyser that takes advantage of the\nTezla representation of Michelson smart contracts to prove simple but\nnon-trivial properties.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 20:49:13 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Reis", "Jo\u00e3o Santos", ""], ["Crocker", "Paul", ""], ["de Sousa", "Sim\u00e3o Melo", ""]]}, {"id": "2005.13014", "submitter": "Tobias Gysi", "authors": "Tobias Gysi, Christoph M\\\"uller, Oleksandr Zinenko, Stephan Herhut,\n  Eddie Davis, Tobias Wicky, Oliver Fuhrer, Torsten Hoefler, Tobias Grosser", "title": "Domain-Specific Multi-Level IR Rewriting for GPU", "comments": "12 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional compilers operate on a single generic intermediate representation\n(IR). These IRs are usually low-level and close to machine instructions. As a\nresult, optimizations relying on domain-specific information are either not\npossible or require complex analysis to recover the missing information. In\ncontrast, multi-level rewriting instantiates a hierarchy of dialects (IRs),\nlowers programs level-by-level, and performs code transformations at the most\nsuitable level. We demonstrate the effectiveness of this approach for the\nweather and climate domain. In particular, we develop a prototype compiler and\ndesign stencil- and GPU-specific dialects based on a set of newly introduced\ndesign principles. We find that two domain-specific optimizations (500 lines of\ncode) realized on top of LLVM's extensible MLIR compiler infrastructure suffice\nto outperform state-of-the-art solutions. In essence, multi-level rewriting\npromises to herald the age of specialized compilers composed from domain- and\ntarget-specific dialects implemented on top of a shared infrastructure.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 20:21:48 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 11:36:09 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Gysi", "Tobias", ""], ["M\u00fcller", "Christoph", ""], ["Zinenko", "Oleksandr", ""], ["Herhut", "Stephan", ""], ["Davis", "Eddie", ""], ["Wicky", "Tobias", ""], ["Fuhrer", "Oliver", ""], ["Hoefler", "Torsten", ""], ["Grosser", "Tobias", ""]]}, {"id": "2005.13057", "submitter": "Mallku Soldevila", "authors": "Mallku Soldevila, Beta Ziliani, Daniel Fridlender", "title": "Understanding Lua's Garbage Collection -- Towards a Formalized Static\n  Analyzer", "comments": "Submitted for revision to PPDP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We provide the semantics of garbage collection (GC) for the Lua programming\nlanguage. Of interest are the inclusion of finalizers(akin to destructors in\nobject-oriented languages) and weak tables (a particular implementation of weak\nreferences). The model expresses several aspects relevant to GC that are not\ncovered in Lua's documentation but that, nevertheless, affect the observable\nbehavior of programs. Our model is mechanized and can be tested with real\nprograms. Our long-term goal is to provide a formalized static analyzer of Lua\nprograms to detect potential dangers. As a first step, we provide a prototype\ntool, LuaSafe, that typechecks programs to ensure their behavior is not\naffected by GC. Our model of GC is validated in practice by the experimentation\nwith its mechanization, and in theory by proving several soundness properties.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 21:44:39 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Soldevila", "Mallku", ""], ["Ziliani", "Beta", ""], ["Fridlender", "Daniel", ""]]}, {"id": "2005.13209", "submitter": "Shaked Brody", "authors": "Shaked Brody, Uri Alon and Eran Yahav", "title": "A Structural Model for Contextual Code Changes", "comments": "Accepted to OOPSLA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of predicting edit completions based on a learned\nmodel that was trained on past edits. Given a code snippet that is partially\nedited, our goal is to predict a completion of the edit for the rest of the\nsnippet. We refer to this task as the EditCompletion task and present a novel\napproach for tackling it. The main idea is to directly represent structural\nedits. This allows us to model the likelihood of the edit itself, rather than\nlearning the likelihood of the edited code. We represent an edit operation as a\npath in the program's Abstract Syntax Tree (AST), originating from the source\nof the edit to the target of the edit. Using this representation, we present a\npowerful and lightweight neural model for the EditCompletion task.\n  We conduct a thorough evaluation, comparing our approach to a variety of\nrepresentation and modeling approaches that are driven by multiple strong\nmodels such as LSTMs, Transformers, and neural CRFs. Our experiments show that\nour model achieves a 28% relative gain over state-of-the-art sequential models\nand 2x higher accuracy than syntactic models that learn to generate the edited\ncode, as opposed to modeling the edits directly.\n  Our code, dataset, and trained models are publicly available at\nhttps://github.com/tech-srl/c3po/ .\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 07:16:19 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 17:52:10 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Brody", "Shaked", ""], ["Alon", "Uri", ""], ["Yahav", "Eran", ""]]}, {"id": "2005.13283", "submitter": "Nader Khammassi", "authors": "N. Khammassi, I. Ashraf, J. v. Someren, R. Nane, A. M. Krol, M. A.\n  Rol, L. Lao, K. Bertels, C. G. Almudever", "title": "OpenQL : A Portable Quantum Programming Framework for Quantum\n  Accelerators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the potential of quantum algorithms to solve intractable classical\nproblems, quantum computing is rapidly evolving and more algorithms are being\ndeveloped and optimized. Expressing these quantum algorithms using a high-level\nlanguage and making them executable on a quantum processor while abstracting\naway hardware details is a challenging task. Firstly, a quantum programming\nlanguage should provide an intuitive programming interface to describe those\nalgorithms. Then a compiler has to transform the program into a quantum\ncircuit, optimize it and map it to the target quantum processor respecting the\nhardware constraints such as the supported quantum operations, the qubit\nconnectivity, and the control electronics limitations. In this paper, we\npropose a quantum programming framework named OpenQL, which includes a\nhigh-level quantum programming language and its associated quantum compiler. We\npresent the programming interface of OpenQL, we describe the different layers\nof the compiler and how we can provide portability over different qubit\ntechnologies. Our experiments show that OpenQL allows the execution of the same\nhigh-level algorithm on two different qubit technologies, namely\nsuperconducting qubits and Si-Spin qubits. Besides the executable code, OpenQL\nalso produces an intermediate quantum assembly code (cQASM), which is\ntechnology-independent and can be simulated using the QX simulator.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 11:23:16 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Khammassi", "N.", ""], ["Ashraf", "I.", ""], ["Someren", "J. v.", ""], ["Nane", "R.", ""], ["Krol", "A. M.", ""], ["Rol", "M. A.", ""], ["Lao", "L.", ""], ["Bertels", "K.", ""], ["Almudever", "C. G.", ""]]}, {"id": "2005.13632", "submitter": "Farzin Houshmand", "authors": "Farzin Houshmand, Mohsen Lesani and Keval Vora", "title": "GraFS: Graph Analytics Fusion and Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph analytics elicits insights from large graphs to inform critical\ndecisions for business, safety and security. Several large-scale graph\nprocessing frameworks feature efficient runtime systems; however, they often\nprovide programming models that are low-level and subtly different from each\nother. Therefore, end users can find implementation and specially optimization\nof graph analytics time-consuming and error-prone. This paper regards the\nabstract interface of the graph processing frameworks as the instruction set\nfor graph analytics, and presents Grafs, a high-level declarative specification\nlanguage for graph analytics and a synthesizer that automatically generates\nefficient code for five high-performance graph processing frameworks. It\nfeatures novel semantics-preserving fusion transformations that optimize the\nspecifications and reduce them to three primitives: reduction over paths,\nmapping over vertices and reduction over vertices. Reductions over paths are\ncommonly calculated based on push or pull models that iteratively apply kernel\nfunctions at the vertices. This paper presents conditions, parametric in terms\nof the kernel functions, for the correctness and termination of the iterative\nmodels, and uses these conditions as specifications to automatically synthesize\nthe kernel functions. Experimental results show that the generated code matches\nor outperforms hand-optimized code, and that fusion accelerates execution.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 20:27:05 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Houshmand", "Farzin", ""], ["Lesani", "Mohsen", ""], ["Vora", "Keval", ""]]}, {"id": "2005.13654", "submitter": "Matija Pretnar", "authors": "\\v{Z}iga Luk\\v{s}i\\v{c} and Matija Pretnar", "title": "Local Algebraic Effect Theories", "comments": "24 pages, 8 figures", "journal-ref": "LUKSIC, Z, & PRETNAR, M. (2020). Local algebraic effect theories.\n  Journal of Functional Programming, 30, E13", "doi": "10.1017/S0956796819000212", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algebraic effects are computational effects that can be described with a set\nof basic operations and equations between them. As many interesting effect\nhandlers do not respect these equations, most approaches assume a trivial\ntheory, sacrificing both reasoning power and safety.\n  We present an alternative approach where the type system tracks equations\nthat are observed in subparts of the program, yielding a sound and flexible\nlogic, and paving a way for practical optimizations and reasoning tools.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 21:08:54 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Luk\u0161i\u010d", "\u017diga", ""], ["Pretnar", "Matija", ""]]}, {"id": "2005.13685", "submitter": "Ameer Haj-Ali", "authors": "Ameer Haj-Ali, Hasan Genc, Qijing Huang, William Moses, John\n  Wawrzynek, Krste Asanovi\\'c, Ion Stoica", "title": "ProTuner: Tuning Programs with Monte Carlo Tree Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore applying the Monte Carlo Tree Search (MCTS) algorithm in a\nnotoriously difficult task: tuning programs for high-performance deep learning\nand image processing. We build our framework on top of Halide and show that\nMCTS can outperform the state-of-the-art beam-search algorithm. Unlike beam\nsearch, which is guided by greedy intermediate performance comparisons between\npartial and less meaningful schedules, MCTS compares complete schedules and\nlooks ahead before making any intermediate scheduling decision. We further\nexplore modifications to the standard MCTS algorithm as well as combining real\nexecution time measurements with the cost model. Our results show that MCTS can\noutperform beam search on a suite of 16 real benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 22:25:10 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Haj-Ali", "Ameer", ""], ["Genc", "Hasan", ""], ["Huang", "Qijing", ""], ["Moses", "William", ""], ["Wawrzynek", "John", ""], ["Asanovi\u0107", "Krste", ""], ["Stoica", "Ion", ""]]}, {"id": "2005.13814", "submitter": "Matija Pretnar", "authors": "Georgios Karachalias, Matija Pretnar, Amr Hany Saleh, Stien\n  Vanderhallen, Tom Schrijvers", "title": "Explicit Effect Subtyping", "comments": "57 pages, 29 figures", "journal-ref": "J. Funct. Prog. 30 (2020) e15", "doi": "10.1017/S0956796820000131", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As popularity of algebraic effects and handlers increases, so does a demand\nfor their efficient execution. Eff, an ML-like language with native support for\nhandlers, has a subtyping-based effect system on which an effect-aware\noptimizing compiler could be built. Unfortunately, in our experience,\nimplementing optimizations for Eff is overly error-prone because its core\nlanguage is implicitly-typed, making code transformations very fragile.\n  To remedy this, we present an explicitly-typed polymorphic core calculus for\nalgebraic effect handlers with a subtyping-based type-and-effect system. It\nreifies appeals to subtyping in explicit casts with coercions that witness the\nsubtyping proof, quickly exposing typing bugs in program transformations.\n  Our typing-directed elaboration comes with a constraint-based inference\nalgorithm that turns an implicitly-typed Eff-like language into our calculus.\nMoreover, all coercions and effect information can be erased in a\nstraightforward way, demonstrating that coercions have no computational\ncontent. Additionally, we present a monadic translation from our calculus into\na pure language without algebraic effects or handlers, using the effect\ninformation to introduce monadic constructs only where necessary.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 07:17:54 GMT"}], "update_date": "2020-06-10", "authors_parsed": [["Karachalias", "Georgios", ""], ["Pretnar", "Matija", ""], ["Saleh", "Amr Hany", ""], ["Vanderhallen", "Stien", ""], ["Schrijvers", "Tom", ""]]}, {"id": "2005.14015", "submitter": "Purushottam Kar", "authors": "Darshak Chhatbar and Umair Z. Ahmed and Purushottam Kar", "title": "MACER: A Modular Framework for Accelerated Compilation Error Repair", "comments": "19 pages, 9 figures. A short version of this paper will appear at the\n  21st International Conference on Artificial Intelligence in Education (AIED).\n  Code for the MACER tool-chain is available at\n  https://github.com/purushottamkar/macer/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated compilation error repair, the problem of suggesting fixes to buggy\nprograms that fail to compile, has generated significant interest in recent\nyears. Apart from being a tool of general convenience, automated code repair\nhas significant pedagogical applications for novice programmers who find\ncompiler error messages cryptic and unhelpful. Existing approaches largely\nsolve this problem using a blackbox-application of a heavy-duty generative\nlearning technique, such as sequence-to-sequence prediction (TRACER) or\nreinforcement learning (RLAssist). Although convenient, such black-box\napplication of learning techniques makes existing approaches bulky in terms of\ntraining time, as well as inefficient at targeting specific error types.\n  We present MACER, a novel technique for accelerated error repair based on a\nmodular segregation of the repair process into repair identification and repair\napplication. MACER uses powerful yet inexpensive discriminative learning\ntechniques such as multi-label classifiers and rankers to first identify the\ntype of repair required and then apply the suggested repair.\n  Experiments indicate that the fine-grained approach adopted by MACER offers\nnot only superior error correction, but also much faster training and\nprediction. On a benchmark dataset of 4K buggy programs collected from actual\nstudent submissions, MACER outperforms existing methods by 20% at suggesting\nfixes for popular errors that exactly match the fix desired by the student.\nMACER is also competitive or better than existing methods at all error types --\nwhether popular or rare. MACER offers a training time speedup of 2x over TRACER\nand 800x over RLAssist, and a test time speedup of 2-4x over both.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 14:00:03 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Chhatbar", "Darshak", ""], ["Ahmed", "Umair Z.", ""], ["Kar", "Purushottam", ""]]}, {"id": "2005.14085", "submitter": "Pietro Barbieri", "authors": "Davide Ancona, Pietro Barbieri, Francesco Dagnino and Elena Zucca", "title": "Sound Regular Corecursion in coFJ", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The aim of the paper is to provide solid foundations for a programming\nparadigm natively supporting the creation and manipulation of cyclic data\nstructures. To this end, we describe coFJ, a Java-like calculus where objects\ncan be infinite and methods are equipped with a codefinition (an alternative\nbody). We provide an abstract semantics of the calculus based on the framework\nof inference systems with corules. In coFJ with this semantics, FJ recursive\nmethods on finite objects can be extended to infinite objects as well, and\nbehave as desired by the programmer, by specifying a codefinition. We also\ndescribe an operational semantics which can be directly implemented in a\nprogramming language, and prove the soundness of such semantics with respect to\nthe abstract one.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 15:27:44 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Ancona", "Davide", ""], ["Barbieri", "Pietro", ""], ["Dagnino", "Francesco", ""], ["Zucca", "Elena", ""]]}, {"id": "2005.14650", "submitter": "Lu\\'is Pedro Arrojado Da Horta", "authors": "Lu\\'is Pedro Arrojado da Horta and Jo\\~ao Santos Reis and M\\'ario\n  Pereira and Sim\\~ao Melo de Sousa", "title": "WhylSon: Proving your Michelson Smart Contracts in Why3", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces WhylSon, a deductive verification tool for smart\ncontracts written in Michelson, which is the low-level language of the Tezos\nblockchain. WhylSon accepts a formally specified Michelson contract and\nautomatically translates it to an equivalent program written in WhyML, the\nprogramming and specification language of the Why3 framework. Smart contract\ninstructions are mapped into a corresponding WhyML shallow-embedding of the\ntheir axiomatic semantics, which we also developed in the context of this work.\nOne major advantage of this approach is that it allows an out-of-the-box\nintegration with the Why3 framework, namely its VCGen and the backend support\nfor several automated theorem provers. We also discuss the use of WhylSon to\nautomatically prove the correctness of diverse annotated smart contracts.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 16:26:19 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["da Horta", "Lu\u00eds Pedro Arrojado", ""], ["Reis", "Jo\u00e3o Santos", ""], ["Pereira", "M\u00e1rio", ""], ["de Sousa", "Sim\u00e3o Melo", ""]]}]