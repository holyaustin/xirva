[{"id": "1701.00148", "submitter": "EPTCS", "authors": "Sibylle Schwarz, Janis Voigtl\\\"ander", "title": "Proceedings 29th and 30th Workshops on (Constraint) Logic Programming\n  and 24th International Workshop on Functional and (Constraint) Logic\n  Programming", "comments": null, "journal-ref": "EPTCS 234, 2017", "doi": "10.4204/EPTCS.234", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Workshops on (Constraint) Logic Programming (WLP) are the annual meeting\nof the German Society of Logic Programming (Gesellschaft f\\\"ur Logische\nProgrammierung e.V., GLP) and bring together researchers interested in logic\nprogramming, constraint programming, answer set programming, and related areas\nlike databases and artificial intelligence (not only from Germany).\n  The International Workshops on Functional and (Constraint) Logic Programming\n(WFLP) aim at bringing together researchers, students, and practitioners\ninterested in functional programming, logic programming, and their integration.\n  The workshops have a tradition of co-location to promote the\ncross-fertilizing exchange of ideas and experiences among and between the\ncommunities interested in the foundations, applications, and combinations of\nhigh-level, declarative programming languages and related areas.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 17:36:07 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Schwarz", "Sibylle", ""], ["Voigtl\u00e4nder", "Janis", ""]]}, {"id": "1701.00161", "submitter": "Bor-Yuh Evan Chang", "authors": "Shawn Meier, Aleksandar Chakarov, Maxwell Russek, Sergio Mover, and\n  Bor-Yuh Evan Chang", "title": "Abstracting Event-Driven Systems with Lifestate Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present lifestate rules--an approach for abstracting event-driven object\nprotocols. Developing applications against event-driven software frameworks is\nnotoriously difficult. One reason why is that to create functioning\napplications, developers must know about and understand the complex protocols\nthat abstract the internal behavior of the framework. Such protocols intertwine\nthe proper registering of callbacks to receive control from the framework with\nappropriate application programming interface (API) calls to delegate back to\nit. Lifestate rules unify lifecycle and typestate constraints in one common\nspecification language. Our primary contribution is a model of event-driven\nsystems from which lifestate rules can be derived. We then apply specification\nmining techniques to learn lifestate specifications for Android framework\ntypes. In the end, our implementation is able to find several rules that\ncharacterize actual behavior of the Android framework.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2016 19:53:37 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Meier", "Shawn", ""], ["Chakarov", "Aleksandar", ""], ["Russek", "Maxwell", ""], ["Mover", "Sergio", ""], ["Chang", "Bor-Yuh Evan", ""]]}, {"id": "1701.00233", "submitter": "EPTCS", "authors": "Horatiu Cirstea (LORIA, Universit\\'e de Lorraine, France), Santiago\n  Escobar (Universitat Polit\\`ecnica de Val\\`encia, Spain)", "title": "Proceedings Third International Workshop on Rewriting Techniques for\n  Program Transformations and Evaluation", "comments": "Dedicated to the memory of Kristoffer H. Rose", "journal-ref": "EPTCS 235, 2017", "doi": "10.4204/EPTCS.235", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the formal proceedings of the Third International\nWorkshop on Rewriting Techniques for Program Transformations and Evaluation\n(WPTE 2016), held on 23rd June 2016 in Porto, Portugal, as a satellite event of\nthe First International Conference on Formal Structures for Computation and\nDeduction (FSCD 2016). The workshop brought together researchers working on\nprogram transformations, evaluation, and operationally based programming\nlanguage semantics, using rewriting methods, in order to share the techniques\nand recent developments and to exchange ideas to encourage further activation\nof research in this area.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 12:26:14 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Cirstea", "Horatiu", "", "LORIA, Universit\u00e9 de Lorraine, France"], ["Escobar", "Santiago", "", "Universitat Polit\u00e8cnica de Val\u00e8ncia, Spain"]]}, {"id": "1701.00622", "submitter": "EPTCS", "authors": "Dietmar Seipel (University of W\\\"urzburg)", "title": "Knowledge Engineering for Hybrid Deductive Databases", "comments": "In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148", "journal-ref": "EPTCS 234, 2017, pp. 1-12", "doi": "10.4204/EPTCS.234.1", "report-no": null, "categories": "cs.DB cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern knowledge base systems frequently need to combine a collection of\ndatabases in different formats: e.g., relational databases, XML databases, rule\nbases, ontologies, etc. In the deductive database system DDBASE, we can manage\nthese different formats of knowledge and reason about them. Even the file\nsystems on different computers can be part of the knowledge base. Often, it is\nnecessary to handle different versions of a knowledge base. E.g., we might want\nto find out common parts or differences of two versions of a relational\ndatabase.\n  We will examine the use of abstractions of rule bases by predicate dependency\nand rule predicate graphs. Also the proof trees of derived atoms can help to\ncompare different versions of a rule base. Moreover, it might be possible to\nhave derivations joining rules with other formalisms of knowledge\nrepresentation.\n  Ontologies have shown their benefits in many applications of intelligent\nsystems, and there have been many proposals for rule languages compatible with\nthe semantic web stack, e.g., SWRL, the semantic web rule language. Recently,\nontologies are used in hybrid systems for specifying the provenance of the\ndifferent components.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:30:37 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Seipel", "Dietmar", "", "University of W\u00fcrzburg"]]}, {"id": "1701.00623", "submitter": "EPTCS", "authors": "Stefan Brass (University of Halle), Heike Stephan (University of\n  Halle)", "title": "Bottom-Up Evaluation of Datalog: Preliminary Report", "comments": "In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148", "journal-ref": "EPTCS 234, 2017, pp. 13-26", "doi": "10.4204/EPTCS.234.2", "report-no": null, "categories": "cs.DB cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bottom-up evaluation of Datalog has been studied for a long time, and is\nstandard material in textbooks. However, if one actually wants to develop a\ndeductive database system, it turns out that there are many implementation\noptions. For instance, the sequence in which rule instances are applied is not\ngiven. In this paper, we study a method that immediately uses a derived tuple\nto derive more tuples (called the Push method). In this way, storage space for\nintermediate results can be reduced. The main contribution of our method is the\nway in which we minimize the copying of values at runtime, and do much work\nalready at compile-time.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:30:51 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Brass", "Stefan", "", "University of Halle"], ["Stephan", "Heike", "", "University of\n  Halle"]]}, {"id": "1701.00626", "submitter": "EPTCS", "authors": "Falco Nogatz, Dietmar Seipel", "title": "Implementing GraphQL as a Query Language for Deductive Databases in\n  SWI-Prolog Using DCGs, Quasi Quotations, and Dicts", "comments": "In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148", "journal-ref": "EPTCS 234, 2017, pp. 42-56", "doi": "10.4204/EPTCS.234.4", "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The methods to access large relational databases in a distributed system are\nwell established: the relational query language SQL often serves as a language\nfor data access and manipulation, and in addition public interfaces are exposed\nusing communication protocols like REST. Similarly to REST, GraphQL is the\nquery protocol of an application layer developed by Facebook. It provides a\nunified interface between the client and the server for data fetching and\nmanipulation. Using GraphQL's type system, it is possible to specify data\nhandling of various sources and to combine, e.g., relational with NoSQL\ndatabases. In contrast to REST, GraphQL provides a single API endpoint and\nsupports flexible queries over linked data.\n  GraphQL can also be used as an interface for deductive databases. In this\npaper, we give an introduction of GraphQL and a comparison to REST. Using\nlanguage features recently added to SWI-Prolog 7, we have developed the Prolog\nlibrary GraphQL.pl, which implements the GraphQL type system and query syntax\nas a domain-specific language with the help of definite clause grammars (DCG),\nquasi quotations, and dicts. Using our library, the type system created for a\ndeductive database can be validated, while the query system provides a unified\ninterface for data access and introspection.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:31:26 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Nogatz", "Falco", ""], ["Seipel", "Dietmar", ""]]}, {"id": "1701.00629", "submitter": "EPTCS", "authors": "Sebastian Krings (Heinrich-Heine Universit\\\"at D\\\"usseldorf), Michael\n  Leuschel (Heinrich-Heine Universit\\\"at D\\\"usseldorf)", "title": "Constraint Logic Programming over Infinite Domains with an Application\n  to Proof", "comments": "In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148", "journal-ref": "EPTCS 234, 2017, pp. 73-87", "doi": "10.4204/EPTCS.234.6", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a CLP(FD)-based constraint solver able to deal with unbounded\ndomains. It is based on constraint propagation, resorting to enumeration if all\nother methods fail. An important aspect is detecting when enumeration was\ncomplete and if this has an impact on the soundness of the result. We present a\ntechnique which guarantees soundness in the following way: if the constraint\nsolver finds a solution it is guaranteed to be correct; if the constraint\nsolver fails to find a solution it can either return the result \"definitely\nfalse\" in case it knows enumeration was exhaustive, or \"unknown\" in case it was\naborted. The technique can deal with nested universal and existential\nquantifiers. It can easily be extended to set comprehensions and other\noperators introducing new quantified variables. We show applications in data\nvalidation and proof.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:31:58 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Krings", "Sebastian", "", "Heinrich-Heine Universit\u00e4t D\u00fcsseldorf"], ["Leuschel", "Michael", "", "Heinrich-Heine Universit\u00e4t D\u00fcsseldorf"]]}, {"id": "1701.00630", "submitter": "EPTCS", "authors": "Frank Flederer (University of Wuerzburg), Ludwig Ostermayer\n  (University of Wuerzburg), Dietmar Seipel (University of Wuerzburg), Sergio\n  Montenegro (University of Wuerzburg)", "title": "Source Code Verification for Embedded Systems using Prolog", "comments": "In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148", "journal-ref": "EPTCS 234, 2017, pp. 88-103", "doi": "10.4204/EPTCS.234.7", "report-no": null, "categories": "cs.SE cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System relevant embedded software needs to be reliable and, therefore, well\ntested, especially for aerospace systems. A common technique to verify programs\nis the analysis of their abstract syntax tree (AST). Tree structures can be\nelegantly analyzed with the logic programming language Prolog. Moreover, Prolog\noffers further advantages for a thorough analysis: On the one hand, it natively\nprovides versatile options to efficiently process tree or graph data\nstructures. On the other hand, Prolog's non-determinism and backtracking eases\ntests of different variations of the program flow without big effort. A\nrule-based approach with Prolog allows to characterize the verification goals\nin a concise and declarative way.\n  In this paper, we describe our approach to verify the source code of a flash\nfile system with the help of Prolog. The flash file system is written in C++\nand has been developed particularly for the use in satellites. We transform a\ngiven abstract syntax tree of C++ source code into Prolog facts and derive the\ncall graph and the execution sequence (tree), which then are further tested\nagainst verification goals. The different program flow branching due to control\nstructures is derived by backtracking as subtrees of the full execution\nsequence. Finally, these subtrees are verified in Prolog.\n  We illustrate our approach with a case study, where we search for incorrect\napplications of semaphores in embedded software using the real-time operating\nsystem RODOS. We rely on computation tree logic (CTL) and have designed an\nembedded domain specific language (DSL) in Prolog to express the verification\ngoals.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:32:17 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Flederer", "Frank", "", "University of Wuerzburg"], ["Ostermayer", "Ludwig", "", "University of Wuerzburg"], ["Seipel", "Dietmar", "", "University of Wuerzburg"], ["Montenegro", "Sergio", "", "University of Wuerzburg"]]}, {"id": "1701.00631", "submitter": "EPTCS", "authors": "Michael Hanus (CAU Kiel), Julia Krone (CAU Kiel)", "title": "A Typeful Integration of SQL into Curry", "comments": "In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148", "journal-ref": "EPTCS 234, 2017, pp. 104-119", "doi": "10.4204/EPTCS.234.8", "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an extension of the declarative programming language Curry to\nsupport the access to data stored in relational databases via SQL. Since Curry\nis statically typed, our emphasis on this SQL integration is on type safety.\nOur extension respects the type system of Curry so that run-time errors due to\nill-typed data are avoided. This is obtained by preprocessing SQL statements at\ncompile time and translating them into type-safe database access operations. As\na consequence, the type checker of the Curry system can spot type errors in SQL\nstatements at compile time. To generate appropriately typed access operations,\nthe preprocessor uses an entity-relationship (ER) model describing the\nstructure of the relational data. In addition to standard SQL, SQL statements\nembedded in Curry can include program expressions and also relationships\nspecified in the ER model. The latter feature is useful to avoid the\nerror-prone use of foreign keys. As a result, our SQL integration supports a\nhigh-level and type-safe access to databases in Curry programs.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:32:32 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Hanus", "Michael", "", "CAU Kiel"], ["Krone", "Julia", "", "CAU Kiel"]]}, {"id": "1701.00632", "submitter": "EPTCS", "authors": "Mar\\'ia-del-Mar Gallardo (Universidad de M\\'alaga), Leticia Lavado\n  (Universidad de M\\'alaga), Laura Panizo (Universidad de M\\'alaga)", "title": "A Simulation Tool for tccp Programs", "comments": "In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148", "journal-ref": "EPTCS 234, 2017, pp. 120-134", "doi": "10.4204/EPTCS.234.9", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Timed Concurrent Constraint Language tccp is a declarative synchronous\nconcurrent language, particularly suitable for modelling reactive systems. In\ntccp, agents communicate and synchronise through a global constraint store. It\nsupports a notion of discrete time that allows all non-blocked agents to\nproceed with their execution simultaneously.\n  In this paper, we present a modular architecture for the simulation of tccp\nprograms. The tool comprises three main components. First, a set of basic\nabstract instructions able to model the tccp agent behaviour, the memory model\nneeded to manage the active agents and the state of the store during the\nexecution. Second, the agent interpreter that executes the instructions of the\ncurrent agent iteratively and calculates the new agents to be executed at the\nnext time instant. Finally, the constraint solver components which are the\nmodules that deal with constraints.\n  In this paper, we describe the implementation of these components and present\nan example of a real system modelled in tccp.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:32:47 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Gallardo", "Mar\u00eda-del-Mar", "", "Universidad de M\u00e1laga"], ["Lavado", "Leticia", "", "Universidad de M\u00e1laga"], ["Panizo", "Laura", "", "Universidad de M\u00e1laga"]]}, {"id": "1701.00633", "submitter": "EPTCS", "authors": "Jason Hemann (Indiana University), Daniel P. Friedman (Indiana\n  University)", "title": "A Framework for Extending microKanren with Constraints", "comments": "In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148", "journal-ref": "EPTCS 234, 2017, pp. 135-149", "doi": "10.4204/EPTCS.234.10", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for building CLP languages with symbolic constraints\nbased on microKanren, a domain-specific logic language shallowly embedded in\nRacket. We rely on Racket's macro system to generate a constraint solver and\nother components of the microKanren embedding. The framework itself and the\nconstraints' implementations amounts to just over 100 lines of code. Our\nframework is both a teachable implementation for CLP as well as a test-bed and\nprototyping tool for symbolic constraint systems.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:33:05 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Hemann", "Jason", "", "Indiana University"], ["Friedman", "Daniel P.", "", "Indiana\n  University"]]}, {"id": "1701.00634", "submitter": "EPTCS", "authors": "Baltasar Tranc\\'on y Widemann (Ilmenau University of Technology, DE),\n  Markus Lepper (semantics GmbH, DE)", "title": "A Practical Study of Control in Objected-Oriented--Functional--Logic\n  Programming with Paisley", "comments": "In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148", "journal-ref": "EPTCS 234, 2017, pp. 150-164", "doi": "10.4204/EPTCS.234.11", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paisley is an extensible lightweight embedded domain-specific language for\nnondeterministic pattern matching in Java. Using simple APIs and programming\nidioms, it brings the power of functional-logic processing of arbitrary data\nobjects to the Java platform, without constraining the underlying\nobject-oriented semantics. Here we present an extension to the Paisley\nframework that adds pattern-based control flow. It exploits recent additions to\nthe Java language, namely functional interfaces and lambda expressions, for an\nexplicit and transparent continuation-passing style approach to control. We\nevaluate the practical impact of the novel features on a real-world case study\nthat reengineers a third-party open-source project to use Paisley in place of\nconventional object-oriented data query idioms. We find the approach viable for\nincremental refactoring of legacy code, with significant qualitative\nimprovements regarding separation of concerns, clarity and intentionality, thus\nmaking for easier code understanding, testing and debugging.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:33:21 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Widemann", "Baltasar Tranc\u00f3n y", "", "Ilmenau University of Technology, DE"], ["Lepper", "Markus", "", "semantics GmbH, DE"]]}, {"id": "1701.00636", "submitter": "EPTCS", "authors": "Sergio Antoy (Portland State University), Michael Hanus (CAU Kiel),\n  Steven Libby (Portland State University)", "title": "Proving Non-Deterministic Computations in Agda", "comments": "In Proceedings WLP'15/'16/WFLP'16, arXiv:1701.00148", "journal-ref": "EPTCS 234, 2017, pp. 180-195", "doi": "10.4204/EPTCS.234.13", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate proving properties of Curry programs using Agda. First, we\naddress the functional correctness of Curry functions that, apart from some\nsyntactic and semantic differences, are in the intersection of the two\nlanguages. Second, we use Agda to model non-deterministic functions with two\ndistinct and competitive approaches incorporating the non-determinism. The\nfirst approach eliminates non-determinism by considering the set of all\nnon-deterministic values produced by an application. The second approach\nencodes every non-deterministic choice that the application could perform. We\nconsider our initial experiment a success. Although proving properties of\nprograms is a notoriously difficult task, the functional logic paradigm does\nnot seem to add any significant layer of difficulty or complexity to the task.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:34:39 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Antoy", "Sergio", "", "Portland State University"], ["Hanus", "Michael", "", "CAU Kiel"], ["Libby", "Steven", "", "Portland State University"]]}, {"id": "1701.00640", "submitter": "EPTCS", "authors": "Nils Dallmeyer (Goethe-University Frankfurt, Germany), Manfred\n  Schmidt-Schauss (Goethe-University Frankfurt, Germany)", "title": "An Environment for Analyzing Space Optimizations in Call-by-Need\n  Functional Languages", "comments": "In Proceedings WPTE 2016, arXiv:1701.00233", "journal-ref": "EPTCS 235, 2017, pp. 78-92", "doi": "10.4204/EPTCS.235.6", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an implementation of an interpreter LRPi for the call-by-need\ncalculus LRP, based on a variant of Sestoft's abstract machine Mark 1, extended\nwith an eager garbage collector. It is used as a tool for exact space usage\nanalyses as a support for our investigations into space improvements of\ncall-by-need calculi.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 10:38:53 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Dallmeyer", "Nils", "", "Goethe-University Frankfurt, Germany"], ["Schmidt-Schauss", "Manfred", "", "Goethe-University Frankfurt, Germany"]]}, {"id": "1701.00649", "submitter": "EPTCS", "authors": "Beniamino Accattoli (INRIA and LIX, \\'Ecole Polytechnique)", "title": "The Complexity of Abstract Machines", "comments": "In Proceedings WPTE 2016, arXiv:1701.00233", "journal-ref": "EPTCS 235, 2017, pp. 1-15", "doi": "10.4204/EPTCS.235.1", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lambda-calculus is a peculiar computational model whose definition does\nnot come with a notion of machine. Unsurprisingly, implementations of the\nlambda-calculus have been studied for decades. Abstract machines are\nimplementations schema for fixed evaluation strategies that are a compromise\nbetween theory and practice: they are concrete enough to provide a notion of\nmachine and abstract enough to avoid the many intricacies of actual\nimplementations. There is an extensive literature about abstract machines for\nthe lambda-calculus, and yet-quite mysteriously-the efficiency of these\nmachines with respect to the strategy that they implement has almost never been\nstudied.\n  This paper provides an unusual introduction to abstract machines, based on\nthe complexity of their overhead with respect to the length of the implemented\nstrategies. It is conceived to be a tutorial, focusing on the case study of\nimplementing the weak head (call-by-name) strategy, and yet it is an original\nre-elaboration of known results. Moreover, some of the observation contained\nhere never appeared in print before.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 11:10:32 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Accattoli", "Beniamino", "", "INRIA and LIX, \u00c9cole Polytechnique"]]}, {"id": "1701.01785", "submitter": "Keehang Kwon", "authors": "Keehang Kwon and Daeseong Kang", "title": "A Concurrent Model for Imperative Languages with Improved Atomicity", "comments": "3 pages. Our scheduler is quite adaptive to the requests of the\n  processes. This makes synchronization simpler", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new concurrent model for imperative languages where concurrency\noccurs at a subprogram level. This model introduces a new {\\it block\nsequential} statement of the form $#(G_1,\\ldots,G_n)$ where each $G_i$ is a\nstatement. This statement tells the machine to execute $G_1,\\ldots,G_n$\nsequentially and atomically (\\ie, without interleaving). It therefore enhances\natomicity and predictability in concurrent programming. We illustrate our idea\nvia $C^{\\|}$, an extension of the core concurrent C with the new block\nsequential statement.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2017 01:55:13 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Kwon", "Keehang", ""], ["Kang", "Daeseong", ""]]}, {"id": "1701.02189", "submitter": "Simon Kramer", "authors": "Simon Kramer", "title": "A Modularity Bug in Java 8", "comments": null, "journal-ref": "Theoretical and Applied Informatics, Volume 28, Issue 3, 2016", "doi": "10.20904/283001", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a modularity bug in the interface system of Java 8 on the\npractical example of a textbook design of a modular interface for vector\nspaces. Our example originates in our teaching of modular object-oriented\ndesign in Java 8 to undergraduate students, simply following standard\nprogramming practices and mathematical definitions. The bug shows up as a\ncompilation error and should be fixed with a language extension due to the\nimportance of best practices (design fidelity).\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 06:53:48 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Kramer", "Simon", ""]]}, {"id": "1701.02284", "submitter": "Tian Zhao", "authors": "Tian Zhao, Xiaobing Huang, Yu Cao", "title": "DeepDSL: A Compilation-based Domain-Specific Language for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Deep Learning (DL) has found great success in domains such\nas multimedia understanding. However, the complex nature of multimedia data\nmakes it difficult to develop DL-based software. The state-of-the art tools,\nsuch as Caffe, TensorFlow, Torch7, and CNTK, while are successful in their\napplicable domains, are programming libraries with fixed user interface,\ninternal representation, and execution environment. This makes it difficult to\nimplement portable and customized DL applications.\n  In this paper, we present DeepDSL, a domain specific language (DSL) embedded\nin Scala, that compiles deep networks written in DeepDSL to Java source code.\nDeep DSL provides (1) intuitive constructs to support compact encoding of deep\nnetworks; (2) symbolic gradient derivation of the networks; (3) static analysis\nfor memory consumption and error detection; and (4) DSL-level optimization to\nimprove memory and runtime efficiency.\n  DeepDSL programs are compiled into compact, efficient, customizable, and\nportable Java source code, which operates the CUDA and CUDNN interfaces running\non Nvidia GPU via a Java Native Interface (JNI) library. We evaluated DeepDSL\nwith a number of popular DL networks. Our experiments show that the compiled\nprograms have very competitive runtime performance and memory efficiency\ncompared to the existing libraries.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 18:02:13 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Zhao", "Tian", ""], ["Huang", "Xiaobing", ""], ["Cao", "Yu", ""]]}, {"id": "1701.02547", "submitter": "Hongseok Yang", "authors": "Chris Heunen and Ohad Kammar and Sam Staton and Hongseok Yang", "title": "A Convenient Category for Higher-Order Probability Theory", "comments": null, "journal-ref": "Logic in Computer Science 2017", "doi": "10.1109/LICS.2017.8005137", "report-no": null, "categories": "cs.PL cs.AI cs.LO math.CT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher-order probabilistic programming languages allow programmers to write\nsophisticated models in machine learning and statistics in a succinct and\nstructured way, but step outside the standard measure-theoretic formalization\nof probability theory. Programs may use both higher-order functions and\ncontinuous distributions, or even define a probability distribution on\nfunctions. But standard probability theory does not handle higher-order\nfunctions well: the category of measurable spaces is not cartesian closed.\n  Here we introduce quasi-Borel spaces. We show that these spaces: form a new\nformalization of probability theory replacing measurable spaces; form a\ncartesian closed category and so support higher-order functions; form a\nwell-pointed category and so support good proof principles for equational\nreasoning; and support continuous probability distributions. We demonstrate the\nuse of quasi-Borel spaces for higher-order functions and probability by:\nshowing that a well-known construction of probability theory involving random\nfunctions gains a cleaner expression; and generalizing de Finetti's theorem,\nthat is a crucial theorem in probability theory, to quasi-Borel spaces.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 12:19:05 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 11:02:46 GMT"}, {"version": "v3", "created": "Tue, 18 Apr 2017 20:02:24 GMT"}, {"version": "v4", "created": "Fri, 20 Nov 2020 08:56:11 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Heunen", "Chris", ""], ["Kammar", "Ohad", ""], ["Staton", "Sam", ""], ["Yang", "Hongseok", ""]]}, {"id": "1701.02648", "submitter": "Thom Fruehwirth", "authors": "Thom Fruehwirth", "title": "Why Can't You Behave? Non-termination Analysis of Direct Recursive Rules\n  with Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with rule-based programs that go wrong. The unwanted\nbehavior of rule applications is non-termination or failure of a computation.\nWe propose a static program analysis of the non-termination problem for\nrecursion in the Constraint Handling Rules (CHR) language.\n  CHR is an advanced concurrent declarative language involving constraint\nreasoning. It has been closely related to many other rule-based approaches, so\nthe results are of a more general interest. In such languages, non-termination\nis due to infinite applications of recursive rules. Failure is due to\naccumulation of contradicting constraints during the computation.\n  We give theorems with so-called misbehavior conditions for potential\nnon-termination and failure (as well as definite termination) of linear direct\nrecursive simplification rules. Logical relationships between the constraints\nin a recursive rule play a crucial role in this kind of program analysis. We\nthink that our approach can be extended to other types of recursion and to a\nmore general class of rules. Therefore this paper can serve as a basic\nreference and a starting point for further research.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 15:50:12 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Fruehwirth", "Thom", ""]]}, {"id": "1701.02668", "submitter": "Thom Fruehwirth", "authors": "Thom Fruehwirth", "title": "Constraint Handling Rules - What Else?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint Handling Rules (CHR) is both an effective concurrent declarative\nconstraint-based programming language and a versatile computational formalism.\nWhile conceptually simple, CHR is distinguished by a remarkable combination of\ndesirable features: - semantic foundation in classical and linear logic, -\neffective and efficient sequential and parallel execution model - guaranteed\nproperties like the anytime online algorithm properties - powerful analysis\nmethods for deciding essential program properties. This overview of CHR\nresearch and applications is by no complete. It concentrates on the years since\n2000. Up-to-date information on CHR can be found at the CHR web-site\nwww.constraint-handling-rules.org, including the slides of the keynote talk\nassociated with this article, an extensive bibliography, online demo versions\nand free downloads of the language.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 16:29:06 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Fruehwirth", "Thom", ""]]}, {"id": "1701.02682", "submitter": "Thom Fruehwirth", "authors": "Thom Fruehwirth", "title": "A Devil's Advocate against Termination of Direct Recursion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A devil's advocate is one who argues against a claim, not as a committed\nopponent but in order to determine the validity of the claim. We are interested\nin a devil's advocate that argues against termination of a program. He does so\nby producing a maleficent program that can cause the non-termination of the\noriginal program. By inspecting and running the malicious program, one may gain\ninsight into the potential reasons for non-termination and produce\ncounterexamples for termination.\n  We introduce our method using the concurrent programming language Constraint\nHandling Rules (CHR). Like in other declarative languages, non-termination\noccurs through unbounded recursion. Given a self-recursive rule, we\nautomatically generate one or more devil's rules from it. The construction of\nthe devil's rules is straightforward and involves no guessing. The devil's\nrules can be simple. For example, they are non-recursive for rules with single\nrecursion.\n  We show that the devil's rules are maximally vicious in the following sense:\nFor any program that contains the self-recursive rule and for any infinite\ncomputation through that rule in that program, there is a corresponding\ninfinite computation with the recursive rule and the devil's rules alone. In\nthat case, the malicious rules serve as a finite witness for non-termination.\nOn the other hand, if the devil's rules do not exhibit an infinite computation,\nthe recursive rule is unconditionally terminating. We also identify cases where\nthe static analysis of the devil's rule decides termination or non-termination\nof the recursive rule.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 16:51:30 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Fruehwirth", "Thom", ""]]}, {"id": "1701.02842", "submitter": "Jana Dunfield", "authors": "Jana Dunfield", "title": "Extensible Datasort Refinements", "comments": "27 pages + 26 page appendix, to appear at ESOP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Refinement types turn typechecking into lightweight verification. The classic\nform of refinement type is the datasort refinement, in which datasorts identify\nsubclasses of inductive datatypes.\n  Existing type systems for datasort refinements require that all the\nrefinements of a type be specified when the type is declared; multiple\nrefinements of the same type can be obtained only by duplicating type\ndefinitions, and consequently, duplicating code.\n  We enrich the traditional notion of a signature, which describes the\ninhabitants of datasorts, to allow re-refinement via signature extension,\nwithout duplicating definitions. Since arbitrary updates to a signature can\ninvalidate the inversion principles used to check case expressions, we develop\na definition of signature well-formedness that ensures that extensions maintain\nexisting inversion principles. This definition allows different parts of a\nprogram to extend the same signature in different ways, without conflicting\nwith each other. Each part can be type-checked independently, allowing separate\ncompilation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 03:56:23 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 18:54:04 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Dunfield", "Jana", ""]]}, {"id": "1701.02944", "submitter": "Hongfei Fu", "authors": "Krishnendu Chatterjee, Hongfei Fu", "title": "Termination of Nondeterministic Recursive Probabilistic Programs", "comments": "35 pages. Comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the termination problem for nondeterministic recursive probabilistic\nprograms. First, we show that a ranking-supermartingales-based approach is both\nsound and complete for bounded terminiation (i.e., bounded expected termination\ntime over all schedulers). Our result also clarifies previous results which\nclaimed that ranking supermartingales are not a complete approach even for\nnondeterministic probabilistic programs without recursion. Second, we show that\nconditionally difference-bounded ranking supermartingales provide a sound\napproach for lower bounds of expected termination time. Finally, we show that\nsupermartingales with lower bounds on conditional absolute difference provide a\nsound approach for almost-sure termination, along with explicit bounds on tail\nprobabilities of nontermination within a given number of steps. We also present\nseveral illuminating counterexamples that establish the necessity of certain\nprerequisites (such as conditionally difference-bounded condition).\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 12:14:06 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Fu", "Hongfei", ""]]}, {"id": "1701.03069", "submitter": "EPTCS", "authors": "Alicia Villanueva (Universitat Polit\\`ecnica de Val\\`encia, Spain)", "title": "Proceedings XVI Jornadas sobre Programaci\\'on y Lenguajes", "comments": null, "journal-ref": "EPTCS 237, 2017", "doi": "10.4204/EPTCS.237", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains a selection of the papers presented at the XVI Jornadas\nsobre Programaci\\'on y Lenguajes (PROLE 2016), held at Salamanca, Spain, during\nSeptember 14th-15th, 2016. Previous editions of the workshop were held in\nSantander (2015), C\\'adiz (2014), Madrid (2013), Almer\\'ia (2012), A Coru\\~na\n(2011), Val\\`encia (2010), San Sebasti\\'an (2009), Gij\\'on (2008), Zaragoza\n(2007), Sitges (2006), Granada (2005), M\\'alaga (2004), Alicante (2003), El\nEscorial (2002), and Almagro (2001). Programming languages provide a conceptual\nframework which is necessary for the development, analysis, optimization and\nunderstanding of programs and programming tasks. The aim of the PROLE series of\nconferences (PROLE stems from PROgramaci\\'on y LEnguajes) is to serve as a\nmeeting point for Spanish research groups which develop their work in the area\nof programming and programming languages. The organization of this series of\nevents aims at fostering the exchange of ideas, experiences and results among\nthese groups. Promoting further collaboration is also one of its main goals.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 17:26:09 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Villanueva", "Alicia", "", "Universitat Polit\u00e8cnica de Val\u00e8ncia, Spain"]]}, {"id": "1701.03319", "submitter": "EPTCS", "authors": "Salvador Tamarit (Universidad Polit\\`ecnica de Madrid), Julio Mari\\~no\n  (Universidad Polit\\`ecnica de Madrid), Guillermo Vigueras (IMDEA Software\n  Institute), Manuel Carro (IMDEA Software Institute)", "title": "Towards a Semantics-Aware Code Transformation Toolchain for\n  Heterogeneous Systems", "comments": "In Proceedings PROLE 2016, arXiv:1701.03069. arXiv admin note:\n  substantial text overlap with arXiv:1603.03011", "journal-ref": "EPTCS 237, 2017, pp. 34-51", "doi": "10.4204/EPTCS.237.3", "report-no": null, "categories": "cs.PL cs.DC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining good performance when programming heterogeneous computing platforms\nposes significant challenges. We present a program transformation environment,\nimplemented in Haskell, where architecture-agnostic scientific C code with\nsemantic annotations is transformed into functionally equivalent code better\nsuited for a given platform. The transformation steps are represented as rules\nthat can be fired when certain syntactic and semantic conditions are fulfilled.\nThese rules are not hard-wired into the rewriting engine: they are written in a\nC-like language and are automatically processed and incorporated into the\nrewriting engine. That makes it possible for end-users to add their own rules\nor to provide sets of rules that are adapted to certain specific domains or\npurposes.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 12:04:30 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Tamarit", "Salvador", "", "Universidad Polit\u00e8cnica de Madrid"], ["Mari\u00f1o", "Julio", "", "Universidad Polit\u00e8cnica de Madrid"], ["Vigueras", "Guillermo", "", "IMDEA Software\n  Institute"], ["Carro", "Manuel", "", "IMDEA Software Institute"]]}, {"id": "1701.03320", "submitter": "EPTCS", "authors": "Ricardo Pe\\~na (Universidad Complutense de Madrid)", "title": "An Introduction to Liquid Haskell", "comments": "In Proceedings PROLE 2016, arXiv:1701.03069", "journal-ref": "EPTCS 237, 2017, pp. 68-80", "doi": "10.4204/EPTCS.237.5", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a tutorial introducing the underlying technology and the use of\nthe tool Liquid Haskell, a type-checker for the functional language Haskell\nthat can help programmers to verify non-trivial properties of their programs\nwith a low effort.\n  The first sections introduce the technology of Liquid Types by explaining its\nprinciples and summarizing how its type inference algorithm manages to prove\nproperties. The remaining sections present a selection of Haskell examples and\nshow the kind of properties that can be proved with the system.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 12:05:03 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Pe\u00f1a", "Ricardo", "", "Universidad Complutense de Madrid"]]}, {"id": "1701.03757", "submitter": "Dustin Tran", "authors": "Dustin Tran, Matthew D. Hoffman, Rif A. Saurous, Eugene Brevdo, Kevin\n  Murphy, David M. Blei", "title": "Deep Probabilistic Programming", "comments": "Appears in International Conference on Learning Representations,\n  2017. A companion webpage for this paper is available at\n  http://edwardlib.org/iclr2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.PL stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Edward, a Turing-complete probabilistic programming language.\nEdward defines two compositional representations---random variables and\ninference. By treating inference as a first class citizen, on a par with\nmodeling, we show that probabilistic programming can be as flexible and\ncomputationally efficient as traditional deep learning. For flexibility, Edward\nmakes it easy to fit the same model using a variety of composable inference\nmethods, ranging from point estimation to variational inference to MCMC. In\naddition, Edward can reuse the modeling representation as part of inference,\nfacilitating the design of rich variational models and generative adversarial\nnetworks. For efficiency, Edward is integrated into TensorFlow, providing\nsignificant speedups over existing probabilistic systems. For example, we show\non a benchmark logistic regression task that Edward is at least 35x faster than\nStan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it\nis as fast as handwritten TensorFlow.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 17:52:07 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 18:41:45 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Tran", "Dustin", ""], ["Hoffman", "Matthew D.", ""], ["Saurous", "Rif A.", ""], ["Brevdo", "Eugene", ""], ["Murphy", "Kevin", ""], ["Blei", "David M.", ""]]}, {"id": "1701.04045", "submitter": "Valentin W\\\"ustholz", "authors": "Valentin W\\\"ustholz and Oswaldo Olivo and Marijn J. H. Heule and Isil\n  Dillig", "title": "Static Detection of DoS Vulnerabilities in Programs that use Regular\n  Expressions (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.FL cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an algorithmic complexity attack, a malicious party takes advantage of the\nworst-case behavior of an algorithm to cause denial-of-service. A prominent\nalgorithmic complexity attack is regular expression denial-of-service (ReDoS),\nin which the attacker exploits a vulnerable regular expression by providing a\ncarefully-crafted input string that triggers worst-case behavior of the\nmatching algorithm. This paper proposes a technique for automatically finding\nReDoS vulnerabilities in programs. Specifically, our approach automatically\nidentifies vulnerable regular expressions in the program and determines whether\nan \"evil\" input string can be matched against a vulnerable regular expression.\nWe have implemented our proposed approach in a tool called REXPLOITER and found\n41 exploitable security vulnerabilities in Java web applications.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 14:05:07 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["W\u00fcstholz", "Valentin", ""], ["Olivo", "Oswaldo", ""], ["Heule", "Marijn J. H.", ""], ["Dillig", "Isil", ""]]}, {"id": "1701.04089", "submitter": "Charles Grellois", "authors": "Ugo Dal Lago and Charles Grellois", "title": "Probabilistic Termination by Monadic Affine Sized Typing (Long Version)", "comments": "63 pages. To appear in ESOP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a system of monadic affine sized types, which substantially\ngeneralise usual sized types, and allows this way to capture probabilistic\nhigher-order programs which terminate almost surely. Going beyond plain, strong\nnormalisation without losing soundness turns out to be a hard task, which\ncannot be accomplished without a richer, quantitative notion of types, but also\nwithout imposing some affinity constraints. The proposed type system is\npowerful enough to type classic examples of probabilistically terminating\nprograms such as random walks. The way typable programs are proved to be almost\nsurely terminating is based on reducibility, but requires a substantial\nadaptation of the technique.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 18:04:44 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Lago", "Ugo Dal", ""], ["Grellois", "Charles", ""]]}, {"id": "1701.04481", "submitter": "EPTCS", "authors": "Paqui Lucio", "title": "A Tutorial on Using Dafny to Construct Verified Software", "comments": "In Proceedings PROLE 2016, arXiv:1701.03069", "journal-ref": "EPTCS 237, 2017, pp. 1-19", "doi": "10.4204/EPTCS.237.1", "report-no": null, "categories": "cs.SE cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a tutorial for newcomers to the field of automated verification\ntools, though we assume the reader to be relatively familiar with Hoare-style\nverification. In this paper, besides introducing the most basic features of the\nlanguage and verifier Dafny, we place special emphasis on how to use Dafny as\nan assistant in the development of verified programs. Our main aim is to\nencourage the software engineering community to make the move towards using\nformal verification tools.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 22:44:47 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Lucio", "Paqui", ""]]}, {"id": "1701.04522", "submitter": "EPTCS", "authors": "Iliano Cervesato, Maribel Fern\\'andez", "title": "Proceedings Fourth International Workshop on Linearity", "comments": null, "journal-ref": "EPTCS 238, 2017", "doi": "10.4204/EPTCS.238", "report-no": null, "categories": "cs.LO cs.CC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the papers presented at LINEARITY 2016, the Fourth\nInternational Workshop on Linearity, held on June 26, 2016 in Porto, Portugal.\nThe workshop was a one-day satellite event of FSCD 2016, the first\nInternational Conference on Formal Structures for Computation and Deduction.\n  The aim of this workshop was to bring together researchers who are developing\ntheory and applications of linear calculi, to foster their interaction and\nprovide a forum for presenting new ideas and work in progress, and enable\nnewcomers to learn about current activities in this area. Of interest were new\nresults that made a central use of linearity, ranging from foundational work to\napplications in any field. This included: sub-linear logics, linear term\ncalculi, linear type systems, linear proof-theory, linear programming\nlanguages, applications to concurrency, interaction-based systems, verification\nof linear systems, and biological and chemical models of computation.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 03:45:13 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Cervesato", "Iliano", ""], ["Fern\u00e1ndez", "Maribel", ""]]}, {"id": "1701.04914", "submitter": "Bernhard Kragl", "authors": "Krishnendu Chatterjee, Bernhard Kragl, Samarth Mishra, Andreas\n  Pavlogiannis", "title": "Faster Algorithms for Weighted Recursive State Machines", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-662-54434-1_11", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pushdown systems (PDSs) and recursive state machines (RSMs), which are\nlinearly equivalent, are standard models for interprocedural analysis. Yet RSMs\nare more convenient as they (a) explicitly model function calls and returns,\nand (b) specify many natural parameters for algorithmic analysis, e.g., the\nnumber of entries and exits. We consider a general framework where RSM\ntransitions are labeled from a semiring and path properties are algebraic with\nsemiring operations, which can model, e.g., interprocedural reachability and\ndataflow analysis problems.\n  Our main contributions are new algorithms for several fundamental problems.\nAs compared to a direct translation of RSMs to PDSs and the best-known existing\nbounds of PDSs, our analysis algorithm improves the complexity for\nfinite-height semirings (that subsumes reachability and standard dataflow\nproperties). We further consider the problem of extracting distance values from\nthe representation structures computed by our algorithm, and give efficient\nalgorithms that distinguish the complexity of a one-time preprocessing from the\ncomplexity of each individual query. Another advantage of our algorithm is that\nour improvements carry over to the concurrent setting, where we improve the\nbest-known complexity for the context-bounded analysis of concurrent RSMs.\nFinally, we provide a prototype implementation that gives a significant\nspeed-up on several benchmarks from the SLAM/SDV project.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 01:28:05 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Kragl", "Bernhard", ""], ["Mishra", "Samarth", ""], ["Pavlogiannis", "Andreas", ""]]}, {"id": "1701.04929", "submitter": "EPTCS", "authors": "Max Willsey (Carnegie Mellon University), Rokhini Prabhu (Carnegie\n  Mellon University), Frank Pfenning (Carnegie Mellon University)", "title": "Design and Implementation of Concurrent C0", "comments": "In Proceedings LINEARITY 2016, arXiv:1701.04522. Extended version at:\n  http://mwillsey.com/papers/cc0-thesis.pdf", "journal-ref": "EPTCS 238, 2017, pp. 73-82", "doi": "10.4204/EPTCS.238.8", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Concurrent C0, a type-safe C-like language with contracts and\nsession-typed communication over channels. Concurrent C0 supports an operation\ncalled forwarding which allows channels to be combined in a well-defined way.\nThe language's type system enables elegant expression of session types and\nmessage-passing concurrent programs. We provide a Go-based implementation with\nlanguage based optimizations that outperforms traditional message passing\ntechniques.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 02:58:50 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Willsey", "Max", "", "Carnegie Mellon University"], ["Prabhu", "Rokhini", "", "Carnegie\n  Mellon University"], ["Pfenning", "Frank", "", "Carnegie Mellon University"]]}, {"id": "1701.05034", "submitter": "Keehang Kwon", "authors": "Keehang Kwon and Daeseong Kang", "title": "Local Modules in Imperative Languages", "comments": "5 pages. A high-level statement for allocating and deallocating heap\n  objects is described and a constructive module language is also described", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a notion of local modules for imperative langauges. To be\nspecific, we introduce a new implication statement of the form $D \\supset G$\nwhere $D$ is a module (i.e., a set of procedure declarations) and $G$ is a\nstatement. This statement tells the machine to add $D$ to the program in the\ncourse of executing $G$. Thus, $D$ acts as a local module and will be discarded\nafter executing $G$. It therefore provides efficient module management. We\nillustrate our idea via C^{mod}, an extension of the core C with the new\nstatement. In addition, we describe a new constructive module language to\nimprove code reuse. Finally, we describe a scheme which considerably improves\nthe heap management in traditional languages.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 12:26:19 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 03:23:16 GMT"}, {"version": "v3", "created": "Wed, 11 Oct 2017 04:46:27 GMT"}, {"version": "v4", "created": "Thu, 19 Oct 2017 06:12:27 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Kwon", "Keehang", ""], ["Kang", "Daeseong", ""]]}, {"id": "1701.05463", "submitter": "Artem Khyzha", "authors": "Artem Khyzha, Mike Dodds, Alexey Gotsman, Matthew Parkinson", "title": "Proving Linearizability Using Partial Orders (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linearizability is the commonly accepted notion of correctness for concurrent\ndata structures. It requires that any execution of the data structure is\njustified by a linearization --- a linear order on operations satisfying the\ndata structure's sequential specification. Proving linearizability is often\nchallenging because an operation's position in the linearization order may\ndepend on future operations. This makes it very difficult to incrementally\nconstruct the linearization in a proof.\n  We propose a new proof method that can handle data structures with such\nfuture-dependent linearizations. Our key idea is to incrementally construct not\na single linear order of operations, but a partial order that describes\nmultiple linearizations satisfying the sequential specification. This allows\ndecisions about the ordering of operations to be delayed, mirroring the\nbehaviour of data structure implementations. We formalise our method as a\nprogram logic based on rely-guarantee reasoning, and demonstrate its\neffectiveness by verifying several challenging data structures: the\nHerlihy-Wing queue, the TS queue and the Optimistic set.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 15:13:14 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 15:58:01 GMT"}, {"version": "v3", "created": "Wed, 28 Jun 2017 23:04:36 GMT"}, {"version": "v4", "created": "Thu, 6 Jul 2017 13:35:08 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Khyzha", "Artem", ""], ["Dodds", "Mike", ""], ["Gotsman", "Alexey", ""], ["Parkinson", "Matthew", ""]]}, {"id": "1701.05650", "submitter": "Yulei Sui Yulei Sui", "authors": "Yulei Sui, Jingling Xue", "title": "Demand-Driven Pointer Analysis with Strong Updates via Value-Flow\n  Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new demand-driven flow- and context-sensitive pointer analysis\nwith strong updates for C programs, called SUPA, that enables computing\npoints-to information via value-flow refinement, in environments with small\ntime and memory budgets such as IDEs. We formulate SUPA by solving a graph\nreachability problem on an inter-procedural value-flow graph representing a\nprogram's def-use chains, which are pre-computed efficiently but\nover-approximately. To answer a client query (a request for a variable's\npoints-to set), SUPA reasons about the flow of values along the pre-computed\ndef-use chains sparsely (rather than across all program points), by performing\nonly the work necessary for the query (rather than analyzing the whole\nprogram). In particular, strong updates are performed to filter out spurious\ndef-use chains through value-flow refinement as long as the total budget is not\nexhausted. SUPA facilitates efficiency and precision tradeoffs by applying\ndifferent pointer analyses in a hybrid multi-stage analysis framework.\n  We have implemented SUPA in LLVM (3.5.0) and evaluate it by choosing\nuninitialized pointer detection as a major client on 18 open-source C programs.\nAs the analysis budget increases, SUPA achieves improved precision, with its\nsingle-stage flow-sensitive analysis reaching 97.4% of that achieved by\nwhole-program flow-sensitive analysis by consuming about 0.18 seconds and 65KB\nof memory per query, on average (with a budget of at most 10000 value-flow\nedges per query). With context-sensitivity also considered, SUPA's two- stage\nanalysis becomes more precise for some programs but also incurs more analysis\ntimes. SUPA is also amenable to parallelization. A parallel implementation of\nits single-stage flow-sensitive analysis achieves a speedup of up to 6.9x with\nan average of 3.05x a 8-core machine with respect its sequential version.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 00:51:09 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Sui", "Yulei", ""], ["Xue", "Jingling", ""]]}, {"id": "1701.05888", "submitter": "Joseph Tassarotti", "authors": "Joseph Tassarotti, Ralf Jung, Robert Harper", "title": "A Higher-Order Logic for Concurrent Termination-Preserving Refinement", "comments": "78 pages, extended version of a conference paper for ESOP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compiler correctness proofs for higher-order concurrent languages are\ndifficult: they involve establishing a termination-preserving refinement\nbetween a concurrent high-level source language and an implementation that uses\nlow-level shared memory primitives. However, existing logics for proving\nconcurrent refinement either neglect properties such as termination, or only\nhandle first-order state. In this paper, we address these limitations by\nextending Iris, a recent higher-order concurrent separation logic, with support\nfor reasoning about termination-preserving refinements. To demonstrate the\npower of these extensions, we prove the correctness of an efficient\nimplementation of a higher-order, session-typed language. To our knowledge,\nthis is the first program logic capable of giving a compiler correctness proof\nfor such a language. The soundness of our extensions and our compiler\ncorrectness proof have been mechanized in Coq.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 18:42:43 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Tassarotti", "Joseph", ""], ["Jung", "Ralf", ""], ["Harper", "Robert", ""]]}, {"id": "1701.06104", "submitter": "Xiaoxiao Yang", "authors": "Xiaoxiao Yang, Joost-Pieter Katoen, Huimin Lin, Hao Wu", "title": "Verifying Concurrent Stacks by Divergence-Sensitive Bisimulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The verification of linearizability -- a key correctness criterion for\nconcurrent objects -- is based on trace refinement whose checking is\nPSPACE-complete. This paper suggests to use \\emph{branching} bisimulation\ninstead. Our approach is based on comparing an abstract specification in which\nobject methods are executed atomically to a real object program. Exploiting\ndivergence sensitivity, this also applies to progress properties such as\nlock-freedom. These results enable the use of \\emph{polynomial-time}\ndivergence-sensitive branching bisimulation checking techniques for verifying\nlinearizability and progress. We conducted the experiment on concurrent\nlock-free stacks to validate the efficiency and effectiveness of our methods.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 23:41:35 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 02:38:45 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Yang", "Xiaoxiao", ""], ["Katoen", "Joost-Pieter", ""], ["Lin", "Huimin", ""], ["Wu", "Hao", ""]]}, {"id": "1701.06477", "submitter": "Justin Hsu", "authors": "Gilles Barthe and Thomas Espitau and Benjamin Gr\\'egoire and Justin\n  Hsu and Pierre-Yves Strub", "title": "Proving uniformity and independence by self-composition and coupling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proof by coupling is a classical proof technique for establishing\nprobabilistic properties of two probabilistic processes, like stochastic\ndominance and rapid mixing of Markov chains. More recently, couplings have been\ninvestigated as a useful abstraction for formal reasoning about relational\nproperties of probabilistic programs, in particular for modeling\nreduction-based cryptographic proofs and for verifying differential privacy. In\nthis paper, we demonstrate that probabilistic couplings can be used for\nverifying non-relational probabilistic properties. Specifically, we show that\nthe program logic pRHL---whose proofs are formal versions of proofs by\ncoupling---can be used for formalizing uniformity and probabilistic\nindependence. We formally verify our main examples using the EasyCrypt proof\nassistant.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 16:23:29 GMT"}, {"version": "v2", "created": "Sat, 1 Apr 2017 20:51:43 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Barthe", "Gilles", ""], ["Espitau", "Thomas", ""], ["Gr\u00e9goire", "Benjamin", ""], ["Hsu", "Justin", ""], ["Strub", "Pierre-Yves", ""]]}, {"id": "1701.06767", "submitter": "Matias Martinez", "authors": "Matias Martinez, Sylvain Lecomte", "title": "Towards the quality improvement of cross-platform mobile applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During last ten years, the number of smartphones and mobile applications has\nbeen constantly growing. Android, iOS and Windows Mobile are three mobile\nplatforms that cover almost all smartphones in the world in 2017. Developing a\nmobile app involves first to choose the platforms the app will run, and then to\ndevelop specific solutions (i.e., native apps) for each chosen platform using\nplatform-related toolkits such as AndroidSDK. Across-platform mobile\napplication is an app that runs on two or more mobile platforms. Several\nframeworks have been proposed to simplify the development of cross-platform\nmobile applications and to reduce development and maintenance costs.They are\ncalled cross-platform mobile app development frameworks.However, to our\nknowledge, the life-cycle and the quality of cross-platforms mobile\napplications built using those frameworks have not been studied in depth. Our\nmain goal is to first study the processes of development and maintenance of\nmobile applications built using cross-platform mobile app development\nframeworks, focusing particularly on the bug-fixing activity. Then, we aim at\ndefining tools for automated repairing bugs from cross-platform mobile\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 08:49:21 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 15:23:22 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Martinez", "Matias", ""], ["Lecomte", "Sylvain", ""]]}, {"id": "1701.07123", "submitter": "EPTCS", "authors": "Guillermo Vigueras (IMDEA Software Institute), Manuel Carro (IMDEA\n  Software Institute and Universidad Polit\\'ecnica de Madrid), Salvador Tamarit\n  (Universidad Polit\\'ecnica de Madrid), Julio Mari\\~no (Universidad\n  Polit\\'ecnica de Madrid)", "title": "Towards Automatic Learning of Heuristics for Mechanical Transformations\n  of Procedural Code", "comments": "In Proceedings PROLE 2016, arXiv:1701.03069. This paper is based on\n  arXiv:1603.03022, and has a thorough description of the proposed approach", "journal-ref": "EPTCS 237, 2017, pp. 52-67", "doi": "10.4204/EPTCS.237.4", "report-no": "EPTCS 1701", "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current trends in next-generation exascale systems go towards integrating\na wide range of specialized (co-)processors into traditional supercomputers.\nDue to the efficiency of heterogeneous systems in terms of Watts and FLOPS per\nsurface unit, opening the access of heterogeneous platforms to a wider range of\nusers is an important problem to be tackled. However, heterogeneous platforms\nlimit the portability of the applications and increase development complexity\ndue to the programming skills required. Program transformation can help make\nprogramming heterogeneous systems easier by defining a step-wise transformation\nprocess that translates a given initial code into a semantically equivalent\nfinal code, but adapted to a specific platform. Program transformation systems\nrequire the definition of efficient transformation strategies to tackle the\ncombinatorial problem that emerges due to the large set of transformations\napplicable at each step of the process. In this paper we propose a machine\nlearning-based approach to learn heuristics to define program transformation\nstrategies. Our approach proposes a novel combination of reinforcement learning\nand classification methods to efficiently tackle the problems inherent to this\ntype of systems. Preliminary results demonstrate the suitability of this\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 01:20:34 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Vigueras", "Guillermo", "", "IMDEA Software Institute"], ["Carro", "Manuel", "", "IMDEA\n  Software Institute and Universidad Polit\u00e9cnica de Madrid"], ["Tamarit", "Salvador", "", "Universidad Polit\u00e9cnica de Madrid"], ["Mari\u00f1o", "Julio", "", "Universidad\n  Polit\u00e9cnica de Madrid"]]}, {"id": "1701.07125", "submitter": "EPTCS", "authors": "Emilio Jes\\'us Gallego Arias (MINES ParisTech, PSL Research\n  University, France), Beno\\^it Pin (MINES ParisTech, PSL Research University,\n  France), Pierre Jouvelot (MINES ParisTech, PSL Research University, France)", "title": "jsCoq: Towards Hybrid Theorem Proving Interfaces", "comments": "In Proceedings UITP 2016, arXiv:1701.06745", "journal-ref": "EPTCS 239, 2017, pp. 15-27", "doi": "10.4204/EPTCS.239.2", "report-no": null, "categories": "cs.PL cs.HC cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe jsCcoq, a new platform and user environment for the Coq\ninteractive proof assistant. The jsCoq system targets the HTML5-ECMAScript 2015\nspecification, and it is typically run inside a standards-compliant browser,\nwithout the need of external servers or services. Targeting educational use,\njsCoq allows the user to start interaction with proof scripts right away,\nthanks to its self-contained nature. Indeed, a full Coq environment is packed\nalong the proof scripts, easing distribution and installation. Starting to use\njsCoq is as easy as clicking on a link. The current release ships more than 10\npopular Coq libraries, and supports popular books such as Software Foundations\nor Certified Programming with Dependent Types. The new target platform has\nopened up new interaction and display possibilities. It has also fostered the\ndevelopment of some new Coq-related technology. In particular, we have\nimplemented a new serialization-based protocol for interaction with the proof\nassistant, as well as a new package format for library distribution.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 01:21:14 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Arias", "Emilio Jes\u00fas Gallego", "", "MINES ParisTech, PSL Research\n  University, France"], ["Pin", "Beno\u00eet", "", "MINES ParisTech, PSL Research University,\n  France"], ["Jouvelot", "Pierre", "", "MINES ParisTech, PSL Research University, France"]]}, {"id": "1701.07232", "submitter": "Rishabh Singh", "authors": "Patrice Godefroid, Hila Peleg, Rishabh Singh", "title": "Learn&Fuzz: Machine Learning for Input Fuzzing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.LG cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzing consists of repeatedly testing an application with modified, or\nfuzzed, inputs with the goal of finding security vulnerabilities in\ninput-parsing code. In this paper, we show how to automate the generation of an\ninput grammar suitable for input fuzzing using sample inputs and\nneural-network-based statistical machine-learning techniques. We present a\ndetailed case study with a complex input format, namely PDF, and a large\ncomplex security-critical parser for this format, namely, the PDF parser\nembedded in Microsoft's new Edge browser. We discuss (and measure) the tension\nbetween conflicting learning and fuzzing goals: learning wants to capture the\nstructure of well-formed inputs, while fuzzing wants to break that structure in\norder to cover unexpected code paths and find bugs. We also present a new\nalgorithm for this learn&fuzz challenge which uses a learnt input probability\ndistribution to intelligently guide where to fuzz inputs.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 10:01:39 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Godefroid", "Patrice", ""], ["Peleg", "Hila", ""], ["Singh", "Rishabh", ""]]}, {"id": "1701.07842", "submitter": "Nicholas V. Lewchenko", "authors": "Arjun Radhakrishna, Nicholas V. Lewchenko, Shawn Meier, Sergio Mover,\n  Krishna Chaitanya Sripada, Damien Zufferey, Bor-Yuh Evan Chang, and Pavol\n  \\v{C}ern\\'y", "title": "DroidStar: Callback Typestates for Android Classes", "comments": "Appearing at ICSE 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-driven programming frameworks, such as Android, are based on components\nwith asynchronous interfaces. The protocols for interacting with these\ncomponents can often be described by finite-state machines we dub *callback\ntypestates*. Callback typestates are akin to classical typestates, with the\ndifference that their outputs (callbacks) are produced asynchronously. While\nuseful, these specifications are not commonly available, because writing them\nis difficult and error-prone.\n  Our goal is to make the task of producing callback typestates significantly\neasier. We present a callback typestate assistant tool, DroidStar, that\nrequires only limited user interaction to produce a callback typestate. Our\napproach is based on an active learning algorithm, L*. We improved the\nscalability of equivalence queries (a key component of L*), thus making active\nlearning tractable on the Android system.\n  We use DroidStar to learn callback typestates for Android classes both for\ncases where one is already provided by the documentation, and for cases where\nthe documentation is unclear. The results show that DroidStar learns callback\ntypestates accurately and efficiently. Moreover, in several cases, the\nsynthesized callback typestates uncovered surprising and undocumented\nbehaviors.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 19:06:45 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 23:43:09 GMT"}, {"version": "v3", "created": "Fri, 2 Mar 2018 18:45:04 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Radhakrishna", "Arjun", ""], ["Lewchenko", "Nicholas V.", ""], ["Meier", "Shawn", ""], ["Mover", "Sergio", ""], ["Sripada", "Krishna Chaitanya", ""], ["Zufferey", "Damien", ""], ["Chang", "Bor-Yuh Evan", ""], ["\u010cern\u00fd", "Pavol", ""]]}, {"id": "1701.07925", "submitter": "EPTCS", "authors": "Catherine Dubois (ENSIIE), Paolo Masci (HASLab, INESC TEC), Dominique\n  M\\'ery (Universit\\'e de Lorraine)", "title": "Proceedings of the Third Workshop on Formal Integrated Development\n  Environment", "comments": null, "journal-ref": "EPTCS 240, 2017", "doi": "10.4204/EPTCS.240", "report-no": null, "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of F-IDE 2016, the third international\nworkshop on Formal Integrated Development Environment, which was held as an FM\n2016 satellite event, on November 8, 2016, in Limassol (Cyprus). High levels of\nsafety, security and also privacy standards require the use of formal methods\nto specify and develop compliant software (sub)systems. Any standard comes with\nan assessment process, which requires a complete documentation of the\napplication in order to ease the justification of design choices and the review\nof code and proofs. Thus tools are needed for handling specifications, program\nconstructs and verification artifacts. The aim of the F-IDE workshop is to\nprovide a forum for presenting and discussing research efforts as well as\nexperience returns on design, development and usage of formal IDE aiming at\nmaking formal methods \"easier\" for both specialists and non-specialists.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 02:23:26 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Dubois", "Catherine", "", "ENSIIE"], ["Masci", "Paolo", "", "HASLab, INESC TEC"], ["M\u00e9ry", "Dominique", "", "Universit\u00e9 de Lorraine"]]}, {"id": "1701.08030", "submitter": "Valentin Touzeau", "authors": "Valentin Touzeau (VERIMAG - IMAG), Claire Ma\\\"iza (VERIMAG - IMAG),\n  David Monniaux (VERIMAG - IMAG)", "title": "Model Checking of Cache for WCET Analysis Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On real-time systems running under timing constraints, scheduling can be\nperformed when one is aware of the worst case execution time (WCET) of tasks.\nUsually, the WCET of a task is unknown and schedulers make use of safe\nover-approximations given by static WCET analysis. To reduce the\nover-approximation, WCET analysis has to gain information about the underlying\nhardware behavior, such as pipelines and caches. In this paper, we focus on the\ncache analysis, which classifies memory accesses as hits/misses according to\nthe set of possible cache states. We propose to refine the results of classical\ncache analysis using a model checker, introducing a new cache model for the\nleast recently used (LRU) policy.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 12:33:58 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 09:04:04 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Touzeau", "Valentin", "", "VERIMAG - IMAG"], ["Ma\u00efza", "Claire", "", "VERIMAG - IMAG"], ["Monniaux", "David", "", "VERIMAG - IMAG"]]}, {"id": "1701.08114", "submitter": "Alexandru Florin Iosif-Lazar", "authors": "Alexandru Florin Iosif-Lazar (IT University of Copenhagen, Denmark),\n  Jean Melo (IT University of Copenhagen, Denmark), Aleksandar S. Dimovski (IT\n  University of Copenhagen, Denmark), Claus Brabrand (IT University of\n  Copenhagen, Denmark), Andrzej Wasowski (IT University of Copenhagen, Denmark)", "title": "Effective Analysis of C Programs by Rewriting Variability", "comments": "The Art, Science, and Engineering of Programming, Vol. 1, Issue 1,\n  Article 1", "journal-ref": "The Art, Science, and Engineering of Programming, 2017, Vol. 1,\n  Issue 1, Article 1", "doi": "10.22152/programming-journal.org/2017/1/1", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context. Variability-intensive programs (program families) appear in many\napplication areas and for many reasons today. Different family members, called\nvariants, are derived by switching statically configurable options (features)\non and off, while reuse of the common code is maximized.\n  Inquiry. Verification of program families is challenging since the number of\nvariants is exponential in the number of features. Existing single-program\nanalysis and verification tools cannot be applied directly to program families,\nand designing and implementing the corresponding variability-aware versions is\ntedious and laborious.\n  Approach. In this work, we propose a range of variability-related\ntransformations for translating program families into single programs by\nreplacing compile-time variability with run-time variability (non-determinism).\nThe obtained transformed programs can be subsequently analyzed using the\nconventional off- the-shelf single-program analysis tools such as type\ncheckers, symbolic executors, model checkers, and static analyzers.\n  Knowledge. Our variability-related transformations are outcome-preserving,\nwhich means that the relation between the outcomes in the transformed single\nprogram and the union of outcomes of all variants derived from the original\nprogram family is equality.\n  Grounding. We show our transformation rules and their correctness with\nrespect to a minimal core imperative language IMP. Then, we discuss our\nexperience of implementing and using the transformations for efficient and\neffective analysis and verification of real-world C program families.\n  Importance. We report some interesting variability-related bugs that we\ndiscovered using various state-of-the-art single-program C verification tools,\nsuch as Frama-C, Clang, LLBMC.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 16:55:51 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Iosif-Lazar", "Alexandru Florin", "", "IT University of Copenhagen, Denmark"], ["Melo", "Jean", "", "IT University of Copenhagen, Denmark"], ["Dimovski", "Aleksandar S.", "", "IT\n  University of Copenhagen, Denmark"], ["Brabrand", "Claus", "", "IT University of\n  Copenhagen, Denmark"], ["Wasowski", "Andrzej", "", "IT University of Copenhagen, Denmark"]]}, {"id": "1701.08119", "submitter": "David H. Lorenz", "authors": "David H. Lorenz (Open University of Israel, Israel), Boaz Rosenan\n  (University of Haifa, Israel)", "title": "Application Embedding: A Language Approach to Declarative Web\n  Programming", "comments": "The Art, Science, and Engineering of Programming, Vol. 1, Issue 1,\n  Article 2", "journal-ref": "The Art, Science, and Engineering of Programming, 2017, Vol. 1,\n  Issue 1, Article 2", "doi": "10.22152/programming-journal.org/2017/1/2", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the early days of the Web, web application developers have aspired to\ndevelop much of their applications declaratively. However, one aspect of the\napplication, namely its business-logic is constantly left imperative. In this\nwork we present Application Embedding, a novel approach to application\ndevelopment which allows all aspects of an application, including its\nbusiness-logic, to be programmed declaratively.\n  We develop this approach in a two-step process. First, we draw a mapping\nbetween web applications and Domain-Specific Languages (DSLs). Second, we note\nthat out of the two methods for implementing DSLs, namely as either internal or\nexternal, most traditional web applications correspond to external DSLs, while\nthe the technique that corresponds to DSL embedding (implementing internal\nDSLs) is left mostly unexplored.\n  By projecting the well-known technique of DSL embedding onto web\napplications, we derive a novel technique--Application Embedding. Application\nembedding offers a separation of code assets that encourages reuse of\nimperative code, while keeping all application-specific assets, including those\nspecifying its business- logic, declarative.\n  As validation, we implemented a simple, though nontrivial web application\nusing the proposed separation of assets. This implementation includes an\napplication-agnostic imperative host application named FishTank, intended to be\napplicable for a wide variety of web applications, and a declarative definition\nof the different aspects of the specific application, intended to be loaded on\nthat host.\n  Our method of separation of code assets facilitates a better separation of\nwork, in comparison to traditional methods. By this separation, host\napplication developers can focus mostly on the extra-functional aspects of a\nweb application, namely on improving performance, scalability, and\navailability, while developers of an embedded application can focus on the\nfunctional aspects of their application, without worrying about extra-\nfunctional concerns. The reusability of the host application makes the effort\nput into a better implementation cost-effective, since it can benefit all\napplications built on top of it.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 17:11:59 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Lorenz", "David H.", "", "Open University of Israel, Israel"], ["Rosenan", "Boaz", "", "University of Haifa, Israel"]]}, {"id": "1701.08122", "submitter": "Johannes H\\\"artel", "authors": "Johannes H\\\"artel (University of Koblenz-Landau, Germany), Lukas\n  H\\\"artel (University of Koblenz and Landau, Germany, Germany), Ralf L\\\"ammel\n  (Universit\\\"at Koblenz-Landau, Germany), Andrei Varanovich (Universit\\\"at\n  Koblenz-Landau, Germany), Marcel Heinz (University of Koblenz, Germany)", "title": "Interconnected Linguistic Architecture", "comments": "The Art, Science, and Engineering of Programming, Vol. 1, Issue 1,\n  Article 3", "journal-ref": "The Art, Science, and Engineering of Programming, 2017, Vol. 1,\n  Issue 1, Article 3", "doi": "10.22152/programming-journal.org/2017/1/3", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The context of the reported research is the documentation of software\ntechnologies such as object/relational mappers, web-application frameworks, or\ncode generators. We assume that documentation should model a macroscopic view\non usage scenarios of technologies in terms of involved artifacts, leveraged\nsoftware languages, data flows, conformance relationships, and others. In\nprevious work, we referred to such documentation also as 'linguistic\narchitecture'. The corresponding models may also be referred to as 'megamodels'\nwhile adopting this term from the technological space of modeling/model-driven\nengineering. This work is an inquiry into making such documentation less\nabstract and more effective by means of connecting (mega)models, systems, and\ndeveloper experience in several ways. To this end, we adopt an approach that is\nprimarily based on prototyping (i.e., implementa- tion of a megamodeling\ninfrastructure with all conceivable connections) and experimentation with\nshowcases (i.e., documentation of concrete software technologies). The\nknowledge gained by this research is a notion of interconnected linguistic\narchitecture on the grounds of connecting primary model elements, inferred\nmodel elements, static and runtime system artifacts, traceability links, system\ncontexts, knowledge resources, plugged interpretations of model elements, and\nIDE views. A corresponding suite of aspects of interconnected linguistic\narchitecture is systematically described. As to the grounding of this research,\nwe describe a literature survey which tracks scattered occurrences and thus\ndemonstrates the relevance of the identified aspects of interconnected\nlinguistic architecture. Further, we describe the MegaL/Xtext+IDE\ninfrastructure which realizes interconnected linguistic architecture. The\nimportance of this work lies in providing more formal (ontologically rich,\nnavigable, verifiable) documentation of software technologies helping\ndevelopers to better understand how to use technologies in new systems\n(prescriptive mode) or how technologies are used in existing systems\n(descriptive mode).\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 17:17:11 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["H\u00e4rtel", "Johannes", "", "University of Koblenz-Landau, Germany"], ["H\u00e4rtel", "Lukas", "", "University of Koblenz and Landau, Germany, Germany"], ["L\u00e4mmel", "Ralf", "", "Universit\u00e4t Koblenz-Landau, Germany"], ["Varanovich", "Andrei", "", "Universit\u00e4t\n  Koblenz-Landau, Germany"], ["Heinz", "Marcel", "", "University of Koblenz, Germany"]]}, {"id": "1701.08124", "submitter": "Ralf L\\\"ammel", "authors": "Ralf L\\\"ammel (University of Koblenz-Landau, Germany)", "title": "Relationship Maintenance in Software Language Repositories", "comments": "The Art, Science, and Engineering of Programming, Vol. 1, Issue 1,\n  Article 4", "journal-ref": "The Art, Science, and Engineering of Programming, 2017, Vol. 1,\n  Issue 1, Article 4", "doi": "10.22152/programming-journal.org/2017/1/4", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The context of this research is testing and building software systems and,\nspecifically, software language repositories (SLRs), i.e., repositories with\ncomponents for language processing (interpreters, translators, analyzers,\ntransformers, pretty printers, etc.). SLRs are typically set up for developing\nand using metaprogramming systems, language workbenches, language definition\nframeworks, executable semantic frameworks, and modeling frameworks. This work\nis an inquiry into testing and building SLRs in a manner that the repository is\nseen as a collection of language-typed artifacts being related by the\napplications of language-typed functions or relations which serve language\nprocessing. The notion of language is used in a broad sense to include text-,\ntree-, graph-based languages as well as representations based on interchange\nformats and also proprietary formats for serialization. The overall approach\nunderlying this research is one of language design driven by a complex case\nstudy, i.e., a specific SLR with a significant number of processed languages\nand language processors as well as a noteworthy heterogeneity in terms of\nrepresentation types and implementation languages. The knowledge gained by our\nresearch is best understood as a declarative language design for regression\ntesting and build management, we introduce a corresponding language Ueber with\nan executable semantics which maintains relationships between language-typed\nartifacts in an SLR. The grounding of the reported research is based on the\ncomprehensive, formal, executable (logic programming-based) definition of the\nUeber language and its systematic application to the management of the SLR YAS\nwhich consists of hundreds of language definition and processing components\n(such as interpreters and transformations) for more than thirty languages (not\ncounting different representation types) with Prolog, Haskell, Java, and Python\nbeing used as implementation languages. The importance of this work follows\nfrom the significant costs implied by regression testing and build management\nand also from the complexity of SLRs which calls for means to help with\nunderstanding.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 17:25:53 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["L\u00e4mmel", "Ralf", "", "University of Koblenz-Landau, Germany"]]}, {"id": "1701.08345", "submitter": "Azadeh Farzan", "authors": "Azadeh Farzan and Victor Nicolet", "title": "Automated Synthesis of Divide and Conquer Parallelism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper focuses on automated synthesis of divide-and-conquer parallelism,\nwhich is a common parallel programming skeleton supported by many\ncross-platform multithreaded libraries. The challenges of producing (manually\nor automatically) a correct divide-and-conquer parallel program from a given\nsequential code are two-fold: (1) assuming that individual worker threads\nexecute a code identical to the sequential code, the programmer has to provide\nthe extra code for dividing the tasks and combining the computation results,\nand (2) sometimes, the sequential code may not be usable as is, and may need to\nbe modified by the programmer. We address both challenges in this paper. We\npresent an automated synthesis technique for the case where no modifications to\nthe sequential code are required, and we propose an algorithm for modifying the\nsequential code to make it suitable for parallelization when some modification\nis necessary. The paper presents theoretical results for when this {\\em\nmodification} is efficiently possible, and experimental evaluation of the\ntechnique and the quality of the produced parallel programs.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 02:05:03 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Farzan", "Azadeh", ""], ["Nicolet", "Victor", ""]]}, {"id": "1701.08467", "submitter": "EPTCS", "authors": "Gurvan Le Guernic, Benoit Combemale, Jos\\'e A. Galindo", "title": "Industrial Experience Report on the Formal Specification of a Packet\n  Filtering Language Using the K Framework", "comments": "In Proceedings F-IDE 2016, arXiv:1701.07925", "journal-ref": "EPTCS 240, 2017, pp. 38-52", "doi": "10.4204/EPTCS.240.3", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many project-specific languages, including in particular filtering languages,\nare defined using non-formal specifications written in natural languages. This\nleads to ambiguities and errors in the specification of those languages. This\npaper reports on an industrial experiment on using a tool-supported language\nspecification framework (K) for the formal specification of the syntax and\nsemantics of a filtering language having a complexity similar to those of\nreal-life projects. This experimentation aims at estimating, in a specific\nindustrial setting, the difficulty and benefits of formally specifying a packet\nfiltering language using a tool-supported formal approach.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 03:32:43 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Guernic", "Gurvan Le", ""], ["Combemale", "Benoit", ""], ["Galindo", "Jos\u00e9 A.", ""]]}, {"id": "1701.08469", "submitter": "EPTCS", "authors": "Stefan Mitsch (Computer Science Department, Carnegie Mellon\n  University), Andr\\'e Platzer (Computer Science Department, Carnegie Mellon\n  University)", "title": "The KeYmaera X Proof IDE - Concepts on Usability in Hybrid Systems\n  Theorem Proving", "comments": "In Proceedings F-IDE 2016, arXiv:1701.07925", "journal-ref": "EPTCS 240, 2017, pp. 67-81", "doi": "10.4204/EPTCS.240.5", "report-no": null, "categories": "cs.LO cs.HC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid systems verification is quite important for developing correct\ncontrollers for physical systems, but is also challenging. Verification\nengineers, thus, need to be empowered with ways of guiding hybrid systems\nverification while receiving as much help from automation as possible. Due to\nundecidability, verification tools need sufficient means for intervening during\nthe verification and need to allow verification engineers to provide system\ndesign insights.\n  This paper presents the design ideas behind the user interface for the hybrid\nsystems theorem prover KeYmaera X. We discuss how they make it easier to prove\nhybrid systems as well as help learn how to conduct proofs in the first place.\nUnsurprisingly, the most difficult user interface challenges come from the\ndesire to integrate automation and human guidance. We also share thoughts how\nthe success of such a user interface design could be evaluated and anecdotal\nobservations about it.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 03:33:24 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Mitsch", "Stefan", "", "Computer Science Department, Carnegie Mellon\n  University"], ["Platzer", "Andr\u00e9", "", "Computer Science Department, Carnegie Mellon\n  University"]]}, {"id": "1701.08682", "submitter": "Ale\\v{s} Bizjak", "authors": "Parosh Aziz Abdulla, Mohamed Faouzi Atig, Ahmed Bouajjani, Tuan Phong\n  Ngo", "title": "A Load-Buffer Semantics for Total Store Ordering", "comments": "Logic in computer science", "journal-ref": "Logical Methods in Computer Science, Volume 14, Issue 1 (January\n  23, 2018) lmcs:4228", "doi": "10.23638/LMCS-14(1:9)2018", "report-no": null, "categories": "cs.FL cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of verifying safety properties of concurrent programs\nrunning over the Total Store Order (TSO) memory model. Known decision\nprocedures for this model are based on complex encodings of store buffers as\nlossy channels. These procedures assume that the number of processes is fixed.\nHowever, it is important in general to prove the correctness of a\nsystem/algorithm in a parametric way with an arbitrarily large number of\nprocesses.\n  In this paper, we introduce an alternative (yet equivalent) semantics to the\nclassical one for the TSO semantics that is more amenable to efficient\nalgorithmic verification and for the extension to parametric verification. For\nthat, we adopt a dual view where load buffers are used instead of store\nbuffers. The flow of information is now from the memory to load buffers. We\nshow that this new semantics allows (1) to simplify drastically the safety\nanalysis under TSO, (2) to obtain a spectacular gain in efficiency and\nscalability compared to existing procedures, and (3) to extend easily the\ndecision procedure to the parametric case, which allows obtaining a new\ndecidability result, and more importantly, a verification algorithm that is\nmore general and more efficient in practice than the one for bounded instances.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 16:26:43 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2017 16:08:39 GMT"}, {"version": "v3", "created": "Wed, 6 Dec 2017 12:57:27 GMT"}, {"version": "v4", "created": "Mon, 22 Jan 2018 08:39:46 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Abdulla", "Parosh Aziz", ""], ["Atig", "Mohamed Faouzi", ""], ["Bouajjani", "Ahmed", ""], ["Ngo", "Tuan Phong", ""]]}]