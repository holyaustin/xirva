[{"id": "2102.00230", "submitter": "Raviraj Joshi", "authors": "Pranali Bora, Tulika Awalgaonkar, Himanshu Palve, Raviraj Joshi, Purvi\n  Goel", "title": "ICodeNet -- A Hierarchical Neural Network Approach for Source Code\n  Author Identification", "comments": "Accepted at ICMLC 2021", "journal-ref": null, "doi": "10.1145/3457682.3457709", "report-no": null, "categories": "cs.LG cs.CV cs.IR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the open-source revolution, source codes are now more easily accessible\nthan ever. This has, however, made it easier for malicious users and\ninstitutions to copy the code without giving regards to the license, or credit\nto the original author. Therefore, source code author identification is a\ncritical task with paramount importance. In this paper, we propose ICodeNet - a\nhierarchical neural network that can be used for source code file-level tasks.\nThe ICodeNet processes source code in image format and is employed for the task\nof per file author identification. The ICodeNet consists of an ImageNet trained\nVGG encoder followed by a shallow neural network. The shallow network is based\neither on CNN or LSTM. Different variations of models are evaluated on a source\ncode author classification dataset. We have also compared our image-based\nhierarchical neural network model with simple image-based CNN architecture and\ntext-based CNN and LSTM models to highlight its novelty and efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2021 14:05:22 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Bora", "Pranali", ""], ["Awalgaonkar", "Tulika", ""], ["Palve", "Himanshu", ""], ["Joshi", "Raviraj", ""], ["Goel", "Purvi", ""]]}, {"id": "2102.00378", "submitter": "Yishuai Li", "authors": "Yishuai Li, Benjamin C. Pierce, and Steve Zdancewic", "title": "Model-Based Testing of Networked Applications", "comments": "11 pages, 15 figures", "journal-ref": "Proceedings of the 30th ACM SIGSOFT International Symposium on\n  Software Testing and Analysis (ISSTA 2021). Association for Computing\n  Machinery, New York, NY, USA, 529--539", "doi": "10.1145/3460319.3464798", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a principled automatic testing framework for application-layer\nprotocols. The key innovation is a domain-specific embedded language for\nwriting nondeterministic models of the behavior of networked servers. These\nmodels are defined within the Coq interactive theorem prover, supporting a\nsmooth transition from testing to formal verification.\n  Given a server model, we show how to automatically derive a tester that\nprobes the server for unexpected behaviors. We address the uncertainties caused\nby both the server's internal choices and the network delaying messages\nnondeterministically. The derived tester accepts server implementations whose\npossible behaviors are a subset of those allowed by the nondeterministic model.\n  We demonstrate the effectiveness of this framework by using it to specify and\ntest a fragment of the HTTP/1.1 protocol, showing that the automatically\nderived tester can capture RFC violations in buggy server implementations,\nincluding the latest versions of Apache and Nginx.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 04:58:10 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 21:33:29 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Li", "Yishuai", ""], ["Pierce", "Benjamin C.", ""], ["Zdancewic", "Steve", ""]]}, {"id": "2102.00510", "submitter": "Vladimir Zamdzhiev", "authors": "Xiaodong Jia, Bert Lindenhovius, Michael Mislove, Vladimir Zamdzhiev", "title": "Commutative Monads for Probabilistic Programming Languages", "comments": null, "journal-ref": null, "doi": "10.1109/LICS52264.2021.9470611", "report-no": null, "categories": "cs.LO cs.PL math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-standing open problem in the semantics of programming languages\nsupporting probabilistic choice is to find a commutative monad for probability\non the category DCPO. In this paper we present three such monads and a general\nconstruction for finding even more. We show how to use these monads to provide\na sound and adequate denotational semantics for the Probabilistic FixPoint\nCalculus (PFPC) -- a call-by-value simply-typed lambda calculus with\nmixed-variance recursive types, term recursion and probabilistic choice. We\nalso show that in the special case where we consider continuous dcpo's, then\nall three monads coincide with the valuations monad of Jones and we fully\ncharacterise the induced Eilenberg-Moore categories by showing that they are\nall isomorphic to the category of continuous Kegelspitzen of Keimel and\nPlotkin.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2021 18:26:38 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Jia", "Xiaodong", ""], ["Lindenhovius", "Bert", ""], ["Mislove", "Michael", ""], ["Zamdzhiev", "Vladimir", ""]]}, {"id": "2102.01024", "submitter": "Chenglong Wang", "authors": "Chenglong Wang and Yu Feng and Rastislav Bodik and Isil Dillig and\n  Alvin Cheung and Amy J. Ko", "title": "Falx: Synthesis-Powered Visualization Authoring", "comments": "CHI 2021", "journal-ref": null, "doi": "10.1145/3411764.3445249", "report-no": null, "categories": "cs.HC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern visualization tools aim to allow data analysts to easily create\nexploratory visualizations. When the input data layout conforms to the\nvisualization design, users can easily specify visualizations by mapping data\ncolumns to visual channels of the design. However, when there is a mismatch\nbetween data layout and the design, users need to spend significant effort on\ndata transformation.\n  We propose Falx, a synthesis-powered visualization tool that allows users to\nspecify visualizations in a similarly simple way but without needing to worry\nabout data layout. In Falx, users specify visualizations using examples of how\nconcrete values in the input are mapped to visual channels, and Falx\nautomatically infers the visualization specification and transforms the data to\nmatch the design. In a study with 33 data analysts on four visualization tasks\ninvolving data transformation, we found that users can effectively adopt Falx\nto create visualizations they otherwise cannot implement.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2021 18:01:20 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Wang", "Chenglong", ""], ["Feng", "Yu", ""], ["Bodik", "Rastislav", ""], ["Dillig", "Isil", ""], ["Cheung", "Alvin", ""], ["Ko", "Amy J.", ""]]}, {"id": "2102.01644", "submitter": "Jonathan Protzenko", "authors": "Jonathan Protzenko, Son Ho", "title": "Zero-cost meta-programmed stateful functors in F*", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Writing code is hard; proving it correct is even harder. As the scale of\nverified software projects reaches new heights, the problem of efficiently\nverifying large amounts of software becomes more and more salient. Nowhere is\nthis issue more evident than in the context of verified cryptographic\nlibraries. To achieve feature-parity and be competitive with unverified\ncryptographic libraries, a very large number of algorithms and APIs need to be\nverified. However, the task is oftentimes repetitive, and factoring out\ncommonality between algorithms is fraught with difficulties, requiring until\nnow a significant amount of manual effort.\n  This paper shows how a judicious combination of known functional programming\ntechniques leads to an order-of-magnitude improvement in the amount of verified\ncode produced by the popular HACL* cryptographic library, without compromising\nperformance. We review three techniques that build upon each other, in order of\nincreasing sophistication. First, we use dependent types to crisply capture the\nspecification and state machine of a block algorithm, a cryptographic notion\nthat was until now only informally and imprecisely specified. Next, we rely on\npartial evaluation to author a higher-order, stateful functor that transforms\nany unsafe block API into a safe counterpart. Finally, we rely on elaborator\nreflection to automate the very process of authoring a functor, using a\ncode-rewriting tactic. This culminates in a style akin to templatized C++ code,\nbut relying on a userland tactic and partial evaluation, rather than built-in\ncompiler support.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 17:58:49 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 20:14:51 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Protzenko", "Jonathan", ""], ["Ho", "Son", ""]]}, {"id": "2102.01687", "submitter": "Ignacio Laguna", "authors": "Hal Finkel, Ignacio Laguna", "title": "Report of the Workshop on Program Synthesis for Scientific Computing", "comments": "29 pages, workshop website:\n  https://prog-synth-science.github.io/2020/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Program synthesis is an active research field in academia, national labs, and\nindustry. Yet, work directly applicable to scientific computing, while having\nsome impressive successes, has been limited. This report reviews the relevant\nareas of program synthesis work for scientific computing, discusses successes\nto date, and outlines opportunities for future work. This report is the result\nof the Workshop on Program Synthesis for Scientific Computing was held\nvirtually on August 4-5 2020 (https://prog-synth-science.github.io/2020/).\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2021 18:55:23 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Finkel", "Hal", ""], ["Laguna", "Ignacio", ""]]}, {"id": "2102.02109", "submitter": "Nick Brown", "authors": "Maurice Jamieson, Nick Brown", "title": "Compact Native Code Generation for Dynamic Languages on Micro-core\n  Architectures", "comments": "Preprint of paper accepted to ACM SIGPLAN 2021 International\n  Conference on Compiler Construction (CC 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micro-core architectures combine many simple, low memory, low power-consuming\nCPU cores onto a single chip. Potentially providing significant performance and\nlow power consumption, this technology is not only of great interest in\nembedded, edge, and IoT uses, but also potentially as accelerators for\ndata-center workloads. Due to the restricted nature of such CPUs, these\narchitectures have traditionally been challenging to program, not least due to\nthe very constrained amounts of memory (often around 32KB) and idiosyncrasies\nof the technology. However, more recently, dynamic languages such as Python\nhave been ported to a number of micro-cores, but these are often delivered as\ninterpreters which have an associated performance limitation.\n  Targeting the four objectives of performance, unlimited code-size,\nportability between architectures, and maintaining the programmer productivity\nbenefits of dynamic languages, the limited memory available means that classic\ntechniques employed by dynamic language compilers, such as just-in-time (JIT),\nare simply not feasible. In this paper we describe the construction of a\ncompilation approach for dynamic languages on micro-core architectures which\naims to meet these four objectives, and use Python as a vehicle for exploring\nthe application of this in replacing the existing micro-core interpreter. Our\nexperiments focus on the metrics of performance, architecture portability,\nminimum memory size, and programmer productivity, comparing our approach\nagainst that of writing native C code. The outcome of this work is the\nidentification of a series of techniques that are not only suitable for\ncompiling Python code, but also applicable to a wide variety of dynamic\nlanguages on micro-cores.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2021 15:36:31 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Jamieson", "Maurice", ""], ["Brown", "Nick", ""]]}, {"id": "2102.02363", "submitter": "EPTCS", "authors": "Dan R. Ghica (Huawei Research, Edinburgh, University of Birmingham,\n  UK)", "title": "Operational Semantics with Hierarchical Abstract Syntax Graphs", "comments": "In Proceedings TERMGRAPH 2020, arXiv:2102.01804", "journal-ref": "EPTCS 334, 2021, pp. 1-10", "doi": "10.4204/EPTCS.334.1", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This is a motivating tutorial introduction to a semantic analysis of\nprogramming languages using a graphical language as the representation of\nterms, and graph rewriting as a representation of reduction rules. We show how\nthe graphical language automatically incorporates desirable features, such as\nalpha-equivalence and how it can describe pure computation, imperative store,\nand control features in a uniform framework. The graph semantics combines some\nof the best features of structural operational semantics and abstract machines,\nwhile offering powerful new methods for reasoning about contextual equivalence.\n  All technical details are available in an extended technical report by Muroya\nand the author and in Muroya's doctoral dissertation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 01:41:23 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Ghica", "Dan R.", "", "Huawei Research, Edinburgh, University of Birmingham,\n  UK"]]}, {"id": "2102.02627", "submitter": "Marco Peressotti", "authors": "Lu\\'is Cruz-Filipe and Fabrizio Montesi, and Marco Peressotti", "title": "Formalising a Turing-Complete Choreographic Language in Coq", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theory of choreographic languages typically includes a number of complex\nresults that are proved by structural induction. The high number of cases and\nthe subtle details in some of them lead to long reviewing processes, and\noccasionally to errors being found in published proofs. In this work, we take a\npublished proof of Turing completeness of a choreographic language and\nformalise it in Coq. Our development includes formalising the choreographic\nlanguage and its basic properties, Kleene's theory of partial recursive\nfunctions, the encoding of these functions as choreographies, and proving this\nencoding correct.\n  With this effort, we show that theorem proving can be a very useful tool in\nthe field of choreographic languages: besides the added degree of confidence\nthat we get from a mechanised proof, the formalisation process led us to a\nsignificant simplification of the underlying theory. Our results offer a\nfoundation for the future formal development of choreographic languages.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2021 14:25:25 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Cruz-Filipe", "Lu\u00eds", ""], ["Montesi", "Fabrizio", ""], ["Peressotti", "Marco", ""]]}, {"id": "2102.02966", "submitter": "Ramy Shahin", "authors": "Ramy Shahin", "title": "Towards Modal Software Engineering", "comments": "ICSE'21 NIER pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we introduce the notion of Modal Software Engineering:\nautomatically turning sequential, deterministic programs into semantically\nequivalent programs efficiently operating on inputs coming from multiple\noverlapping worlds. We are drawing an analogy between modal logics, and\nsoftware application domains where multiple sets of inputs (multiple worlds)\nneed to be processed efficiently. Typically those sets highly overlap, so\nprocessing them independently would involve a lot of redundancy, resulting in\nlower performance, and in many cases intractability. Three application domains\nare presented: reasoning about feature-based variability of Software Product\nLines (SPLs), probabilistic programming, and approximate programming.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 02:45:29 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 23:03:15 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Shahin", "Ramy", ""]]}, {"id": "2102.03614", "submitter": "Jeremie Lagraviere", "authors": "J\\'er\\'emie Lagravi\\`ere, Johannes Langguth, Martina Prugger, Phuong\n  H. Ha, Xing Cai", "title": "A Newcomer In The PGAS World -- UPC++ vs UPC: A Comparative Study", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A newcomer in the Partitioned Global Address Space (PGAS) 'world' has arrived\nin its version 1.0: Unified Parallel C++ (UPC++). UPC++ targets distributed\ndata structures where communication is irregular or fine-grained. The key\nabstractions are global pointers, asynchronous programming via RPC, futures and\npromises. UPC++ API for moving non-contiguous data and handling memories with\ndifferent optimal access methods resemble those used in modern C++. In this\nstudy we provide two kernels implemented in UPC++: a sparse-matrix vector\nmultiplication (SpMV) as part of a Partial-Differential Equation solver, and an\nimplementation of the Heat Equation on a 2D-domain. Code listings of these two\nkernels are available in the article in order to show the differences in\nprogramming style between UPC and UPC++. We provide a performance comparison\nbetween UPC and UPC++ using single-node, multi-node hardware and many-core\nhardware (Intel Xeon Phi Knight's Landing).\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2021 17:06:02 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Lagravi\u00e8re", "J\u00e9r\u00e9mie", ""], ["Langguth", "Johannes", ""], ["Prugger", "Martina", ""], ["Ha", "Phuong H.", ""], ["Cai", "Xing", ""]]}, {"id": "2102.03824", "submitter": "Mirco Giacobbe", "authors": "Mirco Giacobbe, Daniel Kroening, Julian Parsert", "title": "Neural Termination Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel approach to the automated termination analysis of\ncomputer programs: we train neural networks to act as ranking functions.\nRanking functions map program states to values that are bounded from below and\ndecrease as the program runs. The existence of a valid ranking function proves\nthat the program terminates. While in the past ranking functions were usually\nconstructed using static analysis, our method learns them from sampled\nexecutions. We train a neural network so that its output decreases along\nexecution traces as a ranking function would; then, we use formal reasoning to\nverify whether it generalises to all possible executions. We present a custom\nloss function for learning lexicographic ranking functions and use\nsatisfiability modulo theories for verification. Thanks to the ability of\nneural networks to generalise well, our method succeeds over a wide variety of\nprograms. This includes programs that use data structures from standard\nlibraries. We built a prototype analyser for Java bytecode and show the\nefficacy of our method over a standard dataset of benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 15:45:30 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Giacobbe", "Mirco", ""], ["Kroening", "Daniel", ""], ["Parsert", "Julian", ""]]}, {"id": "2102.04472", "submitter": "James Nightingale", "authors": "James. W. Nightingale, Richard G. Hayes, Matthew Griffiths", "title": "PyAutoFit: A Classy Probabilistic Programming Language for Model\n  Composition and Fitting", "comments": "Published in the Journal of Open Source Software", "journal-ref": "Journal of Open Source Software, 6(58), 2550 (2021)", "doi": "10.21105/joss.02550", "report-no": null, "categories": "astro-ph.IM cs.PL physics.data-an physics.ed-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A major trend in academia and data science is the rapid adoption of Bayesian\nstatistics for data analysis and modeling, leading to the development of\nprobabilistic programming languages (PPL). A PPL provides a framework that\nallows users to easily specify a probabilistic model and perform inference\nautomatically. PyAutoFit is a Python-based PPL which interfaces with all\naspects of the modeling (e.g., the model, data, fitting procedure,\nvisualization, results) and therefore provides complete management of every\naspect of modeling. This includes composing high-dimensionality models from\nindividual model components, customizing the fitting procedure and performing\ndata augmentation before a model-fit. Advanced features include database tools\nfor analysing large suites of modeling results and exploiting domain-specific\nknowledge of a problem via non-linear search chaining. Accompanying PyAutoFit\nis the autofit workspace (see https://github.com/Jammy2211/autofit_workspace),\nwhich includes example scripts and the HowToFit lecture series which introduces\nnon-experts to model-fitting and provides a guide on how to begin a project\nusing PyAutoFit. Readers can try PyAutoFit right now by going to the\nintroduction Jupyter notebook on Binder (see\nhttps://mybinder.org/v2/gh/Jammy2211/autofit_workspace/HEAD) or checkout our\nreadthedocs(see https://pyautofit.readthedocs.io/en/latest/) for a complete\noverview of PyAutoFit's features.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 19:00:03 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Nightingale", "James. W.", ""], ["Hayes", "Richard G.", ""], ["Griffiths", "Matthew", ""]]}, {"id": "2102.04625", "submitter": "Yu Wang", "authors": "Yu Wang, Ke Wang, Linzhang Wang", "title": "WheaCha: A Method for Explaining the Predictions of Code Summarization\n  Models", "comments": "The full abstract is presented in the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The last decade has witnessed a rapid advance in machine learning models.\nWhile the black-box nature of these systems allows powerful predictions, it\ncannot be directly explained, posing a threat to the continuing democratization\nof machine learning technology.\n  Tackling the challenge of model explainability, research has made significant\nprogress in demystifying the image classification models. In the same spirit of\nthese works, this paper studies code summarization models, particularly, given\nan input program for which a model makes a prediction, our goal is to reveal\nthe key features that the model uses for predicting the label of the program.\nWe realize our approach in HouYi, which we use to evaluate four prominent code\nsummarization models: extreme summarizer, code2vec, code2seq, and sequence GNN.\nResults show that all models base their predictions on syntactic and lexical\nproperties with little to none semantic implication. Based on this finding, we\npresent a novel approach to explaining the predictions of code summarization\nmodels through the lens of training data.\n  Our work opens up this exciting, new direction of studying what models have\nlearned from source code.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 03:17:46 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 01:48:28 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Wang", "Yu", ""], ["Wang", "Ke", ""], ["Wang", "Linzhang", ""]]}, {"id": "2102.04731", "submitter": "Marco Carbone", "authors": "Marco Carbone, Sonia Marin, Carsten Sch\\\"urmann", "title": "Synchronous Forwarders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session types are types for specifying protocols that processes must follow\nwhen communicating with each other. Session types are in a\npropositions-as-types correspondence with linear logic. Previous work has shown\nthat a multiparty session type, a generalisation of session types to protocols\nof two or more parties, can be modelled as a proof of coherence, a\ngeneralisation of linear logic duality. And, protocols expressed as coherence\ncan be simulated by arbiters, processes that act as a middleware by forwarding\nmessages according to the given protocol. In this paper, we generalise the\nconcept of arbiter to that of synchronous forwarder, that is a processes that\nimplements the behaviour of an arbiter in several different ways. In a\npropositions-as-types fashion, synchronous forwarders form a logic equipped\nwith cut elimination which is a special restriction of classical linear logic.\nOur main result shows that synchronous forwarders are a characterisation of\ncoherence, i.e., coherence proofs can be transformed into synchronous\nforwarders and, viceversa, every synchronous forwarder corresponds to a\ncoherence proofs.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 09:51:59 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Carbone", "Marco", ""], ["Marin", "Sonia", ""], ["Sch\u00fcrmann", "Carsten", ""]]}, {"id": "2102.05081", "submitter": "Simone Campanoni", "authors": "Angelo Matni and Enrico Armenio Deiana and Yian Su and Lukas Gross and\n  Souradip Ghosh and Sotiris Apostolakis and Ziyang Xu and Zujun Tan and Ishita\n  Chaturvedi and David I. August and Simone Campanoni", "title": "NOELLE Offers Empowering LLVM Extensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern and emerging architectures demand increasingly complex compiler\nanalyses and transformations. As the emphasis on compiler infrastructure moves\nbeyond support for peephole optimizations and the extraction of\ninstruction-level parallelism, they should support custom tools designed to\nmeet these demands with higher-level analysis-powered abstractions of wider\nprogram scope. This paper introduces NOELLE, a robust open-source\ndomain-independent compilation layer built upon LLVM providing this support.\nNOELLE is modular and demand-driven, making it easy-to-extend and adaptable to\ncustom-tool-specific needs without unduly wasting compile time and memory. This\npaper shows the power of NOELLE by presenting a diverse set of ten custom tools\nbuilt upon it, with a 33.2% to 99.2% reduction in code size (LoC) compared to\ntheir counterparts without NOELLE.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 19:16:41 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Matni", "Angelo", ""], ["Deiana", "Enrico Armenio", ""], ["Su", "Yian", ""], ["Gross", "Lukas", ""], ["Ghosh", "Souradip", ""], ["Apostolakis", "Sotiris", ""], ["Xu", "Ziyang", ""], ["Tan", "Zujun", ""], ["Chaturvedi", "Ishita", ""], ["August", "David I.", ""], ["Campanoni", "Simone", ""]]}, {"id": "2102.05161", "submitter": "Yann Hamdaoui", "authors": "Yann Hamdaoui and Beno\\^it Valiron", "title": "An Interactive Proof of Termination for a Concurrent $\\lambda$-calculus\n  with References and Explicit Substitutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we introduce a typed, concurrent $\\lambda$-calculus with\nreferences featuring explicit substitutions for variables and references.\nAlongside usual safety properties, we recover strong normalization. The proof\nis based on a reducibility technique and an original interactive property\nreminiscent of the Game Semantics approach.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 22:35:54 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Hamdaoui", "Yann", ""], ["Valiron", "Beno\u00eet", ""]]}, {"id": "2102.05187", "submitter": "Luanzheng Guo", "authors": "Ruiqin Tian, Luanzheng Guo, Jiajia Li, Bin Ren, Gokcen Kestor", "title": "A High-Performance Sparse Tensor Algebra Compiler in Multi-Level IR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Tensor algebra is widely used in many applications, such as scientific\ncomputing, machine learning, and data analytics. The tensors represented\nreal-world data are usually large and sparse. There are tens of storage formats\ndesigned for sparse matrices and/or tensors and the performance of sparse\ntensor operations depends on a particular architecture and/or selected sparse\nformat, which makes it challenging to implement and optimize every tensor\noperation of interest and transfer the code from one architecture to another.\nWe propose a tensor algebra domain-specific language (DSL) and compiler\ninfrastructure to automatically generate kernels for mixed sparse-dense tensor\nalgebra operations, named COMET. The proposed DSL provides high-level\nprogramming abstractions that resemble the familiar Einstein notation to\nrepresent tensor algebra operations. The compiler performs code optimizations\nand transformations for efficient code generation while covering a wide range\nof tensor storage formats. COMET compiler also leverages data reordering to\nimprove spatial or temporal locality for better performance. Our results show\nthat the performance of automatically generated kernels outperforms the\nstate-of-the-art sparse tensor algebra compiler, with up to 20.92x, 6.39x, and\n13.9x performance improvement, for parallel SpMV, SpMM, and TTM over TACO,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 23:43:53 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Tian", "Ruiqin", ""], ["Guo", "Luanzheng", ""], ["Li", "Jiajia", ""], ["Ren", "Bin", ""], ["Kestor", "Gokcen", ""]]}, {"id": "2102.05985", "submitter": "Tomasz Drab", "authors": "Ma{\\l}gorzata Biernacka (1), Witold Charatonik (1), Tomasz Drab (1)\n  ((1) Institute of Computer Science, University of Wroc{\\l}aw, Poland)", "title": "Strong Call by Value is Reasonable for Time", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The invariance thesis of Slot and van Emde Boas states that all reasonable\nmodels of computation simulate each other with polynomially bounded overhead in\ntime and constant-factor overhead in space. In this paper we show that a family\nof strong call-by-value strategies in the $\\lambda$-calculus are reasonable for\ntime. The proof is based on a construction of an appropriate abstract machine,\nsystematically derived using Danvy et al.'s functional correspondence that\nconnects higher-order interpreters with abstract-machine models by a\nwell-established transformation technique. This is the first machine that\nimplements a strong CbV strategy and simulates $\\beta$-reduction with the\noverhead polynomial in the number of $\\beta$-steps and in the size of the\ninitial term. We prove this property using a form of amortized cost analysis\n\\`a la Okasaki.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2021 13:10:56 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Biernacka", "Ma\u0142gorzata", "", "Institute of Computer Science, University of Wroc\u0142aw, Poland"], ["Charatonik", "Witold", "", "Institute of Computer Science, University of Wroc\u0142aw, Poland"], ["Drab", "Tomasz", "", "Institute of Computer Science, University of Wroc\u0142aw, Poland"]]}, {"id": "2102.06513", "submitter": "Meven Lennon-Bertrand", "authors": "Meven Lennon-Bertrand", "title": "Complete Bidirectional Typing for the Calculus of Inductive\n  Constructions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This article presents a bidirectional type system for the Calculus of\nInductive Constructions (CIC). It introduces a new judgement intermediate\nbetween the usual inference and checking, dubbed constrained inference, to\nhandle the presence of computation in types. The key property of the system is\nits completeness with respect to the usual undirected one, which has been\nformally proven in Coq as a part of the MetaCoq project. Although it plays an\nimportant role in an ongoing completeness proof for a realistic typing\nalgorithm, the interest of bidirectionality is wider, as it gives insights and\nstructure when trying to prove properties on CIC or design variations and\nextensions. In particular, we put forward constrained inference, an\nintermediate between the usual inference and checking judgements, to handle the\npresence of computation in types.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 13:31:56 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 14:07:43 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Lennon-Bertrand", "Meven", ""]]}, {"id": "2102.06599", "submitter": "Jack Turner", "authors": "Jack Turner, Elliot J. Crowley, Michael O'Boyle", "title": "Neural Architecture Search as Program Transformation Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Improving the performance of deep neural networks (DNNs) is important to both\nthe compiler and neural architecture search (NAS) communities. Compilers apply\nprogram transformations in order to exploit hardware parallelism and memory\nhierarchy. However, legality concerns mean they fail to exploit the natural\nrobustness of neural networks. In contrast, NAS techniques mutate networks by\noperations such as the grouping or bottlenecking of convolutions, exploiting\nthe resilience of DNNs. In this work, we express such neural architecture\noperations as program transformations whose legality depends on a notion of\nrepresentational capacity. This allows them to be combined with existing\ntransformations into a unified optimization framework. This unification allows\nus to express existing NAS operations as combinations of simpler\ntransformations. Crucially, it allows us to generate and explore new tensor\nconvolutions. We prototyped the combined framework in TVM and were able to find\noptimizations across different DNNs, that significantly reduce inference time -\nover 3$\\times$ in the majority of cases.\n  Furthermore, our scheme dramatically reduces NAS search time. Code is\navailable\nat~\\href{https://github.com/jack-willturner/nas-as-program-transformation-exploration}{this\nhttps url}.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 16:11:05 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Turner", "Jack", ""], ["Crowley", "Elliot J.", ""], ["O'Boyle", "Michael", ""]]}, {"id": "2102.06753", "submitter": "Jingbo Wang", "authors": "Jingbo Wang and Chungha Sung and Mukund Raghothaman and Chao Wang", "title": "Data-Driven Synthesis of Provably Sound Side Channel Analyses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-driven method for synthesizing a static analyzer to detect\nside-channel information leaks in cryptographic software. Compared to the\nconventional way of manually crafting such a static analyzer, which can be\nlabor intensive, error prone and suboptimal, our learning-based technique is\nnot only automated but also provably sound. Our analyzer consists of a set of\ntype-inference rules learned from the training data, i.e., example code\nsnippets annotated with ground truth. Internally, we use syntax-guided\nsynthesis (SyGuS) to generate new features and decision tree learning (DTL) to\ngenerate type-inference rules based on these features. We guarantee soundness\nby formally proving each learned rule via a technique called Datalog query\ncontainment checking. We have implemented our technique in the LLVM compiler\nand used it to detect power side channels in C programs. Our results show that,\nin addition to being automated and provably sound during synthesis, the learned\nanalyzer also has the same empirical accuracy as two state-of-the-art, manually\ncrafted analyzers while being 300X and 900X faster, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2021 20:14:34 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Wang", "Jingbo", ""], ["Sung", "Chungha", ""], ["Raghothaman", "Mukund", ""], ["Wang", "Chao", ""]]}, {"id": "2102.07440", "submitter": "Gordon Fraser", "authors": "Gordon Fraser, Ute Heuer, Nina K\\\"orber, Florian Oberm\\\"uller, Ewald\n  Wasmeier", "title": "LitterBox: A Linter for Scratch Programs", "comments": "To be published in the Proceedings of the International Conference on\n  Software Engineering (ICSE'21), Joint Track on Software Engineering Education\n  and Training", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating programs with block-based programming languages like Scratch is easy\nand fun. Block-based programs can nevertheless contain bugs, in particular when\nlearners have misconceptions about programming. Even when they do not, Scratch\ncode is often of low quality and contains code smells, further inhibiting\nunderstanding, reuse, and fun. To address this problem, in this paper we\nintroduce LitterBox, a linter for Scratch programs. Given a program or its\npublic project ID, LitterBox checks the program against patterns of known bugs\nand code smells. For each issue identified, LitterBox provides not only the\nlocation in the code, but also a helpful explanation of the underlying reason\nand possible misconceptions. Learners can access LitterBox through an easy to\nuse web interface with visual information about the errors in the block-code,\nwhile for researchers LitterBox provides a general, open source, and extensible\nframework for static analysis of Scratch programs.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 10:24:32 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Fraser", "Gordon", ""], ["Heuer", "Ute", ""], ["K\u00f6rber", "Nina", ""], ["Oberm\u00fcller", "Florian", ""], ["Wasmeier", "Ewald", ""]]}, {"id": "2102.07485", "submitter": "Fr\\'ed\\'eric Recoules", "authors": "Fr\\'ed\\'eric Recoules, S\\'ebastien Bardin, Richard Bonichon, Matthieu\n  Lemerre, Laurent Mounier and Marie-Laure Potet", "title": "Interface Compliance of Inline Assembly: Automatically Check, Patch and\n  Refine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inline assembly is still a common practice in low-level C programming,\ntypically for efficiency reasons or for accessing specific hardware resources.\nSuch embedded assembly codes in the GNU syntax (supported by major compilers\nsuch as GCC, Clang and ICC) have an interface specifying how the assembly codes\ninteract with the C environment. For simplicity reasons, the compiler treats\nGNU inline assembly codes as blackboxes and relies only on their interface to\ncorrectly glue them into the compiled C code. Therefore, the adequacy between\nthe assembly chunk and its interface (named compliance) is of primary\nimportance, as such compliance issues can lead to subtle and hard-to-find bugs.\nWe propose RUSTInA, the first automated technique for formally checking inline\nassembly compliance, with the extra ability to propose (proven) patches and\n(optimization) refinements in certain cases. RUSTInA is based on an original\nformalization of the inline assembly compliance problem together with novel\ndedicated algorithms. Our prototype has been evaluated on 202 Debian packages\nwith inline assembly (2656 chunks), finding 2183 issues in 85 packages -- 986\nsignificant issues in 54 packages (including major projects such as ffmpeg or\nALSA), and proposing patches for 92% of them. Currently, 38 patches have\nalready been accepted (solving 156 significant issues), with positive feedback\nfrom development teams.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 11:43:12 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Recoules", "Fr\u00e9d\u00e9ric", ""], ["Bardin", "S\u00e9bastien", ""], ["Bonichon", "Richard", ""], ["Lemerre", "Matthieu", ""], ["Mounier", "Laurent", ""], ["Potet", "Marie-Laure", ""]]}, {"id": "2102.07515", "submitter": "Pierre Vial", "authors": "Pierre Vial", "title": "Sequence Types and Infinitary Semantics", "comments": "78 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new representation of non-idempotent intersection types, using\nsequences (families indexed with natural numbers) instead of lists or\nmultisets. This allows scaling up intersection type theory to the infinitary\nlambda-calculus. We thus characterize hereditary head normalization (Klop's\nProblem) and we give a unique type to all hereditary permutators (TLCA Problem\n#20), which is not possible in a finite system. On our way, we use\nnon-idempotent intersection to retrieve some well-known results on infinitary\nterms. This paper begins with a gentle, high-level introduction to intersection\ntype theory and to the infinitary calculus.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 12:33:41 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Vial", "Pierre", ""]]}, {"id": "2102.07877", "submitter": "Zijian Jiang", "authors": "Zijian Jiang, Hao Zhong, Na Meng", "title": "Investigating and Recommending Co-Changed Entities for JavaScript\n  Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  JavaScript (JS) is one of the most popular programming languages due to its\nflexibility and versatility, but maintaining JS code is tedious and\nerror-prone. In our research, we conducted an empirical study to characterize\nthe relationship between co-changed software entities (e.g., functions and\nvariables), and built a machine learning (ML)-based approach to recommend\nadditional entity to edit given developers' code changes. Specifically, we\nfirst crawled 14,747 commits in 10 open-source projects; for each commit, we\ncreated one or more change dependency graphs (CDGs) to model the\nreferencer-referencee relationship between co-changed entities. Next, we\nextracted the common subgraphs between CDGs to locate recurring co-change\npatterns between entities. Finally, based on those patterns, we extracted code\nfeatures from co-changed entities and trained an ML model that recommends\nentities-to-change given a program commit. According to our empirical\ninvestigation, (1) three recurring patterns commonly exist in all projects; (2)\n80%--90% of co-changed function pairs either invoke the same function(s),\naccess the same variable(s), or contain similar statement(s); (3) our ML-based\napproach CoRec recommended entity changes with high accuracy (73%--78%). CoRec\ncomplements prior work because it suggests changes based on program syntax,\ntextual similarity, as well as software history; it achieved higher accuracy\nthan two existing tools in our evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 22:37:16 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 07:27:38 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Jiang", "Zijian", ""], ["Zhong", "Hao", ""], ["Meng", "Na", ""]]}, {"id": "2102.07888", "submitter": "Alessandro Cheli", "authors": "Alessandro Cheli", "title": "Metatheory.jl: Fast and Elegant Algebraic Computation in Julia with\n  Extensible Equality Saturation", "comments": "3 pages, 1 figure", "journal-ref": "The Open Journal, 2021", "doi": "10.21105/joss.03078", "report-no": "59", "categories": "cs.PL cs.SC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Metatheory.jl: a lightweight and performant general purpose\nsymbolics and metaprogramming framework meant to simplify the act of writing\ncomplex Julia metaprograms and to significantly enhance Julia with a native\nterm rewriting system, based on state-of-the-art equality saturation\ntechniques, and a dynamic first class Abstract Syntax Tree (AST) pattern\nmatching system that is dynamically composable in an algebraic fashion, taking\nfull advantage of the language's powerful reflection capabilities. Our\ncontribution allows to perform general purpose symbolic mathematics,\nmanipulation, optimization, synthesis or analysis of syntactically valid Julia\nexpressions with a clean and concise programming interface, both during\ncompilation or execution of programs.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2021 22:58:18 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Cheli", "Alessandro", ""]]}, {"id": "2102.07901", "submitter": "Weiyu Luo", "authors": "Weiyu Luo, Brian Demsky", "title": "C11Tester: A Race Detector for C/C++ Atomics Technical Report", "comments": "This is the technical report for the paper titled C11Tester: A Race\n  Detector for C/C++ Atomics published at Architectural Support for Programming\n  Languages and Operating Systems (ASPLOS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Writing correct concurrent code that uses atomics under the C/C++ memory\nmodel is extremely difficult. We present C11Tester, a race detector for the\nC/C++ memory model that can explore executions in a larger fragment of the\nC/C++ memory model than previous race detector tools. Relative to previous\nwork, C11Tester's larger fragment includes behaviors that are exhibited by ARM\nprocessors. C11Tester uses a new constraint-based algorithm to implement\nmodification order that is optimized to allow C11Tester to make decisions in\nterms of application-visible behaviors. We evaluate C11Tester on several\nbenchmark applications, and compare C11Tester's performance to both tsan11rec,\nthe state of the art tool that controls scheduling for C/C++; and tsan11, the\nstate of the art tool that does not control scheduling.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 00:59:33 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 21:47:05 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Luo", "Weiyu", ""], ["Demsky", "Brian", ""]]}, {"id": "2102.07924", "submitter": "Iaroslav Postovalov", "authors": "Iaroslav Postovalov", "title": "Compilation of mathematical expressions in Kotlin", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interpreting mathematical expressions at runtime is a standard task in\nscientific software engineering. There are different approaches to this problem\nfrom creating an embedded domain-specific language (eDSL) with its own parser\nand interpreter specifically for that task, to using a full-fledged embedded\ncompiler. This article is dedicated to a middle-ground solution implemented in\nthe KMath library, which uses the Kotlin object builder DSL and its own\nalgebraic abstractions to generate an AST for mathematical operations. This AST\nis then compiled just-in-time to generate JVM bytecode. A similar approach is\ntested on other Kotlin platforms, where its performance is compared across a\nvariety of supported platforms.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 02:23:51 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 14:03:21 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2021 11:29:34 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Postovalov", "Iaroslav", ""]]}, {"id": "2102.08286", "submitter": "Todd Schmid", "authors": "Todd Schmid and Tobias Kapp\\'e and Dexter Kozen and Alexandra Silva", "title": "Guarded Kleene Algebra with Tests: Coequations, Coinduction, and\n  Completeness", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Guarded Kleene Algebra with Tests (GKAT) is an efficient fragment of KAT, as\nit allows for almost linear decidability of equivalence. In this paper, we\nstudy the (co)algebraic properties of GKAT. Our initial focus is on the\nfragment that can distinguish between unsuccessful programs performing\ndifferent actions, by omitting the so-called early termination axiom. We\ndevelop an operational (coalgebraic) and denotational (algebraic) semantics and\nshow that they coincide. We then characterize the behaviors of GKAT expressions\nin this semantics, leading to a coequation that captures the covariety of\nautomata corresponding to behaviors of GKAT expressions. Finally, we prove that\nthe axioms of the reduced fragment are sound and complete w.r.t. the semantics,\nand then build on this result to recover a semantics that is sound and complete\nw.r.t. the full set of axioms.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 17:16:23 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 15:23:22 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Schmid", "Todd", ""], ["Kapp\u00e9", "Tobias", ""], ["Kozen", "Dexter", ""], ["Silva", "Alexandra", ""]]}, {"id": "2102.08777", "submitter": "Felix Weitk\\\"amper", "authors": "Felix Weitk\\\"amper", "title": "An asymptotic analysis of probabilistic logic programming with\n  implications for expressing projective families of distributions", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.PL math.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the last years, there has been increasing research on the scaling\nbehaviour of statistical relational representations with the size of the\ndomain, and on the connections between domain size dependence and lifted\ninference. In particular, the asymptotic behaviour of statistical relational\nrepresentations has come under scrutiny, and projectivity was isolated as the\nstrongest form of domain size independence. In this contribution we show that\nevery probabilistic logic program under the distribution semantics is\nasymptotically equivalent to a probabilistic logic program consisting only of\ndeterminate clauses over probabilistic facts. To facilitate the application of\nclassical results from finite model theory, we introduce the abstract\ndistribution semantics, defined as an arbitrary logical theory over\nprobabilistic facts to bridge the gap to the distribution semantics underlying\nprobabilistic logic programming. In this representation, determinate logic\nprograms correspond to quantifier-free theories, making asymptotic quantifier\nresults avilable for use. We can conclude that every probabilistic logic\nprogram inducing a projective family of distributions is in fact captured by\nthis class, and we can infer interesting consequences for the expressivity of\nprobabilistic logic programs as well as for the asymptotic behaviour of\nprobabilistic rules.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2021 14:07:16 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 12:03:58 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Weitk\u00e4mper", "Felix", ""]]}, {"id": "2102.09436", "submitter": "Luca Roversi", "authors": "Armando B. Matos and Luca Paolini and Luca Roversi", "title": "Interleaving classical and reversible", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Given a simple recursive function, we show how to extract two interacting\nprocesses from it. The two processes can be described by means of iterative\nprograms, one of which is intrinsically reversible, in a language that, up to\nminor details, belongs to the core of widely used imperative programming\nlanguages. We implement the two processes as interleaving synchronous JAVA\nthreads whose interaction is equivalent to the recursive function they are\nextracted from.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 15:54:30 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Matos", "Armando B.", ""], ["Paolini", "Luca", ""], ["Roversi", "Luca", ""]]}, {"id": "2102.09673", "submitter": "Sharjeel Khan", "authors": "Bodhisatwa Chatterjee, Sharjeel Khan, Santosh Pande", "title": "Effective Cache Apportioning for Performance Isolation Under Compiler\n  Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With a growing number of cores per socket in modern data-centers where\nmulti-tenancy of a diverse set of applications must be efficiently supported,\neffective sharing of the last level cache is a very important problem. This is\nchallenging because modern workloads exhibit dynamic phase behavior - their\ncache requirements & sensitivity vary across different execution points. To\ntackle this problem, we propose Com-CAS, a compiler-guided cache apportioning\nsystem that provides smart cache allocation to co-executing applications in a\nsystem. The front-end of Com-CAS is primarily a compiler-framework equipped\nwith learning mechanisms to predict cache requirements, while the backend\nconsists of an allocation framework with a pro-active scheduler that apportions\ncache dynamically to co-executing applications. Our system improved average\nthroughput by 21%, with a maximum of 54% while maintaining the worst individual\napplication execution time degradation within 15% to meet SLA requirements.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 23:44:42 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 06:21:35 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Chatterjee", "Bodhisatwa", ""], ["Khan", "Sharjeel", ""], ["Pande", "Santosh", ""]]}, {"id": "2102.09713", "submitter": "Rachit Nigam", "authors": "Rachit Nigam, Samuel Thomas, Zhijing Li, Adrian Sampson", "title": "A Compiler Infrastructure for Accelerator Generators", "comments": "To appear at ASPLOS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Calyx, a new intermediate language (IL) for compiling high-level\nprograms into hardware designs. Calyx combines a hardware-like structural\nlanguage with a software-like control flow representation with loops and\nconditionals. This split representation enables a new class of hardware-focused\noptimizations that require both structural and control flow information which\nare crucial for high-level programming models for hardware design. The Calyx\ncompiler lowers control flow constructs using finite-state machines and\ngenerates synthesizable hardware descriptions.\n  We have implemented Calyx in an optimizing compiler that translates\nhigh-level programs to hardware. We demonstrate Calyx using two DSL-to-RTL\ncompilers, a systolic array generator and one for a recent imperative\naccelerator language, and compare them to equivalent designs generated using\nhigh-level synthesis (HLS). The systolic arrays are $4.6\\times$ faster and\n$1.1\\times$ larger on average than HLS implementations, and the HLS-like\nimperative language compiler is within a few factors of a highly optimized\ncommercial HLS toolchain. We also describe three optimizations implemented in\nthe Calyx compiler.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 02:29:04 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Nigam", "Rachit", ""], ["Thomas", "Samuel", ""], ["Li", "Zhijing", ""], ["Sampson", "Adrian", ""]]}, {"id": "2102.09823", "submitter": "Gabriel Scherer", "authors": "Fr\\'ed\\'eric Bour, Basile Cl\\'ement, Gabriel Scherer", "title": "Tail Modulo Cons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  OCaml function calls consume space on the system stack. Operating systems set\ndefault limits on the stack space which are much lower than the available\nmemory. If a program runs out of stack space, they get the dreaded \"Stack\nOverflow\" exception -- they crash. As a result, OCaml programmers have to be\ncareful, when they write recursive functions, to remain in the so-called\n_tail-recursive_ fragment, using _tail_ calls that do not consume stack space.\n  This discipline is a source of difficulties for both beginners and experts.\nBeginners have to be taught recursion, and then tail-recursion. Experts\ndisagree on the \"right\" way to write `List.map`. The direct version is\nbeautiful but not tail-recursive, so it crashes on larger inputs. The naive\ntail-recursive transformation is (slightly) slower than the direct version, and\nexperts may want to avoid that cost. Some libraries propose horrible\nimplementations, unrolling code by hand, to compensate for this performance\nloss. In general, tail-recursion requires the programmer to manually perform\nsophisticated program transformations.\n  In this work we propose an implementation of \"Tail Modulo Cons\" (TMC) for\nOCaml. TMC is a program transformation for a fragment of non-tail-recursive\nfunctions, that rewrites them in _destination-passing style_. The supported\nfragment is smaller than other approaches such as continuation-passing-style,\nbut the performance of the transformed code is on par with the direct,\nnon-tail-recursive version. Many useful functions that traverse a recursive\ndatastructure and rebuild another recursive structure are in the TMC fragment,\nin particular `List.map` (and `List.filter`, `List.append`, etc.). Finally\nthose functions can be written in a way that is beautiful, correct on all\ninputs, and efficient.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 09:20:44 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Bour", "Fr\u00e9d\u00e9ric", ""], ["Cl\u00e9ment", "Basile", ""], ["Scherer", "Gabriel", ""]]}, {"id": "2102.09920", "submitter": "Christine Rizkallah", "authors": "Louis Cheung, Liam O'Connor, Christine Rizkallah", "title": "Overcoming Restraint: Modular Refinement using Cogent's Principled\n  Foreign Function Interface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cogent is a restricted functional language designed to reduce the cost of\ndeveloping verified systems code. However, Cogent does not support recursion\nnor iteration, and its type system imposes restrictions that are sometimes too\nstrong for low-level system programming. To overcome these restrictions, Cogent\nprovides a foreign function interface (FFI) between Cogent and C which allows\nfor implementing those parts of the system which cannot be expressed in Cogent,\nsuch as data structures and iterators over these data structures, to be\nimplemented in C and called from Cogent. The Cogent framework automatically\nguarantees correctness of the overall Cogent-C system when provided proofs that\nthe C components are functionally correct and satisfy Cogent's FFI constraints.\nWe previously implemented file systems in Cogent and verified key file system\noperations. However, the C components and the FFI constraints that define the\nCogent-C interoperability were axiomatized. In this paper, we verify the\ncorrectness and FFI constraints of the C implementation of word arrays used in\nthe file systems. We demonstrate how these proofs modularly compose with\nexisting Cogent theorems and result in a functional correctness theorem of the\noverall Cogent-C system. This demonstrates that Cogent 's FFI constraints\nensure correct and safe inter-language interoperability.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 13:28:31 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 05:31:06 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Cheung", "Louis", ""], ["O'Connor", "Liam", ""], ["Rizkallah", "Christine", ""]]}, {"id": "2102.10698", "submitter": "Marco Peressotti", "authors": "Lu\\'is Cruz-Filipe, Fabrizio Montesi, Marco Peressotti", "title": "Certifying Choreography Compilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choreographic programming is a paradigm for developing concurrent and\ndistributed systems, where programs are choreographies that define, from a\nglobal viewpoint, the computations and interactions that communicating\nprocesses should enact. Choreography compilation translates choreographies into\nthe local definitions of process behaviours, given as terms in a process\ncalculus.\n  Proving choreography compilation correct is challenging and error-prone,\nbecause it requires relating languages in different paradigms (global\ninteractions vs local actions) and dealing with a combinatorial explosion of\nproof cases. We present the first certified program for choreography\ncompilation for a nontrivial choreographic language supporting recursion.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 22:00:34 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Cruz-Filipe", "Lu\u00eds", ""], ["Montesi", "Fabrizio", ""], ["Peressotti", "Marco", ""]]}, {"id": "2102.10784", "submitter": "Zihan Zhao", "authors": "Ryan Song, Zihan Zhao, Yuxi Cai, Andreas Veneris, Fan Long", "title": "SigVM: Toward Fully Autonomous Smart Contracts", "comments": "14 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents SigVM, a novel blockchain virtual machine that supports\nan event-driven execution model, enabling developers to build fully autonomous\nsmart contracts. SigVM introduces another way for a contract to interact with\nanother. Contracts in SigVM can emit signal events, on which other contracts\ncan listen. Once an event is triggered, corresponding handler functions are\nautomatically executed as signal transactions. We built an end-to-end\nblockchain platform SigChain and a contract language compiler SigSolid to\nrealize the potential of SigVM. Experimental results show that SigVM enables\ncontracts in our benchmark applications to be reimplemented in a fully\nautonomous way, eliminating the dependency on unreliable mechanisms like\noff-chain relay servers. SigVM can significantly simplify the execution flow of\nour benchmark applications, and can avoid security risks such as front-run\nattacks.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 05:37:30 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Song", "Ryan", ""], ["Zhao", "Zihan", ""], ["Cai", "Yuxi", ""], ["Veneris", "Andreas", ""], ["Long", "Fan", ""]]}, {"id": "2102.11164", "submitter": "Andrew Kenyon-Roberts", "authors": "Andrew Kenyon-Roberts and Luke Ong", "title": "Supermartingales, Ranking Functions and Probabilistic Lambda Calculus", "comments": "35 pages, 3 figures, submitted to LICS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a method for proving almost sure termination in the context of\nlambda calculus with continuous random sampling and explicit recursion, based\non ranking supermartingales. This result is extended in three ways. Antitone\nranking functions have weaker restrictions on how fast they must decrease, and\nare applicable to a wider range of programs. Sparse ranking functions take\nvalues only at a subset of the program's reachable states, so they are simpler\nto define and more flexible. Ranking functions with respect to alternative\nreduction strategies give yet more flexibility, and significantly increase the\napplicability of the ranking supermartingale approach to proving almost sure\ntermination, thanks to a novel (restricted) confluence result which is of\nindependent interest. The notion of antitone ranking function was inspired by\nsimilar work by McIver, Morgan, Kaminski and Katoen in the setting of a\nfirst-order imperative language, but adapted to a higher-order functional\nlanguage. The sparse ranking function and confluent semantics extensions are\nunique to the higher-order setting. Our methods can be used to prove almost\nsure termination of programs that are beyond the reach of methods in the\nliterature, including higher-order and non-affine recursion.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 16:39:42 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 20:39:51 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Kenyon-Roberts", "Andrew", ""], ["Ong", "Luke", ""]]}, {"id": "2102.11605", "submitter": "Romain P\\'echoux", "authors": "Emmanuel Hainry, Bruce M. Kapron, Jean-Yves Marion, Romain P\\'echoux", "title": "A tier-based typed programming language characterizing Feasible\n  Functionals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The class of Basic Feasible Functionals BFF$_2$ is the type-2 counterpart of\nthe class FP of type-1 functions computable in polynomial time. Several\ncharacterizations have been suggested in the literature, but none of these\npresent a programming language with a type system guaranteeing this complexity\nbound. We give a characterization of BFF$_2$ based on an imperative language\nwith oracle calls using a tier-based type system whose inference is decidable.\nSuch a characterization should make it possible to link higher-order complexity\nwith programming theory. The low complexity (cubic in the size of the program)\nof the type inference algorithm contrasts with the intractability of the\naforementioned methods and does not overly constrain the expressive power of\nthe language.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 10:35:31 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Hainry", "Emmanuel", ""], ["Kapron", "Bruce M.", ""], ["Marion", "Jean-Yves", ""], ["P\u00e9choux", "Romain", ""]]}, {"id": "2102.13095", "submitter": "Dmitry Chistikov", "authors": "Dmitry Chistikov and Rupak Majumdar and Philipp Schepper", "title": "Subcubic Certificates for CFL Reachability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.FL cs.CC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in interprocedural program analysis can be modeled as the\ncontext-free language (CFL) reachability problem on graphs and can be solved in\ncubic time. Despite years of efforts, there are no known truly sub-cubic\nalgorithms for this problem. We study the related certification task: given an\ninstance of CFL reachability, are there small and efficiently checkable\ncertificates for the existence and for the non-existence of a path? We show\nthat, in both scenarios, there exist succinct certificates ($O(n^2)$ in the\nsize of the problem) and these certificates can be checked in subcubic (matrix\nmultiplication) time. The certificates are based on grammar-based compression\nof paths (for positive instances) and on invariants represented as matrix\nconstraints (for negative instances). Thus, CFL reachability lies in\nnondeterministic and co-nondeterministic subcubic time.\n  A natural question is whether faster algorithms for CFL reachability will\nlead to faster algorithms for combinatorial problems such as Boolean\nsatisfiability (SAT). As a consequence of our certification results, we show\nthat there cannot be a fine-grained reduction from SAT to CFL reachability for\na conditional lower bound stronger than $n^\\omega$, unless the nondeterministic\nstrong exponential time hypothesis (NSETH) fails.\n  Our results extend to related subcubic equivalent problems: pushdown\nreachability and two-way nondeterministic pushdown automata (2NPDA) language\nrecognition. For example, we describe succinct certificates for pushdown\nnon-reachability (inductive invariants) and observe that they can be checked in\nmatrix multiplication time. We also extract a new hardest 2NPDA language,\ncapturing the \"hard core\" of all these problems.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 18:58:29 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Chistikov", "Dmitry", ""], ["Majumdar", "Rupak", ""], ["Schepper", "Philipp", ""]]}, {"id": "2102.13183", "submitter": "Sankha Guria", "authors": "Sankha Narayan Guria, Jeffrey S. Foster, David Van Horn", "title": "RbSyn: Type- and Effect-Guided Program Synthesis", "comments": "17 pages, 13 figures, this is a technical report (with appendix) of a\n  paper to appear in Programming Language Design and Implementation (PLDI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, researchers have explored component-based synthesis, which\naims to automatically construct programs that operate by composing calls to\nexisting APIs. However, prior work has not considered efficient synthesis of\nmethods with side effects, e.g., web app methods that update a database. In\nthis paper, we introduce RbSyn, a novel type- and effect-guided synthesis tool\nfor Ruby. An RbSyn synthesis goal is specified as the type for the target\nmethod and a series of test cases it must pass. RbSyn works by recursively\ngenerating well-typed candidate method bodies whose write effects match the\nread effects of the test case assertions. After finding a set of candidates\nthat separately satisfy each test, RbSyn synthesizes a solution that branches\nto execute the correct candidate code under the appropriate conditions. We\nformalize RbSyn on a core, object-oriented language $\\lambda_{syn}$ and\ndescribe how the key ideas of the model are scaled-up in our implementation for\nRuby. We evaluated RbSyn on 19 benchmarks, 12 of which come from popular,\nopen-source Ruby apps. We found that RbSyn synthesizes correct solutions for\nall benchmarks, with 15 benchmarks synthesizing in under 9 seconds, while the\nslowest benchmark takes 83 seconds. Using observed reads to guide synthesize is\neffective: using type-guidance alone times out on 10 of 12 app benchmarks. We\nalso found that using less precise effect annotations leads to worse synthesis\nperformance. In summary, we believe type- and effect-guided synthesis is an\nimportant step forward in synthesis of effectful methods from test cases.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2021 21:30:42 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 02:34:02 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Guria", "Sankha Narayan", ""], ["Foster", "Jeffrey S.", ""], ["Van Horn", "David", ""]]}, {"id": "2102.13254", "submitter": "Brennan Saeta", "authors": "Adam Paszke and Brennan Saeta", "title": "Tensors Fitting Perfectly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multidimensional arrays (NDArrays) are a central abstraction in modern\nscientific computing environments. Unfortunately, they can make reasoning about\nprograms harder as the number of different array shapes used in an execution of\na program is usually very large, and they rarely appear explicitly in program\ntext. To make things worse, many operators make implicit assumptions about the\nshapes of their inputs: array addition is commonly enriched with broadcasting\nsemantics, while matrix multiplication assumes that the lengths of contracted\ndimensions are equal. Because precise reasoning about shapes is crucial to\nwrite correct programs using NDArrays, and because shapes are often hard to\ninfer from a quick glance at the program, we developed Tensors Fitting\nPerfectly, a static analysis tool that reasons about NDArray shapes in Swift\nfor TensorFlow programs by synthesizing a set of shape constraints from an\nabstract interpretation of the program. It can both (1) check for possible\ninconsistencies, and (2) provide direct insights about the shapes of\nintermediate values appearing in the program, including via a mechanism called\nshape holes. The static analysis works in concert with optional runtime\nassertions to improve the productivity of program authors.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 01:35:44 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Paszke", "Adam", ""], ["Saeta", "Brennan", ""]]}, {"id": "2102.13267", "submitter": "Brennan Saeta", "authors": "Alex Suhan, Davide Libenzi, Ailing Zhang, Parker Schuh, Brennan Saeta,\n  Jie Young Sohn, and Denys Shabalin", "title": "LazyTensor: combining eager execution with domain-specific compilers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain-specific optimizing compilers have demonstrated significant\nperformance and portability benefits, but require programs to be represented in\ntheir specialized IRs. Existing frontends to these compilers suffer from the\n\"language subset problem\" where some host language features are unsupported in\nthe subset of the user's program that interacts with the domain-specific\ncompiler. By contrast, define-by-run ML frameworks-colloquially called \"eager\"\nmode-are popular due to their ease of use and expressivity, where the full\npower of the host programming language can be used. LazyTensor is a technique\nto target domain specific compilers without sacrificing define-by-run\nergonomics. Initially developed to support PyTorch on Cloud TPUs, the\ntechnique, along with a substantially shared implementation, has been used by\nSwift for TensorFlow across CPUs, GPUs, and TPUs, demonstrating the generality\nof the approach across (1) Tensor implementations, (2) hardware accelerators,\nand (3) programming languages.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 02:22:16 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Suhan", "Alex", ""], ["Libenzi", "Davide", ""], ["Zhang", "Ailing", ""], ["Schuh", "Parker", ""], ["Saeta", "Brennan", ""], ["Sohn", "Jie Young", ""], ["Shabalin", "Denys", ""]]}, {"id": "2102.13460", "submitter": "Martin Schoeberl", "authors": "Andrew Dobis and Tjark Petersen and Kasper Juul Hesse Rasmussen and\n  Enrico Tolotto and Hans Jakob Damsgaard and Simon Thye Andersen and Richard\n  Lin and Martin Schoeberl", "title": "Open-Source Verification with Chisel and Scala", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Performance increase with general-purpose processors has come to a halt. We\ncan no longer depend on Moore's Law to increase computing performance. The only\nway to achieve higher performance or lower energy consumption is by building\ndomain-specific hardware accelerators. To efficiently design and verify those\ndomain-specific accelerators, we need agile hardware development. One of the\nmain obstacles when proposing such a modern method is the lack of modern tools\nto attack it. To be able to verify a design in such a time-constrained\ndevelopment method, one needs to have efficient tools both for design and\nverification.\n  This paper thus proposes ChiselVerify, an open-source tool for verifying\ncircuits described in any Hardware Description Language. It builds on top of\nthe Chisel hardware construction language and uses Scala to drive the\nverification using a testing strategy inspired by the Universal Verification\nMethodology (UVM) and adapted for designs described in Chisel. ChiselVerify is\ncreated based on three key ideas. First, our solution highly increases the\nproductivity of the verification engineer, by allowing hardware testing to be\ndone in a modern high-level programming environment. Second, the framework\nfunctions with any hardware description language thanks to the flexibility of\nChisel blackboxes. Finally, the solution is well integrated into the existing\nChisel universe, making it an extension of currently existing testing\nlibraries.\n  We implement ChiselVerify in a way inspired by the functionalities found in\nSystemVerilog. This allows one to use functional coverage, constrained-random\nverification, bus functional models, transaction-level modeling and much more\nduring the verification process of a design in a contemporary high-level\nprogramming ecosystem.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 13:35:24 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Dobis", "Andrew", ""], ["Petersen", "Tjark", ""], ["Rasmussen", "Kasper Juul Hesse", ""], ["Tolotto", "Enrico", ""], ["Damsgaard", "Hans Jakob", ""], ["Andersen", "Simon Thye", ""], ["Lin", "Richard", ""], ["Schoeberl", "Martin", ""]]}, {"id": "2102.13514", "submitter": "Rahim Mammadli", "authors": "Rahim Mammadli, Marija Selakovic, Felix Wolf, Michael Pradel", "title": "Learning to Make Compiler Optimizations More Effective", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because loops execute their body many times, compiler developers place much\nemphasis on their optimization. Nevertheless, in view of highly diverse source\ncode and hardware, compilers still struggle to produce optimal target code. The\nsheer number of possible loop optimizations, including their combinations,\nexacerbates the problem further. Today's compilers use hard-coded heuristics to\ndecide when, whether, and which of a limited set of optimizations to apply.\nOften, this leads to highly unstable behavior, making the success of compiler\noptimizations dependent on the precise way a loop has been written. This paper\npresents LoopLearner, which addresses the problem of compiler instability by\npredicting which way of writing a loop will lead to efficient compiled code. To\nthis end, we train a neural network to find semantically invariant source-level\ntransformations for loops that help the compiler generate more efficient code.\nOur model learns to extract useful features from the raw source code and\npredicts the speedup that a given transformation is likely to yield. We\nevaluate LoopLearner with 1,895 loops from various performance-relevant\nbenchmarks. Applying the transformations that our model deems most favorable\nprior to compilation yields an average speedup of 1.14x. When trying the top-3\nsuggested transformations, the average speedup even increases to 1.29x.\nComparing the approach with an exhaustive search through all available code\ntransformations shows that LoopLearner helps to identify the most beneficial\ntransformations in several orders of magnitude less time.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 10:42:56 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Mammadli", "Rahim", ""], ["Selakovic", "Marija", ""], ["Wolf", "Felix", ""], ["Pradel", "Michael", ""]]}, {"id": "2102.13516", "submitter": "Jim Pivarski", "authors": "Jim Pivarski, Ianna Osborne, Pratyush Das, David Lange, and Peter\n  Elmer", "title": "AwkwardForth: accelerating Uproot with an internal DSL", "comments": "11 pages, 2 figures, submitted to the 25th International Conference\n  on Computing in High Energy & Nuclear Physics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL hep-ex", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  File formats for generic data structures, such as ROOT, Avro, and Parquet,\npose a problem for deserialization: it must be fast, but its code depends on\nthe type of the data structure, not known at compile-time. Just-in-time\ncompilation can satisfy both constraints, but we propose a more portable\nsolution: specialized virtual machines. AwkwardForth is a Forth-driven virtual\nmachine for deserializing data into Awkward Arrays. As a language, it is not\nintended for humans to write, but it loosens the coupling between Uproot and\nAwkward Array. AwkwardForth programs for deserializing record-oriented formats\n(ROOT and Avro) are about as fast as C++ ROOT and 10-80$\\times$ faster than\nfastavro. Columnar formats (simple TTrees, RNTuple, and Parquet) only require\nspecialization to interpret metadata and are therefore faster with precompiled\ncode.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 18:49:10 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Pivarski", "Jim", ""], ["Osborne", "Ianna", ""], ["Das", "Pratyush", ""], ["Lange", "David", ""], ["Elmer", "Peter", ""]]}]