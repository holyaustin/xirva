[{"id": "1901.00073", "submitter": "EPTCS", "authors": "Bernd Finkbeiner (Saarland University), Samantha Kleinberg (Stevens\n  Institute of Technology)", "title": "Proceedings 3rd Workshop on formal reasoning about Causation,\n  Responsibility, and Explanations in Science and Technology", "comments": null, "journal-ref": "EPTCS 286, 2019", "doi": "10.4204/EPTCS.286", "report-no": null, "categories": "cs.LO cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CREST 2018 workshop is the third in a series of workshops addressing\nformal approaches to reasoning about causation in systems engineering. The\ntopic of formally identifying the cause(s) of specific events - usually some\nform of failures -, and explaining why they occurred, are increasingly in the\nfocus of several, disjoint communities. The main objective of CREST is to bring\ntogether researchers and practitioners from industry and academia in order to\nenable discussions how explicit and implicit reasoning about causation is\nperformed. A further objective is to link to the foundations of causal\nreasoning in the philosophy of sciences and to causal reasoning performed in\nother areas of computer science, engineering, and beyond.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 01:22:19 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Finkbeiner", "Bernd", "", "Saarland University"], ["Kleinberg", "Samantha", "", "Stevens\n  Institute of Technology"]]}, {"id": "1901.00428", "submitter": "Simon Cooksey", "authors": "Simon Cooksey, Sarah Harris, Mark Batty, Radu Grigore, Mikol\\'a\\v{s}\n  Janota", "title": "PrideMM: A Solver for Relaxed Memory Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relaxed memory models are notoriously delicate. To ease their study, several\nad hoc simulators have been developed for axiomatic memory models. We show how\naxiomatic memory models can be simulated using a solver for $\\exists$SO.\nFurther, we show how memory models based on event structures can be simulated\nusing a solver for MSO. Finally, we present a solver for SO, built on top of\nQBF solvers.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 01:04:19 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Cooksey", "Simon", ""], ["Harris", "Sarah", ""], ["Batty", "Mark", ""], ["Grigore", "Radu", ""], ["Janota", "Mikol\u00e1\u0161", ""]]}, {"id": "1901.01001", "submitter": "Will Crichton", "authors": "Anna Zeng and Will Crichton", "title": "Identifying Barriers to Adoption for Rust through Online Discourse", "comments": null, "journal-ref": "Proceedings of the 9th Workshop on Evaluation and Usability of\n  Programming Languages and Tools, 2018", "doi": null, "report-no": null, "categories": "cs.HC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rust is a low-level programming language known for its unique approach to\nmemory-safe systems programming and for its steep learning curve. To understand\nwhat makes Rust difficult to adopt, we surveyed the top Reddit and Hacker News\nposts and comments about Rust; from these online discussions, we identified\nthree hypotheses about Rust's barriers to adoption. We found that certain key\nfeatures, idioms, and integration patterns were not easily accessible to new\nusers.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 07:36:05 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Zeng", "Anna", ""], ["Crichton", "Will", ""]]}, {"id": "1901.01111", "submitter": "Ana Almeida Matos", "authors": "Ana Almeida Matos, Jan Cederquist", "title": "Information flow in a distributed security setting", "comments": "49 pages + Appendix (33 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information flow security is classically formulated in terms of the absence\nof illegal information flows, with respect to a security setting consisting of\na single flow policy that specifies what information flows should be permitted\nin the system. In this paper we investigate the security issues that emerge in\ndistributed security settings, where each computation domain establishes its\nown local security policy, and where programs may exhibit location-dependent\nbehavior. In particular, we study the interplay between two distinct flow\npolicy layers: the declared flow policy, established by the program itself, and\nthe allowed flow policy, established externally to the program by each\ncomputation domain. We refine two security properties that articulate how the\nbehaviors of programs comply to the respective flow policies: Distributed\nNon-disclosure, for enabling programs to declare locally scoped flow policies;\nand Flow Policy Confinement, for controlling the flow policies that are\ndeclared by programs. We present enforcement mechanisms that are based on type\nand effect systems, ranging from purely static mechanisms to hybrid\ncombinations with dynamic migration control, for enforcing the above properties\non an expressive ML-like language with concurrent threads and code migration,\nand which includes an allowed flow policy construct that dynamically tests the\nallowed flow policy of the current context. Finally, we show that the\ncombination of the above two properties guarantees that actual information\nflows do not violate the relevant allowed flow policies. To this end we propose\nand use the Distributed Non-Interference property, a natural generalization of\nNon-Interference to a distributed security setting that ensures that\ninformation flows in a program respect the allowed flow policy of the domains\nwhere they originate.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 14:16:11 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 23:38:35 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Matos", "Ana Almeida", ""], ["Cederquist", "Jan", ""]]}, {"id": "1901.01930", "submitter": "Joseph M. Hellerstein", "authors": "Joseph M. Hellerstein and Peter Alvaro", "title": "Keeping CALM: When Distributed Consistency is Easy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A key concern in modern distributed systems is to avoid the cost of\ncoordination while maintaining consistent semantics. Until recently, there was\nno answer to the question of when coordination is actually required. In this\npaper we present an informal introduction to the CALM Theorem, which answers\nthis question precisely by moving up from traditional storage consistency to\nconsider properties of programs.\n  CALM is an acronym for \"consistency as logical monotonicity\". The CALM\nTheorem shows that the programs that have consistent, coordination-free\ndistributed implementations are exactly the programs that can be expressed in\nmonotonic logic. This theoretical result has practical implications for\ndevelopers of distributed applications. We show how CALM provides a\nconstructive application-level counterpart to conventional \"systems\" wisdom,\nsuch as the apparently negative results of the CAP Theorem. We also discuss\nways that monotonic thinking can influence distributed systems design, and how\nnew programming language designs and tools can help developers write\nconsistent, coordination-free code.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 17:20:25 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 00:16:08 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Hellerstein", "Joseph M.", ""], ["Alvaro", "Peter", ""]]}, {"id": "1901.02819", "submitter": "Vineeth Kashyap", "authors": "Vineeth Kashyap, Jason Ruchti, Lucja Kot, Emma Turetsky, Rebecca\n  Swords, Shih An Pan, Julien Henry, David Melski, and Eric Schulte", "title": "Automated Customized Bug-Benchmark Generation", "comments": null, "journal-ref": "IEEE SCAM 2019", "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Bug-Injector, a system that automatically creates benchmarks for\ncustomized evaluation of static analysis tools. We share a benchmark generated\nusing Bug-Injector and illustrate its efficacy by using it to evaluate the\nrecall of two leading open-source static analysis tools: Clang Static Analyzer\nand Infer.\n  Bug-Injector works by inserting bugs based on bug templates into real-world\nhost programs. It runs tests on the host program to collect dynamic traces,\nsearches the traces for a point where the state satisfies the preconditions for\nsome bug template, then modifies the host program to inject a bug based on that\ntemplate. Injected bugs are used as test cases in a static analysis tool\nevaluation benchmark. Every test case is accompanied by a program input that\nexercises the injected bug. We have identified a broad range of requirements\nand desiderata for bug benchmarks; our approach generates on-demand test\nbenchmarks that meet these requirements. It also allows us to create customized\nbenchmarks suitable for evaluating tools for a specific use case (e.g., a given\ncodebase and set of bug types).\n  Our experimental evaluation demonstrates the suitability of our generated\nbenchmark for evaluating static bug-detection tools and for comparing the\nperformance of different tools.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 16:50:42 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 21:30:18 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Kashyap", "Vineeth", ""], ["Ruchti", "Jason", ""], ["Kot", "Lucja", ""], ["Turetsky", "Emma", ""], ["Swords", "Rebecca", ""], ["Pan", "Shih An", ""], ["Henry", "Julien", ""], ["Melski", "David", ""], ["Schulte", "Eric", ""]]}, {"id": "1901.03208", "submitter": "Rodolphe Lepigre", "authors": "Rodolphe Lepigre (DEDUCTEAM)", "title": "PML 2 : Integrated Program Verification in ML", "comments": null, "journal-ref": "23rd International Conference on Types for Proofs and Programs\n  (TYPES 2017), Jul 2017, Budapest, Hungary. pp.27, 2018", "doi": "10.4230/LIPIcs.TYPES.2017.5", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the PML 2 language, which provides a uniform environment for\nprogramming, and for proving properties of programs in an ML-like setting. The\nlanguage is Curry-style and call-by-value, it provides a control operator\n(interpreted in terms of classical logic), it supports general recursion and a\nvery general form of (implicit, non-coercive) subtyping. In the system,\nequational properties of programs are expressed using two new type formers, and\nthey are proved by constructing terminating programs. Although proofs rely\nheavily on equational reasoning, equalities are exclusively managed by the\ntype-checker. This means that the user only has to choose which equality to\nuse, and not where to use it, as is usually done in mathematical proofs. In the\nsystem, writing proofs mostly amounts to applying lemmas (possibly recursive\nfunction calls), and to perform case analyses (pattern matchings).\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 15:05:43 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Lepigre", "Rodolphe", "", "DEDUCTEAM"]]}, {"id": "1901.03378", "submitter": "Francisco Ferreira Ruiz", "authors": "Brigitte Pientka (1), Andreas Abel (2), Francisco Ferreira (3), David\n  Thibodeau (1), Rebecca Zucchini (4) ((1) McGill University, (2) Gothenburg\n  University, (3) Imperial College London, (4) ENS Paris Saclay)", "title": "Cocon: Computation in Contextual Type Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a Martin-L\\\"of style dependent type theory, called Cocon, that\nallows us to mix the intensional function space that is used to represent\nhigher-order abstract syntax (HOAS) trees with the extensional function space\nthat describes (recursive) computations. We mediate between HOAS\nrepresentations and computations using contextual modal types. Our type theory\nalso supports an infinite hierarchy of universes and hence supports type-level\ncomputation -- thereby providing metaprogramming and (small-scale) reflection.\nOur main contribution is the development of a Kripke-style model for Cocon that\nallows us to prove normalization. From the normalization proof, we derive\nsubject reduction and consistency. Our work lays the foundation to incorporate\nthe methodology of logical frameworks into systems such as Agda and bridges the\nlongstanding gap between these two worlds.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 20:39:40 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Pientka", "Brigitte", ""], ["Abel", "Andreas", ""], ["Ferreira", "Francisco", ""], ["Thibodeau", "David", ""], ["Zucchini", "Rebecca", ""]]}, {"id": "1901.03575", "submitter": "Thodoris Sotiropoulos", "authors": "Thodoris Sotiropoulos (Athens University of Economics and Business)\n  and Benjamin Livshits (Imperial College London)", "title": "Static Analysis for Asynchronous JavaScript Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchrony has become an inherent element of JavaScript, as an effort to\nimprove the scalability and performance of modern web applications. To this\nend, JavaScript provides programmers with a wide range of constructs and\nfeatures for developing code that performs asynchronous computations, including\nbut not limited to timers, promises, and non-blocking I/O.\n  However, the data flow imposed by asynchrony is implicit, and not always\nwell-understood by the developers who introduce many asynchrony-related bugs to\ntheir programs. Worse, there are few tools and techniques available for\nanalyzing and reasoning about such asynchronous applications. In this work, we\naddress this issue by designing and implementing one of the first static\nanalysis schemes capable of dealing with almost all the asynchronous primitives\nof JavaScript up to the 7th edition of the ECMAScript specification.\n  Specifically, we introduce the callback graph, a representation for capturing\ndata flow between asynchronous code. We exploit the callback graph for\ndesigning a more precise analysis that respects the execution order between\ndifferent asynchronous functions. We parameterize our analysis with one novel\ncontext-sensitivity flavor, and we end up with multiple analysis variations for\nbuilding callback graph.\n  We performed a number of experiments on a set of hand-written and real-world\nJavaScript programs. Our results show that our analysis can be applied to\nmedium-sized programs achieving 79% precision on average. The findings further\nsuggest that analysis sensitivity is beneficial for the vast majority of the\nbenchmarks. Specifically, it is able to improve precision by up to 28.5%, while\nit achieves an 88% precision on average without highly sacrificing performance.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 12:48:52 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Sotiropoulos", "Thodoris", "", "Athens University of Economics and Business"], ["Livshits", "Benjamin", "", "Imperial College London"]]}, {"id": "1901.03771", "submitter": "Vinod Grover", "authors": "Mahesh Ravishankar, Vinod Grover", "title": "Automatic acceleration of Numpy applications on GPUs and multicore CPUs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frameworks like Numpy are a popular choice for application developers from\nvaried fields such as image processing to bio-informatics to machine learning.\nNumpy is often used to develop prototypes or for deployment since it provides\nefficient implementation for operations involving arrays. Such an approach\nrequires every operation to be executed eagerly. The result of each operation\nneeds to be stored in memory which increases the memory footprint of the\napplication. It also increases the bandwidth requirements since all uses must\nread from this memory. We propose an approach that records the sequence of\nNumpy operations for defered execution. When the values of an array are needed,\nfor example when the values are stored to disk or displayed on screen, the\nsequence of operations required to compute these value are compiled into a\nfunction and executed. This removes the need to store/load intermediates in\nslow memory, resulting in better performance. In cases where the library\nimplementation is more efficient (like matrix-matrix multiply), those are used\ninstead. The approach also allows us to seamlessly target both multicore CPUs\nand NVIDIA GPUs, thereby porting the Numpy application to these architectures\nwithout changing the user program. The benefit of the approach is evaluated by\ntargeting computation samples from various domains and on average on order of\nmagnitude performance improvement over Numpy is observed.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 23:40:09 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Ravishankar", "Mahesh", ""], ["Grover", "Vinod", ""]]}, {"id": "1901.04615", "submitter": "Ameer Haj-Ali", "authors": "Ameer Haj-Ali, Qijing Huang, William Moses, John Xiang, Ion Stoica,\n  Krste Asanovic, John Wawrzynek", "title": "AutoPhase: Compiler Phase-Ordering for High Level Synthesis with Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of the code generated by a compiler depends on the order in\nwhich the optimization passes are applied. In high-level synthesis, the quality\nof the generated circuit relates directly to the code generated by the\nfront-end compiler. Choosing a good order--often referred to as the\nphase-ordering problem--is an NP-hard problem. In this paper, we evaluate a new\ntechnique to address the phase-ordering problem: deep reinforcement learning.\nWe implement a framework in the context of the LLVM compiler to optimize the\nordering for HLS programs and compare the performance of deep reinforcement\nlearning to state-of-the-art algorithms that address the phase-ordering\nproblem. Overall, our framework runs one to two orders of magnitude faster than\nthese algorithms, and achieves a 16% improvement in circuit performance over\nthe -O3 compiler flag.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 00:13:21 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 01:17:33 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Haj-Ali", "Ameer", ""], ["Huang", "Qijing", ""], ["Moses", "William", ""], ["Xiang", "John", ""], ["Stoica", "Ion", ""], ["Asanovic", "Krste", ""], ["Wawrzynek", "John", ""]]}, {"id": "1901.05082", "submitter": "Matteo Busi", "authors": "Matteo Busi and Pierpaolo Degano and Letterio Galletta", "title": "Translation Validation for Security Properties", "comments": "Presented at PriSC Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secure compilation aims to build compilation chains that preserve security\nproperties when translating programs from a source to a target language. Recent\nresearch led to the definition of secure compilation principles that, if met,\nguarantee that the compilation chain in hand never violates specific families\nof security properties. Still, to the best of our knowledge, no effective\nprocedure is available to check if a compilation chain meets such requirements.\nHere, we outline our ongoing research inspired by translation validation, to\neffectively check one of those principles.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 23:03:02 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Busi", "Matteo", ""], ["Degano", "Pierpaolo", ""], ["Galletta", "Letterio", ""]]}, {"id": "1901.05138", "submitter": "Abhinav Jangda", "authors": "Abhinav Jangda, Gaurav Anand", "title": "Predicting Variable Types in Dynamically Typed Programming Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Programming Languages are quite popular because they increase the\nprogrammer's productivity. However, the absence of types in the source code\nmakes the program written in these languages difficult to understand and\nvirtual machines that execute these programs cannot produced optimized code. To\novercome this challenge, we develop a technique to predict types of all\nidentifiers including variables, and function return types.\n  We propose the first implementation of $2^{nd}$ order Inside Outside\nRecursive Neural Networks with two variants (i) Child-Sum Tree-LSTMs and (ii)\nN-ary RNNs that can handle large number of tree branching. We predict the types\nof all the identifiers given the Abstract Syntax Tree by performing just two\npasses over the tree, bottom-up and top-down, keeping both the content and\ncontext representation for all the nodes of the tree. This allows these\nrepresentations to interact by combining different paths from the parent,\nsiblings and children which is crucial for predicting types. Our best model\nachieves 44.33\\% across 21 classes and top-3 accuracy of 71.5\\% on our gathered\nPython data set from popular Python benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 05:42:22 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Jangda", "Abhinav", ""], ["Anand", "Gaurav", ""]]}, {"id": "1901.05456", "submitter": "Roberto Metere", "authors": "Andreas Lindner, Roberto Guanciale, Roberto Metere", "title": "TrABin: Trustworthy Analyses of Binaries", "comments": "32 pages, 5 figures, journal. arXiv admin note: substantial text\n  overlap with arXiv:1807.10664", "journal-ref": null, "doi": "10.1016/j.scico.2019.01.001", "report-no": null, "categories": "cs.PL cs.CR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification of microkernels, device drivers, and crypto routines requires\nanalyses at the binary level. In order to automate these analyses, in the last\nyears several binary analysis platforms have been introduced. These platforms\nshare a common design: the adoption of hardware-independent intermediate\nrepresentations, a mechanism to translate architecture dependent code to this\nrepresentation, and a set of architecture independent analyses that process the\nintermediate representation. The usage of these platforms to verify software\nintroduces the need for trusting both the correctness of the translation from\nbinary code to intermediate language (called transpilation) and the correctness\nof the analyses. Achieving a high degree of trust is challenging since the\ntranspilation must handle (i) all the side effects of the instructions, (ii)\nmultiple instruction encodings (e.g. ARM Thumb), and (iii) variable instruction\nlength (e.g. Intel). Similarly, analyses can use complex transformations (e.g.\nloop unrolling) and simplifications (e.g. partial evaluation) of the artifacts,\nwhose bugs can jeopardize correctness of the results. We overcome these\nproblems by developing a binary analysis platform on top of the interactive\ntheorem prover HOL4. First, we formally model a binary intermediate language\nand we prove correctness of several supporting tools (i.e. a type checker).\nThen, we implement two proof-producing transpilers, which respectively\ntranslate ARMv8 and CortexM0 programs to the intermediate language and generate\na certificate. This certificate is a HOL4 proof demonstrating correctness of\nthe translation. As demonstrating analysis, we implement a proof-producing\nweakest precondition generator, which can be used to verify that a given\nloop-free program fragment satisfies a contract. Finally, we use an AES\nencryption implementation to benchmark our platform.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 18:14:10 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Lindner", "Andreas", ""], ["Guanciale", "Roberto", ""], ["Metere", "Roberto", ""]]}, {"id": "1901.05750", "submitter": "Emanuele D'Osualdo", "authors": "Emanuele D'Osualdo, Azadeh Farzan, Philippa Gardner and Julian\n  Sutherland", "title": "TaDA Live: Compositional Reasoning for Termination of Fine-grained\n  Concurrent Programs", "comments": "24 pages, 97 pages including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce TaDA Live, a separation logic for reasoning compositionally\nabout the termination of blocking fine-grained concurrent programs. The logic\ncontributes several innovations to obtain modular rely/guarantee style\nreasoning for liveness properties and to blend them with logical atomicity. We\nillustrate the subtlety of our specifications and reasoning on some\nparadigmatic examples.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 12:11:59 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 04:21:51 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["D'Osualdo", "Emanuele", ""], ["Farzan", "Azadeh", ""], ["Gardner", "Philippa", ""], ["Sutherland", "Julian", ""]]}, {"id": "1901.06087", "submitter": "Amir Kafshdar Goharshady", "authors": "Mingzhang Huang, Hongfei Fu, Krishnendu Chatterjee and Amir Kafshdar\n  Goharshady", "title": "Modular Verification for Almost-Sure Termination of Probabilistic\n  Programs", "comments": "Accepted to OOPSLA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the almost-sure termination problem for\nprobabilistic programs that asks whether a given probabilistic program\nterminates with probability 1. Scalable approaches for program analysis often\nrely on modularity as their theoretical basis. In non-probabilistic programs,\nthe classical variant rule (V-rule) of Floyd-Hoare logic provides the\nfoundation for modular analysis. Extension of this rule to almost-sure\ntermination of probabilistic programs is quite tricky, and a probabilistic\nvariant was proposed in [Fioriti and Hermanns 2015]. While the proposed\nprobabilistic variant cautiously addresses the key issue of integrability, we\nshow that the proposed modular rule is still not sound for almost-sure\ntermination of probabilistic programs.\n  Besides establishing unsoundness of the previous rule, our contributions are\nas follows: First, we present a sound modular rule for almost-sure termination\nof probabilistic programs. Our approach is based on a novel notion of descent\nsupermartingales. Second, for algorithmic approaches, we consider descent\nsupermartingales that are linear and show that they can be synthesized in\npolynomial time. Finally, we present experimental results on a variety of\nbenchmarks and several natural examples that model various types of nested\nwhile loops in probabilistic programs and demonstrate that our approach is able\nto efficiently prove their almost-sure termination property.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 05:24:37 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 14:58:11 GMT"}, {"version": "v3", "created": "Mon, 12 Aug 2019 15:45:44 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Huang", "Mingzhang", ""], ["Fu", "Hongfei", ""], ["Chatterjee", "Krishnendu", ""], ["Goharshady", "Amir Kafshdar", ""]]}, {"id": "1901.06540", "submitter": "Alejandro Aguirre", "authors": "Alejandro Aguirre, Gilles Barthe, Justin Hsu, Benjamin Lucien\n  Kaminski, Joost-Pieter Katoen, Christoph Matheja", "title": "A Pre-Expectation Calculus for Probabilistic Sensitivity", "comments": "Major revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensitivity properties describe how changes to the input of a program affect\nthe output, typically by upper bounding the distance between the outputs of two\nruns by a monotone function of the distance between the corresponding inputs.\nWhen programs are probabilistic, the distance between outputs is a distance\nbetween distributions. The Kantorovich lifting provides a general way of\ndefining a distance between distributions by lifting the distance of the\nunderlying sample space; by choosing an appropriate distance on the base space,\none can recover other usual probabilistic distances, such as the Total\nVariation distance. We develop a relational pre-expectation calculus to upper\nbound the Kantorovich distance between two executions of a probabilistic\nprogram. We illustrate our methods by proving algorithmic stability of a\nmachine learning algorithm, convergence of a reinforcement learning algorithm,\nand fast mixing for card shuffling algorithms. We also consider some\nextensions: proving lower bounds on the Total Variation distance and\nconvergence to the uniform distribution. Finally, we describe an asynchronous\nextension of our calculus to reason about pairs of program executions with\ndifferent control flow.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 15:23:19 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 07:57:37 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Aguirre", "Alejandro", ""], ["Barthe", "Gilles", ""], ["Hsu", "Justin", ""], ["Kaminski", "Benjamin Lucien", ""], ["Katoen", "Joost-Pieter", ""], ["Matheja", "Christoph", ""]]}, {"id": "1901.06839", "submitter": "Dominic Steinh\\\"ofel", "authors": "Nathan Wasser and Dominic Steinh\\\"ofel", "title": "Technical Report: Using Loop Scopes with for-Loops", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loop scopes have been shown to be a helpful tool in creating sound loop\ninvariant rules which do not require program transformation of the loop body.\nHere we extend this idea from while-loops to for-loops and also present sound\nloop unrolling rules for while- and for-loops, which require neither program\ntransformation of the loop body, nor the use of nested modalities. This\napproach allows for-loops to be treated as first-class citizens -- rather than\nthe usual approach of transforming for-loops into while-loops -- which makes\nsemi-automated proofs easier to follow for the user, who may need to provide\nhelp in order to finish the proof.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 09:36:17 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 08:30:55 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Wasser", "Nathan", ""], ["Steinh\u00f6fel", "Dominic", ""]]}, {"id": "1901.07665", "submitter": "James Parker", "authors": "James Parker, Niki Vazou, Michael Hicks", "title": "LWeb: Information Flow Security for Multi-tier Web Applications", "comments": null, "journal-ref": "Proc. ACM Program. Lang., Vol. 3, No. POPL, Article 75 (January\n  2019)", "doi": "10.1145/3290388", "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents LWeb, a framework for enforcing label-based, information\nflow policies in database-using web applications. In a nutshell, LWeb marries\nthe LIO Haskell IFC enforcement library with the Yesod web programming\nframework. The implementation has two parts. First, we extract the core of LIO\ninto a monad transformer (LMonad) and then apply it to Yesod's core monad.\nSecond, we extend Yesod's table definition DSL and query functionality to\npermit defining and enforcing label-based policies on tables and enforcing them\nduring query processing. LWeb's policy language is expressive, permitting\ndynamic per-table and per-row policies. We formalize the essence of LWeb in the\n$\\lambda_{LWeb}$ calculus and mechanize the proof of noninterference in Liquid\nHaskell. This mechanization constitutes the first metatheoretic proof carried\nout in Liquid Haskell. We also used LWeb to build a substantial web site\nhosting the Build it, Break it, Fix it security-oriented programming contest.\nThe site involves 40 data tables and sophisticated policies. Compared to\nmanually checking security policies, LWeb imposes a modest runtime overhead of\nbetween 2% to 21%. It reduces the trusted code base from the whole application\nto just 1% of the application code, and 21% of the code overall (when counting\nLWeb too).\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 00:47:32 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Parker", "James", ""], ["Vazou", "Niki", ""], ["Hicks", "Michael", ""]]}, {"id": "1901.07820", "submitter": "Pierre Hyvernat", "authors": "Pierre Hyvernat (LAMA)", "title": "The Size-Change Principle for Mixed Inductive and Coinductive types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes how to use Lee, Jones and Ben Amram's size-change\nprinciple to check correctness of arbitrary recursive definitions in an ML /\nHaskell like programming language. The main point is that the size-change\nprinciple isn't only used to check termination, but also productivity for\ninfinite objects. The main point is that the resulting principle is sound even\nin the presence of arbitrary nestings of inductive and coinductive types. A\nsmall prototype has been implemented and gives a practical argument in favor of\nthis principle.This work relies on a characterization of least and greatest\nfixed points as sets of winning strategies for parity games that was developed\nby L. Santocanale in his work on circular proofs.Half of the paper is devoted\nto the proof of correctness of the criterion, which relies on an untyped\nextension of the language's denotational semantics to a domain of values\nextended with non-deterministic sums. We can recast all the syntactical\nconstructions in this domain and check they are semantically sound.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 11:12:57 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Hyvernat", "Pierre", "", "LAMA"]]}, {"id": "1901.08006", "submitter": "Alexandros Tasos", "authors": "Juliana Franco, Alexandros Tasos, Sophia Drossopoulou, Tobias\n  Wrigstad, Susan Eisenbach", "title": "Safely Abstracting Memory Layouts", "comments": "Presented at FTfJP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Modern architectures require applications to make effective use of caches to\nachieve high performance and hide memory latency. This in turn requires careful\nconsideration of placement of data in memory to exploit spatial locality,\nleverage hardware prefetching and conserve memory bandwidth. In unmanaged\nlanguages like C++, memory optimisations are common, but at the cost of losing\nobject abstraction and memory safety. In managed languages like Java and C#,\nthe abstract view of memory and proliferation of moving compacting garbage\ncollection does not provide enough control over placement and layout.\n  We have proposed SHAPES, a type-driven abstract placement specification that\ncan be integrated with object-oriented languages to enable memory\noptimisations. SHAPES preserves both memory and object abstraction. In this\npaper, we formally specify the SHAPES semantics and describe its memory safety\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 17:09:28 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Franco", "Juliana", ""], ["Tasos", "Alexandros", ""], ["Drossopoulou", "Sophia", ""], ["Wrigstad", "Tobias", ""], ["Eisenbach", "Susan", ""]]}, {"id": "1901.08840", "submitter": "Kees Middelburg", "authors": "J. A. Bergstra, C. A. Middelburg", "title": "Program algebra for Turing-machine programs", "comments": "19 pages, Sect. 2--4 are largely shortened versions of Sect. 2--4 of\n  arXiv:1808.04264, which, in turn, draw from preliminary sections of several\n  earlier papers; 21 pages, some remarks in Sect.1 and Sect.10 added", "journal-ref": "Scientific Annals of Computer Science 29(2):113--139 (2019)", "doi": "10.7561/SACS.2019.2.113", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algebraic theory of instruction sequences with\ninstructions for Turing tapes as basic instructions, the behaviours produced by\nthe instruction sequences concerned under execution, and the interaction\nbetween such behaviours and Turing tapes provided by an execution environment.\nThis theory provides a setting for the development of theory in areas such as\ncomputability and computational complexity that distinguishes itself by\noffering the possibility of equational reasoning and being more general than\nthe setting provided by a known version of the Turing-machine model of\ncomputation. The theory is essentially an instantiation of a parameterized\nalgebraic theory which is the basis of a line of research in which issues\nrelating to a wide variety of subjects from computer science have been\nrigorously investigated thinking in terms of instruction sequences.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 11:39:36 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 12:39:19 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Bergstra", "J. A.", ""], ["Middelburg", "C. A.", ""]]}, {"id": "1901.08857", "submitter": "Andreas Pavlogiannis", "authors": "Andreas Pavlogiannis", "title": "Fast, Sound and Effectively Complete Dynamic Race Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writing concurrent programs is highly error-prone due to the nondeterminism\nin interprocess communication. The most reliable indicators of errors in\nconcurrency are data races, which are accesses to a shared resource that can be\nexecuted consecutively. We study the problem of predicting data races in\nlock-based concurrent programs. The input consists of a concurrent trace $t$,\nand the task is to determine all pairs of events of $t$ that constitute a data\nrace. The problem lies at the heart of concurrent verification and has been\nextensively studied for over three decades. However, existing polynomial-time\nsound techniques are highly incomplete and can miss many simple races.\n  In this work we develop M2: a new polynomial-time algorithm for this problem,\nwhich has no false positives. In addition, our algorithm is complete for input\ntraces that consist of two processes, i.e., it provably detects all races in\nthe trace. We also develop sufficient conditions for detecting completeness\ndynamically in cases of more than two processes. We make an experimental\nevaluation of our algorithm on a standard set of benchmarks taken from recent\nliterature on the topic. Our tool soundly reports thousands of races and misses\nat most one race in the whole benchmark set. In addition, our technique detects\nall racy memory locations of the benchmark set. Finally, its running times are\ncomparable, and often smaller than the theoretically fastest, yet highly\nincomplete, existing methods. To our knowledge, M2 is the first sound algorithm\nthat achieves such a level of performance on both running time and completeness\nof the reported races.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 12:54:07 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 15:19:11 GMT"}, {"version": "v3", "created": "Mon, 8 Apr 2019 14:12:27 GMT"}, {"version": "v4", "created": "Thu, 17 Oct 2019 10:43:25 GMT"}, {"version": "v5", "created": "Tue, 5 Nov 2019 15:09:52 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Pavlogiannis", "Andreas", ""]]}, {"id": "1901.08972", "submitter": "Veljko Pejovic", "authors": "Veljko Pejovic", "title": "Towards Approximate Mobile Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile computing is one of the main drivers of innovation, yet the future\ngrowth of mobile computing capabilities remains critically threatened by\nhardware constraints, such as the already extremely dense transistor packing\nand limited battery capacity. The breakdown of Dennard scaling and stagnating\nenergy storage improvements further amplify these threats. However, the\ncomputational burden we put on our mobile devices is not always justified. In a\nmyriad of situations the result of a computation is further manipulated,\ninterpreted, and finally acted upon. This allows for the computation to be\nrelaxed, so that the result is calculated with \"good enough\", not perfect\naccuracy. For example, results of a Web search may be perfectly acceptable even\nif the order of the last few listed items is shuffled, as an end user decides\nwhich of the available links to follow. Similarly, the quality of a\nvoice-over-IP call may be acceptable, despite being imperfect, as long as the\ntwo involved parties can clearly understand each other. This novel way of\nthinking about computation is termed Approximate Computing (AC) and promises to\nreduce resource usage, while ensuring that satisfactory performance is\ndelivered to end-users. AC is already experimented with on various levels of\ndesktop computer architecture, from the hardware level where incorrect adders\nhave been designed to sacrifice result correctness for reduced energy\nconsumption, to compiler-level optimisations that omit certain lines of code to\nspeed up video encoding. AC is yet to be attempted on mobile devices and in\nthis article we examine the potential benefits of mobile AC and present an\noverview of AC techniques applicable in the mobile domain.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 13:10:00 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Pejovic", "Veljko", ""]]}, {"id": "1901.09056", "submitter": "Abhinav Jangda", "authors": "Abhinav Jangda, Bobby Powers, Emery Berger, Arjun Guha", "title": "Not So Fast: Analyzing the Performance of WebAssembly vs. Native Code", "comments": "Accepted (to appear) at USENIX Annual Technical Conference 2019", "journal-ref": null, "doi": "10.5555/3358807.3358817", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All major web browsers now support WebAssembly, a low-level bytecode intended\nto serve as a compilation target for code written in languages like C and C++.\nA key goal of WebAssembly is performance parity with native code; previous work\nreports near parity, with many applications compiled to WebAssembly running on\naverage 10% slower than native code. However, this evaluation was limited to a\nsuite of scientific kernels, each consisting of roughly 100 lines of code.\nRunning more substantial applications was not possible because compiling code\nto WebAssembly is only part of the puzzle: standard Unix APIs are not available\nin the web browser environment. To address this challenge, we build\nBrowsix-Wasm, a significant extension to Browsix that, for the first time,\nmakes it possible to run unmodified WebAssembly-compiled Unix applications\ndirectly inside the browser. We then use Browsix-Wasm to conduct the first\nlarge-scale evaluation of the performance of WebAssembly vs. native. Across the\nSPEC CPU suite of benchmarks, we find a substantial performance gap:\napplications compiled to WebAssembly run slower by an average of 45% (Firefox)\nto 55% (Chrome), with peak slowdowns of 2.08x (Firefox) and 2.5x (Chrome). We\nidentify the causes of this performance degradation, some of which are due to\nmissing optimizations and code generation issues, while others are inherent to\nthe WebAssembly platform.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 19:20:44 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 22:20:48 GMT"}, {"version": "v3", "created": "Fri, 31 May 2019 14:10:41 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Jangda", "Abhinav", ""], ["Powers", "Bobby", ""], ["Berger", "Emery", ""], ["Guha", "Arjun", ""]]}, {"id": "1901.09409", "submitter": "Blake Johnson", "authors": "Blake Johnson, Rahul Simha", "title": "CRAQL: A Composable Language for Querying Source Code", "comments": "10 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the design and implementation of CRAQL (Composable\nRepository Analysis and Query Language), a new query language for source code.\nThe growth of source code mining and its applications suggest the need for a\nquery language that can fully utilize and correlate across the unique structure\nand metadata of parsed source code.\n  CRAQL is built on an underlying abstraction analogous to the underpinnings of\nSQL, but aimed at parsed source code. Thus, while SQL queries' inputs and\noutputs are sets of tuples, CRAQL queries' inputs and outputs are sets of\nabstract syntax trees (ASTs). This abstraction makes CRAQL queries composable\n(the output of one query can become the input to another) and improves the\npower of the language by allowing for querying of the tree structure and\nmetadata, as well as raw text. Furthermore, the abstraction enables\ntree-specific language optimizations and allows CRAQL to be easily applied to\nany language that is parsable into ASTs. These attributes, along with a\nfamiliar syntax similar to SQL, allow complex queries to be expressed in a\ncompact, straightforward manner. Questions such as \"find the longest series of\nstatements without any loops,\" \"find methods that are never called,\" \"find\ngetters (0-parameter methods with a single statement that returns a member\nvariable),\" or \"find the percentage of variables declared at the top of a\nblock\" all translate to simple, understandable queries in CRAQL.\n  In this paper we describe the language, its features and capabilities. We\ncompare CRAQL to other languages for querying source code and find that it has\npotential advantages in clarity and compactness. We discuss the features and\noptimizations added to support searching parse tree collections effectively and\nefficiently. Finally, we summarize the application of the language to millions\nof Java source files, the details of which are in a companion paper.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 17:54:51 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Johnson", "Blake", ""], ["Simha", "Rahul", ""]]}, {"id": "1901.09606", "submitter": "Julien Lange", "authors": "Julien Lange, Nobuko Yoshida", "title": "Verifying Asynchronous Interactions via Communicating Session Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a sound procedure to verify properties of communicating\nsession automata (CSA), i.e., communicating automata that include multiparty\nsession types. We introduce a new asynchronous compatibility property for CSA,\ncalled k-multiparty compatibility (k-MC), which is a strict superset of the\nsynchronous multiparty compatibility used in theories and tools based on\nsession types. It is decomposed into two bounded properties: (i) a condition\ncalled k-safety which guarantees that, within the bound, all sent messages can\nbe received and each automaton can make a move; and (ii) a condition called\nk-exhaustivity which guarantees that all k-reachable send actions can be fired\nwithin the bound. We show that k-exhaustivity implies existential boundedness,\nand soundly and completely characterises systems where each automaton behaves\nequivalently under bounds greater than or equal to k. We show that checking\nk-MC is PSPACE-complete, and demonstrate its performance empirically over large\nsystems using partial order reduction.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 11:24:29 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 12:20:06 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Lange", "Julien", ""], ["Yoshida", "Nobuko", ""]]}, {"id": "1901.10118", "submitter": "EPTCS", "authors": "Robert Rand, Jennifer Paykin, Dong-Ho Lee, Steve Zdancewic", "title": "ReQWIRE: Reasoning about Reversible Quantum Circuits", "comments": "In Proceedings QPL 2018, arXiv:1901.09476", "journal-ref": "EPTCS 287, 2019, pp. 299-312", "doi": "10.4204/EPTCS.287.17", "report-no": null, "categories": "cs.LO cs.ET cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common quantum algorithms make heavy use of ancillae: scratch qubits that are\ninitialized at some state and later returned to that state and discarded.\nExisting quantum circuit languages let programmers assert that a qubit has been\nreturned to the |0> state before it is discarded, allowing for a range of\noptimizations. However, existing languages do not provide the tools to verify\nthese assertions, introducing a potential source of errors. In this paper we\npresent methods for verifying that ancillae are discarded in the desired state,\nand use these methods to implement a verified compiler from classical functions\nto quantum oracles.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 05:40:06 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Rand", "Robert", ""], ["Paykin", "Jennifer", ""], ["Lee", "Dong-Ho", ""], ["Zdancewic", "Steve", ""]]}, {"id": "1901.10541", "submitter": "Bart Jacobs", "authors": "Willem Penninckx and Amin Timany and Bart Jacobs", "title": "Abstract I/O Specification", "comments": "24 pages, 5 figures. Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recently proposed an approach for the specification and modular formal\nverification of the interactive (I/O) behavior of programs, based on an\nembedding of Petri nets into separation logic. While this approach is scalable\nand modular in terms of the I/O APIs available to a program, enables composing\nlow-level I/O actions into high-level ones, and enables a convenient\nverification experience, it does not support high-level I/O actions that\ninvolve memory manipulation as well as low-level I/O (such as buffered I/O), or\nthat are in fact \"virtual I/O\" actions that are implemented purely through\nmemory manipulation. Furthermore, it does not allow rewriting an I/O\nspecification into an equivalent one.\n  In this paper, we propose a refined approach that does have these properties.\nThe essential insight is to fix the set of places of the Petri net to be the\nset of separation logic assertions, thus making available the full power of\nseparation logic for abstractly stating an arbitrary operation's specification\nin Petri net form, for composing operations into an I/O specification, and for\nequivalence reasoning on I/O specifications. Our refinement resolves the issue\nof the justification of the choice of Petri nets over other formalisms such as\ngeneral state transition systems, in that it \"refines them away\" into the more\nessential constructs of separating conjunction and abstract nested triples. To\nenable a convenient treatment of input operations, we propose the use of\nprophecy variables to eliminate their non-determinism.\n  We illustrate the approach through a number of example programs, including\none where subroutines specified and verified using I/O specifications run as\nthreads communicating through shared memory. The theory and examples of the\npaper have been machine-checked using the Iris library for program verification\nin the Coq proof assistant.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 20:53:02 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Penninckx", "Willem", ""], ["Timany", "Amin", ""], ["Jacobs", "Bart", ""]]}, {"id": "1901.10926", "submitter": "Peter Breuer", "authors": "Peter T. Breuer", "title": "Safe Compilation for Hidden Deterministic Hardware Aliasing and\n  Encrypted Computing", "comments": "16 pages, early version of submission prepared for Ada-Europe 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hardware aliasing occurs when the same logical address sporadically accesses\ndifferent physical memory locations and is a problem encountered by systems\nprogrammers (the opposite, software aliasing, when different addresses access\nthe same location, is more familiar to application programmers). This paper\nshows how to compile so code works in the presence of {\\em hidden\ndeterministic} hardware aliasing. That means that a copy of an address always\naccesses the same location, and recalculating it exactly the same way also\nalways gives the same access, but otherwise access appears arbitrary and\nunpredictable. The technique is extended to cover the emerging technology of\nencrypted computing too.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 16:19:12 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Breuer", "Peter T.", ""]]}, {"id": "1901.10961", "submitter": "M. H. van Emden", "authors": "M.H. van Emden", "title": "Egyptian multiplication and some of its ramifications", "comments": "7 pages and 6 figures", "journal-ref": null, "doi": null, "report-no": "DCS-362-IR", "categories": "math.NA cs.NA cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplication and exponentiation can be defined by equations in which one of\nthe operands is written as the sum of powers of two. When these powers are\nnon-negative integers, the operand is integer; without this restriction it is a\nfraction. The defining equation can be used in evaluation mode or in solving\nmode. In the former case we obtain \"Egyptian\" multiplication, dating from the\n17th century BC. In solving mode we obtain an efficient algorithm for division\nby repeated subtraction, dating from the 20th century AD. In the exponentiation\ncase we also distinguish between evaluation mode and solving mode. Evaluation\nmode yields a possibly new algorithm for raising to a fractional power. Solving\nmode yields the algorithm for logarithms invented by Briggs in the 17th century\nAD.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 17:20:16 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2020 03:31:56 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["van Emden", "M. H.", ""]]}, {"id": "1901.11054", "submitter": "Prakash Murali", "authors": "Prakash Murali, Jonathan M. Baker, Ali Javadi Abhari, Frederic T.\n  Chong, Margaret Martonosi", "title": "Noise-Adaptive Compiler Mappings for Noisy Intermediate-Scale Quantum\n  Computers", "comments": "To appear in ASPLOS'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A massive gap exists between current quantum computing (QC) prototypes, and\nthe size and scale required for many proposed QC algorithms. Current QC\nimplementations are prone to noise and variability which affect their\nreliability, and yet with less than 80 quantum bits (qubits) total, they are\ntoo resource-constrained to implement error correction. The term Noisy\nIntermediate-Scale Quantum (NISQ) refers to these current and near-term systems\nof 1000 qubits or less. Given NISQ's severe resource constraints, low\nreliability, and high variability in physical characteristics such as coherence\ntime or error rates, it is of pressing importance to map computations onto them\nin ways that use resources efficiently and maximize the likelihood of\nsuccessful runs.\n  This paper proposes and evaluates backend compiler approaches to map and\noptimize high-level QC programs to execute with high reliability on NISQ\nsystems with diverse hardware characteristics. Our techniques all start from an\nLLVM intermediate representation of the quantum program (such as would be\ngenerated from high-level QC languages like Scaffold) and generate QC\nexecutables runnable on the IBM Q public QC machine. We then use this framework\nto implement and evaluate several optimal and heuristic mapping methods. These\nmethods vary in how they account for the availability of dynamic machine\ncalibration data, the relative importance of various noise parameters, the\ndifferent possible routing strategies, and the relative importance of\ncompile-time scalability versus runtime success. Using real-system\nmeasurements, we show that fine grained spatial and temporal variations in\nhardware parameters can be exploited to obtain an average $2.9$x (and up to\n$18$x) improvement in program success rate over the industry standard IBM\nQiskit compiler.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 19:21:54 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Murali", "Prakash", ""], ["Baker", "Jonathan M.", ""], ["Abhari", "Ali Javadi", ""], ["Chong", "Frederic T.", ""], ["Martonosi", "Margaret", ""]]}, {"id": "1901.11100", "submitter": "Emery Berger", "authors": "Daniel W. Barowy, Emery D. Berger, Benjamin Zorn", "title": "ExceLint: Automatically Finding Spreadsheet Formula Errors", "comments": "Appeared at OOPSLA 2018", "journal-ref": "Proceedings of the ACM on Programming Languages, Volume 2 Issue\n  OOPSLA, November 2018", "doi": "10.1145/3276518", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spreadsheets are one of the most widely used programming environments, and\nare widely deployed in domains like finance where errors can have catastrophic\nconsequences. We present a static analysis specifically designed to find\nspreadsheet formula errors. Our analysis directly leverages the rectangular\ncharacter of spreadsheets. It uses an information-theoretic approach to\nidentify formulas that are especially surprising disruptions to nearby\nrectangular regions. We present ExceLint, an implementation of our static\nanalysis for Microsoft Excel. We demonstrate that ExceLint is fast and\neffective: across a corpus of 70 spreadsheets, ExceLint takes a median of 5\nseconds per spreadsheet, and it significantly outperforms the state of the art\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 21:00:57 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Barowy", "Daniel W.", ""], ["Berger", "Emery D.", ""], ["Zorn", "Benjamin", ""]]}, {"id": "1901.11411", "submitter": "Gabriel Radanne", "authors": "Gabriel Radanne, J\\'er\\^ome Vouillon, Vincent Balat", "title": "Eliom: A Language for Modular Tierless Web Programming", "comments": "60 pages, 28 appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Tierless Web programming languages allow programmers to combine client-side\nand server-side programming in a single program. Programmers can then define\ncomponents with both client and server parts and get flexible, efficient and\ntypesafe client-server communications. However, the expressive client-server\nfeatures found in most tierless languages are not necessarily compatible with\nfunctionalities found in many mainstream languages. In particular, we would\nlike to benefit from type safety, an efficient execution, static compilation,\nmodularity and separate compilation.\n  In this paper, we propose Eliom, an industrial-strength tierless functional\nWeb programming language which extends OCaml with support for rich\nclient/server interactions. It allows to build whole applications as a single\ndistributed program, in which it is possible to define modular tierless\nlibraries with both server and client behaviors and combine them effortlessly.\nEliom is the only language that combines type-safe and efficient client/server\ncommunications with a static compilation model that supports separate\ncompilation and modularity. It also supports excellent integration with OCaml,\nallowing to transparently leverage its ecosystem.\n  To achieve all these features, Eliom borrows ideas not only from distributed\nprogramming languages, but also from meta-programming and modern module\nsystems. We present the design of Eliom, how it can be used in practice and its\nformalization; including its type system, semantics and compilation scheme. We\nshow that this compilation scheme preserves typing and semantics, and that it\nsupports separate compilation.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 14:57:39 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Radanne", "Gabriel", ""], ["Vouillon", "J\u00e9r\u00f4me", ""], ["Balat", "Vincent", ""]]}]