[{"id": "2002.00620", "submitter": "Kazuhiko Sakaguchi", "authors": "Kazuhiko Sakaguchi", "title": "Validating Mathematical Structures", "comments": "Preprint of the paper accepted in proceedings of IJCAR 2020, LNCS,\n  Springer, including appendix", "journal-ref": "IJCAR 2020, LNCS, Springer, vol. 12167, pp. 138--157", "doi": "10.1007/978-3-030-51054-1_8", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sharing of notations and theories across an inheritance hierarchy of\nmathematical structures, e.g., groups and rings, is important for productivity\nwhen formalizing mathematics in proof assistants. The packed classes\nmethodology is a generic design pattern to define and combine mathematical\nstructures in a dependent type theory with records. When combined with\nmechanisms for implicit coercions and unification hints, packed classes enable\nautomated structure inference and subtyping in hierarchies, e.g., that a ring\ncan be used in place of a group. However, large hierarchies based on packed\nclasses are challenging to implement and maintain. We identify two hierarchy\ninvariants that ensure modularity of reasoning and predictability of inference\nwith packed classes, and propose algorithms to check these invariants. We\nimplement our algorithms as tools for the Coq proof assistant, and show that\nthey significantly improve the development process of Mathematical Components,\na library for formalized mathematics.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 09:31:24 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 13:42:53 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Sakaguchi", "Kazuhiko", ""]]}, {"id": "2002.00776", "submitter": "Dominic Steinh\\\"ofel", "authors": "Nathan Wasser and Dominic Steinh\\\"ofel", "title": "Treating for-Loops as First-Class Citizens in Proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indexed loop scopes have been shown to be a helpful tool in creating sound\nloop invariant rules in dynamic logic for programming languages with abrupt\ncompletion, such as Java. These rules do not require program transformation of\nthe loop body, as other approaches to dealing with abrupt completion do.\nHowever, indexed loop scopes were designed specifically to provide a loop\ninvariant rule for while loops and work rather opaquely. Here we propose\nreplacing indexed loop scopes with a more transparent solution, which also lets\nus extend this idea from while loops to for loops. We further present sound\nloop unrolling rules for while, do and for loops, which require neither program\ntransformation of the loop body, nor the use of nested modalities. This\napproach allows for loops to be treated as first-class citizens in proofs --\nrather than the usual approach of transforming for loops into while loops --\nwhich makes semi-automated proofs more transparent and easier to follow for the\nuser, whose interactions may be required in order to close the proofs.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 14:25:15 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Wasser", "Nathan", ""], ["Steinh\u00f6fel", "Dominic", ""]]}, {"id": "2002.01184", "submitter": "Junpeng Lao", "authors": "Junpeng Lao, Christopher Suter, Ian Langmore, Cyril Chimisov, Ashish\n  Saxena, Pavel Sountsov, Dave Moore, Rif A. Saurous, Matthew D. Hoffman, and\n  Joshua V. Dillon", "title": "tfp.mcmc: Modern Markov Chain Monte Carlo Tools Built for Modern\n  Hardware", "comments": "Based on extended abstract submitted to PROBPROG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.PL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Markov chain Monte Carlo (MCMC) is widely regarded as one of the most\nimportant algorithms of the 20th century. Its guarantees of asymptotic\nconvergence, stability, and estimator-variance bounds using only unnormalized\nprobability functions make it indispensable to probabilistic programming. In\nthis paper, we introduce the TensorFlow Probability MCMC toolkit, and discuss\nsome of the considerations that motivated its design.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 09:27:26 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Lao", "Junpeng", ""], ["Suter", "Christopher", ""], ["Langmore", "Ian", ""], ["Chimisov", "Cyril", ""], ["Saxena", "Ashish", ""], ["Sountsov", "Pavel", ""], ["Moore", "Dave", ""], ["Saurous", "Rif A.", ""], ["Hoffman", "Matthew D.", ""], ["Dillon", "Joshua V.", ""]]}, {"id": "2002.01842", "submitter": "Uwe Meyer", "authors": "Uwe Meyer, Bj\\\"orn Pfarr", "title": "Patterns for Name Analysis and Type Analysis with JastAdd", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two decades, tools have been implemented to more formally specify\nthe semantic analysis phase of a compiler instead of relying on handwritten\ncode. In this paper, we introduce patterns and a method to translate a formal\ndefinition of a language's type system into a specification for JastAdd, which\nis one of the aforementioned tools based on Reference Attribute Grammars. This\nmethodological approach will help language designers and compiler engineers to\nmore systematically use such tools for semantic analysis. As an example, we use\na simple, yet complete imperative language and provide an outlook on how the\nmethod can be extended to cover further language constructs or even type\ninference.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 16:06:40 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Meyer", "Uwe", ""], ["Pfarr", "Bj\u00f6rn", ""]]}, {"id": "2002.01960", "submitter": "Ryan Kavanagh", "authors": "Ryan Kavanagh", "title": "A Domain Semantics for Higher-Order Recursive Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The polarized SILL programming language uniformly integrates functional\nprogramming and session-typed message-passing concurrency. It supports general\nrecursion, asynchronous and synchronous communication, and higher-order\nprograms that communicate channels and processes. We give polarized SILL a\ndomain-theoretic semantics---the first denotational semantics for a language\nwith this combination of features. Session types in polarized SILL denote pairs\nof domains of unidirectional communications. Processes denote continuous\nfunctions between these domains, and process composition is interpreted by a\ntrace operator. We illustrate our semantics by validating expected program\nequivalences.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 19:14:31 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 18:41:28 GMT"}, {"version": "v3", "created": "Sun, 10 May 2020 19:51:23 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Kavanagh", "Ryan", ""]]}, {"id": "2002.02145", "submitter": "Sanket Tavarageri", "authors": "Sanket Tavarageri, Alexander Heinecke, Sasikanth Avancha, Gagandeep\n  Goyal, Ramakrishna Upadrasta, Bharat Kaul", "title": "PolyScientist: Automatic Loop Transformations Combined with Microkernels\n  for Optimization of Deep Learning Primitives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the heart of deep learning training and inferencing are computationally\nintensive primitives such as convolutions which form the building blocks of\ndeep neural networks. Researchers have taken two distinct approaches to\ncreating high performance implementations of deep learning kernels, namely, 1)\nlibrary development exemplified by Intel MKL-DNN for CPUs, 2) automatic\ncompilation represented by the TensorFlow XLA compiler. The two approaches have\ntheir drawbacks: even though a custom built library can deliver very good\nperformance, the cost and time of development of the library can be high.\nAutomatic compilation of kernels is attractive but in practice, till date,\nautomatically generated implementations lag expert coded kernels in performance\nby orders of magnitude.\n  In this paper, we develop a hybrid solution to the development of deep\nlearning kernels that achieves the best of both worlds: the expert coded\nmicrokernels are utilized for the innermost loops of kernels and we use the\nadvanced polyhedral technology to automatically tune the outer loops for\nperformance. We design a novel polyhedral model based data reuse algorithm to\noptimize the outer loops of the kernel. Through experimental evaluation on an\nimportant class of deep learning primitives namely convolutions, we demonstrate\nthat the approach we develop attains the same levels of performance as Intel\nMKL-DNN, a hand coded deep learning library.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 08:02:34 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Tavarageri", "Sanket", ""], ["Heinecke", "Alexander", ""], ["Avancha", "Sasikanth", ""], ["Goyal", "Gagandeep", ""], ["Upadrasta", "Ramakrishna", ""], ["Kaul", "Bharat", ""]]}, {"id": "2002.02171", "submitter": "Ruben Pieters", "authors": "Ruben P. Pieters and Tom Schrijvers", "title": "PaSe: An Extensible and Inspectable DSL for Micro-Animations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents PaSe, an extensible and inspectable DSL embedded in\nHaskell for expressing micro-animations. The philosophy of PaSe is to compose\nanimations based on sequential and parallel composition of smaller animations.\nThis differs from other animation libraries that focus more on sequential\ncomposition and have only limited forms of parallel composition. To provide\nsimilar flexibility as other animation libraries, PaSe features extensibility\nof operations and inspectability of animations. We present the features of PaSe\nwith a to-do list application, discuss the PaSe implementation, and argue that\nthe callback style of extensibility is detrimental for correctly combining PaSe\nfeatures. We contrast with the GreenSock Animation Platform, a\nprofessional-grade and widely used JavaScript animation library, to illustrate\nthis point.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 09:39:43 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Pieters", "Ruben P.", ""], ["Schrijvers", "Tom", ""]]}, {"id": "2002.02268", "submitter": "Bastian Hagedorn", "authors": "Bastian Hagedorn, Johannes Lenfers, Thomas Koehler, Sergei Gorlatch,\n  Michel Steuwer", "title": "A Language for Describing Optimization Strategies", "comments": "https://elevate-lang.org/ https://github.com/elevate-lang", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimizing programs to run efficiently on modern parallel hardware is hard\nbut crucial for many applications. The predominantly used imperative languages\n- like C or OpenCL - force the programmer to intertwine the code describing\nfunctionality and optimizations. This results in a nightmare for portability\nwhich is particularly problematic given the accelerating trend towards\nspecialized hardware devices to further increase efficiency.\n  Many emerging DSLs used in performance demanding domains such as deep\nlearning, automatic differentiation, or image processing attempt to simplify or\neven fully automate the optimization process. Using a high-level - often\nfunctional - language, programmers focus on describing functionality in a\ndeclarative way. In some systems such as Halide or TVM, a separate schedule\nspecifies how the program should be optimized. Unfortunately, these schedules\nare not written in well-defined programming languages. Instead, they are\nimplemented as a set of ad-hoc predefined APIs that the compiler writers have\nexposed.\n  In this paper, we present Elevate: a functional language for describing\noptimization strategies. Elevate follows a tradition of prior systems used in\ndifferent contexts that express optimization strategies as composition of\nrewrites. In contrast to systems with scheduling APIs, in Elevate programmers\nare not restricted to a set of built-in optimizations but define their own\noptimization strategies freely in a composable way. We show how user-defined\noptimization strategies in Elevate enable the effective optimization of\nprograms expressed in a functional data-parallel language demonstrating\ncompetitive performance with Halide and TVM.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 14:20:33 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Hagedorn", "Bastian", ""], ["Lenfers", "Johannes", ""], ["Koehler", "Thomas", ""], ["Gorlatch", "Sergei", ""], ["Steuwer", "Michel", ""]]}, {"id": "2002.02702", "submitter": "Martin Trapp", "authors": "Mohamed Tarek, Kai Xu, Martin Trapp, Hong Ge, Zoubin Ghahramani", "title": "DynamicPPL: Stan-like Speed for Dynamic Probabilistic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the preliminary high-level design and features of DynamicPPL.jl, a\nmodular library providing a lightning-fast infrastructure for probabilistic\nprogramming. Besides a computational performance that is often close to or\nbetter than Stan, DynamicPPL provides an intuitive DSL that allows the rapid\ndevelopment of complex dynamic probabilistic programs. Being entirely written\nin Julia, a high-level dynamic programming language for numerical computing,\nDynamicPPL inherits a rich set of features available through the Julia\necosystem. Since DynamicPPL is a modular, stand-alone library, any\nprobabilistic programming system written in Julia, such as Turing.jl, can use\nDynamicPPL to specify models and trace their model parameters. The main\nfeatures of DynamicPPL are: 1) a meta-programming based DSL for specifying\ndynamic models using an intuitive tilde-based notation; 2) a tracing\ndata-structure for tracking RVs in dynamic probabilistic models; 3) a rich\ncontextual dispatch system allowing tailored behaviour during model execution;\nand 4) a user-friendly syntax for probabilistic queries. Finally, we show in a\nvariety of experiments that DynamicPPL, in combination with Turing.jl, achieves\ncomputational performance that is often close to or better than Stan.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 10:21:49 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Tarek", "Mohamed", ""], ["Xu", "Kai", ""], ["Trapp", "Martin", ""], ["Ge", "Hong", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "2002.02710", "submitter": "Pedro Antonino", "authors": "Pedro Antonino and A. W. Roscoe", "title": "Formalising and verifying smart contracts with Solidifier: a bounded\n  model checker for Solidity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exploitation of smart-contract vulnerabilities can have catastrophic\nconsequences such as the loss of millions of pounds worth of crypto assets.\nFormal verification can be a useful tool in identifying vulnerabilities and\nproving that they have been fixed. In this paper, we present a formalisation of\nSolidity and the Ethereum blockchain using the Solid language and its\nblockchain; a Solid program is obtained by explicating/desugaring a Solidity\nprogram. We make some abstractions that over-approximate the way in which\nSolidity/Ethereum behave. Based on this formalisation, we create Solidifier: a\nbounded model checker for Solidity. It translates Solid into Boogie, an\nintermediate verification language, that is later verified using Corral, a\nbounded model checker for Boogie. Unlike much of the work in this area, we do\nnot try to find specific behavioural/code patterns that might lead to\nvulnerabilities. Instead, we provide a tool to find errors/bad states, i.e.\nprogram states that do not conform with the intent of the developer. Such a bad\nstate, be it a vulnerability or not, might be reached through the execution of\nspecific known code patterns or through behaviours that have not been\nanticipated.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 10:54:57 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Antonino", "Pedro", ""], ["Roscoe", "A. W.", ""]]}, {"id": "2002.02884", "submitter": "Mark Santolucito", "authors": "Kairo Morton, William Hallahan, Elven Shum, Ruzica Piskac, Mark\n  Santolucito", "title": "Grammar Filtering For Syntax-Guided Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming-by-example (PBE) is a synthesis paradigm that allows users to\ngenerate functions by simply providing input-output examples. While a promising\ninteraction paradigm, synthesis is still too slow for realtime interaction and\nmore widespread adoption. Existing approaches to PBE synthesis have used\nautomated reasoning tools, such as SMT solvers, as well as works applying\nmachine learning techniques. At its core, the automated reasoning approach\nrelies on highly domain specific knowledge of programming languages. On the\nother hand, the machine learning approaches utilize the fact that when working\nwith program code, it is possible to generate arbitrarily large training\ndatasets. In this work, we propose a system for using machine learning in\ntandem with automated reasoning techniques to solve Syntax Guided Synthesis\n(SyGuS) style PBE problems. By preprocessing SyGuS PBE problems with a neural\nnetwork, we can use a data driven approach to reduce the size of the search\nspace, then allow automated reasoning-based solvers to more quickly find a\nsolution analytically. Our system is able to run atop existing SyGuS PBE\nsynthesis tools, decreasing the runtime of the winner of the 2019 SyGuS\nCompetition for the PBE Strings track by 47.65% to outperform all of the\ncompeting tools.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 16:35:50 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Morton", "Kairo", ""], ["Hallahan", "William", ""], ["Shum", "Elven", ""], ["Piskac", "Ruzica", ""], ["Santolucito", "Mark", ""]]}, {"id": "2002.02904", "submitter": "Robert Dickerson", "authors": "Robert Dickerson, Qianchuan Ye, Benjamin Delaware", "title": "RHLE: Modular Deductive Verification of Relational $\\forall\\exists$\n  Properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational program logics are used to prove that a desired relationship holds\nbetween the execution of multiple programs. Existing relational program logics\nhave focused on verifying that all runs of a collection of programs do not fall\noutside a desired set of behaviors. Several important relational properties,\nincluding refinement and noninterference, do not fit into this category, as\nthey require the existence of specific desirable executions. This paper\npresents RHLE, a logic for verifying a class of relational properties which we\nterm $\\forall\\exists$ properties. $\\forall\\exists$ properties assert that for\nall executions of a collection of programs, there exist executions of another\nset of programs exhibiting some intended behavior. Importantly, RHLE can reason\nmodularly about programs which make library calls, ensuring that\n$\\forall\\exists$ properties are preserved when the programs are linked with any\nvalid implementation of the library. To achieve this, we develop a novel form\nof function specification that requires the existence of certain behaviors in\nvalid implementations. We have built a tool based on RHLE which we use to\nverify a diverse set of relational properties drawn from the literature,\nincluding refinement and generalized noninterference.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 17:03:53 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 22:17:54 GMT"}, {"version": "v3", "created": "Thu, 28 May 2020 02:50:11 GMT"}, {"version": "v4", "created": "Mon, 30 Nov 2020 23:17:10 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Dickerson", "Robert", ""], ["Ye", "Qianchuan", ""], ["Delaware", "Benjamin", ""]]}, {"id": "2002.02914", "submitter": "Graham Campbell", "authors": "Graham Campbell and Jack Romo and Detlef Plump", "title": "Improving the GP 2 Compiler", "comments": "Technical Report, Department of Computer Science, University of York,\n  42 pages, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  GP 2 is an experimental programming language based on graph transformation\nrules which aims to facilitate program analysis and verification. Writing\nefficient programs in such a language is hard because graph matching is\nexpensive, however GP 2 addresses this problem by providing rooted rules which,\nunder mild conditions, can be matched in constant time using the GP 2 to C\ncompiler. In this report, we document various improvements made to the\ncompiler; most notably the introduction of node lists to improve iteration\nperformance for destructive programs, meaning that binary DAG recognition by\nreduction need only take linear time where the previous implementation required\nquadratic time.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 17:29:54 GMT"}, {"version": "v2", "created": "Fri, 1 Jan 2021 12:44:37 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Campbell", "Graham", ""], ["Romo", "Jack", ""], ["Plump", "Detlef", ""]]}, {"id": "2002.04011", "submitter": "Andr\\'es Ezequiel Viso", "authors": "Antonio Bucciarelli, Delia Kesner, Alejandro R\\'ios, Andr\\'es Viso", "title": "The Bang Calculus Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Call-by-Push-Value (CBPV) is a programming paradigm subsuming both\nCall-by-Name (CBN) and Call-by-Value (CBV) semantics. The paradigm was recently\nmodelled by means of the Bang Calculus, a term language connecting CBPV and\nLinear Logic.\n  This paper presents a revisited version of the Bang Calculus, called $\\lambda\n!$, enjoying some important properties missing in the original system. Indeed,\nthe new calculus integrates commutative conversions to unblock value redexes\nwhile being confluent at the same time. A second contribution is related to\nnon-idempotent types. We provide a quantitative type system for our $\\lambda\n!$-calculus, and we show that the length of the (weak) reduction of a typed\nterm to its normal form \\emph{plus} the size of this normal form is bounded by\nthe size of its type derivation. We also explore the properties of this type\nsystem with respect to CBN/CBV translations. We keep the original CBN\ntranslation from $\\lambda$-calculus to the Bang Calculus, which preserves\nnormal forms and is sound and complete with respect to the (quantitative) type\nsystem for CBN. However, in the case of CBV, we reformulate both the\ntranslation and the type system to restore two main properties: preservation of\nnormal forms and completeness. Last but not least, the quantitative system is\nrefined to a \\emph{tight} one, which transforms the previous upper bound on the\nlength of reduction to normal form plus its size into two independent\n\\emph{exact} measures for them.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 18:38:00 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 13:02:32 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Bucciarelli", "Antonio", ""], ["Kesner", "Delia", ""], ["R\u00edos", "Alejandro", ""], ["Viso", "Andr\u00e9s", ""]]}, {"id": "2002.04607", "submitter": "Klaas Pruiksma", "authors": "Klaas Pruiksma and Frank Pfenning", "title": "Back to Futures", "comments": "31 pages, 3 figures. Submitted to ESOP 2021. This replaces a previous\n  version with similar content, but has been heavily rewritten to reflect\n  increased understanding of the contents", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common approaches to concurrent programming begin with languages whose\nsemantics are naturally sequential and add new constructs that provide limited\naccess to concurrency, as exemplified by futures. This approach has been quite\nsuccessful, but often does not provide a satisfactory theoretical backing for\nthe concurrency constructs, and it can be difficult to give a good semantics\nthat allows a programmer to use more than one of these constructs at a time.\n  We take a different approach, starting with a concurrent language based on a\nCurry-Howard interpretation of adjoint logic, to which we add three atomic\nprimitives that allow us to encode sequential composition and various forms of\nsynchronization. The resulting language is highly expressive, allowing us to\nencode futures, fork/join parallelism, and monadic concurrency in the same\nframework. Notably, since our language is based on adjoint logic, we are able\nto give a formal account of linear futures, which have been used in complexity\nanalysis by Blelloch and Reid-Miller. The uniformity of this approach means\nthat we can similarly work with many of the other concurrency primitives in a\nlinear fashion, and that we can mix several of these forms of concurrency in\nthe same program to serve different purposes.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 18:57:32 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 19:56:45 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Pruiksma", "Klaas", ""], ["Pfenning", "Frank", ""]]}, {"id": "2002.04694", "submitter": "Pavol Bielik", "authors": "Pavol Bielik and Martin Vechev", "title": "Adversarial Robustness for Code", "comments": "Proceedings of the 37th International Conference on Machine Learning,\n  Online, PMLR 119, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning and deep learning in particular has been recently used to\nsuccessfully address many tasks in the domain of code such as finding and\nfixing bugs, code completion, decompilation, type inference and many others.\nHowever, the issue of adversarial robustness of models for code has gone\nlargely unnoticed. In this work, we explore this issue by: (i) instantiating\nadversarial attacks for code (a domain with discrete and highly structured\ninputs), (ii) showing that, similar to other domains, neural models for code\nare vulnerable to adversarial attacks, and (iii) combining existing and novel\ntechniques to improve robustness while preserving high accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 21:32:14 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2020 12:35:28 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Bielik", "Pavol", ""], ["Vechev", "Martin", ""]]}, {"id": "2002.04903", "submitter": "Akash Lal", "authors": "Pantazis Deligiannis and Narayanan Ganapathy and Akash Lal and Shaz\n  Qadeer", "title": "Building Reliable Cloud Services Using P# (Experience Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud services must typically be distributed across a large number of\nmachines in order to make use of multiple compute and storage resources. This\nopens the programmer to several sources of complexity such as concurrency,\norder of message delivery, lossy network, timeouts and failures, all of which\nimpose a high cognitive burden. This paper presents evidence that technology\ninspired by formal-methods, delivered as part of a programming framework, can\nhelp address these challenges. In particular, we describe the experience of\nseveral engineering teams in Microsoft Azure that used the open-source P#\nprogramming framework to build multiple reliable cloud services. P# imposes a\nprincipled design pattern that allows writing formal specifications alongside\nproduction code that can be systematically tested, without deviating from\nroutine engineering practices. Engineering teams that have been using P# have\nreported dramatically increased productivity (in time taken to push new\nfeatures to production) as well as services that have been running live for\nmonths without any issues in features developed and tested with P#.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 10:38:59 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Deligiannis", "Pantazis", ""], ["Ganapathy", "Narayanan", ""], ["Lal", "Akash", ""], ["Qadeer", "Shaz", ""]]}, {"id": "2002.05649", "submitter": "Gabriele Vanoni", "authors": "Beniamino Accattoli and Ugo Dal Lago and Gabriele Vanoni", "title": "The Abstract Machinery of Interaction (Long Version)", "comments": "Accepted at PPDP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper revisits the Interaction Abstract Machine (IAM), a machine based\non Girard's Geometry of Interaction, introduced by Mackie and Danos & Regnier.\nIt is an unusual machine, not relying on environments, presented on linear\nlogic proof nets, and whose soundness proof is convoluted and passes through\nvarious other formalisms. Here we provide a new direct proof of its\ncorrectness, based on a variant of Sands's improvements, a natural notion of\nbisimulation. Moreover, our proof is carried out on a new presentation of the\nIAM, defined as a machine acting directly on $\\lambda$-terms, rather than on\nlinear logic proof nets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 17:32:39 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 10:53:23 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Accattoli", "Beniamino", ""], ["Lago", "Ugo Dal", ""], ["Vanoni", "Gabriele", ""]]}, {"id": "2002.05796", "submitter": "M. Akif \\\"Ozkan", "authors": "M. Akif \\\"Ozkan, Ars\\`ene P\\'erard-Gayot, Richard Membarth, Philipp\n  Slusallek, Roland Leissa, Sebastian Hack, J\\\"urgen Teich, Frank Hannig", "title": "AnyHLS: High-Level Synthesis with Partial Evaluation", "comments": "12 pages, 9 figures", "journal-ref": null, "doi": "10.1109/TCAD.2020.3012172", "report-no": "3012172", "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FPGAs excel in low power and high throughput computations, but they are\nchallenging to program. Traditionally, developers rely on hardware description\nlanguages like Verilog or VHDL to specify the hardware behavior at the\nregister-transfer level. High-Level Synthesis (HLS) raises the level of\nabstraction, but still requires FPGA design knowledge. Programmers usually\nwrite pragma-annotated C/C++ programs to define the hardware architecture of an\napplication. However, each hardware vendor extends its own C dialect using its\nown vendor-specific set of pragmas. This prevents portability across different\nvendors. Furthermore, pragmas are not first-class citizens in the language.\nThis makes it hard to use them in a modular way or design proper abstractions.\nIn this paper, we present AnyHLS, an approach to synthesize FPGA designs in a\nmodular and abstract way. AnyHLS is able to raise the abstraction level of\nexisting HLS tools by resorting to programming language features such as types\nand higher-order functions as follows: It relies on partial evaluation to\nspecialize and to optimize the user application based on a library of\nabstractions. Then, vendor-specific HLS code is generated for Intel and Xilinx\nFPGAs. Portability is obtained by avoiding any vendor-specific pragmas at the\nsource code. In order to validate achievable gains in productivity, a library\nfor the domain of image processing is introduced as a case study, and its\nsynthesis results are compared with several state-of-theart Domain-Specific\nLanguage (DSL) approaches for this domain.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 22:06:26 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 06:03:05 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["\u00d6zkan", "M. Akif", ""], ["P\u00e9rard-Gayot", "Ars\u00e8ne", ""], ["Membarth", "Richard", ""], ["Slusallek", "Philipp", ""], ["Leissa", "Roland", ""], ["Hack", "Sebastian", ""], ["Teich", "J\u00fcrgen", ""], ["Hannig", "Frank", ""]]}, {"id": "2002.06176", "submitter": "Satoshi Egi", "authors": "Satoshi Egi (Rakuten Institute of Technology, Japan), Yuichi Nishiwaki\n  (The University of Tokyo, Japan)", "title": "Functional Programming in Pattern-Match-Oriented Programming Style", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2020, Vol. 4,\n  Issue 3, Article 7", "doi": "10.22152/programming-journal.org/2020/4/7", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout the history of functional programming, recursion has emerged as a\nnatural method for describing loops in programs. However, there does often\nexist a substantial cognitive distance between the recursive definition and the\nsimplest explanation of an algorithm even for the basic list processing\nfunctions such as map, concat, or unique; when we explain these functions, we\nseldom use recursion explicitly as we do in functional programming. For\nexample, map is often explained as follows: the map function takes a function\nand a list and returns a list of the results of applying the function to all\nthe elements of the list.\n  This paper advocates a new programming paradigm called pattern-match-oriented\nprogramming for filling this gap. An essential ingredient of our method is\nutilizing pattern matching for non-free data types. Pattern matching for\nnon-free data types features non-linear pattern matching with backtracking and\nextensibility of pattern-matching algorithms. Several non-standard pattern\nconstructs, such as not-patterns, loop patterns, and sequential patterns, are\nderived from this pattern-matching facility. Based on that result, this paper\nintroduces many programming techniques that replace explicit recursions with an\nintuitive pattern by confining recursions inside patterns. We classify these\ntechniques as pattern-match-oriented programming design patterns.\n  These programming techniques allow us to redefine not only the most basic\nfunctions for list processing such as map, concat, or unique more elegantly\nthan the traditional functional programming style, but also more practical\nmathematical algorithms and software such as a SAT solver, computer algebra\nsystem, and database query language that we had not been able to implement\nconcisely.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 18:55:54 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Egi", "Satoshi", "", "Rakuten Institute of Technology, Japan"], ["Nishiwaki", "Yuichi", "", "The University of Tokyo, Japan"]]}, {"id": "2002.06178", "submitter": "Greg Michaelson", "authors": "Greg Michaelson (Heriot-Watt University, United Kingdom)", "title": "Programming Paradigms, Turing Completeness and Computational Thinking", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2020, Vol. 4,\n  Issue 3, Article 4", "doi": "10.22152/programming-journal.org/2020/4/4", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of programming paradigms, with associated programming languages\nand methodologies, is a well established tenet of Computer Science pedagogy,\nenshrined in international curricula. However, this notion sits ill with Kuhn's\nclassic conceptualisation of a scientific paradigm as a dominant world view,\nwhich supersedes its predecessors through superior explanatory power.\nFurthermore, it is not at all clear how programming paradigms are to be\ncharacterised and differentiated. Indeed, on closer inspection, apparently\ndisparate programming paradigms are very strongly connected. Rather, they\nshould be viewed as different traditions of a unitary Computer Science paradigm\nof Turing complete computation complemented by Computational Thinking.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 18:56:12 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Michaelson", "Greg", "", "Heriot-Watt University, United Kingdom"]]}, {"id": "2002.06179", "submitter": "Tomoki Nakamaru", "authors": "Tomoki Nakamaru (The University of Tokyo, Japan), Shigeru Chiba (The\n  University of Tokyo, Japan)", "title": "Generating a Generic Fluent API in Java", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2020, Vol. 4,\n  Issue 3, Article 9", "doi": "10.22152/programming-journal.org/2020/4/9", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: The algorithms for generating a safe fluent API are actively studied\nthese years. A safe fluent API is the fluent API that reports incorrect\nchaining of the API methods as a type error to the API users. Although such a\nsafe property improves the productivity of its users, the construction of a\nsafe fluent API is too complicated for the developers. The generation\nalgorithms are studied to reduce the development cost of a safe fluent API. The\nstudy on the generation would benefit a number of programmers since a fluent\nAPI is a popular design in the real world.\n  Inquiry: The generation of a generic fluent API has been left untackled. A\ngeneric fluent API refers to the fluent API that provides generic methods\n(methods that contain type parameters in their definitions). The Stream API in\nJava is an example of such a generic API. The recent research on the safe\nfluent API generation rather focuses on the grammar class that the algorithm\ncan deal with for syntax checking. The key idea of the previous study is to use\nnested generics to represent a stack structure for the parser built on top of\nthe type system. In that idea, the role of a type parameter was limited to\ninternally representing a stack element of that parser on the type system. The\nlibrary developers could not use type parameters to include a generic method in\ntheir API so that the semantic constraints for their API would be statically\nchecked, for example, the type constraint on the items passed through a stream.\n  Approach: We propose an algorithm to generate a generic fluent API. Our\ntranslation algorithm is modeled as the construction of deterministic finite\nautomaton (DFA) with type parameter information. Each state of the DFA holds\ninformation about which type parameters are already bound in that state. This\ninformation is used to identify whether a method invocation in a chain newly\nbinds a type to a type parameter, or refers to a previously bound type. The\nidentification is required since a type parameter in a chain is bound at a\nparticular method invocation, and that bound type is referred to in the\nfollowing method invocations. Our algorithm constructs the DFA by analyzing the\nbinding time of type parameters and their propagation among the states in a DFA\nthat is naively constructed from the given grammar.\n  Knowledge and Importance: Our algorithm helps library developers to develop a\ngeneric fluent API. The ability to generate a generic fluent API is essential\nto bring the safe fluent API generation to the real world since the use of type\nparameters is a common technique in the library API design. By our algorithm,\nthe generation of a safe fluent API will be ready for practical use.\n  Grounding: We implemented a generator named Protocool to demonstrate our\nalgorithm. We also generated several libraries using Protocool to show the\nability and the limitations of our algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 18:56:26 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Nakamaru", "Tomoki", "", "The University of Tokyo, Japan"], ["Chiba", "Shigeru", "", "The\n  University of Tokyo, Japan"]]}, {"id": "2002.06180", "submitter": "Mauricio Verano Merino", "authors": "Mauricio Verano Merino (Eindhoven University of Technology,\n  Netherlands), Jurgen Vinju (Centrum Wiskunde & Informatica - TU Eindhoven,\n  Netherlands), Tijs van der Storm (CWI - University of Groningen, Netherlands)", "title": "Bacat\\'a: Notebooks for DSLs, Almost for Free", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2020, Vol. 4,\n  Issue 3, Article 11", "doi": "10.22152/programming-journal.org/2020/4/11", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Computational notebooks are a contemporary style of literate\nprogramming, in which users can communicate and transfer knowledge by\ninterleaving executable code, output, and prose in a single rich document. A\nDomain-Specific Language (DSL) is an artificial software language tailored for\na particular application domain. Usually, DSL users are domain experts that may\nnot have a software engineering background. As a consequence, they might not be\nfamiliar with Integrated Development Environments (IDEs). Thus, the development\nof tools that offer different interfaces for interacting with a DSL is\nrelevant.\n  Inquiry: However, resources available to DSL designers are limited. We would\nlike to leverage tools used to interact with general purpose languages in the\ncontext of DSLs. Computational notebooks are an example of such tools. Then,\nour main question is: What is an efficient and effective method of designing\nand implementing notebook interfaces for DSLs? By addressing this question we\nmight be able to speed up the development of DSL tools, and ease the\ninteraction between end-users and DSLs.\n  Approach: In this paper, we present Bacat\\'a, a mechanism for generating\nnotebook interfaces for DSLs in a language parametric fashion. We designed this\nmechanism in a way in which language engineers can reuse as many language\ncomponents (e.g., language processors, type checkers, code generators) as\npossible.\n  Knowledge: Our results show that notebook interfaces generated by Bacat\\'a\ncan be automatically generated with little manual configuration. There are few\nconsiderations and caveats that should be addressed by language engineers that\nrely on language design aspects. The creation of a notebook for a DSL with\nBacat\\'a becomes a matter of writing the code that wires existing language\ncomponents in the Rascal language workbench with the Jupyter platform.\n  Grounding: We evaluate Bacat\\'a by generating functional computational\nnotebook interfaces for three different non-trivial DSLs, namely: a small\nsubset of Halide (a DSL for digital image processing), SweeterJS (an extended\nversion of JavaScript), and QL (a DSL for questionnaires). Additionally, it is\nrelevant to generate notebook implementations rather than implementing them\nmanually. We measured and compared the number of Source Lines of Code (SLOCs)\nthat we reused from existing implementations of those languages.\n  Importance: The adoption of notebooks by novice-programmers and end-users has\nmade them very popular in several domains such as exploratory programming, data\nscience, data journalism, and machine learning. Why are they popular? In (data)\nscience, it is essential to make results reproducible as well as\nunderstandable. However, notebooks are only available for GPLs. This paper\nopens up the notebook metaphor for DSLs to improve the end-user experience when\ninteracting with code and to increase DSLs adoption.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 18:56:41 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Merino", "Mauricio Verano", "", "Eindhoven University of Technology,\n  Netherlands"], ["Vinju", "Jurgen", "", "Centrum Wiskunde & Informatica - TU Eindhoven,\n  Netherlands"], ["van der Storm", "Tijs", "", "CWI - University of Groningen, Netherlands"]]}, {"id": "2002.06182", "submitter": "Steven Costiou", "authors": "Steven Costiou (INRIA, France), Vincent Aranega (INRIA, France),\n  Marcus Denker (INRIA, France)", "title": "Sub-method, partial behavioral reflection with Reflectivity: Looking\n  back on 10 years of use", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2020, Vol. 4,\n  Issue 3, Article 5", "doi": "10.22152/programming-journal.org/2020/4/5", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context. Refining or altering existing behavior is the daily work of every\ndeveloper, but that cannot be always anticipated, and software sometimes cannot\nbe stopped. In such cases, unanticipated adaptation of running systems is of\ninterest for many scenarios, ranging from functional upgrades to on-the-fly\ndebugging or monitoring of critical applications.\n  Inquiry. A way of altering software at run time is using behavioral\nreflection, which is particularly well-suited for unanticipated adaptation of\nreal-world systems. Partial behavioral reflection is not a new idea, and for\nyears many efforts have been made to propose a practical way of expressing it.\nAll these efforts resulted in practical solutions, but which introduced a\nsemantic gap between the code that requires adaptation and the expression of\nthe partial behavior. For example, in Aspect-Oriented Programming, a pointcut\ndescription is expressed in another language, which introduces a new distance\nbetween the behavior expression (the Advice) and the source code in itself.\n  Approach. Ten years ago, the idea of closing the gap between the code and the\nexpression of the partial behavior led to the implementation of the\nReflectivity framework. Using Reflectivity, developers annotate Abstract Syntax\nTree (AST) nodes with meta-behavior which is taken into account by the compiler\nto produce behavioral variations. In this paper, we present Reflectivity, its\nAPI, its implementation and its usage in Pharo. We reflect on ten years of use\nof Reflectivity, and show how it has been used as a basic building block of\nmany innovative ideas.\n  Knowledge. Reflectivity brings a practical way of working at the AST level,\nwhich is a high-level representation of the source code manipulated by software\ndevelopers. It enables a powerful way of dynamically add and modify behavior.\nReflectivity is also a flexible mean to bridge the gap between the expression\nof the meta-behavior and the source code. This ability to apply unanticipated\nadaptation and to provide behavioral reflection led to many experiments and\nprojects during this last decade by external users. Existing work use\nReflectivity to implement reflective libraries or languages extensions,\nfeatherweight code instrumentation, dynamic software update, debugging tools\nand visualization and software analysis tools.\n  Grounding. Reflectivity is actively used in research projects. During the\npast ten years, it served as a support, either for implementation or as a\nfundamental base, for many research work including PhD theses, conference,\njournal and workshop papers. Reflectivity is now an important library of the\nPharo language, and is integrated at the heart of the platform.\n  Importance. Reflectivity exposes powerful abstractions to deal with partial\nbehavioral adaptation, while providing a mature framework for unanticipated,\nnon-intrusive and partial behavioral reflection based on AST annotation.\nFurthermore, even if Reflectivity found its home inside Pharo, it is not a pure\nSmalltalk-oriented solution. As validation over the practical use of\nReflectivity in dynamic object-oriented languages, the API has been ported to\nPython. Finally, the AST annotation feature of Reflectivity opens new\nexperimentation opportunities about the control that developers could gain on\nthe behavior of their own software.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 18:57:11 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Costiou", "Steven", "", "INRIA, France"], ["Aranega", "Vincent", "", "INRIA, France"], ["Denker", "Marcus", "", "INRIA, France"]]}, {"id": "2002.06183", "submitter": "Jeff Smits", "authors": "Jeff Smits (Delft University of Technology, Netherlands), Gabri\\\"el\n  D.P. Konat (Delft University of Technology, Netherlands), Eelco Visser (Delft\n  University of Technology, Netherlands)", "title": "Constructing Hybrid Incremental Compilers for Cross-Module Extensibility\n  with an Internal Build System", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2020, Vol. 4,\n  Issue 3, Article 16", "doi": "10.22152/programming-journal.org/2020/4/16", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Compilation time is an important factor in the adaptability of a\nsoftware project. Fast recompilation enables cheap experimentation with changes\nto a project, as those changes can be tested quickly. Separate and incremental\ncompilation has been a topic of interest for a long time to facilitate fast\nrecompilation.\n  Inquiry: Despite the benefits of an incremental compiler, such compilers are\nusually not the default. This is because incrementalization requires\ncross-cutting, complicated, and error-prone techniques such as dependency\ntracking, caching, cache invalidation, and change detection. Especially in\ncompilers for languages with cross-module definitions and integration,\ncorrectly and efficiently implementing an incremental compiler can be a\nchallenge. Retrofitting incrementality into a compiler is even harder. We\naddress this problem by developing a compiler design approach that reuses parts\nof an existing non-incremental compiler to lower the cost of building an\nincremental compiler. It also gives an intuition into compiling\ndifficult-to-incrementalize language features through staging.\n  Approach: We use the compiler design approach presented in this paper to\ndevelop an incremental compiler for the Stratego term-rewriting language. This\nlanguage has a set of features that at first glance look incompatible with\nincremental compilation. Therefore, we treat Stratego as our critical case to\ndemonstrate the approach on. We show how this approach decomposes the original\ncompiler and has a solution to compile Stratego incrementally. The key idea on\nwhich we build our incremental compiler is to internally use an incremental\nbuild system to wire together the components we extract from the original\ncompiler.\n  Knowledge: The resulting compiler is already in use as a replacement of the\noriginal whole-program compiler. We find that the incremental build system\ninside the compiler is a crucial component of our approach. This allows a\ncompiler writer to think in multiple steps of compilation, and combine that\ninto a incremental compiler almost effortlessly. Normally, separate compilation\n\\`a la C is facilitated by an external build system, where the programmer is\nresponsible for managing dependencies between files. We reuse an existing sound\nand optimal incremental build system, and integrate its dependency tracking\ninto the compiler.\n  Grounding: The incremental compiler for Stratego is available as an artefact\nalong with this article. We evaluate it on a large Stratego project to test its\nperformance. The benchmark replays edits to the Stratego project from version\ncontrol. These benchmarks are part of the artefact, packaged as a virtual\nmachine image for easy reproducibility.\n  Importance: Although we demonstrate our design approach on the Stratego\nprogramming language, we also describe it generally throughout this paper. Many\ncurrently used programming languages have a compiler that is much slower than\nnecessary. Our design provides an approach to change this, by reusing an\nexisting compiler and making it incremental within a reasonable amount of time.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 18:57:48 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Smits", "Jeff", "", "Delft University of Technology, Netherlands"], ["Konat", "Gabri\u00ebl D. P.", "", "Delft University of Technology, Netherlands"], ["Visser", "Eelco", "", "Delft\n  University of Technology, Netherlands"]]}, {"id": "2002.06184", "submitter": "Pascal Weisenburger", "authors": "Pascal Weisenburger (TU Darmstadt, Germany), Guido Salvaneschi (TU\n  Darmstadt, Germany)", "title": "Implementing a Language for Distributed Systems: Choices and Experiences\n  with Type Level and Macro Programming in Scala", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2020, Vol. 4,\n  Issue 3, Article 17", "doi": "10.22152/programming-journal.org/2020/4/17", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multitier programming languages reduce the complexity of developing\ndistributed systems by developing the distributed system in a single coherent\ncode base. The compiler or the runtime separate the code for the components of\nthe distributed system, enabling abstraction over low level implementation\ndetails such as data representation, serialization and network protocols. Our\nScalaLoci language allows developers to declare the different components and\ntheir architectural relation at the type level, allowing static reasoning about\nabout distribution and remote communication and guaranteeing static type safety\nacross components. The compiler splits the multitier program into the\ncomponent-specific code and automatically generates the communication\nboilerplate. Communication between components can be modeled by declaratively\nspecifying data flows between components using reactive programming.\n  In this paper, we report on the implementation of our design and our\nexperience with embedding our language features into Scala as a host language.\nWe show how a combination of Scala's advanced type level programming and its\nmacro system can be used to enrich the language with new abstractions. We\ncomment on the challenges we encountered and the solutions we developed for our\ncurrent implementation and outline suggestions for an improved macro system to\nsupport the such use cases of embedding of domain-specific abstractions.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 18:58:10 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Weisenburger", "Pascal", "", "TU Darmstadt, Germany"], ["Salvaneschi", "Guido", "", "TU\n  Darmstadt, Germany"]]}, {"id": "2002.06185", "submitter": "Jo\\~ao Costa Seco", "authors": "Jo\\~ao Costa Seco (Universidade NOVA de Lisboa, Portugal), Paulo\n  Ferreira (OutSystems, Portugal), Hugo Louren\\~A{\\S}o (OutSystems, Portugal),\n  Carla Ferreira (Universidade NOVA de Lisboa, Portugal), Lucio Ferrao\n  (OutSystems, Portugal)", "title": "Robust Contract Evolution in a TypeSafe MicroServices Architecture", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2020, Vol. 4,\n  Issue 3, Article 10", "doi": "10.22152/programming-journal.org/2020/4/10", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microservices architectures allow for short deployment cycles and immediate\neffects but offer no safety mechanisms when service contracts need to be\nchanged. Maintaining the soundness of microservice architectures is an\nerror-prone task that is only accessible to the most disciplined development\nteams. We present a microservice management system that statically verifies\nservice interfaces and supports the seamless evolution of compatible\ninterfaces. We define a compatibility relation that captures real evolution\npatterns and embodies known good practices on the evolution of interfaces.\nNamely, we allow for the addition, removal, and renaming of data fields of a\nproducer module without breaking or needing to upgrade consumer services. The\nevolution of interfaces is supported by runtime generated proxy components that\ndynamically adapt data exchanged between services to match with the statically\nchecked service code.The model was instantiated in a core language whose\nsemantics is defined by a labeled transition system and a type system that\nprevents breaking changes from being deployed. Standard soundness results for\nthe core language entail the existence of adapters, hence the absence of\nadaptation errors and the correctness of the management model. This adaptive\napproach allows for gradual deployment of modules, without halting the whole\nsystem and avoiding losing or misinterpreting data exchanged between system\nnodes. Experimental data shows that an average of 69% of deployments that would\nrequire adaptation and recompilation are safe under our approach.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 18:58:26 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Seco", "Jo\u00e3o Costa", "", "Universidade NOVA de Lisboa, Portugal"], ["Ferreira", "Paulo", "", "OutSystems, Portugal"], ["Louren\u00c3\u00a7o", "Hugo", "", "OutSystems, Portugal"], ["Ferreira", "Carla", "", "Universidade NOVA de Lisboa, Portugal"], ["Ferrao", "Lucio", "", "OutSystems, Portugal"]]}, {"id": "2002.06187", "submitter": "Johannes Mey", "authors": "Johannes Mey (TU Dresden, Germany), Thomas K\\\"uhn (Karlsruhe Institute\n  of Technology, Germany), Ren\\'e Sch\\\"one (TU Dresden, Germany), Uwe\n  A{\\ss}mann (TU Dresden, Germany)", "title": "Reusing Static Analysis across Different Domain-Specific Languages using\n  Reference Attribute Grammars", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2020, Vol. 4,\n  Issue 3, Article 15", "doi": "10.22152/programming-journal.org/2020/4/15", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Domain-specific languages (DSLs) enable domain experts to specify\ntasks and problems themselves, while enabling static analysis to elucidate\nissues in the modelled domain early. Although language workbenches have\nsimplified the design of DSLs and extensions to general purpose languages,\nstatic analyses must still be implemented manually.\n  Inquiry: Moreover, static analyses, e.g., complexity metrics, dependency\nanalysis, and declaration-use analysis, are usually domain-dependent and cannot\nbe easily reused. Therefore, transferring existing static analyses to another\nDSL incurs a huge implementation overhead. However, this overhead is not always\nintrinsically necessary: in many cases, while the concepts of the DSL on which\na static analysis is performed are domain-specific, the underlying algorithm\nemployed in the analysis is actually domain-independent and thus can be reused\nin principle, depending on how it is specified. While current approaches either\nimplement static analyses internally or with an external Visitor, the\nimplementation is tied to the language's grammar and cannot be reused easily.\nThus far, a commonly used approach that achieves reusable static analysis\nrelies on the transformation into an intermediate representation upon which the\nanalysis is performed. This, however, entails a considerable additional\nimplementation effort.\n  Approach: To remedy this, it has been proposed to map the necessary\ndomain-specific concepts to the algorithm's domain-independent data structures,\nyet without a practical implementation and the demonstration of reuse. Thus, to\nmake static analysis reusable again, we employ relational Reference Attribute\nGrammars (RAGs) by creating such a mapping to a domain-independent overlay\nstructure using higher-order attributes.\n  Knowledge: We describe how static analysis can be specified on\nanalysis-specific data structures, how relational RAGs can help with the\nspecification, and how a mapping from the domain-specific language can be\nperformed. Furthermore, we demonstrate how a static analysis for a DSL can be\nexternalized and reused in another general purpose language.\n  Grounding: The approach was evaluated using the RAG system JastAdd. To\nillustrate reusability, we implemented two analyses with two addressed\nlanguages each: a cycle detection analysis used in a small state machine DSL\nand for detecting circular dependencies in Java types and packages, and an\nanalysis of variable shadowing, applied to both Java and the Modelica modelling\nlanguage. Thereby, we demonstrate the reuse of two analysis algorithms in three\ncompletely different domains. Additionally, we use the cycle detection analysis\nto evaluate the efficiency by comparing our external analysis to an internal\nreference implementation analysing all Java programs in the Qualitas Corpus and\nthereby are able to show that an externalized analysis incurs only minimal\noverhead.\n  Importance: We make static analysis reusable, again, showing the practicality\nand efficiency of externalizing static analysis for both DSLs and general\npurpose languages using relational RAGs.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 18:58:55 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Mey", "Johannes", "", "TU Dresden, Germany"], ["K\u00fchn", "Thomas", "", "Karlsruhe Institute\n  of Technology, Germany"], ["Sch\u00f6ne", "Ren\u00e9", "", "TU Dresden, Germany"], ["A\u00dfmann", "Uwe", "", "TU Dresden, Germany"]]}, {"id": "2002.06188", "submitter": "Bob Reynders", "authors": "Bob Reynders (Katholieke Universiteit Leuven, Belgium), Frank Piessens\n  (Katholieke Universiteit Leuven, Belgium), Dominique Devriese (Vrije\n  Universiteit Brussel, Belgium)", "title": "Gavial: Programming the web with multi-tier FRP", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2020, Vol. 4,\n  Issue 3, Article 6", "doi": "10.22152/programming-journal.org/2020/4/6", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing web applications requires dealing with their distributed nature\nand the natural asynchronicity of user input and network communication. For\nfacilitating this, different researchers have explored the combination of a\nmulti-tier programming language and functional reactive programming. However,\nexisting proposals take this approach only part of the way (some parts of the\napplication remain imperative) or remain naive, with no regard for avoiding\nglitches across network communication, network traffic overhead, compatibility\nwith common APIs like XMLHttpRequest etc. In this paper, we present Gavial: the\nfirst mature design and implementation of multi-tier FRP that allows\nconstructing an entire web application as a functionally reactive program. By\napplying a number of new ideas, we demonstrate that multi-tier FRP can in fact\ndeal realistically with important practical aspects of building web\napplications. At the same time, we retain the declarative nature of FRP, where\nbehaviors and events have an intuitive, compositional semantics and a clear\ndependency structure.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 18:59:15 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Reynders", "Bob", "", "Katholieke Universiteit Leuven, Belgium"], ["Piessens", "Frank", "", "Katholieke Universiteit Leuven, Belgium"], ["Devriese", "Dominique", "", "Vrije\n  Universiteit Brussel, Belgium"]]}, {"id": "2002.06190", "submitter": "Tomas Petricek", "authors": "Tomas Petricek (University of Kent, United Kingdom)", "title": "Foundations of a live data exploration environment", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2020, Vol. 4,\n  Issue 3, Article 8", "doi": "10.22152/programming-journal.org/2020/4/8", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: A growing amount of code is written to explore and analyze data,\noften by data analysts who do not have a traditional background in programming,\nfor example by journalists.\n  Inquiry: The way such data anlysts write code is different from the way\nsoftware engineers do so. They use few abstractions, work interactively and\nrely heavily on external libraries. We aim to capture this way of working and\nbuild a programming environment that makes data exploration easier by providing\ninstant live feedback.\n  Approach: We combine theoretical and applied approach. We present the\n\\emph{data exploration calculus}, a formal language that captures the structure\nof code written by data analysts. We then implement a data exploration\nenvironment that evaluates code instantly during editing and shows previews of\nthe results.\n  Knowledge: We formally describe an algorithm for providing instant previews\nfor the data exploration calculus that allows the user to modify code in an\nunrestricted way in a text editor. Supporting interactive editing is tricky as\nany edit can change the structure of code and fully recomputing the output\nwould be too expensive.\n  Grounding: We prove that our algorithm is correct and that it reuses previous\nresults when updating previews after a number of common code edit operations.\nWe also illustrate the practicality of our approach with an empirical\nevaluation and a case study.\n  Importance: As data analysis becomes an ever more important use of\nprogramming, research on programming languages and tools needs to consider new\nkinds of programming workflows appropriate for those domains and conceive new\nkinds of tools that can support them. The present paper is one step in this\nimportant direction.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 18:59:30 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Petricek", "Tomas", "", "University of Kent, United Kingdom"]]}, {"id": "2002.06191", "submitter": "Daniel Speicher", "authors": "Daniel Speicher (Bonn-Aachen International Center for Information\n  Technology, B-IT, Germany)", "title": "Did JHotDraw Respect the Law of Good Style?: A deep dive into the nature\n  of false positives of bad code smells", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2020, Vol. 4,\n  Issue 3, Article 14", "doi": "10.22152/programming-journal.org/2020/4/14", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developers need to make a constant effort to improve the quality of their\ncode if they want to stay productive. Tools that highlight code locations that\ncould benefit from refactoring are thus highly desirable. The most common name\nfor such locations is \"bad code smell\". A number of tools offer such quality\nfeedback and there is a substantial body of related research. However, all\nthese tools, including those based on Machine Learning, still produce false\npositives. Every single false positive shown to the developer places a\ncognitive burden on her and should thus be avoided. The literature discusses\nthe choice of metric thresholds, the general subjectivity of such a judgment\nand the relation to conscious design choices, \"design ideas\". To examine false\npositives and the relation between bad smells and design ideas, we designed and\nconducted an exploratory case study. While previous research presented a broad\noverview, we have chosen a narrow setting to reach for even deeper insights:\nThe framework JHotDraw had been designed so thoughtfully that most smell\nwarnings are expected to be false positives. Nevertheless, the \"Law of Good\nStyle\", better known as the \"Law of Demeter\", is a rather restrictive design\nrule so that we still expected to find some potential bad smells, i.e.\nviolations of this \"Law\". This combination led to 1215 potential smells of\nwhich at most 42 are true positives. We found generic as well as specific\ndesign ideas that were traded for the smell. Our confidence in that decision\nranged from high enough to very high. We were surprised to realize that the\nsmell definition itself required the formulation of constructive design ideas.\nFinally we found some smells to be the result of the limitation of the language\nand one could introduce auxiliary constructive design ideas to compensate for\nthem. The decision whether a potential smell occurrence is actually a true\npositive was made very meticulously. For that purpose we took three qualities\nthat the smell could affect negatively into account and we discussed the result\nof the recommended refactorings. If we were convinced that we had found a false\npositive, we described the relationships with design ideas. The realization\nthat not only general design ideas but also specific design ideas have an\ninfluence on whether a potential smell is a true positive turns the problem of\nfalse positives from a scientific problem (\"What is the true definition of the\nsmell?\") to a engineering problem (\"How can we incorporate design ideas into\nsmell definitions?\"). We recommend to add adaptation points to the smell\ndefinitions. Higher layers may then adapt the smell for specific contexts.\nAfter adaptation the tool may continuously provide distinct and precise quality\nfeedback, reducing the cognitive load for the developer and preventing\nhabituation. Furthermore, the schema for the discussion of potential smells may\nbe used to elaborate more sets of true and false smell occurrences. Finally, it\nfollows that smell detection based on machine learning should also take signs\nof design ideas into account.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 18:59:46 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 19:01:20 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Speicher", "Daniel", "", "Bonn-Aachen International Center for Information\n  Technology, B-IT, Germany"]]}, {"id": "2002.06223", "submitter": "Gene Cooperman", "authors": "Gene Cooperman (Northeastern University, United States), Martin\n  Quinson (\\'Ecole Normale Sup\\'erieure Rennes, France)", "title": "Sthread: In-Vivo Model Checking of Multithreaded Programs", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2020, Vol. 4,\n  Issue 3, Article 13", "doi": "10.22152/programming-journal.org/2020/4/13", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work strives to make formal verification of POSIX multithreaded programs\neasily accessible to general programmers. Sthread operates directly on\nmultithreaded C/C++ programs, without the need for an intermediate formal\nmodel. Sthread is in-vivo in that it provides a drop-in replacement for the\npthread library, and operates directly on the compiled target executable and\napplication libraries. There is no compiler-generated intermediate\nrepresentation. The system calls in the application remain unaltered.\nOptionally, the programmer can add a small amount of additional native C code\nto include assertions based on the user's algorithm, declarations of shared\nmemory regions, and progress/liveness conditions. The work has two important\nmotivations: (i) It can be used to verify correctness of a concurrent algorithm\nbeing implemented with multithreading; and (ii) it can also be used\npedagogically to provide immediate feedback to students learning either to\nemploy POSIX threads system calls or to implement multithreaded algorithms.\n  This work represents the first example of in-vivo model checking operating\ndirectly on the standard multithreaded executable and its libraries, without\nthe aid of a compiler-generated intermediate representation. Sthread leverages\nthe open-source SimGrid libraries, and will eventually be integrated into\nSimGrid. Sthread employs a non-preemptive model in which thread context\nswitches occur only at multithreaded system calls (e.g., mutex, semaphore) or\nbefore accesses to shared memory regions. The emphasis is on finding\n\"algorithmic bugs\" (bugs in an original algorithm, implemented as POSIX threads\nand shared memory regions. This work is in contrast to Context-Bounded Analysis\n(CBA), which assumes a preemptive model for threads, and emphasizes\nimplementation bugs such as buffer overruns and write-after-free for memory\nallocation. In particular, the Sthread in-vivo approach has strong future\npotential for pedagogy, by providing immediate feedback to students who are\nfirst learning the correct use of Pthreads system calls in implementation of\nconcurrent algorithms based on multithreading.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 19:21:42 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Cooperman", "Gene", "", "Northeastern University, United States"], ["Quinson", "Martin", "", "\u00c9cole Normale Sup\u00e9rieure Rennes, France"]]}, {"id": "2002.07020", "submitter": "Vikraman Choudhury", "authors": "Chao-Hong Chen, Vikraman Choudhury, Jacques Carette, Amr Sabry", "title": "Fractional Types: Expressive and Safe Space Management for Ancilla Bits", "comments": "For the agda formalization, see\n  https://github.com/DreamLinuxer/FracAncilla", "journal-ref": null, "doi": "10.1007/978-3-030-52482-1_10", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reversible computing, the management of space is subject to two broad\nclasses of constraints. First, as with general-purpose computation, every\nallocation must be paired with a matching de-allocation. Second, space can only\nbe safely de-allocated if its contents are restored to their initial value from\nallocation time. Generally speaking, the state of the art provides limited\npartial solutions that address the first constraint by imposing a stack\ndiscipline and by leaving the second constraint to programmers' assertions. We\npropose a novel approach based on the idea of fractional types. As a simple\nintuitive example, allocation of a new boolean value initialized to\n$\\texttt{false}$ also creates a value $1/{\\texttt{false}}$ that can be thought\nof as a garbage collection (GC) process specialized to reclaim, and only\nreclaim, storage containing the value $\\texttt{false}$. This GC process is a\nfirst-class entity that can be manipulated, decomposed into smaller processes\nand combined with other GC processes. We formalize this idea in the context of\na reversible language founded on type isomorphisms, prove its fundamental\ncorrectness properties, and illustrate its expressiveness using a wide variety\nof examples. The development is backed by a fully-formalized Agda\nimplementation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 16:06:23 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Chen", "Chao-Hong", ""], ["Choudhury", "Vikraman", ""], ["Carette", "Jacques", ""], ["Sabry", "Amr", ""]]}, {"id": "2002.07262", "submitter": "Norman Danner", "authors": "Norman Danner and Daniel R. Licata", "title": "Denotational semantics as a foundation for cost recurrence extraction\n  for functional languages", "comments": "Revisions, mainly to clarify the goal of this paper and to place it\n  in context of related work. Also many minor revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A standard informal method for analyzing the asymptotic complexity of a\nprogram is to extract a recurrence that describes its cost in terms of the size\nof its input, and then to compute a closed-form upper bound on that recurrence.\nWe give a formal account of that method for functional programs in a\nhigher-order language with let-polymorphism The method consists of two phases.\nIn the first phase, a monadic translation is performed to extract a\ncost-annotated version of the original program. In the second phase, the\nextracted program is interpreted in a model. The key feature of this second\nphase is that different models describe different notions of size. This plays\nout specifically for values of inductive type, where different notions of size\nmay be appropriate depending on the analysis, and for polymorphic functions,\nwhere we show that the notion of size for a polymorphic function can be\ndescribed formally as the data that is common to the notions of size of its\ninstances. We give several examples of different models that formally justify\nvarious informal cost analyses to show the applicability of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 21:36:38 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 14:18:57 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Danner", "Norman", ""], ["Licata", "Daniel R.", ""]]}, {"id": "2002.07472", "submitter": "Mark van der Loo", "authors": "Mark P.J. van der Loo", "title": "A method for deriving information from running R code", "comments": "11 pages, 1 figure. Accepted for publication by the R Journal\n  (2020-02-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often useful to tap information from a running R script. Obvious use\ncases include monitoring the consumption of resources (time, memory) and\nlogging. Perhaps less obvious cases include tracking changes in R objects\norcollecting output of unit tests. In this paper we demonstrate an approach\nthat abstracts collection and processing of such secondary information from the\nrunning R script. Our approach is based on a combination of three elements. The\nfirst element is to build a customized way to evaluate code. The second is\nlabeled \\emph{local masking} and it involves temporarily masking auser-facing\nfunction so an alternative version of it is called. The third element we label\n\\emph{local side effect}. This refers to the fact that the masking function\nexports information to the secondary information flow without altering a global\nstate. The result is a method for building systems in pure R that lets users\ncreate and control secondary flows of information with minimal impact on their\nworkflow, and no global side effects.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 10:23:59 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 15:32:24 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["van der Loo", "Mark P. J.", ""]]}, {"id": "2002.07770", "submitter": "John Toman", "authors": "John Toman, Ren Siqi, Kohei Suenaga, Atsushi Igarashi, Naoki Kobayashi", "title": "ConSORT: Context- and Flow-Sensitive Ownership Refinement Types for\n  Imperative Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present ConSORT, a type system for safety verification in the presence of\nmutability and aliasing. Mutability requires strong updates to model changing\ninvariants during program execution, but aliasing between pointers makes it\ndifficult to determine which invariants must be updated in response to\nmutation. Our type system addresses this difficulty with a novel combination of\nrefinement types and fractional ownership types. Fractional ownership types\nprovide flow-sensitive and precise aliasing information for reference\nvariables. ConSORT interprets this ownership information to soundly handle\nstrong updates of potentially aliased references. We have proved ConSORT sound\nand implemented a prototype, fully automated inference tool. We evaluated our\ntool and found it verifies non-trivial programs including data structure\nimplementations.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 18:01:25 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Toman", "John", ""], ["Siqi", "Ren", ""], ["Suenaga", "Kohei", ""], ["Igarashi", "Atsushi", ""], ["Kobayashi", "Naoki", ""]]}, {"id": "2002.07951", "submitter": "Remy Wang", "authors": "Yisu Remy Wang, Shana Hutchison, Jonathan Leang, Bill Howe, Dan Suciu", "title": "SPORES: Sum-Product Optimization via Relational Equality Saturation for\n  Large Scale Linear Algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms are commonly specified in linear algebra (LA). LA\nexpressions can be rewritten into more efficient forms, by taking advantage of\ninput properties such as sparsity, as well as program properties such as common\nsubexpressions and fusible operators. The complex interaction among these\nproperties' impact on the execution cost poses a challenge to optimizing\ncompilers. Existing compilers resort to intricate heuristics that complicate\nthe codebase and add maintenance cost but fail to search through the large\nspace of equivalent LA expressions to find the cheapest one. We introduce a\ngeneral optimization technique for LA expressions, by converting the LA\nexpressions into Relational Algebra (RA) expressions, optimizing the latter,\nthen converting the result back to (optimized) LA expressions. One major\nadvantage of this method is that it is complete, meaning that any equivalent LA\nexpression can be found using the equivalence rules in RA. The challenge is the\nmajor size of the search space, and we address this by adopting and extending a\ntechnique used in compilers, called equality saturation. We integrate the\noptimizer into SystemML and validate it empirically across a spectrum of\nmachine learning tasks; we show that we can derive all existing hand-coded\noptimizations in SystemML, and perform new optimizations that lead to speedups\nfrom 1.2X to 5X.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 01:15:17 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 19:40:44 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Wang", "Yisu Remy", ""], ["Hutchison", "Shana", ""], ["Leang", "Jonathan", ""], ["Howe", "Bill", ""], ["Suciu", "Dan", ""]]}, {"id": "2002.07970", "submitter": "Patrick Diehl", "authors": "Tianyi Zhang, Shahrzad Shirzad, Bibek Wagle, Adrian S. Lemoine,\n  Patrick Diehl, and Hartmut Kaiser", "title": "Supporting OpenMP 5.0 Tasks in hpxMP -- A study of an OpenMP\n  implementation within Task Based Runtime Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  OpenMP has been the de facto standard for single node parallelism for more\nthan a decade. Recently, asynchronous many-task runtime (AMT) systems have\nincreased in popularity as a new programming paradigm for high performance\ncomputing applications. One of the major challenges of this new paradigm is the\nincompatibility of the OpenMP thread model and other AMTs. Highly optimized\nOpenMP-based libraries do not perform well when coupled with AMTs because the\nthreading of both libraries will compete for resources. This paper is a\nfollow-up paper on the fundamental implementation of hpxMP, an implementation\nof the OpenMP standard which utilizes the C++ standard library for Parallelism\nand Concurrency (HPX) to schedule and manage tasks. In this paper, we present\nthe implementation of task features, e.g. taskgroup, task depend, and\ntask_reduction, of the OpenMP 5.0 standard and optimization of the #pragma omp\nparallel for pragma. We use the daxpy benchmark, the Barcelona OpenMP Tasks\nSuite, Parallel research kernels, and OpenBLAS benchmarks to compare the\ndifferent OpenMp implementations: hpxMP, llvm-OpenMP, and GOMP.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 02:50:04 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Zhang", "Tianyi", ""], ["Shirzad", "Shahrzad", ""], ["Wagle", "Bibek", ""], ["Lemoine", "Adrian S.", ""], ["Diehl", "Patrick", ""], ["Kaiser", "Hartmut", ""]]}, {"id": "2002.08155", "submitter": "Zhangyin Feng", "authors": "Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming\n  Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, Ming Zhou", "title": "CodeBERT: A Pre-Trained Model for Programming and Natural Languages", "comments": "Accepted to Findings of EMNLP 2020. 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CodeBERT, a bimodal pre-trained model for programming language\n(PL) and nat-ural language (NL). CodeBERT learns general-purpose\nrepresentations that support downstream NL-PL applications such as natural\nlanguage codesearch, code documentation generation, etc. We develop CodeBERT\nwith Transformer-based neural architecture, and train it with a hybrid\nobjective function that incorporates the pre-training task of replaced token\ndetection, which is to detect plausible alternatives sampled from generators.\nThis enables us to utilize both bimodal data of NL-PL pairs and unimodal data,\nwhere the former provides input tokens for model training while the latter\nhelps to learn better generators. We evaluate CodeBERT on two NL-PL\napplications by fine-tuning model parameters. Results show that CodeBERT\nachieves state-of-the-art performance on both natural language code search and\ncode documentation generation tasks. Furthermore, to investigate what type of\nknowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and\nevaluate in a zero-shot setting where parameters of pre-trained models are\nfixed. Results show that CodeBERT performs better than previous pre-trained\nmodels on NL-PL probing.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 13:09:07 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 08:51:49 GMT"}, {"version": "v3", "created": "Mon, 27 Apr 2020 04:35:54 GMT"}, {"version": "v4", "created": "Fri, 18 Sep 2020 15:38:12 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Feng", "Zhangyin", ""], ["Guo", "Daya", ""], ["Tang", "Duyu", ""], ["Duan", "Nan", ""], ["Feng", "Xiaocheng", ""], ["Gong", "Ming", ""], ["Shou", "Linjun", ""], ["Qin", "Bing", ""], ["Liu", "Ting", ""], ["Jiang", "Daxin", ""], ["Zhou", "Ming", ""]]}, {"id": "2002.08241", "submitter": "Pui Yiu Carol Mak", "authors": "Carol Mak, Luke Ong", "title": "A Differential-form Pullback Programming Language for Higher-order\n  Reverse-mode Automatic Differentiation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Building on the observation that reverse-mode automatic differentiation (AD)\n-- a generalisation of backpropagation -- can naturally be expressed as\npullbacks of differential 1-forms, we design a simple higher-order programming\nlanguage with a first-class differential operator, and present a reduction\nstrategy which exactly simulates reverse-mode AD. We justify our reduction\nstrategy by interpreting our language in any differential $\\lambda$-category\nthat satisfies the Hahn-Banach Separation Theorem, and show that the reduction\nstrategy precisely captures reverse-mode AD in a truly higher-order setting.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 15:38:03 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Mak", "Carol", ""], ["Ong", "Luke", ""]]}, {"id": "2002.08334", "submitter": "Sophia Drossopoulou", "authors": "Sophia Drossopoulou, James Noble, Julian Mackay, Susan Eisenbach", "title": "Holistic Specifications for Robust Programs", "comments": "44 pages, 1 Table, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional specifications describe what program components do: the sufficient\nconditions to invoke a component's operations. They allow us to reason about\nthe use of components in the closed world setting, where the component\ninteracts with known client code, and where the client code must establish the\nappropriate pre-conditions before calling into the component.\n  Sufficient conditions are not enough to reason about the use of components in\nthe open world setting, where the component interacts with external code,\npossibly of unknown provenance, and where the component itself may evolve over\ntime. In this open world setting, we must also consider the necessary}\nconditions, i.e, what are the conditions without which an effect will not\nhappen.\n  In this paper we propose the language Chainmail for writing holistic\nspecifications that focus on necessary conditions (as well as sufficient\nconditions). We give a formal semantics for \\Chainmail. The core of Chainmail\nhas been mechanised in the Coq proof assistant.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 18:19:32 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Drossopoulou", "Sophia", ""], ["Noble", "James", ""], ["Mackay", "Julian", ""], ["Eisenbach", "Susan", ""]]}, {"id": "2002.08392", "submitter": "Giulio Guerrieri", "authors": "Ugo Dal Lago (1) and Giulio Guerrieri (2) and Willem Heijltjes (2)\n  ((1) Dipartimento di Informatica - Scienza e Ingegneria, Universit\\`a di\n  Bologna, Bologna, Italy and (2) Department of Computer Science University of\n  Bath, Bath, UK)", "title": "Decomposing Probabilistic Lambda-calculi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A notion of probabilistic lambda-calculus usually comes with a prescribed\nreduction strategy, typically call-by-name or call-by-value, as the calculus is\nnon-confluent and these strategies yield different results. This is a break\nwith one of the main advantages of lambda-calculus: confluence, which means\nresults are independent from the choice of strategy. We present a probabilistic\nlambda-calculus where the probabilistic operator is decomposed into two\nsyntactic constructs: a generator, which represents a probabilistic event; and\na consumer, which acts on the term depending on a given event. The resulting\ncalculus, the Probabilistic Event Lambda-Calculus, is confluent, and interprets\nthe call-by-name and call-by-value strategies through different interpretations\nof the probabilistic operator into our generator and consumer constructs. We\npresent two notions of reduction, one via fine-grained local rewrite steps, and\none by generation and consumption of probabilistic events. Simple types for the\ncalculus are essentially standard, and they convey strong normalization. We\ndemonstrate how we can encode call-by-name and call-by-value probabilistic\nevaluation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 19:09:49 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Lago", "Ugo Dal", ""], ["Guerrieri", "Giulio", ""], ["Heijltjes", "Willem", ""]]}, {"id": "2002.08489", "submitter": "Francesco Gavazzo", "authors": "Gilles Barthe, Rapha\\\"elle Crubill\\'e, Ugo Dal Lago, Francesco Gavazzo", "title": "On the Versatility of Open Logical Relations: Continuity, Automatic\n  Differentiation, and a Containment Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Logical relations are one of the most powerful techniques in the theory of\nprogramming languages, and have been used extensively for proving properties of\na variety of higher-order calculi. However, there are properties that cannot be\nimmediately proved by means of logical relations, for instance program\ncontinuity and differentiability in higher-order languages extended with\nreal-valued functions. Informally, the problem stems from the fact that these\nproperties are naturally expressed on terms of non-ground type (or,\nequivalently, on open terms of base type), and there is no apparent good\ndefinition for a base case (i.e. for closed terms of ground types). To overcome\nthis issue, we study a generalization of the concept of a logical relation,\ncalled \\emph{open logical relation}, and prove that it can be fruitfully\napplied in several contexts in which the property of interest is about\nexpressions of first-order type. Our setting is a simply-typed\n$\\lambda$-calculus enriched with real numbers and real-valued first-order\nfunctions from a given set, such as the one of continuous or differentiable\nfunctions. We first prove a containment theorem stating that for any such a\ncollection of functions including projection functions and closed under\nfunction composition, any well-typed term of first-order type denotes a\nfunction belonging to that collection. Then, we show by way of open logical\nrelations the correctness of the core of a recently published algorithm for\nforward automatic differentiation. Finally, we define a refinement-based type\nsystem for local continuity in an extension of our calculus with conditionals,\nand prove the soundness of the type system using open logical relations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 22:55:04 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Barthe", "Gilles", ""], ["Crubill\u00e9", "Rapha\u00eblle", ""], ["Lago", "Ugo Dal", ""], ["Gavazzo", "Francesco", ""]]}, {"id": "2002.08523", "submitter": "Brandon Bohrer", "authors": "Brandon Bohrer, Andr\\'e Platzer", "title": "Constructive Game Logic", "comments": "74 pages, extended preprint for ESOP", "journal-ref": null, "doi": "10.1007/978-3-030-44914-8_4", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game Logic is an excellent setting to study proofs-about-programs via the\ninterpretation of those proofs as programs, because constructive proofs for\ngames correspond to effective winning strategies to follow in response to the\nopponent's actions. We thus develop Constructive Game Logic which extends\nParikh's Game Logic (GL) with constructivity and with first-order programs a la\nPratt's first-order dynamic logic (DL). Our major contributions include:\n  1) a novel realizability semantics capturing the adversarial dynamics of\ngames, 2) a natural deduction calculus and operational semantics describing the\ncomputational meaning of strategies via proof-terms, and 3) theoretical results\nincluding soundness of the proof calculus w.r.t. realizability semantics,\nprogress and preservation of the operational semantics of proofs, and Existence\nProperties on support of the extraction of computational artifacts from game\nproofs.\n  Together, these results provide the most general account of a Curry-Howard\ninterpretation for any program logic to date, and the first at all for Game\nLogic.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 01:44:24 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 12:44:05 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Bohrer", "Brandon", ""], ["Platzer", "Andr\u00e9", ""]]}, {"id": "2002.08738", "submitter": "Francesco Dagnino", "authors": "Francesco Dagnino, Viviana Bono, Elena Zucca, Mariangiola\n  Dezani-Ciancaglini", "title": "Soundness conditions for big-step semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general proof technique to show that a predicate is sound, that\nis, prevents stuck computation, with respect to a big-step semantics. This\nresult may look surprising, since in big-step semantics there is no difference\nbetween non-terminating and stuck computations, hence soundness cannot even be\nexpressed. The key idea is to define constructions yielding an extended version\nof a given arbitrary big-step semantics, where the difference is made explicit.\nThe extended semantics are exploited in the meta-theory, notably they are\nnecessary to show that the proof technique works. However, they remain\ntransparent when using the proof technique, since it consists in checking three\nconditions on the original rules only, as we illustrate by several examples.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 14:06:54 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 10:50:06 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Dagnino", "Francesco", ""], ["Bono", "Viviana", ""], ["Zucca", "Elena", ""], ["Dezani-Ciancaglini", "Mariangiola", ""]]}, {"id": "2002.08874", "submitter": "Robin Piedeleu", "authors": "Filippo Bonchi, Robin Piedeleu, Pawel Sobocinski, Fabio Zanasi", "title": "Contextual Equivalence for Signal Flow Graphs", "comments": "Accepted for publication in the proceedings of the 23rd International\n  Conference on Foundations of Software Science and Computation Structures\n  (FoSSaCS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the signal flow calculus---a compositional account of the classical\nsignal flow graph model of computation---to encompass affine behaviour, and\nfurnish it with a novel operational semantics. The increased expressive power\nallows us to define a canonical notion of contextual equivalence, which we show\nto coincide with denotational equality. Finally, we characterise the realisable\nfragment of the calculus: those terms that express the computations of (affine)\nsignal flow graphs.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 17:20:14 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Bonchi", "Filippo", ""], ["Piedeleu", "Robin", ""], ["Sobocinski", "Pawel", ""], ["Zanasi", "Fabio", ""]]}, {"id": "2002.09002", "submitter": "Yusuke Matsushita", "authors": "Yusuke Matsushita and Takeshi Tsukada and Naoki Kobayashi", "title": "RustHorn: CHC-based Verification for Rust Programs (full version)", "comments": "Full version of the same-titled paper in ESOP2020", "journal-ref": null, "doi": "10.1007/978-3-030-44914-8_18", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reduction to the satisfiability problem for constrained Horn clauses (CHCs)\nis a widely studied approach to automated program verification. The current\nCHC-based methods for pointer-manipulating programs, however, are not very\nscalable. This paper proposes a novel translation of pointer-manipulating Rust\nprograms into CHCs, which clears away pointers and memories by leveraging\nownership. We formalize the translation for a simplified core of Rust and prove\nits correctness. We have implemented a prototype verifier for a subset of Rust\nand confirmed the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 20:28:08 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 06:31:16 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Matsushita", "Yusuke", ""], ["Tsukada", "Takeshi", ""], ["Kobayashi", "Naoki", ""]]}, {"id": "2002.09030", "submitter": "Augustus Odena", "authors": "Augustus Odena, Charles Sutton", "title": "Learning to Represent Programs with Property Signatures", "comments": "ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of property signatures, a representation for programs\nand program specifications meant for consumption by machine learning\nalgorithms. Given a function with input type $\\tau_{in}$ and output type\n$\\tau_{out}$, a property is a function of type: $(\\tau_{in}, \\tau_{out})\n\\rightarrow \\texttt{Bool}$ that (informally) describes some simple property of\nthe function under consideration. For instance, if $\\tau_{in}$ and $\\tau_{out}$\nare both lists of the same type, one property might ask `is the input list the\nsame length as the output list?'. If we have a list of such properties, we can\nevaluate them all for our function to get a list of outputs that we will call\nthe property signature. Crucially, we can `guess' the property signature for a\nfunction given only a set of input/output pairs meant to specify that function.\nWe discuss several potential applications of property signatures and show\nexperimentally that they can be used to improve over a baseline synthesizer so\nthat it emits twice as many programs in less than one-tenth of the time.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 01:50:11 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Odena", "Augustus", ""], ["Sutton", "Charles", ""]]}, {"id": "2002.09115", "submitter": "Yu-Yang Lin", "authors": "Yu-Yang Lin, Nikos Tzevelekos", "title": "Symbolic Execution Game Semantics", "comments": "41 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for symbolically executing and model checking\nhigher-order programs with external (open) methods. We focus on the\nclient-library paradigm and in particular we aim to check libraries with\nrespect to any definable client. We combine traditional symbolic execution\ntechniques with operational game semantics to build a symbolic execution\nsemantics that captures arbitrary external behaviour. We prove the symbolic\nsemantics to be sound and complete. This yields a bounded technique by imposing\nbounds on the depth of recursion and callbacks. We provide an implementation of\nour technique in the K framework and showcase its performance on a custom\nbenchmark based on higher-order coding errors such as reentrancy bugs.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 03:53:15 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Lin", "Yu-Yang", ""], ["Tzevelekos", "Nikos", ""]]}, {"id": "2002.09519", "submitter": "David Kahn", "authors": "David M Kahn and Jan Hoffmann", "title": "Exponential Automatic Amortized Resource Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic amortized resource analysis (AARA) is a type-based technique for\ninferring concrete (non-asymptotic) bounds on a program's resource usage.\nExisting work on AARA has focused on bounds that are polynomial in the sizes of\nthe inputs. This paper presents and extension of AARA to exponential bounds\nthat preserves the benefits of the technique, such as compositionality and\nefficient type inference based on linear constraint solving. A key idea is the\nuse of the Stirling numbers of the second kind as the basis of potential\nfunctions, which play the same role as the binomial coefficients in polynomial\nAARA. To formalize the similarities with the existing analyses, the paper\npresents a general methodology for AARA that is instantiated to the polynomial\nversion, the exponential version, and a combined system with potential\nfunctions that are formed by products of Stirling numbers and binomial\ncoefficients. The soundness of exponential AARA is proved with respect to an\noperational cost semantics and the analysis of representative example programs\ndemonstrates the effectiveness of the new analysis.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 19:29:32 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 19:40:52 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Kahn", "David M", ""], ["Hoffmann", "Jan", ""]]}, {"id": "2002.09857", "submitter": "Divyesh Unadkat", "authors": "Supratik Chakraborty, Ashutosh Gupta, Divyesh Unadkat", "title": "Verifying Array Manipulating Programs with Full-Program Induction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a full-program induction technique for proving (a sub-class of)\nquantified as well as quantifier-free properties of programs manipulating\narrays of parametric size N. Instead of inducting over individual loops, our\ntechnique inducts over the entire program (possibly containing multiple loops)\ndirectly via the program parameter N. Significantly, this does not require\ngeneration or use of loop-specific invariants. We have developed a prototype\ntool Vajra to assess the efficacy of our technique. We demonstrate the\nperformance of Vajra vis-a-vis several state-of-the-art tools on a set of array\nmanipulating benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 08:12:44 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Chakraborty", "Supratik", ""], ["Gupta", "Ashutosh", ""], ["Unadkat", "Divyesh", ""]]}, {"id": "2002.10213", "submitter": "Jian Gu", "authors": "Javier Cabrera Arteaga, Shrinish Donde, Jian Gu, Orestis Floros, Lucas\n  Satabin, Benoit Baudry and Martin Monperrus", "title": "Superoptimization of WebAssembly Bytecode", "comments": "4 pages, 3 figures. Proceedings of MoreVMs: Workshop on Modern\n  Language Runtimes, Ecosystems, and VMs (2020)", "journal-ref": "Proceedings of MoreVMs: Workshop on Modern Language Runtimes,\n  Ecosystems, and VMs (2020)", "doi": "10.1145/3397537.3397567", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the fast adoption of WebAssembly, we propose the first\nfunctional pipeline to support the superoptimization of WebAssembly bytecode.\nOur pipeline works over LLVM and Souper. We evaluate our superoptimization\npipeline with 12 programs from the Rosetta code project. Our pipeline improves\nthe code section size of 8 out of 12 programs. We discuss the challenges faced\nin superoptimization of WebAssembly with two case studies.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 12:58:25 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Arteaga", "Javier Cabrera", ""], ["Donde", "Shrinish", ""], ["Gu", "Jian", ""], ["Floros", "Orestis", ""], ["Satabin", "Lucas", ""], ["Baudry", "Benoit", ""], ["Monperrus", "Martin", ""]]}, {"id": "2002.10803", "submitter": "Luigi Liquori", "authors": "Luigi Liquori and Claude Stolze", "title": "A Type Checker for a Logical Framework with Union and Intersection Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the syntax, semantics, and typing rules of Bull, a prototype\ntheorem prover based on the Delta-Framework, i.e. a fully-typed lambda-calculus\ndecorated with union and intersection types, as described in previous papers by\nthe authors. Bull also implements a subtyping algorithm for the Type Theory Xi\nof Barbanera-Dezani-de'Liguoro. Bull has a command-line interface where the\nuser can declare axioms, terms, and perform computations and some basic\nterminal-style features like error pretty-printing, subexpressions\nhighlighting, and file loading. Moreover, it can typecheck a proof or normalize\nit. These terms can be incomplete, therefore the typechecking algorithm uses\nunification to try to construct the missing subterms. Bull uses the syntax of\nBerardi's Pure Type Systems to improve the compactness and the modularity of\nthe kernel. Abstract and concrete syntax are mostly aligned and similar to the\nconcrete syntax of Coq. Bull uses a higher-order unification algorithm for\nterms, while typechecking and partial type inference are done by a\nbidirectional refinement algorithm, similar to the one found in Matita and\nBeluga. The refinement can be split into two parts: the essence refinement and\nthe typing refinement. Binders are implemented using commonly-used de Bruijn\nindices. We have defined a concrete language syntax that will allow the user to\nwrite Delta-terms. We have defined the reduction rules and an evaluator. We\nhave implemented from scratch a refiner which does partial typechecking and\ntype reconstruction. We have experimented Bull with classical examples of the\nintersection and union literature, such as the ones formalized by Pfenning with\nhis Refinement Types in LF. We hope that this research vein could be useful to\nexperiment, in a proof theoretical setting, forms of polymorphism alternatives\nto Girard's parametric one.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 11:46:26 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Liquori", "Luigi", ""], ["Stolze", "Claude", ""]]}, {"id": "2002.10900", "submitter": "Farzane Karami", "authors": "Farzane Karami, Olaf Owe, Gerardo Schneider", "title": "Security Wrappers for Information-Flow Control in Active Object\n  Languages with Futures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a run-time mechanism for preventing leakage of secure\ninformation in distributed systems. We consider a general concurrency language\nmodel, where concurrent objects interact by asynchronous method calls and\nfutures. The aim is to prevent leakage of confidential information to low-level\nviewers. The approach is based on the notion of a security wrapper, which\nencloses an object or a component and controls its interactions with the\nenvironment. A wrapper is a mechanism added by the run-time system to provide\nprotection of an insecure component according to some security policies. The\nsecurity policies of a wrapper are formalized based on a notion of security\nlevels. At run-time, future components will be wrapped upon need, while only\nobjects of unsafe classes will be wrapped, using static checking to limit the\nnumber of unsafe classes and thereby reducing run-time overhead. We define an\noperational semantics and prove that non-interference is satisfied. A service\nprovider may use wrappers to protect its services in an insecure environment,\nand vice-versa: a system platform may use wrappers to protect itself from\ninsecure service providers.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 14:40:01 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Karami", "Farzane", ""], ["Owe", "Olaf", ""], ["Schneider", "Gerardo", ""]]}, {"id": "2002.11054", "submitter": "Uday Bondhugula", "authors": "Chris Lattner, Mehdi Amini, Uday Bondhugula, Albert Cohen, Andy Davis,\n  Jacques Pienaar, River Riddle, Tatiana Shpeisman, Nicolas Vasilache,\n  Oleksandr Zinenko", "title": "MLIR: A Compiler Infrastructure for the End of Moore's Law", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work presents MLIR, a novel approach to building reusable and extensible\ncompiler infrastructure. MLIR aims to address software fragmentation, improve\ncompilation for heterogeneous hardware, significantly reduce the cost of\nbuilding domain specific compilers, and aid in connecting existing compilers\ntogether. MLIR facilitates the design and implementation of code generators,\ntranslators and optimizers at different levels of abstraction and also across\napplication domains, hardware targets and execution environments. The\ncontribution of this work includes (1) discussion of MLIR as a research\nartifact, built for extension and evolution, and identifying the challenges and\nopportunities posed by this novel design point in design, semantics,\noptimization specification, system, and engineering. (2) evaluation of MLIR as\na generalized infrastructure that reduces the cost of building\ncompilers-describing diverse use-cases to show research and educational\nopportunities for future programming languages, compilers, execution\nenvironments, and computer architecture. The paper also presents the rationale\nfor MLIR, its original design principles, structures and semantics.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 17:24:50 GMT"}, {"version": "v2", "created": "Sun, 1 Mar 2020 00:38:46 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Lattner", "Chris", ""], ["Amini", "Mehdi", ""], ["Bondhugula", "Uday", ""], ["Cohen", "Albert", ""], ["Davis", "Andy", ""], ["Pienaar", "Jacques", ""], ["Riddle", "River", ""], ["Shpeisman", "Tatiana", ""], ["Vasilache", "Nicolas", ""], ["Zinenko", "Oleksandr", ""]]}, {"id": "2002.11562", "submitter": "Maximiliano Cristia", "authors": "Maximiliano Cristi\\'a and Andrea Fois and Gianfranco Rossi", "title": "Declarative Programming with Intensional Sets in Java Using JSetL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Intensional sets are sets given by a property rather than by enumerating\ntheir elements. In previous work, we have proposed a decision procedure for a\nfirst-order logic language which provides Restricted Intensional Sets (RIS),\ni.e., a sub-class of intensional sets that are guaranteed to denote\nfinite---though unbounded---sets. In this paper we show how RIS can be\nexploited as a convenient programming tool also in a conventional setting,\nnamely, the imperative O-O language Java. We do this by considering a Java\nlibrary, called JSetL, that integrates the notions of logical variable, (set)\nunification and constraints that are typical of constraint logic programming\nlanguages into the Java language. We show how JSetL is naturally extended to\naccommodate for RIS and RIS constraints, and how this extension can be\nexploited, on the one hand, to support a more declarative style of programming\nand, on the other hand, to effectively enhance the expressive power of the\nconstraint language provided by the library.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 15:28:47 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 19:46:01 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Cristi\u00e1", "Maximiliano", ""], ["Fois", "Andrea", ""], ["Rossi", "Gianfranco", ""]]}, {"id": "2002.12793", "submitter": "Hans H\\\"uttel", "authors": "Mario Bravetti, Adrian Francalanza, Iaroslav Golovanov, Hans H\\\"uttel,\n  Mathias Steen Jakobsen, Mikkel Klinke Kettunen, and Ant\\'onio Ravara", "title": "Behavioural Types for Memory and Method Safety in a Core Object-Oriented\n  Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a type-based analysis ensuring memory safety and object protocol\ncompletion in the Java-like language Mungo. Objects are annotated with usages,\ntypestates-like specifications of the admissible sequences of method calls. The\nanalysis entwines usage checking, controlling the order in which methods are\ncalled, with a static check determining whether references may contain null\nvalues. The analysis prevents null pointer dereferencing and memory leaks and\nensures that the intended usage protocol of every object is respected and\ncompleted. The type system has been implemented in the form of a type checker.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 15:21:55 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Bravetti", "Mario", ""], ["Francalanza", "Adrian", ""], ["Golovanov", "Iaroslav", ""], ["H\u00fcttel", "Hans", ""], ["Jakobsen", "Mathias Steen", ""], ["Kettunen", "Mikkel Klinke", ""], ["Ravara", "Ant\u00f3nio", ""]]}]