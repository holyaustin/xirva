[{"id": "1601.00073", "submitter": "Oliver Kennedy", "authors": "Arindam Nandi, Ying Yang, Oliver Kennedy, Boris Glavic, Ronny Fehling,\n  Zhen Hua Liu, Dieter Gawlick", "title": "Mimir: Bringing CTables into Practice", "comments": "Under submission; The first two authors should be considered a joint\n  first-author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present state of the art in analytics requires high upfront investment of\nhuman effort and computational resources to curate datasets, even before the\nfirst query is posed. So-called pay-as-you-go data curation techniques allow\nthese high costs to be spread out, first by enabling queries over uncertain and\nincomplete data, and then by assessing the quality of the query results. We\ndescribe the design of a system, called Mimir, around a recently introduced\nclass of probabilistic pay-as-you-go data cleaning operators called Lenses.\nMimir wraps around any deterministic database engine using JDBC, extending it\nwith support for probabilistic query processing. Queries processed through\nMimir produce uncertainty-annotated result cursors that allow client\napplications to quickly assess result quality and provenance. We also present a\nGUI that provides analysts with an interactive tool for exploring the\nuncertainty exposed by the system. Finally, we present optimizations that make\nLenses scalable, and validate this claim through experimental evidence.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2016 11:21:33 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Nandi", "Arindam", ""], ["Yang", "Ying", ""], ["Kennedy", "Oliver", ""], ["Glavic", "Boris", ""], ["Fehling", "Ronny", ""], ["Liu", "Zhen Hua", ""], ["Gawlick", "Dieter", ""]]}, {"id": "1601.00713", "submitter": "Michael Bukatin", "authors": "Michael Bukatin and Steve Matthews", "title": "Almost Continuous Transformations of Software and Higher-order Dataflow\n  Programming", "comments": "12 pages, July 9, 2015 preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two classes of stream-based computations which admit taking\nlinear combinations of execution runs: probabilistic sampling and generalized\nanimation. The dataflow architecture is a natural platform for programming with\nstreams. The presence of linear combinations allows us to introduce the notion\nof almost continuous transformation of dataflow graphs. We introduce a new\napproach to higher-order dataflow programming: a dynamic dataflow program is a\nstream of dataflow graphs evolving by almost continuous transformations. A\ndynamic dataflow program would typically run while it evolves. We introduce\nFluid, an experimental open source system for programming with dataflow graphs\nand almost continuous transformations.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 01:44:35 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Bukatin", "Michael", ""], ["Matthews", "Steve", ""]]}, {"id": "1601.01050", "submitter": "Michael Bukatin", "authors": "Michael Bukatin and Steve Matthews", "title": "Dataflow Graphs as Matrices and Programming with Higher-order Matrix\n  Elements", "comments": "8 pages, August 27, 2015 preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider dataflow architecture for two classes of computations which admit\ntaking linear combinations of execution runs: probabilistic sampling and\ngeneralized animation. We improve the earlier technique of almost continuous\nprogram transformations by adopting a discipline of bipartite graphs linking\nnodes obtained via general transformations and nodes obtained via linear\ntransformations which makes it possible to develop and evolve dataflow programs\nover these classes of computations by continuous program transformations. The\nuse of bipartite graphs allows us to represent the dataflow programs from this\nclass as matrices of real numbers and evolve and modify programs by continuous\nchange of these numbers.\n  We develop a formalism for higher-order dataflow programming for this class\nof dataflow graphs based on the higher-order matrix elements. Some of our\nsoftware experiments are briefly discussed.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 02:07:18 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Bukatin", "Michael", ""], ["Matthews", "Steve", ""]]}, {"id": "1601.01233", "submitter": "Beniamino Accattoli", "authors": "Beniamino Accattoli (INRIA), Ugo Dal Lago (University of Bologna &\n  INRIA)", "title": "(Leftmost-Outermost) Beta Reduction is Invariant, Indeed", "comments": "arXiv admin note: substantial text overlap with arXiv:1405.3311", "journal-ref": "Logical Methods in Computer Science, Volume 12, Issue 1 (March 9,\n  2016) lmcs:1627", "doi": "10.2168/LMCS-12(1:4)2016", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slot and van Emde Boas' weak invariance thesis states that reasonable\nmachines can simulate each other within a polynomially overhead in time. Is\nlambda-calculus a reasonable machine? Is there a way to measure the\ncomputational complexity of a lambda-term? This paper presents the first\ncomplete positive answer to this long-standing problem. Moreover, our answer is\ncompletely machine-independent and based over a standard notion in the theory\nof lambda-calculus: the length of a leftmost-outermost derivation to normal\nform is an invariant cost model. Such a theorem cannot be proved by directly\nrelating lambda-calculus with Turing machines or random access machines,\nbecause of the size explosion problem: there are terms that in a linear number\nof steps produce an exponentially long output. The first step towards the\nsolution is to shift to a notion of evaluation for which the length and the\nsize of the output are linearly related. This is done by adopting the linear\nsubstitution calculus (LSC), a calculus of explicit substitutions modeled after\nlinear logic proof nets and admitting a decomposition of leftmost-outermost\nderivations with the desired property. Thus, the LSC is invariant with respect\nto, say, random access machines. The second step is to show that LSC is\ninvariant with respect to the lambda-calculus. The size explosion problem seems\nto imply that this is not possible: having the same notions of normal form,\nevaluation in the LSC is exponentially longer than in the lambda-calculus. We\nsolve such an impasse by introducing a new form of shared normal form and\nshared reduction, deemed useful. Useful evaluation avoids those steps that only\nunshare the output without contributing to beta-redexes, i.e. the steps that\ncause the blow-up in size. The main technical contribution of the paper is\nindeed the definition of useful reductions and the thorough analysis of their\nproperties.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 16:31:48 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 13:45:53 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Accattoli", "Beniamino", "", "INRIA"], ["Lago", "Ugo Dal", "", "University of Bologna &\n  INRIA"]]}, {"id": "1601.01586", "submitter": "Ranald Clouston", "authors": "Ale\\v{s} Bizjak, Hans Bugge Grathwohl, Ranald Clouston, Rasmus E.\n  M{\\o}gelberg, Lars Birkedal", "title": "Guarded Dependent Type Theory with Coinductive Types", "comments": "This is the technical report version of a paper to appear in the\n  proceedings of FoSSaCS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present guarded dependent type theory, gDTT, an extensional dependent type\ntheory with a `later' modality and clock quantifiers for programming and\nproving with guarded recursive and coinductive types. The later modality is\nused to ensure the productivity of recursive definitions in a modular, type\nbased, way. Clock quantifiers are used for controlled elimination of the later\nmodality and for encoding coinductive types using guarded recursive types. Key\nto the development of gDTT are novel type and term formers involving what we\ncall `delayed substitutions'. These generalise the applicative functor rules\nfor the later modality considered in earlier work, and are crucial for\nprogramming and proving with dependent types. We show soundness of the type\ntheory with respect to a denotational model.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 16:26:14 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Bizjak", "Ale\u0161", ""], ["Grathwohl", "Hans Bugge", ""], ["Clouston", "Ranald", ""], ["M\u00f8gelberg", "Rasmus E.", ""], ["Birkedal", "Lars", ""]]}, {"id": "1601.01725", "submitter": "Emanuele D'Osualdo", "authors": "Emanuele D'Osualdo, C.-H. Luke Ong", "title": "On Hierarchical Communication Topologies in the pi-calculus", "comments": "42 pages, ESOP16. arXiv admin note: text overlap with\n  arXiv:1502.00944", "journal-ref": null, "doi": "10.1007/978-3-662-49498-1_7", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the shape invariants satisfied by the\ncommunication topology of {\\pi}-terms, and the automatic inference of these\ninvariants. A {\\pi}-term P is hierarchical if there is a finite forest T such\nthat the communication topology of every term reachable from P satisfies a\nT-shaped invariant. We design a static analysis to prove a term hierarchical by\nmeans of a novel type system that enjoys decidable inference. The soundness\nproof of the type system employs a non-standard view of {\\pi}-calculus\nreactions. The coverability problem for hierarchical terms is decidable. This\nis proved by showing that every hierarchical term is depth-bounded, an\nundecidable property known in the literature. We thus obtain an expressive\nstatic fragment of the {\\pi}-calculus with decidable safety verification\nproblems.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2016 23:35:48 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2016 14:30:11 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["D'Osualdo", "Emanuele", ""], ["Ong", "C. -H. Luke", ""]]}, {"id": "1601.02059", "submitter": "Andrew Black", "authors": "Andrew P. Black, Kim B. Bruce, and James Noble", "title": "The Essence of Inheritance", "comments": "This paper was submitted for inclusion in a Festschrift entitled \"A\n  list of successes that can change the world\", to be published by Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming languages serve a dual purpose: to communicate programs to\ncomputers, and to communicate programs to humans. Indeed, it is this dual\npurpose that makes programming language design a constrained and challenging\nproblem. Inheritance is an essential aspect of that second purpose: it is a\ntool to improve communication. Humans understand new concepts most readily by\nfirst looking at a number of concrete examples, and later abstracting over\nthose examples. The essence of inheritance is that it mirrors this process: it\nprovides a formal mechanism for moving from the concrete to the abstract.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2016 01:05:03 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Black", "Andrew P.", ""], ["Bruce", "Kim B.", ""], ["Noble", "James", ""]]}, {"id": "1601.02484", "submitter": "James Cheney", "authors": "Faris Abou-Saleh, James Cheney, Jeremy Gibbons, James McKinna, and\n  Perdita Stevens", "title": "Reflections on Monadic Lenses", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-30936-1_1", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bidirectional transformations (bx) have primarily been modeled as pure\nfunctions, and do not account for the possibility of the side-effects that are\navailable in most programming languages. Recently several formulations of bx\nthat use monads to account for effects have been proposed, both among\npractitioners and in academic research. The combination of bx with effects\nturns out to be surprisingly subtle, leading to problems with some of these\nproposals and increasing the complexity of others. This paper reviews the\nproposals for monadic lenses to date, and offers some improved definitions,\npaying particular attention to the obstacles to naively adding monadic effects\nto existing definitions of pure bx such as lenses and symmetric lenses, and the\nsubtleties of equivalence of symmetric bidirectional transformations in the\npresence of effects.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 15:47:08 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Abou-Saleh", "Faris", ""], ["Cheney", "James", ""], ["Gibbons", "Jeremy", ""], ["McKinna", "James", ""], ["Stevens", "Perdita", ""]]}, {"id": "1601.02536", "submitter": "Pradeeban Kathiravelu", "authors": "Pradeeban Kathiravelu, Xiao Chen, Dipesh Dugar Mitthalal, Lu\\'is Veiga", "title": "JikesRVM: Internal Mechanisms Study and Garbage Collection with MMTk", "comments": "Technical Report Submitted at Instituto Superior T\\'ecnico,\n  Universidade de Lisboa as part of the AVExe Virtual Execution Environments\n  Module", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High Level Language Virtual Machines is a core topic of interest for the\nresearchers who are into virtual execution environments. As an open source\nvirtual machine released to 16 universities, as early as 2001, Jikes RVM has\nbeen a major drive for many researches. While working on this project, we\nstudied the JIT compilation of Jikes RVM as well as the Garbage Collection (GC)\nwhich is handled by the Memory Management Toolkit (MMTk), a part of the Jikes\nRVM. We also studied the Compressor Mark-Compact Collector algorithm and\nimplemented it for MMTk. We have also implemented a micro-benchmark for the GC\nalgorithms in Java, named \"XPDBench\", for benchmarking the implementations.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 17:42:06 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Kathiravelu", "Pradeeban", ""], ["Chen", "Xiao", ""], ["Mitthalal", "Dipesh Dugar", ""], ["Veiga", "Lu\u00eds", ""]]}, {"id": "1601.02800", "submitter": "Pedro Lopez-Garcia", "authors": "Umer Liqat, Zorana Bankovic, Pedro Lopez-Garcia and Manuel V.\n  Hermenegildo", "title": "Inferring Energy Bounds via Static Program Analysis and Evolutionary\n  Modeling of Basic Blocks", "comments": "Pre-proceedings paper presented at the 27th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2017), Namur,\n  Belgium, 10-12 October 2017 (arXiv:1708.07854). Improved version of the one\n  presented at the HIP3ES 2016 workshop (v1): more experimental results (added\n  benchmark to Table 1, added figure for new benchmark, added Table 3),\n  improved Fig. 1, added Fig. 4", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2017/31", "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever increasing number and complexity of energy-bound devices (such as\nthe ones used in Internet of Things applications, smart phones, and mission\ncritical systems) pose an important challenge on techniques to optimize their\nenergy consumption and to verify that they will perform their function within\nthe available energy budget. In this work we address this challenge from the\nsoftware point of view and propose a novel parametric approach to estimating\ntight bounds on the energy consumed by program executions that are practical\nfor their application to energy verification and optimization. Our approach\ndivides a program into basic (branchless) blocks and estimates the maximal and\nminimal energy consumption for each block using an evolutionary algorithm. Then\nit combines the obtained values according to the program control flow, using\nstatic analysis, to infer functions that give both upper and lower bounds on\nthe energy consumption of the whole program and its procedures as functions on\ninput data sizes. We have tested our approach on (C-like) embedded programs\nrunning on the XMOS hardware platform. However, our method is general enough to\nbe applied to other microprocessor architectures and programming languages. The\nbounds obtained by our prototype implementation can be tight while remaining on\nthe safe side of budgets in practice, as shown by our experimental evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 10:52:48 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 07:46:56 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Liqat", "Umer", ""], ["Bankovic", "Zorana", ""], ["Lopez-Garcia", "Pedro", ""], ["Hermenegildo", "Manuel V.", ""]]}, {"id": "1601.03116", "submitter": "Jeffrey Murphy", "authors": "Muyuan Li, Daniel E McArdle, Jeffrey C Murphy, Bhargav Shivkumar,\n  Lukasz Ziarek", "title": "Adding Real-time Capabilities to a SML Compiler", "comments": "6 pages, 9 figures, ACM SIGBED", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been much recent interest in adopting functional and reactive\nprogramming for use in real-time system design. Moving toward a more\ndeclarative methodology for developing real-time systems purports to improve\nthe fidelity of software. To study the benefits of functional and reactive\nprogramming for real-time systems, real-time aware functional compilers and\nlanguage runtimes are required. In this paper we examine the necessary changes\nto a modern Standard ML compiler, MLton, to provide basic support for real-time\nexecution. We detail our current progress in modifying MLton with a threading\nmodel that supports priorities, a chunked object model to support real-time\ngarbage collection, and low level modification to execute on top of a real-time\noperating system. We present preliminary numbers and our work in progress\nprototype, which is able to boot ML programs compiled with MLton on x86\nmachines.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 02:44:24 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Li", "Muyuan", ""], ["McArdle", "Daniel E", ""], ["Murphy", "Jeffrey C", ""], ["Shivkumar", "Bhargav", ""], ["Ziarek", "Lukasz", ""]]}, {"id": "1601.03271", "submitter": "Andr\\'es Ezequiel Viso", "authors": "Andr\\'es Viso, Eduardo Bonelli, Mauricio Ayala-Rinc\\'on", "title": "Type Soundness for Path Polymorphism", "comments": null, "journal-ref": null, "doi": "10.1016/j.entcs.2016.06.015", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Path polymorphism is the ability to define functions that can operate\nuniformly over arbitrary recursively specified data structures. Its essence is\ncaptured by patterns of the form $x\\,y$ which decompose a compound data\nstructure into its parts. Typing these kinds of patterns is challenging since\nthe type of a compound should determine the type of its components. We propose\na static type system (i.e. no run-time analysis) for a pattern calculus that\ncaptures this feature. Our solution combines type application, constants as\ntypes, union types and recursive types. We address the fundamental properties\nof Subject Reduction and Progress that guarantee a well-behaved dynamics. Both\nthese results rely crucially on a notion of pattern compatibility and also on a\ncoinductive characterisation of subtyping.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 14:47:49 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 15:12:12 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Viso", "Andr\u00e9s", ""], ["Bonelli", "Eduardo", ""], ["Ayala-Rinc\u00f3n", "Mauricio", ""]]}, {"id": "1601.04299", "submitter": "Benedikt Ahrens", "authors": "Benedikt Ahrens and Ralph Matthes", "title": "Heterogeneous substitution systems revisited", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matthes and Uustalu (TCS 327(1-2):155-174, 2004) presented a categorical\ndescription of substitution systems capable of capturing syntax involving\nbinding which is independent of whether the syntax is made up from least or\ngreatest fixed points. We extend this work in two directions: we continue the\nanalysis by creating more categorical structure, in particular by organizing\nsubstitution systems into a category and studying its properties, and we\ndevelop the proofs of the results of the cited paper and our new ones in\nUniMath, a recent library of univalent mathematics formalized in the Coq\ntheorem prover.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2016 14:14:37 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Ahrens", "Benedikt", ""], ["Matthes", "Ralph", ""]]}, {"id": "1601.04943", "submitter": "Hongseok Yang", "authors": "Sam Staton, Hongseok Yang, Chris Heunen, Ohad Kammar, Frank Wood", "title": "Semantics for probabilistic programming: higher-order functions,\n  continuous distributions, and soft constraints", "comments": null, "journal-ref": "Logic in Computer Science 525--534, 2016", "doi": "10.1145/2933575.2935313", "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the semantic foundation of expressive probabilistic programming\nlanguages, that support higher-order functions, continuous distributions, and\nsoft constraints (such as Anglican, Church, and Venture). We define a\nmetalanguage (an idealised version of Anglican) for probabilistic computation\nwith the above features, develop both operational and denotational semantics,\nand prove soundness, adequacy, and termination. They involve measure theory,\nstochastic labelled transition systems, and functor categories, but admit\nintuitive computational readings, one of which views sampled random variables\nas dynamically allocated read-only variables. We apply our semantics to\nvalidate nontrivial equations underlying the correctness of certain compiler\noptimisations and inference algorithms such as sequential Monte Carlo\nsimulation. The language enables defining probability distributions on\nhigher-order functions, and we study their properties.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 14:53:54 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2016 05:51:41 GMT"}, {"version": "v3", "created": "Wed, 4 May 2016 04:38:55 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Staton", "Sam", ""], ["Yang", "Hongseok", ""], ["Heunen", "Chris", ""], ["Kammar", "Ohad", ""], ["Wood", "Frank", ""]]}, {"id": "1601.05106", "submitter": "Jana Dunfield", "authors": "Jana Dunfield and Neelakantan R. Krishnaswami", "title": "Sound and Complete Bidirectional Typechecking for Higher-Rank\n  Polymorphism with Existentials and Indexed Types", "comments": "28 pages, plus lemmas and proofs (191 pages); accepted to POPL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bidirectional typechecking, in which terms either synthesize a type or are\nchecked against a known type, has become popular for its applicability to a\nvariety of type systems, its error reporting, and its ease of implementation.\nFollowing principles from proof theory, bidirectional typing can be applied to\nmany type constructs. The principles underlying a bidirectional approach to\nindexed types (generalized algebraic datatypes) are less clear. Building on\nproof-theoretic treatments of equality, we give a declarative specification of\ntyping based on focalization. This approach permits declarative rules for\ncoverage of pattern matching, as well as support for first-class existential\ntypes using a focalized subtyping judgment. We use refinement types to avoid\nexplicitly passing equality proofs in our term syntax, making our calculus\nsimilar to languages such as Haskell and OCaml. We also extend the declarative\nspecification with an explicit rules for deducing when a type is principal,\npermitting us to give a complete declarative specification for a rich type\nsystem with significant type inference. We also give a set of algorithmic\ntyping rules, and prove that it is sound and complete with respect to the\ndeclarative system. The proof requires a number of technical innovations,\nincluding proving soundness and completeness in a mutually recursive fashion.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 21:36:21 GMT"}, {"version": "v2", "created": "Sat, 22 Sep 2018 20:07:27 GMT"}, {"version": "v3", "created": "Sat, 10 Nov 2018 22:43:05 GMT"}, {"version": "v4", "created": "Sat, 19 Sep 2020 22:48:42 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Dunfield", "Jana", ""], ["Krishnaswami", "Neelakantan R.", ""]]}, {"id": "1601.05400", "submitter": "Mads Kristensen", "authors": "Mads R. B. Kristensen, Simon A. F. Lund, Troels Blum, James Avery", "title": "Fusion of Array Operations at Runtime", "comments": "Preprint", "journal-ref": "Proceeding PACT '16 Proceedings of the 2016 International\n  Conference on Parallel Architectures and Compilation Pages 71-85", "doi": "10.1145/2967938.2967945", "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of fusing array operations based on criteria such as\nshape compatibility, data reusability, and communication. We formulate the\nproblem as a graph partition problem that is general enough to handle loop\nfusion, combinator fusion, and other types of subroutines.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2016 20:21:01 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 10:31:17 GMT"}], "update_date": "2016-09-07", "authors_parsed": [["Kristensen", "Mads R. B.", ""], ["Lund", "Simon A. F.", ""], ["Blum", "Troels", ""], ["Avery", "James", ""]]}, {"id": "1601.05520", "submitter": "Christine Rizkallah", "authors": "Liam O'Connor, Christine Rizkallah, Zilin Chen, Sidney Amani, Japheth\n  Lim, Yutaka Nagashima, Thomas Sewell, Alex Hixon, Gabriele Keller, Toby\n  Murray, Gerwin Klein", "title": "COGENT: Certified Compilation for a Functional Systems Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a self-certifying compiler for the COGENT systems language. COGENT\nis a restricted, polymorphic, higher-order, and purely functional language with\nlinear types and without the need for a trusted runtime or garbage collector.\nIt compiles to efficient C code that is designed to interoperate with existing\nC functions. The language is suited for layered systems code with minimal\nsharing such as file systems or network protocol control code. For a well-typed\nCOGENT program, the compiler produces C code, a high-level shallow embedding of\nits semantics in Isabelle/HOL, and a proof that the C code correctly implements\nthis embedding. The aim is for proof engineers to reason about the full\nsemantics of real-world systems code productively and equationally, while\nretaining the interoperability and leanness of C. We describe the formal\nverification stages of the compiler, which include automated formal refinement\ncalculi, a switch from imperative update semantics to functional value\nsemantics formally justified by the linear type system, and a number of\nstandard compiler phases such as type checking and monomorphisation. The\ncompiler certificate is a series of language-level meta proofs and per-program\ntranslation validation phases, combined into one coherent top-level theorem in\nIsabelle/HOL.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 05:54:07 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["O'Connor", "Liam", ""], ["Rizkallah", "Christine", ""], ["Chen", "Zilin", ""], ["Amani", "Sidney", ""], ["Lim", "Japheth", ""], ["Nagashima", "Yutaka", ""], ["Sewell", "Thomas", ""], ["Hixon", "Alex", ""], ["Keller", "Gabriele", ""], ["Murray", "Toby", ""], ["Klein", "Gerwin", ""]]}, {"id": "1601.06183", "submitter": "Andr\\'e Platzer", "authors": "Andr\\'e Platzer", "title": "A Complete Uniform Substitution Calculus for Differential Dynamic Logic", "comments": "Long article extending the conference version that appeared at CADE\n  2015, arXiv:1503.01981", "journal-ref": "Journal of Automated Reasoning, 59(2), pages 219-265, 2017", "doi": "10.1007/s10817-016-9385-1", "report-no": null, "categories": "cs.LO cs.PL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a relatively complete proof calculus for differential\ndynamic logic (dL) that is entirely based on uniform substitution, a proof rule\nthat substitutes a formula for a predicate symbol everywhere. Uniform\nsubstitutions make it possible to use axioms instead of axiom schemata, thereby\nsubstantially simplifying implementations. Instead of subtle schema variables\nand soundness-critical side conditions on the occurrence patterns of logical\nvariables to restrict infinitely many axiom schema instances to sound ones, the\nresulting calculus adopts only a finite number of ordinary dL formulas as\naxioms, which uniform substitutions instantiate soundly. The static semantics\nof differential dynamic logic and the soundness-critical restrictions it\nimposes on proof steps is captured exclusively in uniform substitutions and\nvariable renamings as opposed to being spread in delicate ways across the\nprover implementation. In addition to sound uniform substitutions, this article\nintroduces differential forms for differential dynamic logic that make it\npossible to internalize differential invariants, differential substitutions,\nand derivatives as first-class axioms to reason about differential equations\naxiomatically. The resulting axiomatization of differential dynamic logic is\nproved to be sound and relatively complete.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 22:00:10 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 14:37:41 GMT"}, {"version": "v3", "created": "Wed, 27 Jul 2016 18:50:35 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Platzer", "Andr\u00e9", ""]]}, {"id": "1601.06298", "submitter": "Jonathan Sterling", "authors": "Jonathan Sterling and Darin Morrison", "title": "Syntax and Semantics of Abstract Binding Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contribution of this paper is the development of the syntax and semantics\nof multi-sorted nominal abstract binding trees (abts), an extension of second\norder universal algebra to support symbol-indexed families of operators.\nNominal abts are essential for correctly treating the syntax of languages with\ngenerative phenomena, including exceptions and mutable state. Additionally we\nhave developed the categorical semantics for abstract binding trees formally in\nConstructive Type Theory using the Agda proof assistant. Multi-sorted nominal\nabts also form the syntactic basis for the upcoming version of the JonPRL proof\nassistant, an implementation of an extensional constructive type theory in the\nNuprl tradition.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2016 17:37:40 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Sterling", "Jonathan", ""], ["Morrison", "Darin", ""]]}, {"id": "1601.06517", "submitter": "Yong Lin", "authors": "Yong Lin, Martin Henz", "title": "Programmable Restoration Granularity in Constraint Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most constraint programming systems, a limited number of search engines is\noffered while the programming of user-customized search algorithms requires\nlow-level efforts, which complicates the deployment of such algorithms. To\nalleviate this limitation, concepts such as computation spaces have been\ndeveloped. Computation spaces provide a coarse-grained restoration mechanism,\nbecause they store all information contained in a search tree node. Other\ngranularities are possible, and in this paper we make the case for dynamically\nadapting the restoration granularity during search. In order to elucidate\nprogrammable restoration granularity, we present restoration as an aspect of a\nconstraint programming system, using the model of aspect-oriented programming.\nA proof-of-concept implementation using Gecode shows promising results.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2016 08:56:27 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2016 09:34:54 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Lin", "Yong", ""], ["Henz", "Martin", ""]]}, {"id": "1601.07224", "submitter": "Yura Perov N", "authors": "Yura N Perov", "title": "Bachelor's thesis on generative probabilistic programming (in Russian\n  language, June 2014)", "comments": "49 pages, in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This Bachelor's thesis, written in Russian, is devoted to a relatively new\ndirection in the field of machine learning and artificial intelligence, namely\nprobabilistic programming. The thesis gives a brief overview to the already\nexisting probabilistic programming languages: Church, Venture, and Anglican. It\nalso describes the results of the first experiments on the automatic induction\nof probabilistic programs. The thesis was submitted, in June 2014, in partial\nfulfilment of the requirements for the degree of Bachelor of Science in\nMathematics in the Department of Mathematics and Computer Science, Siberian\nFederal University, Krasnoyarsk, Russia. The work, which is described in this\nthesis, has been performing in 2012-2014 in the Massachusetts Institute of\nTechnology and in the University of Oxford by the colleagues of the author and\nby himself.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2016 23:31:05 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Perov", "Yura N", ""]]}, {"id": "1601.07300", "submitter": "Yong Lin", "authors": "Yong Lin, Martin Henz", "title": "Recollection: an Alternative Restoration Technique for Constraint\n  Programming Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search is a key service within constraint programming systems, and it demands\nthe restoration of previously accessed states during the exploration of a\nsearch tree. Restoration proceeds either bottom-up within the tree to roll back\npreviously performed operations using a trail, or top-down to redo them,\nstarting from a previously stored state and using suitable information stored\nalong the way. In this paper, we elucidate existing restoration techniques\nusing a pair of abstract methods and employ them to present a new technique\nthat we call recollection. The proposed technique stores the variables that\nwere affected by constraint propagation during fix points reasoning steps, and\nit conducts neither operation roll-back nor recomputation, while consuming much\nless memory than storing previous visited states. We implemented this idea as a\nprototype within the Gecode solver. An empirical evaluation reveals that\nconstraint problems with expensive propagation and frequent failures can\nbenefit from recollection with respect to runtime at the expense of a marginal\nincrease in memory consumption, comparing with the most competitive variant of\nrecomputation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2016 09:34:36 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2016 09:48:00 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Lin", "Yong", ""], ["Henz", "Martin", ""]]}]