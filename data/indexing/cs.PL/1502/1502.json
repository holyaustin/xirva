[{"id": "1502.00045", "submitter": "Dirk Beyer", "authors": "Dirk Beyer, Stefan L\\\"owe, and Philipp Wendler", "title": "Domain-Type-Guided Refinement Selection Based on Sliced Path Prefixes", "comments": "10 pages, 5 figures, 1 table, 4 algorithms", "journal-ref": null, "doi": null, "report-no": "MIP-1501", "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstraction is a successful technique in software verification, and\ninterpolation on infeasible error paths is a successful approach to\nautomatically detect the right level of abstraction in counterexample-guided\nabstraction refinement. Because the interpolants have a significant influence\non the quality of the abstraction, and thus, the effectiveness of the\nverification, an algorithm for deriving the best possible interpolants is\ndesirable. We present an analysis-independent technique that makes it possible\nto extract several alternative sequences of interpolants from one given\ninfeasible error path, if there are several reasons for infeasibility in the\nerror path. We take as input the given infeasible error path and apply a\nslicing technique to obtain a set of error paths that are more abstract than\nthe original error path but still infeasible, each for a different reason. The\n(more abstract) constraints of the new paths can be passed to a standard\ninterpolation engine, in order to obtain a set of interpolant sequences, one\nfor each new path. The analysis can then choose from this set of interpolant\nsequences and select the most appropriate, instead of being bound to the single\ninterpolant sequence that the interpolation engine would normally return. For\nexample, we can select based on domain types of variables in the interpolants,\nprefer to avoid loop counters, or compare with templates for potential loop\ninvariants, and thus control what kind of information occurs in the abstraction\nof the program. We implemented the new algorithm in the open-source\nverification framework CPAchecker and show that our proof-technique-independent\napproach yields a significant improvement of the effectiveness and efficiency\nof the verification process.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 00:16:32 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Beyer", "Dirk", ""], ["L\u00f6we", "Stefan", ""], ["Wendler", "Philipp", ""]]}, {"id": "1502.00096", "submitter": "Dirk Beyer", "authors": "Dirk Beyer, Matthias Dangl, and Philipp Wendler", "title": "Combining k-Induction with Continuously-Refined Invariants", "comments": "12 pages, 5 figures, 2 tables, 2 algorithms", "journal-ref": null, "doi": null, "report-no": "MIP-1503", "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bounded model checking (BMC) is a well-known and successful technique for\nfinding bugs in software. k-induction is an approach to extend BMC-based\napproaches from falsification to verification. Automatically generated\nauxiliary invariants can be used to strengthen the induction hypothesis. We\nimprove this approach and further increase effectiveness and efficiency in the\nfollowing way: we start with light-weight invariants and refine these\ninvariants continuously during the analysis. We present and evaluate an\nimplementation of our approach in the open-source verification-framework\nCPAchecker. Our experiments show that combining k-induction with\ncontinuously-refined invariants significantly increases effectiveness and\nefficiency, and outperforms all existing implementations of k-induction-based\nsoftware verification in terms of successful verification results.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 12:32:10 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Beyer", "Dirk", ""], ["Dangl", "Matthias", ""], ["Wendler", "Philipp", ""]]}, {"id": "1502.00138", "submitter": "Zachary Kincaid", "authors": "Azadeh Farzan and Zachary Kincaid", "title": "Compositional Invariant Generation via Linear Recurrence Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method for automatically generating numerical\ninvariants for imperative programs. Given a program, our procedure computes a\nbinary input/output relation on program states which over-approximates the\nbehaviour of the program. It is compositional in the sense that it operates by\ndecomposing the program into parts, computing an abstract meaning of each part,\nand then composing the meanings. Our method for approximating loop behaviour is\nbased on first approximating the meaning of the loop body, extracting\nrecurrence relations from that approximation, and then using the closed forms\nto approximate the loop. Our experiments demonstrate that on verification\ntasks, our method is competitive with leading invariant generation and\nverification tools.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 16:43:46 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Farzan", "Azadeh", ""], ["Kincaid", "Zachary", ""]]}, {"id": "1502.00238", "submitter": "Kees Middelburg", "authors": "J. A. Bergstra, C. A. Middelburg", "title": "On instruction sets for Boolean registers in program algebra", "comments": "18 pages, the preliminaries are largely the same as the preliminaries\n  in arXiv:1402.4950 [cs.PL] and some earlier papers; 21 pages, presentation\n  improved", "journal-ref": "Scientific Annals of Computer Science 26(1):1--26, 2016", "doi": "10.7561/SACS.2016.1.1", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In previous work carried out in the setting of program algebra, including\nwork in the area of instruction sequence size complexity, we chose instruction\nsets for Boolean registers that contain only instructions of a few of the\npossible kinds. In the current paper, we study instruction sequence size\nbounded functional completeness of all possible instruction sets for Boolean\nregisters. We expect that the results of this study will turn out to be useful\nto adequately assess results of work that is concerned with lower bounds of\ninstruction sequence size complexity.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 12:17:12 GMT"}, {"version": "v2", "created": "Mon, 6 Jun 2016 18:28:22 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Bergstra", "J. A.", ""], ["Middelburg", "C. A.", ""]]}, {"id": "1502.00944", "submitter": "Emanuele D'Osualdo", "authors": "Emanuele D'Osualdo, Luke Ong", "title": "A Type System for proving Depth Boundedness in the pi-calculus", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The depth-bounded fragment of the pi-calculus is an expressive class of\nsystems enjoying decidability of some important verification problems.\nUnfortunately membership of the fragment is undecidable. We propose a novel\ntype system, parameterised over a finite forest, that formalises name usage by\npi-terms in a manner that respects the forest. Type checking is decidable and\ntype inference is computable; furthermore typable pi-terms are guaranteed to be\ndepth bounded.\n  The second contribution of the paper is a proof of equivalence between the\nsemantics of typable terms and nested data class memory automata, a class of\nautomata over data words. We believe this connection can help to establish new\nlinks between the rich theory of infinite-alphabet automata and nominal\ncalculi.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 18:02:32 GMT"}, {"version": "v2", "created": "Mon, 23 Feb 2015 12:44:18 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["D'Osualdo", "Emanuele", ""], ["Ong", "Luke", ""]]}, {"id": "1502.01278", "submitter": "Robert Jakob", "authors": "Robert Jakob and Peter Thiemann", "title": "A Falsification View of Success Typing", "comments": "extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic languages are praised for their flexibility and expressiveness, but\nstatic analysis often yields many false positives and verification is\ncumbersome for lack of structure. Hence, unit testing is the prevalent\nincomplete method for validating programs in such languages.\n  Falsification is an alternative approach that uncovers definite errors in\nprograms. A falsifier computes a set of inputs that definitely crash a program.\n  Success typing is a type-based approach to document programs in dynamic\nlanguages. We demonstrate that success typing is, in fact, an instance of\nfalsification by mapping success (input) types into suitable logic formulae.\nOutput types are represented by recursive types. We prove the correctness of\nour mapping (which establishes that success typing is falsification) and we\nreport some experiences with a prototype implementation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 18:10:21 GMT"}, {"version": "v2", "created": "Thu, 5 Feb 2015 09:22:30 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Jakob", "Robert", ""], ["Thiemann", "Peter", ""]]}, {"id": "1502.02389", "submitter": "Michel Steuwer", "authors": "Michel Steuwer and Christian Fensch and Christophe Dubach", "title": "Patterns and Rewrite Rules for Systematic Code Generation (From\n  High-Level Functional Patterns to High-Performance OpenCL Code)", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing systems have become increasingly complex with the emergence of\nheterogeneous hardware combining multicore CPUs and GPUs. These parallel\nsystems exhibit tremendous computational power at the cost of increased\nprogramming effort. This results in a tension between achieving performance and\ncode portability. Code is either tuned using device-specific optimizations to\nachieve maximum performance or is written in a high-level language to achieve\nportability at the expense of performance.\n  We propose a novel approach that offers high-level programming, code\nportability and high-performance. It is based on algorithmic pattern\ncomposition coupled with a powerful, yet simple, set of rewrite rules. This\nenables systematic transformation and optimization of a high-level program into\na low-level hardware specific representation which leads to high performance\ncode.\n  We test our design in practice by describing a subset of the OpenCL\nprogramming model with low-level patterns and by implementing a compiler which\ngenerates high performance OpenCL code. Our experiments show that we can\nsystematically derive high-performance device-specific implementations from\nsimple high-level algorithmic expressions. The performance of the generated\nOpenCL code is on par with highly tuned implementations for multicore CPUs and\nGPUs written by experts\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 07:28:22 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Steuwer", "Michel", ""], ["Fensch", "Christian", ""], ["Dubach", "Christophe", ""]]}, {"id": "1502.02519", "submitter": "Fabrizio Montesi", "authors": "Fabrizio Montesi", "title": "Kickstarting Choreographic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an overview of some recent efforts aimed at the development of\nChoreographic Programming, a programming paradigm for the production of\nconcurrent software that is guaranteed to be correct by construction from\nglobal descriptions of communication behaviour.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 15:20:03 GMT"}, {"version": "v2", "created": "Tue, 10 Feb 2015 12:03:10 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Montesi", "Fabrizio", ""]]}, {"id": "1502.02921", "submitter": "Albert Saa-Garriga", "authors": "Albert Saa-Garriga, David Castells-Rufas and Jordi Carrabina", "title": "OMP2MPI: Automatic MPI code generation from OpenMP programs", "comments": "Presented at HIP3ES, 2015 (arXiv: 1501.03064)", "journal-ref": null, "doi": null, "report-no": "HIP3ES/2015/06", "categories": "cs.DC cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present OMP2MPI a tool that generates automatically MPI\nsource code from OpenMP. With this transformation the original program can be\nadapted to be able to exploit a larger number of processors by surpassing the\nlimits of the node level on large HPC clusters. The transformation can also be\nuseful to adapt the source code to execute in distributed memory many-cores\nwith message passing support. In addition, the resulting MPI code can be used\nas an starting point that still can be further optimized by software engineers.\nThe transformation process is focused on detecting OpenMP parallel loops and\ndistributing them in a master/worker pattern. A set of micro-benchmarks have\nbeen used to verify the correctness of the the transformation and to measure\nthe resulting performance. Surprisingly not only the automatically generated\ncode is correct by construction, but also it often performs faster even when\nexecuted with MPI.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 14:32:25 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2015 08:52:41 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Saa-Garriga", "Albert", ""], ["Castells-Rufas", "David", ""], ["Carrabina", "Jordi", ""]]}, {"id": "1502.02942", "submitter": "Mitesh Jain", "authors": "Mitesh Jain and Panagiotis Manolios", "title": "Skipping Refinement", "comments": "Submitted to CAV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce skipping refinement, a new notion of correctness for reasoning\nabout optimized reactive systems. Reasoning about reactive systems using\nrefinement involves defining an abstract, high-level specification system and a\nconcrete, low-level implementation system. One then shows that every behavior\nallowed by the implementation is also allowed by the specification. Due to the\ndifference in abstraction levels, it is often the case that the implementation\nrequires many steps to match one step of the specification, hence, it is quite\nuseful for refinement to directly account for stuttering. Some optimized\nimplementations, however, can actually take multiple specification steps at\nonce. For example, a memory controller can buffer the commands to the memory\nand at a later time simultaneously update multiple memory locations, thereby\nskipping several observable states of the abstract specification, which only\nupdates one memory location at a time. We introduce skipping simulation\nrefinement and provide a sound and complete characterization consisting of\n\"local\" proof rules that are amenable to mechanization and automated\nverification. We present case studies that highlight the applicability of\nskipping refinement: a JVM-inspired stack machine, a simple memory controller\nand a scalar to vector compiler transformation. Our experimental results\ndemonstrate that current model-checking and automated theorem proving tools\nhave difficultly automatically analyzing these systems using existing notions\nof correctness, but they can analyze the systems if we use skipping refinement.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 15:16:50 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Jain", "Mitesh", ""], ["Manolios", "Panagiotis", ""]]}, {"id": "1502.03157", "submitter": "EPTCS", "authors": "Javier C\\'amara (Carnegie Mellon University), Jos\\'e Proen\\c{c}a (KU\n  Leuven)", "title": "Proceedings 13th International Workshop on Foundations of Coordination\n  Languages and Self-Adaptive Systems", "comments": null, "journal-ref": "EPTCS 175, 2015", "doi": "10.4204/EPTCS.175", "report-no": null, "categories": "cs.DC cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of FOCLASA 2014, the 13th International\nWorkshop on the Foundations of Coordination Languages and Self-Adaptive\nSystems. FOCLASA 2014 was held in Rome, Italy, on September 9, 2014 as a\nsatellite event of CONCUR 2014, the 25th International Conference on\nConcurrency Theory.\n  Modern software systems are distributed, concurrent, mobile, and often\ninvolve composition of heterogeneous components and stand-alone services.\nService coordination and self-adaptation constitute the core characteristics of\ndistributed and service-oriented systems. Coordination languages and formal\napproaches to modelling and reasoning about self-adaptive behaviour help to\nsimplify the development of complex distributed service-based systems, enable\nfunctional correctness proofs and improve reusability and maintainability of\nsuch systems. The goal of the FOCLASA workshop is to put together researchers\nand practitioners of the aforementioned fields, to share and identify common\nproblems, and to devise general solutions in the context of coordination\nlanguages and self-adaptive systems.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 00:30:30 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["C\u00e1mara", "Javier", "", "Carnegie Mellon University"], ["Proen\u00e7a", "Jos\u00e9", "", "KU\n  Leuven"]]}, {"id": "1502.03504", "submitter": "Matthew Sottile", "authors": "Craig Rasmussen and Matthew Sottile and Daniel Nagle and Soren\n  Rasmussen", "title": "Locally-Oriented Programming: A Simple Programming Model for\n  Stencil-Based Computations on Multi-Level Distributed Memory Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging hybrid accelerator architectures for high performance computing are\noften suited for the use of a data-parallel programming model. Unfortunately,\nprogrammers of these architectures face a steep learning curve that frequently\nrequires learning a new language (e.g., OpenCL). Furthermore, the distributed\n(and frequently multi-level) nature of the memory organization of clusters of\nthese machines provides an additional level of complexity. This paper presents\npreliminary work examining how programming with a local orientation can be\nemployed to provide simpler access to accelerator architectures. A\nlocally-oriented programming model is especially useful for the solution of\nalgorithms requiring the application of a stencil or convolution kernel. In\nthis programming model, a programmer codes the algorithm by modifying only a\nsingle array element (called the local element), but has read-only access to a\nsmall sub-array surrounding the local element. We demonstrate how a\nlocally-oriented programming model can be adopted as a language extension using\nsource-to-source program transformations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 01:34:33 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Rasmussen", "Craig", ""], ["Sottile", "Matthew", ""], ["Nagle", "Daniel", ""], ["Rasmussen", "Soren", ""]]}, {"id": "1502.03513", "submitter": "EPTCS", "authors": "Denis Darquennes, Jean-Marie Jacquet, Isabelle Linden", "title": "On Distributed Density in Tuple-based Coordination Languages", "comments": "In Proceedings FOCLASA 2014, arXiv:1502.03157", "journal-ref": "EPTCS 175, 2015, pp. 36-53", "doi": "10.4204/EPTCS.175.3", "report-no": null, "categories": "cs.PL cs.DC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the chemical metaphor, this paper proposes an extension of\nLinda-like languages in the aim of modeling the coordination of complex\ndistributed systems. The new language manipulates finite sets of tuples and\ndistributes a density among them. This new concept adds to the non-determinism\ninherent in the selection of matched tuples a non-determinism to the tell, ask\nand get primitives on the consideration of different tuples. Furthermore,\nthanks to de Boer and Palamidessi's notion of modular embedding, we establish\nthat this new language strictly increases the expressiveness of the Dense Bach\nlanguage introduced earlier and, consequently, Linda-like languages.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 02:15:02 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Darquennes", "Denis", ""], ["Jacquet", "Jean-Marie", ""], ["Linden", "Isabelle", ""]]}, {"id": "1502.04634", "submitter": "Danko Ilik", "authors": "Danko Ilik", "title": "The exp-log normal form of types", "comments": null, "journal-ref": "POPL 2017 Proceedings of the 44th ACM SIGPLAN Symposium on\n  Principles of Programming Languages. Pages 387-399. Paris, France -- January\n  15 - 21, 2017", "doi": "10.1145/3009837.3009841", "report-no": null, "categories": "cs.LO cs.PL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lambda calculi with algebraic data types lie at the core of functional\nprogramming languages and proof assistants, but conceal at least two\nfundamental theoretical problems already in the presence of the simplest\nnon-trivial data type, the sum type. First, we do not know of an explicit and\nimplemented algorithm for deciding the beta-eta-equality of terms---and this in\nspite of the first decidability results proven two decades ago. Second, it is\nnot clear how to decide when two types are essentially the same, i.e.\nisomorphic, in spite of the meta-theoretic results on decidability of the\nisomorphism.\n  In this paper, we present the exp-log normal form of types---derived from the\nrepresentation of exponential polynomials via the unary exponential and\nlogarithmic functions---that any type built from arrows, products, and sums,\ncan be isomorphically mapped to. The type normal form can be used as a simple\nheuristic for deciding type isomorphism, thanks to the fact that it is a\nsystematic application of the high-school identities.\n  We then show that the type normal form allows to reduce the standard beta-eta\nequational theory of the lambda calculus to a specialized version of itself,\nwhile preserving the completeness of equality on terms. We end by describing an\nalternative representation of normal terms of the lambda calculus with sums,\ntogether with a Coq-implemented converter into/from our new term calculus. The\ndifference with the only other previously implemented heuristic for deciding\ninteresting instances of eta-equality by Balat, Di Cosmo, and Fiore, is that we\nexploit the type information of terms substantially and this often allows us to\nobtain a canonical representation of terms without performing sophisticated\nterm analyses.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 17:14:52 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2015 18:34:09 GMT"}, {"version": "v3", "created": "Tue, 10 May 2016 09:41:43 GMT"}, {"version": "v4", "created": "Thu, 30 Jun 2016 13:39:23 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Ilik", "Danko", ""]]}, {"id": "1502.04770", "submitter": "EPTCS", "authors": "Jennifer Paykin (University of Pennsylvania), Steve Zdancewic\n  (University of Pennsylvania)", "title": "A Linear/Producer/Consumer Model of Classical Linear Logic", "comments": "In Proceedings LINEARITY 2014, arXiv:1502.04419", "journal-ref": "EPTCS 176, 2015, pp. 9-23", "doi": "10.4204/EPTCS.176.2", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper defines a new proof- and category-theoretic framework for\nclassical linear logic that separates reasoning into one linear regime and two\npersistent regimes corresponding to ! and ?. The resulting\nlinear/producer/consumer (LPC) logic puts the three classes of propositions on\nthe same semantic footing, following Benton's linear/non-linear formulation of\nintuitionistic linear logic. Semantically, LPC corresponds to a system of three\ncategories connected by adjunctions reflecting the linear/producer/consumer\nstructure. The paper's metatheoretic results include admissibility theorems for\nthe cut and duality rules, and a translation of the LPC logic into category\ntheory. The work also presents several concrete instances of the LPC model.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 02:27:16 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Paykin", "Jennifer", "", "University of Pennsylvania"], ["Zdancewic", "Steve", "", "University of Pennsylvania"]]}, {"id": "1502.04772", "submitter": "EPTCS", "authors": "Edward Gan (Facebook), Jesse A. Tov (Northeastern University), Greg\n  Morrisett (Harvard University)", "title": "Type Classes for Lightweight Substructural Types", "comments": "In Proceedings LINEARITY 2014, arXiv:1502.04419", "journal-ref": "EPTCS 176, 2015, pp. 34-48", "doi": "10.4204/EPTCS.176.4", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear and substructural types are powerful tools, but adding them to\nstandard functional programming languages often means introducing extra\nannotations and typing machinery. We propose a lightweight substructural type\nsystem design that recasts the structural rules of weakening and contraction as\ntype classes; we demonstrate this design in a prototype language, Clamp.\n  Clamp supports polymorphic substructural types as well as an expressive\nsystem of mutable references. At the same time, it adds little additional\noverhead to a standard Damas-Hindley-Milner type system enriched with type\nclasses. We have established type safety for the core model and implemented a\ntype checker with type inference in Haskell.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 02:27:31 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Gan", "Edward", "", "Facebook"], ["Tov", "Jesse A.", "", "Northeastern University"], ["Morrisett", "Greg", "", "Harvard University"]]}, {"id": "1502.04774", "submitter": "EPTCS", "authors": "Ugo Dal Lago (Universit\\`a di Bologna & INRIA), Margherita Zorzi\n  (Universit\\`a di Verona)", "title": "Wave-Style Token Machines and Quantum Lambda Calculi", "comments": "In Proceedings LINEARITY 2014, arXiv:1502.04419", "journal-ref": "EPTCS 176, 2015, pp. 64-78", "doi": "10.4204/EPTCS.176.6", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle-style token machines are a way to interpret proofs and programs,\nwhen the latter are written following the principles of linear logic. In this\npaper, we show that token machines also make sense when the programs at hand\nare those of a simple quantum lambda-calculus with implicit qubits. This,\nhowever, requires generalising the concept of a token machine to one in which\nmore than one particle travel around the term at the same time. The presence of\nmultiple tokens is intimately related to entanglement and allows us to give a\nsimple operational semantics to the calculus, coherently with the principles of\nquantum computation.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 02:27:51 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Lago", "Ugo Dal", "", "Universit\u00e0 di Bologna & INRIA"], ["Zorzi", "Margherita", "", "Universit\u00e0 di Verona"]]}, {"id": "1502.04775", "submitter": "EPTCS", "authors": "Marco Solieri (LIPN, Paris 13, Sorbonne Paris Cit\\'e, CNRS -- DISI,\n  Bologna, INRIA)", "title": "Geometry of Resource Interaction - A Minimalist Approach", "comments": "In Proceedings LINEARITY 2014, arXiv:1502.04419", "journal-ref": "EPTCS 176, 2015, pp. 79-94", "doi": "10.4204/EPTCS.176.7", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Resource $\\lambda$-calculus is a variation of the $\\lambda$-calculus\nwhere arguments can be superposed and must be linearly used. Hence it is a\nmodel for linear and non-deterministic programming languages, and the target\nlanguage of Ehrhard-Taylor expansion of $\\lambda$-terms. In a strictly typed\nrestriction of the Resource $\\lambda$-calculus, we study the notion of path\npersistence, and we define a Geometry of Interaction that characterises it, is\ninvariant under reduction, and counts addends in normal forms.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 02:28:03 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Solieri", "Marco", "", "LIPN, Paris 13, Sorbonne Paris Cit\u00e9, CNRS -- DISI,\n  Bologna, INRIA"]]}, {"id": "1502.05094", "submitter": "Melissa O'Neill", "authors": "Christopher A. Stone, Melissa E. O'Neill, Sonja A. Bohr, Adam M.\n  Cozzette, M. Joe DeBlasio, Julia Matsieva, Stuart A. Pernsteiner, Ari D.\n  Schumer", "title": "Observationally Cooperative Multithreading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite widespread interest in multicore computing, concur- rency models in\nmainstream languages often lead to subtle, error-prone code.\n  Observationally Cooperative Multithreading (OCM) is a new approach to\nshared-memory parallelism. Programmers write code using the well-understood\ncooperative (i.e., nonpreemptive) multithreading model for uniprocessors. OCM\nthen allows threads to run in parallel, so long as results remain consistent\nwith the cooperative model.\n  Programmers benefit because they can reason largely sequentially. Remaining\ninterthread interactions are far less chaotic than in other models, permitting\neasier reasoning and debugging. Programmers can also defer the choice of\nconcurrency-control mechanism (e.g., locks or transactions) until after they\nhave written their programs, at which point they can compare\nconcurrency-control strategies and choose the one that offers the best\nperformance. Implementers and researchers also benefit from the agnostic nature\nof OCM -- it provides a level of abstraction to investigate, compare, and\ncombine a variety of interesting concurrency-control techniques.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 01:06:43 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Stone", "Christopher A.", ""], ["O'Neill", "Melissa E.", ""], ["Bohr", "Sonja A.", ""], ["Cozzette", "Adam M.", ""], ["DeBlasio", "M. Joe", ""], ["Matsieva", "Julia", ""], ["Pernsteiner", "Stuart A.", ""], ["Schumer", "Ari D.", ""]]}, {"id": "1502.06286", "submitter": "Sayan Mitra", "authors": "Yixiao Lin and Sayan Mitra", "title": "StarL: Towards a Unified Framework for Programming, Simulating and\n  Verifying Distributed Robotic Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed StarL as a framework for programming, simulating, and verifying\ndistributed systems that interacts with physical processes. StarL framework has\n(a) a collection of distributed primitives for coordination, such as mutual\nexclusion, registration and geocast that can be used to build sophisticated\napplications, (b) theory libraries for verifying StarL applications in the PVS\ntheorem prover, and (c) an execution environment that can be used to deploy the\napplications on hardware or to execute them in a discrete event simulator. The\nprimitives have (i) abstract, nondeterministic specifications in terms of\ninvariants, and assume-guarantee style progress properties, (ii)\nimplementations in Java/Android that always satisfy the invariants and attempt\nprogress using best effort strategies. The PVS theories specify the invariant\nand progress properties of the primitives, and have to be appropriately\ninstantiated and composed with the application's state machine to prove\nproperties about the application. We have built two execution environments: one\nfor deploying applications on Android/iRobot Create platform and a second one\nfor simulating large instantiations of the applications in a discrete even\nsimulator. The capabilities are illustrated with a StarL application for\nvehicle to vehicle coordination in a automatic intersection that uses\nprimitives for point-to-point motion, mutual exclusion, and registration.\n", "versions": [{"version": "v1", "created": "Sun, 22 Feb 2015 23:05:54 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Lin", "Yixiao", ""], ["Mitra", "Sayan", ""]]}, {"id": "1502.07118", "submitter": "Andreas Holzer", "authors": "Andreas Haas, Thomas A. Henzinger, Andreas Holzer, Christoph M.\n  Kirsch, Michael Lippautz, Hannes Payer, Ali Sezgin, Ana Sokolova, Helmut\n  Veith", "title": "Local Linearizability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semantics of concurrent data structures is usually given by a sequential\nspecification and a consistency condition. Linearizability is the most popular\nconsistency condition due to its simplicity and general applicability.\nNevertheless, for applications that do not require all guarantees offered by\nlinearizability, recent research has focused on improving performance and\nscalability of concurrent data structures by relaxing their semantics.\n  In this paper, we present local linearizability, a relaxed consistency\ncondition that is applicable to container-type concurrent data structures like\npools, queues, and stacks. While linearizability requires that the effect of\neach operation is observed by all threads at the same time, local\nlinearizability only requires that for each thread T, the effects of its local\ninsertion operations and the effects of those removal operations that remove\nvalues inserted by T are observed by all threads at the same time. We\ninvestigate theoretical and practical properties of local linearizability and\nits relationship to many existing consistency conditions. We present a generic\nimplementation method for locally linearizable data structures that uses\nexisting linearizable data structures as building blocks. Our implementations\nshow performance and scalability improvements over the original building blocks\nand outperform the fastest existing container-type implementations.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 10:40:13 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 07:40:31 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2015 05:24:41 GMT"}, {"version": "v4", "created": "Fri, 24 Jun 2016 15:39:28 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Haas", "Andreas", ""], ["Henzinger", "Thomas A.", ""], ["Holzer", "Andreas", ""], ["Kirsch", "Christoph M.", ""], ["Lippautz", "Michael", ""], ["Payer", "Hannes", ""], ["Sezgin", "Ali", ""], ["Sokolova", "Ana", ""], ["Veith", "Helmut", ""]]}, {"id": "1502.07447", "submitter": "Mehmet Ali Arslan", "authors": "Mehmet Ali Arslan, Flavius Gruian, Krzysztof Kuchcinski", "title": "A Comparative Study of Scheduling Techniques for Multimedia Applications\n  on SIMD Pipelines", "comments": "Presented at DATE Friday Workshop on Heterogeneous Architectures and\n  Design Methods for Embedded Image Systems (HIS 2015) (arXiv:1502.07241)", "journal-ref": null, "doi": null, "report-no": "DATEHIS/2015/02", "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel architectures are essential in order to take advantage of the\nparallelism inherent in streaming applications. One particular branch of these\nemploy hardware SIMD pipelines. In this paper, we analyse several scheduling\ntechniques, namely ad hoc overlapped execution, modulo scheduling and modulo\nscheduling with unrolling, all of which aim to efficiently utilize the special\narchitecture design. Our investigation focuses on improving throughput while\nanalysing other metrics that are important for streaming applications, such as\nregister pressure, buffer sizes and code size. Through experiments conducted on\nseveral media benchmarks, we present and discuss trade-offs involved when\nselecting any one of these scheduling techniques.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 06:15:23 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Arslan", "Mehmet Ali", ""], ["Gruian", "Flavius", ""], ["Kuchcinski", "Krzysztof", ""]]}, {"id": "1502.07448", "submitter": "Frank Hannig", "authors": "Oliver Reiche, Konrad H\\\"aublein, Marc Reichenbach, Frank Hannig,\n  J\\\"urgen Teich, Dietmar Fey", "title": "Automatic Optimization of Hardware Accelerators for Image Processing", "comments": "Presented at DATE Friday Workshop on Heterogeneous Architectures and\n  Design Methods for Embedded Image Systems (HIS 2015) (arXiv:1502.07241)", "journal-ref": null, "doi": null, "report-no": "DATEHIS/2015/03", "categories": "cs.PL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the domain of image processing, often real-time constraints are required.\nIn particular, in safety-critical applications, such as X-ray computed\ntomography in medical imaging or advanced driver assistance systems in the\nautomotive domain, timing is of utmost importance. A common approach to\nmaintain real-time capabilities of compute-intensive applications is to offload\nthose computations to dedicated accelerator hardware, such as Field\nProgrammable Gate Arrays (FPGAs). Programming such architectures is a\nchallenging task, with respect to the typical FPGA-specific design criteria:\nAchievable overall algorithm latency and resource usage of FPGA primitives\n(BRAM, FF, LUT, and DSP). High-Level Synthesis (HLS) dramatically simplifies\nthis task by enabling the description of algorithms in well-known higher\nlanguages (C/C++) and its automatic synthesis that can be accomplished by HLS\ntools. However, algorithm developers still need expert knowledge about the\ntarget architecture, in order to achieve satisfying results. Therefore, in\nprevious work, we have shown that elevating the description of image algorithms\nto an even higher abstraction level, by using a Domain-Specific Language (DSL),\ncan significantly cut down the complexity for designing such algorithms for\nFPGAs. To give the developer even more control over the common trade-off,\nlatency vs. resource usage, we will present an automatic optimization process\nwhere these criteria are analyzed and fed back to the DSL compiler, in order to\ngenerate code that is closer to the desired design specifications. Finally, we\ngenerate code for stereo block matching algorithms and compare it with\nhandwritten implementations to quantify the quality of our results.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 06:16:51 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Reiche", "Oliver", ""], ["H\u00e4ublein", "Konrad", ""], ["Reichenbach", "Marc", ""], ["Hannig", "Frank", ""], ["Teich", "J\u00fcrgen", ""], ["Fey", "Dietmar", ""]]}, {"id": "1502.07639", "submitter": "Viktor Vafeiadis", "authors": "Soham Chakraborty (MPI-SWS), Thomas A. Henzinger (IST Austria), Ali\n  Sezgin (University of Cambridge), Viktor Vafeiadis (MPI-SWS)", "title": "Aspect-oriented linearizability proofs", "comments": "33 pages, LMCS", "journal-ref": "Logical Methods in Computer Science, Volume 11, Issue 1 (April 1,\n  2015) lmcs:1051", "doi": "10.2168/LMCS-11(1:20)2015", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linearizability of concurrent data structures is usually proved by monolithic\nsimulation arguments relying on the identification of the so-called\nlinearization points. Regrettably, such proofs, whether manual or automatic,\nare often complicated and scale poorly to advanced non-blocking concurrency\npatterns, such as helping and optimistic updates. In response, we propose a\nmore modular way of checking linearizability of concurrent queue algorithms\nthat does not involve identifying linearization points. We reduce the task of\nproving linearizability with respect to the queue specification to establishing\nfour basic properties, each of which can be proved independently by simpler\narguments. As a demonstration of our approach, we verify the Herlihy and Wing\nqueue, an algorithm that is challenging to verify by a simulation proof.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 17:17:23 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2015 18:08:10 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Chakraborty", "Soham", "", "MPI-SWS"], ["Henzinger", "Thomas A.", "", "IST Austria"], ["Sezgin", "Ali", "", "University of Cambridge"], ["Vafeiadis", "Viktor", "", "MPI-SWS"]]}]