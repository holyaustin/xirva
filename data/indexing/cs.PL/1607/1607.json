[{"id": "1607.00825", "submitter": "Stefan Richthofer", "authors": "Stefan Richthofer", "title": "Garbage Collection in JyNI - How to bridge Mark/Sweep and Reference\n  Counting GC", "comments": null, "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2015-01", "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Jython is a Java-based Python implementation and the most seamless way to\nintegrate Python and Java. It achieves high efficiency by compiling Python code\nto Java bytecode and thus letting Java's JIT optimize it - an approach that\nenables Python code to call Java functions or to subclass Java classes. It\nenables Python code to leverage Java's multithreading features and utilizes\nJava's built-in garbage collection (GC). However, it currently does not support\nCPython's C-API and thus does not support native extensions like NumPy and\nSciPy. Since most scientific code depends on such extensions, it is not\nrunnable with Jython. Jython Native Interface (JyNI) is a compatibility layer\nthat aims to provide CPython's native C extension API on top of Jython. JyNI is\nimplemented using the Java Native Interface (JNI) and its native part is\ndesigned to be binary compatible with existing extension builds [...].\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 19:11:01 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Richthofer", "Stefan", ""]]}, {"id": "1607.01590", "submitter": "Ulrich Neumerkel", "authors": "Ulrich Neumerkel and Stefan Kral", "title": "Indexing dif/2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Prolog programs are unnecessarily impure because of inadequate means to\nexpress syntactic inequality. While the frequently provided built-in `dif/2` is\nable to correctly describe expected answers, its direct use in programs often\nleads to overly complex and inefficient definitions --- mainly due to the lack\nof adequate indexing mechanisms. We propose to overcome these problems by using\na new predicate that subsumes both equality and inequality via reification.\nCode complexity is reduced with a monotonic, higher-order if-then-else\nconstruct based on `call/N`. For comparable correct uses of impure definitions,\nour approach is as determinate and similarly efficient as its impure\ncounterparts.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 12:27:45 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Neumerkel", "Ulrich", ""], ["Kral", "Stefan", ""]]}, {"id": "1607.01835", "submitter": "EPTCS", "authors": "Geoff Hamilton (Dublin City University, Republic of Ireland), Alexei\n  Lisitsa (The University of Liverpool, UK), Andrei P. Nemytykh (Program\n  Systems Institute of RAS, Russia)", "title": "Proceedings of the Fourth International Workshop on Verification and\n  Program Transformation", "comments": null, "journal-ref": "EPTCS 216, 2016", "doi": "10.4204/EPTCS.216", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the revised versions of papers presented at the Fourth\nInternational Workshop on Verification and Program Transformation (VPT 2016) on\nApril 2, 2016 in Eindhoven, The Netherlands. The workshop is an event of the\nEuropean Joint Conferences on Theory and Practice of Software (ETAPS 2016).\n  The aim of the VPT workshops is to provide a forum where people from the area\nof program transformation and the area of program verification can fruitfully\nexchange ideas and gain a deeper understanding of the interactions between\nthose two fields. The research papers which have been recently published in\nthose fields, show that the interactions are very beneficial and, indeed, go\nboth ways. In one direction, methods and tools developed in the field of\nprogram transformation, such as partial deduction, partial evaluation,\nfold/unfold transformations, and supercompilation, have all been applied with\nsuccess for the verification of systems, and in particular, the verification of\ninfinite state and parameterized systems. In the other direction, methods\ndeveloped in program verification, such as model checking, abstract\ninterpretation, SAT and SMT solving, and automated theorem proving, have been\nused to enhance program transformation techniques, thereby making these\ntechniques more powerful and useful in practice.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 22:46:53 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Hamilton", "Geoff", "", "Dublin City University, Republic of Ireland"], ["Lisitsa", "Alexei", "", "The University of Liverpool, UK"], ["Nemytykh", "Andrei P.", "", "Program\n  Systems Institute of RAS, Russia"]]}, {"id": "1607.01993", "submitter": "Nikos Gorogiannis", "authors": "James Brotherston, Nikos Gorogiannis and Max Kanovich", "title": "Biabduction (and Related Problems) in Array Separation Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.DS cs.PL math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate array separation logic (ASL), a variant of symbolic-heap\nseparation logic in which the data structures are either pointers or arrays,\ni.e., contiguous blocks of allocated memory. This logic provides a language for\ncompositional memory safety proofs of imperative array programs.\n  We focus on the biabduction problem for this logic, which has been\nestablished as the key to automatic specification inference at the industrial\nscale. We present an NP decision procedure for biabduction in ASL that produces\nsolutions of reasonable quality, and we also show that the problem of finding a\nconsistent solution is NP-hard.\n  Along the way, we study satisfiability and entailment in our logic, giving\ndecision procedures and complexity bounds for both problems. We show\nsatisfiability to be NP-complete, and entailment to be decidable with high\ncomplexity. The somewhat surprising fact that biabduction is much simpler than\nentailment is explained by the fact that, as we show, the element of choice\nover biabduction solutions enables us to dramatically reduce the search space.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 12:49:04 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 21:44:27 GMT"}, {"version": "v3", "created": "Fri, 18 Nov 2016 11:20:20 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Brotherston", "James", ""], ["Gorogiannis", "Nikos", ""], ["Kanovich", "Max", ""]]}, {"id": "1607.02225", "submitter": "EPTCS", "authors": "Gerg\\\"o Barany (CEA, LIST, Software Reliability Laboratory)", "title": "Hybrid Information Flow Analysis for Programs with Arrays", "comments": "In Proceedings VPT 2016, arXiv:1607.01835", "journal-ref": "EPTCS 216, 2016, pp. 5-23", "doi": "10.4204/EPTCS.216.1", "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information flow analysis checks whether certain pieces of (confidential)\ndata may affect the results of computations in unwanted ways and thus leak\ninformation. Dynamic information flow analysis adds instrumentation code to the\ntarget software to track flows at run time and raise alarms if a flow policy is\nviolated; hybrid analyses combine this with preliminary static analysis.\n  Using a subset of C as the target language, we extend previous work on hybrid\ninformation flow analysis that handled pointers to scalars. Our extended\nformulation handles arrays, pointers to array elements, and pointer arithmetic.\nInformation flow through arrays of pointers is tracked precisely while arrays\nof non-pointer types are summarized efficiently.\n  A prototype of our approach is implemented using the Frama-C program analysis\nand transformation framework. Work on a full machine-checked proof of the\ncorrectness of our approach using Isabelle/HOL is well underway; we present the\nexisting parts and sketch the rest of the correctness argument.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 05:30:24 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Barany", "Gerg\u00f6", "", "CEA, LIST, Software Reliability Laboratory"]]}, {"id": "1607.02226", "submitter": "EPTCS", "authors": "Julien Cohen (Universit\\'e de Nantes)", "title": "Renaming Global Variables in C Mechanically Proved Correct", "comments": "In Proceedings VPT 2016, arXiv:1607.01835", "journal-ref": "EPTCS 216, 2016, pp. 50-64", "doi": "10.4204/EPTCS.216.3", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most integrated development environments are shipped with refactoring tools.\nHowever, their refactoring operations are often known to be unreliable. As a\nconsequence, developers have to test their code after applying an automatic\nrefactoring. In this article, we consider a refactoring operation (renaming of\nglobal variables in C), and we prove that its core implementation preserves the\nset of possible behaviors of transformed programs. That proof of correctness\nrelies on the operational semantics of C provided by CompCert C in Coq.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 05:30:43 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Cohen", "Julien", "", "Universit\u00e9 de Nantes"]]}, {"id": "1607.02228", "submitter": "EPTCS", "authors": "D\\'aniel Horp\\'acsi, Judit K\\H{o}szegi, Simon Thompson", "title": "Towards Trustworthy Refactoring in Erlang", "comments": "In Proceedings VPT 2016, arXiv:1607.01835", "journal-ref": "EPTCS 216, 2016, pp. 83-103", "doi": "10.4204/EPTCS.216.5", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tool-assisted refactoring transformations must be trustworthy if programmers\nare to be confident in applying them on arbitrarily extensive and complex code\nin order to improve style or efficiency. We propose a simple, high-level but\nrigorous, notation for defining refactoring transformations in Erlang, and show\nthat this notation provides an extensible, verifiable and executable\nspecification language for refactoring. To demonstrate the applicability of our\napproach, we show how to define and verify a number of example refactorings in\nthe system.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 05:31:02 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Horp\u00e1csi", "D\u00e1niel", ""], ["K\u0151szegi", "Judit", ""], ["Thompson", "Simon", ""]]}, {"id": "1607.02229", "submitter": "EPTCS", "authors": "Venkatesh Kannan (School of Computing, Dublin City University,\n  Ireland), G. W. Hamilton (School of Computing, Dublin City University,\n  Ireland)", "title": "Program Transformation to Identify List-Based Parallel Skeletons", "comments": "In Proceedings VPT 2016, arXiv:1607.01835", "journal-ref": "EPTCS 216, 2016, pp. 118-136", "doi": "10.4204/EPTCS.216.7", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic skeletons are used as building-blocks to ease the task of\nparallel programming by abstracting the details of parallel implementation from\nthe developer. Most existing libraries provide implementations of skeletons\nthat are defined over flat data types such as lists or arrays. However,\nskeleton-based parallel programming is still very challenging as it requires\nintricate analysis of the underlying algorithm and often uses inefficient\nintermediate data structures. Further, the algorithmic structure of a given\nprogram may not match those of list-based skeletons. In this paper, we present\na method to automatically transform any given program to one that is defined\nover a list and is more likely to contain instances of list-based skeletons.\nThis facilitates the parallel execution of a transformed program using existing\nimplementations of list-based parallel skeletons. Further, by using an existing\ntransformation called distillation in conjunction with our method, we produce\ntransformed programs that contain fewer inefficient intermediate data\nstructures.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 05:31:26 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Kannan", "Venkatesh", "", "School of Computing, Dublin City University,\n  Ireland"], ["Hamilton", "G. W.", "", "School of Computing, Dublin City University,\n  Ireland"]]}, {"id": "1607.02230", "submitter": "EPTCS", "authors": "Antonina Nepeivoda", "title": "Turchin's Relation for Call-by-Name Computations: A Formal Approach", "comments": "In Proceedings VPT 2016, arXiv:1607.01835", "journal-ref": "EPTCS 216, 2016, pp. 137-159", "doi": "10.4204/EPTCS.216.8", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supercompilation is a program transformation technique that was first\ndescribed by V. F. Turchin in the 1970s. In supercompilation, Turchin's\nrelation as a similarity relation on call-stack configurations is used both for\ncall-by-value and call-by-name semantics to terminate unfolding of the program\nbeing transformed. In this paper, we give a formal grammar model of\ncall-by-name stack behaviour. We classify the model in terms of the Chomsky\nhierarchy and then formally prove that Turchin's relation can terminate all\ncomputations generated by the model.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 05:31:36 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Nepeivoda", "Antonina", ""]]}, {"id": "1607.02231", "submitter": "EPTCS", "authors": "Mirko Viroli (University of Bologna, Cesena, Italy), Jacob Beal\n  (Raytheon BBN Technologies Cambridge, MA, USA)", "title": "Resiliency with Aggregate Computing: State of the Art and Roadmap", "comments": "In Proceedings FORECAST 2016, arXiv:1607.02001", "journal-ref": "EPTCS 217, 2016, pp. 5-18", "doi": "10.4204/EPTCS.217.3", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the difficulties in developing collective adaptive systems is the\nchallenge of simultaneously engineering both the desired resilient behaviour of\nthe collective and the details of its implementation on individual devices.\nAggregate computing simplifies this problem by separating these aspects into\ndifferent layers of abstraction by means of a unifying notion of computational\nfield and a functional computational model. We review the state of the art in\naggregate computing, discuss the various resiliency properties it supports, and\ndevelop a roadmap of foundational problems still needing to be addressed in the\ncontinued development of this emerging discipline.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 05:35:59 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Viroli", "Mirko", "", "University of Bologna, Cesena, Italy"], ["Beal", "Jacob", "", "Raytheon BBN Technologies Cambridge, MA, USA"]]}, {"id": "1607.02238", "submitter": "Duc-Hiep Chu", "authors": "Duc-Hiep Chu, Joxan Jaffar, Vijayaraghavan Murali", "title": "Incremental Quantitative Analysis on Dynamic Costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In quantitative program analysis, values are assigned to execution traces to\nrepresent a quality measure. Such analyses cover important applications, e.g.\nresource usage. Examining all traces is well known to be intractable and\ntherefore traditional algorithms reason over an over-approximated set.\nTypically, inaccuracy arises due to inclusion of infeasible paths in this set.\nThus path-sensitivity is one cure. However, there is another reason for the\ninaccuracy: that the cost model, i.e., the way in which the analysis of each\ntrace is quantified, is dynamic. That is, the cost of a trace is dependent on\nthe context in which the trace is executed. Thus the goal of accurate analysis,\nalready challenged by path-sensitivity, is now further challenged by\ncontext-sensitivity.\n  In this paper, we address the problem of quantitative analysis defined over a\ndynamic cost model. Our algorithm is an \"anytime\" algorithm: it generates an\nanswer quickly, but if the analysis resource budget allows, it progressively\nproduces better solutions via refinement iterations. The result of each\niteration remains sound, but importantly, must converge to an exact analysis\nwhen given an unlimited resource budget. In order to be scalable, our algorithm\nis designed to be incremental. We finally give evidence that a new level of\npracticality is achieved by an evaluation on a realistic collection of\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 05:48:14 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Chu", "Duc-Hiep", ""], ["Jaffar", "Joxan", ""], ["Murali", "Vijayaraghavan", ""]]}, {"id": "1607.02902", "submitter": "Yewen Pu", "authors": "Yewen Pu, Karthik Narasimhan, Armando Solar-Lezama, Regina Barzilay", "title": "sk_p: a neural program corrector for MOOCs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel technique for automatic program correction in MOOCs,\ncapable of fixing both syntactic and semantic errors without manual, problem\nspecific correction strategies. Given an incorrect student program, it\ngenerates candidate programs from a distribution of likely corrections, and\nchecks each candidate for correctness against a test suite.\n  The key observation is that in MOOCs many programs share similar code\nfragments, and the seq2seq neural network model, used in the natural-language\nprocessing task of machine translation, can be modified and trained to recover\nthese fragments.\n  Experiment shows our scheme can correct 29% of all incorrect submissions and\nout-performs state of the art approach which requires manual, problem specific\ncorrection strategies.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 11:08:00 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Pu", "Yewen", ""], ["Narasimhan", "Karthik", ""], ["Solar-Lezama", "Armando", ""], ["Barzilay", "Regina", ""]]}, {"id": "1607.02927", "submitter": "Silvia Crafa", "authors": "Silvia Crafa and Luca Padovani", "title": "On the chemistry of typestate-oriented actors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typestate-oriented programming is an extension of the OO paradigm in which\nobjects are modeled not just in terms of interfaces but also in terms of their\nusage protocols, describing legal sequences of method calls, possibly depending\non the object's internal state. We argue that the Actor Model allows\ntypestate-OOP in an inherently distributed setting, whereby objects/actors can\nbe accessed concurrently by several processes, and local entities cooperate to\ncarry out a communication protocol. In this article we illustrate the approach\nby means of a number of examples written in Scala Akka. We show that Scala's\nabstractions support clean and natural typestate-oriented actor programming\nwith the usual asynchronous and non-blocking semantics. We also show that the\nstandard type system of Scala and a typed wrapping of usual (untyped) Akka's\nActorRef are enough to provide rich forms of type safety so that well-typed\nactors respect their intended communication protocols. This approach draws on a\nsolid theoretical background, consisting of a sound behavioral type system for\nthe Join Calculus, that is a foundational calculus of distributed asynchronous\nprocesses whose semantics is based on the Chemical Abstract Machine, that\nunveiled its strong connections with typestate-oriented programming of both\nconcurrent objects and actors.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 12:49:23 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Crafa", "Silvia", ""], ["Padovani", "Luca", ""]]}, {"id": "1607.02963", "submitter": "EPTCS", "authors": "Natalia Zo\\'n, Vashti Galpin, Stephen Gilmore", "title": "Modelling movement for collective adaptive systems with CARMA", "comments": "In Proceedings FORECAST 2016, arXiv:1607.02001", "journal-ref": "EPTCS 217, 2016, pp. 43-52", "doi": "10.4204/EPTCS.217.6", "report-no": null, "categories": "cs.MA cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Space and movement through space play an important role in many collective\nadaptive systems (CAS). CAS consist of multiple components interacting to\nachieve some goal in a system or environment that can change over time. When\nthese components operate in space, then their behaviour can be affected by\nwhere they are located in that space. Examples include the possibility of\ncommunication between two components located at different points, and rates of\nmovement of a component that may be affected by location. The CARMA language\nand its associated software tools can be used to model such systems. In\nparticular, a graphical editor for CARMA allows for the specification of\nspatial structure and generation of templates that can be used in a CARMA model\nwith space. We demonstrate the use of this tool to experiment with a model of\npedestrian movement over a network of paths.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 05:36:27 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Zo\u0144", "Natalia", ""], ["Galpin", "Vashti", ""], ["Gilmore", "Stephen", ""]]}, {"id": "1607.03445", "submitter": "Nadia Polikarpova", "authors": "Nadia Polikarpova and Deian Stefan and Jean Yang and Shachar Itzhaky\n  and Travis Hance and Armando Solar-Lezama", "title": "Liquid Information Flow Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Lifty, a domain-specific language for data-centric applications\nthat manipulate sensitive data. A Lifty programmer annotates the sources of\nsensitive data with declarative security policies, and the language statically\nand automatically verifies that the application handles the data according to\nthe policies. Moreover, if verification fails, Lifty suggests a provably\ncorrect repair, thereby easing the programmer burden of implementing policy\nenforcing code throughout the application.\n  The main insight behind Lifty is to encode information flow control using\nliquid types, an expressive yet decidable type system. Liquid types enable\nfully automatic checking of complex, data dependent policies, and power our\nrepair mechanism via type-driven error localization and patch synthesis. Our\nexperience using Lifty to implement three case studies from the literature\nshows that (1) the Lifty policy language is sufficiently expressive to specify\nmany real-world policies, (2) the Lifty type checker is able to verify secure\nprograms and find leaks in insecure programs quickly, and (3) even if the\nprogrammer leaves out all policy enforcing code, the Lifty repair engine is\nable to patch all leaks automatically within a reasonable time.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 17:43:54 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 20:59:00 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 01:18:23 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Polikarpova", "Nadia", ""], ["Stefan", "Deian", ""], ["Yang", "Jean", ""], ["Itzhaky", "Shachar", ""], ["Hance", "Travis", ""], ["Solar-Lezama", "Armando", ""]]}, {"id": "1607.03455", "submitter": "Justin Hsu", "authors": "Gilles Barthe, Benjamin Gr\\'egoire, Justin Hsu, Pierre-Yves Strub", "title": "Coupling proofs are probabilistic product programs", "comments": null, "journal-ref": null, "doi": "10.1145/3009837.3009896", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Couplings are a powerful mathematical tool for reasoning about pairs of\nprobabilistic processes. Recent developments in formal verification identify a\nclose connection between couplings and pRHL, a relational program logic\nmotivated by applications to provable security, enabling formal construction of\ncouplings from the probability theory literature. However, existing work using\npRHL merely shows existence of a coupling and does not give a way to prove\nquantitative properties about the coupling, which are need to reason about\nmixing and convergence of probabilistic processes. Furthermore, pRHL is\ninherently incomplete, and is not able to capture some advanced forms of\ncouplings such as shift couplings. We address both problems as follows.\n  First, we define an extension of pRHL, called xpRHL, which explicitly\nconstructs the coupling in a pRHL derivation in the form of a probabilistic\nproduct program that simulates two correlated runs of the original program.\nExisting verification tools for probabilistic programs can then be directly\napplied to the probabilistic product to prove quantitative properties of the\ncoupling. Second, we equip pRHL with a new rule for while loops, where\nreasoning can freely mix synchronized and unsynchronized loop iterations. Our\nproof rule can capture examples of shift couplings, and the logic is relatively\ncomplete for deterministic programs.\n  We show soundness of xpRHL and use it to analyze two classes of examples.\nFirst, we verify rapid mixing using different tools from coupling: standard\ncoupling, shift coupling, and path coupling, a compositional principle for\ncombining local couplings into a global coupling. Second, we verify\n(approximate) equivalence between a source and an optimized program for several\ninstances of loop optimizations from the literature.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 18:19:22 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 14:21:33 GMT"}, {"version": "v3", "created": "Sun, 24 Jul 2016 14:33:13 GMT"}, {"version": "v4", "created": "Tue, 26 Jul 2016 10:27:12 GMT"}, {"version": "v5", "created": "Mon, 7 Nov 2016 17:19:19 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Barthe", "Gilles", ""], ["Gr\u00e9goire", "Benjamin", ""], ["Hsu", "Justin", ""], ["Strub", "Pierre-Yves", ""]]}, {"id": "1607.03733", "submitter": "EPTCS", "authors": "Stephen Gilmore", "title": "Data as processes: introducing measurement data into CARMA models", "comments": "In Proceedings FORECAST 2016, arXiv:1607.02001", "journal-ref": "EPTCS 217, 2016, pp. 31-42", "doi": "10.4204/EPTCS.217.5", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measurement data provides a precise and detailed description of components\nwithin a complex system but it is rarely used directly as a component of a\nsystem model. In this paper we introduce a model-based representation of\nmeasurement data and use it together with modeller-defined components expressed\nin the CARMA modelling language. We assess both liveness and safety properties\nof these models with embedded data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 05:36:18 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Gilmore", "Stephen", ""]]}, {"id": "1607.04033", "submitter": "EPTCS", "authors": "John P. Gallagher (Roskilde University), Philipp R\\\"ummer (Uppsala\n  University)", "title": "Proceedings 3rd Workshop on Horn Clauses for Verification and Synthesis", "comments": null, "journal-ref": "EPTCS 219, 2016", "doi": "10.4204/EPTCS.219", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of HCVS 2016, the Third Workshop on Horn\nClauses for Verification and Synthesis which was held on April 3, 2016 in\nEindhoven, The Netherlands as a satellite event of the European Joint\nConferences on Theory and Practice of Software (ETAPS 2016). Many program\nverification and synthesis problems of interest can be modeled directly using\nHorn clauses and many recent advances in the CLP and CAV communities have\ncentered around efficiently solving problems presented as Horn clauses. The\nThird Workshop on Horn Clauses for Verification and Synthesis was organised\nwith the aim to bring together researchers working in the two communities of\nConstraint/Logic Programming and Program Verification on the topic of Horn\nclause based analysis, verification and synthesis. Horn clauses for\nverification and synthesis have been advocated by these two communities in\ndifferent times and from different perspectives, and this workshop is organized\nto stimulate interaction and a fruitful exchange and integration of\nexperiences.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 08:46:44 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Gallagher", "John P.", "", "Roskilde University"], ["R\u00fcmmer", "Philipp", "", "Uppsala\n  University"]]}, {"id": "1607.04104", "submitter": "James Cheney", "authors": "Stefan Fehrenbach and James Cheney", "title": "Language-integrated provenance", "comments": "Accepted to Science of Computer Programming special issue on PPDP\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Provenance, or information about the origin or derivation of data, is\nimportant for assessing the trustworthiness of data and identifying and\ncorrecting mistakes. Most prior implementations of data provenance have\ninvolved heavyweight modifications to database systems and little attention has\nbeen paid to how the provenance data can be used outside such a system. We\npresent extensions to the Links programming language that build on its support\nfor language-integrated query to support provenance queries by rewriting and\nnormalizing monadic comprehensions and extending the type system to distinguish\nprovenance metadata from normal data. The main contribution of this article is\nto show that the two most common forms of provenance can be implemented\nefficiently and used safely as a programming language feature with no changes\nto the database system.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 12:20:11 GMT"}, {"version": "v2", "created": "Sun, 18 Sep 2016 19:42:19 GMT"}, {"version": "v3", "created": "Fri, 18 Aug 2017 15:57:54 GMT"}, {"version": "v4", "created": "Tue, 22 Aug 2017 09:42:04 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Fehrenbach", "Stefan", ""], ["Cheney", "James", ""]]}, {"id": "1607.04180", "submitter": "Cyrus Omar", "authors": "Cyrus Omar, Ian Voysey, Michael Hilton, Jonathan Aldrich, Matthew A.\n  Hammer", "title": "Hazelnut: A Bidirectionally Typed Structure Editor Calculus", "comments": null, "journal-ref": "Proceedings of the 44th ACM SIGPLAN Symposium on Principles of\n  Programming Languages (POPL 2017)", "doi": "10.1145/3093333.3009900", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure editors allow programmers to edit the tree structure of a program\ndirectly. This can have cognitive benefits, particularly for novice and\nend-user programmers. It also simplifies matters for tool designers, because\nthey do not need to contend with malformed program text. This paper defines\nHazelnut, a structure editor based on a small bidirectionally typed lambda\ncalculus extended with holes and a cursor. Hazelnut goes one step beyond\nsyntactic well-formedness: its edit actions operate over statically meaningful\nincomplete terms. Naively, this would force the programmer to construct terms\nin a rigid \"outside-in\" manner. To avoid this problem, the action semantics\nautomatically places terms assigned a type that is inconsistent with the\nexpected type inside a hole. This safely defers the type consistency check\nuntil the term inside the hole is finished. Hazelnut is a foundational\ntype-theoretic account of typed structure editing, rather than an end-user tool\nitself. To that end, we describe how Hazelnut's rich metatheory, which we have\nmechanized in Agda, guides the definition of an extension to the calculus. We\nalso discuss various plausible evaluation strategies for terms with holes, and\nin so doing reveal connections with gradual typing and contextual modal type\ntheory, the Curry-Howard interpretation of contextual modal logic. Finally, we\ndiscuss how Hazelnut's semantics lends itself to implementation as a functional\nreactive program. Our reference implementation is written using js_of_ocaml.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 16:07:29 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2016 09:39:21 GMT"}, {"version": "v3", "created": "Thu, 13 Oct 2016 12:00:55 GMT"}, {"version": "v4", "created": "Wed, 7 Dec 2016 20:45:12 GMT"}, {"version": "v5", "created": "Mon, 4 Feb 2019 02:12:51 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Omar", "Cyrus", ""], ["Voysey", "Ian", ""], ["Hilton", "Michael", ""], ["Aldrich", "Jonathan", ""], ["Hammer", "Matthew A.", ""]]}, {"id": "1607.04197", "submitter": "Romain Vernoux", "authors": "Romain Vernoux", "title": "Design of an intermediate representation for query languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data oriented applications, usually written in a high-level, general-purpose\nprogramming language (such as Java) interact with database through a coarse\ninterface. Informally, the text of a query is built on the application side\n(either via plain string concatenation or through an abstract notion of\nstatement) and shipped to the database over the wire where it is executed. The\nresults are then serialized and sent back to the \"client-code\" where they are\ntranslated in the language's native datatypes. This round trip is detrimental\nto performances but, worse, such a programming model prevents one from having\nricher queries, namely queries containing user-defined functions (that is\nfunctions defined by the programmer and used e.g. in the filter condition of a\nSQL query). While some databases also possess a \"server-side\" language (e.g.\nPL/SQL in Oracle database), its integration with the very-optimized query\nexecution engine is still minimal and queries containing (PL/SQL) user-defined\nfunctions remain notoriously inefficient. In this setting, we reviewed existing\nlanguage-integrated query frameworks, highlighting that existing database query\nlanguages (including SQL) share high-level querying primitives (e.g.,\nfiltering, joins, aggregation) that can be represented by operators, but differ\nwidely regarding the semantics of their expression language. In order to\nrepresent queries in an application language- and database-agnostic manner, we\ndesigned a small calculus, dubbed \"QIR\" for Query Intermediate Representation.\nQIR contains expressions, corresponding to a small extension of the pure\nlambda-calculus, and operators to represent usual querying primitives. In the\neffort to send efficient queries to the database, we abstracted the idea of\n\"good\" query representations in a measure on QIR terms. Then, we designed an\nevaluation strategy rewriting QIR query representations into \"better\" ones.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 16:37:17 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Vernoux", "Romain", ""]]}, {"id": "1607.04332", "submitter": "Mathys Rennela", "authors": "Mathys Rennela", "title": "Convexity and Order in Probabilistic Call-by-Name FPC", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 16, Issue 4 (November\n  12, 2020) lmcs:6901", "doi": "10.23638/LMCS-16(4:10)2020", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kegelspitzen are mathematical structures coined by Keimel and Plotkin, in\norder to encompass the structure of a convex set and the structure of a dcpo.\nIn this paper, we ask ourselves what are Kegelspitzen the model of. We adopt a\ncategorical viewpoint and show that Kegelspitzen model stochastic matrices onto\na category of domains. Consequently, Kegelspitzen form a denotational model of\npPCF, an abstract functional programming language for probabilistic computing.\nWe conclude the present work with a discussion of the interpretation of\n(probabilistic) recursive types, which are types for entities which might\ncontain other entities of the same type, such as lists and trees.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 21:45:23 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 17:44:48 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 20:15:26 GMT"}, {"version": "v4", "created": "Tue, 29 Sep 2020 17:51:23 GMT"}, {"version": "v5", "created": "Wed, 11 Nov 2020 16:54:33 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Rennela", "Mathys", ""]]}, {"id": "1607.04457", "submitter": "EPTCS", "authors": "Pierre-Lo\\\"ic Garoche (DTIM, UFT, Onera - The French Aerospace Lab),\n  Temesghen Kahsai (Nasa Ames / CMU), Xavier Thirioux (IRIT/ENSEEIHT, UFT,\n  CNRS)", "title": "Hierarchical State Machines as Modular Horn Clauses", "comments": "In Proceedings HCVS2016, arXiv:1607.04033", "journal-ref": "EPTCS 219, 2016, pp. 15-28", "doi": "10.4204/EPTCS.219.2", "report-no": null, "categories": "cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In model based development, embedded systems are modeled using a mix of\ndataflow formalism, that capture the flow of computation, and hierarchical\nstate machines, that capture the modal behavior of the system. For safety\nanalysis, existing approaches rely on a compilation scheme that transform the\noriginal model (dataflow and state machines) into a pure dataflow formalism.\nSuch compilation often result in loss of important structural information that\ncapture the modal behaviour of the system. In previous work we have developed a\ncompilation technique from a dataflow formalism into modular Horn clauses. In\nthis paper, we present a novel technique that faithfully compile hierarchical\nstate machines into modular Horn clauses. Our compilation technique preserves\nthe structural and modal behavior of the system, making the safety analysis of\nsuch models more tractable.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 11:04:49 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Garoche", "Pierre-Lo\u00efc", "", "DTIM, UFT, Onera - The French Aerospace Lab"], ["Kahsai", "Temesghen", "", "Nasa Ames / CMU"], ["Thirioux", "Xavier", "", "IRIT/ENSEEIHT, UFT,\n  CNRS"]]}, {"id": "1607.04459", "submitter": "EPTCS", "authors": "Bishoksan Kafle (Roskilde University), John P. Gallagher (Roskilde\n  University and IMDEA Software Institute), Pierre Ganty (IMDEA Software\n  Institute, Spain)", "title": "Solving non-linear Horn clauses using a linear Horn clause solver", "comments": "In Proceedings HCVS2016, arXiv:1607.04033", "journal-ref": "EPTCS 219, 2016, pp. 33-48", "doi": "10.4204/EPTCS.219.4", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that checking satisfiability of a set of non-linear\nHorn clauses (also called a non-linear Horn clause program) can be achieved\nusing a solver for linear Horn clauses. We achieve this by interleaving a\nprogram transformation with a satisfiability checker for linear Horn clauses\n(also called a solver for linear Horn clauses). The program transformation is\nbased on the notion of tree dimension, which we apply to a set of non-linear\nclauses, yielding a set whose derivation trees have bounded dimension. Such a\nset of clauses can be linearised. The main algorithm then proceeds by applying\nthe linearisation transformation and solver for linear Horn clauses to a\nsequence of sets of clauses with successively increasing dimension bound. The\napproach is then further developed by using a solution of clauses of lower\ndimension to (partially) linearise clauses of higher dimension. We constructed\na prototype implementation of this approach and performed some experiments on a\nset of verification problems, which shows some promise.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 11:05:08 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Kafle", "Bishoksan", "", "Roskilde University"], ["Gallagher", "John P.", "", "Roskilde\n  University and IMDEA Software Institute"], ["Ganty", "Pierre", "", "IMDEA Software\n  Institute, Spain"]]}, {"id": "1607.04460", "submitter": "EPTCS", "authors": "Emanuele De Angelis, Fabio Fioravanti, Alberto Pettorossi, Maurizio\n  Proietti", "title": "Removing Unnecessary Variables from Horn Clause Verification Conditions", "comments": "In Proceedings HCVS2016, arXiv:1607.04033", "journal-ref": "EPTCS 219, 2016, pp. 49-55", "doi": "10.4204/EPTCS.219.5", "report-no": null, "categories": "cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification conditions (VCs) are logical formulas whose satisfiability\nguarantees program correctness. We consider VCs in the form of constrained Horn\nclauses (CHC) which are automatically generated from the encoding of (an\ninterpreter of) the operational semantics of the programming language. VCs are\nderived through program specialization based on the unfold/fold transformation\nrules and, as it often happens when specializing interpreters, they contain\nunnecessary variables, that is, variables which are not required for the\ncorrectness proofs of the programs under verification. In this paper we adapt\nto the CHC setting some of the techniques that were developed for removing\nunnecessary variables from logic programs, and we show that, in some cases, the\napplication of these techniques increases the effectiveness of Horn clause\nsolvers when proving program correctness.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 11:05:17 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["De Angelis", "Emanuele", ""], ["Fioravanti", "Fabio", ""], ["Pettorossi", "Alberto", ""], ["Proietti", "Maurizio", ""]]}, {"id": "1607.04461", "submitter": "EPTCS", "authors": "Gabriele Paganelli", "title": "Horn Binary Serialization Analysis", "comments": "In Proceedings HCVS2016, arXiv:1607.04033", "journal-ref": "EPTCS 219, 2016, pp. 56-68", "doi": "10.4204/EPTCS.219.6", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bit layout is a sequence of fields of certain bit lengths that specifies\nhow to interpret a serial stream, e.g., the MP3 audio format. A layout with\nvariable length fields needs to include meta-information to help the parser\ninterpret unambiguously the rest of the stream; e.g. a field providing the\nlength of a following variable length field. If no such information is\navailable, then the layout is ambiguous. I present a linear-time algorithm to\ndetermine whether a layout is ambiguous or not by modelling the behaviour of a\nserial parser reading the stream as forward chaining reasoning on a collection\nof Horn clauses.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 11:05:26 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Paganelli", "Gabriele", ""]]}, {"id": "1607.04822", "submitter": "Shumo Chu", "authors": "Shumo Chu, Konstantin Weitz, Alvin Cheung, Dan Suciu", "title": "HoTTSQL: Proving Query Rewrites with Univalent SQL Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every database system contains a query optimizer that performs query\nrewrites. Unfortunately, developing query optimizers remains a highly\nchallenging task. Part of the challenges comes from the intricacies and rich\nfeatures of query languages, which makes reasoning about rewrite rules\ndifficult. In this paper, we propose a machine-checkable denotational semantics\nfor SQL, the de facto language for relational database, for rigorously\nvalidating rewrite rules. Unlike previously proposed semantics that are either\nnon-mechanized or only cover a small amount of SQL language features, our\nsemantics covers all major features of SQL, including bags, correlated\nsubqueries, aggregation, and indexes. Our mechanized semantics, called HoTTSQL,\nis based on K-Relations and homotopy type theory, where we denote relations as\nmathematical functions from tuples to univalent types. We have implemented\nHoTTSQL in Coq, which takes only fewer than 300 lines of code and have proved a\nwide range of SQL rewrite rules, including those from database research\nliterature (e.g., magic set rewrites) and real-world query optimizers (e.g.,\nsubquery elimination). Several of these rewrite rules have never been\npreviously proven correct. In addition, while query equivalence is generally\nundecidable, we have implemented an automated decision procedure using HoTTSQL\nfor conjunctive queries: a well-studied decidable fragment of SQL that\nencompasses many real-world queries.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2016 03:15:20 GMT"}, {"version": "v2", "created": "Fri, 5 Aug 2016 20:44:47 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Chu", "Shumo", ""], ["Weitz", "Konstantin", ""], ["Cheung", "Alvin", ""], ["Suciu", "Dan", ""]]}, {"id": "1607.05443", "submitter": "Catalin Hritcu", "authors": "Leonidas Lampropoulos, Diane Gallois-Wong, Catalin Hritcu, John\n  Hughes, Benjamin C. Pierce, Li-yao Xia", "title": "Beginner's Luck: A Language for Property-Based Generators", "comments": "long version of POPL 2017 camera ready with the missing ERC\n  acknowledgements", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Property-based random testing a la QuickCheck requires building efficient\ngenerators for well-distributed random data satisfying complex logical\npredicates, but writing these generators can be difficult and error prone. We\npropose a domain-specific language in which generators are conveniently\nexpressed by decorating predicates with lightweight annotations to control both\nthe distribution of generated values and the amount of constraint solving that\nhappens before each variable is instantiated. This language, called Luck, makes\ngenerators easier to write, read, and maintain.\n  We give Luck a formal semantics and prove several fundamental properties,\nincluding the soundness and completeness of random generation with respect to a\nstandard predicate semantics. We evaluate Luck on common examples from the\nproperty-based testing literature and on two significant case studies, showing\nthat it can be used in complex domains with comparable bug-finding\neffectiveness and a significant reduction in testing code size compared to\nhandwritten generators.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 08:01:17 GMT"}, {"version": "v2", "created": "Fri, 18 Nov 2016 00:45:48 GMT"}, {"version": "v3", "created": "Sat, 12 Oct 2019 11:25:44 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Lampropoulos", "Leonidas", ""], ["Gallois-Wong", "Diane", ""], ["Hritcu", "Catalin", ""], ["Hughes", "John", ""], ["Pierce", "Benjamin C.", ""], ["Xia", "Li-yao", ""]]}, {"id": "1607.05609", "submitter": "Philipp Haller", "authors": "Philipp Haller and Alexandre Loiko", "title": "Object Capabilities and Lightweight Affinity in Scala: Implementation,\n  Formalization, and Soundness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aliasing is a known source of challenges in the context of imperative\nobject-oriented languages, which have led to important advances in type systems\nfor aliasing control. However, their large-scale adoption has turned out to be\na surprisingly difficult challenge. While new language designs show promise,\nthey do not address the need of aliasing control in existing languages.\n  This paper presents a new approach to isolation and uniqueness in an\nexisting, widely-used language, Scala. The approach is unique in the way it\naddresses some of the most important obstacles to the adoption of type system\nextensions for aliasing control. First, adaptation of existing code requires\nonly a minimal set of annotations. Only a single bit of information is required\nper class. Surprisingly, the paper shows that this information can be provided\nby the object-capability discipline, widely-used in program security. We\nformalize our approach as a type system and prove key soundness theorems. The\ntype system is implemented for the full Scala language, providing, for the\nfirst time, a sound integration with Scala's local type inference. Finally, we\nempirically evaluate the conformity of existing Scala open-source code on a\ncorpus of over 75,000 LOC.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 14:37:11 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2016 09:53:16 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Haller", "Philipp", ""], ["Loiko", "Alexandre", ""]]}, {"id": "1607.05707", "submitter": "Sreepathi Pai", "authors": "Sreepathi Pai and Keshav Pingali", "title": "Lowering IrGL to CUDA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The IrGL intermediate representation is an explicitly parallel representation\nfor irregular programs that targets GPUs. In this report, we describe IrGL\nconstructs, examples of their use and how IrGL is compiled to CUDA by the\nGalois GPU compiler.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 19:38:39 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Pai", "Sreepathi", ""], ["Pingali", "Keshav", ""]]}, {"id": "1607.05830", "submitter": "Alexandra Silva", "authors": "Steffen Smolka, Praveen Kumar, Nate Foster, Dexter Kozen, Alexandra\n  Silva", "title": "Cantor meets Scott: Semantic Foundations for Probabilistic Networks", "comments": "to appear at POPL 2017, Paris", "journal-ref": null, "doi": "10.1145/3009837.3009843", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ProbNetKAT is a probabilistic extension of NetKAT with a denotational\nsemantics based on Markov kernels. The language is expressive enough to\ngenerate continuous distributions, which raises the question of how to compute\neffectively in the language. This paper gives an new characterization of\nProbNetKAT's semantics using domain theory, which provides the foundation\nneeded to build a practical implementation. We show how to use the semantics to\napproximate the behavior of arbitrary ProbNetKAT programs using distributions\nwith finite support. We develop a prototype implementation and show how to use\nit to solve a variety of problems including characterizing the expected\ncongestion induced by different routing schemes and reasoning probabilistically\nabout reachability in a network.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 06:05:07 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 02:24:52 GMT"}, {"version": "v3", "created": "Tue, 9 Aug 2016 21:31:19 GMT"}, {"version": "v4", "created": "Tue, 20 Sep 2016 04:59:36 GMT"}, {"version": "v5", "created": "Mon, 21 Nov 2016 02:34:53 GMT"}, {"version": "v6", "created": "Sat, 15 Dec 2018 09:01:32 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Smolka", "Steffen", ""], ["Kumar", "Praveen", ""], ["Foster", "Nate", ""], ["Kozen", "Dexter", ""], ["Silva", "Alexandra", ""]]}, {"id": "1607.06927", "submitter": "Daniel Poetzl", "authors": "Daniel Kroening, Daniel Poetzl, Peter Schrammel, Bj\\\"orn Wachter", "title": "Sound Static Deadlock Analysis for C/Pthreads (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a static deadlock analysis approach for C/pthreads. The design of\nour method has been guided by the requirement to analyse real-world code. Our\napproach is sound (i.e., misses no deadlocks) for programs that have defined\nbehaviour according to the C standard, and precise enough to prove\ndeadlock-freedom for a large number of programs. The method consists of a\npipeline of several analyses that build on a new context- and thread-sensitive\nabstract interpretation framework. We further present a lightweight dependency\nanalysis to identify statements relevant to deadlock analysis and thus speed up\nthe overall analysis. In our experimental evaluation, we succeeded to prove\ndeadlock-freedom for 262 programs from the Debian GNU/Linux distribution with\nin total 2.6 MLOC in less than 11 hours.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 12:25:32 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Kroening", "Daniel", ""], ["Poetzl", "Daniel", ""], ["Schrammel", "Peter", ""], ["Wachter", "Bj\u00f6rn", ""]]}, {"id": "1607.07727", "submitter": "Navid Khoshavi", "authors": "Navid Khoshavi, Mohammad Maghsoudloo, Hamid R. Zarandi", "title": "Leveraging the Potential of Control-Flow Error Resilient Techniques in\n  Multithreaded Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a software-based technique to recover control-flow errors\nin multithreaded programs. Control-flow error recovery is achieved through\ninserting additional instructions into multithreaded program at compile time\nregarding to two dependency graphs. These graphs are extracted to model\ncontrol-flow and data dependencies among basic blocks and thread interactions\nbetween different threads of a program. In order to evaluate the proposed\ntechnique, three multithreaded benchmarks quick sort, matrix multiplication and\nlinked list utilized to run on a multi-core processor, and a total of 5000\ntransient faults has been injected into several executable points of each\nprogram. The results show that this technique detects and corrects between\n91.9% and 93.8% of the injected faults with acceptable performance and memory\noverheads.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 15:39:55 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Khoshavi", "Navid", ""], ["Maghsoudloo", "Mohammad", ""], ["Zarandi", "Hamid R.", ""]]}, {"id": "1607.08228", "submitter": "Danfeng Zhang", "authors": "Danfeng Zhang and Daniel Kifer", "title": "LightDP: Towards Automating Differential Privacy Proofs", "comments": null, "journal-ref": null, "doi": "10.1145/3009837.3009884", "report-no": null, "categories": "cs.PL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing popularity and adoption of differential privacy in academic and\nindustrial settings has resulted in the development of increasingly\nsophisticated algorithms for releasing information while preserving privacy.\nAccompanying this phenomenon is the natural rise in the development and\npublication of incorrect algorithms, thus demonstrating the necessity of formal\nverification tools. However, existing formal methods for differential privacy\nface a dilemma: methods based on customized logics can verify sophisticated\nalgorithms but come with a steep learning curve and significant annotation\nburden on the programmers, while existing programming platforms lack expressive\npower for some sophisticated algorithms.\n  In this paper, we present LightDP, a simple imperative language that strikes\na better balance between expressive power and usability. The core of LightDP is\na novel relational type system that separates relational reasoning from privacy\nbudget calculations. With dependent types, the type system is powerful enough\nto verify sophisticated algorithms where the composition theorem falls short.\nIn addition, the inference engine of LightDP infers most of the proof details,\nand even searches for the proof with minimal privacy cost bound when multiple\nproofs exist. We show that LightDP verifies sophisticated algorithms with\nlittle manual effort.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 19:50:10 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 15:17:30 GMT"}, {"version": "v3", "created": "Fri, 9 Dec 2016 22:20:42 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Zhang", "Danfeng", ""], ["Kifer", "Daniel", ""]]}, {"id": "1607.08864", "submitter": "Christoph Redl", "authors": "Christoph Redl", "title": "The DLVHEX System for Knowledge Representation: Recent Advances (System\n  Description)", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,\n  LaTeX, 3 PDF figures (arXiv:1607.08864)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DLVHEX system implements the HEX-semantics, which integrates answer set\nprogramming (ASP) with arbitrary external sources. Since its first release ten\nyears ago, significant advancements were achieved. Most importantly, the\nexploitation of properties of external sources led to efficiency improvements\nand flexibility enhancements of the language, and technical improvements on the\nsystem side increased user's convenience. In this paper, we present the current\nstatus of the system and point out the most important recent enhancements over\nearly versions. While existing literature focuses on theoretical aspects and\nspecific components, a bird's eye view of the overall system is missing. In\norder to promote the system for real-world applications, we further present\napplications which were already successfully realized on top of DLVHEX. This\npaper is under consideration for acceptance in Theory and Practice of Logic\nProgramming.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 16:26:54 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 13:23:12 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Redl", "Christoph", ""]]}]