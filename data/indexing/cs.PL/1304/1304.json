[{"id": "1304.0538", "submitter": "Sen Ma Dr", "authors": "Sen Ma", "title": "OESPA:A Theory of Programming that Support Software Engineering", "comments": "18 pages for FOCS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new theory of programming is proposed. The theory consists of OE (Operation\nExpression), SP (Semantic Predicate) and A (Axiom), abbreviated as OESPA. OE is\nfor programming: its syntax is given by BNF formulas and its semantics is\ndefined by axioms on these formulas. Similar to predicates in logic, SP is for\ndescribing properties of OE (i.e. programs) and for program property analysis.\nBut SP is different from predicates, it directly relates the final values of\nvariables upon termination of a given OE with initial values of these variables\nbefore the same OE. As such, it is feasible to prove or disprove whether a\ngiven SP is a property of a given OE by computation based on A (Axioms). SP\ncalculus is proposed for program specification and specification analysis, that\nis missing in software engineering.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2013 05:57:44 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["Ma", "Sen", ""]]}, {"id": "1304.0660", "submitter": "Pranav Garg", "authors": "Pranav Garg and P. Madhusudan and Gennaro Parlato", "title": "Quantified Data Automata on Skinny Trees: an Abstract Domain for Lists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to heap analysis through an abstract domain of\nautomata, called automatic shapes. The abstract domain uses a particular kind\nof automata, called quantified data automata on skinny trees (QSDAs), that\nallows to define universally quantified properties of singly-linked lists. To\nensure convergence of the abstract fixed-point computation, we introduce a\nsub-class of QSDAs called elastic QSDAs, which also form an abstract domain. We\nevaluate our approach on several list manipulating programs and we show that\nthe proposed domain is powerful enough to prove a large class of these programs\ncorrect.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2013 15:07:47 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["Garg", "Pranav", ""], ["Madhusudan", "P.", ""], ["Parlato", "Gennaro", ""]]}, {"id": "1304.0809", "submitter": "Guillaume Allais", "authors": "Guillaume Allais, Pierre Boutillier, Conor McBride", "title": "New Equations for Neutral Terms: A Sound and Complete Decision\n  Procedure, Formalized", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The definitional equality of an intensional type theory is its test of type\ncompatibility. Today's systems rely on ordinary evaluation semantics to compare\nexpressions in types, frustrating users with type errors arising when\nevaluation fails to identify two `obviously' equal terms. If only the machine\ncould decide a richer theory! We propose a way to decide theories which\nsupplement evaluation with `$\\nu$-rules', rearranging the neutral parts of\nnormal forms, and report a successful initial experiment.\n  We study a simple -calculus with primitive fold, map and append operations on\nlists and develop in Agda a sound and complete decision procedure for an\nequational theory enriched with monoid, functor and fusion laws.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2013 22:55:32 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2013 00:45:07 GMT"}, {"version": "v3", "created": "Mon, 17 Jun 2013 15:30:20 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Allais", "Guillaume", ""], ["Boutillier", "Pierre", ""], ["McBride", "Conor", ""]]}, {"id": "1304.0864", "submitter": "David Monniaux", "authors": "Alexis Fouilh\\'e (VERIMAG - IMAG), David Monniaux (VERIMAG - IMAG),\n  Micha\\\"el P\\'erin (VERIMAG - IMAG)", "title": "Efficient Generation of Correctness Certificates for the Abstract Domain\n  of Polyhedra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polyhedra form an established abstract domain for inferring runtime\nproperties of programs using abstract interpretation. Computations on them need\nto be certified for the whole static analysis results to be trusted. In this\nwork, we look at how far we can get down the road of a posteriori verification\nto lower the overhead of certification of the abstract domain of polyhedra. We\ndemonstrate methods for making the cost of inclusion certificate generation\nnegligible. From a performance point of view, our single-representation,\nconstraints-based implementation compares with state-of-the-art\nimplementations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 08:01:42 GMT"}], "update_date": "2013-04-04", "authors_parsed": [["Fouilh\u00e9", "Alexis", "", "VERIMAG - IMAG"], ["Monniaux", "David", "", "VERIMAG - IMAG"], ["P\u00e9rin", "Micha\u00ebl", "", "VERIMAG - IMAG"]]}, {"id": "1304.1835", "submitter": "Alex Rubinsteyn", "authors": "Eric Hielscher, Alex Rubinsteyn, Dennis Shasha", "title": "Locality Optimization for Data Parallel Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": "NYU CS TR2013-955", "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Productivity languages such as NumPy and Matlab make it much easier to\nimplement data-intensive numerical algorithms. However, these languages can be\nintolerably slow for programs that don't map well to their built-in primitives.\nIn this paper, we discuss locality optimizations for our system Parakeet, a\njust-in-time compiler and runtime system for an array-oriented subset of\nPython. Parakeet dynamically compiles whole user functions to high performance\nmulti-threaded native code. Parakeet makes extensive use of the classic data\nparallel operators Map, Reduce, and Scan. We introduce a new set of data\nparallel operators,TiledMap, TiledReduce, and TiledScan, that break up their\ncomputations into local pieces of bounded size so as better to make use of\nsmall fast memories. We introduce a novel tiling transformation to generate\ntiled operators automatically. Applying this transformation once tiles the\nprogram for cache, and applying it again enables tiling for registers. The\nsizes for cache tiles are left unspecified until runtime, when an autotuning\nsearch is performed. Finally, we evaluate our optimizations on benchmarks and\nshow significant speedups on programs that exhibit data locality.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2013 23:52:00 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Hielscher", "Eric", ""], ["Rubinsteyn", "Alex", ""], ["Shasha", "Dennis", ""]]}, {"id": "1304.1913", "submitter": "Carlo Spaccasassi Mr", "authors": "Carlo Spaccasassi and Vasileios Koutavas", "title": "Towards Efficient Abstractions for Concurrent Consensus", "comments": "15 pages, 5 figures, symposium: TFP 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus is an often occurring problem in concurrent and distributed\nprogramming. We present a programming language with simple semantics and\nbuild-in support for consensus in the form of communicating transactions. We\nmotivate the need for such a construct with a characteristic example of\ngeneralized consensus which can be naturally encoded in our language. We then\nfocus on the challenges in achieving an implementation that can efficiently run\nsuch programs. We setup an architecture to evaluate different implementation\nalternatives and use it to experimentally evaluate runtime heuristics. This is\nthe basis for a research project on realistic programming language support for\nconsensus.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2013 17:00:00 GMT"}, {"version": "v2", "created": "Tue, 7 May 2013 16:47:24 GMT"}], "update_date": "2013-05-08", "authors_parsed": [["Spaccasassi", "Carlo", ""], ["Koutavas", "Vasileios", ""]]}, {"id": "1304.2550", "submitter": "Daniel Merkle", "authors": "Felix P. Hargreaves and Daniel Merkle", "title": "FooPar: A Functional Object Oriented Parallel Framework in Scala", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present FooPar, an extension for highly efficient Parallel Computing in\nthe multi-paradigm programming language Scala. Scala offers concise and clean\nsyntax and integrates functional programming features. Our framework FooPar\ncombines these features with parallel computing techniques. FooPar is designed\nmodular and supports easy access to different communication backends for\ndistributed memory architectures as well as high performance math libraries. In\nthis article we use it to parallelize matrix matrix multiplication and show its\nscalability by a isoefficiency analysis. In addition, results based on a\nempirical analysis on two supercomputers are given. We achieve close-to-optimal\nperformance wrt. theoretical peak performance. Based on this result we conclude\nthat FooPar allows to fully access Scala's design features without suffering\nfrom performance drops when compared to implementations purely based on C and\nMPI.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2013 12:23:47 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2013 14:17:20 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Hargreaves", "Felix P.", ""], ["Merkle", "Daniel", ""]]}, {"id": "1304.3140", "submitter": "Evgeniy Grigoriev A.", "authors": "Evgeniy Grigoriev", "title": "On PROGRESS Operation. How to Make Object-Oriented Programming System\n  More Object-Oriented (DRAFT)", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A system, which implements persistent objects, has to provide different\nopportunities to change the objects in arbitrary ways during their existence. A\ntraditional realization of OO paradigm in modern programming systems has\nfundamental drawbacks which complicate an implementation of persistent\nmodifiable objects considerably. There is alternative realization that does not\nhave these drawbacks. In the article the PROGRESS operation is offered, which\nmodify existing object within an existing inheritance hierarchy.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2013 04:20:23 GMT"}], "update_date": "2013-04-12", "authors_parsed": [["Grigoriev", "Evgeniy", ""]]}, {"id": "1304.3260", "submitter": "Mark Anderson", "authors": "John Collins, Brian Farrimond, David Flower, Mark Anderson and David\n  Gill", "title": "The Removal of Numerical Drift from Scientific Models", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer programs often behave differently under different compilers or in\ndifferent computing environments. Relative debugging is a collection of\ntechniques by which these differences are analysed. Differences may arise\nbecause of different interpretations of errors in the code, because of bugs in\nthe compilers or because of numerical drift, and all of these were observed in\nthe present study. Numerical drift arises when small and acceptable differences\nin values computed by different systems are integrated, so that the results\ndrift apart. This is well understood and need not degrade the validity of the\nprogram results. Coding errors and compiler bugs may degrade the results and\nshould be removed. This paper describes a technique for the comparison of two\nprogram runs which removes numerical drift and therefore exposes coding and\ncompiler errors. The procedure is highly automated and requires very little\nintervention by the user. The technique is applied to the Weather Research and\nForecasting model, the most widely used weather and climate modelling code.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2013 11:35:44 GMT"}], "update_date": "2013-04-12", "authors_parsed": [["Collins", "John", ""], ["Farrimond", "Brian", ""], ["Flower", "David", ""], ["Anderson", "Mark", ""], ["Gill", "David", ""]]}, {"id": "1304.3390", "submitter": "Alexander Green", "authors": "Alexander S. Green, Peter LeFanu Lumsdaine, Neil J. Ross, Peter\n  Selinger and Beno\\^it Valiron", "title": "Quipper: A Scalable Quantum Programming Language", "comments": "10 pages, PLDI 2013", "journal-ref": "ACM SIGPLAN Notices 48(6):333-342, 2013", "doi": "10.1145/2499370.2462177", "report-no": null, "categories": "cs.PL cs.ET quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of quantum algorithms is vibrant. Still, there is currently a lack\nof programming languages for describing quantum computation on a practical\nscale, i.e., not just at the level of toy problems. We address this issue by\nintroducing Quipper, a scalable, expressive, functional, higher-order quantum\nprogramming language. Quipper has been used to program a diverse set of\nnon-trivial quantum algorithms, and can generate quantum gate representations\nusing trillions of gates. It is geared towards a model of computation that uses\na classical computer to control a quantum device, but is not dependent on any\nparticular model of quantum hardware. Quipper has proven effective and easy to\nuse, and opens the door towards using formal methods to analyze quantum\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2013 18:28:16 GMT"}], "update_date": "2013-07-08", "authors_parsed": [["Green", "Alexander S.", ""], ["Lumsdaine", "Peter LeFanu", ""], ["Ross", "Neil J.", ""], ["Selinger", "Peter", ""], ["Valiron", "Beno\u00eet", ""]]}, {"id": "1304.3596", "submitter": "Sandrine Blazy", "authors": "Sandrine Blazy (INRIA - IRISA), Vincent Laporte (INRIA - IRISA),\n  Andr\\'e Maroneze (INRIA - IRISA), David Pichardie (INRIA - IRISA)", "title": "Formal Verification of a C Value Analysis Based on Abstract\n  Interpretation", "comments": null, "journal-ref": "SAS - 20th Static Analysis Symposium Lecture Notes in Computer\n  Science (2013) 324-344", "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static analyzers based on abstract interpretation are complex pieces of\nsoftware implementing delicate algorithms. Even if static analysis techniques\nare well understood, their implementation on real languages is still\nerror-prone. This paper presents a formal verification using the Coq proof\nassistant: a formalization of a value analysis (based on abstract\ninterpretation), and a soundness proof of the value analysis. The formalization\nrelies on generic interfaces. The mechanized proof is facilitated by a\ntranslation validation of a Bourdoncle fixpoint iterator. The work has been\nintegrated into the CompCert verified C-compiler. Our verified analysis\ndirectly operates over an intermediate language of the compiler having the same\nexpressiveness as C. The automatic extraction of our value analysis into OCaml\nyields a program with competitive results, obtained from experiments on a\nnumber of benchmarks and comparisons with the Frama-C tool.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 10:32:40 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Blazy", "Sandrine", "", "INRIA - IRISA"], ["Laporte", "Vincent", "", "INRIA - IRISA"], ["Maroneze", "Andr\u00e9", "", "INRIA - IRISA"], ["Pichardie", "David", "", "INRIA - IRISA"]]}, {"id": "1304.3804", "submitter": "Emilio Coppa", "authors": "Emilio Coppa, Camil Demetrescu, Irene Finocchi and Romolo Marotta", "title": "Multithreaded Input-Sensitive Profiling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Input-sensitive profiling is a recent performance analysis technique that\nmakes it possible to estimate the empirical cost function of individual\nroutines of a program, helping developers understand how performance scales to\nlarger inputs and pinpoint asymptotic bottlenecks in the code. A current\nlimitation of input-sensitive profilers is that they specifically target\nsequential computations, ignoring any communication between threads. In this\npaper we show how to overcome this limitation, extending the range of\napplicability of the original approach to multithreaded applications and to\napplications that operate on I/O streams. We develop new metrics for\nautomatically estimating the size of the input given to each routine\nactivation, addressing input produced by non-deterministic memory stores\nperformed by other threads as well as by the OS kernel (e.g., in response to\nI/O or network operations). We provide real case studies, showing that our\nextension allows it to characterize the behavior of complex applications more\nprecisely than previous approaches. An extensive experimental investigation on\na variety of benchmark suites (including the SPEC OMP2012 and the PARSEC\nbenchmarks) shows that our Valgrind-based input-sensitive profiler incurs an\noverhead comparable to other prominent heavyweight analysis tools, while\ncollecting significantly more performance points from each profiling session\nand correctly characterizing both thread-induced and external input.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2013 12:39:34 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Coppa", "Emilio", ""], ["Demetrescu", "Camil", ""], ["Finocchi", "Irene", ""], ["Marotta", "Romolo", ""]]}, {"id": "1304.5197", "submitter": "Daniele Cono D'Elia", "authors": "Daniele Cono D'Elia, Camil Demetrescu, Irene Finocchi", "title": "Ball-Larus Path Profiling Across Multiple Loop iterations", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the hottest paths in the control flow graph of a routine can\ndirect optimizations to portions of the code where most resources are consumed.\nThis powerful methodology, called path profiling, was introduced by Ball and\nLarus in the mid 90s and has received considerable attention in the last 15\nyears for its practical relevance. A shortcoming of Ball-Larus path profiling\nwas the inability to profile cyclic paths, making it difficult to mine\ninteresting execution patterns that span multiple loop iterations. Previous\nresults, based on rather complex algorithms, have attempted to circumvent this\nlimitation at the price of significant performance losses already for a small\nnumber of iterations. In this paper, we present a new approach to multiple\niterations path profiling, based on data structures built on top of the\noriginal Ball-Larus numbering technique. Our approach allows it to profile all\nexecuted paths obtained as a concatenation of up to k Ball-Larus acyclic paths,\nwhere k is a user-defined parameter. An extensive experimental investigation on\na large variety of Java benchmarks on the Jikes RVM shows that, surprisingly,\nour approach can be even faster than Ball-Larus due to fewer operations on\nsmaller hash tables, producing compact representations of cyclic paths even for\nlarge values of k.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2013 17:34:38 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["D'Elia", "Daniele Cono", ""], ["Demetrescu", "Camil", ""], ["Finocchi", "Irene", ""]]}, {"id": "1304.5485", "submitter": "Alexander Green", "authors": "Alexander S. Green, Peter LeFanu Lumsdaine, Neil J. Ross, Peter\n  Selinger and Beno\\^it Valiron", "title": "An Introduction to Quantum Programming in Quipper", "comments": "15 pages, RC2013", "journal-ref": "Lecture Notes in Computer Science 7948:110-124, Springer, 2013", "doi": "10.1007/978-3-642-38986-3_10", "report-no": null, "categories": "cs.PL cs.ET quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quipper is a recently developed programming language for expressing quantum\ncomputations. This paper gives a brief tutorial introduction to the language,\nthrough a demonstration of how to make use of some of its key features. We\nillustrate many of Quipper's language features by developing a few well known\nexamples of Quantum computation, including quantum teleportation, the quantum\nFourier transform, and a quantum circuit for addition.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 17:33:33 GMT"}], "update_date": "2013-07-08", "authors_parsed": [["Green", "Alexander S.", ""], ["Lumsdaine", "Peter LeFanu", ""], ["Ross", "Neil J.", ""], ["Selinger", "Peter", ""], ["Valiron", "Beno\u00eet", ""]]}, {"id": "1304.5531", "submitter": "Edwin Westbrook IV", "authors": "Edwin Westbrook, Swarat Chaudhuri", "title": "A Semantics for Approximate Program Transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approximate program transformation is a transformation that can change the\nsemantics of a program within a specified empirical error bound. Such\ntransformations have wide applications: they can decrease computation time,\npower consumption, and memory usage, and can, in some cases, allow\nimplementations of incomputable operations. Correctness proofs of approximate\nprogram transformations are by definition quantitative. Unfortunately, unlike\nwith standard program transformations, there is as of yet no modular way to\nprove correctness of an approximate transformation itself. Error bounds must be\nproved for each transformed program individually, and must be re-proved each\ntime a program is modified or a different set of approximations are applied. In\nthis paper, we give a semantics that enables quantitative reasoning about a\nlarge class of approximate program transformations in a local, composable way.\nOur semantics is based on a notion of distance between programs that defines\nwhat it means for an approximate transformation to be correct up to an error\nbound. The key insight is that distances between programs cannot in general be\nformulated in terms of metric spaces and real numbers. Instead, our semantics\nadmits natural notions of distance for each type construct; for example,\nnumbers are used as distances for numerical data, functions are used as\ndistances for functional data, an polymorphic lambda-terms are used as\ndistances for polymorphic data. We then show how our semantics applies to two\nexample approximations: replacing reals with floating-point numbers, and loop\nperforation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 20:06:11 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Westbrook", "Edwin", ""], ["Chaudhuri", "Swarat", ""]]}, {"id": "1304.5661", "submitter": "Viktor Kuncak", "authors": "Etienne Kneuss, Viktor Kuncak, Ivan Kuraj, and Philippe Suter", "title": "On Integrating Deductive Synthesis and Verification Systems", "comments": "17 pages. 46 references", "journal-ref": null, "doi": null, "report-no": "EPFL-REPORT-186043", "categories": "cs.PL cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe techniques for synthesis and verification of recursive functional\nprograms over unbounded domains. Our techniques build on top of an algorithm\nfor satisfiability modulo recursive functions, a framework for deductive\nsynthesis, and complete synthesis procedures for algebraic data types. We\npresent new counterexample-guided algorithms for constructing verified\nprograms. We have implemented these algorithms in an integrated environment for\ninteractive verification and synthesis from relational specifications. Our\nsystem was able to synthesize a number of useful recursive functions that\nmanipulate unbounded numbers and data structures.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2013 18:58:36 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Kneuss", "Etienne", ""], ["Kuncak", "Viktor", ""], ["Kuraj", "Ivan", ""], ["Suter", "Philippe", ""]]}, {"id": "1304.5893", "submitter": "Sabah Al-Fedaghi Dr.", "authors": "Sabah Al-Fedaghi", "title": "Conceptual Understanding of Computer Program Execution: Application to\n  C++", "comments": "11 pages, 18 figures", "journal-ref": "International Journal of Computer Science Issues, Volume 10, Issue\n  2, March 2013", "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A visual programming language uses pictorial tools such as diagrams to\nrepresent its structural units and control stream. It is useful for enhancing\nunderstanding, maintenance, verification, testing, and parallelism. This paper\nproposes a diagrammatic methodology that produces a conceptual representation\nof instructions for programming source codes. Without loss of generality in the\npotential for using the methodology in a wider range of applications, this\npaper focuses on using these diagrams in teaching of C++ programming. C++\nprogramming constructs are represented in the proposed method in order to show\nthat it can provide a foundation for understanding the behavior of running\nprograms. Applying the method to actual C++ classes demonstrates that it\nimproves understanding of the activities in the computer system corresponding\nto a C++ program.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2013 09:41:42 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Al-Fedaghi", "Sabah", ""]]}, {"id": "1304.6038", "submitter": "David Monniaux", "authors": "Thomas Braibant (INRIA Rocquencourt), Jacques-Henri Jourdan (INRIA\n  Rocquencourt), David Monniaux (VERIMAG - IMAG)", "title": "Implementing hash-consed structures in Coq", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on three different approaches to use hash-consing in programs\ncertified with the Coq system, using binary decision diagrams (BDD) as running\nexample. The use cases include execution inside Coq, or execution of the\nextracted OCaml code. There are different trade-offs between faithful use of\npristine extracted code, and code that is fine-tuned to make use of OCaml\nprogramming constructs not available in Coq. We discuss the possible\nconsequences in terms of performances and guarantees.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2013 18:07:36 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Braibant", "Thomas", "", "INRIA Rocquencourt"], ["Jourdan", "Jacques-Henri", "", "INRIA\n  Rocquencourt"], ["Monniaux", "David", "", "VERIMAG - IMAG"]]}, {"id": "1304.6274", "submitter": "Rohan Padhye", "authors": "Rohan Padhye and Uday P. Khedker", "title": "Interprocedural Data Flow Analysis in Soot using Value Contexts", "comments": "SOAP 2013 Final Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An interprocedural analysis is precise if it is flow sensitive and fully\ncontext-sensitive even in the presence of recursion. Many methods of\ninterprocedural analysis sacrifice precision for scalability while some are\nprecise but limited to only a certain class of problems.\n  Soot currently supports interprocedural analysis of Java programs using graph\nreachability. However, this approach is restricted to IFDS/IDE problems, and is\nnot suitable for general data flow frameworks such as heap reference analysis\nand points-to analysis which have non-distributive flow functions.\n  We describe a general-purpose interprocedural analysis framework for Soot\nusing data flow values for context-sensitivity. This framework is not\nrestricted to problems with distributive flow functions, although the lattice\nmust be finite. It combines the key ideas of the tabulation method of the\nfunctional approach and the technique of value-based termination of call string\nconstruction.\n  The efficiency and precision of interprocedural analyses is heavily affected\nby the precision of the underlying call graph. This is especially important for\nobject-oriented languages like Java where virtual method invocations cause an\nexplosion of spurious call edges if the call graph is constructed naively. We\nhave instantiated our framework with a flow and context-sensitive points-to\nanalysis in Soot, which enables the construction of call graphs that are far\nmore precise than those constructed by Soot's SPARK engine.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2013 13:02:09 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2013 06:48:43 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Padhye", "Rohan", ""], ["Khedker", "Uday P.", ""]]}, {"id": "1304.6284", "submitter": "Clemens Grabmayer", "authors": "Clemens Grabmayer and Jan Rochel", "title": "Expressibility in the Lambda Calculus with mu", "comments": "24 pages, 7 figures, Extended version of an article in the\n  proceedings of the 24th International Conference on Rewriting Techniques and\n  Applications, Einhoven, the Netherlands, June 24-26, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a problem connected to the unfolding semantics of functional\nprogramming languages: give a useful characterization of those infinite\nlambda-terms that are lambda_{letrec}-expressible in the sense that they arise\nas infinite unfoldings of terms in lambda_{letrec}, the lambda-calculus with\nletrec. We provide two characterizations, using concepts we introduce for\ninfinite lambda-terms: regularity, strong regularity, and binding-capturing\nchains. It turns out that lambda_{letrec}-expressible infinite lambda-terms\nform a proper subclass of the regular infinite lambda-terms. In this paper we\nestablish these characterizations only for expressibility in lambda_{mu}, the\nlambda-calculus with explicit mu-recursion. We show that for all infinite\nlambda-terms T the following are equivalent: (i): T is lambda_{mu}-expressible;\n(ii): T is strongly regular; (iii): T is regular, and it only has finite\nbinding-capturing chains.\n  We define regularity and strong regularity for infinite lambda-terms as two\ndifferent generalizations of regularity for infinite first-order terms: as the\nexistence of only finitely many subterms that are defined as the reducts of two\nrewrite relations for decomposing lambda-terms. These rewrite relations act on\ninfinite lambda-terms furnished with a marked prefix of abstractions for\ncollecting decomposed lambda-abstractions and keeping the terms closed under\ndecomposition. They differ in how vacuous abstractions in the prefix are\nremoved.\n  This report accompanies the article with the same title for the proceedings\nof the conference RTA 2013, and mainly differs from that by providing the proof\nof the characterization of lambda_{mu}-expressibility with binding-capturing\nchains.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2013 13:26:02 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2013 13:52:17 GMT"}, {"version": "v3", "created": "Mon, 27 May 2013 11:48:26 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Grabmayer", "Clemens", ""], ["Rochel", "Jan", ""]]}, {"id": "1304.7544", "submitter": "Jimmy Lin", "authors": "Jimmy Lin", "title": "Monoidify! Monoids as a Design Principle for Efficient MapReduce\n  Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that since the sort/shuffle stage in MapReduce is costly,\nlocal aggregation is one important principle to designing efficient algorithms.\nThis short paper represents an attempt to more clearly articulate this design\nprinciple in terms of monoids, which generalizes the use of combiners and the\nin-mapper combining pattern.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2013 00:30:36 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Lin", "Jimmy", ""]]}, {"id": "1304.7600", "submitter": "Piotr Beling", "authors": "Piotr Beling", "title": "C++11 - okre\\'slanie typ\\'ow", "comments": "6 pages, in Polish", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a review of some new futures introduced to C++ language\nby ISO/IEC 14882:2011 standard (known as C++11). It describes new language\nelements which allow to easier expressed of types of variables: auto and\ndecltype keywords, new function declaration syntax, and tools which are\nincluded in type_traits header.\n  -----\n  Niniejszy artyku{\\l} jest jednym z serii artyku{\\l}\\'ow w kt\\'orych zawarto\nprzegl{\\ka}d nowych element\\'ow j{\\ke}zyka C++ wprowadzonych przez standard\nISO/IEC 14882:2011, znany pod nazw{\\ka} C++11. W artykule przedstawiono nowe\nmo\\.zliwo\\'sci zwi{\\ka}zane ze wskazywaniem typ\\'ow zmiennych. Opisano s{\\l}owa\nkluczowe auto i decltype, now{\\ka} sk{\\l}adnie deklarowania funkcji/metod oraz\nnarz{\\ke}dzia zawarte w pliku nag{\\l}\\'owkowym <type_traits>.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2013 09:30:33 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Beling", "Piotr", ""]]}, {"id": "1304.7615", "submitter": "Adrian Jackson", "authors": "Adrian Jackson, Par Strand", "title": "MDMP: Managed Data Message Passing", "comments": "Submitted to SC13, 10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MDMP is a new parallel programming approach that aims to provide users with\nan easy way to add parallelism to programs, optimise the message passing costs\nof traditional scientific simulation algorithms, and enable existing MPI-based\nparallel programs to be optimised and extended without requiring the whole code\nto be re-written from scratch. MDMP utilises a directives based approach to\nenable users to specify what communications should take place in the code, and\nthen implements those communications for the user in an optimal manner using\nboth the information provided by the user and data collected from instrumenting\nthe code and gathering information on the data to be communicated. This work\nwill present the basic concepts and functionality of MDMP and discuss the\nperformance that can be achieved using our prototype implementation of MDMP on\nsome model scientific simulation applications.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2013 10:35:48 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Jackson", "Adrian", ""], ["Strand", "Par", ""]]}, {"id": "1304.7856", "submitter": "EPTCS", "authors": "Caleb Eggensperger", "title": "Proof Pad: A New Development Environment for ACL2", "comments": "In Proceedings ACL2 2013, arXiv:1304.7123", "journal-ref": "EPTCS 114, 2013, pp. 13-28", "doi": "10.4204/EPTCS.114.2", "report-no": null, "categories": "cs.SE cs.HC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most software development projects rely on Integrated Development\nEnvironments (IDEs) based on the desktop paradigm, with an interactive,\nmouse-driven user interface. The standard installation of ACL2, on the other\nhand, is designed to work closely with Emacs. ACL2 experts, on the whole, like\nthis mode of operation, but students and other new programmers who have learned\nto program with desktop IDEs often react negatively to the process of adapting\nto an unfamiliar form of interaction.\n  This paper discusses Proof Pad, a new IDE for ACL2. Proof Pad is not the only\nattempt to provide ACL2 IDEs catering to students and beginning programmers.\nThe ACL2 Sedan and DrACuLa systems arose from similar motivations. Proof Pad\nbuilds on the work of those systems, while also taking into account the unique\nworkflow of the ACL2 theorem proving system.\n  The design of Proof Pad incorporated user feedback from the outset, and that\nprocess continued through all stages of development. Feedback took the form of\ndirect observation of users interacting with the IDE as well as questionnaires\ncompleted by users of Proof Pad and other ACL2 IDEs. The result is a\nstreamlined interface and fast, responsive system that supports using ACL2 as a\nprogramming language and a theorem proving system. Proof Pad also provides a\nproperty-based testing environment with random data generation and automated\ninterpretation of properties as ACL2 theorem definitions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 04:14:06 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Eggensperger", "Caleb", ""]]}, {"id": "1304.7875", "submitter": "EPTCS", "authors": "Sebastiaan J. C. Joosten, Bernard van Gastel, Julien Schmaltz", "title": "A Macro for Reusing Abstract Functions and Theorems", "comments": "In Proceedings ACL2 2013, arXiv:1304.7123", "journal-ref": "EPTCS 114, 2013, pp. 29-41", "doi": "10.4204/EPTCS.114.3", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though the ACL2 logic is first order, the ACL2 system offers several\nmechanisms providing users with some operations akin to higher order logic\nones. In this paper, we propose a macro, named instance-of-defspec, to ease the\nreuse of abstract functions and facts proven about them. Defspec is an ACL2\nbook allowing users to define constrained functions and their associated\nproperties. It contains macros facilitating the definition of such abstract\nspecifications and instances thereof. Currently, lemmas and theorems derived\nfrom these abstract functions are not automatically instantiated. This is\nexactly the purpose of our new macro. instance-of-defspec will not only\ninstantiate functions and theorems within a specification but also many more\nfunctions and theorems built on top of the specification. As a working example,\nwe describe various fold functions over monoids, which we gradually built from\narbitrary functions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 04:43:47 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Joosten", "Sebastiaan J. C.", ""], ["van Gastel", "Bernard", ""], ["Schmaltz", "Julien", ""]]}]