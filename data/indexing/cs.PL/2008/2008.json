[{"id": "2008.00421", "submitter": "Germ\\'an Vidal", "authors": "Fred Mesnard, Etienne Payet, German Vidal", "title": "Concolic Testing in CLP", "comments": "Paper presented at the 36th International Conference on Logic\n  Programming (ICLP 2020), University Of Calabria, Rende (CS), Italy, September\n  2020, 16 pages", "journal-ref": "Theory and Practice of Logic Programming 20 (2020) 671-686", "doi": "10.1017/S1471068420000216", "report-no": null, "categories": "cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concolic testing is a popular software verification technique based on a\ncombination of concrete and symbolic execution. Its main focus is finding bugs\nand generating test cases with the aim of maximizing code coverage. A previous\napproach to concolic testing in logic programming was not sound because it only\ndealt with positive constraints (by means of substitutions) but could not\nrepresent negative constraints. In this paper, we present a novel framework for\nconcolic testing of CLP programs that generalizes the previous technique. In\nthe CLP setting, one can represent both positive and negative constraints in a\nnatural way, thus giving rise to a sound and (potentially) more efficient\ntechnique. Defining verification and testing techniques for CLP programs is\nincreasingly relevant since this framework is becoming popular as an\nintermediate representation to analyze programs written in other programming\nparadigms.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 07:15:43 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 08:43:44 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 07:00:37 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Mesnard", "Fred", ""], ["Payet", "Etienne", ""], ["Vidal", "German", ""]]}, {"id": "2008.00425", "submitter": "Yican Sun", "authors": "Jinyi Wang, Yican Sun, Hongfei Fu, Mingzhang Huang, Amir Kafshdar\n  Goharshady, Krishnendu Chatterjee", "title": "Concentration-Bound Analysis for Probabilistic Programs and\n  Probabilistic Recurrence Relations", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing probabilistic programs and randomized algorithms are classical\nproblems in computer science. The first basic problem in the analysis of\nstochastic processes is to consider the expectation or mean, and another basic\nproblem is to consider concentration bounds, i.e. showing that large deviations\nfrom the mean have small probability. Similarly, in the context of\nprobabilistic programs and randomized algorithms, the analysis of expected\ntermination time/running time and their concentration bounds are fundamental\nproblems.In this work, we focus on concentration bounds for probabilistic\nprograms and probabilistic recurrences of randomized algorithms. For\nprobabilistic programs, the basic technique to achieve concentration bounds is\nto consider martingales and apply the classical Azuma's inequality. For\nprobabilistic recurrences of randomized algorithms, Karp's classical \"cookbook\"\nmethod, which is similar to the master theorem for recurrences, is the standard\napproach to obtain concentration bounds. In this work, we propose a novel\napproach for deriving concentration bounds for probabilistic programs and\nprobabilistic recurrence relations through the synthesis of exponential\nsupermartingales. For probabilistic programs, we present algorithms for\nsynthesis of such supermartingales in several cases. We also show that our\napproach can derive better concentration bounds than simply applying the\nclassical Azuma's inequality over various probabilistic programs considered in\nthe literature. For probabilistic recurrences, our approach can derive tighter\nbounds than the Karp's well-established methods on classical algorithms.\nMoreover, we show that our approach could derive bounds comparable to the\noptimal bound for quicksort, proposed by McDiarmid and Hayward. We also present\na prototype implementation that can automatically infer these bounds\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 07:31:02 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 09:46:19 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2020 02:01:02 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Wang", "Jinyi", ""], ["Sun", "Yican", ""], ["Fu", "Hongfei", ""], ["Huang", "Mingzhang", ""], ["Goharshady", "Amir Kafshdar", ""], ["Chatterjee", "Krishnendu", ""]]}, {"id": "2008.00840", "submitter": "Tristan Miller", "authors": "Tristan Miller and Denis Auroux", "title": "GPP, the Generic Preprocessor", "comments": "4 pages, 0 figures", "journal-ref": "Journal of Open Source Software, 5(51), 2020", "doi": "10.21105/joss.02400", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In computer science, a preprocessor (or macro processor) is a tool that\nprogramatically alters its input, typically on the basis of inline annotations,\nto produce data that serves as input for another program. Preprocessors are\nused in software development and document processing workflows to translate or\nextend programming or markup languages, as well as for conditional or\npattern-based generation of source code and text. Early preprocessors were\nrelatively simple string replacement tools that were tied to specific\nprogramming languages and application domains, and while these have since given\nrise to more powerful, general-purpose tools, these often require the user to\nlearn and use complex macro languages with their own syntactic conventions. In\nthis paper, we present GPP, an extensible, general-purpose preprocessor whose\nprincipal advantage is that its syntax and behaviour can be customized to suit\nany given preprocessing task. This makes GPP of particular benefit to research\napplications, where it can be easily adapted for use with novel markup,\nprogramming, and control languages.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 12:41:04 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Miller", "Tristan", ""], ["Auroux", "Denis", ""]]}, {"id": "2008.01452", "submitter": "Steven Ramsay", "authors": "Eddie Jones and Steven Ramsay", "title": "Intensional Datatype Refinement", "comments": "26 pages plus bibliography and appendices. Tool available from\n  https://github.com/bristolpl/intensional-datatys. [v2] fixed accidental\n  unicode-related formatting issues in bibliography. [v3] Improvements\n  incorporated, thanks to POPL 2021 reviewers", "journal-ref": null, "doi": "10.1145/3434336", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The pattern-match safety problem is to verify that a given functional program\nwill never crash due to non-exhaustive patterns in its function definitions. We\npresent a refinement type system that can be used to solve this problem. The\nsystem extends ML-style type systems with algebraic datatypes by a limited form\nof structural subtyping and environment-level intersection. We describe a fully\nautomatic, sound and complete type inference procedure for this system which,\nunder reasonable assumptions, is worst-case linear-time in the program size.\nCompositionality is essential to obtaining this complexity guarantee. A\nprototype implementation for Haskell is able to analyse a selection of packages\nfrom the Hackage database in a few hundred milliseconds.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 10:47:08 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 08:49:28 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 12:14:46 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Jones", "Eddie", ""], ["Ramsay", "Steven", ""]]}, {"id": "2008.01566", "submitter": "Md Rafiqul Islam Rabin", "authors": "Md Rafiqul Islam Rabin, Nghi D. Q. Bui, Ke Wang, Yijun Yu, Lingxiao\n  Jiang, Mohammad Amin Alipour", "title": "On the Generalizability of Neural Program Models with respect to\n  Semantic-Preserving Program Transformations", "comments": "Information and Software Technology, IST Journal 2021, Elsevier.\n  arXiv admin note: substantial text overlap with arXiv:2004.07313", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the prevalence of publicly available source code repositories to train\ndeep neural network models, neural program models can do well in source code\nanalysis tasks such as predicting method names in given programs that cannot be\neasily done by traditional program analysis techniques. Although such neural\nprogram models have been tested on various existing datasets, the extent to\nwhich they generalize to unforeseen source code is largely unknown. Since it is\nvery challenging to test neural program models on all unforeseen programs, in\nthis paper, we propose to evaluate the generalizability of neural program\nmodels with respect to semantic-preserving transformations: a generalizable\nneural program model should perform equally well on programs that are of the\nsame semantics but of different lexical appearances and syntactical structures.\nWe compare the results of various neural program models for the method name\nprediction task on programs before and after automated semantic-preserving\ntransformations. We use three Java datasets of different sizes and three\nstate-of-the-art neural network models for code, namely code2vec, code2seq, and\nGGNN, to build nine such neural program models for evaluation. Our results show\nthat even with small semantically preserving changes to the programs, these\nneural program models often fail to generalize their performance. Our results\nalso suggest that neural program models based on data and control dependencies\nin programs generalize better than neural program models based only on abstract\nsyntax trees. On the positive side, we observe that as the size of the training\ndataset grows and diversifies the generalizability of correct predictions\nproduced by the neural program models can be improved too. Our results on the\ngeneralizability of neural program models provide insights to measure their\nlimitations and provide a stepping stone for their improvement.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 20:39:20 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 12:55:45 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 07:35:13 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Rabin", "Md Rafiqul Islam", ""], ["Bui", "Nghi D. Q.", ""], ["Wang", "Ke", ""], ["Yu", "Yijun", ""], ["Jiang", "Lingxiao", ""], ["Alipour", "Mohammad Amin", ""]]}, {"id": "2008.01725", "submitter": "Jyoti Prakash", "authors": "Abhishek Tiwari, Jyoti Prakash, Sascha Gross, Christian Hammer", "title": "A Large Scale Analysis of Android-Web Hybridization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Android applications embed webpages via WebView components and execute\nJavaScript code within Android. Hybrid applications leverage dedicated APIs to\nload a resource and render it in a WebView. Furthermore, Android objects can be\nshared with the JavaScript world. However, bridging the interfaces of the\nAndroid and JavaScript world might also incur severe security threats:\nPotentially untrusted webpages and their JavaScript might interfere with the\nAndroid environment and its access to native features. No general analysis is\ncurrently available to assess the implications of such hybrid apps bridging the\ntwo worlds. To understand the semantics and effects of hybrid apps, we perform\na large-scale study on the usage of the hybridization APIs in the wild. We\nanalyze and categorize the parameters to hybridization APIs for 7,500 randomly\nselected and the 196 most popular applications from the Google Playstore as\nwell as 1000 malware samples. Our results advance the general understanding of\nhybrid applications, as well as implications for potential program analyses,\nand the current security situation: We discovered thousands of flows of\nsensitive data from Android to JavaScript, the vast majority of which could\nflow to potentially untrustworthy code. Our analysis identified numerous web\npages embedding vulnerabilities, which we exemplarily exploited. Additionally,\nwe discovered a multitude of applications in which potentially untrusted\nJavaScript code may interfere with (trusted) Android objects, both in benign\nand malign applications.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 17:58:07 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 02:34:44 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Tiwari", "Abhishek", ""], ["Prakash", "Jyoti", ""], ["Gross", "Sascha", ""], ["Hammer", "Christian", ""]]}, {"id": "2008.01792", "submitter": "Elcin Huseyn", "authors": "Elcin Huseyn", "title": "Deep Learning Based Early Diagnostics of Parkinsons Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.PL eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the world, about 7 to 10 million elderly people are suffering from\nParkinson's Disease (PD) disease. Parkinson's disease is a common neurological\ndegenerative disease, and its clinical characteristics are Tremors, rigidity,\nbradykinesia, and decreased autonomy. Its clinical manifestations are very\nsimilar to Multiple System Atrophy (MSA) disorders. Studies have shown that\npatients with Parkinson's disease often reach an irreparable situation when\ndiagnosed, so As Parkinson's disease can be distinguished from MSA disease and\nget an early diagnosis, people are constantly exploring new methods. With the\nadvent of the era of big data, deep learning has made major breakthroughs in\nimage recognition and classification. Therefore, this study proposes to use The\ndeep learning method to realize the diagnosis of Parkinson's disease, multiple\nsystem atrophy, and healthy people. This data source is from Istanbul\nUniversity Cerrahpasa Faculty of Medicine Hospital. The processing of the\noriginal magnetic resonance image (Magnetic Resonance Image, MRI) is guided by\nthe doctor of Istanbul University Cerrahpasa Faculty of Medicine Hospital. The\nfocus of this experiment is to improve the existing neural network so that it\ncan obtain good results in medical image recognition and diagnosis. An improved\nalgorithm was proposed based on the pathological characteristics of Parkinson's\ndisease, and good experimental results were obtained by comparing indicators\nsuch as model loss and accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 19:50:52 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Huseyn", "Elcin", ""]]}, {"id": "2008.02140", "submitter": "Francesco Dagnino", "authors": "Francesco Dagnino, Davide Ancona, Elena Zucca", "title": "Flexible coinductive logic programming", "comments": "Paper presented at the 36th International Conference on Logic\n  Programming (ICLP 2019), University Of Calabria, Rende (CS), Italy, September\n  2020, 16 pages", "journal-ref": "Theory and Practice of Logic Programming 20 (2020) 818-833", "doi": "10.1017/S147106842000023X", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive definitions of predicates are usually interpreted either\ninductively or coinductively. Recently, a more powerful approach has been\nproposed, called flexible coinduction, to express a variety of intermediate\ninterpretations, necessary in some cases to get the correct meaning. We provide\na detailed formal account of an extension of logic programming supporting\nflexible coinduction. Syntactically, programs are enriched by coclauses,\nclauses with a special meaning used to tune the interpretation of predicates.\nAs usual, the declarative semantics can be expressed as a fixed point which,\nhowever, is not necessarily the least, nor the greatest one, but is determined\nby the coclauses. Correspondingly, the operational semantics is a combination\nof standard SLD resolution and coSLD resolution. We prove that the operational\nsemantics is sound and complete with respect to declarative semantics\nrestricted to finite comodels. This paper is under consideration for acceptance\nin TPLP.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 13:57:21 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Dagnino", "Francesco", ""], ["Ancona", "Davide", ""], ["Zucca", "Elena", ""]]}, {"id": "2008.02483", "submitter": "EPTCS", "authors": "Laurent Fribourg (CNRS & ENS Paris-Saclay, France), Matthias Heizmann\n  (University of Freiburg, Germany)", "title": "Proceedings 8th International Workshop on Verification and Program\n  Transformation and 7th Workshop on Horn Clauses for Verification and\n  Synthesis", "comments": null, "journal-ref": "EPTCS 320, 2020", "doi": "10.4204/EPTCS.320", "report-no": null, "categories": "cs.LO cs.PL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proceedings consist of a keynote paper by Alberto followed by 6 invited\npapers written by Lorenzo Clemente (U. Warsaw), Alain Finkel (U. Paris-Saclay),\nJohn Gallagher (Roskilde U. and IMDEA Software Institute) et al., Neil Jones\n(U. Copenhagen) et al., Michael Leuschel (Heinrich-Heine U.) and Maurizio\nProietti (IASI-CNR) et al.. These invited papers are followed by 4 regular\npapers accepted at VPT 2020 and the papers of HCVS 2020 which consist of three\ncontributed papers and an invited paper on the third competition of solvers for\nConstrained Horn Clauses.\n  In addition, the abstracts (in HTML format) of 3 invited talks at VPT 2020 by\nAndrzej Skowron (U. Warsaw), Sophie Renault (EPO) and Moa Johansson (Chalmers\nU.), are included.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 07:11:26 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Fribourg", "Laurent", "", "CNRS & ENS Paris-Saclay, France"], ["Heizmann", "Matthias", "", "University of Freiburg, Germany"]]}, {"id": "2008.02927", "submitter": "EPTCS", "authors": "Alberto Pettorossi (University of Rome Tor Vergata, Rome, Italy)", "title": "A Historical Account of My Early Research Interests", "comments": "In Proceedings VPT/HCVS 2020, arXiv:2008.02483", "journal-ref": "EPTCS 320, 2020, pp. 1-28", "doi": "10.4204/EPTCS.320.1", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a brief account of some of the my early research\ninterests. This historical account starts from my laurea thesis on Signal\nTheory and my master thesis on Computation Theory. It recalls some results in\nCombinatory Logic and Term Rewriting Systems. Some other results concern\nProgram Transformation, Parallel Computation, Theory of Concurrency, and Proof\nof Program Properties. My early research activity has been mainly done in\ncooperation with Andrzej Skowron, Anna Labella, and Maurizio Proietti.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 01:22:23 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Pettorossi", "Alberto", "", "University of Rome Tor Vergata, Rome, Italy"]]}, {"id": "2008.02931", "submitter": "EPTCS", "authors": "John P. Gallagher (Roskilde University, Denmark and IMDEA Software\n  Institute, Spain), Manuel Hermenegildo (IMDEA Software Institute, Spain),\n  Bishoksan Kafle (IMDEA Software Institute, Spain), Maximiliano Klemen (IMDEA\n  Software Institute, Spain), Pedro L\\'opez Garc\\'ia (IMDEA Software Institute,\n  Spain), Jos\\'e Morales (IMDEA Software Institute, Spain)", "title": "From Big-Step to Small-Step Semantics and Back with Interpreter\n  Specialisation", "comments": "In Proceedings VPT/HCVS 2020, arXiv:2008.02483", "journal-ref": "EPTCS 320, 2020, pp. 50-64", "doi": "10.4204/EPTCS.320.4", "report-no": null, "categories": "cs.PL cs.LO cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate representations of imperative programs as constrained Horn\nclauses. Starting from operational semantics transition rules, we proceed by\nwriting interpreters as constrained Horn clause programs directly encoding the\nrules. We then specialise an interpreter with respect to a given source program\nto achieve a compilation of the source language to Horn clauses (an instance of\nthe first Futamura projection). The process is described in detail for an\ninterpreter for a subset of C, directly encoding the rules of big-step\noperational semantics for C. A similar translation based on small-step\nsemantics could be carried out, but we show an approach to obtaining a\nsmall-step representation using a linear interpreter for big-step Horn clauses.\nThis interpreter is again specialised to achieve the translation from big-step\nto small-step style. The linear small-step program can be transformed back to a\nbig-step non-linear program using a third interpreter. A regular path\nexpression is computed for the linear program using Tarjan's algorithm, and\nthis regular expression then guides an interpreter to compute a program path.\nThe transformation is realised by specialisation of the path interpreter. In\nall of the transformation phases, we use an established partial evaluator and\nexploit standard logic program transformation to remove redundant data\nstructures and arguments in predicates and rename predicates to make clear\ntheir link to statements in the original source program.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 01:23:04 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Gallagher", "John P.", "", "Roskilde University, Denmark and IMDEA Software\n  Institute, Spain"], ["Hermenegildo", "Manuel", "", "IMDEA Software Institute, Spain"], ["Kafle", "Bishoksan", "", "IMDEA Software Institute, Spain"], ["Klemen", "Maximiliano", "", "IMDEA\n  Software Institute, Spain"], ["Garc\u00eda", "Pedro L\u00f3pez", "", "IMDEA Software Institute,\n  Spain"], ["Morales", "Jos\u00e9", "", "IMDEA Software Institute, Spain"]]}, {"id": "2008.02932", "submitter": "EPTCS", "authors": "Neil D. Jones (University of Copenhagen), Siddharth Bhaskar\n  (University of Copenhagen), Cynthia Kop (Radboud University, Nijmegen), Jakob\n  Grue Simonsen (University of Copenhagen)", "title": "Cons-free Programs and Complexity Classes between LOGSPACE and PTIME", "comments": "In Proceedings VPT/HCVS 2020, arXiv:2008.02483", "journal-ref": "EPTCS 320, 2020, pp. 65-79", "doi": "10.4204/EPTCS.320.5", "report-no": null, "categories": "cs.CC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming language concepts are used to give some new perspectives on a\nlong-standing open problem: is logspace = ptime ?\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 01:23:16 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Jones", "Neil D.", "", "University of Copenhagen"], ["Bhaskar", "Siddharth", "", "University of Copenhagen"], ["Kop", "Cynthia", "", "Radboud University, Nijmegen"], ["Simonsen", "Jakob Grue", "", "University of Copenhagen"]]}, {"id": "2008.02933", "submitter": "EPTCS", "authors": "Michael Leuschel (University of D\\\"usseldorf)", "title": "Prolog for Verification, Analysis and Transformation Tools", "comments": "In Proceedings VPT/HCVS 2020, arXiv:2008.02483", "journal-ref": "EPTCS 320, 2020, pp. 80-94", "doi": "10.4204/EPTCS.320.6", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article examines the use of the Prolog language for writing\nverification, analysis and transformation tools. Guided by experience in\nteaching and the development of verification tools like ProB or specialisation\ntools like ECCE and LOGEN, the article presents an assessment of various\naspects of Prolog and provides guidelines for using them. The article shows the\nusefulness of a few key Prolog features. In particular, it discusses how to\ndeal with negation at the level of the object programs being verified or\nanalysed.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 01:23:29 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Leuschel", "Michael", "", "University of D\u00fcsseldorf"]]}, {"id": "2008.02934", "submitter": "EPTCS", "authors": "Emanuele De Angelis (CNR-IASI, Rome, Italy), Fabio Fioravanti (DEC,\n  University \"G. d'Annunzio\" of Chieti-Pescara, Italy), Maurizio Proietti\n  (CNR-IASI, Rome, Italy)", "title": "Transformational Verification of Quicksort", "comments": "In Proceedings VPT/HCVS 2020, arXiv:2008.02483", "journal-ref": "EPTCS 320, 2020, pp. 95-109", "doi": "10.4204/EPTCS.320.7", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many transformation techniques developed for constraint logic programs, also\nknown as constrained Horn clauses (CHCs), have found new useful applications in\nthe field of program verification. In this paper, we work out a nontrivial case\nstudy through the transformation-based verification approach. We consider the\nfamiliar Quicksort program for sorting lists, written in a functional\nprogramming language, and we verify the pre/-postconditions that specify the\nintended correctness properties of the functions defined in the program. We\nverify these properties by: (1) translating them into CHCs, (2) transforming\nthe CHCs by removing all list occurrences, and (3) checking the satisfiability\nof the transformed CHCs by using the Eldarica solver over booleans and\nintegers. The transformation mentioned at Point (2) requires an extension of\nthe algorithms for the elimination of inductively defined data structures\npresented in previous work, because during one stage of the transformation we\nuse as lemmas some properties that have been proved at previous stages.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 01:23:40 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["De Angelis", "Emanuele", "", "CNR-IASI, Rome, Italy"], ["Fioravanti", "Fabio", "", "DEC,\n  University \"G. d'Annunzio\" of Chieti-Pescara, Italy"], ["Proietti", "Maurizio", "", "CNR-IASI, Rome, Italy"]]}, {"id": "2008.02935", "submitter": "EPTCS", "authors": "Horatiu Cirstea (LORIA, CNRS & INRIA & Universit\\'e de Lorraine),\n  Alexis Grall (LORIA, CNRS & INRIA & Universit\\'e de Lorraine), Dominique\n  M\\'ery (LORIA, CNRS & INRIA & Universit\\'e de Lorraine)", "title": "Generating Distributed Programs from Event-B Models", "comments": "In Proceedings VPT/HCVS 2020, arXiv:2008.02483", "journal-ref": "EPTCS 320, 2020, pp. 110-124", "doi": "10.4204/EPTCS.320.8", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed algorithms offer challenges in checking that they meet their\nspecifications. Verification techniques can be extended to deal with the\nverification of safety properties of distributed algorithms. In this paper, we\npresent an approach for combining correct-by-construction approaches and\ntransformations of formal models (Event-B) into programs (DistAlgo) to address\nthe design of verified distributed programs. We define a subset LB (Local\nEvent-B) of the Event-B modelling language restricted to events modelling the\nclassical actions of distributed programs as internal or local computations,\nsending messages and receiving messages. We define then transformations of the\nvarious elements of the LB language into DistAlgo programs. The general\nmethodology consists in starting from a statement of the problem to program and\nthen progressively producing an LB model obtained after several refinement\nsteps of the initial LB model. The derivation of the LB model is not described\nin the current paper and has already been addressed in other works. The\ntransformation of LB models into DistAlgo programs is illustrated through a\nsimple example. The refinement process and the soundness of the transformation\nallow one to produce correct-by-construction distributed programs.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 01:23:53 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Cirstea", "Horatiu", "", "LORIA, CNRS & INRIA & Universit\u00e9 de Lorraine"], ["Grall", "Alexis", "", "LORIA, CNRS & INRIA & Universit\u00e9 de Lorraine"], ["M\u00e9ry", "Dominique", "", "LORIA, CNRS & INRIA & Universit\u00e9 de Lorraine"]]}, {"id": "2008.02937", "submitter": "EPTCS", "authors": "John P. Gallagher (Roskilde University, Denmark and IMDEA Software\n  Institute, Spain), Robert Gl\\\"uck (Copenhagen University, Denmark)", "title": "An Experiment Combining Specialization with Abstract Interpretation", "comments": "In Proceedings VPT/HCVS 2020, arXiv:2008.02483", "journal-ref": "EPTCS 320, 2020, pp. 155-158", "doi": "10.4204/EPTCS.320.11", "report-no": null, "categories": "cs.PL cs.LO cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was previously shown that control-flow refinement can be achieved by a\nprogram specializer incorporating property-based abstraction, to improve\ntermination and complexity analysis tools. We now show that this purpose-built\nspecializer can be reconstructed in a more modular way, and that the previous\nresults can be achieved using an off-the-shelf partial evaluation tool, applied\nto an abstract interpreter. The key feature of the abstract interpreter is the\nabstract domain, which is the product of the property-based abstract domain\nwith the concrete domain. This language-independent framework provides a\npractical approach to implementing a variety of powerful specializers, and\ncontributes to a stream of research on using interpreters and specialization to\nachieve program transformations.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 01:24:31 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Gallagher", "John P.", "", "Roskilde University, Denmark and IMDEA Software\n  Institute, Spain"], ["Gl\u00fcck", "Robert", "", "Copenhagen University, Denmark"]]}, {"id": "2008.03441", "submitter": "Julie Steele", "authors": "Julie Steele and William Byrd", "title": "dxo: A System for Relational Algebra and Differentiation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present dxo, a relational system for algebra and differentiation, written\nin miniKanren. dxo operates over math expressions, represented as\ns-expressions. dxo supports addition, multiplication, exponentiation, variables\n(represented as tagged symbols), and natural numbers (represented as\nlittle-endian binary lists). We show the full code for dxo, and describe in\ndetail the four main relations that compose dxo. We present example problems\ndxo can solve by combining the main relations. Our differentiation relation,\ndo, can differentiate polynomials, and by running backwards, can also\nintegrate. Similarly, our simplification relation, simpo, can simplify\nexpressions that include addition, multiplication, exponentiation, variables,\nand natural numbers, and by running backwards, can complicate any expression in\nsimplified form. Our evaluation relation, evalo, takes the same types of\nexpressions as simpo, along with an environment associating variables with\nnatural numbers. By evaluating the expression with respect to the environment,\nevalo can produce a natural number; by running backwards, evalo can generate\nexpressions (or the associated environments) that evaluate to a given value.\nreordero also takes the same types of expressions as simpo, and relates\nreordered expressions.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 04:26:21 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 00:23:13 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Steele", "Julie", ""], ["Byrd", "William", ""]]}, {"id": "2008.03649", "submitter": "Edward Pantridge", "authors": "Edward Pantridge, Lee Spector", "title": "Code Building Genetic Programming", "comments": "Proceedings of the 2020 Genetic and Evolutionary Computation\n  Conference, Genetic Programming Track", "journal-ref": null, "doi": "10.1145/3377930.3390239", "report-no": null, "categories": "cs.PL cs.NE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years the field of genetic programming has made significant\nadvances towards automatic programming. Research and development of\ncontemporary program synthesis methods, such as PushGP and Grammar Guided\nGenetic Programming, can produce programs that solve problems typically\nassigned in introductory academic settings. These problems focus on a narrow,\npredetermined set of simple data structures, basic control flow patterns, and\nprimitive, non-overlapping data types (without, for example, inheritance or\ncomposite types). Few, if any, genetic programming methods for program\nsynthesis have convincingly demonstrated the capability of synthesizing\nprograms that use arbitrary data types, data structures, and specifications\nthat are drawn from existing codebases. In this paper, we introduce Code\nBuilding Genetic Programming (CBGP) as a framework within which this can be\ndone, by leveraging programming language features such as reflection and\nfirst-class specifications. CBGP produces a computational graph that can be\nexecuted or translated into source code of a host language. To demonstrate the\nnovel capabilities of CBGP, we present results on new benchmarks that use\nnon-primitive, polymorphic data types as well as some standard program\nsynthesis benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 04:33:04 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Pantridge", "Edward", ""], ["Spector", "Lee", ""]]}, {"id": "2008.03714", "submitter": "Ekaterina Komendantskaya Dr", "authors": "Ekaterina Komendantskaya and Dmitry Rozplokhas and Henning Basold", "title": "The New Normal: We Cannot Eliminate Cuts in Coinductive Calculi, But We\n  Can Explore Them", "comments": "Paper presented at the 36th International Conference on Logic\n  Programming (ICLP 2019), University Of Calabria, Rende (CS), Italy, September\n  2020, 16 pages", "journal-ref": "Theory and Practice of Logic Programming, 2020", "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sequent calculi, cut elimination is a property that guarantees that any\nprovable formula can be proven analytically. For example, Gentzen's classical\nand intuitionistic calculi LK and LJ enjoy cut elimination. The property is\nless studied in coinductive extensions of sequent calculi. In this paper, we\nuse coinductive Horn clause theories to show that cut is not eliminable in a\ncoinductive extension of LJ, a system we call CLJ. We derive two further\npractical results from this study. We show that CoLP by Gupta et al. gives rise\nto cut-free proofs in CLJ with fixpoint terms, and we formulate and implement a\nnovel method of coinductive theory exploration that provides several heuristics\nfor discovery of cut formulae in CLJ.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 12:27:13 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Komendantskaya", "Ekaterina", ""], ["Rozplokhas", "Dmitry", ""], ["Basold", "Henning", ""]]}, {"id": "2008.03719", "submitter": "Daniel Ritter", "authors": "Daniel Ritter and Jan Bro{\\ss}", "title": "A Rule-based Language for Application Integration", "comments": "14 pages, work from 2013/14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although message-based (business) application integration is based on\norchestrated message flows, current modeling languages exclusively cover (parts\nof) the control flow, while under-specifying the data flow. Especially for more\ndata-intensive integration scenarios, this fact adds to the inherent data\nprocessing weakness in conventional integration systems.\n  We argue that with a more data-centric integration language and a relational\nlogic based implementation of integration semantics, optimizations from the\ndata management domain(e.g., data partitioning, parallelization) can be\ncombined with common integration processing (e.g., scatter/gather,\nsplitter/gather). With the Logic Integration Language (LiLa) we redefine\nintegration logic tailored for data-intensive processing and propose a novel\napproach to data-centric integration modeling, from which we derive the\ncontrol-and data flow and apply them to a conventional integration system.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 13:02:12 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Ritter", "Daniel", ""], ["Bro\u00df", "Jan", ""]]}, {"id": "2008.04133", "submitter": "Jarrett Holtz", "authors": "Jarrett Holtz, Arjun Guha, Joydeep Biswas", "title": "Robot Action Selection Learning via Layered Dimension Informed Program\n  Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action selection policies (ASPs), used to compose low-level robot skills into\ncomplex high-level tasks are commonly represented as neural networks (NNs) in\nthe state of the art. Such a paradigm, while very effective, suffers from a few\nkey problems: 1) NNs are opaque to the user and hence not amenable to\nverification, 2) they require significant amounts of training data, and 3) they\nare hard to repair when the domain changes. We present two key insights about\nASPs for robotics. First, ASPs need to reason about physically meaningful\nquantities derived from the state of the world, and second, there exists a\nlayered structure for composing these policies. Leveraging these insights, we\nintroduce layered dimension-informed program synthesis (LDIPS) - by reasoning\nabout the physical dimensions of state variables, and dimensional constraints\non operators, LDIPS directly synthesizes ASPs in a human-interpretable\ndomain-specific language that is amenable to program repair. We present\nempirical results to demonstrate that LDIPS 1) can synthesize effective ASPs\nfor robot soccer and autonomous driving domains, 2) requires two orders of\nmagnitude fewer training examples than a comparable NN representation, and 3)\ncan repair the synthesized ASPs with only a small number of corrections when\ntransferring from simulation to real robots.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 13:52:01 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 20:17:32 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Holtz", "Jarrett", ""], ["Guha", "Arjun", ""], ["Biswas", "Joydeep", ""]]}, {"id": "2008.04165", "submitter": "Alasdair Hill", "authors": "Alasdair Hill, Ekaterina Komendantskaya, Ronald P. A. Petrick", "title": "Proof-Carrying Plans: a Resource Logic for AI Planning", "comments": "PPDP 2020, 13 pages, 9 figures", "journal-ref": null, "doi": "10.1145/3414080.3414094", "report-no": null, "categories": "cs.LO cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent trends in AI verification and Explainable AI have raised the question\nof whether AI planning techniques can be verified. In this paper, we present a\nnovel resource logic, the Proof Carrying Plans (PCP) logic that can be used to\nverify plans produced by AI planners. The PCP logic takes inspiration from\nexisting resource logics (such as Linear logic and Separation logic) as well as\nHoare logic when it comes to modelling states and resource-aware plan\nexecution. It also capitalises on the Curry-Howard approach to logics, in its\ntreatment of plans as functions and plan pre- and post-conditions as types.\nThis paper presents two main results. From the theoretical perspective, we show\nthat the PCP logic is sound relative to the standard possible world semantics\nused in AI planning. From the practical perspective, we present a complete Agda\nformalisation of the PCP logic and of its soundness proof. Moreover, we\nshowcase the Curry-Howard, or functional, value of this implementation by\nsupplementing it with the library that parses AI plans into Agda's proofs\nautomatically. We provide evaluation of this library and the resulting Agda\nfunctions.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 14:45:52 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 03:49:19 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Hill", "Alasdair", ""], ["Komendantskaya", "Ekaterina", ""], ["Petrick", "Ronald P. A.", ""]]}, {"id": "2008.04534", "submitter": "Thomas Ehrhard", "authors": "Thomas Ehrhard (IRIF)", "title": "Upper approximating probabilities of convergence in probabilistic\n  coherence spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a theory of probabilistic coherence spaces equipped with an\nadditional extensional structure and apply it to approximating probability of\nconvergence of ground type programs of probabilistic PCF whose free variables\nare of ground types. To this end we define an adapted version of Krivine\nMachine which computes polynomial approximations of the semantics of these\nprograms in the model. These polynomials provide approximations from below and\nfrom above of probabilities of convergence; this is made possible by extending\nthe language with an error symbol which is extensionally maximal in the model.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 06:17:58 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Ehrhard", "Thomas", "", "IRIF"]]}, {"id": "2008.04669", "submitter": "EPTCS", "authors": "Dimitur Nikolaev Krustev (IGE+XAO Balkan)", "title": "Optimizing Program Size Using Multi-result Supercompilation", "comments": "In Proceedings VPT/HCVS 2020, arXiv:2008.02483. arXiv admin note:\n  identical to arXiv:2006.02204, which has added appendices", "journal-ref": "EPTCS 320, 2020, pp. 125-139", "doi": "10.4204/EPTCS.320.9", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supercompilation is a powerful program transformation technique with numerous\ninteresting applications. Existing methods of supercompilation, however, are\noften very unpredictable with respect to the size of the resulting programs. We\nconsider an approach for controlling result size, based on a combination of\nmulti-result supercompilation and a specific generalization strategy, which\navoids code duplication. The current early experiments with this method show\npromising results - we can keep the size of the result small, while still\nperforming powerful optimizations.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 01:24:07 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Krustev", "Dimitur Nikolaev", "", "IGE+XAO Balkan"]]}, {"id": "2008.04720", "submitter": "Andy King", "authors": "Ed Robbins, Andy King and Jacob M. Howe", "title": "Backjumping is Exception Handling", "comments": null, "journal-ref": "Theory and Practice of Logic Programming 21 (2021) 125-144", "doi": "10.1017/S1471068420000435", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ISO Prolog provides catch and throw to realise the control flow of exception\nhandling. This pearl demonstrates that catch and throw are inconspicuously\namenable to the implementation of backjumping. In fact, they have precisely the\nsemantics required: rewinding the search to a specific point, and carrying of a\npreserved term to that point. The utility of these properties is demonstrated\nthrough an implementation of graph colouring with backjumping and a backjumping\nSAT solver that applies Conflict Driven Clause Learning.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 14:27:22 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Robbins", "Ed", ""], ["King", "Andy", ""], ["Howe", "Jacob M.", ""]]}, {"id": "2008.05166", "submitter": "Albert Benveniste", "authors": "Albert Benveniste (HYCOMES), Beno\\^it Caillaud (HYCOMES), Mathias\n  Malandain (HYCOMES)", "title": "The Mathematical Foundations of Physical Systems Modeling Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern modeling languages for general physical systems, such as Modelica,\nAmesim, or Simscape, rely on Differential Algebraic Equations (DAE), i.e.,\nconstraints of the form f(dot{x},x,u)=0. This drastically facilitates modeling\nfrom first principles of the physics and the reuse of models. In this paper we\ndevelop the mathematical theory needed to establish the development of\ncompilers and tools for DAE based physical modeling languages on solid\nmathematical bases. Unlike Ordinary Differential Equations, DAE exhibit subtle\nissues because of the notion of differentiation index and related latent\nequations -- ODE are DAE of index zero for which no latent equation needs to be\nconsidered. Prior to generating execution code and calling solvers, the\ncompilation of such languages requires a nontrivial \\emph{structural analysis}\nstep that reduces the differentiation index to a level acceptable by DAE\nsolvers. The models supported by tools of the Modelica class involve multiple\nmodes with mode-dependent DAE based dynamics and state-dependent mode\nswitching. Multimode DAE are much more difficult than DAE. The main difficulty\nis the handling of the events of mode change. Unfortunately, the large\nliterature devoted to the mathematical analysis of DAEs does not cover the\nmultimode case, typically saying nothing about mode changes. This lack of\nfoundations causes numerous difficulties to the existing modeling tools. Some\nmodels are well handled, others are not, with no clear boundary between the two\nclasses. In this paper, we develop a comprehensive mathematical approach\nsupporting compilation and code generation for this class of languages. Its\ncore is the structural analysis of multimode DAE systems. As a byproduct of\nthis structural analysis, we propose well sound criteria for accepting or\nrejecting models. For our mathematical development, we rely on nonstandard\nanalysis, which allows us to cast hybrid systems dynamics to discrete time\ndynamics with infinitesimal step size, thus providing a uniform framework for\nhandling both continuous dynamics and mode change events.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 08:30:17 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2021 10:36:10 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Benveniste", "Albert", "", "HYCOMES"], ["Caillaud", "Beno\u00eet", "", "HYCOMES"], ["Malandain", "Mathias", "", "HYCOMES"]]}, {"id": "2008.05555", "submitter": "Ilya Sergey", "authors": "Vaivaswatha Nagaraj, Jacob Johannsen, Anton Trunov, George P\\^irlea,\n  Amrit Kumar, Ilya Sergey", "title": "Compiling a Higher-Order Smart Contract Language to LLVM", "comments": "This is an extended talk abstract submitted to the 2020 Virtual LLVM\n  Developers' Meeting (LLVM2020). It has been accepted as a poster presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scilla is a higher-order polymorphic typed intermediate level language for\nimplementing smart contracts. In this talk, we describe a Scilla compiler\ntargeting LLVM, with a focus on mapping Scilla types, values, and its\nfunctional language constructs to LLVM-IR.\n  The compiled LLVM-IR, when executed with LLVM's JIT framework, achieves a\nspeedup of about 10x over the reference interpreter on a typical Scilla\ncontract. This reduced latency is crucial in the setting of blockchains, where\nsmart contracts are executed as parts of transactions, to achieve peak\ntransactions processed per second. Experiments on the Ackermann function\nachieved a speedup of more than 45x. This talk abstract is aimed at both\nprogramming language researchers looking to implement an LLVM based compiler\nfor their functional language, as well as at LLVM practitioners.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 20:06:33 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Nagaraj", "Vaivaswatha", ""], ["Johannsen", "Jacob", ""], ["Trunov", "Anton", ""], ["P\u00eerlea", "George", ""], ["Kumar", "Amrit", ""], ["Sergey", "Ilya", ""]]}, {"id": "2008.06295", "submitter": "{\\DJ}or{\\dj}e \\v{Z}ikeli\\'c", "authors": "Krishnendu Chatterjee (1), Ehsan Kafshdar Goharshady (2), Petr\n  Novotn\\'y (3), Ji\\v{r}i Z\\'arev\\'ucky (3), {\\DJ}or{\\dj}e \\v{Z}ikeli\\'c (1)\n  ((1) IST Austria, Klosterneuburg, Austria, (2) Ferdowsi University of\n  Mashhad, Mashhad, Iran, (3) Masaryk University, Brno, Czech Republic)", "title": "Proving Almost-Sure Termination of Probabilistic Programs via\n  Incremental Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extension of classical imperative programs with real-valued random\nvariables and random branching gives rise to probabilistic programs. The\ntermination problem is one of the most fundamental liveness properties for such\nprograms. The qualitative (aka almost-sure) termination problem asks whether a\ngiven program terminates with probability 1. Ranking functions provide a sound\nand complete approach for termination of non-probabilistic programs, and their\nextension to probabilistic programs is achieved via ranking supermartingales\n(RSMs). RSMs have been extended to lexicographic RSMs to handle programs with\ninvolved control-flow structure, as well as for compositional approach. There\nare two key limitations of the existing RSM-based approaches: First, the\nlexicographic RSM-based approach requires a strong nonnegativity assumption,\nwhich need not always be satisfied. The second key limitation of the existing\nRSM-based algorithmic approaches is that they rely on pre-computed invariants.\nThe main drawback of relying on pre-computed invariants is the\ninsufficiency-inefficiency trade-off: weak invariants might be insufficient for\nRSMs to prove termination, while using strong invariants leads to inefficiency\nin computing them. Our contributions are twofold: First, we show how to relax\nthe strong nonnegativity condition and still provide soundness guarantee for\nalmost-sure termination. Second, we present an incremental approach where the\nprocess of computing lexicographic RSMs proceeds by iterative pruning of parts\nof the program that were already shown to be terminating, in cooperation with a\nsafety prover. In particular, our technique does not rely on strong\npre-computed invariants. We present experimental results to show the\napplicability of our approach to examples of probabilistic programs from the\nliterature.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 11:22:24 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Goharshady", "Ehsan Kafshdar", ""], ["Novotn\u00fd", "Petr", ""], ["Z\u00e1rev\u00facky", "Ji\u0159i", ""], ["\u017dikeli\u0107", "\u0110or\u0111e", ""]]}, {"id": "2008.06453", "submitter": "Davide Ancona", "authors": "Davide Ancona and Angelo Ferrando and Viviana Mascardi", "title": "Can determinism and compositionality coexist in RML? (extended version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Runtime verification (RV) consists in dynamically verifying that the event\ntraces generated by single runs of a system under scrutiny (SUS) are compliant\nwith the formal specification of its expected properties. RML (Runtime\nMonitoring Language) is a simple but expressive Domain Specific Language for\nRV; its semantics is based on a trace calculus formalized by a deterministic\nrewriting system which drives the implementation of the interpreter of the\nmonitors generated by the RML compiler from the specifications. While\ndeterminism of the trace calculus ensures better performances of the generated\nmonitors, it makes the semantics of its operators less intuitive. In this paper\nwe move a first step towards a compositional semantics of the RML trace\ncalculus, by interpreting its basic operators as operations on sets of\ninstantiated event traces and by proving that such an interpretation is\nequivalent to the operational semantics of the calculus.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 16:33:36 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 15:24:13 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Ancona", "Davide", ""], ["Ferrando", "Angelo", ""], ["Mascardi", "Viviana", ""]]}, {"id": "2008.06587", "submitter": "Amine Hamri", "authors": "Maamar El Amine Hamri", "title": "An Object-Oriented Framework for Designing Reusable and Maintainable\n  DEVS Models using Design Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design patterns are well practices to share software development experiences.\nThese patterns allow enhancing reusability, readability and maintainability of\narchitecture and code of software applications. As simulation applies\ncomputerized models to produce traces in order to obtain results and\nconclusions, designers of simulation explored design patterns to make the\nsimulation code more reusable, more readable and easy to maintain, in addition\nto design complex software oriented simulation modeling.\n  In DEVS (Discrete Event System specification), the designers have\nsuccessfully designed simulations, frameworks, tools, etc. However, some issues\nremain still open and should be explored like how a piece of code that\nimplements a set of states, events and transitions may be reused to design a\nnew DEVS model? How may a DEVS model be extended to a new formalism? Etc.\n  In this paper, we address these issues and we propose a set of patterns that\nmay serve as guidelines to designers of DEVS models and its extensions and may\ncontribute to the design of an operational simulation framework. These patterns\nare inspired partly by the available designs of DEVS community and software\nengineering developers.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 22:03:27 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Hamri", "Maamar El Amine", ""]]}, {"id": "2008.06692", "submitter": "Torsten Schaub", "authors": "Roland Kaminski and Javier Romero and Torsten Schaub and Philipp Wanko", "title": "How to build your own ASP-based system?!", "comments": "69 pages, submitted to TPLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) has become a popular and quite sophisticated\napproach to declarative problem solving. This is arguably due to its attractive\nmodeling-grounding-solving workflow that provides an easy approach to problem\nsolving, even for laypersons outside computer science. Unlike this, the high\ndegree of sophistication of the underlying technology makes it increasingly\nhard for ASP experts to put ideas into practice.\n  For addressing this issue, this tutorial aims at enabling users to build\ntheir own ASP-based systems. More precisely, we show how the ASP system CLINGO\ncan be used for extending ASP and for implementing customized special-purpose\nsystems. To this end, we propose two alternatives. We begin with a traditional\nAI technique and show how meta programming can be used for extending ASP. This\nis a rather light approach that relies on CLINGO's reification feature to use\nASP itself for expressing new functionalities. Unlike this, the major part of\nthis tutorial uses traditional programming (in PYTHON) for manipulating CLINGO\nvia its application programming interface. This approach allows for changing\nand controlling the entire model-ground-solve workflow of ASP. Central to this\nis CLINGO's new Application class that allows us to draw on CLINGO's\ninfrastructure by customizing processes similar to the one in CLINGO. For\ninstance, we may engage manipulations to programs' abstract syntax trees,\ncontrol various forms of multi-shot solving, and set up theory propagators for\nforeign inferences. Another cross-sectional structure, spanning meta as well as\napplication programming, is CLINGO's intermediate format, ASPIF, that specifies\nthe interface among the underlying grounder and solver. We illustrate the\naforementioned concepts and techniques throughout this tutorial by means of\nexamples and several non-trivial case-studies.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 10:08:50 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Kaminski", "Roland", ""], ["Romero", "Javier", ""], ["Schaub", "Torsten", ""], ["Wanko", "Philipp", ""]]}, {"id": "2008.07185", "submitter": "Javier Cabrera Arteaga", "authors": "Javier Cabrera Arteaga, Orestis Floros Malivitsis, Oscar Luis Vera\n  P\\'erez, Benoit Baudry, Martin Monperrus", "title": "CROW: Code Diversification for WebAssembly", "comments": null, "journal-ref": "Proceedings of the Workshop on Measurements, Attacks, and Defenses\n  for the Web (MADWeb), 2021", "doi": "10.14722/madweb.2021.23xxx", "report-no": null, "categories": "cs.SE cs.CR cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The adoption of WebAssembly has rapidly increased in the last few years as it\nprovides a fast and safe model for program execution. However, WebAssembly is\nnot exempt from vulnerabilities that could be exploited by side channels\nattacks. This class of vulnerabilities that can be addressed by code\ndiversification. In this paper, we present the first fully automated workflow\nfor the diversification of WebAssembly binaries. We present CROW, an\nopen-source tool implementing this workflow. We evaluate CROW's capabilities on\n303 C programs and study its use on a real-life security-sensitive program:\nlibsodium, a cryptographic library. Overall, CROWis able to generate diverse\nvariants for 239 out of 303,(79%) small programs. Furthermore, our experiments\nshow that our approach and tool is able to successfully diversify off-the-shelf\ncryptographic software (libsodium).\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 10:00:32 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 07:27:38 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 11:02:47 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Arteaga", "Javier Cabrera", ""], ["Malivitsis", "Orestis Floros", ""], ["P\u00e9rez", "Oscar Luis Vera", ""], ["Baudry", "Benoit", ""], ["Monperrus", "Martin", ""]]}, {"id": "2008.07485", "submitter": "Yuxin Wang", "authors": "Yuxin Wang, Zeyu Ding, Daniel Kifer, Danfeng Zhang", "title": "CheckDP: An Automated and Integrated Approach for Proving Differential\n  Privacy or Finding Precise Counterexamples", "comments": null, "journal-ref": null, "doi": "10.1145/3372297.3417282", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose CheckDP, the first automated and integrated approach for proving\nor disproving claims that a mechanism is differentially private. CheckDP can\nfind counterexamples for mechanisms with subtle bugs for which prior\ncounterexample generators have failed. Furthermore, it was able to\n\\emph{automatically} generate proofs for correct mechanisms for which no formal\nverification was reported before. CheckDP is built on static program analysis,\nallowing it to be more efficient and more precise in catching infrequent events\nthan existing counterexample generators (which run mechanisms hundreds of\nthousands of times to estimate their output distribution). Moreover, its sound\napproach also allows automatic verification of correct mechanisms. When\nevaluated on standard benchmarks and newer privacy mechanisms, CheckDP\ngenerates proofs (for correct mechanisms) and counterexamples (for incorrect\nmechanisms) within 70 seconds without any false positives or false negatives.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 17:25:09 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 02:34:34 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Wang", "Yuxin", ""], ["Ding", "Zeyu", ""], ["Kifer", "Daniel", ""], ["Zhang", "Danfeng", ""]]}, {"id": "2008.07901", "submitter": "Yanhong Annie Liu", "authors": "David S. Warren and Yanhong A. Liu", "title": "LPOP: Challenges and Advances in Logic and Practice of Programming", "comments": "arXiv admin note: substantial text overlap with arXiv:1804.10247 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes the work presented at the first Logic and Practice of\nProgramming (LPOP) Workshop, which was held in Oxford, UK, on July 18, 2018, in\nconjunction with the Federated Logic Conference (FLoC) 2018. Its focus is\nchallenges and advances in logic and practice of programming. The workshop was\norganized around a challenge problem that specifies issues in role-based access\ncontrol (RBAC), with many participants proposing combined imperative and\ndeclarative solutions expressed in the languages of their choice.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 14:28:46 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Warren", "David S.", ""], ["Liu", "Yanhong A.", ""]]}, {"id": "2008.08272", "submitter": "Tung D. Le", "authors": "Tian Jin, Gheorghe-Teodor Bercea, Tung D. Le, Tong Chen, Gong Su,\n  Haruki Imai, Yasushi Negishi, Anh Leu, Kevin O'Brien, Kiyokuni Kawachiya, and\n  Alexandre E. Eichenberger", "title": "Compiling ONNX Neural Network Models Using MLIR", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network models are becoming increasingly popular and have been\nused in various tasks such as computer vision, speech recognition, and natural\nlanguage processing. Machine learning models are commonly trained in a\nresource-rich environment and then deployed in a distinct environment such as\nhigh availability machines or edge devices. To assist the portability of\nmodels, the open-source community has proposed the Open Neural Network Exchange\n(ONNX) standard. In this paper, we present a high-level, preliminary report on\nour onnx-mlir compiler, which generates code for the inference of deep neural\nnetwork models described in the ONNX format. Onnx-mlir is an open-source\ncompiler implemented using the Multi-Level Intermediate Representation (MLIR)\ninfrastructure recently integrated in the LLVM project. Onnx-mlir relies on the\nMLIR concept of dialects to implement its functionality. We propose here two\nnew dialects: (1) an ONNX specific dialect that encodes the ONNX standard\nsemantics, and (2) a loop-based dialect to provide for a common lowering point\nfor all ONNX dialect operations. Each intermediate representation facilitates\nits own characteristic set of graph-level and loop-based optimizations\nrespectively. We illustrate our approach by following several models through\nthe proposed representations and we include some early optimization work and\nperformance results.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 05:28:08 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 01:15:28 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Jin", "Tian", ""], ["Bercea", "Gheorghe-Teodor", ""], ["Le", "Tung D.", ""], ["Chen", "Tong", ""], ["Su", "Gong", ""], ["Imai", "Haruki", ""], ["Negishi", "Yasushi", ""], ["Leu", "Anh", ""], ["O'Brien", "Kevin", ""], ["Kawachiya", "Kiyokuni", ""], ["Eichenberger", "Alexandre E.", ""]]}, {"id": "2008.08923", "submitter": "EPTCS", "authors": "Jurriaan Hage (Utrecht University)", "title": "Proceedings Eighth and Ninth International Workshop on Trends in\n  Functional Programming in Education", "comments": null, "journal-ref": "EPTCS 321, 2020", "doi": "10.4204/EPTCS.321", "report-no": null, "categories": "cs.PL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains five papers, accepted after post-reviewing, based on\npresentations submitted to TFPIE 2019 and TFPIE 2020 that took places in\nVancouver, Canada and Krakow, Poland respectively. TFPIE stands for Trends in\nFunctional Programming in Education, where authors present research and\nexperiences in teaching concepts of functional programming at any level.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 12:21:53 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Hage", "Jurriaan", "", "Utrecht University"]]}, {"id": "2008.09231", "submitter": "Jialu Bao", "authors": "Jialu Bao, Simon Docherty, Justin Hsu, Alexandra Silva", "title": "A Bunched Logic for Conditional Independence", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independence and conditional independence are fundamental concepts for\nreasoning about groups of random variables in probabilistic programs.\nVerification methods for independence are still nascent, and existing methods\ncannot handle conditional independence. We extend the logic of bunched\nimplications (BI) with a non-commutative conjunction and provide a model based\non Markov kernels; conditional independence can be directly captured as a\nlogical formula in this model. Noting that Markov kernels are Kleisli arrows\nfor the distribution monad, we then introduce a second model based on the\npowerset monad and show how it can capture join dependency, a non-probabilistic\nanalogue of conditional independence from database theory. Finally, we develop\na program logic for verifying conditional independence in probabilistic\nprograms.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 23:46:00 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 00:55:54 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Bao", "Jialu", ""], ["Docherty", "Simon", ""], ["Hsu", "Justin", ""], ["Silva", "Alexandra", ""]]}, {"id": "2008.09253", "submitter": "EPTCS", "authors": "Oliver Westphal, Janis Voigtl\\\"ander", "title": "Describing Console I/O Behavior for Testing Student Submissions in\n  Haskell", "comments": "In Proceedings TFPIE 2019 and 2020, arXiv:2008.08923", "journal-ref": "EPTCS 321, 2020, pp. 19-36", "doi": "10.4204/EPTCS.321.2", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a small, formal language for specifying the behavior of simple\nconsole I/O programs. The design is driven by the concrete application case of\ntesting interactive Haskell programs written by students. Specifications are\nstructurally similar to lexical analysis regular expressions, but are augmented\nwith features like global variables that track state and history of program\nruns, enabling expression of an interesting range of dynamic behavior. We give\na semantics for our specification language based on acceptance of execution\ntraces. From this semantics we derive a definition of the set of all traces\nvalid for a given specification. Sampling that set enables us to mechanically\ncheck program behavior against specifications in a probabilistic fashion.\nBeyond testing, other possible uses of the specification language in an\neducation context include related activities like providing more helpful\nfeedback, generating sample solutions, and even generating random exercise\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 01:21:59 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Westphal", "Oliver", ""], ["Voigtl\u00e4nder", "Janis", ""]]}, {"id": "2008.09610", "submitter": "W{\\l}odzimierz Drabent", "authors": "W{\\l}odzimierz Drabent", "title": "Implementing backjumping by throw/1 and catch/3 of Prolog", "comments": "7 pages. This version - an extension (Approach 1a)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss how to implement backjumping (or intelligent backtracking) in\nProlog programs by means of exception handling. This seems impossible in a\ngeneral case. We provide a solution, which works in certain cases, in\nparticular for binary programs. We also provide a kind of approximate solution,\nfor arbitrary programs.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 08:24:26 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 22:03:36 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Drabent", "W\u0142odzimierz", ""]]}, {"id": "2008.09680", "submitter": "Ryan Bernstein", "authors": "Ryan Bernstein, Matthijs V\\'ak\\'ar, Jeannette Wing", "title": "Transforming Probabilistic Programs for Model Checking", "comments": "To be published in Proceedings of the 2020 ACM-IMS Foundations of\n  Data Science Conference", "journal-ref": null, "doi": "10.1145/3412815.3416896", "report-no": null, "categories": "cs.AI cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic programming is perfectly suited to reliable and transparent\ndata science, as it allows the user to specify their models in a high-level\nlanguage without worrying about the complexities of how to fit the models.\nStatic analysis of probabilistic programs presents even further opportunities\nfor enabling a high-level style of programming, by automating time-consuming\nand error-prone tasks. We apply static analysis to probabilistic programs to\nautomate large parts of two crucial model checking methods: Prior Predictive\nChecks and Simulation-Based Calibration. Our method transforms a probabilistic\nprogram specifying a density function into an efficient forward-sampling form.\nTo achieve this transformation, we extract a factor graph from a probabilistic\nprogram using static analysis, generate a set of proposal directed acyclic\ngraphs using a SAT solver, select a graph which will produce provably correct\nsampling code, then generate one or more sampling programs. We allow minimal\nuser interaction to broaden the scope of application beyond what is possible\nwith static analysis alone. We present an implementation targeting the popular\nStan probabilistic programming language, automating large parts of a robust\nBayesian workflow for a wide community of probabilistic programming users.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 21:06:34 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Bernstein", "Ryan", ""], ["V\u00e1k\u00e1r", "Matthijs", ""], ["Wing", "Jeannette", ""]]}, {"id": "2008.09707", "submitter": "Hazem Torfah", "authors": "Sumukh Shivakumar, Hazem Torfah, Ankush Desai, Sanjit A. Seshia", "title": "SOTER on ROS: A Run-Time Assurance Framework on the Robot Operating\n  System", "comments": "20th International Conference on Runtime Verification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an implementation of SOTER, a run-time assurance framework for\nbuilding safe distributed mobile robotic (DMR) systems, on top of the Robot\nOperating System (ROS). The safety of DMR systems cannot always be guaranteed\nat design time, especially when complex, off-the-shelf components are used that\ncannot be verified easily. SOTER addresses this by providing a language-based\napproach for run-time assurance for DMR systems. SOTER implements the reactive\nrobotic software using the language P, a domain-specific language designed for\nimplementing asynchronous event-driven systems, along with an integrated\nrun-time assurance system that allows programmers to use unfortified components\nbut still provide safety guarantees. We describe an implementation of SOTER for\nROS and demonstrate its efficacy using a multi-robot surveillance case study,\nwith multiple run-time assurance modules. Through rigorous simulation, we show\nthat SOTER enabled systems ensure safety, even when using unknown and untrusted\ncomponents.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 22:48:26 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Shivakumar", "Sumukh", ""], ["Torfah", "Hazem", ""], ["Desai", "Ankush", ""], ["Seshia", "Sanjit A.", ""]]}, {"id": "2008.09836", "submitter": "Jinwoo Kim", "authors": "Jinwoo Kim, Qinheping Hu, Loris D'Antoni, Thomas Reps", "title": "Semantics-Guided Synthesis", "comments": "Updated version for camera-ready", "journal-ref": null, "doi": "10.1145/3434311", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a new framework for program synthesis, called\nsemantics-guided synthesis (SemGuS), that allows a user to provide both the\nsyntax and the semantics for the constructs in the language. SemGuS accepts a\nrecursively defined big-step semantics, which allows it, for example, to be\nused to specify and solve synthesis problems over an imperative programming\nlanguage that may contain loops with unbounded behavior. The customizable\nnature of SemGuS also allows synthesis problems to be defined over a\nnon-standard semantics, such as an abstract semantics. In addition to the\nSemGuS framework, we develop an algorithm for solving SemGuS problems that is\ncapable of both synthesizing programs and proving unrealizability, by encoding\na SemGuS problem as a proof search over Constrained Horn Clauses: in\nparticular, our approach is the first that we are aware of that can prove\nunrealizabilty for synthesis problems that involve imperative programs with\nunbounded loops, over an infinite syntactic search space. We implemented the\ntechnique in a tool called MESSY, and applied it to both SyGuS problems(i.e.,\nover expressions) and synthesis problems over an imperative programming\nlanguage.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 13:39:20 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 15:20:14 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Kim", "Jinwoo", ""], ["Hu", "Qinheping", ""], ["D'Antoni", "Loris", ""], ["Reps", "Thomas", ""]]}, {"id": "2008.09909", "submitter": "Yotam Feldman", "authors": "Yotam M. Y. Feldman and Mooly Sagiv and Sharon Shoham and James R.\n  Wilcox", "title": "Learning the Boundary of Inductive Invariants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of invariant inference and its connections to exact\nconcept learning. We define a condition on invariants and their geometry,\ncalled the fence condition, which permits applying theoretical results from\nexact concept learning to answer open problems in invariant inference theory.\nThe condition requires the invariant's boundary---the states whose Hamming\ndistance from the invariant is one---to be backwards reachable from the bad\nstates in a small number of steps. Using this condition, we obtain the first\npolynomial complexity result for an interpolation-based invariant inference\nalgorithm, efficiently inferring monotone DNF invariants with access to a SAT\nsolver as an oracle. We further harness Bshouty's seminal result in concept\nlearning to efficiently infer invariants of a larger syntactic class of\ninvariants beyond monotone DNF. Lastly, we consider the robustness of inference\nunder program transformations. We show that some simple transformations\npreserve the fence condition, and that it is sensitive to more complex\ntransformations.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 20:34:13 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 19:28:55 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Feldman", "Yotam M. Y.", ""], ["Sagiv", "Mooly", ""], ["Shoham", "Sharon", ""], ["Wilcox", "James R.", ""]]}, {"id": "2008.10707", "submitter": "Yangruibo Ding", "authors": "Yangruibo Ding, Baishakhi Ray, Premkumar Devanbu, Vincent J.\n  Hellendoorn", "title": "Patching as Translation: the Data and the Metaphor", "comments": null, "journal-ref": null, "doi": "10.1145/3324884.3416587", "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning models from other fields, like Computational Linguistics,\nhave been transplanted to Software Engineering tasks, often quite successfully.\nYet a transplanted model's initial success at a given task does not necessarily\nmean it is well-suited for the task. In this work, we examine a common example\nof this phenomenon: the conceit that \"software patching is like language\ntranslation\". We demonstrate empirically that there are subtle, but critical\ndistinctions between sequence-to-sequence models and translation model: while\nprogram repair benefits greatly from the former, general modeling architecture,\nit actually suffers from design decisions built into the latter, both in terms\nof translation accuracy and diversity. Given these findings, we demonstrate how\na more principled approach to model design, based on our empirical findings and\ngeneral knowledge of software development, can lead to better solutions. Our\nfindings also lend strong support to the recent trend towards synthesizing\nedits of code conditional on the buggy context, to repair bugs. We implement\nsuch models ourselves as \"proof-of-concept\" tools and empirically confirm that\nthey behave in a fundamentally different, more effective way than the studied\ntranslation-based architectures. Overall, our results demonstrate the merit of\nstudying the intricacies of machine learned models in software engineering: not\nonly can this help elucidate potential issues that may be overshadowed by\nincreases in accuracy; it can also help innovate on these models to raise the\nstate-of-the-art further. We will publicly release our replication data and\nmaterials at https://github.com/ARiSE-Lab/Patch-as-translation.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 21:05:27 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 02:33:19 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Ding", "Yangruibo", ""], ["Ray", "Baishakhi", ""], ["Devanbu", "Premkumar", ""], ["Hellendoorn", "Vincent J.", ""]]}, {"id": "2008.11256", "submitter": "Gilbert Bernstein", "authors": "Gilbert Bernstein and Michael Mara and Tzu-Mao Li and Dougal Maclaurin\n  and Jonathan Ragan-Kelley", "title": "Differentiating a Tensor Language", "comments": "In-progress Draft; unsubmitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  How does one compile derivatives of tensor programs, such that the resulting\ncode is purely functional (hence easier to optimize and parallelize) and\nprovably efficient relative to the original program? We show that naively\ndifferentiating tensor code---as done in popular systems like Tensorflow and\nPyTorch---can cause asymptotic slowdowns in pathological cases, violating the\nCheap Gradients Principle. However, all existing automatic differentiation\nmethods that guarantee this principle (for variable size data) do so by relying\non += mutation through aliases/pointers---which complicates downstream\noptimization. We provide the first purely functional, provably efficient,\nadjoint/reverse-mode derivatives of array/tensor code by explicitly accounting\nfor sparsity. We do this by focusing on the indicator function from Iverson's\nAPL. We also introduce a new \"Tensor SSA\" normal form and a new derivation of\nreverse-mode automatic differentiation based on the universal property of\ninner-products.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 20:30:05 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Bernstein", "Gilbert", ""], ["Mara", "Michael", ""], ["Li", "Tzu-Mao", ""], ["Maclaurin", "Dougal", ""], ["Ragan-Kelley", "Jonathan", ""]]}, {"id": "2008.11476", "submitter": "M. Akif \\\"Ozkan", "authors": "M. Akif \\\"Ozkan, Burak Ok, Bo Qiao, J\\\"urgen Teich, Frank Hannig", "title": "HipaccVX: Wedding of OpenVX and DSL-based Code Generation", "comments": null, "journal-ref": "Journal of Real-Time Image Processing, 2020", "doi": "10.1007/s11554-020-01015-5", "report-no": null, "categories": "cs.CV cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writing programs for heterogeneous platforms optimized for high performance\nis hard since this requires the code to be tuned at a low level with\narchitecture-specific optimizations that are most times based on fundamentally\ndiffering programming paradigms and languages. OpenVX promises to solve this\nissue for computer vision applications with a royalty-free industry standard\nthat is based on a graph-execution model. Yet, the OpenVX' algorithm space is\nconstrained to a small set of vision functions. This hinders accelerating\ncomputations that are not included in the standard.\n  In this paper, we analyze OpenVX vision functions to find an orthogonal set\nof computational abstractions. Based on these abstractions, we couple an\nexisting Domain-Specific Language (DSL) back end to the OpenVX environment and\nprovide language constructs to the programmer for the definition of\nuser-defined nodes. In this way, we enable optimizations that are not possible\nto detect with OpenVX graph implementations using the standard computer vision\nfunctions. These optimizations can double the throughput on an Nvidia GTX GPU\nand decrease the resource usage of a Xilinx Zynq FPGA by 50% for our\nbenchmarks. Finally, we show that our proposed compiler framework, called\nHipaccVX, can achieve better results than the state-of-the-art approaches\nNvidia VisionWorks and Halide-HLS.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 10:30:18 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["\u00d6zkan", "M. Akif", ""], ["Ok", "Burak", ""], ["Qiao", "Bo", ""], ["Teich", "J\u00fcrgen", ""], ["Hannig", "Frank", ""]]}, {"id": "2008.11999", "submitter": "Michael Hanus", "authors": "Michael Hanus, Finn Teegen", "title": "Memoized Pull-Tabbing for Functional Logic Programming", "comments": "Part of WFLP 2020 pre-proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pull-tabbing is an evaluation technique for functional logic programs which\ncomputes all non-deterministic results in a single graph structure. Pull-tab\nsteps are local graph transformations to move non-deterministic choices towards\nthe root of an expression. Pull-tabbing is independent of a search strategy so\nthat different strategies (depth-first, breadth-first, parallel) can be used to\nextract the results of a computation. It has been used to compile functional\nlogic languages into imperative or purely functional target languages. Pull-tab\nsteps might duplicate choices in case of shared subexpressions. This could\nresult in a dramatic increase of execution time compared to a backtracking\nimplementation. In this paper we propose a refinement which avoids this\nefficiency problem while keeping all the good properties of pull-tabbing. We\nevaluate a first implementation of this improved technique in the Julia\nprogramming language.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 09:07:36 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Hanus", "Michael", ""], ["Teegen", "Finn", ""]]}, {"id": "2008.12375", "submitter": "EPTCS", "authors": "Marco T. Moraz\\'an (Seton Hall University)", "title": "How to Design While Loops", "comments": "In Proceedings TFPIE 2019 and 2020, arXiv:2008.08923", "journal-ref": "EPTCS 321, 2020, pp. 1-18", "doi": "10.4204/EPTCS.321.1", "report-no": null, "categories": "cs.OH cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beginning students find the syntactic construct known as a while loop\ndifficult to master. The difficulties revolve around guaranteeing loop\ntermination and around learning how to properly sequence mutations to solve a\nproblem. In fact, both of these are intertwined and students need to be taught\na model that helps them reason about how to design while loops. For students\nthat have been introduced to how to design programs using structural recursion,\ngenerative recursion, accumulative recursion, and mutation, the task of\nteaching them how to design while loops is made easier. These students are\nfamiliar, for example, with state variables, termination arguments, and\naccumulator invariants. All of these are fundamental in the design of while\nloops. This articles presents a novel technique used at Seton Hall University\nto introduce beginners to the design of while loops. It presents a design\nrecipe that students can follow step-by-step to establish such things as the\ndriver of the loop, the loop invariant, and the proper sequencing of mutations.\nThe article also presents an example of designing a while-loop based function\nusing the new design recipe.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 01:21:47 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Moraz\u00e1n", "Marco T.", "", "Seton Hall University"]]}, {"id": "2008.12414", "submitter": "EPTCS", "authors": "Ornela Dardha (University of Glasgow), Jurriaan Rot (Radboud\n  University)", "title": "Proceedings Combined 27th International Workshop on Expressiveness in\n  Concurrency and 17th Workshop on Structural Operational Semantics", "comments": null, "journal-ref": "EPTCS 322, 2020", "doi": "10.4204/EPTCS.322", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of EXPRESS/SOS 2020: the Combined 27th\nInternational Workshop on Expressiveness in Concurrency and the 17th Workshop\non Structural Operational Semantics, which was held online, as an affiliated\nworkshop of CONCUR 2020, the 31st International Conference on Concurrency\nTheory. The EXPRESS/SOS workshop series aims at bringing together researchers\ninterested in the formal semantics of systems and programming concepts, and in\nthe expressiveness of computational models.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 23:55:59 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Dardha", "Ornela", "", "University of Glasgow"], ["Rot", "Jurriaan", "", "Radboud\n  University"]]}, {"id": "2008.12543", "submitter": "Philipp K\\\"orner", "authors": "Philipp K\\\"orner, David Schneider, Michael Leuschel", "title": "On the Performance of Bytecode Interpreters in Prolog", "comments": "15 pages. Part of WFLP 2020 pre-proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semantics and the recursive execution model of Prolog make it very\nnatural to express language interpreters in form of AST (Abstract Syntax Tree)\ninterpreters where the execution follows the tree representation of a program.\nAn alternative implementation technique is that of bytecode interpreters. These\ninterpreters transform the program into a compact and linear representation\nbefore evaluating it and are generally considered to be faster and to make\nbetter use of resources.\n  In this paper, we discuss different ways to express the control flow of\ninterpreters in Prolog and present several implementations of AST and bytecode\ninterpreters. On a simple language designed for this purpose, we evaluate\nwhether techniques best known from imperative languages are applicable in\nProlog and how well they perform. Our ultimate goal is to assess which\ninterpreter design in Prolog is the most efficient, as we intend to apply these\nresults to a more complex language. However, we believe the analysis in this\npaper to be of more general interest.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 09:14:53 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["K\u00f6rner", "Philipp", ""], ["Schneider", "David", ""], ["Leuschel", "Michael", ""]]}, {"id": "2008.12545", "submitter": "Philipp K\\\"orner", "authors": "Isabel Wingen, Philipp K\\\"orner", "title": "Effectiveness of Annotation-Based Static Type Inference", "comments": "15 pages. Part of WFLP 2020 pre-proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benefits of static type systems are well-known: they offer guarantees that no\ntype error will occur during runtime and, inherently, inferred types serve as\ndocumentation on how functions are called. On the other hand, many type systems\nhave to limit expressiveness of the language because, in general, it is\nundecidable whether a given program is correct regarding types. Another concern\nthat was not addressed so far is that, for logic programming languages such as\nProlog, it is impossible to distinguish between intended and unintended failure\nand, worse, intended and unintended success without additional annotations.\n  In this paper, we elaborate on and discuss the aforementioned issues. As an\nalternative, we present a static type analysis which is based on plspec.\nInstead of ensuring full type-safety, we aim to statically identify type errors\non a best-effort basis without limiting the expressiveness of Prolog programs.\nFinally, we evaluate our approach on real-world code featured in the SWI\ncommunity packages and a large project implementing a model checker.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 09:17:05 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Wingen", "Isabel", ""], ["K\u00f6rner", "Philipp", ""]]}, {"id": "2008.12592", "submitter": "Nick Webster", "authors": "N. Webster and M. Servetto", "title": "Smoothly Navigating between Functional Reactive Programming and Actors", "comments": "Part of WFLP 2020 pre-proceedings (updated post-comments)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formally define an elegant multi-paradigm unification of Functional\nReactive Programming, Actor Systems, and Object-Oriented Programming. This\nenables an intuitive form of declarative programming, harvesting the power of\nconcurrency while maintaining safety.\n  We use object and reference capabilities to highlight and tame imperative\nfeatures: reference capabilities track aliasing and mutability, and object\ncapabilities track I/O. Formally, our type system limits the scope, impact and\ninteractions of impure code.\n  - Scope: Expressions whose input is pure will behave deterministically.\n  - Impact: Data-races and synchronisation issues are avoided. The only way for\nan actor to behave nondeterministically, is by mutating its state based on\nmessage delivery order.\n  - Interactions: Signals provide a functional boundary between imperative and\nfunctional code, preventing impure code from invalidating functional\nassumptions.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 11:40:21 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 04:56:38 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Webster", "N.", ""], ["Servetto", "M.", ""]]}, {"id": "2008.12613", "submitter": "Kiara Grouwstra", "authors": "Kiara Grouwstra", "title": "Type-driven Neural Programming by Example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.PL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this thesis we look into programming by example (PBE), which is about\nfinding a program mapping given inputs to given outputs. PBE has traditionally\nseen a split between formal versus neural approaches, where formal approaches\ntypically involve deductive techniques such as SAT solvers and types, while the\nneural approaches involve training on sample input-outputs with their\ncorresponding program, typically using sequence-based machine learning\ntechniques such as LSTMs [41]. As a result of this split, programming types had\nyet to be used in neural program synthesis techniques.\n  We propose a way to incorporate programming types into a neural program\nsynthesis approach for PBE. We introduce the Typed Neuro-Symbolic Program\nSynthesis (TNSPS) method based on this idea, and test it in the functional\nprogramming context to empirically verify type information may help improve\ngeneralization in neural synthesizers on limited-size datasets.\n  Our TNSPS model builds upon the existing Neuro-Symbolic Program Synthesis\n(NSPS), a tree-based neural synthesizer combining info from input-output\nexamples plus the current program, by further exposing information on types of\nthose input-output examples, of the grammar production rules, as well as of the\nhole that we wish to expand in the program.\n  We further explain how we generated a dataset within our domain, which uses a\nlimited subset of Haskell as the synthesis language. Finally we discuss several\ntopics of interest that may help take these ideas further. For reproducibility,\nwe release our code publicly.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 12:30:05 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 17:42:54 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2020 11:00:05 GMT"}, {"version": "v4", "created": "Wed, 9 Sep 2020 17:58:09 GMT"}, {"version": "v5", "created": "Thu, 17 Sep 2020 09:51:08 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Grouwstra", "Kiara", ""]]}, {"id": "2008.12716", "submitter": "Baltasar Tranc\\'on Y Widemann", "authors": "Baltasar Tranc\\'on y Widemann, Markus Lepper", "title": "Practical Idiomatic Considerations for Checkable Meta-Logic in\n  Experimental Functional Programming", "comments": "Part of WFLP 2020 pre-proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implementing a complex concept as an executable model in a strongly typed,\npurely functional language hits a sweet spot between mere simulation and formal\nspecification. For research and education it is often desirable to enrich the\nalgorithmic code with meta-logical annotations, variously embodied as\nassertions, theorems or test cases. Checking frameworks use the inherent\nlogical power of the functional paradigm to approximate theorem proving by\nheuristic testing. Here we propose several novel idioms to enhance the\npractical expressivity of checking, namely meta-language marking, nominal\naxiomatics, and constructive existentials. All of these are formulated in\nliterate Haskell'98 with some common language extensions. Their use and impact\nare illustrated by application to a realistic modeling problem.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 15:55:46 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Widemann", "Baltasar Tranc\u00f3n y", ""], ["Lepper", "Markus", ""]]}, {"id": "2008.13016", "submitter": "Moreno Falaschi", "authors": "Linda Brodo, Roberto Bruni and Moreno Falaschi", "title": "SOS Rules for Equivalences of Reaction Systems", "comments": "Part of WFLP 2020 pre-proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reaction Systems (RSs) are a successful computational framework inspired by\nbiological systems. A RS pairs a set of entities with a set of reactions over\nthem. Entities can be used to enable or inhibit each reaction, and are produced\nby reactions. Entities can also be provided by an external context. RS\nsemantics is defined in terms of an (unlabelled) rewrite system: given the\ncurrent set of entities, a rewrite step consists of the application of all and\nonly the enabled reactions. In this paper we define, for the first time, a\nlabelled transition system for RSs in the structural operational semantics\n(SOS) style. This is achieved by distilling a signature whose operators\ndirectly correspond to the ingredients of RSs and by defining some simple SOS\ninference rules for any such operator to define the behaviour of the RS in a\ncompositional way. The rich information recorded in the labels allows us to\ndefine an assertion language to tailor behavioural equivalences on some\nspecific properties or entities. The SOS approach is suited to drive additional\nenhancements of RSs along features such as quantitative measurements of\nentities and communication between RSs. The SOS rules have been also exploited\nto design a prototype implementation in logic programming.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 17:12:02 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Brodo", "Linda", ""], ["Bruni", "Roberto", ""], ["Falaschi", "Moreno", ""]]}, {"id": "2008.13064", "submitter": "Md Rafiqul Islam Rabin", "authors": "Md Rafiqul Islam Rabin, Arjun Mukherjee, Omprakash Gnawali, Mohammad\n  Amin Alipour", "title": "Towards Demystifying Dimensions of Source Code Embeddings", "comments": "1st ACM SIGSOFT International Workshop on Representation Learning for\n  Software Engineering and Program Languages, Co-located with ESEC/FSE\n  (RL+SE&PL'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source code representations are key in applying machine learning techniques\nfor processing and analyzing programs. A popular approach in representing\nsource code is neural source code embeddings that represents programs with\nhigh-dimensional vectors computed by training deep neural networks on a large\nvolume of programs. Although successful, there is little known about the\ncontents of these vectors and their characteristics. In this paper, we present\nour preliminary results towards better understanding the contents of code2vec\nneural source code embeddings. In particular, in a small case study, we use the\ncode2vec embeddings to create binary SVM classifiers and compare their\nperformance with the handcrafted features. Our results suggest that the\nhandcrafted features can perform very close to the highly-dimensional code2vec\nembeddings, and the information gains are more evenly distributed in the\ncode2vec embeddings compared to the handcrafted features. We also find that the\ncode2vec embeddings are more resilient to the removal of dimensions with low\ninformation gains than the handcrafted features. We hope our results serve a\nstepping stone toward principled analysis and evaluation of these code\nrepresentations.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 21:59:11 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 03:53:21 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2020 00:19:28 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Rabin", "Md Rafiqul Islam", ""], ["Mukherjee", "Arjun", ""], ["Gnawali", "Omprakash", ""], ["Alipour", "Mohammad Amin", ""]]}, {"id": "2008.13240", "submitter": "Vanderson Martins do Rosario", "authors": "Vanderson Martins do Rosario, Raphael Zinsly, Sandro Rigo, Edson Borin", "title": "Employing Simulation to Facilitate the Design of Dynamic Code Generators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Translation (DT) is a sophisticated technique that allows the\nimplementation of high-performance emulators and high-level-language virtual\nmachines. In this technique, the guest code is compiled dynamically at runtime.\nConsequently, achieving good performance depends on several design decisions,\nincluding the shape of the regions of code being translated. Researchers and\nengineers explore these decisions to bring the best performance possible.\nHowever, a real DT engine is a very sophisticated piece of software, and\nmodifying one is a hard and demanding task. Hence, we propose using simulation\nto evaluate the impact of design decisions on dynamic translators and present\nRAIn, an open-source DT simulator that facilitates the test of DT's design\ndecisions, such as Region Formation Techniques (RFTs). RAIn outputs several\nstatistics that support the analysis of how design decisions may affect the\nbehavior and the performance of a real DT. We validated RAIn running a set of\nexperiments with six well known RFTs (NET, MRET2, LEI, NETPlus, NET-R, and\nNETPlus-e-r) and showed that it can reproduce well-known results from the\nliterature without the effort of implementing them on a real and complex\ndynamic translator engine.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 18:45:26 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Rosario", "Vanderson Martins do", ""], ["Zinsly", "Raphael", ""], ["Rigo", "Sandro", ""], ["Borin", "Edson", ""]]}, {"id": "2008.13358", "submitter": "EPTCS", "authors": "Ryan Kavanagh (Carnegie Mellon University)", "title": "Substructural Observed Communication Semantics", "comments": "In Proceedings EXPRESS/SOS 2020, arXiv:2008.12414", "journal-ref": "EPTCS 322, 2020, pp. 69-87", "doi": "10.4204/EPTCS.322.7", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session-types specify communication protocols for communicating processes,\nand session-typed languages are often specified using substructural operational\nsemantics given by multiset rewriting systems. We give an observed\ncommunication semantics for a session-typed language with recursion, where a\nprocess's observation is given by its external communications. To do so, we\nintroduce fair executions for multiset rewriting systems, and extract observed\ncommunications from fair process executions. This semantics induces an\nintuitively reasonable notion of observational equivalence that we conjecture\ncoincides with semantic equivalences induced by denotational semantics,\nbisimulations, and barbed congruences for these languages.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 04:34:53 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Kavanagh", "Ryan", "", "Carnegie Mellon University"]]}, {"id": "2008.13359", "submitter": "EPTCS", "authors": "Manfred Schmidt-Schau{\\ss} (Goethe-University, Frankfurt, Germany),\n  David Sabel (LMU, Munich, Germany)", "title": "Correctly Implementing Synchronous Message Passing in the Pi-Calculus By\n  Concurrent Haskell's MVars", "comments": "In Proceedings EXPRESS/SOS 2020, arXiv:2008.12414", "journal-ref": "EPTCS 322, 2020, pp. 88-105", "doi": "10.4204/EPTCS.322.8", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparison of concurrent programming languages and correctness of program\ntransformations in concurrency are the focus of this research. As criterion we\nuse contextual semantics adapted to concurrency, where may -- as well as should\n-- convergence are observed. We investigate the relation between the\nsynchronous pi-calculus and a core language of Concurrent Haskell (CH). The\ncontextual semantics is on the one hand forgiving with respect to the details\nof the operational semantics, and on the other hand implies strong requirements\nfor the interplay between the processes after translation. Our result is that\nCH embraces the synchronous pi-calculus. Our main task is to find and prove\ncorrectness of encodings of pi-calculus channels by CH's concurrency\nprimitives, which are MVars. They behave like (blocking) 1-place buffers\nmodelling the shared-memory. The first developed translation uses an extra\nprivate MVar for every communication.We also automatically generate and check\npotentially correct translations that reuse the MVars where one MVar contains\nthe message and two additional MVars for synchronization are used to model the\nsynchronized communication of a single channel in the pi-calculus.Our automated\nexperimental results lead to the conjecture that one additional MVar is\ninsufficient.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 04:35:18 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Schmidt-Schau\u00df", "Manfred", "", "Goethe-University, Frankfurt, Germany"], ["Sabel", "David", "", "LMU, Munich, Germany"]]}, {"id": "2008.13460", "submitter": "Jan C. Dagef\\\"orde", "authors": "Jan C. Dagef\\\"orde, Herbert Kuchen", "title": "Constraint-Logic Object-Oriented Programming with Free Arrays", "comments": "Part of WFLP 2020 pre-proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint-logic object-oriented programming provides a useful symbiosis\nbetween object-oriented programming and constraint-logic search. The ability to\nuse logic variables, constraints, non-deterministic search, and object-oriented\nprogramming in an integrated way facilitates the combination of search-related\nprogram parts and other business logic in object-oriented applications. With\nthis work we conceptualize array-typed logic variables (\"free arrays\"), thus\ncompleting the set of types that logic variables can assume in constraint-logic\nobject-oriented programming. Free arrays exhibit interesting properties, such\nas indeterminate lengths and non-deterministic accesses to array elements.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 09:57:19 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Dagef\u00f6rde", "Jan C.", ""], ["Kuchen", "Herbert", ""]]}]