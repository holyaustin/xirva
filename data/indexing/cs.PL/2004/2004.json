[{"id": "2004.00215", "submitter": "Eric Goodman", "authors": "Eric L. Goodman, Dirk Grunwald", "title": "Streaming Temporal Graphs: Subgraph Matching", "comments": "Big Data 2019", "journal-ref": "Big Data 2019, pp. 4977-4986", "doi": "10.1109/BigData47090.2019.9006429", "report-no": null, "categories": "cs.PL cs.DC cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate solutions to subgraph matching within a temporal stream of\ndata. We present a high-level language for describing temporal subgraphs of\ninterest, the Streaming Analytics Language (SAL). SAL programs are translated\ninto C++ code that is run in parallel on a cluster. We call this implementation\nof SAL the Streaming Analytics Machine (SAM). SAL programs are succinct,\nrequiring about 20 times fewer lines of code than using the SAM library\ndirectly, or writing an implementation using Apache Flink. To benchmark SAM we\ncalculate finding temporal triangles within streaming netflow data. Also, we\ncompare SAM to an implementation written for Flink. We find that SAM is able to\nscale to 128 nodes or 2560 cores, while Apache Flink has max throughput with 32\nnodes and degrades thereafter. Apache Flink has an advantage when triangles are\nrare, with max aggregate throughput for Flink at 32 nodes greater than the max\nachievable rate of SAM. In our experiments, when triangle occurrence was faster\nthan five per second per node, SAM performed better. Both frameworks may miss\nresults due to latencies in network communication. SAM consistently reported an\naverage of 93.7% of expected results while Flink decreases from 83.7% to 52.1%\nas we increase to the maximum size of the cluster. Overall, SAM can obtain\nrates of 91.8 billion netflows per day.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 03:42:00 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Goodman", "Eric L.", ""], ["Grunwald", "Dirk", ""]]}, {"id": "2004.00262", "submitter": "Kaan Gen\\c{c}", "authors": "Kaan Gen\\c{c} (1), Michael D. Bond (1), Guoqing Harry Xu (2) ((1) Ohio\n  State University, (2) University of California, Los Angeles)", "title": "Crafty: Efficient, HTM-Compatible Persistent Transactions", "comments": "32 pages, 24 figures. To appear in PLDI 2020", "journal-ref": null, "doi": "10.1145/3385412.3385991", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Byte-addressable persistent memory, such as Intel/Micron 3D XPoint, is an\nemerging technology that bridges the gap between volatile memory and persistent\nstorage. Data in persistent memory survives crashes and restarts; however, it\nis challenging to ensure that this data is consistent after failures. Existing\napproaches incur significant performance costs to ensure crash consistency.\nThis paper introduces Crafty, a new approach for ensuring consistency and\natomicity on persistent memory operations using commodity hardware with\nexisting hardware transactional memory (HTM) capabilities, while incurring low\noverhead. Crafty employs a novel technique called nondestructive undo logging\nthat leverages commodity HTM to control persist ordering. Our evaluation shows\nthat Crafty outperforms state-of-the-art prior work under low contention, and\nperforms competitively under high contention.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 07:26:29 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 03:49:56 GMT"}, {"version": "v3", "created": "Tue, 21 Apr 2020 02:55:26 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Gen\u00e7", "Kaan", ""], ["Bond", "Michael D.", ""], ["Xu", "Guoqing Harry", ""]]}, {"id": "2004.00348", "submitter": "Irene Vlassi Pandi", "authors": "Irene Vlassi Pandi, Earl T. Barr, Andrew D. Gordon, and Charles Sutton", "title": "OptTyper: Probabilistic Type Inference by Optimising Logical and Natural\n  Constraints", "comments": "29 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to the type inference problem for dynamic\nlanguages. Our goal is to combine \\emph{logical} constraints, that is,\ndeterministic information from a type system, with \\emph{natural} constraints,\nthat is, uncertain statistical information about types learnt from sources like\nidentifier names. To this end, we introduce a framework for probabilistic type\ninference that combines logic and learning: logical constraints on the types\nare extracted from the program, and deep learning is applied to predict types\nfrom surface-level code properties that are statistically associated. The\nforemost insight of our method is to constrain the predictions from the\nlearning procedure to respect the logical constraints, which we achieve by\nrelaxing the logical inference problem of type prediction into a continuous\noptimisation problem. We build a tool called OptTyper to predict missing types\nfor TypeScript files. OptTyper combines a continuous interpretation of logical\nconstraints derived by classical static analysis of TypeScript code, with\nnatural constraints obtained from a deep learning model, which learns naming\nconventions for types from a large codebase. By evaluating OptTyper, we show\nthat the combination of logical and natural constraints yields a large\nimprovement in performance over either kind of information individually and\nachieves a 4% improvement over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 11:32:28 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 17:12:41 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 19:17:53 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Pandi", "Irene Vlassi", ""], ["Barr", "Earl T.", ""], ["Gordon", "Andrew D.", ""], ["Sutton", "Charles", ""]]}, {"id": "2004.00396", "submitter": "Jan Stolarek", "authors": "Frank Emrich, Sam Lindley, Jan Stolarek, James Cheney, Jonathan Coates", "title": "FreezeML: Complete and Easy Type Inference for First-Class Polymorphism", "comments": "48 pages, 23 Figures. Accepted for PLDI 2020", "journal-ref": null, "doi": "10.1145/3385412.3386003", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ML is remarkable in providing statically typed polymorphism without the\nprogrammer ever having to write any type annotations. The cost of this\nparsimony is that the programmer is limited to a form of polymorphism in which\nquantifiers can occur only at the outermost level of a type and type variables\ncan be instantiated only with monomorphic types.\n  Type inference for unrestricted System F-style polymorphism is undecidable in\ngeneral. Nevertheless, the literature abounds with a range of proposals to\nbridge the gap between ML and System F.\n  We put forth a new proposal, FreezeML, a conservative extension of ML with\ntwo new features. First, let- and lambda-binders may be annotated with\narbitrary System F types. Second, variable occurrences may be frozen,\nexplicitly disabling instantiation. FreezeML is equipped with type-preserving\ntranslations back and forth between System F and admits a type inference\nalgorithm, an extension of algorithm W, that is sound and complete and which\nyields principal types.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 12:48:07 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Emrich", "Frank", ""], ["Lindley", "Sam", ""], ["Stolarek", "Jan", ""], ["Cheney", "James", ""], ["Coates", "Jonathan", ""]]}, {"id": "2004.00514", "submitter": "Roberto Di Cosmo", "authors": "Roberto Di Cosmo (IRIF)", "title": "Archiving and referencing source code with Software Heritage", "comments": "arXiv admin note: substantial text overlap with arXiv:1909.10760", "journal-ref": null, "doi": "10.1007/978-3-030-52200-1_36", "report-no": null, "categories": "cs.DL cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software, and software source code in particular, is widely used in modern\nresearch. It must be properly archived, referenced, described and cited in\norder to build a stable and long lasting corpus of scientic knowledge. In this\narticle we show how the Software Heritage universal source code archive\nprovides a means to fully address the first two concerns, by archiving\nseamlessly all publicly available software source code, and by providing\nintrinsic persistent identifiers that allow to reference it at various\ngranularities in a way that is at the same time convenient and effective. We\ncall upon the research community to adopt widely this approach.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 11:48:58 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Di Cosmo", "Roberto", "", "IRIF"]]}, {"id": "2004.00577", "submitter": "Robert Colvin", "authors": "Robert J. Colvin and Kirsten Winter", "title": "An abstract semantics of speculative execution for reasoning about\n  security vulnerabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning about correctness and security of software is increasingly\ndifficult due to the complexity of modern microarchitectural features such as\nout-of-order execution. A class of security vulnerabilities termed Spectre that\nexploits side effects of speculative, out-of-order execution was announced in\n2018 and has since drawn much attention. In this paper we formalise speculative\nexecution and its side effects with the intention of allowing speculation to be\nreasoned about abstractly at the program level, limiting the exposure to\nprocessor-specific or low-level semantics. To this end we encode and expose\nspeculative execution explicitly in the programming language, rather than\nsolely in the operational semantics; as a result the effects of speculative\nexecution are captured by redefining the meaning of a conditional statement,\nand introducing novel language constructs that model transient execution of an\nalternative branch. We add an abstract cache to the global state of the system,\nand derive some general refinement rules that expose cache side effects due to\nspeculative loads. Underlying this extension is a semantic model that is based\non instruction-level parallelism. The rules are encoded in a simulation tool,\nwhich we use to analyse an abstract specification of a Spectre attack and\nvulnerable code fragments.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 00:19:49 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Colvin", "Robert J.", ""], ["Winter", "Kirsten", ""]]}, {"id": "2004.00768", "submitter": "Justin Gottschlich", "authors": "Roshni G. Iyer, Yizhou Sun, Wei Wang, Justin Gottschlich", "title": "Software Language Comprehension using a Program-Derived Semantics Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional code transformation structures, such as abstract syntax trees\n(ASTs), conteXtual flow graphs (XFGs), and more generally, compiler\nintermediate representations (IRs), may have limitations in extracting\nhigher-order semantics from code. While work has already begun on higher-order\nsemantics lifting (e.g., Aroma's simplified parse tree (SPT), verified\nlifting's lambda calculi, and Halide's intentional domain specific language\n(DSL)), research in this area is still immature. To continue to advance this\nresearch, we present the program-derived semantics graph, a new graphical\nstructure to capture semantics of code. The PSG is designed to provide a single\nstructure for capturing program semantics at multiple levels of abstraction.\nThe PSG may be in a class of emerging structural representations that cannot be\nbuilt from a traditional set of predefined rules and instead must be learned.\nIn this paper, we describe the PSG and its fundamental structural differences\ncompared to state-of-the-art structures. Although our exploration into the PSG\nis in its infancy, our early results and architectural analysis indicate it is\na promising new research direction to automatically extract program semantics.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 01:37:57 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 18:29:39 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2020 17:48:56 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Iyer", "Roshni G.", ""], ["Sun", "Yizhou", ""], ["Wang", "Wei", ""], ["Gottschlich", "Justin", ""]]}, {"id": "2004.00878", "submitter": "Qinheping Hu", "authors": "Qinheping Hu, John Cyphert, Loris D'Antoni, Thomas Reps", "title": "Exact and Approximate Methods for Proving Unrealizability of\n  Syntax-Guided Synthesis Problems", "comments": null, "journal-ref": "PLDI 2020", "doi": "10.1145/3385412.3385979", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of automatically establishing that a given\nsyntax-guided-synthesis (SyGuS) problem is unrealizable (i.e., has no\nsolution). We formulate the problem of proving that a SyGuS problem is\nunrealizable over a finite set of examples as one of solving a set of\nequations: the solution yields an overapproximation of the set of possible\noutputs that any term in the search space can produce on the given examples. If\nnone of the possible outputs agrees with all of the examples, our technique has\nproven that the given SyGuS problem is unrealizable. We then present an\nalgorithm for exactly solving the set of equations that result from SyGuS\nproblems over linear integer arithmetic (LIA) and LIA with conditionals (CLIA),\nthereby showing that LIA and CLIA SyGuS problems over finitely many examples\nare decidable. We implement the proposed technique and algorithms in a tool\ncalled Nay. Nay can prove unrealizability for 70/132 existing SyGuS benchmarks,\nwith running times comparable to those of the state-of-the-art tool Nope.\nMoreover, Nay can solve 11 benchmarks that Nope cannot solve.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 08:53:17 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Hu", "Qinheping", ""], ["Cyphert", "John", ""], ["D'Antoni", "Loris", ""], ["Reps", "Thomas", ""]]}, {"id": "2004.01062", "submitter": "EPTCS", "authors": "Stephanie Balzer (Carnegie Mellon University), Luca Padovani\n  (Universit\\`a di Torino)", "title": "Proceedings of the 12th International Workshop on Programming Language\n  Approaches to Concurrency- and Communication-cEntric Software", "comments": null, "journal-ref": "EPTCS 314, 2020", "doi": "10.4204/EPTCS.314", "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern hardware platforms, from the very small to the very large,\nincreasingly provide parallel and distributed computing resources for\napplications to maximise performance. Many applications therefore need to make\neffective use of tens, hundreds, and even thousands of compute nodes.\nComputation in such systems is thus inherently concurrent and communication\ncentric. Effectively programming such applications is challenging; performance,\ncorrectness, and scalability are difficult to achieve. The development of\neffective programming methodologies for this increasingly parallel landscape\ntherefore demands exploration and understanding of a wide variety of\nfoundational and practical ideas. The International Workshop on Programming\nLanguage Approaches to Concurrency- and Communication-cEntric Software (PLACES)\nis dedicated to work in this area. The workshop offers a forum for researchers\nfrom different fields to exchange new ideas about these challenges to modern\nand future programming, where concurrency and distribution are the norm rather\nthan a marginal concern. This volume contains the proceedings of the 12th\nedition of PLACES, which was co-located with ETAPS 2020 in Dublin, Ireland.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 08:44:26 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Balzer", "Stephanie", "", "Carnegie Mellon University"], ["Padovani", "Luca", "", "Universit\u00e0 di Torino"]]}, {"id": "2004.01122", "submitter": "Xiaodi Wu", "authors": "Shaopeng Zhu, Shih-Han Hung, Shouvanik Chakrabarti, and Xiaodi Wu", "title": "On the Principles of Differentiable Quantum Programming Languages", "comments": "Codes are available at https://github.com/LibertasSpZ/adcompile", "journal-ref": null, "doi": "10.1145/3385412.3386011", "report-no": null, "categories": "cs.PL cs.LG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Quantum Circuits (VQCs), or the so-called quantum\nneural-networks, are predicted to be one of the most important near-term\nquantum applications, not only because of their similar promises as classical\nneural-networks, but also because of their feasibility on near-term noisy\nintermediate-size quantum (NISQ) machines. The need for gradient information in\nthe training procedure of VQC applications has stimulated the development of\nauto-differentiation techniques for quantum circuits. We propose the first\nformalization of this technique, not only in the context of quantum circuits\nbut also for imperative quantum programs (e.g., with controls), inspired by the\nsuccess of differentiable programming languages in classical machine learning.\nIn particular, we overcome a few unique difficulties caused by exotic quantum\nfeatures (such as quantum no-cloning) and provide a rigorous formulation of\ndifferentiation applied to bounded-loop imperative quantum programs, its\ncode-transformation rules, as well as a sound logic to reason about their\ncorrectness. Moreover, we have implemented our code transformation in OCaml and\ndemonstrated the resource-efficiency of our scheme both analytically and\nempirically. We also conduct a case study of training a VQC instance with\ncontrols, which shows the advantage of our scheme over existing\nauto-differentiation for quantum circuits without controls.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 16:46:13 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Zhu", "Shaopeng", ""], ["Hung", "Shih-Han", ""], ["Chakrabarti", "Shouvanik", ""], ["Wu", "Xiaodi", ""]]}, {"id": "2004.01320", "submitter": "EPTCS", "authors": "Bas van den Heuvel (University of Groningen), Jorge A. P\\'erez\n  (University of Groningen)", "title": "Session Type Systems based on Linear Logic: Classical versus\n  Intuitionistic", "comments": "In Proceedings PLACES 2020, arXiv:2004.01062", "journal-ref": "EPTCS 314, 2020, pp. 1-11", "doi": "10.4204/EPTCS.314.1", "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session type systems have been given logical foundations via Curry-Howard\ncorrespondences based on both intuitionistic and classical linear logic. The\ntype systems derived from the two logics enforce communication correctness on\nthe same class of pi-calculus processes, but they are significantly different.\nCaires, Pfenning and Toninho informally observed that, unlike the classical\ntype system, the intuitionistic type system enforces locality for shared\nchannels, i.e. received channels cannot be used for replicated input. In this\npaper, we revisit this observation from a formal standpoint. We develop United\nLinear Logic (ULL), a logic encompassing both classical and intuitionistic\nlinear logic. Then, following the Curry-Howard correspondences for session\ntypes, we define piULL, a session type system for the pi-calculus based on ULL.\nUsing piULL we can formally assess the difference between the intuitionistic\nand classical type systems, and justify the role of locality and symmetry\ntherein.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 01:25:03 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Heuvel", "Bas van den", "", "University of Groningen"], ["P\u00e9rez", "Jorge A.", "", "University of Groningen"]]}, {"id": "2004.01321", "submitter": "EPTCS", "authors": "Anson Miu (Imperial College London), Francisco Ferreira (Imperial\n  College London), Nobuko Yoshida (Imperial College London), Fangyi Zhou\n  (Imperial College London)", "title": "Generating Interactive WebSocket Applications in TypeScript", "comments": "In Proceedings PLACES 2020, arXiv:2004.01062", "journal-ref": "EPTCS 314, 2020, pp. 12-22", "doi": "10.4204/EPTCS.314.2", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancements in mobile device computing power have made interactive web\napplications possible, allowing the web browser to render contents dynamically\nand support low-latency communication with the server. This comes at a cost to\nthe developer, who now needs to reason more about correctness of communication\npatterns in their application as web applications support more complex\ncommunication patterns.\n  Multiparty session types (MPST) provide a framework for verifying conformance\nof implementations to their prescribed communication protocol. Existing\nproposals for applying the MPST framework in application developments either\nneglect the event-driven nature of web applications, or lack compatibility with\nindustry tools and practices, which discourages mainstream adoption by web\ndevelopers.\n  In this paper, we present an implementation of the MPST framework for\ndeveloping interactive web applications using familiar industry tools using\nTypeScript and the React.js framework. The developer can use the Scribble\nprotocol language to specify the protocol and use the Scribble toolchain to\nvalidate and obtain the local protocol for each role. The local protocol\ndescribes the interactions of the global communication protocol observed by the\nrole. We encode the local protocol into TypeScript types, catering for\nserver-side and client-side targets separately. We show that our encoding\nguarantees that only implementations which conform to the protocol can\ntype-check. We demonstrate the effectiveness of our approach through a\nweb-based implementation of the classic Noughts and Crosses game from an MPST\nformalism of the game logic.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 01:25:18 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Miu", "Anson", "", "Imperial College London"], ["Ferreira", "Francisco", "", "Imperial\n  College London"], ["Yoshida", "Nobuko", "", "Imperial College London"], ["Zhou", "Fangyi", "", "Imperial College London"]]}, {"id": "2004.01322", "submitter": "EPTCS", "authors": "Simon J. Gay (School of Computing Science, University of Glasgow, UK),\n  Peter Thiemann (Institut f\\\"ur Informatik, University of Freiburg, Germany),\n  Vasco T. Vasconcelos (Faculdade de Ci\\^encias, University of Lisbon,\n  Portugal)", "title": "Duality of Session Types: The Final Cut", "comments": "In Proceedings PLACES 2020, arXiv:2004.01062", "journal-ref": "EPTCS 314, 2020, pp. 23-33", "doi": "10.4204/EPTCS.314.3", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Duality is a central concept in the theory of session types. Since a flaw was\nfound in the original definition of duality for recursive types, several other\ndefinitions have been published. As their connection is not obvious, we compare\nthe competing definitions, discuss tradeoffs, and prove some equivalences. Some\nof the results are mechanized in Agda.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 01:25:32 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Gay", "Simon J.", "", "School of Computing Science, University of Glasgow, UK"], ["Thiemann", "Peter", "", "Institut f\u00fcr Informatik, University of Freiburg, Germany"], ["Vasconcelos", "Vasco T.", "", "Faculdade de Ci\u00eancias, University of Lisbon,\n  Portugal"]]}, {"id": "2004.01323", "submitter": "EPTCS", "authors": "Nicolas Dilley (University of Kent), Julien Lange (University of Kent)", "title": "Bounded verification of message-passing concurrency in Go using Promela\n  and Spin", "comments": "In Proceedings PLACES 2020, arXiv:2004.01062", "journal-ref": "EPTCS 314, 2020, pp. 34-45", "doi": "10.4204/EPTCS.314.4", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a static verification framework for the message-passing\nfragment of the Go programming language. Our framework extracts models that\nover-approximate the message-passing behaviour of a program. These models, or\nbehavioural types, are encoded in Promela, hence can be efficiently verified\nwith Spin. We improve on previous works by verifying programs that include\ncommunication-related parameters that are unknown at compile-time, i.e.,\nprograms that spawn a parameterised number of threads or that create channels\nwith a parameterised capacity. These programs are checked via a bounded\nverification approach with bounds provided by the user.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 01:25:46 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Dilley", "Nicolas", "", "University of Kent"], ["Lange", "Julien", "", "University of Kent"]]}, {"id": "2004.01324", "submitter": "EPTCS", "authors": "Filipe Casal (LASIGE, Faculdade de Ci\\^encias, Universidade de Lisboa,\n  Portugal), Andreia Mordido (LASIGE, Faculdade de Ci\\^encias, Universidade de\n  Lisboa, Portugal), Vasco T. Vasconcelos (LASIGE, Faculdade de Ci\\^encias,\n  Universidade de Lisboa, Portugal)", "title": "Mixed Sessions: the Other Side of the Tape", "comments": "In Proceedings PLACES 2020, arXiv:2004.01062", "journal-ref": "EPTCS 314, 2020, pp. 46-60", "doi": "10.4204/EPTCS.314.5", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The original paper on Mixed Sessions introduce the side A of the tape: there\nis an encoding of classical sessions into mixed sessions. Here we present side\nB: there is a translation of (a subset of) mixed sessions into classical\nsession types. We prove that the translation is a minimal encoding, according\nto the criteria put forward by Kouzapas, P\\'erez, and Yoshida.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 01:26:04 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Casal", "Filipe", "", "LASIGE, Faculdade de Ci\u00eancias, Universidade de Lisboa,\n  Portugal"], ["Mordido", "Andreia", "", "LASIGE, Faculdade de Ci\u00eancias, Universidade de\n  Lisboa, Portugal"], ["Vasconcelos", "Vasco T.", "", "LASIGE, Faculdade de Ci\u00eancias,\n  Universidade de Lisboa, Portugal"]]}, {"id": "2004.01325", "submitter": "EPTCS", "authors": "Shunsuke Kimura (Gifu University, Japan), Keigo Imai (Gifu University,\n  Japan)", "title": "Fluent Session Programming in C#", "comments": "In Proceedings PLACES 2020, arXiv:2004.01062", "journal-ref": "EPTCS 314, 2020, pp. 61-75", "doi": "10.4204/EPTCS.314.6", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose SessionC#, a lightweight session typed library for safe\nconcurrent/distributed programming. The key features are (1) the improved\nfluent interface which enables writing communication in chained method calls,\nby exploiting C#'s out variables, and (2) amalgamation of session delegation\nwith async/await, which materialises session cancellation in a limited form,\nwhich we call session intervention. We show the effectiveness of our proposal\nvia a Bitcoin miner application.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 01:26:22 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Kimura", "Shunsuke", "", "Gifu University, Japan"], ["Imai", "Keigo", "", "Gifu University,\n  Japan"]]}, {"id": "2004.01360", "submitter": "Mohsen Lesani", "authors": "Jeremiah Griffin, Mohsen Lesani, Narges Shadab, Xizhe Yin", "title": "Temporal Logic of Composable Distributed Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed systems are critical to reliable and scalable computing; however,\nthey are complicated in nature and prone to bugs. To modularly manage this\ncomplexity, network middleware has been traditionally built in layered stacks\nof components. We present a novel approach to compositional verification of\ndistributed stacks to verify each component based on only the specification of\nlower components. We present TLC (Temporal Logic of Components), a novel\ntemporal program logic that offers intuitive inference rules for verification\nof both safety and liveness properties of functional implementations of\ndistributed components. To support compositional reasoning, we define a novel\ntransformation on the assertion language that lowers the specification of a\ncomponent to be used as a subcomponent. We prove the soundness of TLC and the\nlowering transformation with respect to the operational semantics for stacks of\ndistributed components. We successfully apply TLC to compose and verify a stack\nof fundamental distributed components.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 03:39:29 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Griffin", "Jeremiah", ""], ["Lesani", "Mohsen", ""], ["Shadab", "Narges", ""], ["Yin", "Xizhe", ""]]}, {"id": "2004.01618", "submitter": "Timofey Bryksin", "authors": "Timofey Bryksin, Victor Petukhov, Ilya Alexin, Stanislav Prikhodko,\n  Alexey Shpilman, Vladimir Kovalenko, Nikita Povarov", "title": "Using Large-Scale Anomaly Detection on Code to Improve Kotlin Compiler", "comments": null, "journal-ref": null, "doi": "10.1145/3379597.3387447", "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we apply anomaly detection to source code and bytecode to\nfacilitate the development of a programming language and its compiler. We\ndefine anomaly as a code fragment that is different from typical code written\nin a particular programming language. Identifying such code fragments is\nbeneficial to both language developers and end users, since anomalies may\nindicate potential issues with the compiler or with runtime performance.\nMoreover, anomalies could correspond to problems in language design. For this\nstudy, we choose Kotlin as the target programming language. We outline and\ndiscuss approaches to obtaining vector representations of source code and\nbytecode and to the detection of anomalies across vectorized code snippets. The\npaper presents a method that aims to detect two types of anomalies: syntax tree\nanomalies and so-called compiler-induced anomalies that arise only in the\ncompiled bytecode. We describe several experiments that employ different\ncombinations of vectorization and anomaly detection techniques and discuss\ntypes of detected anomalies and their usefulness for language developers. We\ndemonstrate that the extracted anomalies and the underlying extraction\ntechnique provide additional value for language development.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 15:20:06 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Bryksin", "Timofey", ""], ["Petukhov", "Victor", ""], ["Alexin", "Ilya", ""], ["Prikhodko", "Stanislav", ""], ["Shpilman", "Alexey", ""], ["Kovalenko", "Vladimir", ""], ["Povarov", "Nikita", ""]]}, {"id": "2004.01683", "submitter": "Esmitt Ram\\'irez", "authors": "Amaro Duarte, Esmitt Ramirez", "title": "Interpreted Programming Language Extension for 3D Render on the Web", "comments": "in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are tools to ease the 2D/3D graphics development for programmers.\nSometimes, these are not directly accessible for all users requiring commercial\nlicenses or based on trials, or long learning periods before to use them. In\nthe modern world, the time to release final programs is crucial for the company\nsuccessfully, also for saving money. Then, if programmers can handle tools to\nminimize the development time using well-known programming languages, they can\ndeliver final programs on time, with minimum effort. This concept is the goal\nof this paper, offering a tool to create 3D renders over a familiarize\nprogramming language to speed up the web development time process. We present\nan extension of an interpreted programming language with an easy syntax to\ndisplay 3D graphics on the web generating a template in a well-known web\nprogramming language, which can be customized and extended. Our proposal is\nbased on Lua programming language as the input language for programmers,\noffering a web editor which interprets its syntax and exporting templates in\nWebGL over Javascript, also getting immediate output in a web browser. Tests\nshow the effectiveness of our approach focus on the written code lines, also\ngetting the expected output using a few computational resources.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 17:26:32 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Duarte", "Amaro", ""], ["Ramirez", "Esmitt", ""]]}, {"id": "2004.01908", "submitter": "Ingo M\\\"uller", "authors": "Ingo M\\\"uller (1) and Renato Marroqu\\'in (2) and Dimitrios Koutsoukos\n  (1) and Mike Wawrzoniak (1) and Sabir Akhadov (3) and Gustavo Alonso (1) ((1)\n  Systems Group, Department of Computer Science, ETH Zurich, (2) Oracle Labs,\n  (3) Databricks)", "title": "The Collection Virtual Machine: An Abstraction for Multi-Frontend\n  Multi-Backend Data Analysis", "comments": "This paper is currently under review at DaMoN'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Getting the best performance from the ever-increasing number of hardware\nplatforms has been a recurring challenge for data processing systems. In recent\nyears, the advent of data science with its increasingly numerous and complex\ntypes of analytics has made this challenge even more difficult. In practice,\nsystem designers are overwhelmed by the number of combinations and typically\nimplement only one analysis/platform combination, leading to repeated\nimplementation effort -- and a plethora of semi-compatible tools for data\nscientists.\n  In this paper, we propose the \"Collection Virtual Machine\" (or CVM) -- an\nextensible compiler framework designed to keep the specialization process of\ndata analytics systems tractable. It can capture at the same time the essence\nof a large span of low-level, hardware-specific implementation techniques as\nwell as high-level operations of different types of analyses. At its core lies\na language for defining nested, collection-oriented intermediate\nrepresentations (IRs). Frontends produce programs in their IR flavors defined\nin that language, which get optimized through a series of rewritings (possibly\nchanging the IR flavor multiple times) until the program is finally expressed\nin an IR of platform-specific operators. While reducing the overall\nimplementation effort, this also improves the interoperability of both analyses\nand hardware platforms. We have used CVM successfully to build specialized\nbackends for platforms as diverse as multi-core CPUs, RDMA clusters, and\nserverless computing infrastructure in the cloud and expect similar results for\nmany more frontends and hardware platforms in the near future.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 11:02:36 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 19:48:05 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["M\u00fcller", "Ingo", ""], ["Marroqu\u00edn", "Renato", ""], ["Koutsoukos", "Dimitrios", ""], ["Wawrzoniak", "Mike", ""], ["Akhadov", "Sabir", ""], ["Alonso", "Gustavo", ""]]}, {"id": "2004.02504", "submitter": "Nicola Manca Dr.", "authors": "Andrea Corallo, Luca Nassi, Nicola Manca", "title": "Bringing GNU Emacs to Native Code", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": "10.5281/zenodo.3736363", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emacs Lisp (Elisp) is the Lisp dialect used by the Emacs text editor family.\nGNU Emacs can currently execute Elisp code either interpreted or\nbyte-interpreted after it has been compiled to byte-code. In this work we\ndiscuss the implementation of an optimizing compiler approach for Elisp\ntargeting native code. The native compiler employs the byte-compiler's internal\nrepresentation as input and exploits libgccjit to achieve code generation using\nthe GNU Compiler Collection (GCC) infrastructure. Generated executables are\nstored as binary files and can be loaded and unloaded dynamically. Most of the\nfunctionality of the compiler is written in Elisp itself, including several\noptimization passes, paired with a C back-end to interface with the GNU Emacs\ncore and libgccjit. Though still a work in progress, our implementation is able\nto bootstrap a functional Emacs and compile all lexically scoped Elisp files,\nincluding the whole GNU Emacs Lisp Package Archive (ELPA). Native-compiled\nElisp shows an increase of performance ranging from 2.3x up to 42x with respect\nto the equivalent byte-code, measured over a set of small benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 09:18:29 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Corallo", "Andrea", ""], ["Nassi", "Luca", ""], ["Manca", "Nicola", ""]]}, {"id": "2004.02870", "submitter": "Stefan Muller", "authors": "Stefan K. Muller and Kyle Singer and Noah Goldstein and Umut A. Acar\n  and Kunal Agrawal and I-Ting Angelina Lee", "title": "Responsive Parallelism with Futures and State", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the increasing shift to multicore computers, recent work has\ndeveloped language support for responsive parallel applications that mix\ncompute-intensive tasks with latency-sensitive, usually interactive, tasks.\nThese developments include calculi that allow assigning priorities to threads,\ntype systems that can rule out priority inversions, and accompanying cost\nmodels for predicting responsiveness. These advances share one important\nlimitation: all of this work assumes purely functional programming. This is a\nsignificant restriction, because many realistic interactive applications, from\ngames to robots to web servers, use mutable state, e.g., for communication\nbetween threads.\n  In this paper, we lift the restriction concerning the use of state. We\npresent $\\lambda_i^4$, a calculus with implicit parallelism in the form of\nprioritized futures and mutable state in the form of references. Because both\nfutures and references are first-class values, $\\lambda_i^4$ programs can\nexhibit complex dependencies, including interaction between threads and with\nthe external world (users, network, etc). To reason about the responsiveness of\n$\\lambda_i^4$ programs, we extend traditional graph-based cost models for\nparallelism to account for dependencies created via mutable state, and we\npresent a type system to outlaw priority inversions that can lead to unbounded\nblocking. We show that these techniques are practical by implementing them in\nC++ and present an empirical evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 17:59:22 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Muller", "Stefan K.", ""], ["Singer", "Kyle", ""], ["Goldstein", "Noah", ""], ["Acar", "Umut A.", ""], ["Agrawal", "Kunal", ""], ["Lee", "I-Ting Angelina", ""]]}, {"id": "2004.02942", "submitter": "Rhys Compton", "authors": "Rhys Compton, Eibe Frank, Panos Patros, Abigail Koay", "title": "Embedding Java Classes with code2vec: Improvements from Variable\n  Obfuscation", "comments": "In 17th International Conference on Mining Software Repositories\n  (MSR) 2020, Seoul, Republic of Korea. 11 pages", "journal-ref": null, "doi": "10.1145/3379597.3387445", "report-no": null, "categories": "cs.LG cs.PL cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic source code analysis in key areas of software engineering, such as\ncode security, can benefit from Machine Learning (ML). However, many standard\nML approaches require a numeric representation of data and cannot be applied\ndirectly to source code. Thus, to enable ML, we need to embed source code into\nnumeric feature vectors while maintaining the semantics of the code as much as\npossible. code2vec is a recently released embedding approach that uses the\nproxy task of method name prediction to map Java methods to feature vectors.\nHowever, experimentation with code2vec shows that it learns to rely on variable\nnames for prediction, causing it to be easily fooled by typos or adversarial\nattacks. Moreover, it is only able to embed individual Java methods and cannot\nembed an entire collection of methods such as those present in a typical Java\nclass, making it difficult to perform predictions at the class level (e.g., for\nthe identification of malicious Java classes). Both shortcomings are addressed\nin the research presented in this paper. We investigate the effect of\nobfuscating variable names during the training of a code2vec model to force it\nto rely on the structure of the code rather than specific names and consider a\nsimple approach to creating class-level embeddings by aggregating sets of\nmethod embeddings. Our results, obtained on a challenging new collection of\nsource-code classification problems, indicate that obfuscating variable names\nproduces an embedding model that is both impervious to variable naming and more\naccurately reflects code semantics. The datasets, models, and code are shared\nfor further ML research on source code.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 19:05:18 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Compton", "Rhys", ""], ["Frank", "Eibe", ""], ["Patros", "Panos", ""], ["Koay", "Abigail", ""]]}, {"id": "2004.02983", "submitter": "Brijesh Dongol", "authors": "Sadegh Dalvandi, Brijesh Dongol, and Simon Doherty", "title": "Integrating Owicki-Gries for C11-Style Memory Models into Isabelle/HOL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weak memory presents a new challenge for program verification and has\nresulted in the development of a variety of specialised logics. For C11-style\nmemory models, our previous work has shown that it is possible to extend Hoare\nlogic and Owicki-Gries reasoning to verify correctness of weak memory programs.\nThe technique introduces a set of high-level assertions over C11 states\ntogether with a set of basic Hoare-style axioms over atomic weak memory\nstatements (e.g., reads/writes), but retains all other standard proof\nobligations for compound statements. This paper takes this line of work further\nby showing Nipkow and Nieto's encoding of Owicki-Gries in the Isabelle theorem\nprover can be extended to handle C11-style weak memory models in a\nstraightforward manner. We exemplify our techniques over several litmus tests\nfrom the literature and a non-trivial example: Peterson's algorithm adapted for\nC11. For the examples we consider, the proof outlines can be automatically\ndischarged using the existing Isabelle tactics developed by Nipkow and Nieto.\nThe benefit here is that programs can be written using a familiar pseudocode\nsyntax with assertions embedded directly into the program.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 20:20:30 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 21:58:52 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Dalvandi", "Sadegh", ""], ["Dongol", "Brijesh", ""], ["Doherty", "Simon", ""]]}, {"id": "2004.03082", "submitter": "Max Willsey", "authors": "Max Willsey, Chandrakana Nandi, Yisu Remy Wang, Oliver Flatt, Zachary\n  Tatlock, Pavel Panchekha", "title": "egg: Fast and Extensible Equality Saturation", "comments": "25 pages, 15 figures, POPL 2021", "journal-ref": "POPL 2021", "doi": "10.1145/3434304", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  An e-graph efficiently represents a congruence relation over many\nexpressions. Although they were originally developed in the late 1970s for use\nin automated theorem provers, a more recent technique known as equality\nsaturation repurposes e-graphs to implement state-of-the-art, rewrite-driven\ncompiler optimizations and program synthesizers. However, e-graphs remain\nunspecialized for this newer use case. Equality saturation workloads exhibit\ndistinct characteristics and often require ad-hoc e-graph extensions to\nincorporate transformations beyond purely syntactic rewrites.\n  This work contributes two techniques that make e-graphs fast and extensible,\nspecializing them to equality saturation. A new amortized invariant restoration\ntechnique called rebuilding takes advantage of equality saturation's distinct\nworkload, providing asymptotic speedups over current techniques in practice. A\ngeneral mechanism called e-class analyses integrates domain-specific analyses\ninto the e-graph, reducing the need for ad hoc manipulation.\n  We implemented these techniques in a new open-source library called egg. Our\ncase studies on three previously published applications of equality saturation\nhighlight how egg's performance and flexibility enable state-of-the-art results\nacross diverse domains.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 02:26:12 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 19:56:18 GMT"}, {"version": "v3", "created": "Sat, 7 Nov 2020 20:33:53 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Willsey", "Max", ""], ["Nandi", "Chandrakana", ""], ["Wang", "Yisu Remy", ""], ["Flatt", "Oliver", ""], ["Tatlock", "Zachary", ""], ["Panchekha", "Pavel", ""]]}, {"id": "2004.03170", "submitter": "Francesco Ranzato", "authors": "Francesco Ranzato", "title": "Decidability and Synthesis of Abstract Inductive Invariants", "comments": null, "journal-ref": "Proceedings of CONCUR 2020", "doi": "10.4230/LIPIcs.CONCUR.2020.48", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decidability and synthesis of inductive invariants ranging in a given domain\nplay an important role in many software and hardware verification systems. We\nconsider here inductive invariants belonging to an abstract domain $A$ as\ndefined in abstract interpretation, namely, ensuring the existence of the best\napproximation in $A$ of any system property. In this setting, we study the\ndecidability of the existence of abstract inductive invariants in $A$ of\ntransition systems and their corresponding algorithmic synthesis. Our model\nrelies on some general results which relate the existence of abstract inductive\ninvariants with least fixed points of best correct approximations in $A$ of the\ntransfer functions of transition systems and their completeness properties.\nThis approach allows us to derive decidability and synthesis results for\nabstract inductive invariants which are applied to the well-known Kildall's\nconstant propagation and Karr's affine equalities abstract domains. Moreover,\nwe show that a recent general algorithm for synthesizing inductive invariants\nin domains of logical formulae can be systematically derived from our results\nand generalized to a range of algorithms for computing abstract inductive\ninvariants.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 07:31:52 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 16:04:11 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Ranzato", "Francesco", ""]]}, {"id": "2004.03494", "submitter": "Fabian Schuiki", "authors": "Fabian Schuiki, Andreas Kurth, Tobias Grosser, Luca Benini", "title": "LLHD: A Multi-level Intermediate Representation for Hardware Description\n  Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern Hardware Description Languages (HDLs) such as SystemVerilog or VHDL\nare, due to their sheer complexity, insufficient to transport designs through\nmodern circuit design flows. Instead, each design automation tool lowers HDLs\nto its own Intermediate Representation (IR). These tools are monolithic and\nmostly proprietary, disagree in their implementation of HDLs, and while many\nredundant IRs exists, no IR today can be used through the entire circuit design\nflow. To solve this problem, we propose the LLHD multi-level IR. LLHD is\ndesigned as simple, unambiguous reference description of a digital circuit, yet\nfully captures existing HDLs. We show this with our reference compiler on\ndesigns as complex as full CPU cores. LLHD comes with lowering passes to a\nhardware-near structural IR, which readily integrates with existing tools. LLHD\nestablishes the basis for innovation in HDLs and tools without redundant\ncompilers or disjoint IRs. For instance, we implement an LLHD simulator that\nruns up to 2.4x faster than commercial simulators but produces equivalent,\ncycle-accurate results. An initial vertically-integrated research prototype is\ncapable of representing all levels of the IR, implements lowering from the\nbehavioural to the structural IR, and covers a sufficient subset of\nSystemVerilog to support a full CPU design.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 15:46:01 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Schuiki", "Fabian", ""], ["Kurth", "Andreas", ""], ["Grosser", "Tobias", ""], ["Benini", "Luca", ""]]}, {"id": "2004.03557", "submitter": "Stelios Tsampas", "authors": "Stelios Tsampas, Andreas Nuyts, Dominique Devriese and Frank Piessens", "title": "A categorical approach to secure compilation", "comments": "Accepted in Coalgebraic Methods in Computer Science, ver. 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel approach to secure compilation based on maps of\ndistributive laws. We demonstrate through four examples that the coherence\ncriterion for maps of distributive laws can potentially be a viable alternative\nfor compiler security instead of full abstraction, which is the preservation\nand reflection of contextual equivalence. To that end, we also make use of the\nwell-behavedness properties of distributive laws to construct a categorical\nargument for the contextual connotations of bisimilarity.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 17:32:21 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Tsampas", "Stelios", ""], ["Nuyts", "Andreas", ""], ["Devriese", "Dominique", ""], ["Piessens", "Frank", ""]]}, {"id": "2004.03673", "submitter": "Robert Y. Lewis", "authors": "Floris van Doorn, Gabriel Ebner, and Robert Y. Lewis", "title": "Maintaining a Library of Formal Mathematics", "comments": "To appear in Proceedings of CICM 2020", "journal-ref": null, "doi": "10.1007/978-3-030-53518-6_16", "report-no": null, "categories": "cs.PL cs.MS math.HO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lean mathematical library mathlib is developed by a community of users\nwith very different backgrounds and levels of experience. To lower the barrier\nof entry for contributors and to lessen the burden of reviewing contributions,\nwe have developed a number of tools for the library which check proof\ndevelopments for subtle mistakes in the code and generate documentation suited\nfor our varied audience.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 19:52:20 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 11:47:21 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["van Doorn", "Floris", ""], ["Ebner", "Gabriel", ""], ["Lewis", "Robert Y.", ""]]}, {"id": "2004.03924", "submitter": "Dominik Wagner", "authors": "Carol Mak, C.-H. Luke Ong, Hugo Paquet and Dominik Wagner", "title": "Densities of Almost Surely Terminating Probabilistic Programs are\n  Differentiable Almost Everywhere", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-72019-3_16", "report-no": null, "categories": "cs.LO cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the differential properties of higher-order statistical\nprobabilistic programs with recursion and conditioning. Our starting point is\nan open problem posed by Hongseok Yang: what class of statistical probabilistic\nprograms have densities that are differentiable almost everywhere? To formalise\nthe problem, we consider Statistical PCF (SPCF), an extension of call-by-value\nPCF with real numbers, and constructs for sampling and conditioning. We give\nSPCF a sampling-style operational semantics a la Borgstrom et al., and study\nthe associated weight (commonly referred to as the density) function and value\nfunction on the set of possible execution traces. Our main result is that\nalmost-surely terminating SPCF programs, generated from a set of primitive\nfunctions (e.g. the set of analytic functions) satisfying mild closure\nproperties, have weight and value functions that are almost-everywhere\ndifferentiable. We use a stochastic form of symbolic execution to reason about\nalmost-everywhere differentiability. A by-product of this work is that\nalmost-surely terminating deterministic (S)PCF programs with real parameters\ndenote functions that are almost-everywhere differentiable. Our result is of\npractical interest, as almost-everywhere differentiability of the density\nfunction is required to hold for the correctness of major gradient-based\ninference algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 10:40:14 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 16:00:29 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Mak", "Carol", ""], ["Ong", "C. -H. Luke", ""], ["Paquet", "Hugo", ""], ["Wagner", "Dominik", ""]]}, {"id": "2004.04338", "submitter": "Quan Nguyen Hoang", "authors": "Quan Nguyen, Andre Cronje, Michael Kong", "title": "OV: Validity-based Optimistic Smart Contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart contract (SC) platforms form blocks of transactions into a chain and\nexecute them via user-defined smart contracts. In conventional platforms like\nBitcoin and Ethereum, the transactions within a block are executed\n\\emph{sequentially} by the miner and are then validated \\emph{sequentially} by\nthe validators to reach consensus about the final state of the block.\n  In order to leverage the advances of multicores, this paper explores the next\ngeneration of smart contract platforms that enables concurrent execution of\nsuch contracts. Reasoning about the validity of the object states is\nchallenging in concurrent smart contracts. We examine a programming model to\nsupport \\emph{optimistic} execution of SCTs. We introduce a novel programming\nlanguage, so-called OV, and a Solidity API to ease programing of optimistic\nsmart contracts. OV language together with static checking will help reasoning\nabout a crucial property of optimistically executed smart contracts -- the\nvalidity of object states in trustless systems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 02:37:25 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Nguyen", "Quan", ""], ["Cronje", "Andre", ""], ["Kong", "Michael", ""]]}, {"id": "2004.04526", "submitter": "EPTCS", "authors": "Mario Rom\\'an (Tallinn University of Technology)", "title": "Open Diagrams via Coend Calculus", "comments": "In Proceedings ACT 2020, arXiv:2101.07888", "journal-ref": "EPTCS 333, 2021, pp. 65-78", "doi": "10.4204/EPTCS.333.5", "report-no": null, "categories": "math.CT cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphisms in a monoidal category are usually interpreted as processes, and\ngraphically depicted as square boxes. In practice, we are faced with the\nproblem of interpreting what non-square boxes ought to represent in terms of\nthe monoidal category and, more importantly, how should they be composed.\nExamples of this situation include lenses or learners. We propose a description\nof these non-square boxes, which we call open diagrams, using the monoidal\nbicategory of profunctors. A graphical coend calculus can then be used to\nreason about open diagrams and their compositions.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 13:15:06 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 12:55:34 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 13:45:34 GMT"}, {"version": "v4", "created": "Tue, 26 Jan 2021 00:02:44 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Rom\u00e1n", "Mario", "", "Tallinn University of Technology"]]}, {"id": "2004.04613", "submitter": "Nouraldin Jaber", "authors": "Nouraldin Jaber (1), Christopher Wagner (1), Swen Jacobs (2), Milind\n  Kulkarni (1), Roopsha Samanta (1) ((1) Purdue University, (2) CISPA Helmholtz\n  Center for Information Security)", "title": "QuickSilver: A Modeling and Parameterized Verification Framework for\n  Systems with Distributed Agreement", "comments": "41 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully-automated parameterized verification of distributed systems, i.e.,\nverification of systems instantiated with an arbitrary number of processes,\nsuffers from scalability challenges, even when it is decidable. This paper\nseeks to push the boundaries of parameterized verification in the types of\nsystems that can be verified automatically as well as practically, by\nincorporating abstractions into the verification pipeline. We develop a\nframework---QuickSilver---for modeling and automated parameterized reasoning\nabout systems that build on distributed agreement protocols, such as consensus\nor leader election. QuickSilver includes a modeling language, Mercury, with\nprimitives for abstracting distributed agreement, syntactic conditions for\ndecidable and practical parameterized verification of systems modeled in\nMercury, and an implementation that has been demonstrably used for efficient,\nautomated parameterized verification of several benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 15:48:28 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 05:42:23 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Jaber", "Nouraldin", ""], ["Wagner", "Christopher", ""], ["Jacobs", "Swen", ""], ["Kulkarni", "Milind", ""], ["Samanta", "Roopsha", ""]]}, {"id": "2004.04852", "submitter": "Rachit Nigam", "authors": "Rachit Nigam, Sachille Atapattu, Samuel Thomas, Zhijing Li, Theodore\n  Bauer, Yuwei Ye, Apurva Koti, Adrian Sampson, Zhiru Zhang", "title": "Predictable Accelerator Design with Time-Sensitive Affine Types", "comments": "Full paper with soundness proof and MachSuite ports", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Field-programmable gate arrays (FPGAs) provide an opportunity to co-design\napplications with hardware accelerators, yet they remain difficult to program.\nHigh-level synthesis (HLS) tools promise to raise the level of abstraction by\ncompiling C or C++ to accelerator designs. Repurposing legacy software\nlanguages, however, requires complex heuristics to map imperative code onto\nhardware structures. We find that the black-box heuristics in HLS can be\nunpredictable: changing parameters in the program that should improve\nperformance can counterintuitively yield slower and larger designs. This paper\nproposes a type system that restricts HLS to programs that can predictably\ncompile to hardware accelerators. The key idea is to model consumable hardware\nresources with a time-sensitive affine type system that prevents simultaneous\nuses of the same hardware structure. We implement the type system in Dahlia, a\nlanguage that compiles to HLS C++, and show that it can reduce the size of HLS\nparameter spaces while accepting Pareto-optimal designs.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 23:23:07 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 16:48:13 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Nigam", "Rachit", ""], ["Atapattu", "Sachille", ""], ["Thomas", "Samuel", ""], ["Li", "Zhijing", ""], ["Bauer", "Theodore", ""], ["Ye", "Yuwei", ""], ["Koti", "Apurva", ""], ["Sampson", "Adrian", ""], ["Zhang", "Zhiru", ""]]}, {"id": "2004.04896", "submitter": "Nouraldin Jaber", "authors": "Nouraldin Jaber (1), Swen Jacobs (2), Christopher Wagner (1), Milind\n  Kulkarni (1), Roopsha Samanta (1) ((1) Purdue University, (2) CISPA Helmholtz\n  Center for Information Security)", "title": "Parameterized Verification of Systems with Global Synchronization and\n  Guards", "comments": "Conference version published at CAV 2020; this version contains a\n  correction of guard-compatibility conditions C2.1 and C2.2", "journal-ref": "Lecture Notes in Computer Science, vol 12224. Springer (2020)", "doi": "10.1007/978-3-030-53288-8_15", "report-no": null, "categories": "cs.FL cs.DC cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by distributed applications that use consensus or other agreement\nprotocols for global coordination, we define a new computational model for\nparameterized systems that is based on a general global synchronization\nprimitive and allows for global transition guards. Our model generalizes many\nexisting models in the literature, including broadcast protocols and guarded\nprotocols. We show that reachability properties are decidable for systems\nwithout guards, and give sufficient conditions under which they remain\ndecidable in the presence of guards. Furthermore, we investigate cutoffs for\nreachability properties and provide sufficient conditions for small cutoffs in\na number of cases that are inspired by our target applications.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 03:59:22 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 17:23:55 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 02:47:55 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Jaber", "Nouraldin", ""], ["Jacobs", "Swen", ""], ["Wagner", "Christopher", ""], ["Kulkarni", "Milind", ""], ["Samanta", "Roopsha", ""]]}, {"id": "2004.05106", "submitter": "Sam Blackshear", "authors": "Sam Blackshear, David L. Dill, Shaz Qadeer, Clark W. Barrett, John C.\n  Mitchell, Oded Padon, Yoni Zohar", "title": "Resources: A Safe Language Abstraction for Money", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart contracts are programs that implement potentially sophisticated\ntransactions on modern blockchain platforms. In the rapidly evolving blockchain\nenvironment, smart contract programming languages must allow users to write\nexpressive programs that manage and transfer assets, yet provide strong\nprotection against sophisticated attacks. Addressing this need, we present\nflexible and reliable abstractions for programming with digital currency in the\nMove language [Blackshear et al. 2019]. Move uses novel linear [Girard 1987]\nresource types with semantics drawing on C++11 [Stroustrup 2013] and Rust\n[Matsakis and Klock 2014]: when a resource value is assigned to a new memory\nlocation, the location previously holding it must be invalidated. In addition,\na resource type can only be created or destroyed by procedures inside its\ndeclaring module. We present an executable bytecode language with resources and\nprove that it enjoys resource safety, a conservation property for program\nvalues that is analogous to conservation of mass in the physical world.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 16:43:49 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 17:57:52 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Blackshear", "Sam", ""], ["Dill", "David L.", ""], ["Qadeer", "Shaz", ""], ["Barrett", "Clark W.", ""], ["Mitchell", "John C.", ""], ["Padon", "Oded", ""], ["Zohar", "Yoni", ""]]}, {"id": "2004.05249", "submitter": "Gareth Ari Aye", "authors": "Gareth Ari Aye and Gail E. Kaiser", "title": "Sequence Model Design for Code Completion in the Modern IDE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code completion plays a prominent role in modern integrated development\nenvironments (IDEs). Machine learning has become ubiquitous in analogous\nnatural language writing and search software, surfacing more relevant\nautocompletions and search suggestions in fewer keystrokes. Prior research has\nreported training high-accuracy, deep neural networks for modeling source code,\nbut little attention has been given to the practical constraints imposed by\ninteractive developer tools. In particular, neural language models for source\ncode modeling like the one described in Maybe Deep Neural Networks are the Best\nChoice for Modeling Source Code are framed around code completion, but only\nreport accuracy of next-token prediction. However, in order for a language\nmodel (LM) to work well within real-world code completion systems, it must also\nalways make suggestions that produce valid code that typechecks to support code\ncompletion's role in correctness-checking; return instantaneous results to help\nprogrammers code more efficiently in fewer keystrokes; and be small enough to\nfit comfortably on disk and in memory on developer workstations, since\nvirtually all modern IDEs run locally and support offline usage. To meet these\nadditional requirements, we propose a novel design for predicting top-k next\ntokens that combines static analysis' ability to enumerate all valid keywords\nand in-scope identifiers with the ability of a language model to place a\nprobability distribution over them. Our model mixes character-level input\nrepresentation with token output to represent out-of-vocabulary (OOV) tokens\nmeaningfully and minimize prediction latency. OOV tokens can be predicted\nthrough detection of local repetition common in software. This design achieves\nstate-of-art accuracy in source code modeling and fits the constraints imposed\nby real-world code completion implementations in modern IDEs.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 22:40:49 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Aye", "Gareth Ari", ""], ["Kaiser", "Gail E.", ""]]}, {"id": "2004.05267", "submitter": "Benjamin Goertzel", "authors": "Ben Goertzel", "title": "What Kind of Programming Language Best Suits Integrative AGI?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What kind of programming language would be most appropriate to serve the\nneeds of integrative, multi-paradigm, multi-software-system approaches to AGI?\nThis question is broached via exploring the more particular question of how to\ncreate a more scalable and usable version of the \"Atomese\" programming language\nthat forms a key component of the OpenCog AGI design (an \"Atomese 2.0\") . It is\ntentatively proposed that the core of Atomese 2.0 should be a very flexible\nframework of rewriting rules for rewriting a metagraph (where the rules\nthemselves are represented within the same metagraph, and some of the\nintermediate data created and used during the rule-interpretation process may\nbe represented in the same metagraph). This framework should support concurrent\nrewriting of the metagraph according to rules that are labeled with various\nsorts of uncertainty-quantifications, and that are labeled with various sorts\nof types associated with various type systems. A gradual typing approach should\nbe used to enable mixture of rules and other metagraph nodes/links associated\nwith various type systems, and untyped metagraph nodes/links not associated\nwith any type system. This must be done in a way that allows reasonable\nefficiency and scalability, including in concurrent and distributed processing\ncontexts, in the case where a large percentage of of processing time is\noccupied with evaluating static pattern-matching queries on specific subgraphs\nof a large metagraph (including a rich variety of queries such as matches\nagainst nodes representing variables, and matches against whole subgraphs,\netc.).\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 00:20:48 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Goertzel", "Ben", ""]]}, {"id": "2004.05500", "submitter": "Chunyan Mu", "authors": "Chunyan Mu", "title": "Analysing Flow Security Properties in Virtualised Computing Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of reasoning about flow security properties in\nvirtualised computing networks with mobility from perspective of formal\nlanguage. We propose a distributed process algebra CSP_{4v} with security\nlabelled processes for the purpose of formal modelling of virtualised computing\nsystems. Specifically, information leakage can come from observations on\nprocess executions, communications and from cache side channels in the\nvirtualised environment. We describe a cache flow policy to identify such\nflows. A type system of the language is presented to enforce the flow policy\nand control the leakage introduced by observing behaviours of communicating\nprocesses and behaviours of virtual machine (VM) instances during accessing\nshared memory cache.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 22:29:08 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Mu", "Chunyan", ""]]}, {"id": "2004.06969", "submitter": "Martin Sulzmann", "authors": "Martin Sulzmann and Kai Stadtm\\\"uller", "title": "Efficient, Near Complete and Often Sound Hybrid Dynamic Data Race\n  Prediction (extended version)", "comments": "typos, appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic data race prediction aims to identify races based on a single program\nrun represented by a trace. The challenge is to remain efficient while being as\nsound and as complete as possible. Efficient means a linear run-time as\notherwise the method unlikely scales for real-world programs. We introduce an\nefficient, near complete and often sound dynamic data race prediction method\nthat combines the lockset method with several improvements made in the area of\nhappens-before methods. By near complete we mean that the method is complete in\ntheory but for efficiency reasons the implementation applies some optimizations\nthat may result in incompleteness. The method can be shown to be sound for two\nthreads but is unsound in general. We provide extensive experimental data that\nshows that our method works well in practice.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 09:31:15 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 06:49:47 GMT"}, {"version": "v3", "created": "Wed, 19 Aug 2020 22:53:18 GMT"}, {"version": "v4", "created": "Fri, 21 Aug 2020 05:32:54 GMT"}, {"version": "v5", "created": "Wed, 4 Nov 2020 08:09:00 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Sulzmann", "Martin", ""], ["Stadtm\u00fcller", "Kai", ""]]}, {"id": "2004.07313", "submitter": "Md Rafiqul Islam Rabin", "authors": "Md Rafiqul Islam Rabin, Mohammad Amin Alipour", "title": "Evaluation of Generalizability of Neural Program Analyzers under\n  Semantic-Preserving Transformations", "comments": "for related work, see arXiv:2008.01566", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The abundance of publicly available source code repositories, in conjunction\nwith the advances in neural networks, has enabled data-driven approaches to\nprogram analysis. These approaches, called neural program analyzers, use neural\nnetworks to extract patterns in the programs for tasks ranging from development\nproductivity to program reasoning. Despite the growing popularity of neural\nprogram analyzers, the extent to which their results are generalizable is\nunknown.\n  In this paper, we perform a large-scale evaluation of the generalizability of\ntwo popular neural program analyzers using seven semantically-equivalent\ntransformations of programs. Our results caution that in many cases the neural\nprogram analyzers fail to generalize well, sometimes to programs with\nnegligible textual differences. The results provide the initial stepping stones\nfor quantifying robustness in neural program analyzers.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 19:55:06 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 07:10:31 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Rabin", "Md Rafiqul Islam", ""], ["Alipour", "Mohammad Amin", ""]]}, {"id": "2004.07749", "submitter": "Alberto Pettorossi", "authors": "Emanuele De Angelis (1 and 3), Fabio Fioravanti (1), Alberto\n  Pettorossi (2 and 3), Maurizio Proietti (3) ((1) DEC, University G.\n  D'Annunzio, Pescara, Italy, (2) DICII, University of Rome Tor Vergata, Roma,\n  Italy, (3) CNR-IASI, Roma, Italy)", "title": "Removing Algebraic Data Types from Constrained Horn Clauses Using\n  Difference Predicates", "comments": "10th International Joint Conference on Automated Reasoning (IJCAR\n  2020) - version with appendix; added DOI of the final authenticated Springer\n  publication; minor corrections", "journal-ref": "Lecture Notes in Computer Science, vol 12166. Springer, Cham,\n  2020, pp. 83-102", "doi": "10.1007/978-3-030-51074-9_6", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of proving the satisfiability of Constrained Horn\nClauses (CHCs) with Algebraic Data Types (ADTs), such as lists and trees. We\npropose a new technique for transforming CHCs with ADTs into CHCs where\npredicates are defined over basic types, such as integers and booleans, only.\nThus, our technique avoids the explicit use of inductive proof rules during\nsatisfiability proofs. The main extension over previous techniques for ADT\nremoval is a new transformation rule, called differential replacement, which\nallows us to introduce auxiliary predicates corresponding to the lemmas that\nare often needed when making inductive proofs. We present an algorithm that\nuses the new rule, together with the traditional folding/unfolding\ntransformation rules, for the automatic removal of ADTs. We prove that if the\nset of the transformed clauses is satisfiable, then so is the set of the\noriginal clauses. By an experimental evaluation, we show that the use of the\ndifferential replacement rule significantly improves the effectiveness of ADT\nremoval, and we show that our transformation-based approach is competitive with\nrespect to a well-established technique that extends the CVC4 solver with\ninduction.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 16:30:17 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 12:59:09 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 21:24:23 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["De Angelis", "Emanuele", "", "1 and 3"], ["Fioravanti", "Fabio", "", "2 and 3"], ["Pettorossi", "Alberto", "", "2 and 3"], ["Proietti", "Maurizio", ""]]}, {"id": "2004.07761", "submitter": "Pengyu Nie", "authors": "Pengyu Nie, Karl Palmskog, Junyi Jessy Li, Milos Gligoric", "title": "Deep Generation of Coq Lemma Names Using Elaborated Terms", "comments": "Accepted in International Joint Conference on Automated Reasoning\n  (IJCAR 2020). With Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coding conventions for naming, spacing, and other essentially stylistic\nproperties are necessary for developers to effectively understand, review, and\nmodify source code in large software projects. Consistent conventions in\nverification projects based on proof assistants, such as Coq, increase in\nimportance as projects grow in size and scope. While conventions can be\ndocumented and enforced manually at high cost, emerging approaches\nautomatically learn and suggest idiomatic names in Java-like languages by\napplying statistical language models on large code corpora. However, due to its\npowerful language extension facilities and fusion of type checking and\ncomputation, Coq is a challenging target for automated learning techniques. We\npresent novel generation models for learning and suggesting lemma names for Coq\nprojects. Our models, based on multi-input neural networks, are the first to\nleverage syntactic and semantic information from Coq's lexer (tokens in lemma\nstatements), parser (syntax trees), and kernel (elaborated terms) for naming;\nthe key insight is that learning from elaborated terms can substantially boost\nmodel performance. We implemented our models in a toolchain, dubbed Roosterize,\nand applied it on a large corpus of code derived from the Mathematical\nComponents family of projects, known for its stringent coding conventions. Our\nresults show that Roosterize substantially outperforms baselines for suggesting\nlemma names, highlighting the importance of using multi-input models and\nelaborated terms.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 16:54:21 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 16:50:17 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Nie", "Pengyu", ""], ["Palmskog", "Karl", ""], ["Li", "Junyi Jessy", ""], ["Gligoric", "Milos", ""]]}, {"id": "2004.08200", "submitter": "Brijesh Dongol", "authors": "Eleni Bila, Simon Doherty, Brijesh Dongol, John Derrick, Gerhard\n  Schellhorn, and Heike Wehrheim", "title": "Defining and Verifying Durable Opacity: Correctness for Persistent\n  Software Transactional Memory", "comments": "This is the full version of the paper that is to appear in FORTE 2020\n  (https://www.discotec.org/2020/forte)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-volatile memory (NVM), aka persistent memory, is a new paradigm for\nmemory that preserves its contents even after power loss. The expected ubiquity\nof NVM has stimulated interest in the design of novel concepts ensuring\ncorrectness of concurrent programming abstractions in the face of persistency.\nSo far, this has lead to the design of a number of persistent concurrent data\nstructures, built to satisfy an associated notion of correctness: durable\nlinearizability.\n  In this paper, we transfer the principle of durable concurrent correctness to\nthe area of software transactional memory (STM). Software transactional memory\nalgorithms allow for concurrent access to shared state. Like linearizability\nfor concurrent data structures, opacity is the established notion of\ncorrectness for STMs. First, we provide a novel definition of durable opacity\nextending opacity to handle crashes and recovery in the context of NVM. Second,\nwe develop a durably opaque version of an existing STM algorithm, namely the\nTransactional Mutex Lock (TML). Third, we design a proof technique for durable\nopacity based on refinement between TML and an operational characterisation of\ndurable opacity by adapting the TMS2 specification. Finally, we apply this\nproof technique to show that the durable version of TML is indeed durably\nopaque. The correctness proof is mechanized within Isabelle.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 12:20:36 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Bila", "Eleni", ""], ["Doherty", "Simon", ""], ["Dongol", "Brijesh", ""], ["Derrick", "John", ""], ["Schellhorn", "Gerhard", ""], ["Wehrheim", "Heike", ""]]}, {"id": "2004.08450", "submitter": "Eric Koskinen", "authors": "Eric Koskinen, Kshitij Bansal", "title": "Reducing Commutativity Verification to Reachability with Differencing\n  Abstractions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commutativity of data structure methods is of ongoing interest, with roots in\nthe database community. In recent years commutativity has been shown to be a\nkey ingredient to enabling multicore concurrency in contexts such as\nparallelizing compilers, transactional memory, speculative execution and, more\nbroadly, software scalability. Despite this interest, it remains an open\nquestion as to how a data structure's commutativity specification can be\nverified automatically from its implementation.\n  In this paper, we describe techniques to automatically prove the correctness\nof method commutativity conditions from data structure implementations. We\nintroduce a new kind of abstraction that characterizes the ways in which the\neffects of two methods differ depending on the order in which the methods are\napplied, and abstracts away effects of methods that would be the same\nregardless of the order. We then describe a novel algorithm that reduces the\nproblem to reachability, so that off-the-shelf program analysis tools can\nperform the reasoning necessary for proving commutativity. Finally, we describe\na proof-of-concept implementation and experimental results, showing that our\ntool can verify commutativity of data structures such as a memory cell,\ncounter, two-place Set, array-based stack, queue, and a rudimentary hash table.\nWe conclude with a discussion of what makes a data structure's commutativity\nprovable with today's tools and what needs to be done to prove more in the\nfuture.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 20:57:03 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Koskinen", "Eric", ""], ["Bansal", "Kshitij", ""]]}, {"id": "2004.08799", "submitter": "Chengyu Zhang", "authors": "Dominik Winterer, Chengyu Zhang, Zhendong Su", "title": "On the Unusual Effectiveness of Type-Aware Operator Mutations for\n  Testing SMT Solvers", "comments": null, "journal-ref": null, "doi": "10.1145/3428261", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose type-aware operator mutation, a simple, but unusually effective\napproach for testing SMT solvers. The key idea is to mutate operators of\nconforming types within the seed formulas to generate well-typed mutant\nformulas. These mutant formulas are then used as the test cases for SMT\nsolvers. We realized type-aware operator mutation within the OpFuzz tool and\nused it to stress-test Z3 and CVC4, two state-of-the-art SMT solvers.\nType-aware operator mutations are unusually effective: During one year of\nextensive testing with OpFuzz, we reported 1,092 bugs on Z3's and CVC4's\nrespective GitHub issue trackers, out of which 819 unique bugs were confirmed\nand 685 of the confirmed bugs were fixed by the developers. The detected bugs\nare highly diverse -- we found bugs of many different types (soundness bugs,\ninvalid model bugs, crashes, etc.), logics and solver configurations. We have\nfurther conducted an in-depth study of the bugs found by OpFuzz. The study\nresults show that the bugs found by OpFuzz are of high quality. Many of them\naffect core components of the SMT solvers' codebases, and some required major\nchanges for the developers to fix. Among the 819 confirmed bugs found by\nOpFuzz, 184 were soundness bugs, the most critical bugs in SMT solvers, and 489\nwere in the default modes of the solvers. Notably, OpFuzz found 27 critical\nsoundness bugs in CVC4, which has proved to be a very stable SMT solver.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 08:47:29 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 06:28:42 GMT"}, {"version": "v3", "created": "Sun, 25 Oct 2020 14:41:55 GMT"}, {"version": "v4", "created": "Thu, 12 Nov 2020 12:55:36 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Winterer", "Dominik", ""], ["Zhang", "Chengyu", ""], ["Su", "Zhendong", ""]]}, {"id": "2004.09045", "submitter": "Longbin Lai", "authors": "Lu Qin, Longbin Lai, Kongzhang Hao, Zhongxin Zhou, Yiwei Zhao, Yuxing\n  Han, Xuemin Lin, Zhengping Qian, Jingren Zhou", "title": "Taming the Expressiveness and Programmability of Graph Analytical\n  Queries", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph database has enjoyed a boom in the last decade, and graph queries\naccordingly gain a lot of attentions from both the academia and industry. We\nfocus on analytical queries in this paper. While analyzing existing\ndomain-specific languages (DSLs) for analytical queries regarding the\nperspectives of completeness, expressiveness and programmability, we find out\nthat none of existing work has achieved a satisfactory coverage of these\nperspectives. Motivated by this, we propose the \\flash DSL, which is named\nafter the three primitive operators Filter, LocAl and PuSH. We prove that\n\\flash is Turing complete (completeness), and show that it achieves both good\nexpressiveness and programmability for analytical queries. We provide an\nimplementation of \\flash based on code generation, and compare it with native\nC++ codes and existing DSL using representative queries. The experiment results\ndemonstrate \\flash's expressiveness, and its capability of programming complex\nalgorithms that achieve satisfactory runtime.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 04:08:28 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 07:28:06 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Qin", "Lu", ""], ["Lai", "Longbin", ""], ["Hao", "Kongzhang", ""], ["Zhou", "Zhongxin", ""], ["Zhao", "Yiwei", ""], ["Han", "Yuxing", ""], ["Lin", "Xuemin", ""], ["Qian", "Zhengping", ""], ["Zhou", "Jingren", ""]]}, {"id": "2004.09843", "submitter": "Marco Devillers", "authors": "M.C.A. (Marco) Devillers", "title": "Egel -- Graph Rewriting with a Twist", "comments": "3 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egel is an untyped eager combinator toy language. Its primary purpose is to\nshowcase an abstract graph-rewriting semantics allowing a robust memory-safe\nconstruction in C++. Though graph rewriters are normally implemented by\nelaborate machines, this can mostly be avoided with a change in the\nrepresentation of term graphs. With an informal inductive argument, that\nrepresentation is shown to always form directed acyclic graphs. Moreover, this\ngraph semantics can trivially be extended to allow exception handling and cheap\nconcurrency. Egel, the interpreter, exploits this semantics with a\nstraight-forward mapping from combinators to reference-counted C++ objects.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 09:20:46 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["A.", "M. C.", "", "Marco"], ["Devillers", "", ""]]}, {"id": "2004.10158", "submitter": "Kartik Nagar", "authors": "Kartik Nagar, Prasita Mukherjee, Suresh Jagannathan", "title": "Semantics, Specification, and Bounded Verification of Concurrent\n  Libraries in Replicated Systems", "comments": "Extended Version of CAV20 Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geo-replicated systems provide a number of desirable properties such as\nglobally low latency, high availability, scalability, and built-in fault\ntolerance. Unfortunately, programming correct applications on top of such\nsystems has proven to be very challenging, in large part because of the weak\nconsistency guarantees they offer. These complexities are exacerbated when we\ntry to adapt existing highly-performant concurrent libraries developed for\nshared-memory environments to this setting. The use of these libraries,\ndeveloped with performance and scalability in mind, is highly desirable. But,\nidentifying a suitable notion of correctness to check their validity under a\nweakly consistent execution model has not been well-studied, in large part\nbecause it is problematic to naively transplant criteria such as\nlinearizability that has a useful interpretation in a shared-memory context to\na distributed one where the cost of imposing a (logical) global ordering on all\nactions is prohibitive.\n  In this paper, we tackle these issues by proposing appropriate semantics and\nspecifications for highly-concurrent libraries in a weakly-consistent,\nreplicated setting. We use these specifications to develop a static analysis\nframework that can automatically detect correctness violations of library\nimplementations parameterized with respect to the different consistency\npolicies provided by the underlying system. We use our framework to analyze the\nbehavior of a number of highly non-trivial library implementations of stacks,\nqueues, and exchangers. Our results provide the first demonstration that\nautomated correctness checking of concurrent libraries in a weakly\ngeo-replicated setting is both feasible and practical.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 17:03:47 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Nagar", "Kartik", ""], ["Mukherjee", "Prasita", ""], ["Jagannathan", "Suresh", ""]]}, {"id": "2004.10263", "submitter": "Grant Passmore", "authors": "Grant Olney Passmore, Simon Cruanes, Denis Ignatovich, Dave Aitken,\n  Matt Bray, Elijah Kagan, Kostya Kanishev, Ewen Maclean, and Nicola Mometto", "title": "The Imandra Automated Reasoning System (system description)", "comments": "To appear in Proceedings of The International Joint Conference on\n  Automated Reasoning (IJCAR) 2020, Lecture Notes in Artificial Intelligence,\n  Springer-Verlag", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.PL cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Imandra, a modern computational logic theorem prover designed to\nbridge the gap between decision procedures such as SMT, semi-automatic\ninductive provers of the Boyer-Moore family like ACL2, and interactive proof\nassistants for typed higher-order logics. Imandra's logic is computational,\nbased on a pure subset of OCaml in which all functions are terminating, with\nrestrictions on types and higher-order functions that allow conjectures to be\ntranslated into multi-sorted first-order logic with theories, including\narithmetic and datatypes. Imandra has novel features supporting large-scale\nindustrial applications, including a seamless integration of bounded and\nunbounded verification, first-class computable counterexamples, efficiently\nexecutable models and a cloud-native architecture supporting live multiuser\ncollaboration.\n  The core reasoning mechanisms of Imandra are (i) a semi-complete procedure\nfor finding models of formulas in the logic mentioned above, centered around\nthe lazy expansion of recursive functions, and (ii) an inductive waterfall and\nsimplifier which \"lifts\" many Boyer-Moore ideas to our typed higher-order\nsetting.\n  These mechanisms are tightly integrated and subject to many forms of user\ncontrol. Imandra's user interfaces include an interactive toplevel, Jupyter\nnotebooks and asynchronous document-based verification (in the spirit of\nIsabelle's Prover IDE) with VS Code.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 19:57:34 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Passmore", "Grant Olney", ""], ["Cruanes", "Simon", ""], ["Ignatovich", "Denis", ""], ["Aitken", "Dave", ""], ["Bray", "Matt", ""], ["Kagan", "Elijah", ""], ["Kanishev", "Kostya", ""], ["Maclean", "Ewen", ""], ["Mometto", "Nicola", ""]]}, {"id": "2004.10652", "submitter": "Jordan Ott", "authors": "Jordan Ott, Mike Pritchard, Natalie Best, Erik Linstead, Milan Curcic,\n  Pierre Baldi", "title": "A Fortran-Keras Deep Learning Bridge for Scientific Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implementing artificial neural networks is commonly achieved via high-level\nprogramming languages like Python and easy-to-use deep learning libraries like\nKeras. These software libraries come pre-loaded with a variety of network\narchitectures, provide autodifferentiation, and support GPUs for fast and\nefficient computation. As a result, a deep learning practitioner will favor\ntraining a neural network model in Python, where these tools are readily\navailable. However, many large-scale scientific computation projects are\nwritten in Fortran, making it difficult to integrate with modern deep learning\nmethods. To alleviate this problem, we introduce a software library, the\nFortran-Keras Bridge (FKB). This two-way bridge connects environments where\ndeep learning resources are plentiful, with those where they are scarce. The\npaper describes several unique features offered by FKB, such as customizable\nlayers, loss functions, and network ensembles.\n  The paper concludes with a case study that applies FKB to address open\nquestions about the robustness of an experimental approach to global climate\nsimulation, in which subgrid physics are outsourced to deep neural network\nemulators. In this context, FKB enables a hyperparameter search of one hundred\nplus candidate models of subgrid cloud and radiation physics, initially\nimplemented in Keras, to be transferred and used in Fortran. Such a process\nallows the model's emergent behavior to be assessed, i.e. when fit\nimperfections are coupled to explicit planetary-scale fluid dynamics. The\nresults reveal a previously unrecognized strong relationship between offline\nvalidation error and online performance, in which the choice of optimizer\nproves unexpectedly critical. This reveals many neural network architectures\nthat produce considerable improvements in stability including some with reduced\nerror, for an especially challenging training dataset.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 15:10:09 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 00:15:48 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Ott", "Jordan", ""], ["Pritchard", "Mike", ""], ["Best", "Natalie", ""], ["Linstead", "Erik", ""], ["Curcic", "Milan", ""], ["Baldi", "Pierre", ""]]}, {"id": "2004.10657", "submitter": "Miltiadis Allamanis", "authors": "Miltiadis Allamanis, Earl T. Barr, Soline Ducousso, and Zheng Gao", "title": "Typilus: Neural Type Hints", "comments": "Accepted to PLDI 2020", "journal-ref": null, "doi": "10.1145/3385412.3385997", "report-no": null, "categories": "cs.PL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Type inference over partial contexts in dynamically typed languages is\nchallenging. In this work, we present a graph neural network model that\npredicts types by probabilistically reasoning over a program's structure,\nnames, and patterns. The network uses deep similarity learning to learn a\nTypeSpace -- a continuous relaxation of the discrete space of types -- and how\nto embed the type properties of a symbol (i.e. identifier) into it.\nImportantly, our model can employ one-shot learning to predict an open\nvocabulary of types, including rare and user-defined ones. We realise our\napproach in Typilus for Python that combines the TypeSpace with an optional\ntype checker. We show that Typilus accurately predicts types. Typilus\nconfidently predicts types for 70% of all annotatable symbols; when it predicts\na type, that type optionally type checks 95% of the time. Typilus can also find\nincorrect type annotations; two important and popular open source libraries,\nfairseq and allennlp, accepted our pull requests that fixed the annotation\nerrors Typilus discovered.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 11:14:03 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Allamanis", "Miltiadis", ""], ["Barr", "Earl T.", ""], ["Ducousso", "Soline", ""], ["Gao", "Zheng", ""]]}, {"id": "2004.10675", "submitter": "Yuan Xue", "authors": "Shuangbai Xue and Yuan Xue", "title": "Speeding-up Logic Design and Refining Hardware EDA Flow by Exploring\n  Chinese Character based Graphical Representation", "comments": "6 pages, may add some evaluations soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electrical design automation (EDA) techniques have deeply influenced the\ncomputer hardware design, especially in the field of very large scale\nIntegration (VLSI) circuits. Particularly, the popularity of FPGA, ASIC and SOC\napplications have been dramatically increased due to the well developed EDA\ntool chains. Over decades, improving EDA tool in terms of functionality,\nefficiency, accuracy and intelligence is not only the academic research hot\nspot, but the industry attempting goal as well.\n  In this paper, a novel perspective is taken to review current mainstream EDA\nworking flow and design methods, aiming to shorten the EDA design periods and\nsimplify the logic design overload significantly. Specifically, three major\ncontributions are devoted. First, a Chinese character based representation\nsystem (CCRS), which is used for presenting logical abstract syntax tree, is\nproposed. Second, the register-transfer-level (RTL) level symbolic description\ntechnique for CCRS are introduced to replace traditional text-based programming\nmethods. Finally, the refined EDA design flow based on CCRS is discussed. It is\nconvincing that the graphic non-pure-english based EDA flow could lower the\ndesign cost and complexity. As a fundamental trial in this new field, it is\nconfirmative that a lot of following works will make the related EDA\ndevelopment prosperous.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 18:24:43 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Xue", "Shuangbai", ""], ["Xue", "Yuan", ""]]}, {"id": "2004.11663", "submitter": "Kc Sivaramakrishnan", "authors": "KC Sivaramakrishnan, Stephen Dolan, Leo White, Sadiq Jaffer, Tom\n  Kelly, Anmol Sahoo, Sudha Parimala, Atul Dhiman and Anil Madhavapeddy", "title": "Retrofitting Parallelism onto OCaml", "comments": "Accepted to ICFP 2020", "journal-ref": null, "doi": "10.1145/3408995", "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  OCaml is an industrial-strength, multi-paradigm programming language, widely\nused in industry and academia. OCaml is also one of the few modern managed\nsystem programming languages to lack support for shared memory parallel\nprogramming. This paper describes the design, a full-fledged implementation and\nevaluation of a mostly-concurrent garbage collector (GC) for the multicore\nextension of the OCaml programming language. Given that we propose to add\nparallelism to a widely used programming language with millions of lines of\nexisting code, we face the challenge of maintaining backwards\ncompatibility--not just in terms of the language features but also the\nperformance of single-threaded code running with the new GC. To this end, the\npaper presents a series of novel techniques and demonstrates that the new GC\nstrikes a balance between performance and feature backwards compatibility for\nsequential programs and scales admirably on modern multicore processors.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 11:27:37 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 16:09:24 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 16:47:41 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Sivaramakrishnan", "KC", ""], ["Dolan", "Stephen", ""], ["White", "Leo", ""], ["Jaffer", "Sadiq", ""], ["Kelly", "Tom", ""], ["Sahoo", "Anmol", ""], ["Parimala", "Sudha", ""], ["Dhiman", "Atul", ""], ["Madhavapeddy", "Anil", ""]]}, {"id": "2004.11787", "submitter": "Andreas Humenberger", "authors": "Andreas Humenberger and Laura Kov\\'acs", "title": "Algebra-based Loop Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for synthesizing program loops satisfying a given\npolynomial loop invariant. The class of loops we consider can be modeled by a\nsystem of algebraic recurrence equations with constant coefficients. We turn\nthe task of loop synthesis into a polynomial constraint problem by precisely\ncharacterizing the set of all loops satisfying the given invariant. We prove\nsoundness of our approach, as well as its completeness with respect to an a\npriori fixed upper bound on the number of program variables. Our work has\napplications towards program verification, as well as generating number\nsequences from algebraic relations. We implemented our work in the Absynth tool\nand report on our initial experiments with loop synthesis.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 15:03:42 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 06:20:39 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Humenberger", "Andreas", ""], ["Kov\u00e1cs", "Laura", ""]]}, {"id": "2004.11960", "submitter": "Arnab Das", "authors": "Arnab Das, Ian Briggs, Ganesh Gopalakrishnan, Pavel Panchekha, Sriram\n  Krishnamoorthy", "title": "An Abstraction-guided Approach to Scalable and Rigorous Floating-Point\n  Error Analysis", "comments": "A more informative and updated version of this paper has been\n  accepted for publication at SuperComputing 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.NA cs.SC math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated techniques for rigorous floating-point round-off error analysis are\nimportant in areas including formal verification of correctness and precision\ntuning. Existing tools and techniques, while providing tight bounds, fail to\nanalyze expressions with more than a few hundred operators, thus unable to\ncover important practical problems. In this work, we present Satire, a new tool\nthat sheds light on how scalability and bound-tightness can be attained through\na combination of incremental analysis, abstraction, and judicious use of\nconcrete and symbolic evaluation. Satire has handled problems exceeding 200K\noperators. We present Satire's underlying error analysis approach,\ninformation-theoretic abstraction heuristics, and a wide range of case studies,\nwith evaluation covering FFT, Lorenz system of equations, and various PDE\nstencil types. Our results demonstrate the tightness of Satire's bounds, its\nacceptable runtime, and valuable insights provided.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 19:42:33 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 03:28:54 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 01:45:35 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Das", "Arnab", ""], ["Briggs", "Ian", ""], ["Gopalakrishnan", "Ganesh", ""], ["Panchekha", "Pavel", ""], ["Krishnamoorthy", "Sriram", ""]]}, {"id": "2004.12403", "submitter": "EPTCS", "authors": "Ansgar Fehnker (University of Twente), Hubert Garavel (INRIA Grenoble\n  Rh\\^one-Alpes)", "title": "Proceedings of the 4th Workshop on Models for Formal Analysis of Real\n  Systems", "comments": null, "journal-ref": "EPTCS 316, 2020", "doi": "10.4204/EPTCS.316", "report-no": null, "categories": "cs.LO cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of MARS 2020, the fourth workshop on\nModels for Formal Analysis of Real Systems held as part of ETAPS 2020, the\nEuropean Joint Conferences on Theory and Practice of Software.\n  The MARS workshop brings together researchers from different communities who\nare developing formal models of real systems in areas where complex models\noccur, such as networks, cyber-physical systems, hardware/software codesign,\nbiology, etc.\n  The MARS workshops stem from two observations:\n  (1) Large case studies are essential to show that specification formalisms\nand modelling techniques are applicable to real systems, whereas many research\npapers only consider toy examples or tiny case studies.\n  (2) Developing an accurate model of a real system takes a large amount of\ntime, often months or years. In most scientific papers, however, salient\ndetails of the model need to be skipped due to lack of space, and to leave room\nfor formal verification methodologies and results.\n  The MARS workshop remedies these issues, emphasising modelling over\nverification, so as to retain lessons learnt from formal modelling, which are\nnot usually discussed elsewhere.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 14:59:03 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Fehnker", "Ansgar", "", "University of Twente"], ["Garavel", "Hubert", "", "INRIA Grenoble\n  Rh\u00f4ne-Alpes"]]}, {"id": "2004.12859", "submitter": "Julia Gabet", "authors": "Julia Gabet, Nobuko Yoshida", "title": "Static Race Detection and Mutex Safety and Liveness for Go Programs\n  (extended version)", "comments": "To be published in: ECOOP 2020; 26 pages + references and appendix;\n  Main body: 17 figures + 1 table; Appendix: 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Go is a popular concurrent programming language thanks to its ability to\nefficiently combine concurrency and systems programming. In Go programs, a\nnumber of concurrency bugs can be caused by a mixture of data races and\ncommunication problems. In this paper, we develop a theory based on behavioural\ntypes to statically detect data races and deadlocks in Go programs. We first\nspecify lock safety and liveness and data race properties over a Go program\nmodel, using the happens-before relation defined in the Go memory model. We\nrepresent these properties of programs in a $\\mu$-calculus model of types, and\nvalidate them using type-level model-checking. We then extend the framework to\naccount for Go's channels, and implement a static verification tool which can\ndetect concurrency errors. This is, to the best of our knowledge, the first\nstatic verification framework of this kind for the Go language, uniformly\nanalysing concurrency errors caused by a mix of shared memory accesses and\nasynchronous message-passing communications.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 15:12:11 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Gabet", "Julia", ""], ["Yoshida", "Nobuko", ""]]}, {"id": "2004.12885", "submitter": "Jean-Joseph Marty", "authors": "Jean-Joseph Marty, Lucas Franceschino, Jean-Pierre Talpin, Niki Vazou", "title": "LIO*: Low Level Information Flow Control in F*", "comments": "Submitted to ICFP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Labeled Input Output in F* (LIO*), a verified framework that\nenforces information flow control (IFC) policies developed in F* and\nautomatically extracted to C. Inspired by LIO, we encapsulated IFC policies\ninto effects, but using F* we derived efficient, low level, and provably\ncorrect code. Concretely, runtime checks are lifted to static proof\nobligations, the developed code is automatically extracted to C and proved\nnon-interferent using metaprogramming. We benchmarked our framework on three\nclients and observed up to 54% speedup when IFC runtime checks are proved\nstatically. Our framework is designed to aid development of embedded devices\nwhere both enforcement of security policies and low level efficient code is\ncritical.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 15:48:52 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Marty", "Jean-Joseph", ""], ["Franceschino", "Lucas", ""], ["Talpin", "Jean-Pierre", ""], ["Vazou", "Niki", ""]]}, {"id": "2004.13301", "submitter": "Ryan Marcus", "authors": "Lujing Cen, Ryan Marcus, Hongzi Mao, Justin Gottschlich, Mohammad\n  Alizadeh, Tim Kraska", "title": "Learned Garbage Collection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several programming languages use garbage collectors (GCs) to automatically\nmanage memory for the programmer. Such collectors must decide when to look for\nunreachable objects to free, which can have a large performance impact on some\napplications. In this preliminary work, we propose a design for a learned\ngarbage collector that autonomously learns over time when to perform\ncollections. By using reinforcement learning, our design can incorporate\nuser-defined reward functions, allowing an autonomous garbage collector to\nlearn to optimize the exact metric the user desires (e.g., request latency or\nqueries per second). We conduct an initial experimental study on a prototype,\ndemonstrating that an approach based on tabular Q learning may be promising.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 05:06:24 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Cen", "Lujing", ""], ["Marcus", "Ryan", ""], ["Mao", "Hongzi", ""], ["Gottschlich", "Justin", ""], ["Alizadeh", "Mohammad", ""], ["Kraska", "Tim", ""]]}, {"id": "2004.13312", "submitter": "Ilya Sergey", "authors": "Kiran Gopinathan and Ilya Sergey", "title": "Certifying Certainty and Uncertainty in Approximate Membership Query\n  Structures -- Extended Version", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Approximate Membership Query structures (AMQs) rely on randomisation for\ntime- and space-efficiency, while introducing a possibility of false positive\nand false negative answers. Correctness proofs of such structures involve\nsubtle reasoning about bounds on probabilities of getting certain outcomes.\nBecause of these subtleties, a number of unsound arguments in such proofs have\nbeen made over the years. In this work, we address the challenge of building\nrigorous and reusable computer-assisted proofs about probabilistic\nspecifications of AMQs. We describe the framework for systematic decomposition\nof AMQs and their properties into a series of interfaces and reusable\ncomponents. We implement our framework as a library in the Coq proof assistant\nand showcase it by encoding in it a number of non-trivial AMQs, such as Bloom\nfilters, counting filters, quotient filters and blocked constructions, and\nmechanising the proofs of their probabilistic specifications. We demonstrate\nhow AMQs encoded in our framework guarantee the absence of false negatives by\nconstruction. We also show how the proofs about probabilities of false\npositives for complex AMQs can be obtained by means of verified reduction to\nthe implementations of their simpler counterparts. Finally, we provide a\nlibrary of domain-specific theorems and tactics that allow a high degree of\nautomation in probabilistic proofs.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 05:48:09 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Gopinathan", "Kiran", ""], ["Sergey", "Ilya", ""]]}, {"id": "2004.13472", "submitter": "Peng Fu", "authors": "Peng Fu, Kohei Kishida, Peter Selinger", "title": "Linear Dependent Type Theory for Quantum Programming Languages", "comments": "submitted to LMCS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO math.CT quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern quantum programming languages integrate quantum resources and\nclassical control. They must, on the one hand, be linearly typed to reflect the\nno-cloning property of quantum resources. On the other hand, high-level and\npractical languages should also support quantum circuits as first-class\ncitizens, as well as families of circuits that are indexed by some classical\nparameters. Quantum programming languages thus need linear dependent type\ntheory. This paper defines a general semantic structure for such a type theory\nvia certain fibrations of monoidal categories. The categorical model of the\nquantum circuit description language Proto-Quipper-M by Rios and Selinger\n(2017) constitutes an example of such a fibration, which means that the\nlanguage can readily be integrated with dependent types. We then devise both a\ngeneral linear dependent type system and a dependently typed extension of\nProto-Quipper-M, and provide them with operational semantics as well as a\nprototype implementation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 13:11:06 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 18:38:54 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Fu", "Peng", ""], ["Kishida", "Kohei", ""], ["Selinger", "Peter", ""]]}, {"id": "2004.13907", "submitter": "Santosh Nagarakatte", "authors": "Mohammadreza Soltaniyeh, Richard P. Martin, and Santosh Nagarakatte", "title": "Synergistic CPU-FPGA Acceleration of Sparse Linear Algebra", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": "Rutgers Computer Science Technical Report DCS-TR-750", "categories": "cs.DC cs.MS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes REAP, a software-hardware approach that enables high\nperformance sparse linear algebra computations on a cooperative CPU-FPGA\nplatform. REAP carefully separates the task of organizing the matrix elements\nfrom the computation phase. It uses the CPU to provide a first-pass\nre-organization of the matrix elements, allowing the FPGA to focus on the\ncomputation. We introduce a new intermediate representation that allows the CPU\nto communicate the sparse data and the scheduling decisions to the FPGA. The\ncomputation is optimized on the FPGA for effective resource utilization with\npipelining. REAP improves the performance of Sparse General Matrix\nMultiplication (SpGEMM) and Sparse Cholesky Factorization by 3.2X and 1.85X\ncompared to widely used sparse libraries for them on the CPU, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 01:06:52 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Soltaniyeh", "Mohammadreza", ""], ["Martin", "Richard P.", ""], ["Nagarakatte", "Santosh", ""]]}, {"id": "2004.14084", "submitter": "Yuki Nishida", "authors": "Yuki Nishida and Atsushi Igarashi", "title": "Compilation of Coordinated Choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, we have proposed coordinated choices, which are nondeterministic\nchoices equipped with names. The main characteristic of coordinated choices is\nthat they synchronize nondeterministic decision among choices of the same name.\n  The motivation of the synchronization mechanism is to solve a theoretical\nproblem. So, as a practical programming language, we still want to use\ncoordinated choices like standard ones. In other words, we want to avoid\nsynchronization. Now, there are two problems: (i) practically, it is a bit\ncomplicated work to write a program using coordinated choices in which\nexecution synchronization never happens; and (ii) theoretically, it is unknown\nwhether any programs using standard choices can be written by using only\ncoordinated ones.\n  In this paper, we define two simply typed lambda calculi called\n$\\lambda^\\parallel$ equipped with standard choices and\n$\\lambda^{\\parallel\\omega}$ equipped with coordinated choices, and give\ncompilation rules from the former into the latter. The challenge is to show the\ncorrectness of the compilation because behavioral correspondence between\nexpressions before and after compiling cannot be defined directly by the\ncompilation rules. For the challenge, we give an effect system for\n$\\lambda^{\\parallel\\omega}$ that characterizes expressions in which execution\nsynchronization never happens. Then, we show that all compiled expressions can\nbe typed by the effect system. As a result, we can easily show the correctness\nbecause the main concern of the correctness is whether synchronization happens\nor not.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 11:15:19 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Nishida", "Yuki", ""], ["Igarashi", "Atsushi", ""]]}, {"id": "2004.14212", "submitter": "EPTCS", "authors": "Lina Marsso", "title": "Specifying a Cryptographical Protocol in Lustre and SCADE", "comments": "In Proceedings MARS 2020, arXiv:2004.12403. arXiv admin note: text\n  overlap with arXiv:1703.06573", "journal-ref": "EPTCS 316, 2020, pp. 149-199", "doi": "10.4204/EPTCS.316.7", "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SCADE and Lustre models of the Message Authenticator Algorithm\n(MAA), which is one of the first cryptographic functions for computing a\nmessage authentication code. The MAA was adopted between 1987 and 2001, in\ninternational standards (ISO 8730 and ISO 8731-2), to ensure the authenticity\nand integrity of banking transactions. This paper discusses the choices and the\nchallenges of our MAA implementations. Our SCADE and Lustre models validate 201\nofficial test vectors for the MAA.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 04:23:44 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Marsso", "Lina", ""]]}, {"id": "2004.14437", "submitter": "Pablo Gordillo", "authors": "Elvira Albert, Jes\\'us Correas, Pablo Gordillo, Alejandro\n  Hern\\'andez-Cerezo Guillermo Rom\\'an-D\\'iez, Albert Rubio", "title": "Analyzing Smart Contracts: From EVM to a sound Control-Flow Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The EVM language is a simple stack-based language with words of 256 bits,\nwith one significant difference between the EVM and other virtual machine\nlanguages (like Java Bytecode or CLI for .Net programs): the use of the stack\nfor saving the jump addresses instead of having it explicit in the code of the\njumping instructions. Static analyzers need the complete control flow graph\n(CFG) of the EVM program in order to be able to represent all its execution\npaths. This report addresses the problem of obtaining a precise and complete\nstack-sensitive CFG by means of a static analysis, cloning the blocks that\nmight be executed using different states of the execution stack. The soundness\nof the analysis presented is proved.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 19:05:02 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 16:18:15 GMT"}, {"version": "v3", "created": "Mon, 5 Oct 2020 07:36:31 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Albert", "Elvira", ""], ["Correas", "Jes\u00fas", ""], ["Gordillo", "Pablo", ""], ["Rom\u00e1n-D\u00edez", "Alejandro Hern\u00e1ndez-Cerezo Guillermo", ""], ["Rubio", "Albert", ""]]}, {"id": "2004.14735", "submitter": "EPTCS", "authors": "Max S. New, Sam Lindley", "title": "Proceedings Eighth Workshop on Mathematically Structured Functional\n  Programming", "comments": null, "journal-ref": "EPTCS 317, 2020", "doi": "10.4204/EPTCS.317", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of the Eighth Workshop on Mathematically\nStructured Functional Programming (MSFP 2020). The meeting was originally\nscheduled to take place in Dublin, Ireland on the 25th of April as a satellite\nevent of the European Joint Conferences on Theory & Practice of Software (ETAPS\n2020).\n  Due to the COVID-19 pandemic, ETAPS 2020, and consequently MSFP 2020, has\nbeen postponed to a date yet to be determined.\n  The MSFP workshop highlights applications of mathematical structures to\nprogramming applications. We promote the use of category theory, type theory,\nand formal language semantics to the development of simple and reasonable\nprograms. This year's papers cover a variety of topics ranging from array\nprogramming to dependent types to effects.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 12:50:47 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["New", "Max S.", ""], ["Lindley", "Sam", ""]]}, {"id": "2004.14750", "submitter": "EPTCS", "authors": "Bob Coecke (University of Oxford), Matthew Leifer (Chapman University)", "title": "Proceedings 16th International Conference on Quantum Physics and Logic", "comments": null, "journal-ref": "EPTCS 318, 2020", "doi": "10.4204/EPTCS.318", "report-no": null, "categories": "cs.LO cs.FL cs.IT cs.PL math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of the 16th International Conference on\nQuantum Physics and Logic (QPL 2017), which was held June 10-14, 2019. Quantum\nPhysics and Logic is an annual conference that brings together researchers\nworking on mathematical foundations of quantum physics, quantum computing, and\nrelated areas, with a focus on structural perspectives and the use of logical\ntools, ordered algebraic and category-theoretic structures, formal languages,\nsemantical methods, and other computer science techniques applied to the study\nof physical behaviour in general. Work that applies structures and methods\ninspired by quantum theory to other fields (including computer science) is also\nwelcome.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 13:14:56 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Coecke", "Bob", "", "University of Oxford"], ["Leifer", "Matthew", "", "Chapman University"]]}, {"id": "2004.14756", "submitter": "Matthew Mirman", "authors": "Matthew Mirman, Timon Gehr, Martin Vechev", "title": "Robustness Certification of Generative Models", "comments": "Prior version submitted to ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative neural networks can be used to specify continuous transformations\nbetween images via latent-space interpolation. However, certifying that all\nimages captured by the resulting path in the image manifold satisfy a given\nproperty can be very challenging. This is because this set is highly\nnon-convex, thwarting existing scalable robustness analysis methods, which are\noften based on convex relaxations. We present ApproxLine, a scalable\ncertification method that successfully verifies non-trivial specifications\ninvolving generative models and classifiers. ApproxLine can provide both sound\ndeterministic and probabilistic guarantees, by capturing either infinite\nnon-convex sets of neural network activation vectors or distributions over such\nsets. We show that ApproxLine is practically useful and can verify interesting\ninterpolations in the networks latent space.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 13:23:02 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Mirman", "Matthew", ""], ["Gehr", "Timon", ""], ["Vechev", "Martin", ""]]}]