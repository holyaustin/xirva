[{"id": "1805.00120", "submitter": "Deepak Garg", "authors": "Vineet Rajani, Deepak Garg", "title": "Types for Information Flow Control: Labeling Granularity and Semantic\n  Models", "comments": "31st IEEE Symposium on Computer Security Foundations (CSF 2018)", "journal-ref": null, "doi": "10.1109/CSF.2018.00024", "report-no": null, "categories": "cs.CR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language-based information flow control (IFC) tracks dependencies within a\nprogram using sensitivity labels and prohibits public outputs from depending on\nsecret inputs. In particular, literature has proposed several type systems for\ntracking these dependencies. On one extreme, there are fine-grained type\nsystems (like Flow Caml) that label all values individually and track\ndependence at the level of individual values. On the other extreme are\ncoarse-grained type systems (like HLIO) that track dependence coarsely, by\nassociating a single label with an entire computation context and not labeling\nall values individually.\n  In this paper, we show that, despite their glaring differences, both these\nstyles are, in fact, equally expressive. To do this, we show a semantics- and\ntype-preserving translation from a coarse-grained type system to a fine-grained\none and vice-versa. The forward translation isn't surprising, but the backward\ntranslation is: It requires a construct to arbitrarily limit the scope of a\ncontext label in the coarse-grained type system (e.g., HLIO's \"toLabeled\"\nconstruct). As a separate contribution, we show how to extend work on logical\nrelation models of IFC types to higher-order state. We build such logical\nrelations for both the fine-grained type system and the coarse-grained type\nsystem. We use these relations to prove the two type systems and our\ntranslations between them sound.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 22:30:23 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Rajani", "Vineet", ""], ["Garg", "Deepak", ""]]}, {"id": "1805.00155", "submitter": "Cyrus Omar", "authors": "Cyrus Omar, Ian Voysey, Ravi Chugh, Matthew A. Hammer", "title": "Live Functional Programming with Typed Holes", "comments": "Published in PACMPL issue POPL 2019. Please cite the conference\n  paper!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a dynamic semantics for incomplete functional programs,\nstarting from the static semantics developed in recent work on Hazelnut. We\nmodel incomplete functional programs as expressions with holes, with empty\nholes standing for missing expressions or types, and non-empty holes operating\nas membranes around static and dynamic type inconsistencies. Rather than\naborting when evaluation encounters any of these holes as in some existing\nsystems, evaluation proceeds around holes, tracking the closure around each\nhole instance as it flows through the remainder of the program. Editor services\ncan use the information in these hole closures to help the programmer develop\nand confirm their mental model of the behavior of the complete portions of the\nprogram as they decide how to fill the remaining holes. Hole closures also\nenable a fill-and-resume operation that avoids the need to restart evaluation\nafter edits that amount to hole filling. Formally, the semantics borrows\nmachinery from both gradual type theory (which supplies the basis for handling\nunfilled type holes) and contextual modal type theory (which supplies a logical\nbasis for hole closures), combining these and developing additional machinery\nnecessary to continue evaluation past holes while maintaining type safety. We\nhave mechanized the metatheory of the core calculus, called Hazelnut Live,\nusing the Agda proof assistant.\n  We have also implemented these ideas into the Hazel programming environment.\nThe implementation inserts holes automatically, following the Hazelnut edit\naction calculus, to guarantee that every editor state has some (possibly\nincomplete) type. Taken together with this paper's type safety property, the\nresult is a proof-of-concept live programming environment where rich dynamic\nfeedback is truly available without gaps, i.e. for every reachable editor\nstate.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 02:26:49 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 18:32:55 GMT"}, {"version": "v3", "created": "Tue, 13 Nov 2018 21:57:47 GMT"}, {"version": "v4", "created": "Tue, 20 Nov 2018 19:48:21 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Omar", "Cyrus", ""], ["Voysey", "Ian", ""], ["Chugh", "Ravi", ""], ["Hammer", "Matthew A.", ""]]}, {"id": "1805.00185", "submitter": "Thanh Nguyen", "authors": "Thanh Hai Nguyen, Enrico Pontelli, Tran Cao Son", "title": "Phylotastic: An Experiment in Creating, Manipulating, and Evolving\n  Phylogenetic Biology Workflows Using Logic Programming", "comments": "Paper presented at the 34th International Conference on Logic\n  Programming (ICLP 2018), Oxford, UK, July 14 to July 17, 2018 17 pages,\n  LaTeX, 10 PDF figures (arXiv:YYMM.NNNNN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary Biologists have long struggled with the challenge of developing\nanalysis workflows in a flexible manner, thus facilitating the reuse of\nphylogenetic knowledge. An evolutionary biology workflow can be viewed as a\nplan which composes web services that can retrieve, manipulate, and produce\nphylogenetic trees. The Phylotastic project was launched two years ago as a\ncollaboration between evolutionary biologists and computer scientists, with the\ngoal of developing an open architecture to facilitate the creation of such\nanalysis workflows. While composition of web services is a problem that has\nbeen extensively explored in the literature, including within the logic\nprogramming domain, the incarnation of the problem in Phylotastic provides a\nnumber of additional challenges. Along with the need to integrate preferences\nand formal ontologies in the description of the desired workflow, evolutionary\nbiologists tend to construct workflows in an incremental manner, by\nsuccessively refining the workflow, by indicating desired changes (e.g.,\nexclusion of certain services, modifications of the desired output). This leads\nto the need of successive iterations of incremental replanning, to develop a\nnew workflow that integrates the requested changes while minimizing the changes\nto the original workflow. This paper illustrates how Phylotastic has addressed\nthe challenges of creating and refining phylogenetic analysis workflows using\nlogic programming technology and how such solutions have been used within the\ngeneral framework of the Phylotastic project. Under consideration in Theory and\nPractice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 04:54:45 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Nguyen", "Thanh Hai", ""], ["Pontelli", "Enrico", ""], ["Son", "Tran Cao", ""]]}, {"id": "1805.00289", "submitter": "Marco Paviotti", "authors": "Rasmus E. M{\\o}gelberg and Marco Paviotti", "title": "Denotational semantics of recursive types in synthetic guarded domain\n  theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Just like any other branch of mathematics, denotational semantics of\nprogramming languages should be formalised in type theory, but adapting\ntraditional domain theoretic semantics, as originally formulated in classical\nset theory to type theory has proven challenging. This paper is part of a\nproject on formulating denotational semantics in type theories with guarded\nrecursion. This should have the benefit of not only giving simpler semantics\nand proofs of properties such as adequacy, but also hopefully in the future to\nscale to languages with advanced features, such as general references, outside\nthe reach of traditional domain theoretic techniques. Working in Guarded\nDependent Type Theory (GDTT), we develop denotational semantics for FPC, the\nsimply typed lambda calculus extended with recursive types, modelling the\nrecursive types of FPC using the guarded recursive types of GDTT. We prove\nsoundness and computational adequacy of the model in GDTT using a logical\nrelation between syntax and semantics constructed also using guarded recursive\ntypes. The denotational semantics is intensional in the sense that it counts\nthe number of unfold-fold reductions needed to compute the value of a term, but\nwe construct a relation relating the denotations of extensionally equal terms,\ni.e., pairs of terms that compute the same value in a different number of\nsteps. Finally we show how the denotational semantics of terms can be executed\ninside type theory and prove that executing the denotation of a boolean term\ncomputes the same value as the operational semantics of FPC.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 12:26:33 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 12:02:20 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["M\u00f8gelberg", "Rasmus E.", ""], ["Paviotti", "Marco", ""]]}, {"id": "1805.00401", "submitter": "Brigitte Pientka", "authors": "Rohan Jacob-Rao, Brigitte Pientka, David Thibodeau", "title": "Index-Stratified Types (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Tores, a core language for encoding metatheoretic proofs. The\nnovel features we introduce are well-founded Mendler-style (co)recursion over\nindexed data types and a form of recursion over objects in the index language\nto build new types. The latter, which we call index-stratified types, are\nanalogue to the concept of large elimination in dependently typed languages.\nThese features combined allow us to encode sophisticated case studies such as\nnormalization for lambda calculi and normalization by evaluation. We prove the\nsoundness of Tores as a programming and proof language via the key theorems of\nsubject reduction and termination.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 15:33:39 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Jacob-Rao", "Rohan", ""], ["Pientka", "Brigitte", ""], ["Thibodeau", "David", ""]]}, {"id": "1805.00468", "submitter": "Benjamin Sherman", "authors": "Benjamin Sherman, Luke Sciarappa, Adam Chlipala, Michael Carbin", "title": "Computable decision making on the reals and other spaces via partiality\n  and nondeterminism", "comments": "This is an extended version of a paper due to appear in the\n  proceedings of the ACM/IEEE Symposium on Logic in Computer Science (LICS) in\n  July 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though many safety-critical software systems use floating point to represent\nreal-world input and output, programmers usually have idealized versions in\nmind that compute with real numbers. Significant deviations from the ideal can\ncause errors and jeopardize safety. Some programming systems implement exact\nreal arithmetic, which resolves this matter but complicates others, such as\ndecision making. In these systems, it is impossible to compute (total and\ndeterministic) discrete decisions based on connected spaces such as\n$\\mathbb{R}$. We present programming-language semantics based on constructive\ntopology with variants allowing nondeterminism and/or partiality. Either\nnondeterminism or partiality suffices to allow computable decision making on\nconnected spaces such as $\\mathbb{R}$. We then introduce pattern matching on\nspaces, a language construct for creating programs on spaces, generalizing\npattern matching in functional programming, where patterns need not represent\ndecidable predicates and also may overlap or be inexhaustive, giving rise to\nnondeterminism or partiality, respectively. Nondeterminism and/or partiality\nalso yield formal logics for constructing approximate decision procedures. We\nimplemented these constructs in the Marshall language for exact real\narithmetic.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 17:58:14 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Sherman", "Benjamin", ""], ["Sciarappa", "Luke", ""], ["Chlipala", "Adam", ""], ["Carbin", "Michael", ""]]}, {"id": "1805.00808", "submitter": "Zheng Yang", "authors": "Zheng Yang, Hang Lei", "title": "Formal Process Virtual Machine for Smart Contracts Verification", "comments": "9 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:1803.00403", "journal-ref": "International Journal of Performability Engineering, 2018", "doi": "10.23940/ijpe.18.08.p9.17261734", "report-no": "Volume 14, Number 8, pp. 1726-1734", "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on the development and verification of a novel formal\nsymbolic process virtual machine (FSPVM) for verifying the reliability and\nsecurity of Ethereum smart contracts, denoted as FSPVM-E, completely in Coq\nproof assistant. It adopts execution-verification isomorphism (EVI), an\nextension of Curry-Howard isomorphism (CHI), as its fundamental theoretical\nframework. The current version of FSPVM-E is constructed on a general,\nextensible, and reusable formal memory (GERM) framework, an extensible and\nuniversal formal intermediate programming language Lolisa, which is a large\nsubset of the Solidity programming language using generalized algebraic\ndatatypes, and the corresponding formally verified interpreter of Lolisa,\ndenoted as FEther. It supports the ERC20 standard and can automatically\nsimultaneously symbolically execute the smart contract programs of Ethereum and\nverify their reliability and security properties using Hoare logic in Coq. In\naddition, this work, contributes to solving the problems of automation,\ninconsistency and reusability in higher-order logic theorem proving.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 16:03:23 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Yang", "Zheng", ""], ["Lei", "Hang", ""]]}, {"id": "1805.00907", "submitter": "Jordan Fix", "authors": "Nadav Rotem, Jordan Fix, Saleem Abdulrasool, Garret Catron, Summer\n  Deng, Roman Dzhabarov, Nick Gibson, James Hegeman, Meghan Lele, Roman\n  Levenstein, Jack Montgomery, Bert Maher, Satish Nadathur, Jakob Olesen,\n  Jongsoo Park, Artem Rakhov, Misha Smelyanskiy, Man Wang", "title": "Glow: Graph Lowering Compiler Techniques for Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the design of Glow, a machine learning compiler for\nheterogeneous hardware. It is a pragmatic approach to compilation that enables\nthe generation of highly optimized code for multiple targets. Glow lowers the\ntraditional neural network dataflow graph into a two-phase strongly-typed\nintermediate representation. The high-level intermediate representation allows\nthe optimizer to perform domain-specific optimizations. The lower-level\ninstruction-based address-only intermediate representation allows the compiler\nto perform memory-related optimizations, such as instruction scheduling, static\nmemory allocation and copy elimination. At the lowest level, the optimizer\nperforms machine-specific code generation to take advantage of specialized\nhardware features. Glow features a lowering phase which enables the compiler to\nsupport a high number of input operators as well as a large number of hardware\ntargets by eliminating the need to implement all operators on all targets. The\nlowering phase is designed to reduce the input space and allow new hardware\nbackends to focus on a small number of linear algebra primitives.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 17:04:53 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 23:00:08 GMT"}, {"version": "v3", "created": "Wed, 3 Apr 2019 19:54:50 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Rotem", "Nadav", ""], ["Fix", "Jordan", ""], ["Abdulrasool", "Saleem", ""], ["Catron", "Garret", ""], ["Deng", "Summer", ""], ["Dzhabarov", "Roman", ""], ["Gibson", "Nick", ""], ["Hegeman", "James", ""], ["Lele", "Meghan", ""], ["Levenstein", "Roman", ""], ["Montgomery", "Jack", ""], ["Maher", "Bert", ""], ["Nadathur", "Satish", ""], ["Olesen", "Jakob", ""], ["Park", "Jongsoo", ""], ["Rakhov", "Artem", ""], ["Smelyanskiy", "Misha", ""], ["Wang", "Man", ""]]}, {"id": "1805.00923", "submitter": "Yunming Zhang", "authors": "Yunming Zhang, Mengjiao Yang, Riyadh Baghdadi, Shoaib Kamil, Julian\n  Shun, Saman Amarasinghe", "title": "GraphIt: A High-Performance DSL for Graph Analytics", "comments": "Paper Accepted at OOPSLA 2018", "journal-ref": "OOPSLA 2018", "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance bottlenecks of graph applications depend not only on the\nalgorithm and the underlying hardware, but also on the size and structure of\nthe input graph. Programmers must try different combinations of a large set of\ntechniques to develop the best implementation for a specific algorithm and type\nof graph. Existing graph frameworks lack flexibility, supporting only a limited\nset of optimizations.\n  This paper introduces GraphIt, a new DSL for graph computations that\ngenerates fast implementations for algorithms with different performance\ncharacteristics running on graphs with different sizes and structures. GraphIt\nseparates what is computed (algorithm) from how it is computed (schedule).\nProgrammers specify the algorithm using an algorithm language, and performance\noptimizations are specified using a scheduling language. The algorithm language\nsimplifies expressing the algorithms. We formulate graph optimizations,\nincluding edge traversal direction, data layout, parallelization, cache, NUMA,\nand kernel fusion optimizations, as tradeoffs among locality, parallelism, and\nwork-efficiency. The scheduling language enables programmers to easily search\nthrough this complicated tradeoff space by composing together optimizations. We\nalso built an autotuner to automatically find high-performance schedules. The\ncompiler uses a new scheduling representation, the graph iteration space, to\nmodel, compose, and ensure the validity of the large number of optimizations.\nGraphIt outperforms the next fastest of six state-of-the-art shared-memory\nframeworks (Ligra, Green-Marl, GraphMat, Galois, Gemini, and Grazelle) on 24\nout of 32 experiments by up to 4.8$\\times$, and is never more than 43% slower\nthan the fastest framework on the other experiments. GraphIt also reduces the\nlines of code by up to an order of magnitude compared to the next fastest\nframework.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 17:38:35 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 19:48:48 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Zhang", "Yunming", ""], ["Yang", "Mengjiao", ""], ["Baghdadi", "Riyadh", ""], ["Kamil", "Shoaib", ""], ["Shun", "Julian", ""], ["Amarasinghe", "Saman", ""]]}, {"id": "1805.01863", "submitter": "Eric Atkinson", "authors": "Eric Atkinson, Cambridge Yang, Michael Carbin", "title": "Verifying Handcoded Probabilistic Inference Procedures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers have recently proposed several systems that ease the process of\nperforming Bayesian probabilistic inference. These include systems for\nautomatic inference algorithm synthesis as well as stronger abstractions for\nmanual algorithm development. However, existing systems whose performance\nrelies on the developer manually constructing a part of the inference algorithm\nhave limited support for reasoning about the correctness of the resulting\nalgorithm.\n  In this paper, we present Shuffle, a programming language for manually\ndeveloping inference procedures that 1) enforces the basic rules of probability\ntheory, 2) enforces the statistical dependencies of the algorithm's\ncorresponding probabilistic model, and 3) generates an optimized\nimplementation. We have used Shuffle to develop inference algorithms for\nseveral standard probabilistic models. Our results demonstrate that Shuffle\nenables a developer to deliver correct and performant implementations of these\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 17:13:15 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Atkinson", "Eric", ""], ["Yang", "Cambridge", ""], ["Carbin", "Michael", ""]]}, {"id": "1805.01965", "submitter": "Zhongxing Yu", "authors": "Zhongxing Yu, Chenggang Bai, Lionel Seinturier, Martin Monperrus", "title": "Characterizing the Usage, Evolution and Impact of Java Annotations in\n  Practice", "comments": "TO APPEAR IN IEEE TRANSACTIONS ON SOFTWARE ENGINEERING", "journal-ref": "IEEE Transactions on Software Engineering, 2019", "doi": "10.1109/TSE.2019.2910516", "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotations have been formally introduced into Java since Java 5. Since then,\nannotations have been widely used by the Java community for different purposes,\nsuch as compiler guidance and runtime processing. Despite the ever-growing use,\nthere is still limited empirical knowledge about the actual usage of\nannotations in practice, the changes made to annotations during software\nevolution, and the potential impact of annotations on code quality. To fill\nthis gap, we perform the first large-scale empirical study about Java\nannotations on 1,094 notable open-source projects hosted on GitHub. Our study\nsystematically investigates annotation usage, annotation evolution, and\nannotation impact, and generates 10 novel and important findings. We also\npresent the implications of our findings, which shed light for developers,\nresearchers, tool builders, and language or library designers in order to\nimprove all facets of Java annotation engineering.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 23:29:19 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 15:02:59 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Yu", "Zhongxing", ""], ["Bai", "Chenggang", ""], ["Seinturier", "Lionel", ""], ["Monperrus", "Martin", ""]]}, {"id": "1805.02436", "submitter": "Heiko Becker", "authors": "Heiko Becker, Pavel Pancheckha, Eva Darulova, Zachary Tatlock", "title": "Combining Tools for Optimization and Analysis of Floating-Point\n  Computations", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent renewed interest in optimizing and analyzing floating-point programs\nhas lead to a diverse array of new tools for numerical programs. These tools\nare often complementary, each focusing on a distinct aspect of numerical\nprogramming. Building reliable floating point applications typically requires\naddressing several of these aspects, which makes easy composition essential.\nThis paper describes the composition of two recent floating-point tools:\nHerbie, which performs accuracy optimization, and Daisy, which performs\naccuracy verification. We find that the combination provides numerous benefits\nto users, such as being able to use Daisy to check whether Herbie's unsound\noptimizations improved the worst-case roundoff error, as well as benefits to\ntool authors, including uncovering a number of bugs in both tools. The\ncombination also allowed us to compare the different program rewriting\ntechniques implemented by these tools for the first time. The paper lays out a\nroad map for combining other floating-point tools and for surmounting common\nchallenges.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 10:52:47 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Becker", "Heiko", ""], ["Pancheckha", "Pavel", ""], ["Darulova", "Eva", ""], ["Tatlock", "Zachary", ""]]}, {"id": "1805.03374", "submitter": "Michael Kruse", "authors": "Michael Kruse (1), Hal Finkel (1) ((1) Argonne National Laboratory)", "title": "A Proposal for Loop-Transformation Pragmas", "comments": "IWOMP'18 preprint", "journal-ref": null, "doi": "10.1007/978-3-319-98521-3_3", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pragmas for loop transformations, such as unrolling, are implemented in most\nmainstream compilers. They are used by application programmers because of their\nease of use compared to directly modifying the source code of the relevant\nloops. We propose additional pragmas for common loop transformations that go\nfar beyond the transformations today's compilers provide and should make most\nsource rewriting for the sake of loop optimization unnecessary. To encourage\ncompilers to implement these pragmas, and to avoid a diversity of incompatible\nsyntaxes, we would like to spark a discussion about an inclusion to the OpenMP\nstandard.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 05:15:37 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 15:02:03 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Kruse", "Michael", "", "Argonne National Laboratory"], ["Finkel", "Hal", "", "Argonne National Laboratory"]]}, {"id": "1805.03441", "submitter": "Zheng Wang", "authors": "Zheng Wang and Michael O'Boyle", "title": "Machine Learning in Compiler Optimisation", "comments": "Accepted to be published at Proceedings of the IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.DC cs.LG cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last decade, machine learning based compilation has moved from an an\nobscure research niche to a mainstream activity. In this article, we describe\nthe relationship between machine learning and compiler optimisation and\nintroduce the main concepts of features, models, training and deployment. We\nthen provide a comprehensive survey and provide a road map for the wide variety\nof different research areas. We conclude with a discussion on open issues in\nthe area and potential research directions. This paper provides both an\naccessible introduction to the fast moving area of machine learning based\ncompilation and a detailed bibliography of its main achievements.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 10:04:28 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Wang", "Zheng", ""], ["O'Boyle", "Michael", ""]]}, {"id": "1805.03740", "submitter": "Benedikt Ahrens", "authors": "Benedikt Ahrens, Andr\\'e Hirschowitz, Ambroise Lafont, Marco Maggesi", "title": "Presentable signatures and initial semantics", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 17, Issue 2 (May 26,\n  2021) lmcs:7511", "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a device for specifying and reasoning about syntax for datatypes,\nprogramming languages, and logic calculi. More precisely, we study a notion of\n\"signature\" for specifying syntactic constructions.\n  In the spirit of Initial Semantics, we define the \"syntax generated by a\nsignature\" to be the initial object -- if it exists -- in a suitable category\nof models. In our framework, the existence of an associated syntax to a\nsignature is not automatically guaranteed. We identify, via the notion of\npresentation of a signature, a large class of signatures that do generate a\nsyntax.\n  Our (presentable) signatures subsume classical algebraic signatures (i.e.,\nsignatures for languages with variable binding, such as the pure lambda\ncalculus) and extend them to include several other significant examples of\nsyntactic constructions.\n  One key feature of our notions of signature, syntax, and presentation is that\nthey are highly compositional, in the sense that complex examples can be\nobtained by gluing simpler ones. Moreover, through the Initial Semantics\napproach, our framework provides, beyond the desired algebra of terms, a\nwell-behaved substitution and the induction and recursion principles associated\nto the syntax.\n  This paper builds upon ideas from a previous attempt by Hirschowitz-Maggesi,\nwhich, in turn, was directly inspired by some earlier work of\nGhani-Uustalu-Hamana and Matthes-Uustalu.\n  The main results presented in the paper are computer-checked within the\nUniMath system.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 21:32:06 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 14:48:29 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2020 16:07:36 GMT"}, {"version": "v4", "created": "Sun, 2 May 2021 13:48:54 GMT"}, {"version": "v5", "created": "Tue, 25 May 2021 11:17:37 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Ahrens", "Benedikt", ""], ["Hirschowitz", "Andr\u00e9", ""], ["Lafont", "Ambroise", ""], ["Maggesi", "Marco", ""]]}, {"id": "1805.03949", "submitter": "Guillaume Houzeaux", "authors": "Marta Garcia-Gasulla, Guillaume Houzeaux, Roger Ferrer, Antoni\n  Artigues, Victor L\\'opez, Jes\\'us Labarta and Mariano V\\'azquez", "title": "MPI+X: task-based parallelization and dynamic load balance of finite\n  element assembly", "comments": null, "journal-ref": null, "doi": "10.1080/10618562.2019.1617856", "report-no": null, "categories": "cs.MS cs.DC cs.PF cs.PL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The main computing tasks of a finite element code(FE) for solving partial\ndifferential equations (PDE's) are the algebraic system assembly and the\niterative solver. This work focuses on the first task, in the context of a\nhybrid MPI+X paradigm. Although we will describe algorithms in the FE context,\na similar strategy can be straightforwardly applied to other discretization\nmethods, like the finite volume method. The matrix assembly consists of a loop\nover the elements of the MPI partition to compute element matrices and\nright-hand sides and their assemblies in the local system to each MPI\npartition. In a MPI+X hybrid parallelism context, X has consisted traditionally\nof loop parallelism using OpenMP. Several strategies have been proposed in the\nliterature to implement this loop parallelism, like coloring or substructuring\ntechniques to circumvent the race condition that appears when assembling the\nelement system into the local system. The main drawback of the first technique\nis the decrease of the IPC due to bad spatial locality. The second technique\navoids this issue but requires extensive changes in the implementation, which\ncan be cumbersome when several element loops should be treated. We propose an\nalternative, based on the task parallelism of the element loop using some\nextensions to the OpenMP programming model. The taskification of the assembly\nsolves both aforementioned problems. In addition, dynamic load balance will be\napplied using the DLB library, especially efficient in the presence of hybrid\nmeshes, where the relative costs of the different elements is impossible to\nestimate a priori. This paper presents the proposed methodology, its\nimplementation and its validation through the solution of large computational\nmechanics problems up to 16k cores.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 16:01:01 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Garcia-Gasulla", "Marta", ""], ["Houzeaux", "Guillaume", ""], ["Ferrer", "Roger", ""], ["Artigues", "Antoni", ""], ["L\u00f3pez", "Victor", ""], ["Labarta", "Jes\u00fas", ""], ["V\u00e1zquez", "Mariano", ""]]}, {"id": "1805.04058", "submitter": "Julian Dolby", "authors": "Julian Dolby, Avraham Shinnar, Allison Allain, Jenna Reinen", "title": "Ariadne: Analysis for Machine Learning Program", "comments": null, "journal-ref": null, "doi": "10.1145/3211346.3211349", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has transformed domains like vision and translation, and is\nnow increasingly used in science, where the correctness of such code is vital.\nPython is popular for machine learning, in part because of its wealth of\nmachine learning libraries, and is felt to make development faster; however,\nthis dynamic language has less support for error detection at code creation\ntime than tools like Eclipse. This is especially problematic for machine\nlearning: given its statistical nature, code with subtle errors may run and\nproduce results that look plausible but are meaningless. This can vitiate\nscientific results. We report on Ariadne: applying a static framework, WALA, to\nmachine learning code that uses TensorFlow. We have created static analysis for\nPython, a type system for tracking tensors---Tensorflow's core data\nstructures---and a data flow analysis to track their usage. We report on how it\nwas built and present some early results.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 16:51:40 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Dolby", "Julian", ""], ["Shinnar", "Avraham", ""], ["Allain", "Allison", ""], ["Reinen", "Jenna", ""]]}, {"id": "1805.04255", "submitter": "EPTCS", "authors": "Simon Thompson", "title": "Proceedings Sixth Workshop on Trends in Functional Programming in\n  Education", "comments": null, "journal-ref": "EPTCS 270, 2018", "doi": "10.4204/EPTCS.270", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Sixth International Workshops on Trends in Functional Programming in\nEducation, TFPIE 2017, was held on 22 June 2017 at the University of Kent, in\nCanterbury, UK, and was co-located with TFP, the Symposium on Trends in\nFunctional Programming.\n  The goal of TFPIE is to gather researchers, professors, teachers, and all\nprofessionals interested in functional programming in education. This includes\nthe teaching of functional programming, but also the application of functional\nprogramming as a tool for teaching other topics and disciplines.\n  A particular topic of this year's TFPIE was that of MOOCs and other online\nlearning and, as well as a session on this, we were delighted to welcome\nHeather Miller of EFPL and Northeastern University to give a keynote on this\ntopic entitled \"Functional Programming for All! Scaling a MOOC for Students and\nProfessionals Alike\". Heather works on and around the Scala programming\nlanguage and is Executive Director of the Scala Center.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 07:08:11 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Thompson", "Simon", ""]]}, {"id": "1805.04650", "submitter": "Christiano Braga", "authors": "Christiano Braga", "title": "{\\pi}: Towards a Simple Formal Semantic Framework for Compiler\n  Construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes {\\pi}, a formal semantic framework for compiler\nconstruction together with program validation. {\\pi} is comprised by {\\pi} Lib,\na set of programming languages constructs inspired by Peter Mosses'\nComponent-Based Semantics and {\\pi} Automata, an automata-based formalism to\ndescribe the operational semantics of programming languages, that generalizes\nGordon Plotkin's Interpreting Automata.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 04:36:07 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 18:15:06 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Braga", "Christiano", ""]]}, {"id": "1805.04775", "submitter": "Paolo Bientinesi", "authors": "Daniele G. Spampinato (1), Diego Fabregat-Traver (2), Paolo Bientinesi\n  (2), Markus Pueschel (1), ((1) ETH Zurich, (2) RWTH Aachen University)", "title": "Program Generation for Small-Scale Linear Algebra Applications", "comments": "CGO 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SLinGen, a program generation system for linear algebra. The input\nto SLinGen is an application expressed mathematically in a\nlinear-algebra-inspired language (LA) that we define. LA provides basic\nscalar/vector/matrix additions/multiplications and higher level operations\nincluding linear systems solvers, Cholesky and LU factorizations. The output of\nSLinGen is performance-optimized single-source C code, optionally vectorized\nwith intrinsics. The target of SLinGen are small-scale computations on\nfixed-size operands, for which a straightforward implementation using optimized\nlibraries (e.g., BLAS or LAPACK) is known to yield suboptimal performance\n(besides increasing code size and introducing dependencies), but which are\ncrucial in control, signal processing, computer vision, and other domains.\nInternally, SLinGen uses synthesis and DSL-based techniques to optimize at a\nhigh level of abstraction. We benchmark our program generator on three\nprototypical applications: the Kalman filter, Gaussian process regression, and\nan L1-analysis convex solver, as well as basic routines including Cholesky\nfactorization and solvers for the continuous-time Lyapunov and Sylvester\nequations. The results show significant speed-ups compared to straightforward C\nwith Intel icc and clang with a polyhedral optimizer, as well as library-based\nand template-based implementations.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 20:21:40 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Spampinato", "Daniele G.", "", "ETH Zurich"], ["Fabregat-Traver", "Diego", "", "RWTH Aachen University"], ["Bientinesi", "Paolo", "", "RWTH Aachen University"], ["Pueschel", "Markus", "", "ETH Zurich"]]}, {"id": "1805.05124", "submitter": "EPTCS", "authors": "Marco T. Moraz\\'an (Seton Hall University)", "title": "Vector Programming Using Structural Recursion", "comments": "In Proceedings TFPIE 2017, arXiv:1805.04255. Students traditionally\n  believe that the learning process flows from the professor to the students.\n  In my case, nothing can be further from the truth. The work presented in this\n  article is inspired by the difficulties faced and by the questions addressed\n  to me by my beginning students. The author thanks them for providing me with\n  valuable lessons regarding how to teach an introduction to vector\n  programming. In particular, I thank Josephine Des Rosiers for her many heated\n  debates with me about designing programs", "journal-ref": "EPTCS 270, 2018, pp. 1-17", "doi": "10.4204/EPTCS.270.1", "report-no": null, "categories": "cs.PL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vector programming is an important topic in many Introduction to Computer\nScience courses. Despite the importance of vectors, learning vector programming\nis a source for frustration to many students given that they feel left adrift\nwhen it comes to resolving vector indexing errors. Even though the size of a\nvector is a natural number, there have been no efforts to define a useful\nrecursive data definition to help beginners design vector processing functions.\nThis article defines the concept of a vector interval and describes how to\nexploit its recursive structure to design vector processing functions. The\ndescribed methodology provides a context beginners can use to reason about\nproper vector indexing instead of leaving them adrift with this responsibility.\nA key feature of properly using the described methodology is that if students\nprocess the correct vector interval then vector indexing errors can not arise.\nThe classroom deployment of this approach is described in detail. Students, to\ndate, have found vector intervals helpful in avoiding out-of-bounds indexing\nerrors when all the vector elements of the interval are processed.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 11:43:52 GMT"}], "update_date": "2018-05-19", "authors_parsed": [["Moraz\u00e1n", "Marco T.", "", "Seton Hall University"]]}, {"id": "1805.05126", "submitter": "EPTCS", "authors": "Jeremy Singer (University of Glasgow), Blair Archibald (University of\n  Glasgow)", "title": "Functional Baby Talk: Analysis of Code Fragments from Novice Haskell\n  Programmers", "comments": "In Proceedings TFPIE 2017, arXiv:1805.04255", "journal-ref": "EPTCS 270, 2018, pp. 37-51", "doi": "10.4204/EPTCS.270.3", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What kinds of mistakes are made by novice Haskell developers, as they learn\nabout functional programming? Is it possible to analyze these errors in order\nto improve the pedagogy of Haskell? In 2016, we delivered a massive open online\ncourse which featured an interactive code evaluation environment. We captured\nand analyzed 161K interactions from learners. We report typical novice\ndeveloper behavior; for instance, the mean time spent on an interactive\ntutorial is around eight minutes. Although our environment was restricted, we\ngain some understanding of Haskell novice errors. Parenthesis mismatches,\nlexical scoping errors and do block misunderstandings are common. Finally, we\nmake recommendations about how such beginner code evaluation environments might\nbe enhanced.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 11:44:40 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Singer", "Jeremy", "", "University of Glasgow"], ["Archibald", "Blair", "", "University of\n  Glasgow"]]}, {"id": "1805.05127", "submitter": "EPTCS", "authors": "Stephen Adams (University of Kent)", "title": "Teaching Erlang through the Internet: An Experience Report", "comments": "In Proceedings TFPIE 2017, arXiv:1805.04255", "journal-ref": "EPTCS 270, 2018, pp. 52-60", "doi": "10.4204/EPTCS.270.4", "report-no": null, "categories": "cs.CY cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today functional programming languages are seen as a practical solution to\nthe difficult problems of concurrent and distributed programming. Erlang is a\nfunctional language designed to build massively scalable and fault tolerant\napplications. This paper describes the authors' experiences delivering a\nmassively online open course (MOOC) on the FutureLearn platform.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 11:44:58 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Adams", "Stephen", "", "University of Kent"]]}, {"id": "1805.05400", "submitter": "Christine Rizkallah", "authors": "Dmitri Garbuzov, William Mansky, Christine Rizkallah, Steve Zdancewic", "title": "Structural Operational Semantics for Control Flow Graph Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compilers use control flow graph (CFG) representations of low-level programs\nbecause they are suited to program analysis and optimizations. However,\nformalizing the behavior and metatheory of CFG programs is non-trivial: CFG\nprograms don't compose well, their semantics depends on auxiliary state, and,\nas a consequence, they do not enjoy a simple equational theory that can be used\nfor reasoning about the correctness of program transformations.\nLambda-calculus-based intermediate representations, in contrast, have\nwell-understood operational semantics and metatheory, including rich equational\ntheories, all of which makes them amenable to formal verification.\n  This paper establishes a tight equivalence between (a variant of) Levy's\ncall-by-push-value (CBPV) calculus and a control flow graph machine whose\ninstructions are in static single assignment (SSA) form. The correspondence is\nmade precise via a series of abstract machines that align the transitions of\nthe structural operational semantics of the CBPV language with the computation\nsteps of the SSA form.\n  The target machine, which is derived from the CBPV language, accurately\ncaptures the execution model of control flow graphs, including direct jumps,\nmutually recursive code blocks, and multi-argument function calls, and the\nclosure-free subset is similar to the SSA intermediate representations found in\nmodern compilers such as LLVM and GCC. The definitions of all the\nlanguage/abstract machine semantics and the theorems relating them are fully\nverified in Coq.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 19:41:47 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Garbuzov", "Dmitri", ""], ["Mansky", "William", ""], ["Rizkallah", "Christine", ""], ["Zdancewic", "Steve", ""]]}, {"id": "1805.05516", "submitter": "EPTCS", "authors": "Dines Bj{\\o}rner (Technical University of Denmark)", "title": "Domain Analysis & Description - The Implicit and Explicit Semantics\n  Problem", "comments": "In Proceedings IMPEX 2017 and FM&MDD 2017, arXiv:1805.04636", "journal-ref": "EPTCS 271, 2018, pp. 1-23", "doi": "10.4204/EPTCS.271.1", "report-no": null, "categories": "cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A domain analysis & description calculus is introduced. It is shown to\nalleviate the issue of implicit semantics. The claim is made that domain\ndescriptions, whether informal, or as also here, formal, amount to an explicit\nsemantics for what is otherwise implicit if not described.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 01:19:40 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Bj\u00f8rner", "Dines", "", "Technical University of Denmark"]]}, {"id": "1805.05576", "submitter": "Georges-Axel Jaloyan", "authors": "Georges-Axel Jaloyan, Yannick Moy, Andrei Paskevich", "title": "Borrowing Safe Pointers from Rust in SPARK", "comments": "17 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the field of deductive software verification, programs with pointers\npresent a major challenge due to pointer aliasing. In this paper, we introduce\npointers to SPARK, a well-defined subset of the Ada language, intended for\nformal verification of mission-critical software. Our solution uses a\npermission-based static alias analysis method inspired by Rust's borrow-checker\nand affine types, and enforces the Concurrent Read, Exclusive Write policy.\nThis analysis has been implemented in the GNAT Ada compiler and tested against\na number of challenging examples. In the paper, we give a formal presentation\nof the analysis rules for a miniature version of SPARK and prove their\nsoundness. We discuss the implementation and compare our solution with Rust.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 05:58:50 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Jaloyan", "Georges-Axel", ""], ["Moy", "Yannick", ""], ["Paskevich", "Andrei", ""]]}, {"id": "1805.06090", "submitter": "Brett Boston", "authors": "Brett Boston, Zoe Gong, Michael Carbin", "title": "Verifying Programs Under Custom Application-Specific Execution Models", "comments": "23 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers have recently designed a number of application-specific fault\ntolerance mechanisms that enable applications to either be naturally resilient\nto errors or include additional detection and correction steps that can bring\nthe overall execution of an application back into an envelope for which an\nacceptable execution is eventually guaranteed. A major challenge to building an\napplication that leverages these mechanisms, however, is to verify that the\nimplementation satisfies the basic invariants that these mechanisms\nrequire--given a model of how faults may manifest during the application's\nexecution.\n  To this end we present Leto, an SMT based automatic verification system that\nenables developers to verify their applications with respect to a first-class\nexecution model specification. Namely, Leto enables software and platform\ndevelopers to programmatically specify the execution semantics of the\nunderlying hardware system as well as verify assertions about the behavior of\nthe application's resulting execution. In this paper, we present the Leto\nprogramming language and its corresponding verification system. We also\ndemonstrate Leto on several applications that leverage application-specific\nfault tolerance mechanisms.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 01:42:01 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Boston", "Brett", ""], ["Gong", "Zoe", ""], ["Carbin", "Michael", ""]]}, {"id": "1805.06196", "submitter": "Azalea Raad", "authors": "Azalea Raad, Ori Lahav, Viktor Vafeiadis", "title": "On the Semantics of Snapshot Isolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Snapshot isolation (SI) is a standard transactional consistency model used in\ndatabases, distributed systems and software transactional memory (STM). Its\nsemantics is formally defined both declaratively as an acyclicity axiom, and\noperationally as a concurrent algorithm with memory bearing timestamps.\n  We develop two simpler equivalent operational definitions of SI as lock-based\nreference implementations that do not use timestamps. Our first locking\nimplementation is prescient in that requires a priori knowledge of the data\naccessed by a transaction and carries out transactional writes eagerly\n(in-place). Our second implementation is non-prescient and performs\ntransactional writes lazily by recording them in a local log and propagating\nthem to memory at commit time. Whilst our first implementation is simpler and\nmay be better suited for developing a program logic for SI transactions, our\nsecond implementation is more practical due to its non-prescience. We show that\nboth implementations are sound and complete against the declarative SI\nspecification and thus yield equivalent operational definitions for SI.\n  We further consider, for the first time formally, the use of SI in a context\nwith racy non-transactional accesses, as can arise in STM implementations of\nSI. We introduce robust snapshot isolation (RSI), an adaptation of SI with\nsimilar semantics and guarantees in this mixed setting. We present a\ndeclarative specification of RSI as an acyclicity axiom and analogously develop\ntwo operational models as lock-based reference implementations (one eager, one\nlazy). We show that these operational models are both sound and complete\nagainst the declarative RSI model.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 08:59:47 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 16:36:54 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Raad", "Azalea", ""], ["Lahav", "Ori", ""], ["Vafeiadis", "Viktor", ""]]}, {"id": "1805.06267", "submitter": "Dominik Aumayr", "authors": "Dominik Aumayr, Stefan Marr, Cl\\'ement B\\'era, Elisa Gonzalez Boix,\n  Hanspeter M\\\"ossenb\\\"ock", "title": "Efficient and Deterministic Record & Replay for Actor Languages", "comments": "International Conference on Managed Languages & Runtimes (ManLang'18)", "journal-ref": null, "doi": "10.1145/3237009.3237015", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ubiquity of parallel commodity hardware, developers turn to\nhigh-level concurrency models such as the actor model to lower the complexity\nof concurrent software. However, debugging concurrent software is hard,\nespecially for concurrency models with a limited set of supporting tools. Such\ntools often deal only with the underlying threads and locks, which is at the\nwrong abstraction level and may even introduce additional complexity. To\nimprove on this situation, we present a low-overhead record & replay approach\nfor actor languages. It allows one to debug concurrency issues\ndeterministically based on a previously recorded trace. Our evaluation shows\nthat the average run-time overhead for tracing on benchmarks from the Savina\nsuite is 10% (min. 0%, max. 20%). For Acme-Air, a modern web application, we\nsee a maximum increase of 1% in latency for HTTP requests and about 1.4 MB/s of\ntrace data. These results are a first step towards deterministic replay\ndebugging of actor systems in production.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 12:18:17 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 09:30:21 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Aumayr", "Dominik", ""], ["Marr", "Stefan", ""], ["B\u00e9ra", "Cl\u00e9ment", ""], ["Boix", "Elisa Gonzalez", ""], ["M\u00f6ssenb\u00f6ck", "Hanspeter", ""]]}, {"id": "1805.06562", "submitter": "Chung-chieh Shan", "authors": "Rajan Walia, Praveen Narayanan, Jacques Carette, Sam Tobin-Hochstadt,\n  Chung-chieh Shan", "title": "From high-level inference algorithms to efficient code", "comments": "Final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic programming languages are valuable because they allow domain\nexperts to express probabilistic models and inference algorithms without\nworrying about irrelevant details. However, for decades there remained an\nimportant and popular class of probabilistic inference algorithms whose\nefficient implementation required manual low-level coding that is tedious and\nerror-prone. They are algorithms whose idiomatic expression requires random\narray variables that are latent or whose likelihood is conjugate. Although that\nis how practitioners communicate and compose these algorithms on paper,\nexecuting such expressions requires eliminating the latent variables and\nrecognizing the conjugacy by symbolic mathematics. Moreover, matching the\nperformance of handwritten code requires speeding up loops by more than a\nconstant factor.\n  We show how probabilistic programs that directly and concisely express these\ndesired inference algorithms can be compiled while maintaining efficiency. We\nintroduce new transformations that turn high-level probabilistic programs with\narrays into pure loop code. We then make great use of domain-specific\ninvariants and norms to optimize the code, and to specialize and JIT-compile\nthe code per execution. The resulting performance is competitive with manual\nimplementations.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 00:55:43 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 21:22:43 GMT"}, {"version": "v3", "created": "Sat, 14 Jul 2018 01:08:21 GMT"}, {"version": "v4", "created": "Wed, 10 Apr 2019 01:32:30 GMT"}, {"version": "v5", "created": "Tue, 2 Jul 2019 01:24:58 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Walia", "Rajan", ""], ["Narayanan", "Praveen", ""], ["Carette", "Jacques", ""], ["Tobin-Hochstadt", "Sam", ""], ["Shan", "Chung-chieh", ""]]}, {"id": "1805.06798", "submitter": "Csongor Kiss", "authors": "Csongor Kiss, Matthew Pickering, Nicolas Wu", "title": "Generic Deriving of Generic Traversals", "comments": "28 pages, ICFP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional programmers have an established tradition of using traversals as a\ndesign pattern to work with recursive data structures. The technique is so\nprolific that a whole host of libraries have been designed to help in the task\nof automatically providing traversals by analysing the generic structure of\ndata types. More recently, lenses have entered the functional scene and have\nproved themselves to be a simple and versatile mechanism for working with\nproduct types. They make it easy to focus on the salient parts of a data\nstructure in a composable and reusable manner.\n  In this paper, we use the combination of lenses and traversals to give rise\nto an expressive and flexible library for querying and modifying complex data\nstructures. Furthermore, since our lenses and traversals are based on the\ngeneric shape of data, we are able to use this information to produce code that\nis as efficient as hand-written versions. The technique leverages the structure\nof data to produce generic abstractions that are then eliminated by the\nstandard workhorses of modern functional compilers: inlining and\nspecialisation.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 14:25:52 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Kiss", "Csongor", ""], ["Pickering", "Matthew", ""], ["Wu", "Nicolas", ""]]}, {"id": "1805.06893", "submitter": "Moez AbdelGawad", "authors": "Moez A. AbdelGawad", "title": "Java Subtyping as an Infinite Self-Similar Partial Graph Product", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to supporting variance annotations, such as wildcard types, the subtyping\nrelation in Java and other generic nominally-typed OO programming languages is\nboth interesting and intricate. In these languages, the subtyping relation\nbetween ground object types, i.e., ones with no type variables, is the basis\nfor defining the full OO subtyping relation, i.e., that includes type\nvariables.\n  As an ordering relation over the set of types, the subtyping relation in\nobject-oriented programming languages can always be represented as a directed\ngraph. In order to better understand some of the subtleties of the subtyping\nrelation in Java, in this paper we present how the subtyping relation between\nground Java types can be precisely constructed using two new operations (a\nbinary operation and a unary one) on directed graphs. The binary operation we\nuse, called a partial Cartesian graph product, is similar in its essence to\nstandard graph products and group products. Its definition is based in\nparticular on that of the standard Cartesian graph product.\n  We believe the use of graph operations in constructing the ground generic\nJava subtyping relation reveals some of the not-immediately-obvious structure\nof the subtyping relation not only in Java but, more generally, also in\nmainstream generic nominally-typed OO programming languages such as C#, Scala\nand Kotlin. Accordingly, we believe that describing precisely how graph\noperations can be used to explicitly construct the subtyping relation in these\nlanguages, as we do in this paper, may significantly improve our understanding\nof features of the type systems of these languages such as wildcard types and\nvariance annotations, and of the dependency of these features on nominal\nsubtyping in nominally-typed OOP.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 11:48:00 GMT"}, {"version": "v2", "created": "Sun, 15 Jul 2018 14:52:36 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["AbdelGawad", "Moez A.", ""]]}, {"id": "1805.07155", "submitter": "Moez AbdelGawad", "authors": "Moez A. AbdelGawad", "title": "Partial Cartesian Graph Product", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we define a new product-like binary operation on directed\ngraphs, and we discuss some of its properties. We also briefly discuss its\napplication in constructing the subtyping relation in generic nominally-typed\nobject-oriented programming languages.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 11:45:05 GMT"}, {"version": "v2", "created": "Sun, 15 Jul 2018 14:38:56 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["AbdelGawad", "Moez A.", ""]]}, {"id": "1805.07176", "submitter": "Francisco Ferreira Ruiz", "authors": "Francisco Ferreira Ruiz", "title": "Proofs and Programs about Open Terms", "comments": "PhD thesis, McGill Univ (2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Formal deductive systems are very common in computer science. They are used\nto represent logics, programming languages, and security systems. Moreover,\nwriting programs that manipulate them and that reason about them is important\nand common. Consider proof assistants, language interpreters, compilers and\nother software that process input described by formal systems. This thesis\nshows that contextual types can be used to build tools for convenient\nimplementation and reasoning about deductive systems with binders. We discuss\nthree aspects of this: the reconstruction of implicit parameters that makes\nwriting proofs and programs with dependent types easier, the addition of\ncontextual objects to an existing programming language that make implementing\nformal systems with binders easier, and finally, we explore the idea of\nembedding the logical framework LF using contextual types in fully dependently\ntyped theory. These are three aspects of the same message: programming using\nthe right abstraction allows us to solve deeper problems with less effort. In\nthis sense we want: easier to write programs and proofs (with implicit\nparameters), languages that support binders (by embedding a syntactic framework\nusing contextual types), and the power of the logical framework LF with the\nexpressivity of dependent types.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 12:44:49 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Ruiz", "Francisco Ferreira", ""]]}, {"id": "1805.07208", "submitter": "Pablo Gordillo", "authors": "Elvira Albert and Pablo Gordillo and Benjamin Livshits and Albert\n  Rubio and Ilya Sergey", "title": "EthIR: A Framework for High-Level Analysis of Ethereum Bytecode", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-01090-4_30", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing Ethereum bytecode, rather than the source code from which it was\ngenerated, is a necessity when: (1) the source code is not available (e.g., the\nblockchain only stores the bytecode), (2) the information to be gathered in the\nanalysis is only visible at the level of bytecode (e.g., gas consumption is\nspecified at the level of EVM instructions), (3) the analysis results may be\naffected by optimizations performed by the compiler (thus the analysis should\nbe done ideally after compilation). This paper presents EthIR, a framework for\nanalyzing Ethereum bytecode, which relies on (an extension of) OYENTE, a tool\nthat generates CFGs; EthIR produces from the CFGs, a rule-based representation\n(RBR) of the bytecode that enables the application of (existing) high-level\nanalyses to infer properties of EVM code.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 20:49:49 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Albert", "Elvira", ""], ["Gordillo", "Pablo", ""], ["Livshits", "Benjamin", ""], ["Rubio", "Albert", ""], ["Sergey", "Ilya", ""]]}, {"id": "1805.07490", "submitter": "Hongbo Rong", "authors": "Hongbo Rong", "title": "Productively Expressing High-performance Spatial Designs of Givens\n  Rotation-based QR Decomposition Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  QR decomposition is used prevalently in wireless communication. In this\npaper, we express the Givens-rotation-based QR decomposition algorithm on a\nspatial architecture using T2S (Temporal To Spatial), a high-productivity\nspatial programming methodology for expressing high-performance spatial\ndesigns. There are interesting challenges: the loop iteration space is not\nrectangular, and it is not obvious how the imperative algorithm can be\nexpressed in a functional notation, the starting point of T2S. Using QR\ndecomposition as an example, this paper elucidates some general principle, and\nde-mystifies high-performance spatial programming. The paper also serves as a\ntutorial of spatial programming for programmers who are not mathematicians, not\nexpert programmers, and not experts on spatial architectures, but still hope to\nintuitively identify a high-performance design and map to spatial architectures\nefficiently.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 01:52:06 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Rong", "Hongbo", ""]]}, {"id": "1805.07886", "submitter": "Sizhuo Zhang", "authors": "Sizhuo Zhang, Muralidaran Vijayaraghavan, Andrew Wright, Mehdi\n  Alipour, Arvind", "title": "Constructing a Weak Memory Model", "comments": null, "journal-ref": "Computer Architecture (ISCA), 2018 ACM/IEEE 45th Annual\n  International Symposium on, pp. 124-137. IEEE, 2018", "doi": "10.1109/ISCA.2018.00021", "report-no": null, "categories": "cs.AR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weak memory models are a consequence of the desire on part of architects to\npreserve all the uniprocessor optimizations while building a shared memory\nmultiprocessor. The efforts to formalize weak memory models of ARM and POWER\nover the last decades are mostly empirical -- they try to capture empirically\nobserved behaviors -- and end up providing no insight into the inherent nature\nof weak memory models. This paper takes a constructive approach to find a\ncommon base for weak memory models: we explore what a weak memory would look\nlike if we constructed it with the explicit goal of preserving all the\nuniprocessor optimizations. We will disallow some optimizations which break a\nprogrammer's intuition in highly unexpected ways. The constructed model, which\nwe call General Atomic Memory Model (GAM), allows all four load/store\nreorderings. We give the construction procedure of GAM, and provide insights\nwhich are used to define its operational and axiomatic semantics. Though no\nattempt is made to match GAM to any existing weak memory model, we show by\nsimulation that GAM has comparable performance with other models. No deep\nknowledge of memory models is needed to read this paper.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 04:13:51 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 05:57:21 GMT"}, {"version": "v3", "created": "Tue, 18 Sep 2018 18:00:34 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Zhang", "Sizhuo", ""], ["Vijayaraghavan", "Muralidaran", ""], ["Wright", "Andrew", ""], ["Alipour", "Mehdi", ""], ["Arvind", "", ""]]}, {"id": "1805.08059", "submitter": "Sandra Dylus", "authors": "Sandra Dylus (CAU Kiel, Germany), Jan Christiansen (Flensburg\n  University of Applied Sciences, Germany), Finn Teegen (University of Kiel,\n  Germany)", "title": "One Monad to Prove Them All", "comments": null, "journal-ref": "The Art, Science, and Engineering of Programming, 2019, Vol. 3,\n  Issue 3, Article 8", "doi": "10.22152/programming-journal.org/2019/3/8", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One Monad to Prove Them All is a modern fairy tale about curiosity and\nperseverance, two important properties of a successful PhD student. We follow\nthe PhD student Mona on her adventure of proving properties about Haskell\nprograms in the proof assistant Coq. On the one hand, as a PhD student in\ncomputer science Mona observes an increasing demand for correct software\nproducts. In particular, because of the large amount of existing software,\nverifying existing software products becomes more important. Verifying programs\nin the functional programming language Haskell is no exception. On the other\nhand, Mona is delighted to see that communities in the area of theorem proving\nare becoming popular. Thus, Mona sets out to learn more about the interactive\ntheorem prover Coq and verifying Haskell programs in Coq. To prove properties\nabout a Haskell function in Coq, Mona has to translate the function into Coq\ncode. As Coq programs have to be total and Haskell programs are often not, Mona\nhas to model partiality explicitly in Coq. In her quest for a solution Mona\nfinds an ancient manuscript that explains how properties about Haskell\nfunctions can be proven in the proof assistant Agda by translating Haskell\nprograms into monadic Agda programs. By instantiating the monadic program with\na concrete monad instance the proof can be performed in either a total or a\npartial setting. Mona discovers that the proposed transformation does not work\nin Coq due to a restriction in the termination checker. In fact the\ntransformation does not work in Agda anymore as well, as the termination\nchecker in Agda has been improved. We follow Mona on an educational journey\nthrough the land of functional programming where she learns about concepts like\nfree monads and containers as well as basics and restrictions of proof\nassistants like Coq. These concepts are well-known individually, but their\ninterplay gives rise to a solution for Mona's problem based on the originally\nproposed monadic tranformation that has not been presented before. When Mona\nstarts to test her approach by proving a statement about simple Haskell\nfunctions, she realizes that her approach has an additional advantage over the\noriginal idea in Agda. Mona's final solution not only works for a specific\nmonad instance but even allows her to prove monad-generic properties. Instead\nof proving properties over and over again for specific monad instances she is\nable to prove properties that hold for all monads representable by a\ncontainer-based instance of the free monad. In order to strengthen her\nconfidence in the practicability of her approach, Mona evaluates her approach\nin a case study that compares two implementations for queues. In order to share\nthe results with other functional programmers the fairy tale is available as a\nliterate Coq file. If you are a citizen of the land of functional programming\nor are at least familiar with its customs, had a journey that involved\nreasoning about functional programs of your own, or are just a curious soul\nlooking for the next story about monads and proofs, then this tale is for you.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 13:54:58 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 10:45:23 GMT"}, {"version": "v3", "created": "Mon, 1 Oct 2018 14:25:19 GMT"}, {"version": "v4", "created": "Fri, 1 Feb 2019 18:59:40 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Dylus", "Sandra", "", "CAU Kiel, Germany"], ["Christiansen", "Jan", "", "Flensburg\n  University of Applied Sciences, Germany"], ["Teegen", "Finn", "", "University of Kiel,\n  Germany"]]}, {"id": "1805.08106", "submitter": "Joachim Breitner", "authors": "Joachim Breitner", "title": "The sufficiently smart compiler is a theorem prover", "comments": "Published in the IFL 2017 pre-proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  That the Haskell Compiler GHC is capable of proving non-trivial equalities\nbetween Haskell code, by virtue of its aggressive optimizer, in particular the\nterm rewriting engine in the simplifier. We demonstrate this with a surprising\nlittle code in a GHC plugin, explains the knobs we had to turn, discuss the\nlimits of the approach and related applications of the same idea, namely\ntesting that promises from Haskell libraries with domain-specific optimizations\nhold.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 15:03:24 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Breitner", "Joachim", ""]]}, {"id": "1805.08288", "submitter": "Johannes de Fine Licht", "authors": "Johannes de Fine Licht, Maciej Besta, Simon Meierhans, Torsten Hoefler", "title": "Transformations of High-Level Synthesis Codes for High-Performance\n  Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial computing architectures promise a major stride in performance and\nenergy efficiency over the traditional load/store devices currently employed in\nlarge scale computing systems. The adoption of high-level synthesis (HLS) from\nlanguages such as C++ and OpenCL has greatly increased programmer productivity\nwhen designing for such platforms. While this has enabled a wider audience to\ntarget spatial computing architectures, the optimization principles known from\ntraditional software design are no longer sufficient to implement\nhigh-performance codes, due to fundamentally distinct aspects of hardware\ndesign, such as programming for deep pipelines, distributed memory resources,\nand scalable routing. To alleviate this, we present a collection of optimizing\ntransformations for HLS, targeting scalable and efficient architectures for\nhigh-performance computing (HPC) applications. We systematically identify\nclasses of transformations (pipelining, scalability, and memory), the\ncharacteristics of their effect on the HLS code and the resulting hardware\n(e.g., increasing data reuse or resource consumption), and the objectives that\neach transformation can target (e.g., resolve interface contention, or increase\nparallelism). We show how these can be used to efficiently exploit pipelining,\non-chip distributed fast memory, and on-chip dataflow, allowing for massively\nparallel architectures. To quantify the effect of various transformations, we\ncover the optimization process of a sample set of HPC kernels, provided as open\nsource reference codes. We aim to establish a common toolbox to guide both\nperformance engineers and compiler engineers in tapping into the performance\npotential offered by spatial computing architectures using HLS.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 20:55:09 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 00:58:25 GMT"}, {"version": "v3", "created": "Wed, 8 Aug 2018 12:25:43 GMT"}, {"version": "v4", "created": "Fri, 11 Oct 2019 12:11:04 GMT"}, {"version": "v5", "created": "Tue, 29 Oct 2019 09:19:39 GMT"}, {"version": "v6", "created": "Mon, 23 Nov 2020 14:10:19 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Licht", "Johannes de Fine", ""], ["Besta", "Maciej", ""], ["Meierhans", "Simon", ""], ["Hoefler", "Torsten", ""]]}, {"id": "1805.08490", "submitter": "Miltiadis Allamanis", "authors": "Marc Brockschmidt, Miltiadis Allamanis, Alexander L. Gaunt, Oleksandr\n  Polozov", "title": "Generative Code Modeling with Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models for source code are an interesting structured prediction\nproblem, requiring to reason about both hard syntactic and semantic constraints\nas well as about natural, likely programs. We present a novel model for this\nproblem that uses a graph to represent the intermediate state of the generated\noutput. The generative procedure interleaves grammar-driven expansion steps\nwith graph augmentation and neural message passing steps. An experimental\nevaluation shows that our new model can generate semantically meaningful\nexpressions, outperforming a range of strong baselines.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 10:38:41 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 21:56:13 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Brockschmidt", "Marc", ""], ["Allamanis", "Miltiadis", ""], ["Gaunt", "Alexander L.", ""], ["Polozov", "Oleksandr", ""]]}, {"id": "1805.08842", "submitter": "EPTCS", "authors": "Kavon Farvardin (University of Chicago), John Reppy (University of\n  Chicago)", "title": "Compiling with Continuations and LLVM", "comments": "In Proceedings ML/OCAML 2016, arXiv:1812.10891", "journal-ref": "EPTCS 285, 2018, pp. 131-142", "doi": "10.4204/EPTCS.285.5", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LLVM is an infrastructure for code generation and low-level optimizations,\nwhich has been gaining popularity as a backend for both research and industrial\ncompilers, including many compilers for functional languages. While LLVM\nprovides a relatively easy path to high-quality native code, its design is\nbased on a traditional runtime model which is not well suited to alternative\ncompilation strategies used in high-level language compilers, such as the use\nof heap-allocated continuation closures.\n  This paper describes a new LLVM-based backend that supports heap-allocated\ncontinuation closures, which enables constant-time callcc and very-lightweight\nmultithreading. The backend has been implemented in the Parallel ML compiler,\nwhich is part of the Manticore system, but the results should be useful for\nother compilers, such as Standard ML of New Jersey, that use heap-allocated\ncontinuation closures.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 20:07:17 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2018 02:10:39 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Farvardin", "Kavon", "", "University of Chicago"], ["Reppy", "John", "", "University of\n  Chicago"]]}, {"id": "1805.09141", "submitter": "Maxwell Scale Uwadia Osagie", "authors": "U. Onu Fergus, S. U. M. Osagie, M. A. John-Otumu, M. E. Igboke", "title": "OOP and its Calculated Measures in Programming Interactivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study examines the object oriented programming (OOP) and its calculated\nmeasures in programming interactivity in Nigeria. It focused on the existing\nprogramming languages used by programmers and examines the need for integrating\nprogramming interactivity with OOP. A survey was conducted to measure\ninteractivity amongst professionals using certain parameters like flexibility,\ninteractivity, speed, interoperability, scalability, dynamism, and solving real\nlife problems. Data was gathered using questionnaire, and analysis was carried\nout using frequency, percentage ratio, and mean in arriving at a more proactive\nstand. The results revealed that the some of the parameters used are highly in\nsupport of the programming interactivity with OOP.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 15:38:49 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Fergus", "U. Onu", ""], ["Osagie", "S. U. M.", ""], ["John-Otumu", "M. A.", ""], ["Igboke", "M. E.", ""]]}, {"id": "1805.10383", "submitter": "Christopher Jenkins", "authors": "Christopher Jenkins, Aaron Stump", "title": "Spine-local Type Inference", "comments": "Submitted to IFL'18 (Implementation and Application of Functional\n  Languages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present spine-local type inference, a partial type inference system for\ninferring omitted type annotations for System F terms based on local type\ninference. Local type inference relies on bidirectional inference rules to\npropagate type information into and out of adjacent nodes of the AST and\nrestricts type-argument inference to occur only within a single node.\nSpine-local inference relaxes the restriction on type-argument inference by\nallowing it to occur only within an {application spine and improves upon it by\nusing contextual type-argument inference. As our goal is to explore the design\nspace of local type inference, we show that, relative to other variants,\nspine-local type inference enables desirable features such as first-class\ncurried applications, partial type applications, and the ability to infer types\nfor some terms not otherwise possible. Our approach enjoys usual properties of\na bidirectional system of having a specification for our inference algorithm\nand predictable requirements for typing annotations, and in particular\nmaintains some the advantages of local type inference such as a relatively\nsimple implementation and a tendency to produce good-quality error messages\nwhen type inference fails.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 22:44:08 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Jenkins", "Christopher", ""], ["Stump", "Aaron", ""]]}, {"id": "1805.10438", "submitter": "Maja Hanne Kirkeby", "authors": "Henning Christiansen, Maja H. Kirkeby", "title": "Confluence of CHR revisited: invariants and modulo equivalence", "comments": "Pre-proceedings paper presented at the 28th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2018), Frankfurt\n  am Main, Germany, 4-6 September 2018 (arXiv:1808.03326)", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2018/14", "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract simulation of one transition system by another is introduced as a\nmeans to simulate a potentially infinite class of similar transition sequences\nwithin a single transition sequence. This is useful for proving confluence\nunder invariants of a given system, as it may reduce the number of proof cases\nto consider from infinity to a finite number. The classical confluence results\nfor Constraint Handling Rules (CHR) can be explained in this way, using CHR as\na simulation of itself. Using an abstract simulation based on a ground\nrepresentation, we extend these results to include confluence under invariant\nand modulo equivalence, which have not been done in a satisfactory way before.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 07:15:12 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 07:38:06 GMT"}, {"version": "v3", "created": "Tue, 2 Oct 2018 14:07:55 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Christiansen", "Henning", ""], ["Kirkeby", "Maja H.", ""]]}, {"id": "1805.10749", "submitter": "Toru Takisaka", "authors": "Toru Takisaka, Yuichiro Oyabu, Natsuki Urabe, Ichiro Hasuo", "title": "Ranking and Repulsing Supermartingales for Reachability in Probabilistic\n  Programs", "comments": null, "journal-ref": "Automated Technology for Verification and Analysis. ATVA 2018.\n  Lecture Notes in Computer Science, vol 11138. Springer, Cham", "doi": "10.1007/978-3-030-01090-4_28", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing reachability probabilities is a fundamental problem in the analysis\nof probabilistic programs. This paper aims at a comprehensive and comparative\naccount on various martingale-based methods for over- and under-approximating\nreachability probabilities. Based on the existing works that stretch across\ndifferent communities (formal verification, control theory, etc.), we offer a\nunifying account. In particular, we emphasize the role of order-theoretic fixed\npoints---a classic topic in computer science---in the analysis of probabilistic\nprograms. This leads us to two new martingale-based techniques, too. We give\nrigorous proofs for their soundness and completeness. We also make an\nexperimental comparison using our implementation of template-based synthesis\nalgorithms for those martingales.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 03:08:48 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 08:43:45 GMT"}, {"version": "v3", "created": "Sat, 15 Sep 2018 02:39:50 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Takisaka", "Toru", ""], ["Oyabu", "Yuichiro", ""], ["Urabe", "Natsuki", ""], ["Hasuo", "Ichiro", ""]]}, {"id": "1805.10931", "submitter": "Moez AbdelGawad", "authors": "Moez A. AbdelGawad", "title": "Towards Taming Java Wildcards and Extending Java with Interval Types", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Of the complex features of generic nominally-typed OO type systems, wildcard\ntypes and variance annotations are probably the hardest to fully grasp. As\ndemonstrated when adding closures (a.k.a., lambdas) and when extending type\ninference in Java, wildcard types and variance annotations make the development\nand progress of OO programming languages, and of their type systems in\nparticular, a challenging and delicate task.\n  In this work we build on our concurrent work, in which we model Java\nsubtyping using a partial graph product, to suggest how wildcard types in Java\ncan be generalized, and simplified, to interval types. In particular, interval\ntypes correspond to endpoints of paths in the Java subtyping graph.\n  In addition to being a simple and more familiar notion, that is easier to\ngrasp than wildcard types, interval types are strictly more expressive than\nwildcard types. As such, we believe interval types, when developed and analyzed\nin full, will be a welcome addition to Java and other similar generic\nnominally-typed OO programming languages.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 11:49:39 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 16:04:34 GMT"}, {"version": "v3", "created": "Sun, 15 Jul 2018 15:07:42 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["AbdelGawad", "Moez A.", ""]]}, {"id": "1805.11006", "submitter": "EPTCS", "authors": "Dmitrii Kosarev (Saint Petersburg State University), Dmitry Boulytchev\n  (Saint Petersburg State University)", "title": "Typed Embedding of a Relational Language in OCaml", "comments": "In Proceedings ML/OCAML 2016, arXiv:1812.10891", "journal-ref": "EPTCS 285, 2018, pp. 1-22", "doi": "10.4204/EPTCS.285.1", "report-no": null, "categories": "cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an implementation of the relational programming language\nminiKanren as a set of combinators and syntax extensions for OCaml. The key\nfeature of our approach is polymorphic unification, which can be used to unify\ndata structures of arbitrary types. In addition we provide a useful generic\nprogramming pattern to systematically develop relational specifications in a\ntyped manner, and address the problem of integration of relational subsystems\ninto functional applications.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 16:06:08 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 18:36:17 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 10:17:21 GMT"}, {"version": "v4", "created": "Mon, 31 Dec 2018 02:08:57 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Kosarev", "Dmitrii", "", "Saint Petersburg State University"], ["Boulytchev", "Dmitry", "", "Saint Petersburg State University"]]}, {"id": "1805.11021", "submitter": "Adrien Guatto", "authors": "Adrien Guatto", "title": "A Generalized Modality for Recursion", "comments": "17 pages, 13 figures, LICS 2018 (extended version); (fixed typos in\n  op. semantics on 2020-08-03)", "journal-ref": null, "doi": "10.1145/3209108.3209148", "report-no": null, "categories": "cs.PL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nakano's later modality allows types to express that the output of a function\ndoes not immediately depend on its input, and thus that computing its fixpoint\nis safe. This idea, guarded recursion, has proved useful in various contexts,\nfrom functional programming with infinite data structures to formulations of\nstep-indexing internal to type theory. Categorical models have revealed that\nthe later modality corresponds in essence to a simple reindexing of the\ndiscrete time scale.\n  Unfortunately, existing guarded type theories suffer from significant\nlimitations for programming purposes. These limitations stem from the fact that\nthe later modality is not expressive enough to capture precise input-output\ndependencies of functions. As a consequence, guarded type theories reject many\nproductive definitions.\n  Combining insights from guarded type theories and synchronous programming\nlanguages, we propose a new modality for guarded recursion. This modality can\napply any well-behaved reindexing of the time scale to a type. We call such\nreindexings time warps. Several modalities from the literature, including\nlater, correspond to fixed time warps, and thus arise as special cases of ours.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 16:32:12 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 16:01:32 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Guatto", "Adrien", ""]]}, {"id": "1805.11035", "submitter": "Oscar Karnalim", "authors": "Oscar Karnalim, Setia Budi", "title": "The Effectiveness of Low-Level Structure-based Approach Toward Source\n  Code Plagiarism Level Taxonomy", "comments": "The 6th International Conference on Information and Communication\n  Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-level approach is a novel way to detect source code plagiarism. Such\napproach is proven to be effective when compared to baseline approach (i.e., an\napproach which relies on source code token subsequence matching) in controlled\nenvironment. We evaluate the effectiveness of state of the art in low-level\napproach based on Faidhi \\& Robinson's plagiarism level taxonomy; real\nplagiarism cases are employed as dataset in this work. Our evaluation shows\nthat state of the art in low-level approach is effective to handle most\nplagiarism attacks. Further, it also outperforms its predecessor and baseline\napproach in most plagiarism levels.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 23:15:06 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Karnalim", "Oscar", ""], ["Budi", "Setia", ""]]}, {"id": "1805.11651", "submitter": "Vadim Markovtsev", "authors": "Vadim Markovtsev, Waren Long, Egor Bulychev, Romain Keramitas,\n  Konstantin Slavnov, Gabor Markowski", "title": "Splitting source code identifiers using Bidirectional LSTM Recurrent\n  Neural Network", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.PL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Programmers make rich use of natural language in the source code they write\nthrough identifiers and comments. Source code identifiers are selected from a\npool of tokens which are strongly related to the meaning, naming conventions,\nand context. These tokens are often combined to produce more precise and\nobvious designations. Such multi-part identifiers count for 97% of all naming\ntokens in the Public Git Archive - the largest dataset of Git repositories to\ndate. We introduce a bidirectional LSTM recurrent neural network to detect\nsubtokens in source code identifiers. We trained that network on 41.7 million\ndistinct splittable identifiers collected from 182,014 open source projects in\nPublic Git Archive, and show that it outperforms several other machine learning\nmodels. The proposed network can be used to improve the upstream models which\nare based on source code identifiers, as well as improving developer experience\nallowing writing code without switching the keyboard case.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 06:46:55 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 08:05:03 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Markovtsev", "Vadim", ""], ["Long", "Waren", ""], ["Bulychev", "Egor", ""], ["Keramitas", "Romain", ""], ["Slavnov", "Konstantin", ""], ["Markowski", "Gabor", ""]]}, {"id": "1805.11683", "submitter": "Michael Pradel", "authors": "Michael Pradel and Koushik Sen", "title": "DeepBugs: A Learning Approach to Name-based Bug Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language elements in source code, e.g., the names of variables and\nfunctions, convey useful information. However, most existing bug detection\ntools ignore this information and therefore miss some classes of bugs. The few\nexisting name-based bug detection approaches reason about names on a syntactic\nlevel and rely on manually designed and tuned algorithms to detect bugs. This\npaper presents DeepBugs, a learning approach to name-based bug detection, which\nreasons about names based on a semantic representation and which automatically\nlearns bug detectors instead of manually writing them. We formulate bug\ndetection as a binary classification problem and train a classifier that\ndistinguishes correct from incorrect code. To address the challenge that\neffectively learning a bug detector requires examples of both correct and\nincorrect code, we create likely incorrect code examples from an existing\ncorpus of code through simple code transformations. A novel insight learned\nfrom our work is that learning from artificially seeded bugs yields bug\ndetectors that are effective at finding bugs in real-world code. We implement\nour idea into a framework for learning-based and name-based bug detection.\nThree bug detectors built on top of the framework detect accidentally swapped\nfunction arguments, incorrect binary operators, and incorrect operands in\nbinary operations. Applying the approach to a corpus of 150,000 JavaScript\nfiles yields bug detectors that have a high accuracy (between 89% and 95%), are\nvery efficient (less than 20 milliseconds per analyzed file), and reveal 102\nprogramming mistakes (with 68% true positive rate) in real-world code.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 13:03:57 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Pradel", "Michael", ""], ["Sen", "Koushik", ""]]}, {"id": "1805.11799", "submitter": "Taro Sekiyama", "authors": "Taro Sekiyama and Kohei Suenaga", "title": "Automated proof synthesis for propositional logic with deep neural\n  networks", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores the application of deep learning, a machine learning\ntechnique that uses deep neural networks (DNN) in its core, to an automated\ntheorem proving (ATP) problem. To this end, we construct a statistical model\nwhich quantifies the likelihood that a proof is indeed a correct one of a given\nproposition. Based on this model, we give a proof-synthesis procedure that\nsearches for a proof in the order of the likelihood. This procedure uses an\nestimator of the likelihood of an inference rule being applied at each step of\na proof. As an implementation of the estimator, we propose a\nproposition-to-proof architecture, which is a DNN tailored to the automated\nproof synthesis problem. To empirically demonstrate its usefulness, we apply\nour model to synthesize proofs of propositional logic. We train the\nproposition-to-proof model using a training dataset of proposition-proof pairs.\nThe evaluation against a benchmark set shows the very high accuracy and an\nimprovement to the recent work of neural proof synthesis.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 04:22:51 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Sekiyama", "Taro", ""], ["Suenaga", "Kohei", ""]]}, {"id": "1805.12263", "submitter": "Nikolaos Kouvelas", "authors": "Nikos Kouvelas, Vijay Rao, R.R. Venkatesha Prasad", "title": "Employing p-CSMA on a LoRa Network Simulator", "comments": "ns3, LoRaWAN, p-CSMA, channel sensing, persistence, hidden terminals,\n  scalability", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-Power Wide-Area Networks (LPWANs) emerged to cover the needs of Internet\nof Things (IoT)-devices for operational longevity and long operating range.\nAmong LPWANs, Long Range (LoRa) WAN has been the most promising; an upcoming\nIoT protocol, already adopted by big mobile operators like KPN and TTN. With\nLoRaWANs, IoT-devices transmit data to their corresponding gateways over many\nkilometers in a single hop and with 1% duty-cycle. However, in a LoRa network,\nany device claims the channel for data-transmission without performing\nchannel-sensing or synchronization with other devices. This increases\nhumongously the number of collisions of information-packets when the number of\nIoT-devices that are connected per gateway increases.\n  To improve the utilization of the channel, we propose the application of\npersistent-Carrier Sense Multiple Access (p-CSMA) protocols on the MAC layer of\nLoRaWANs. In this manuscript, we report on the initial design of a p-CSMA\ncomponent for the simulation of LoRa networks in ns3. In particular, the\nclasses adding p-CSMA functionality to the IoT-devices are presented.\nAdditionally, the dependencies and relations between these classes and an\nexisting LoRaWAN module on which they apply are detailed. Further, we evaluate\nthis new p-CSMA LoRaWAN module in terms of Packet Reception Ratio (PRR) by\nsimulating LoRa networks. The current report is the first step in the creation\nof a holistic p-CSMA module, directed to support network-researchers and\nconnoisseurs in simulating all aspects of LoRa networks in ns3.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 23:46:23 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Kouvelas", "Nikos", ""], ["Rao", "Vijay", ""], ["Prasad", "R. R. Venkatesha", ""]]}]