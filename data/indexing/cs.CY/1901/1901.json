[{"id": "1901.00181", "submitter": "Ruwan Wickramarachchi", "authors": "W.W.N.C.K. Palagolla and A.P.R. Wickramarachchi", "title": "Effective integration of ICT to facilitate the secondary education in\n  Sri Lanka", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Information and Communication Technology (ICT) has been the phenomenon of the\n21st century and its growing advances continue to play a vital role as an ideal\ntool to acquire, store, disseminate and apply knowledge than ever before. Thus,\nthe integration of ICT in diverse business processes has increased the\nimportance as an imperative source of economic growth in the rapidly changing\nknowledge economy. The paper mainly explores potential barriers towards the\neffective integration of ICT and its impact on the performance of the secondary\neducation. A structured survey questionnaire gathered empirical data from a\nrandom sample of teachers from selected schools in the North Central Province\n(NCP) of Sri Lanka. Results show very low integration of ICT in schools and\nteachers ICT competency. ICT infrastructure, leadership support and school\nplanning are revealed as major organizational constraints for the effective\nintegration of ICT in schools. In contrast, respondents' fairly positive\nattitudes towards ICT indicate potential future developments. Comparative\nfindings revealed that ICT education and English language proficiency are\nsignificant demographic predictors of ICT utilization. Results also reported a\npositive impact of ICT on teachers job performance.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2019 17:14:28 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Palagolla", "W. W. N. C. K.", ""], ["Wickramarachchi", "A. P. R.", ""]]}, {"id": "1901.00552", "submitter": "Antonio Luca Alfeo", "authors": "A.L. Alfeo, M.G.C.A. Cimino, G. Vaglini", "title": "Measuring Physical Activity of Older Adults via Smartwatch and\n  Stigmergic Receptive Fields", "comments": "mail: luca.alfeo@ing.unipi.it", "journal-ref": "INSTICC The 6th International Conference on Pattern Recognition\n  Applications and Methods (ICPRAM 2017), pp. 724-730, Porto, Portugal, 24-26\n  February 2016", "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Physical activity level (PAL) in older adults can enhance healthy aging,\nimprove functional capacity, and prevent diseases. It is known that human\nannotations of PAL can be affected by subjectivity and inaccuracy. Recently\ndeveloped smart devices can allow a non-invasive, analytic, and continuous\ngathering of physiological signals. We present an innovative computational\nsystem fed by signals of heartbeat rate, wrist motion and pedometer sensed by a\nsmartwatch. More specifically, samples of each signal are aggregated by\nfunctional structures called trails. The trailing process is inspired by\nstigmergy, an insects' coordination mechanism, and is managed by computational\nunits called stigmergic receptive fields (SRFs). SRFs, which compute the\nsimilarity between trails, are arranged in a stigmergic perceptron to detect a\ncollection of micro-behaviours of the raw signal, called archetypes. A SRF is\nadaptive to subjects: its structural parameters are tuned by a differential\nevolution algorithm. SRFs are used in a multilayer architecture, providing\nfurther levels of processing to realize macro analyses in the application\ndomain. As a result, the architecture provides a daily PAL, useful to detect\nbehavioural shift indicating initial signs of disease or deviations in\nperformance. As a proof of concept, the approach has been experimented on three\nsubjects.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 23:11:16 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Alfeo", "A. L.", ""], ["Cimino", "M. G. C. A.", ""], ["Vaglini", "G.", ""]]}, {"id": "1901.00553", "submitter": "Antonio Luca Alfeo", "authors": "A.L. Alfeo, F.P. Appio, M.G.C.A. Cimino, A. Lazzeri, A. Martini, G.\n  Vaglini", "title": "An adaptive stigmergy-based system for evaluating technological\n  indicator dynamics in the context of smart specialization", "comments": "mail: luca.alfeo@ing.unipi.it", "journal-ref": "INSTICC The 5th International Conference on Pattern Recognition\n  Applications and Methods (ICPRAM 2016), pp. 497-502, Rome, Italy, 2016", "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Regional innovation is more and more considered an important enabler of\nwelfare. It is no coincidence that the European Commission has started looking\nat regional peculiarities and dynamics, in order to focus Research and\nInnovation Strategies for Smart Specialization towards effective investment\npolicies. In this context, this work aims to support policy makers in the\nanalysis of innovation-relevant trends. We exploit a European database of the\nregional patent application to determine the dynamics of a set of technological\ninnovation indicators. For this purpose, we design and develop a software\nsystem for assessing unfolding trends in such indicators. In contrast with\nconventional knowledge-based design, our approach is biologically-inspired and\nbased on self-organization of information. This means that a functional\nstructure, called track, appears and stays spontaneous at runtime when local\ndynamism in data occurs. A further prototyping of tracks allows a better\ndistinction of the critical phenomena during unfolding events, with a better\nassessment of the progressing levels. The proposed mechanism works if\nstructural parameters are correctly tuned for the given historical context.\nDetermining such correct parameters is not a simple task since different\nindicators may have different dynamics. For this purpose, we adopt an\nadaptation mechanism based on differential evolution. The study includes the\nproblem statement and its characterization in the literature, as well as the\nproposed solving approach, experimental setting and results.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 23:13:52 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Alfeo", "A. L.", ""], ["Appio", "F. P.", ""], ["Cimino", "M. G. C. A.", ""], ["Lazzeri", "A.", ""], ["Martini", "A.", ""], ["Vaglini", "G.", ""]]}, {"id": "1901.00590", "submitter": "EPTCS", "authors": "Kevin Baum (Saarland University, Germany), Holger Hermanns (Saarland\n  University, Germany), Timo Speith (Saarland University, Germany)", "title": "Towards a Framework Combining Machine Ethics and Machine Explainability", "comments": "In Proceedings CREST 2018, arXiv:1901.00073", "journal-ref": "EPTCS 286, 2019, pp. 34-49", "doi": "10.4204/EPTCS.286.4", "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We find ourselves surrounded by a rapidly increasing number of autonomous and\nsemi-autonomous systems. Two grand challenges arise from this development:\nMachine Ethics and Machine Explainability. Machine Ethics, on the one hand, is\nconcerned with behavioral constraints for systems, so that morally acceptable,\nrestricted behavior results; Machine Explainability, on the other hand, enables\nsystems to explain their actions and argue for their decisions, so that human\nusers can understand and justifiably trust them.\n  In this paper, we try to motivate and work towards a framework combining\nMachine Ethics and Machine Explainability. Starting from a toy example, we\ndetect various desiderata of such a framework and argue why they should and how\nthey could be incorporated in autonomous systems. Our main idea is to apply a\nframework of formal argumentation theory both, for decision-making under\nethical constraints and for the task of generating useful explanations given\nonly limited knowledge of the world. The result of our deliberations can be\ndescribed as a first version of an ethically motivated, principle-governed\nframework combining Machine Ethics and Machine Explainability\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 02:52:38 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Baum", "Kevin", "", "Saarland University, Germany"], ["Hermanns", "Holger", "", "Saarland\n  University, Germany"], ["Speith", "Timo", "", "Saarland University, Germany"]]}, {"id": "1901.00642", "submitter": "Wanling Gao", "authors": "Zhifei Zhang, Wanling Gao, Fan Zhang, Yunyou Huang, Shaopeng Dai,\n  Fanda Fan, Jianfeng Zhan, Mengjia Du, Silin Yin, Longxin Xiong, Juan Du,\n  Yumei Cheng, Xiexuan Zhou, Rui Ren, Lei Wang, Hainan Ye", "title": "Landscape of Big Medical Data: A Pragmatic Survey on Prioritized Tasks", "comments": "To appear in IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big medical data poses great challenges to life scientists, clinicians,\ncomputer scientists, and engineers. In this paper, a group of life scientists,\nclinicians, computer scientists and engineers sit together to discuss several\nfundamental issues. First, what are the unique characteristics of big medical\ndata different from those of the other domains? Second, what are the\nprioritized tasks in clinician research and practices utilizing big medical\ndata? And do we have enough publicly available data sets for performing those\ntasks? Third, do the state-of-the-practice and state-of-the-art algorithms\nperform good jobs? Fourth, are there any benchmarks for measuring algorithms\nand systems for big medical data? Fifth, what are the performance gaps of\nstate-of-the-practice and state-of-the-art systems handling big medical data\ncurrently or in future? Finally but not least, are we, life scientists,\nclinicians, computer scientists and engineers, ready for working together? We\nbelieve answering the above issues will help define and shape the landscape of\nbig medical data.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 08:03:35 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Zhang", "Zhifei", ""], ["Gao", "Wanling", ""], ["Zhang", "Fan", ""], ["Huang", "Yunyou", ""], ["Dai", "Shaopeng", ""], ["Fan", "Fanda", ""], ["Zhan", "Jianfeng", ""], ["Du", "Mengjia", ""], ["Yin", "Silin", ""], ["Xiong", "Longxin", ""], ["Du", "Juan", ""], ["Cheng", "Yumei", ""], ["Zhou", "Xiexuan", ""], ["Ren", "Rui", ""], ["Wang", "Lei", ""], ["Ye", "Hainan", ""]]}, {"id": "1901.00699", "submitter": "Spencer Wheatley Dr.", "authors": "Spencer Wheatley, Annette Hofmann and Didier Sornette", "title": "Data breaches in the catastrophe framework & beyond", "comments": "Previous version presented at Symposium on Insurance and Emerging\n  Risks, St. John's University, March 10th, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Development of sustainable insurance for cyber risks, with associated\nbenefits, inter alia requires reduction of ambiguity of the risk. Considering\ncyber risk, and data breaches in particular, as a man-made catastrophe\nclarifies the actuarial need for multiple levels of analysis - going beyond\nclaims-driven loss statistics alone to include exposure, hazard, breach size,\nand so on - and necessitating specific advances in scope, quality, and\nstandards of both data and models. The prominent human element, as well as\ndynamic, networked, and multi-type nature, of cyber risk makes it perhaps\nuniquely challenging. Complementary top-down statistical, and bottom-up\nanalytical approaches are discussed. Focusing on data breach severity, measured\nin private information items ('ids') extracted, we exploit relatively mature\nopen data for U.S. data breaches. We show that this extremely heavy-tailed risk\nis worsening for external attacker ('hack') events - both in frequency and\nseverity. Writing in Q2-2018, the median predicted number of ids breached in\nthe U.S. due to hacking, for the last 6 months of 2018, is 0.5 billion. But\nwith a 5% chance that the figure exceeds 7 billion - doubling the historical\ntotal. 'Fortunately' the total breach in that period turned out to be near the\nmedian.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 12:38:32 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 17:16:47 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Wheatley", "Spencer", ""], ["Hofmann", "Annette", ""], ["Sornette", "Didier", ""]]}, {"id": "1901.00720", "submitter": "Arthur Carvalho", "authors": "Natalia Jerdack, Akmaral Dauletbek, Meredith Divine, Michael Hult,\n  Arthur Carvalho", "title": "Understanding What Drives Bitcoin Trading Activities", "comments": null, "journal-ref": "Proceedings of the 2018 Annual Meeting of the Decision Sciences\n  Institute, pages 1864-1872, Chicago, USA, 2018", "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryptocurrencies have gained tremendous popularity over the past few years.\nThe purpose of this study is to try to understand the factors that are driving\ncryptocurrency-related trading activities. Focusing on the well-established\ncryptocurrency called Bitcoin, we find that online search popularity and the\nvolume of trade in unrelated stock markets positively and negatively,\nrespectively, influence Bitcoin trading volume. We also find no statistical\nevidence that the underlying sentiment behind relevant financial news influence\nBitcoin trading volume. We believe these results might be of great value to\ninvestors interested in cryptocurrencies and might instigate further research\non this topic.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 19:48:05 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Jerdack", "Natalia", ""], ["Dauletbek", "Akmaral", ""], ["Divine", "Meredith", ""], ["Hult", "Michael", ""], ["Carvalho", "Arthur", ""]]}, {"id": "1901.00724", "submitter": "Augusto Ciuffoletti", "authors": "Augusto Ciuffoletti", "title": "On-line Remote EKG as a Web Service", "comments": "Keywords: Non-diagnostic EKG; Websocket; Cloud PaaS; EKG acquisition;\n  Arduino; Raspberry Pi", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A 3-leads, non-diagnostic EKG has a role in emergency rescue and homecare. In\nthis paper we introduce the design and a prototype of a service, provided to a\ndoctor and a patient, for the on-line remote visualization of patient's 3-leads\nEKG. The architecture is based on the HTTP protocol, using commercial\noff-the-shelf devices to implement the sensor on patient's side, a browser on a\nlaptop PC on the doctor's side as viewer, and a cloud container to connect the\ntwo using Websockets. A prototype is built to evaluate signal latency, power\nconsumption of the patient side device, and the quality of the rendering. After\nsome experiments, latency is measured below $1sec$, and power consumption is\nestimated in the 2A*3.3V range; visualization is comparable to commercial,\nnon-diagnostic products. The prototype patient device is portable, and can be\noperated using rechargeable battery packs. Its cost is below 100\\$, and all the\nrequired equipment is commercially available. The architecture is ready for on\nfield evaluation, and we indicate how to improve power consumption while\nreducing cost.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 11:32:21 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Ciuffoletti", "Augusto", ""]]}, {"id": "1901.00733", "submitter": "Jiang Zhang", "authors": "Yufeng Zhan, Yuanqing Xia, Jiang Zhang, Ting Li, Yu Wang", "title": "Crowdsensing Game with Demand Uncertainties: A Deep Reinforcement\n  Learning Approach", "comments": "Submitted to Information Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, explosive increase of smartphones with powerful built-in sensors\nsuch as GPS, accelerometers, gyroscopes and cameras has made the design of\ncrowdsensing applications possible, which create a new interface between human\nbeings and life environment. Until now, various mobile crowdsensing\napplications have been designed, where the crowdsourcers can employ mobile\nusers (MUs) to complete the required sensing tasks. In this paper, emerging\nlearning-based techniques are leveraged to address crowdsensing game with\ndemand uncertainties and private information protection of MUs. Firstly, a\nnovel economic model for mobile crowdsensing is designed, which takes MUs'\nresources constraints and demand uncertainties into consideration. Secondly, an\nincentive mechanism based on Stackelberg game is provided, where the\nsensing-platform (SP) is the leader and the MUs are the followers. Then, the\nexistence and uniqueness of the Stackelberg Equilibrium (SE) is proven and the\nprocedure for computing the SE is given. Furthermore, a dynamic incentive\nmechanism (DIM) based on deep reinforcement learning (DRL) approach is\ninvestigated without knowing the private information of the MUs. It enables the\nSP to learn the optimal pricing strategy directly from game experience without\nany prior knowledge about MUs' information. Finally, numerical simulations are\nimplemented to evaluate the performance and theoretical properties of the\nproposed mechanism and approach.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 07:48:55 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Zhan", "Yufeng", ""], ["Xia", "Yuanqing", ""], ["Zhang", "Jiang", ""], ["Li", "Ting", ""], ["Wang", "Yu", ""]]}, {"id": "1901.00736", "submitter": "Solomia Fedushko", "authors": "Yuriy Syerov, Olha Trach, Solomia Fedushko", "title": "Effect of implementation of improved methods of the life cycle stages\n  organisation to the online community management", "comments": "5 pages, 3 figures", "journal-ref": "International Journal of Computational Research and Development\n  (IJCRD)2016", "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the current problem of investigation of the effect of\nimplementation of improved methods of the life cycle stages organisation to\nonline community management. The online community life cycle is the sum of the\nstages of organisation and development of online community. The current\napproaches of scientific researches of social processes within the WWW are\nanalysed. The types of life cycles are distinguished and implemented in the\nonline community management. The algorithm of life cycle stages of the online\ncommunity is designed. The total quality of the life cycle stages execution and\nquality of the life cycle stages execution of Lviv Polytechnic online community\nfrom 2013 till 2016. Currently, the appropriate study and development of\ncommunity management hardware and software provoke the most interest because\nonline communities are a widespread and popular phenomenon, and the existing\nmanagement software for them is imperfect and not complex.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 14:41:58 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Syerov", "Yuriy", ""], ["Trach", "Olha", ""], ["Fedushko", "Solomia", ""]]}, {"id": "1901.00740", "submitter": "Emre Calisir", "authors": "Emre Calisir and Marco Brambilla", "title": "Characterizing Long-Running Political Phenomena on Social Media", "comments": "11 pages, 8 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social media provides many opportunities to monitor and evaluate political\nphenomena such as referendums and elections. In this study, we propose a set of\napproaches to analyze long-running political events on social media with a\nreal-world experiment: the debate about Brexit, i.e., the process through which\nthe United Kingdom activated the option of leaving the European Union. We\naddress the following research questions: Could Twitter-based stance\nclassification be used to demonstrate public stance with respect to political\nevents? What is the most efficient and comprehensive approach to measuring the\nimpact of politicians on social media? Which of the polarized sides of the\ndebate is more responsive to politician messages and the main issues of the\nBrexit process? What is the share of bot accounts in the Brexit discussion and\nwhich side are they for? By combining the user stance classification, topic\ndiscovery, sentiment analysis, and bot detection, we show that it is possible\nto obtain useful insights about political phenomena from social media data. We\nare able to detect relevant topics in the discussions, such as the demand for a\nnew referendum, and to understand the position of social media users with\nrespect to the different topics in the debate. Our comparative and temporal\nanalysis of political accounts can detect the critical periods of the Brexit\nprocess and the impact they have on the debate.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 13:30:48 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Calisir", "Emre", ""], ["Brambilla", "Marco", ""]]}, {"id": "1901.00746", "submitter": "Mohammad Hessam Olya", "authors": "Mohammad Hessam Olya, Dongxiao Zhu, Kai Yang", "title": "Multi-task Prediction of Patient Workload", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing reliable workload predictive models can affect many aspects of\nclinical decision making procedure. The primary challenge in healthcare systems\nis handling the demand uncertainty over the time. This issue becomes more\ncritical for the healthcare facilities that provide service for chronic disease\ntreatment because of the need for continuous treatments over the time. Although\nsome researchers focused on exploring the methods for workload prediction\nrecently, few types of research mainly focused on forecasting a quantitative\nmeasure for the workload of healthcare providers. Also, among the mentioned\nstudies most of them just focused on workload prediction within one facility.\nThe drawback of the previous studies is the problem is not investigated for\nmultiple facilities where the quality of provided service, the equipment, and\nresources used for provided service as well as the diagnosis and treatment\nprocedures may differ even for patients with similar conditions. To tackle the\nmentioned issue, this paper suggests a framework for patient workload\nprediction by using patients data from VA facilities across the US. To capture\nthe information of patients with similar attributes and make the prediction\nmore accurate, a heuristic cluster based algorithm for single task learning as\nwell as a multi task learning approach are developed in this research.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 17:51:22 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Olya", "Mohammad Hessam", ""], ["Zhu", "Dongxiao", ""], ["Yang", "Kai", ""]]}, {"id": "1901.00748", "submitter": "Zhengbo Zou", "authors": "Zhengbo Zou and Di Sha", "title": "The Impact of Countdown Clocks on Subway Ridership in New York City", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protecting the passengers' safety and increasing ridership are two never\nending pursuits of public transit agencies. One of the proposed methods to\nachieve both goals for subway service is to implement real time train arriving\ncountdown clocks in subway stations. Metropolitan Transportation Authority\n(MTA) of New York City (NYC) chose to install such countdown clocks in their\nstations starting from 2007 on a selection of subway lines. Due to the recent\ndevelopment of Bluetooth Beacon technology, the MTA could now install countdown\nclocks and train trackers in a non intrusive manner with much faster speed. As\na result, the MTA is aiming to install countdown clocks in every subway station\non every line. However, with such an aggressive plan, the impact of countdown\nclocks on subway ridership has not been fully studied. This paper proposes\nusing Panel Regression methods, specifically, Random Effect (RE) model and\nFixed Effect (FE) model to quantify the impact of countdown clocks on subway\nridership. Machine Learning methods, namely Random Forest (RF) with AdaBoost\nand Decision Tree (DT) Regression, are also used as alternative data driven\napproaches for the FE and RE model. The results show that for the G line\nservice, which runs between Brooklyn and Queens, the introduction of countdown\nclocks could increase weekly ridership by about 1783 per station. The study\nalso found that the machine learning methods provide better accuracy in\npredicting the ridership than RE and FE models.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 15:59:18 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Zou", "Zhengbo", ""], ["Sha", "Di", ""]]}, {"id": "1901.00750", "submitter": "Ryan Ebardo", "authors": "Ryan Ebardo", "title": "Visibility and Training in Open Source Software Adoption: A Case in\n  Philippine Higher Education", "comments": "Proceedings of 2018 the 8th International Workshop on Computer\n  Science and Engineering (WCSE 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open Source Software (OSS) has been widely used in the educational\nenvironment largely due to its reduced cost of ownership. While OSS has evolved\nover the years, challenges exist in its implementation and wide adoption.\nForemost among these detriments is the lack of available skills across\nindustries. Since future users of this technology will settle in an\norganizational ecosystem where proprietary and OSS technologies coexist, it is\nvital to understand their learning environment where they initially acquire\ntheir technology skills. The University implements courses that champion the\nuse of open source technologies within its curricula. However, other courses\nare also anchored on technologies that are proprietary. This study is based on\nthe premise that training or the learning experience and visibility or the\nprevalence of OSS in the environment influences its adoption among students.\nEmpirical evidences explore the relationship of visibility and training in the\nadoption of OSS from the perspectives of students in a Philippine university. A\nmodified Technology Acceptance Model incorporating additional constructs is\nvalidated using Partial Least Squares Structural Equation Model. Results of the\nstudy confirms the applicability of TAM in this study, training has positive\ninfluence on both perceived ease of use and perceived usefulness and visibility\nhas a positive influence on perceived ease of use towards initial acceptability\nof students of OSS. Educational implications of the study are discussed,\nlimitations are acknowledged and research frontiers are recommended.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 09:33:34 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Ebardo", "Ryan", ""]]}, {"id": "1901.00751", "submitter": "Neil Deshmukh", "authors": "Neil Deshmukh", "title": "Low-Cost Device Prototype for Automatic Medical Diagnosis Using Deep\n  Learning Methods", "comments": "Best Machine Learning Paper at the 9th IEEE Columbia Ubiquitous\n  Computing, Electronics & Mobile Communication Conference (UEMCON) and\n  presented at the IEEE MIT Undergraduate Research Technology Conference (URTC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel low-cost device prototype for the automatic\ndiagnosis of diseases, utilizing inputted symptoms and personal background. The\nengineering goal is to solve the problem of limited healthcare access with a\nsingle device. Diagnosing diseases automatically is an immense challenge, owing\nto their variable properties and symptoms. On the other hand, Neural Networks\nhave developed into a powerful tool in the field of machine learning, one that\nis showing to be extremely promising at computing diagnosis even with\ninconsistent variables.\n  In this research, a cheap device was created to allow for straightforward\ndiagnosis and treatment of human diseases. By utilizing Deep Neural Networks\n(DNNs) and Convolutional Neural Networks (CNNs), outfitted on a Raspberry Pi\nZero processor ($5), the device is able to detect up to 1537 different diseases\nand conditions and utilize a CNN for on-device visual diagnostics. The user can\ninput the symptoms using the buttons on the device and can take pictures using\nthe same mechanism. The algorithm processes inputted symptoms, providing\ndiagnosis and possible treatment options for common conditions. The purpose of\nthis work was to be able to diagnose diseases through an affordable processor\nwith high accuracy, as it is currently achieving an accuracy of 90% for Top-5\nsymptom-based diagnoses, and 91% for visual skin diseases. The NNs achieve\nperformance far above any other tested system, and its efficiency and ease of\nuse will prove it to be a helpful tool for people around the world. This device\ncould potentially provide low-cost universal access to vital diagnostics and\ntreatment options.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 03:27:56 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2019 01:59:35 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Deshmukh", "Neil", ""]]}, {"id": "1901.00755", "submitter": "Abdur R. Shahid", "authors": "Supriya Sarker, Abdur R. Shahid", "title": "Cyberbullying of High School Students in Bangladesh: An Exploratory\n  Study", "comments": "Independent technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study explores the cyberbullying experience of the high school students\nin Bangladesh. The motivation of the work is to identify the internet usage and\nonline activities that may cause cyberbullying victimization of the students of\nthe age between 13 and 18. The study also investigates cyberbullying prevalence\nand impacts both as victimization and perpetration perspectives, discusses\ntheir reporting practices to parents, school officials, other adults and\nsuggest policies to teach cyber safety strategy and generate awareness among\nstudents.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 02:51:30 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Sarker", "Supriya", ""], ["Shahid", "Abdur R.", ""]]}, {"id": "1901.00756", "submitter": "Avishek Choudhury", "authors": "Avishek Choudhury, Christopher Greene", "title": "Classification of Functioning, Disability, and Health for Children and\n  Youth: ICF-CY Self Care (SCADI Dataset) Using Predictive Analytics", "comments": "In: Proceedings of the 2019 IISE Annual Conference. Edited by\n  Romeijn. HE, Schaefer. A, Thomas. R. Orlando: IISE; 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The International Classification of Functioning, Disability, and Health for\nChildren and Youth (ICF-CY) is a scaffold for designating and systematizing\ndata on functioning and disability. It offers a standard semantic and a\ntheoretical foundation for the demarcation and extent of wellbeing and\ninfirmity. The multidimensional layout of ICF-CY comprehends a plethora of\ninformation with about 1400 categories making it difficult to analyze. Our\nresearch proposes a predictive model that classify self-care problems on\nSelf-Care Activities Dataset based on the ICF- CY. The data used in this study\nresides 206 attributes of 70 children with motor and physical disability. Our\nstudy implements, compare and analyze Random Forest, Support vector machine,\nNaive Bayes, Hoeffding tree, and Lazy locally weighted learning using\ntwo-tailed T-test at 95% confidence interval. Boruta algorithm involved in the\nstudy minimizes the data dimensionality to advocate the minimal-optimal set of\npredictors. Random forest gave the best classification accuracy of 84.75%; root\nmean squared error of 0.18 and receiver operating characteristic of 0.99.\nPredictive analytics can simplify the usage of ICF-CY by automating the\nclassification process of disability, functioning, and health.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 04:31:51 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 09:59:34 GMT"}, {"version": "v3", "created": "Wed, 21 Aug 2019 04:03:32 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Choudhury", "Avishek", ""], ["Greene", "Christopher", ""]]}, {"id": "1901.00760", "submitter": "Joseph Chow", "authors": "Tai-Yu Ma, Saeid Rasulkhani, Joseph Y. J. Chow, Sylvain Klein", "title": "A dynamic ridesharing dispatch and idle vehicle repositioning strategy\n  with integrated transit transfers", "comments": null, "journal-ref": "Transportation Research Part E 128, 417-442 (2019)", "doi": "10.1016/j.tre.2019.07.002", "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a ridesharing strategy with integrated transit in which a private\non-demand mobility service operator may drop off a passenger directly\ndoor-to-door, commit to dropping them at a transit station or picking up from a\ntransit station, or to both pickup and drop off at two different stations with\ndifferent vehicles. We study the effectiveness of online solution algorithms\nfor this proposed strategy. Queueing-theoretic vehicle dispatch and idle\nvehicle relocation algorithms are customized for the problem. Several\nexperiments are conducted first with a synthetic instance to design and test\nthe effectiveness of this integrated solution method, the influence of\ndifferent model parameters, and measure the benefit of such cooperation.\nResults suggest that rideshare vehicle travel time can drop by 40-60%\nconsistently while passenger journey times can be reduced by 50-60% when demand\nis high. A case study of Long Island commuters to New York City (NYC) suggests\nhaving the proposed operating strategy can substantially cut user journey times\nand operating costs by up to 54% and 60% each for a range of 10-30 taxis\ninitiated per zone. This result shows that there are settings where such\nservice is highly warranted.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 19:45:46 GMT"}, {"version": "v2", "created": "Sat, 13 Jul 2019 16:46:16 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Ma", "Tai-Yu", ""], ["Rasulkhani", "Saeid", ""], ["Chow", "Joseph Y. J.", ""], ["Klein", "Sylvain", ""]]}, {"id": "1901.00861", "submitter": "Bradley Gram-Hansen", "authors": "Bradley Gram-Hansen, Patrick Helber, Indhu Varatharajan, Faiza Azam,\n  Alejandro Coca-Castro, Veronika Kopackova, Piotr Bilinski", "title": "Mapping Informal Settlements in Developing Countries using Machine\n  Learning and Low Resolution Multi-spectral Data", "comments": "Published at the AAAI/ACM Conference on AI, ethics and society.\n  Extended results from our previous workshop: arXiv:1812.00812", "journal-ref": "AAAI/ACM Conference on AI, Ethics, and Society (AIES 2019)", "doi": "10.1145/3306618.3314253", "report-no": null, "categories": "cs.CY cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Informal settlements are home to the most socially and economically\nvulnerable people on the planet. In order to deliver effective economic and\nsocial aid, non-government organizations (NGOs), such as the United Nations\nChildren's Fund (UNICEF), require detailed maps of the locations of informal\nsettlements. However, data regarding informal and formal settlements is\nprimarily unavailable and if available is often incomplete. This is due, in\npart, to the cost and complexity of gathering data on a large scale. To address\nthese challenges, we, in this work, provide three contributions. 1) A brand new\nmachine learning data-set, purposely developed for informal settlement\ndetection. 2) We show that it is possible to detect informal settlements using\nfreely available low-resolution (LR) data, in contrast to previous studies that\nuse very-high resolution (VHR) satellite and aerial imagery, something that is\ncost-prohibitive for NGOs. 3) We demonstrate two effective classification\nschemes on our curated data set, one that is cost-efficient for NGOs and\nanother that is cost-prohibitive for NGOs, but has additional utility. We\nintegrate these schemes into a semi-automated pipeline that converts either a\nLR or VHR satellite image into a binary map that encodes the locations of\ninformal settlements.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 16:51:40 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 23:18:26 GMT"}, {"version": "v3", "created": "Thu, 30 May 2019 11:11:39 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Gram-Hansen", "Bradley", ""], ["Helber", "Patrick", ""], ["Varatharajan", "Indhu", ""], ["Azam", "Faiza", ""], ["Coca-Castro", "Alejandro", ""], ["Kopackova", "Veronika", ""], ["Bilinski", "Piotr", ""]]}, {"id": "1901.00897", "submitter": "Kostas Drakonakis", "authors": "Kostas Drakonakis, Panagiotis Ilia, Sotiris Ioannidis, Jason Polakis", "title": "Please Forget Where I Was Last Summer: The Privacy Risks of Public\n  Location (Meta)Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exposure of location data constitutes a significant privacy risk to users\nas it can lead to de-anonymization, the inference of sensitive information, and\neven physical threats. In this paper we present LPAuditor, a tool that conducts\na comprehensive evaluation of the privacy loss caused by publicly available\nlocation metadata. First, we demonstrate how our system can pinpoint users' key\nlocations at an unprecedented granularity by identifying their actual postal\naddresses. Our experimental evaluation on Twitter data highlights the\neffectiveness of our techniques which outperform prior approaches by\n18.9%-91.6% for homes and 8.7%-21.8% for workplaces. Next we present a novel\nexploration of automated private information inference that uncovers\n\"sensitive\" locations that users have visited (pertaining to health, religion,\nand sex/nightlife). We find that location metadata can provide additional\ncontext to tweets and thus lead to the exposure of private information that\nmight not match the users' intentions.\n  We further explore the mismatch between user actions and information exposure\nand find that older versions of the official Twitter apps follow a\nprivacy-invasive policy of including precise GPS coordinates in the metadata of\ntweets that users have geotagged at a coarse-grained level (e.g., city). The\nimplications of this exposure are further exacerbated by our finding that users\nare considerably privacy-cautious in regards to exposing precise location data.\nWhen users can explicitly select what location data is published, there is a\n94.6% reduction in tweets with GPS coordinates. As part of current efforts to\ngive users more control over their data, LPAuditor can be adopted by major\nservices and offered as an auditing tool that informs users about sensitive\ninformation they (indirectly) expose through location metadata.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 20:09:11 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Drakonakis", "Kostas", ""], ["Ilia", "Panagiotis", ""], ["Ioannidis", "Sotiris", ""], ["Polakis", "Jason", ""]]}, {"id": "1901.00912", "submitter": "Kaicheng Yang", "authors": "Kai-Cheng Yang, Onur Varol, Clayton A. Davis, Emilio Ferrara,\n  Alessandro Flammini, Filippo Menczer", "title": "Arming the public with artificial intelligence to counter social bots", "comments": "Published in Human Behavior and Emerging Technologies", "journal-ref": "Hum Behav & Emerg Tech. 2019;e115", "doi": "10.1002/hbe2.115", "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increased relevance of social media in our daily life has been\naccompanied by efforts to manipulate online conversations and opinions.\nDeceptive social bots -- automated or semi-automated accounts designed to\nimpersonate humans -- have been successfully exploited for these kinds of\nabuse. Researchers have responded by developing AI tools to arm the public in\nthe fight against social bots. Here we review the literature on different types\nof bots, their impact, and detection methods. We use the case study of\nBotometer, a popular bot detection tool developed at Indiana University, to\nillustrate how people interact with AI countermeasures. A user experience\nsurvey suggests that bot detection has become an integral part of the social\nmedia experience for many users. However, barriers in interpreting the output\nof AI tools can lead to fundamental misunderstandings. The arms race between\nmachine learning methods to develop sophisticated bots and effective\ncountermeasures makes it necessary to update the training data and features of\ndetection tools. We again use the Botometer case to illustrate both algorithmic\nand interpretability improvements of bot scores, designed to meet user\nexpectations. We conclude by discussing how future AI developments may affect\nthe fight between malicious bots and the public.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 20:46:49 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 18:44:33 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Yang", "Kai-Cheng", ""], ["Varol", "Onur", ""], ["Davis", "Clayton A.", ""], ["Ferrara", "Emilio", ""], ["Flammini", "Alessandro", ""], ["Menczer", "Filippo", ""]]}, {"id": "1901.01061", "submitter": "Lloyd Montgomery", "authors": "Lloyd Montgomery, Guy Evans, Francis Harrison, and Daniela Damian", "title": "Towards a Live Anonymous Question Queue To Address Student Apprehension", "comments": "Accepted for publication to the 20th Western Canadian Conference on\n  Computing Education (WCCCE'15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's university climate many first and second year classes have over a\nhundred students. Large classrooms make some students apprehensive about asking\nquestions. An anonymous method of submitting questions to an instructor would\nallow students to ask their questions without feeling apprehensive. In this\npaper we propose a Live Anonymous Question Queue (LAQQ), a system that\nfacilitates anonymous question submissions in real time to mitigate student\napprehension, increase student participation, and provide real-time feedback to\nthe instructor. To study the necessary features of an LAQQ, we conducted a\nstudy of a system, namely Google Moderator, which best approached our concept\nof an LAQQ. We deployed Google moderator in large lectures and studied its\nsupport of a number of features that we envisioned for an LAQQ. Through our\nclass observations, interviews with instructors, and surveys with the students,\nour results suggest that an LAQQ system must provide support for: notification\nof question submission to provide awareness for the instructor, and context for\nquestions to allow an instructor to easily answer a question. Additionally our\nresults suggest that an LAQQ system must be accessible and usable on multiple\nplatforms. Finally our results suggest that in order to be successful in the\nclassroom an LAQQ system must be fully adopted by the instructor and the\nclassroom organizational structure must change to accommodate the use of the\nLAQQ.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 11:35:19 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Montgomery", "Lloyd", ""], ["Evans", "Guy", ""], ["Harrison", "Francis", ""], ["Damian", "Daniela", ""]]}, {"id": "1901.01276", "submitter": "Prakruthi Karuna", "authors": "Prakruthi Karuna, Hemant Purohit, Vivian Motti", "title": "UTPO: User's Trust Profile Ontology - Modeling trust towards Online\n  Health Information Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the overwhelming quantity of health information that is available\nonline today, finding reliable and trustworthy information is still\nchallenging, even when advanced recommender systems are used. To tackle this\nchallenge and improve our recommended sources, we need to first understand and\ncapture the user behavior of what is considered to be trustworthy. This paper\npresents a taxonomy of relevant factors that influence user' trust towards\nonline health information sources. We design a survey experiment to validate\nthe taxonomical factors, and propose an ontology using the taxonomy of factors,\nsuch that each user's trust could be modeled as an instance of our ontology and\nthis could later be used programmatically to recommend trustworthy information\nsources. Our work will inform the design of personalized recommendation systems\nand websites to improve online delivery of health information.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 19:05:13 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Karuna", "Prakruthi", ""], ["Purohit", "Hemant", ""], ["Motti", "Vivian", ""]]}, {"id": "1901.01465", "submitter": "Kevin Lin", "authors": "Kevin Lin and David DeLiema", "title": "Subgoals, Problem Solving Phases, and Sources of Knowledge: A Complex\n  Mangle", "comments": "ACM Student Research Competition (SRC) submission in Proceedings of\n  the 50th ACM Technical Symposium on Computer Science Education (SIGCSE '19);\n  3 pages; Poster:\n  https://docs.google.com/drawings/d/1OrfWGp7-o8sI7KJyx4-leY-A8TioXP1IQFKNBDceht4/edit?usp=sharing", "journal-ref": null, "doi": "10.1145/3287324.3293712", "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Educational researchers have increasingly drawn attention to how students\ndevelop computational thinking (CT) skills, including in science, math, and\nliteracy contexts. A key component of CT is the process of abstraction, a\nparticularly challenging concept for novice programmers, but one vital to\nproblem solving. We propose a framework based on situated cognition that can be\nused to document how instructors and students communicate about abstractions\nduring the problem solving process. We develop this framework in a multimodal\ninteraction analysis of a 32-minute long excerpt of a middle school student\nworking in the PixelBots JavaScript programming environment at a two-week\nsummer programming workshop taught by undergraduate CS majors. Through a\nmicrogenetic analysis of the process of teaching and learning about abstraction\nin this excerpt, we document the extemporaneous prioritization of subgoals and\nthe back-and-forth coordination of problem solving phases. In our case study,\nwe identify that (a) problem solving phases are nested with several instances\nof context-switching within a single phase; (b) the introduction of new ideas\nand information create bridges or opportunities to move between different\nproblem solving phases; (c) planning to solve a problem is a non-linear\nprocess; and (d) pedagogical moves such as modeling and prompting highlight\nsituated resources and advance problem solving. Future research should address\nhow to help students structure subgoals and reflect on connections between\nproblem solving phases, and how to help instructors reflect on their routes to\nsupporting students in the problem solving process.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2019 20:24:52 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Lin", "Kevin", ""], ["DeLiema", "David", ""]]}, {"id": "1901.01579", "submitter": "Ruwan Wickramarachchi", "authors": "W. W. N. C. K. Palagolla and A. P. R. Wickramarachchi", "title": "Promoting effective application and management of ICT to enhance\n  performance in secondary schools", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Information and Communication Technology (ICT) is widely used to enhance\nteaching and learning in modern education. However, it is still difficult in\nsome countries to exploit the full potential of ICT in this regard due to\nvarious constraints. The paper explores existing barriers for the effective use\nof ICT to enhance the performance of secondary schools. The study was focused\non secondary schools in the North Central Province (NCP) of Sri Lanka. Results\nshow a very low use of ICT among schools in the territory due to poor ICT\ninfrastructure, leadership support, school planning, and ICT competency.\nHowever, teachers fairly positive attitudes towards ICT indicated a positive\naspect of future developments. Individuals demographic characteristics show\nsignificant differences in the use of ICT in schools. Moreover, positive\nrelationships were found between ICT usage and performance as perceived by\nteachers.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 17:34:09 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Palagolla", "W. W. N. C. K.", ""], ["Wickramarachchi", "A. P. R.", ""]]}, {"id": "1901.01769", "submitter": "Mansoor Ahmed", "authors": "Mansoor Ahmed-Rengers, Ilia Shumailov, Ross Anderson", "title": "Tendrils of Crime: Visualizing the Diffusion of Stolen Bitcoins", "comments": "Accepted at The Fifth International Workshop on Graphical Models for\n  Security, hosted at FLoC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first six months of 2018 saw cryptocurrency thefts of $761 million, and\nthe technology is also the latest and greatest tool for money laundering. This\nincrease in crime has caused both researchers and law enforcement to look for\nways to trace criminal proceeds. Although tracing algorithms have improved\nrecently, they still yield an enormous amount of data of which very few\ndatapoints are relevant or interesting to investigators, let alone ordinary\nbitcoin owners interested in provenance. In this work we describe efforts to\nvisualize relevant data on a blockchain. To accomplish this we come up with a\ngraphical model to represent the stolen coins and then implement this using a\nvariety of visualization techniques.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 12:18:27 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 01:08:33 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Ahmed-Rengers", "Mansoor", ""], ["Shumailov", "Ilia", ""], ["Anderson", "Ross", ""]]}, {"id": "1901.01914", "submitter": "Jason R.C. Nurse Dr", "authors": "Jason R. C. Nurse and Maria Bada", "title": "The Group Element of Cybercrime: Types, Dynamics, and Criminal\n  Operations", "comments": null, "journal-ref": "The Oxford Handbook of Cyberpsychology, 2018", "doi": "10.1093/oxfordhb/9780198812746.013.36", "report-no": null, "categories": "cs.CR cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While cybercrime can often be an individual activity pursued by lone hackers,\nit has increasingly grown into a group activity, with networks across the\nworld. This chapter critically examines the group element of cybercrime from\nseveral perspectives. It identifies the platforms that online\ngroups---cybercriminal and otherwise---use to interact, and considers groups as\nboth perpetrators and victims of cybercrime. A key novelty is the discovery of\nnew types of online groups whose collective actions border on criminality. The\nchapter also analyzes how online cybercrime groups form, organize, and operate.\nIt explores issues such as trust, motives, and means, and draws on several\npoignant examples, from Anonymous to LulzSec, to illustrate the arguments.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 16:43:22 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Nurse", "Jason R. C.", ""], ["Bada", "Maria", ""]]}, {"id": "1901.02056", "submitter": "Hideo Hirose", "authors": "Hideo Hirose", "title": "Prediction of Success or Failure for Final Examination using Nearest\n  Neighbor Method to the Trend of Weekly Online Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the trends of estimated abilities in terms of item response theory for\nonline testing, we can predict the success/failure status for the final\nexamination to each student at early stages in courses. In prediction, we\napplied the newly developed nearest neighbor method for determining the\nsimilarity of learning skill in the trends of estimated abilities, resulting a\nbetter prediction accuracy for success or failure. This paper shows that the\nuse of the learning analytics incorporating the trends for abilities is\neffective. ROC curve and recall precision curve are informative to assist the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 01:00:41 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Hirose", "Hideo", ""]]}, {"id": "1901.02510", "submitter": "Sawsen Ben Nasr", "authors": "Sawsen Ben Nasr", "title": "New approach for a stable multi-criteria ridesharing system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DS", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The witnessed boom in mobility results in many problems such as urbanization,\ncostly construction of many highways and air pollution. In an attempt to\naddress these problems, in this master, we are interested in the implementation\nof a ridesharing system. Ridesharing is recognized as a highly effective means\nof transport to solve energy consumption, environmental pollution and traffic\ncongestion issues. Indeed, ridesharing can reduce the number of vehicles on the\nroads to avoid traffic jams and thus it contributes to a reduction in\ngreenhouse gas emissions. Its main thrust resides in sharing transport\nexpenses, meeting different people and making traveling more enjoyable. In this\nrespect, we introduce in this dissertation an effective ridesharing system,\ncalled the Stable Multi-Criteria Rideshare Matching (SMRM) system, that (i)\nconsiders users' personal preferences when sharing a private space with others\nand (ii) enables a stable matching between driver and passenger sets. The\nperformed experiments show that the introduced system outperforms its\ncompetitors in terms of stability quality and cost.\n  Keywords: Smart cities, Social sustainability, Ridesharing , Social\npreferences , TOPSIS , Stable marriage .\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 20:44:07 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 18:16:21 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Nasr", "Sawsen Ben", ""]]}, {"id": "1901.02547", "submitter": "Solon Barocas", "authors": "Samir Passi and Solon Barocas", "title": "Problem Formulation and Fairness", "comments": "Conference on Fairness, Accountability, and Transparency (FAT* '19),\n  January 29-31, 2019, Atlanta, GA, USA", "journal-ref": null, "doi": "10.1145/3287560.3287567", "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formulating data science problems is an uncertain and difficult process. It\nrequires various forms of discretionary work to translate high-level objectives\nor strategic goals into tractable problems, necessitating, among other things,\nthe identification of appropriate target variables and proxies. While these\nchoices are rarely self-evident, normative assessments of data science projects\noften take them for granted, even though different translations can raise\nprofoundly different ethical concerns. Whether we consider a data science\nproject fair often has as much to do with the formulation of the problem as any\nproperty of the resulting model. Building on six months of ethnographic\nfieldwork with a corporate data science team---and channeling ideas from\nsociology and history of science, critical data studies, and early writing on\nknowledge discovery in databases---we describe the complex set of actors and\nactivities involved in problem formulation. Our research demonstrates that the\nspecification and operationalization of the problem are always negotiated and\nelastic, and rarely worked out with explicit normative considerations in mind.\nIn so doing, we show that careful accounts of everyday data science work can\nhelp us better understand how and why data science problems are posed in\ncertain ways---and why specific formulations prevail in practice, even in the\nface of what might seem like normatively preferable alternatives. We conclude\nby discussing the implications of our findings, arguing that effective\nnormative interventions will require attending to the practical work of problem\nformulation.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 22:56:45 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Passi", "Samir", ""], ["Barocas", "Solon", ""]]}, {"id": "1901.02672", "submitter": "Jason R.C. Nurse Dr", "authors": "Maria Bada and Angela M. Sasse and Jason R. C. Nurse", "title": "Cyber Security Awareness Campaigns: Why do they fail to change\n  behaviour?", "comments": null, "journal-ref": "International Conference on Cyber Security for Sustainable\n  Society, 2015", "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper focuses on Cyber Security Awareness Campaigns, and aims to\nidentify key factors regarding security which may lead them to failing to\nappropriately change people's behaviour. Past and current efforts to improve\ninformation-security practices and promote a sustainable society have not had\nthe desired impact. It is important therefore to critically reflect on the\nchallenges involved in improving information-security behaviours for citizens,\nconsumers and employees. In particular, our work considers these challenges\nfrom a Psychology perspective, as we believe that understanding how people\nperceive risks is critical to creating effective awareness campaigns. Changing\nbehaviour requires more than providing information about risks and reactive\nbehaviours - firstly, people must be able to understand and apply the advice,\nand secondly, they must be motivated and willing to do so - and the latter\nrequires changes to attitudes and intentions. These antecedents of behaviour\nchange are identified in several psychological models of behaviour. We review\nthe suitability of persuasion techniques, including the widely used 'fear\nappeals'. From this range of literature, we extract essential components for an\nawareness campaign as well as factors which can lead to a campaign's success or\nfailure. Finally, we present examples of existing awareness campaigns in\ndifferent cultures (the UK and Africa) and reflect on these.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 10:45:57 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Bada", "Maria", ""], ["Sasse", "Angela M.", ""], ["Nurse", "Jason R. C.", ""]]}, {"id": "1901.02704", "submitter": "Ivens Portugal", "authors": "Ivens Portugal, Paulo Alencar, Donald Cowan", "title": "Cluster Lifecycle Analysis: Challenges, Techniques, and Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novel forms of data analysis methods have emerged as a significant research\ndirection in the transportation domain. These methods can potentially help to\nimprove our understanding of the dynamic flows of vehicles, people, and goods.\nUnderstanding these dynamics has economic and social consequences, which can\nimprove the quality of life locally or worldwide. Aiming at this objective, a\nsignificant amount of research has focused on clustering moving objects to\naddress problems in many domains, including the transportation, health, and\nenvironment. However, previous research has not investigated the lifecycle of a\ncluster, including cluster genesis, existence, and disappearance. The\nrepresentation and analysis of cluster lifecycles can create novel avenues for\nresearch, result in new insights for analyses, and allow unique forms of\nprediction. This technical report focuses on studying the lifecycle of clusters\nby investigating the relations that a cluster has with moving elements and\nother clusters. This technical report also proposes a big data framework that\nmanages the identification and processing of a cluster lifecycle. The ongoing\nresearch approach will lead to new ways to perform cluster analysis and advance\nthe state of the art by leading to new insights related to cluster lifecycle.\nThese results can have a significant impact on transport industry data science\napplications in a wide variety of areas, including congestion management,\nresource optimization, and hotspot management.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 11:37:23 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Portugal", "Ivens", ""], ["Alencar", "Paulo", ""], ["Cowan", "Donald", ""]]}, {"id": "1901.02709", "submitter": "Bernard Yannou", "authors": "Michael Saidani (LGI), Bernard Yannou (LGI), Yann Leroy (LGI),\n  Fran\\c{c}ois Cluzel (LGI), Alissa Kendall (UC Davis)", "title": "A taxonomy of circular economy indicators", "comments": null, "journal-ref": "Journal of Cleaner Production, Elsevier, 2019, 207, pp.542-559", "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implementing circular economy (CE) principles is increasingly recommended as\na convenient solution to meet the goals of sustainable development. New tools\nare required to support practitioners, decision-makers and policy-makers\ntowards more CE practices, as well as to monitor the effects of CE adoption.\nWorldwide, academics, industrialists and politicians all agree on the need to\nuse CE-related measuring instruments to manage this transition at different\nsystemic levels. In this context, a wide range of circularity indicators\n(C-indicators) has been developed in recent years. Yet, as there is not one\nsingle definition of the CE concept, it is of the utmost importance to know\nwhat the available indicators measure in order to use them properly. Indeed,\nthrough a systematic literature review-considering both academic and grey\nliterature-55 sets of C-indicators, developed by scholars, consulting companies\nand governmental agencies, have been identified, encompassing different\npurposes, scopes, and potential usages. Inspired by existing taxonomies of\neco-design tools and sustainability indicators, and in line with the CE\ncharacteristics, a classification of indicators aiming to assess, improve,\nmonitor and communicate on the CE performance is proposed and discussed. In the\ndeveloped taxonomy including 10 categories, C-indicators are differentiated\nregarding criteria such as the levels of CE implementation (e.g. micro, meso,\nmacro), the CE loops (maintain, reuse, remanufacture, recycle), the performance\n(intrinsic, impacts), the perspective of circularity (actual, potential) they\nare taking into account, or their degree of transversality (generic,\nsector-specific). In addition, the database inventorying the 55 sets of\nC-indicators is linked to an Excel-based query tool to facilitate the selection\nof appropriate indicators according to the specific user's needs and\nrequirements. This study enriches the literature by giving a first need-driven\ntaxonomy of C-indicators, which is experienced on several use cases. It\nprovides a synthesis and clarification to the emerging and must-needed research\ntheme of C-indicators, and sheds some light on remaining key challenges like\ntheir effective uptake by industry. Eventually, limitations, improvement areas,\nas well as implications of the proposed taxonomy are intently addressed to\nguide future research on C-indicators and CE implementation.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 15:11:43 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Saidani", "Michael", "", "LGI"], ["Yannou", "Bernard", "", "LGI"], ["Leroy", "Yann", "", "LGI"], ["Cluzel", "Fran\u00e7ois", "", "LGI"], ["Kendall", "Alissa", "", "UC Davis"]]}, {"id": "1901.02710", "submitter": "Ei Pa Pa Pe-Than", "authors": "Ei Pa Pa Pe-Than and Alexander Nolte (Editors)", "title": "The 2nd Workshop on Hacking and Making at Time-Bounded Events", "comments": null, "journal-ref": null, "doi": null, "report-no": "CMU-ISR-18-109", "categories": "cs.CY cs.SE physics.ed-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In hackathons, small teams work together over a specified period of time to\ncomplete a project of interest. Such time-bounded hackathon-style events have\nbecome increasingly popular across different domains in recent years.\nCollegiate hackathons, just one of the many variants of hackathons, that are\nsupported by the largest hackathon league (https://mlh.io/) alone attract over\n65,000 participants among more than 200 events each year. Variously known as\ndata dives, codefests, hack-days, sprints, edit-a-thons, mapathons, and so on,\nsuch events vary depending on different audiences and with divergent aims: for\nexample, whether teams know each other beforehand, whether the event is\nstructured as a competition with prizes, whether the event is open or requires\nmembership or invitations, and whether the desired outcome is primarily a\nproduct innovation, learning a new skill, forming a community around a cause,\nsolving a technical problem that requires intensive focus by a group, or just\nhaving fun. Taken together, hackathons offer new opportunities and challenges\nfor collaboration by affording explicit, predictable, time-bounded spaces for\ncollaborative work and engaging with new audiences. With the goal of discussing\nopportunities and challenges surrounding hackathons of different kinds, this\none-day workshop brought together researchers, experienced event organizers,\nand practitioners to share and discuss their practical experiences. Empirical\ninsights from studying these events may help position the CHI community to\nbetter study, plan and design hackathon-style events as socio-technical systems\nthat support new modes of production and collaboration.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 19:50:57 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Pe-Than", "Ei Pa Pa", "", "Editors"], ["Nolte", "Alexander", "", "Editors"]]}, {"id": "1901.02714", "submitter": "Avishek Choudhury", "authors": "Avishek Choudhury", "title": "Hourly Forecasting of Emergency Department Arrivals : Time Series\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The stochastic behavior of patient arrival at an emergency\ndepartment (ED) complicates the management of an ED. More than 50% of hospitals\nED capacity tends to operate beyond its normal capacity and eventually fails to\ndeliver high-quality care. To address the concern of stochastics ED arrivals,\nmany types of research has been done using yearly, monthly and weekly time\nseries forecasting. Aim: Our research team believes that hourly time-series\nforecasting of the load can improve ED management by predicting the arrivals of\nfuture patients, and thus, can support strategic decisions in terms of quality\nenhancement. Methods: Our research does not involve any human subject, only ED\nadmission data from January 2014 to August 2017 retrieved from the UnityPoint\nHealth database. Autoregressive integrated moving average (ARIMA), Holt\nWinters, TBATS, and neural network methods were implemented to forecast hourly\nED patient arrival. Findings: ARIMA (3,0,0) (2,1,0) was selected as the best\nfit model with minimum Akaike information criterion and Schwartz Bayesian\ncriterion. The model was stationary and qualified the Box Ljung correlation\ntest and the Jarque Bera test for normality. The mean error (ME) and root mean\nsquare error (RMSE) were selected as performance measures. An ME of 1.001 and\nan RMSE of 1.55 was obtained. Conclusions: ARIMA can be used to provide hourly\nforecasts for ED arrivals and can be utilized as a decision support system in\nthe healthcare industry. Application: This technique can be implemented in\nhospitals worldwide to predict ED patient arrival.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 03:59:53 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Choudhury", "Avishek", ""]]}, {"id": "1901.02715", "submitter": "Yanling Chang", "authors": "Yanling Chang, Eleftherios Iakovou and Weidong Shi4", "title": "Blockchain in Global Supply Chains and Cross Border Trade: A Critical\n  Synthesis of the State-of-the-Art, Challenges and Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain in supply chain management is expected to boom over the next five\nyears. It is estimated that the global blockchain supply chain market would\ngrow at a compound annual growth rate of 87% and increase from \\$45 million in\n2018 to \\$3,314.6 million by 2023. Blockchain will improve business for all\nglobal supply chain stakeholders by providing enhanced traceability,\nfacilitating digitisation, and securing chain-of-custody. This paper provides a\nsynthesis of the existing challenges in global supply chain and trade\noperations, as well as the relevant capabilities and potential of blockchain.\nWe further present leading pilot initiatives on applying blockchains to supply\nchains and the logistics industry to fulfill a range of needs. Finally, we\ndiscuss the implications of blockchain on customs and governmental agencies,\nsummarize challenges in enabling the wide scale deployment of blockchain in\nglobal supply chain management, and identify future research directions.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2019 20:33:36 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Chang", "Yanling", ""], ["Iakovou", "Eleftherios", ""], ["Shi4", "Weidong", ""]]}, {"id": "1901.02719", "submitter": "Emanuele Fabbiani", "authors": "Andrea Marziali, Emanuele Fabbiani and Giuseppe De Nicolao", "title": "Forecasting residential gas demand: machine learning approaches and\n  seasonal role of temperature forecasts", "comments": null, "journal-ref": "Int. J. Oil, Gas and Coal Technology, Vol. 26, No. 2, pp.202-224\n  (2021)", "doi": "10.1504/IJOGCT.2021.10035081", "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gas demand forecasting is a critical task for energy providers as it impacts\non pipe reservation and stock planning. In this paper, the one-day-ahead\nforecasting of residential gas demand at country level is investigated by\nimplementing and comparing five models: Ridge Regression, Gaussian Process\n(GP), k-Nearest Neighbour, Artificial Neural Network (ANN), and Torus Model.\nItalian demand data from 2007 to 2017 are used for training and testing the\nproposed algorithms. The choice of the relevant covariates and the most\nsignificant aspects of the pre-processing and feature extraction steps are\ndiscussed in-depth, lending particular attention to the role of one-day-ahead\ntemperature forecasts. Our best model, in terms of Root Mean Squared Error\n(RMSE), is the ANN, closely followed by the GP. If the Mean Absolute Error\n(MAE) is taken as an error measure, the GP becomes the best model, although by\na narrow margin. A main novel contribution is the development of a model\ndescribing the propagation of temperature errors to gas forecasting errors that\nis successfully validated on experimental data. Being able to predict the\nquantitative impact of temperature forecasts on gas forecasts could be useful\nin order to assess potential improvement margins associated with more\nsophisticated weather forecasts. On the Italian data, it is shown that\ntemperature forecast errors account for some 18% of the mean squared error of\ngas demand forecasts provided by ANN.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 12:28:23 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 16:21:18 GMT"}, {"version": "v3", "created": "Sun, 17 Feb 2019 11:48:49 GMT"}, {"version": "v4", "created": "Fri, 3 May 2019 08:01:29 GMT"}, {"version": "v5", "created": "Tue, 10 Mar 2020 16:04:32 GMT"}, {"version": "v6", "created": "Wed, 12 Aug 2020 15:40:15 GMT"}, {"version": "v7", "created": "Thu, 13 Aug 2020 08:16:26 GMT"}, {"version": "v8", "created": "Sat, 23 Jan 2021 17:35:31 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Marziali", "Andrea", ""], ["Fabbiani", "Emanuele", ""], ["De Nicolao", "Giuseppe", ""]]}, {"id": "1901.03192", "submitter": "Andres Abeliuk", "authors": "Andr\\'es Abeliuk, Khaled Elbassioni, Talal Rahwan, Manuel Cebrian,\n  Iyad Rahwan", "title": "Price of Anarchy in Algorithmic Matching of Romantic Partners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic-matching sites offer users access to an unprecedented number of\npotential mates. However, they also pose a principal-agent problem with a\npotential moral hazard. The agent's interest is to maximize usage of the Web\nsite, while the principal's interest is to find the best possible romantic\npartners. This creates a conflict of interest: optimally matching users would\nlead to stable couples and fewer singles using the site, which is detrimental\nfor the online dating industry. Here, we borrow the notion of Price-of-Anarchy\nfrom game theory to quantify the decrease in social efficiency of online dating\nsites caused by the agent's self-interest. We derive theoretical bounds on the\nprice-of-anarchy, showing it can be bounded by a constant that does not depend\non the number of users of the dating site. This suggests that as online dating\nsites grow, their potential benefits scale up without sacrificing social\nefficiency. Further, we performed experiments involving human subjects in a\nmatching market, and compared the social welfare achieved by an optimal\nmatching service against a self-interest matching algorithm. We show that by\nintroducing competition among dating sites, the selfish behavior of agents\naligns with its users, and social efficiency increases.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 20:18:29 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 19:12:37 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Abeliuk", "Andr\u00e9s", ""], ["Elbassioni", "Khaled", ""], ["Rahwan", "Talal", ""], ["Cebrian", "Manuel", ""], ["Rahwan", "Iyad", ""]]}, {"id": "1901.03229", "submitter": "Aaron Wong", "authors": "Aaron S.W. Wong and Ryan Jeffery and Peter Turner and Scott Sleap and\n  Stephan K. Chalup", "title": "RoboCup Junior in the Hunter Region: Driving the Future of Robotic STEM\n  Education", "comments": "12 pages, 3 Figures, RoboCup Symposium 2018 (Accepted, in Press)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.CY cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RoboCup Junior is a project-oriented educational initiative that sponsors\nregional, national and international robotic events for young students in\nprimary and secondary school. It leads children to the fundamentals of teamwork\nand complex problem solving through step-by-step logical thinking using\ncomputers and robots. The Faculty of Engineering and Built Environment at the\nUniversity of Newcastle in Australia has hosted and organized the Hunter\nregional tournament since 2012. This paper presents an analysis of data\ncollected from RoboCup Junior in the Hunter Region, New South Wales, Australia,\nfor a period of six years 2012-2017 inclusive. Our study evaluates the\neffectiveness of the competition in terms of geographical spread, participation\nnumbers, and gender balance. We also present a case study about current\nuniversity students who have previously participated in RoboCup Junior.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 05:22:52 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Wong", "Aaron S. W.", ""], ["Jeffery", "Ryan", ""], ["Turner", "Peter", ""], ["Sleap", "Scott", ""], ["Chalup", "Stephan K.", ""]]}, {"id": "1901.03536", "submitter": "David Glowacki", "authors": "Lisa May Thomas, Helen M. Deeks, Alex J. Jones, Oussama Metatla, David\n  R. Glowacki", "title": "Somatic Practices for Understanding Real, Imagined, and Virtual\n  Realities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In most VR experiences, the visual sense dominates other modes of sensory\ninput, encouraging non-visual senses to respond as if the visual were real. The\nsimulated visual world thus becomes a sort of felt actuality, where the\n'actual' physical body and environment can 'drop away', opening up\npossibilities for designing entirely new kinds of experience. Most VR\nexperiences place visual sensory input (of the simulated environment) in the\nperceptual foreground, and the physical body in the background. In what\nfollows, we discuss methods for resolving the apparent tension which arises\nfrom VR's prioritization of visual perception. We specifically aim to\nunderstand how somatic techniques encouraging participants to 'attend to their\nattention' enable them to access more subtle aspects of sensory phenomena in a\nVR experience, bound neither by rigid definitions of vision-based virtuality\nnor body-based corporeality. During a series of workshops, we implemented\nexperimental somatic-dance practices to better understand perceptual and\nimaginative subtleties that arise for participants whilst they are embedded in\na multi-person VR framework. Our preliminary observations suggest that somatic\nmethods can be used to design VR experiences which enable (i) a tactile quality\nor felt sense of phenomena in the virtual environment (VE), (ii) lingering\nimpacts on participant imagination even after the VR headset is taken off, and\n(iii) an expansion of imaginative potential.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 10:06:46 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Thomas", "Lisa May", ""], ["Deeks", "Helen M.", ""], ["Jones", "Alex J.", ""], ["Metatla", "Oussama", ""], ["Glowacki", "David R.", ""]]}, {"id": "1901.03589", "submitter": "Marcos Oliveira", "authors": "Marcos Oliveira, Ronaldo Menezes", "title": "Spatial concentration and temporal regularities in crime", "comments": "15 pages, 4 figures. To appear in \"Understanding Crime through\n  Science\" (Springer, 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though crime is linked to different socio-economic factors, it exhibits\nremarkable regularities regardless of cities' particularities. In this chapter,\nwe consider two fundamental regularities in crime regarding two essential\naspects of criminal activity: time and space. For more than one century, we\nknow that (1) crime occurs unevenly within a city and (2) crime peaks during\nspecific times of the year. Here we describe the tendency of crime to\nconcentrate spatially and to exhibit temporal regularities. We examine these\nphenomena from the complex-system perspective of cities, accounting for the\npossibility of both spatial heterogeneity and non-stationarity in urban\nphenomena.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 14:23:10 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Oliveira", "Marcos", ""], ["Menezes", "Ronaldo", ""]]}, {"id": "1901.03723", "submitter": "George Grispos", "authors": "George Grispos, William Bradley Glisson, Tim Storer", "title": "How Good is Your Data? Investigating the Quality of Data Generated\n  During Security Incident Response Investigations", "comments": "George Grispos, William Bradley Glisson, Tim Storer (2019). How Good\n  is Your Data? Investigating the Quality of Data Generated During Security\n  Incident Response Investigations. The 52nd Hawaii International Conference on\n  System Sciences (HICSS-52), Maui, HI, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of cybersecurity incidents prompts organizations to\nexplore alternative security solutions, such as threat intelligence programs.\nFor such programs to succeed, data needs to be collected, validated, and\nrecorded in relevant datastores. One potential source supplying these\ndatastores is an organization's security incident response team. However,\nresearchers have argued that these teams focus more on eradication and recovery\nand less on providing feedback to enhance organizational security. This prompts\nthe idea that data collected during security incident investigations may be of\ninsufficient quality for threat intelligence analysis. While previous\ndiscussions focus on data quality issues from threat intelligence sharing\nperspectives, minimal research examines the data generated during incident\nresponse investigations. This paper presents the results of a case study\nidentifying data quality challenges in a Fortune 500 organization's incident\nresponse team. Furthermore, the paper provides the foundation for future\nresearch regarding data quality concerns in security incident response.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 19:32:52 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Grispos", "George", ""], ["Glisson", "William Bradley", ""], ["Storer", "Tim", ""]]}, {"id": "1901.03724", "submitter": "George Grispos", "authors": "George Grispos, William Bradley Glisson, Peter Cooper", "title": "A Bleeding Digital Heart: Identifying Residual Data Generation from\n  Smartphone Applications Interacting with Medical Devices", "comments": "George Grispos, William Bradley Glisson, Peter Cooper (2019). A\n  Bleeding Digital Heart: Identifying Residual Data Generation from Smartphone\n  Applications Interacting with Medical Devices. The 52nd Hawaii International\n  Conference on System Sciences (HICSS-52), Maui, HI, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration of medical devices in everyday life prompts the idea that\nthese devices will increasingly have evidential value in civil and criminal\nproceedings. However, the investigation of these devices presents new\nchallenges for the digital forensics community. Previous research has shown\nthat mobile devices provide investigators with a wealth of information. Hence,\nmobile devices that are used within medical environments potentially provide an\navenue for investigating and analyzing digital evidence from such devices. The\nresearch contribution of this paper is twofold. First, it provides an empirical\nanalysis of the viability of using information from smartphone applications\ndeveloped to complement a medical device, as digital evidence. Second, it\nincludes documentation on the artifacts that are potentially useful in a\ndigital forensics investigation of smartphone applications that interact with\nmedical devices.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 19:35:26 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Grispos", "George", ""], ["Glisson", "William Bradley", ""], ["Cooper", "Peter", ""]]}, {"id": "1901.03888", "submitter": "Erion \\c{C}ano", "authors": "Erion \\c{C}ano, Maurizio Morisio", "title": "Hybrid Recommender Systems: A Systematic Literature Review", "comments": "38 pages, 9 figures, 14 tables. The final authenticated version is\n  available online at\n  https://content.iospress.com/articles/intelligent-data-analysis/ida163209", "journal-ref": "Intelligent Data Analysis, vol. 21, no. 6, pp. 1487-1524, 2017", "doi": "10.3233/IDA-163209", "report-no": null, "categories": "cs.IR cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recommender systems are software tools used to generate and provide\nsuggestions for items and other entities to the users by exploiting various\nstrategies. Hybrid recommender systems combine two or more recommendation\nstrategies in different ways to benefit from their complementary advantages.\nThis systematic literature review presents the state of the art in hybrid\nrecommender systems of the last decade. It is the first quantitative review\nwork completely focused in hybrid recommenders. We address the most relevant\nproblems considered and present the associated data mining and recommendation\ntechniques used to overcome them. We also explore the hybridization classes\neach hybrid recommender belongs to, the application domains, the evaluation\nprocess and proposed future research directions. Based on our findings, most of\nthe studies combine collaborative filtering with another technique often in a\nweighted way. Also cold-start and data sparsity are the two traditional and top\nproblems being addressed in 23 and 22 studies each, while movies and movie\ndatasets are still widely used by most of the authors. As most of the studies\nare evaluated by comparisons with similar methods using accuracy metrics,\nproviding more credible and user oriented evaluations remains a typical\nchallenge. Besides this, newer challenges were also identified such as\nresponding to the variation of user context, evolving user tastes or providing\ncross-domain recommendations. Being a hot topic, hybrid recommenders represent\na good basis with which to respond accordingly by exploring newer opportunities\nsuch as contextualizing recommendations, involving parallel hybrid algorithms,\nprocessing larger datasets, etc.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 18:12:44 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["\u00c7ano", "Erion", ""], ["Morisio", "Maurizio", ""]]}, {"id": "1901.04072", "submitter": "Adrian Wilke", "authors": "Adrian Wilke, Johannes Magenheim", "title": "Critical Incidents for Technology Enhanced Learning in Vocational\n  Education and Training - Observations from the field of mechanical\n  engineering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, observations of the Vocational Education and Training (VET) in\nmechanical engineering companies are carried out. A Learning Management System\n(LMS) had been developed for the assistance in solving typical task structures,\nthat are used for a period of three and a half years in the apprenticeship. In\nthis study, the Critical Incident Technique (CIT) is applied for the\nobservations. For the subsequent analysis, a classification of incidents is\nperformed. The most important incidents as well as conclusions for Technical\nEnhanced Learning (TEL) in similar domains are presented.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 21:59:25 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Wilke", "Adrian", ""], ["Magenheim", "Johannes", ""]]}, {"id": "1901.04542", "submitter": "Pablo Su\\'arez-Serrato", "authors": "E.I. Velazquez Richards, E. Gallagher, P. Su\\'arez-Serrato", "title": "BoostNet: Bootstrapping detection of socialbots, and a case study from\n  Guatemala", "comments": "7 pages, 4 figures", "journal-ref": "Selected Contributions on Statistics and Data Science in Latin\n  America. FNE 2018. Springer Proceedings in Mathematics & Statistics, vol 301", "doi": "10.1007/978-3-030-31551-1_11", "report-no": null, "categories": "cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a method to reconstruct networks of socialbots given minimal\ninput. Then we use Kernel Density Estimates of Botometer scores from 47,000\nsocial networking accounts to find clusters of automated accounts, discovering\nover 5,000 socialbots. This statistical and data driven approach allows for\ninference of thresholds for socialbot detection, as illustrated in a case study\nwe present from Guatemala.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 20:00:45 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Richards", "E. I. Velazquez", ""], ["Gallagher", "E.", ""], ["Su\u00e1rez-Serrato", "P.", ""]]}, {"id": "1901.04562", "submitter": "Alex Beutel", "authors": "Alex Beutel, Jilin Chen, Tulsee Doshi, Hai Qian, Allison Woodruff,\n  Christine Luu, Pierre Kreitmann, Jonathan Bischof, Ed H. Chi", "title": "Putting Fairness Principles into Practice: Challenges, Metrics, and\n  Improvements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As more researchers have become aware of and passionate about algorithmic\nfairness, there has been an explosion in papers laying out new metrics,\nsuggesting algorithms to address issues, and calling attention to issues in\nexisting applications of machine learning. This research has greatly expanded\nour understanding of the concerns and challenges in deploying machine learning,\nbut there has been much less work in seeing how the rubber meets the road.\n  In this paper we provide a case-study on the application of fairness in\nmachine learning research to a production classification system, and offer new\ninsights in how to measure and address algorithmic fairness issues. We discuss\nopen questions in implementing equality of opportunity and describe our\nfairness metric, conditional equality, that takes into account distributional\ndifferences. Further, we provide a new approach to improve on the fairness\nmetric during model training and demonstrate its efficacy in improving\nperformance for a real-world product\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 21:02:29 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Beutel", "Alex", ""], ["Chen", "Jilin", ""], ["Doshi", "Tulsee", ""], ["Qian", "Hai", ""], ["Woodruff", "Allison", ""], ["Luu", "Christine", ""], ["Kreitmann", "Pierre", ""], ["Bischof", "Jonathan", ""], ["Chi", "Ed H.", ""]]}, {"id": "1901.04730", "submitter": "Songuel Tolan", "authors": "Song\\\"ul Tolan", "title": "Fair and Unbiased Algorithmic Decision Making: Current State and Future\n  Challenges", "comments": "Background paper to the European Commission's flagship report:\n  \"Artificial Intelligence: A European Perspective\",\n  https://ec.europa.eu/jrc/en/publication/eur-scientific-and-technical-research-reports/artificial-intelligence-european-perspective\n  25 pages, including cover, figures and references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning algorithms are now frequently used in sensitive contexts\nthat substantially affect the course of human lives, such as credit lending or\ncriminal justice. This is driven by the idea that `objective' machines base\ntheir decisions solely on facts and remain unaffected by human cognitive\nbiases, discriminatory tendencies or emotions. Yet, there is overwhelming\nevidence showing that algorithms can inherit or even perpetuate human biases in\ntheir decision making when they are based on data that contains biased human\ndecisions. This has led to a call for fairness-aware machine learning. However,\nfairness is a complex concept which is also reflected in the attempts to\nformalize fairness for algorithmic decision making. Statistical formalizations\nof fairness lead to a long list of criteria that are each flawed (or harmful\neven) in different contexts. Moreover, inherent tradeoffs in these criteria\nmake it impossible to unify them in one general framework. Thus, fairness\nconstraints in algorithms have to be specific to the domains to which the\nalgorithms are applied. In the future, research in algorithmic decision making\nsystems should be aware of data and developer biases and add a focus on\ntransparency to facilitate regular fairness audits.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 09:40:03 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Tolan", "Song\u00fcl", ""]]}, {"id": "1901.04824", "submitter": "Detlef Steuer", "authors": "Ursula Garzcarek and Detlef Steuer", "title": "Approaching Ethical Guidelines for Data Scientists", "comments": "18 pages, submitted Nov 12th 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.AI cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this article is to inspire data scientists to participate in the\ndebate on the impact that their professional work has on society, and to become\nactive in public debates on the digital world as data science professionals.\nHow do ethical principles (e.g., fairness, justice, beneficence, and\nnon-maleficence) relate to our professional lives? What lies in our\nresponsibility as professionals by our expertise in the field? More\nspecifically this article makes an appeal to statisticians to join that debate,\nand to be part of the community that establishes data science as a proper\nprofession in the sense of Airaksinen, a philosopher working on professional\nethics. As we will argue, data science has one of its roots in statistics and\nextends beyond it. To shape the future of statistics, and to take\nresponsibility for the statistical contributions to data science, statisticians\nshould actively engage in the discussions. First the term data science is\ndefined, and the technical changes that have led to a strong influence of data\nscience on society are outlined. Next the systematic approach from CNIL is\nintroduced. Prominent examples are given for ethical issues arising from the\nwork of data scientists. Further we provide reasons why data scientists should\nengage in shaping morality around and to formulate codes of conduct and codes\nof practice for data science. Next we present established ethical guidelines\nfor the related fields of statistics and computing machinery. Thereafter\nnecessary steps in the community to develop professional ethics for data\nscience are described. Finally we give our starting statement for the debate:\nData science is in the focal point of current societal development. Without\nbecoming a profession with professional ethics, data science will fail in\nbuilding trust in its interaction with and its much needed contributions to\nsociety!\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 16:13:27 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Garzcarek", "Ursula", ""], ["Steuer", "Detlef", ""]]}, {"id": "1901.04972", "submitter": "Istv\\'an Andr\\'as Seres", "authors": "Istv\\'an Andr\\'as Seres, L\\'aszl\\'o Guly\\'as, D\\'aniel A. Nagy,\n  P\\'eter Burcsi", "title": "Topological Analysis of Bitcoin's Lightning Network", "comments": "Added some remarks regarding how centralized is liquidity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin's Lightning Network (LN) is a scalability solution for Bitcoin\nallowing transactions to be issued with negligible fees and settled instantly\nat scale. In order to use LN, funds need to be locked in payment channels on\nthe Bitcoin blockchain (Layer-1) for subsequent use in LN (Layer-2). LN is\ncomprised of many payment channels forming a payment channel network. LN's\npromise is that relatively few payment channels already enable anyone to\nefficiently, securely and privately route payments across the whole network. In\nthis paper, we quantify the structural properties of LN and argue that LN's\ncurrent topological properties can be ameliorated in order to improve the\nsecurity of LN, enabling it to reach its true potential.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 18:46:24 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 21:41:03 GMT"}, {"version": "v3", "created": "Sun, 14 Apr 2019 11:09:09 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Seres", "Istv\u00e1n Andr\u00e1s", ""], ["Guly\u00e1s", "L\u00e1szl\u00f3", ""], ["Nagy", "D\u00e1niel A.", ""], ["Burcsi", "P\u00e9ter", ""]]}, {"id": "1901.05139", "submitter": "Maiia Popel", "authors": "Maiia Popel", "title": "Using CoCalc as a Training Tool for Mathematics Teachers Pre-Service\n  Training", "comments": "11 pages, 1 figure, Information Technologies and Learning Tools,\n  68(6), 251-261", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ed-ph cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper deals with the problem of theoretical justification and\ndevelopment of scientific and methodological support for using the cloud\nservice CoCalc as a tool for the formation of mathematics teachers professional\ncompetencies. The paper describes the professional training of mathematics\nteachers in universities of Ukraine, and considers the national and foreign\nexperience of using the cloud-based services in mathematics teachers\npre-service training and also the tendencies and prospects of using CoCalc in\nteaching mathematical disciplines. The process of system design of mathematics\nteachers professional competencies is characterized, and the model of using the\ncloud service CoCalc as a tool for forming mathematics teachers professional\ncompetencies is developed. The indicators and levels (high, sufficient, medium,\nlow) were identified for each component of the pre-service mathematics teachers\nprofessional competence system within the proposed model. The method of using\nCoCalc as a tool for forming professional competencies of mathematics teachers\nis developed and its basic components such as purpose, content, tools, methods\nand results are elaborated. Information regarding the stages of research and\nalso the experimental work objectives and content are presented; the\nquantitative and qualitative analysis of the main stages (ascertaining,\nformative) of the pedagogical experiment is performed, confirming the\nhypothesis of the study.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 05:42:44 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Popel", "Maiia", ""]]}, {"id": "1901.05228", "submitter": "Dongwoo Kim", "authors": "Dongwoo Kim, Timothy Graham, Zimin Wan, Marian-Andrei Rizoiu", "title": "Analysing user identity via time-sensitive semantic edit distance\n  (t-SED): A case study of Russian trolls on Twitter", "comments": null, "journal-ref": null, "doi": "10.1007/s42001-019-00051-x", "report-no": null, "categories": "cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the digital era, individuals are increasingly profiled and grouped based\non the traces they leave behind in online social networks such as Twitter and\nFacebook. In this paper, we develop and evaluate a novel text analysis approach\nfor studying user identity and social roles by redefining identity as a\nsequence of timestamped items (e.g. tweet texts). We operationalise this idea\nby developing a novel text distance metric, the time-sensitive semantic edit\ndistance (t-SED), which accounts for the temporal context across multiple\ntraces. To evaluate this method we undertake a case study of Russian\nonline-troll activity within US political discourse. The novel metric allows us\nto classify the social roles of trolls based on their traces, in this case\ntweets, into one of the predefined categories left-leaning, right-leaning, and\nnews feed. We show the effectiveness of the t-SED metric to measure the\nsimilarities between tweets while accounting for the temporal context, and we\nuse novel data visualisation techniques and qualitative analysis to uncover new\nempirical insights into Russian troll activity that have not been identified in\nprevious work. Additionally, we highlight a connection with the field of\nActor-Network Theory and the related hypotheses of Gabriel Tarde, and we\ndiscuss how social sequence analysis using t-SED may provide new avenues for\ntackling a longstanding problem in social theory: how to analyse society\nwithout separating reality into micro versus macro levels.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 10:59:15 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 23:43:34 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Kim", "Dongwoo", ""], ["Graham", "Timothy", ""], ["Wan", "Zimin", ""], ["Rizoiu", "Marian-Andrei", ""]]}, {"id": "1901.05240", "submitter": "Lars Ailo Bongo", "authors": "Bj{\\o}rn Fjukstad, Nina Angelvik, Morten Gr{\\o}nnesby, Maria Wulff\n  Hauglann, Hedinn Gunhildrud, Fredrik H{\\o}is{\\ae}ther Rasch, Julianne\n  Iversen, Margaret Dalseng, Lars Ailo Bongo", "title": "Teaching Electronics and Programming in Norwegian Schools Using the\n  air:bit Sensor Kit", "comments": null, "journal-ref": null, "doi": "10.1145/3304221.3325527", "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe lessons learned from using the air:bit project to introduce more\nthan 150 students in the Norwegian upper secondary school to computer\nprogramming, engineering and environmental sciences. In the air:bit project,\nstudents build and code a portable air quality sensor kits, and use their\nair:bit to collect data to investigate patterns in air quality in their local\nenvironment. When the project ended students had collected more than 400,000\nmeasurements with their air:bit kits, and could describe local patterns in air\nquality. Students participate in all parts of the project, from soldering\ncomponents and programming the sensors, to analyzing the air quality\nmeasurements. We conducted a survey after the project and describe our lessons\nlearned from the project. The results show that the project successfully taught\nthe students fundamental concepts in computer programming, electronics, and the\nscientific method. In addition, all the participating teachers reported that\ntheir students had showed good learning outcomes.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 11:48:04 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Fjukstad", "Bj\u00f8rn", ""], ["Angelvik", "Nina", ""], ["Gr\u00f8nnesby", "Morten", ""], ["Hauglann", "Maria Wulff", ""], ["Gunhildrud", "Hedinn", ""], ["Rasch", "Fredrik H\u00f8is\u00e6ther", ""], ["Iversen", "Julianne", ""], ["Dalseng", "Margaret", ""], ["Bongo", "Lars Ailo", ""]]}, {"id": "1901.05389", "submitter": "M\\'arton Karsai", "authors": "Jacobo Levy Abitbol, M\\'arton Karsai, and Eric Fleury", "title": "Location, Occupation, and Semantics based Socioeconomic Status Inference\n  on Twitter", "comments": "Accepted as a full paper in the 2018 IEEE 18th International\n  Conference on Data Mining - IWSC'18 2nd International Workshop on Social\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The socioeconomic status of people depends on a combination of individual\ncharacteristics and environmental variables, thus its inference from online\nbehavioral data is a difficult task. Attributes like user semantics in\ncommunication, habitat, occupation, or social network are all known to be\ndeterminant predictors of this feature. In this paper we propose three\ndifferent data collection and combination methods to first estimate and, in\nturn, infer the socioeconomic status of French Twitter users from their online\nsemantics. Our methods are based on open census data, crawled professional\nprofiles, and remotely sensed, expert annotated information on living\nenvironment. Our inference models reach similar performance of earlier results\nwith the advantage of relying on broadly available datasets and of providing a\ngeneralizable framework to estimate socioeconomic status of large numbers of\nTwitter users. These results may contribute to the scientific discussion on\nsocial stratification and inequalities, and may fuel several applications.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 16:56:14 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Abitbol", "Jacobo Levy", ""], ["Karsai", "M\u00e1rton", ""], ["Fleury", "Eric", ""]]}, {"id": "1901.05406", "submitter": "Gregory D. Hager", "authors": "Gregory D. Hager, Ann Drobnis, Fei Fang, Rayid Ghani, Amy Greenwald,\n  Terah Lyons, David C. Parkes, Jason Schultz, Suchi Saria, Stephen F. Smith,\n  and Milind Tambe", "title": "Artificial Intelligence for Social Good", "comments": "A Computing Community Consortium (CCC) workshop report, 22 pages", "journal-ref": null, "doi": null, "report-no": "ccc2016report_1", "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Computing Community Consortium (CCC), along with the White House Office\nof Science and Technology Policy (OSTP), and the Association for the\nAdvancement of Artificial Intelligence (AAAI), co-sponsored a public workshop\non Artificial Intelligence for Social Good on June 7th, 2016 in Washington, DC.\nThis was one of five workshops that OSTP co-sponsored and held around the\ncountry to spur public dialogue on artificial intelligence, machine learning,\nand to identify challenges and opportunities related to AI. In the AI for\nSocial Good workshop, the successful deployments and the potential use of AI in\nvarious topics that are essential for social good were discussed, including but\nnot limited to urban computing, health, environmental sustainability, and\npublic welfare. This report highlights each of these as well as a number of\ncrosscutting issues.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 17:42:43 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Hager", "Gregory D.", ""], ["Drobnis", "Ann", ""], ["Fang", "Fei", ""], ["Ghani", "Rayid", ""], ["Greenwald", "Amy", ""], ["Lyons", "Terah", ""], ["Parkes", "David C.", ""], ["Schultz", "Jason", ""], ["Saria", "Suchi", ""], ["Smith", "Stephen F.", ""], ["Tambe", "Milind", ""]]}, {"id": "1901.05484", "submitter": "Marcelo Ponce", "authors": "Marcelo Ponce, Erik Spence, Ramses van Zon, Daniel Gruner", "title": "Bridging the Educational Gap between Emerging and Established Scientific\n  Computing Disciplines", "comments": "Presented at the Workshop on Strategies for Enhancing HPC Education\n  and Training (SEHET18) @ PEARC18", "journal-ref": "Journal of Computational Science Education, vol 10 (2019)", "doi": "10.22369/issn.2153-4136/10/1/1", "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe our experience in developing curriculum courses\naimed at graduate students in emerging computational fields, including biology\nand medical science. We focus primarily on computational data analysis and\nstatistical analysis, while at the same time teaching students best practices\nin coding and software development. Our approach combines a theoretical\nbackground and practical applications of concepts. The outcomes and feedback we\nhave obtained so far have revealed several issues: students in these particular\nareas lack instruction like this although they would tremendously benefit from\nit; we have detected several weaknesses in the formation of students, in\nparticular in the statistical foundations but also in analytical thinking\nskills. We present here the tools, techniques and methodology we employ while\nteaching and developing this type of courses. We also show several outcomes\nfrom this initiative, including potential pathways for fruitful\nmulti-disciplinary collaborations.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 19:02:07 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Ponce", "Marcelo", ""], ["Spence", "Erik", ""], ["van Zon", "Ramses", ""], ["Gruner", "Daniel", ""]]}, {"id": "1901.05517", "submitter": "Roc\\'io D\\'iaz de Le\\'on Torres", "authors": "Roc\\'io D\\'iaz de Le\\'on Torres, Mart\\'in Molina, Pascual Campoy", "title": "Survey of Bayesian Networks Applications to Intelligent Autonomous\n  Vehicles", "comments": "34 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article reviews the applications of Bayesian Networks to Intelligent\nAutonomous Vehicles (IAV) from the decision making point of view, which\nrepresents the final step for fully Autonomous Vehicles (currently under\ndiscussion). Until now, when it comes making high level decisions for\nAutonomous Vehicles (AVs), humans have the last word. Based on the works cited\nin this article and analysis done here, the modules of a general decision\nmaking framework and its variables are inferred. Many efforts have been made in\nthe labs showing Bayesian Networks as a promising computer model for decision\nmaking. Further research should go into the direction of testing Bayesian\nNetwork models in real situations. In addition to the applications, Bayesian\nNetwork fundamentals are introduced as elements to consider when developing\nIAVs with the potential of making high level judgement calls.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 20:32:17 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 21:17:39 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Torres", "Roc\u00edo D\u00edaz de Le\u00f3n", ""], ["Molina", "Mart\u00edn", ""], ["Campoy", "Pascual", ""]]}, {"id": "1901.05520", "submitter": "Marcelo Ponce", "authors": "Ramses van Zon, Marcelo Ponce, Erik Spence, Daniel Gruner", "title": "Trends in Demand, Growth, and Breadth in Scientific Computing Training\n  Delivered by a High-Performance Computing Center", "comments": "Presented at the Fifth Workshop on Best Practices for Enhancing HPC\n  Training and Education (BPHTE18) @ SC18", "journal-ref": "Journal of Computational Science Education Volume 10, Issue 1, pp.\n  53-60 (2019)", "doi": "10.22369/issn.2153-4136/10/1/9", "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the changes in the training and educational efforts of the SciNet\nHPC Consortium, a Canadian academic High Performance Computing center, in the\nareas of Scientific Computing and High-Performance Computing, over the last six\nyears. Initially, SciNet offered isolated training events on how to use HPC\nsystems and write parallel code, but the training program now consists of a\nbroad range of workshops and courses that users can take toward certificates in\nscientific computing, data science, or high-performance computing. Using data\non enrollment, attendence, and certificate numbers from SciNet's education\nwebsite, used by almost 1800 users so far, we extract trends on the growth,\ndemand, and breadth of SciNet's training program. Among the results are a\nsteady overall growth, a sharp and steady increase in the demand for data\nscience training, and a wider participation of 'non-traditional' computing\ndisciplines, which has motivated an increasingly broad spectrum of training\nofferings. Of interest is also that many of the training initiatives have\nevolved into courses that can be taken as part of the graduate curriculum at\nthe University of Toronto.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 20:36:54 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["van Zon", "Ramses", ""], ["Ponce", "Marcelo", ""], ["Spence", "Erik", ""], ["Gruner", "Daniel", ""]]}, {"id": "1901.05670", "submitter": "Oluwaseyi Feyisetan", "authors": "Oluwaseyi Feyisetan and Elena Simperl", "title": "Beyond monetary incentives: experiments in paid microtask contests\n  modelled as continuous-time markov chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to gain a better understanding into how paid microtask\ncrowdsourcing could leverage its appeal and scaling power by using contests to\nboost crowd performance and engagement. We introduce our microtask-based\nannotation platform Wordsmith, which features incentives such as points,\nleaderboards and badges on top of financial remuneration. Our analysis focuses\non a particular type of incentive, contests, as a means to apply crowdsourcing\nin near-real-time scenarios, in which requesters need labels quickly. We model\ncrowdsourcing contests as a continuous-time Markov chain with the objective to\nmaximise the output of the crowd workers, while varying a parameter which\ndetermines whether a worker is eligible for a reward based on their present\nrank on the leaderboard. We conduct empirical experiments in which crowd\nworkers recruited from CrowdFlower carry out annotation microtasks on Wordsmith\n- in our case, to identify named entities in a stream of Twitter posts. In the\nexperimental conditions, we test different reward spreads and record the total\nnumber of annotations received. We compare the results against a control\ncondition in which the same annotation task was completed on CrowdFlower\nwithout a time or contest constraint. The experiments show that rewarding only\nthe best contributors in a live contest could be a viable model to deliver\nresults faster, though quality might suffer for particular types of annotation\ntasks. Increasing the reward spread leads to more work being completed,\nespecially by the top contestants. Overall, the experiments shed light on\npossible design improvements of paid microtasks platforms to boost task\nperformance and speed, and make the overall experience more fair and\ninteresting for crowd workers.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 07:58:07 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Feyisetan", "Oluwaseyi", ""], ["Simperl", "Elena", ""]]}, {"id": "1901.05997", "submitter": "Emiliano De Cristofaro", "authors": "Savvas Zannettou, Tristan Caulfield, Barry Bradlyn, Emiliano De\n  Cristofaro, Gianluca Stringhini, and Jeremy Blackburn", "title": "Characterizing the Use of Images in State-Sponsored Information Warfare\n  Operations by Russian Trolls on Twitter", "comments": "To appear at the 14th International AAAI Conference on Web and Social\n  Media (ICWSM 2020). Please cite accordingly", "journal-ref": "Proceedings of the International AAAI Conference on Web and Social\n  Media, 14(1), 774-785 (2020)", "doi": null, "report-no": null, "categories": "cs.SI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-sponsored organizations are increasingly linked to efforts aimed to\nexploit social media for information warfare and manipulating public opinion.\nTypically, their activities rely on a number of social network accounts they\ncontrol, aka trolls, that post and interact with other users disguised as\n\"regular\" users. These accounts often use images and memes, along with textual\ncontent, in order to increase the engagement and the credibility of their\nposts.\n  In this paper, we present the first study of images shared by state-sponsored\naccounts by analyzing a ground truth dataset of 1.8M images posted to Twitter\nby accounts controlled by the Russian Internet Research Agency. First, we\nanalyze the content of the images as well as their posting activity. Then,\nusing Hawkes Processes, we quantify their influence on popular Web communities\nlike Twitter, Reddit, 4chan's Politically Incorrect board (/pol/), and Gab,\nwith respect to the dissemination of images. We find that the extensive image\nposting activity of Russian trolls coincides with real-world events (e.g., the\nUnite the Right rally in Charlottesville), and shed light on their targets as\nwell as the content disseminated via images. Finally, we show that the trolls\nwere more effective in disseminating politics-related imagery than other\nimages.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 20:15:23 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 10:31:18 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Zannettou", "Savvas", ""], ["Caulfield", "Tristan", ""], ["Bradlyn", "Barry", ""], ["De Cristofaro", "Emiliano", ""], ["Stringhini", "Gianluca", ""], ["Blackburn", "Jeremy", ""]]}, {"id": "1901.06039", "submitter": "Theresa Breiner", "authors": "Theresa Breiner, Chieu Nguyen, Daan van Esch, Jeremy O'Brien", "title": "Automatic Keyboard Layout Design for Low-Resource Latin-Script Languages", "comments": "4 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present our approach to automatically designing and implementing keyboard\nlayouts on mobile devices for typing low-resource languages written in the\nLatin script. For many speakers, one of the barriers in accessing and creating\ntext content on the web is the absence of input tools for their language. Ease\nin typing in these languages would lower technological barriers to online\ncommunication and collaboration, likely leading to the creation of more web\ncontent. Unfortunately, it can be time-consuming to develop layouts manually\neven for language communities that use a keyboard layout very similar to\nEnglish; starting from scratch requires many configuration files to describe\nmultiple possible behaviors for each key. With our approach, we only need a\nsmall amount of data in each language to generate keyboard layouts with very\nlittle human effort. This process can help serve speakers of low-resource\nlanguages in a scalable way, allowing us to develop input tools for more\nlanguages. Having input tools that reflect the linguistic diversity of the\nworld will let as many people as possible use technology to learn, communicate,\nand express themselves in their own native languages.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 00:09:24 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Breiner", "Theresa", ""], ["Nguyen", "Chieu", ""], ["van Esch", "Daan", ""], ["O'Brien", "Jeremy", ""]]}, {"id": "1901.06242", "submitter": "Charith Perera", "authors": "Yuchao Zhou, Suparna De, Gideon Ewa, Charith Perera, Klaus Moessner", "title": "Data-driven Air Quality Characterisation for Urban Environments: a Case\n  Study", "comments": "IEEE ACCESS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The economic and social impact of poor air quality in towns and cities is\nincreasingly being recognised, together with the need for effective ways of\ncreating awareness of real-time air quality levels and their impact on human\nhealth. With local authority maintained monitoring stations being\ngeographically sparse and the resultant datasets also featuring missing labels,\ncomputational data-driven mechanisms are needed to address the data sparsity\nchallenge. In this paper, we propose a machine learning-based method to\naccurately predict the Air Quality Index (AQI), using environmental monitoring\ndata together with meteorological measurements. To do so, we develop an air\nquality estimation framework that implements a neural network that is enhanced\nwith a novel Non-linear Autoregressive neural network with exogenous input\n(NARX), especially designed for time series prediction. The framework is\napplied to a case study featuring different monitoring sites in London, with\ncomparisons against other standard machine-learning based predictive algorithms\nshowing the feasibility and robust performance of the proposed method for\ndifferent kinds of areas within an urban region.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 13:40:03 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Zhou", "Yuchao", ""], ["De", "Suparna", ""], ["Ewa", "Gideon", ""], ["Perera", "Charith", ""], ["Moessner", "Klaus", ""]]}, {"id": "1901.06244", "submitter": "Kevin Vincent", "authors": "Kevin Vincent", "title": "Ethical Implications: The ACM/IEEE-CS Software Engineering Code applied\n  to Tesla's \"Autopilot\" System", "comments": "10 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On October 14, 2015, Tesla Inc. an American electric car company, released\nthe initial version of the Autopilot system. This system promised to provide\nsemi-autonomous driving using the existing hardware already installed on Tesla\nvehicles. On March 23rd, 2018, a Tesla vehicle ran into a divider at highway\nspeed, killing the driver. This occurred under the control of the Autopilot\nsystem with no driver intervention. Critics argue that though Tesla gives\ndrivers warnings in its owner's manual, it is ultimately unethical to release a\nsystem that is marketed as an Autopilot yet still makes grave mistakes that any\nhuman driver would not make. Others defend Tesla by stating that their\nadvisories are suitable and that drivers should ultimately be at fault for any\nmistakes of the Autopilot. This paper will scrutinize the ethical implications\nof Tesla's choice to develop, market, and ship a beta product that requires\nextensive testing. It will further analyze the implications of Tesla's\naggressive advertisement of the product under the name Autopilot along with\nassociated marketing materials. By applying the joint ACM/IEEE-CS Software\nEngineering Code of Ethics, this paper will show that Tesla's choices and\nactions during this event are inconsistent with the code and are unethical\nsince they are responsible for adequately testing and honestly marketing their\nproduct.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 22:08:31 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Vincent", "Kevin", ""]]}, {"id": "1901.06245", "submitter": "Gaurav Varshney", "authors": "Gourang Aggarwal, Vimal Patel, Gaurav Varshney, Kimberly Oostman", "title": "Understanding the Social Factors Affecting the Cryptocurrency Market", "comments": "Presented in ICITST 2018, London", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockchain and its application on cryptocurrency transactions have gathered a\nlot of attention and popularity since the birth of the pioneer Bitcoin in 2009.\nMore than 1500 cryptocurrencies are currently circulated in the market. The\ntechnology underpinning Bitcoin and other cryptocurrencies is Blockchain and is\na rapidly growing decentralized distributed ledger technology which find its\nmajor involvement in cryptocurrencies. But cryptocurrencies are of extremely\nvolatile and fragile nature which makes it difficult to be used as a stable\ncurrency for transactions and devoid this market of human trust. Cryptocurrency\nmarket is controlled by various social and government factors which keeps it\nfluctuating. This paper identifies and discusses the important factors that\ngovern the cryptocurrency market and analyzes the impact of these factors. A\npilot user survey has also been presented at the end of this paper to\nunderstand and demonstrate the societal view of the acceptance of\ncryptocurrencies.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 06:17:38 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Aggarwal", "Gourang", ""], ["Patel", "Vimal", ""], ["Varshney", "Gaurav", ""], ["Oostman", "Kimberly", ""]]}, {"id": "1901.06246", "submitter": "Jasper Snoek", "authors": "D Sculley and Jasper Snoek and Alex Wiltschko", "title": "Avoiding a Tragedy of the Commons in the Peer Review Process", "comments": "Appeared in the 2018 Advances in Neural Information Processing\n  Systems Workshop on Critiquing and Correcting Trends in Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peer review is the foundation of scientific publication, and the task of\nreviewing has long been seen as a cornerstone of professional service. However,\nthe massive growth in the field of machine learning has put this community\nbenefit under stress, threatening both the sustainability of an effective\nreview process and the overall progress of the field. In this position paper,\nwe argue that a tragedy of the commons outcome may be avoided by emphasizing\nthe professional aspects of this service. In particular, we propose a rubric to\nhold reviewers to an objective standard for review quality. In turn, we also\npropose that reviewers be given appropriate incentive. As one possible such\nincentive, we explore the idea of financial compensation on a per-review basis.\nWe suggest reasonable funding models and thoughts on long term effects.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 15:32:29 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Sculley", "D", ""], ["Snoek", "Jasper", ""], ["Wiltschko", "Alex", ""]]}, {"id": "1901.06248", "submitter": "Mojtaba Noghabaei", "authors": "Navid Kayhani, Hosein Taghaddos, Mojtaba Noghabaee, and Ulrich (Rick)\n  Hermann", "title": "Utilization of Virtual Reality Visualizations on Heavy Mobile Crane\n  Planning for Modular Construction", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": "10.22260/ISARC2018/0170", "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many kinds of industrial projects involve the use of prefabricated modules\nbuilt offsite, and installation on-site using mobile cranes. Due to their\ncostly operation and safety concerns, utilization of such heavy lift mobile\ncranes requires a precise heavy lift planning. Traditional heavy lift path\nplanning methods on congested industrial job sites are ineffective,\ntime-consuming and non-precise in many cases, whereas computer-based simulation\nmodels and visualization can be a substantial improving tool. This paper\nprovides a Virtual Reality (VR) environment in which the user can experience\nlifting process in an immerse virtual environment. Providing such a VR model\nnot only facilitates planning for critical lifts (e.g. modules, heavy vessels),\nbut also it provides a training environment to enhance safe climate prior to\nthe actual lift. The developed VR model is implemented successfully on an\nactual construction site of a petrochemical plant on a modular basis in which\nheavy lift mobile cranes are employed.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 21:22:57 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Kayhani", "Navid", "", "Rick"], ["Taghaddos", "Hosein", "", "Rick"], ["Noghabaee", "Mojtaba", "", "Rick"], ["Ulrich", "", "", "Rick"], ["Hermann", "", ""]]}, {"id": "1901.06249", "submitter": "Oluwafunmilola Kesa", "authors": "Oluwafunmilola Kesa, Violette Mahoro", "title": "Rwandacoin: Prospects and challenges of developing a cryptocurrency for\n  transactions in Rwanda", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The use of cryptocurrencies such as Bitcoin and Ethereum in performing online\ntransactions has been on the rise in the world. Africa as a continent is not\nleft out in the adoption of blockchain and cryptocurrencies. This paper\nexplores the prospects and challenges of developing a cryptocurrency in Rwanda\nwhich we denote Rwandacoin. In addition, the paper discusses the potentials of\nRwandacoin easing intercountry trading in East Africa.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 11:32:58 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Kesa", "Oluwafunmilola", ""], ["Mahoro", "Violette", ""]]}, {"id": "1901.06252", "submitter": "Temitayo Fagbola", "authors": "Temitayo Matthew Fagbola, Ibrahim Adepoju Adeyanju, Olatayo Olaniyan,\n  Adebimpe Esan, Bolaji Omodunbi, Ayodele Oloyede and Funmilola Egbetola", "title": "Development of Mobile-Interfaced Machine Learning-Based Predictive\n  Models for Improving Students Performance in Programming Courses", "comments": "11 pages", "journal-ref": "(IJACSA) International Journal of Advanced Computer Science and\n  Applications, Vol. 9, No. 5, 2018", "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Student performance modelling (SPM) is a critical step to assessing and\nimproving students performances in their learning discourse. However, most\nexisting SPM are based on statistical approaches, which on one hand are based\non probability, depicting that results are based on estimation; and on the\nother hand, actual influences of hidden factors that are peculiar to students,\nlecturers, learning environment and the family, together with their overall\neffect on student performance have not been exhaustively investigated. In this\npaper, Student Performance Models (SPM) for improving students performance in\nprogramming courses were developed using M5P Decision Tree (MDT) and Linear\nRegression Classifier (LRC). The data used was gathered using a structured\nquestionnaire from 295 students in 200 and 300 levels of study who offered Web\nprogramming, C or JAVA at Federal University, Oye-Ekiti, Nigeria between 2012\nand 2016. Hidden factors that are significant to students performance in\nprogramming were identified. The relevant data gathered, normalized, coded and\nprepared as variable and factor datasets, and fed into the MDT algorithm and\nLRC to develop the predictive models. The evaluation results obtained indicate\nthat the variable-based LRC produced the best model in terms of MAE, RMSE, RAE\nand the RRSE having yielded the least values in all the evaluations conducted.\nFurther results obtained established the strong significance of attitude of\nstudents and lecturers, fearful perception of students, erratic power supply,\nuniversity facilities, student health and students attendance to the\nperformance of students in programming courses. The variable-based LRC model\npresented in this paper could provide baseline information about students\nperformance thereby offering better decision making towards improving\nteaching/learning outcomes in programming courses.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 12:29:04 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Fagbola", "Temitayo Matthew", ""], ["Adeyanju", "Ibrahim Adepoju", ""], ["Olaniyan", "Olatayo", ""], ["Esan", "Adebimpe", ""], ["Omodunbi", "Bolaji", ""], ["Oloyede", "Ayodele", ""], ["Egbetola", "Funmilola", ""]]}, {"id": "1901.06253", "submitter": "Huimin Lu", "authors": "Huimin Lu, Dong Wang, Yujie Li, Jianru Li, Xin Li, Hyoungseop Kim,\n  Seiichi Serikawa, Iztok Humar", "title": "CONet: A Cognitive Ocean Network", "comments": "Accepted by IEEE Wireless Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The scientific and technological revolution of the Internet of Things has\nbegun in the area of oceanography. Historically, humans have observed the ocean\nfrom an external viewpoint in order to study it. In recent years, however,\nchanges have occurred in the ocean, and laboratories have been built on the\nseafloor. Approximately 70.8% of the Earth's surface is covered by oceans and\nrivers. The Ocean of Things is expected to be important for disaster\nprevention, ocean-resource exploration, and underwater environmental\nmonitoring. Unlike traditional wireless sensor networks, the Ocean Network has\nits own unique features, such as low reliability and narrow bandwidth. These\nfeatures will be great challenges for the Ocean Network. Furthermore, the\nintegration of the Ocean Network with artificial intelligence has become a\ntopic of increasing interest for oceanology researchers. The Cognitive Ocean\nNetwork (CONet) will become the mainstream of future ocean science and\nengineering developments. In this article, we define the CONet. The\ncontributions of the paper are as follows: (1) a CONet architecture is proposed\nand described in detail; (2) important and useful demonstration applications of\nthe CONet are proposed; and (3) future trends in CONet research are presented.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 04:20:55 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Lu", "Huimin", ""], ["Wang", "Dong", ""], ["Li", "Yujie", ""], ["Li", "Jianru", ""], ["Li", "Xin", ""], ["Kim", "Hyoungseop", ""], ["Serikawa", "Seiichi", ""], ["Humar", "Iztok", ""]]}, {"id": "1901.06257", "submitter": "Jun Suzuki", "authors": "Jun Suzuki, Yoshihiko Suhara, Hiroyuki Toda, Kyosuke Nishida", "title": "Personalized Visited-POI Assignment to Individual Raw GPS Trajectories", "comments": "31 pages, 10 figures", "journal-ref": "ACM Transactions on Spatial Algorithms and Systems (TSAS) Volume 5\n  Issue 3, September 2019", "doi": "10.1145/3317667", "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge discovery from GPS trajectory data is an important topic in several\nscientific areas, including data mining, human behavior analysis, and user\nmodeling. This paper proposes a task that assigns personalized visited-POIs.\nIts goal is to estimate fine-grained and pre-defined locations (i.e., points of\ninterest (POI)) that are actually visited by users and assign visited-location\ninformation to the corresponding span of their (personal) GPS trajectories. We\nalso introduce a novel algorithm to solve this assignment task. First, we\nexhaustively extract stay-points as candidates for significant locations using\na variant of a conventional stay-point extraction method. Then we select\nsignificant locations and simultaneously assign visited-POIs to them by\nconsidering various aspects, which we formulate in integer linear programming.\nExperimental results conducted on an actual user dataset show that our method\nachieves higher accuracy in the visited-POI assignment task than the various\ncascaded procedures of conventional methods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 14:25:44 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Suzuki", "Jun", ""], ["Suhara", "Yoshihiko", ""], ["Toda", "Hiroyuki", ""], ["Nishida", "Kyosuke", ""]]}, {"id": "1901.06262", "submitter": "Gerald Schweiger", "authors": "Gerald Schweiger, Claudio Gomes, Georg Engel, Josef-Peter Schoeggl,\n  Alfred Posch, Irene Hafner, Thierry Nouidu", "title": "An Empirical Survey on Co-simulation: Promising Standards, Challenges\n  and Research Needs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-simulation is a promising approach for the modelling and simulation of\ncomplex systems, that makes use of mature simulation tools in the respective\ndomains. It has been applied in wildly different domains, oftentimes without a\ncomprehensive study of the impact to the simulation results. As a consequence,\nover the recent years, researchers have set out to understand the essential\nchallenges arising from the application of this technique. This paper\ncomplements the existing surveys in that the social and empirical aspects were\naddressed. More than 50 experts participated in a two-stage Delphi study to\ndetermine current challenges, research needs and promising standards and tools.\nFurthermore, an analysis of the strengths, weakness, opportunities and threats\nof co-simulation utilizing the analytic hierarchy process resulting in a\nSWOT-AHP analysis is presented. The empirical results of this study show that\nexperts consider the FMI standard to be the most promising standard for\ncontinuous time, discrete event and hybrid co-simulation. The results of the\nSWOT-AHP analysis indicate that factors related to strengths and opportunities\npredominate.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 10:45:18 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Schweiger", "Gerald", ""], ["Gomes", "Claudio", ""], ["Engel", "Georg", ""], ["Schoeggl", "Josef-Peter", ""], ["Posch", "Alfred", ""], ["Hafner", "Irene", ""], ["Nouidu", "Thierry", ""]]}, {"id": "1901.06263", "submitter": "Grigore Stamatescu", "authors": "Grigore Stamatescu and Iulia Stamatescu and Nicoleta Arghira and Ioana\n  Fagarasan", "title": "Data-driven Modelling of Smart Building Ventilation Subsystem", "comments": "11 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Considering the advances in building monitoring and control through networks\nof interconnected devices, effective handling of the associated rich data\nstreams is becoming an important challenge. In many situations the application\nof conventional system identification or approximate grey-box models, partly\ntheoretic and partly data-driven, is either unfeasible or unsuitable. The paper\ndiscusses and illustrates an application of black-box modelling achieved using\ndata mining techniques with the purpose of smart building ventilation subsystem\ncontrol. We present the implementation and evaluation of a data mining\nmethodology on collected data over one year of operation. The case study is\ncarried out on four air handling units of a modern campus building for\npreliminary decision support for facility managers. The data processing and\nlearning framework is based on two steps: raw data streams are compressed using\nthe Symbolic Aggregate Approximation method, followed by the resulting segments\nbeing input into a Support Vector Machine algorithm. The results are useful for\nderiving the behaviour of each equipment in various modi of operation and can\nbe built upon for fault detection or energy efficiency applications. Challenges\nrelated to online operation within a commercial Building Management System are\nalso discussed as the approach shows promise for deployment.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 14:12:42 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Stamatescu", "Grigore", ""], ["Stamatescu", "Iulia", ""], ["Arghira", "Nicoleta", ""], ["Fagarasan", "Ioana", ""]]}, {"id": "1901.06265", "submitter": "Loic Jeanson", "authors": "Michel Cotte, Florent Laroche (LS2N), Matthieu Quantin (LS2N), Loic\n  Jeanson (LS2N), Nicolas Bourgeois", "title": "Analysis of a site's integrity by 3D models and Integrated database,\n  case study : the pic-du-midi high-mountain observatory (France)", "comments": null, "journal-ref": "Scientific Symposium of the 19th General Assembly 2017, Dec 2017,\n  New Dehli, India. 19th General Assembly 2017 - Results of the Scientific\n  Symposium.\n  https://www.icomos.org/fr/77-articles-en-francais/49783-19th-general-assembly-2017-results-of-the-scientific-symposium", "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of \"integrity\", as currently used in the analysis of World\nHeritage sites or cultural landscapes mainly consists of 1) the composition of\na given site, regarding its origins and its current state of conservation; 2)\nthe visual and functional relationships between its components (attributes).\nOne of the major questions is \"what defines the origin period?\". The integrity\nanalysis has to clearly understand and evaluate which tangible components\nexist, in order to correctly identify their origin period, estimate how much of\nthe original structure and function remain. An additional difficulty rises in\ncase of scientific and/or technical heritage assessment: the very historical\nnecessity of updating and implementing technical/scientific innovations. These\nfollow advances in science and/or technique(s), and lead to frequent successive\nchanges, impacting the site's structure. Hence, for living sites, one cannot\nsee the origin of the project as the only reference state, as it could be for\nclassical heritage; it is instead requisite to enlighten a chronological series\nof major reference states. The Pic-du-Midi Observatory (France) is a remarkable\nexample as high-mountain scientific station (2860 m). Its various and\nsuccessive scientific uses led to a series of reshaping and structural\nevolutions over around 150 years, till nowadays. We have there a series of\nreference states very conform to an active scientific station and observatory.\nThe project aims to combine historical data and measurements on the current\nsite's state, to build a succession of 3D models for a series of 5 or 6\n'reference state' of the observatory within associated database, and to\ndocument the integrity analysis more accurately than usual. The final uses of\nthe digital achievement will also serve for the management of the site and for\nfurther development of\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 11:22:26 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Cotte", "Michel", "", "LS2N"], ["Laroche", "Florent", "", "LS2N"], ["Quantin", "Matthieu", "", "LS2N"], ["Jeanson", "Loic", "", "LS2N"], ["Bourgeois", "Nicolas", ""]]}, {"id": "1901.06270", "submitter": "Vatsala Nundloll", "authors": "Vatsala Nundloll, Barry Porter, Gordon Blair, Jack Cosby, Bridget\n  Emmett, Ben Winterbourn, Graham Dean, Philip Beattie, Rory Shaw, Davey Jones,\n  Dave Chadwick, Mike Brown, Wayne Shelley, Izhar Ullah", "title": "The Design and Deployment of an End-to-end IoT Infrastructure for the\n  Natural Environment", "comments": null, "journal-ref": null, "doi": "10.3390/fi11060129", "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Internet of Things (IoT) systems have seen recent growth in popularity for\ncity and home environments. We report on the design, deployment and use of IoT\ninfrastructure for environmental monitoring and management. Working closely\nwith hydrologists, soil scientists and animal behaviour scientists, we\nsuccessfully deployed and utilised a system to deliver integrated information\nacross these two fields in the first such example of real-time\nmulti-dimensional environmental science. We describe the design of this system,\nits requirements and operational effectiveness for hydrological, soil and\nethological scientists, and our experiences from building, maintaining and\nusing the deployment at a remote site in difficult conditions. Based on this\nexperience we discuss key future work for the IoT community when working in\nthese kinds of environmental deployments.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 10:34:08 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Nundloll", "Vatsala", ""], ["Porter", "Barry", ""], ["Blair", "Gordon", ""], ["Cosby", "Jack", ""], ["Emmett", "Bridget", ""], ["Winterbourn", "Ben", ""], ["Dean", "Graham", ""], ["Beattie", "Philip", ""], ["Shaw", "Rory", ""], ["Jones", "Davey", ""], ["Chadwick", "Dave", ""], ["Brown", "Mike", ""], ["Shelley", "Wayne", ""], ["Ullah", "Izhar", ""]]}, {"id": "1901.06291", "submitter": "Eda Okur", "authors": "Eda Okur, Nese Alyuz, Sinem Aslan, Utku Genc, Cagri Tanriover, Asli\n  Arslan Esme", "title": "Detecting Behavioral Engagement of Students in the Wild Based on\n  Contextual and Visual Data", "comments": "12th Women in Machine Learning Workshop (WiML 2017), co-located with\n  the 31st Conference on Neural Information Processing Systems (NeurIPS 2017),\n  Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To investigate the detection of students' behavioral engagement (On-Task vs.\nOff-Task), we propose a two-phase approach in this study. In Phase 1,\ncontextual logs (URLs) are utilized to assess active usage of the content\nplatform. If there is active use, the appearance information is utilized in\nPhase 2 to infer behavioral engagement. Incorporating the contextual\ninformation improved the overall F1-scores from 0.77 to 0.82. Our\ncross-classroom and cross-platform experiments showed the proposed generic and\nmulti-modal behavioral engagement models' applicability to a different set of\nstudents or different subject areas.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 06:45:56 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Okur", "Eda", ""], ["Alyuz", "Nese", ""], ["Aslan", "Sinem", ""], ["Genc", "Utku", ""], ["Tanriover", "Cagri", ""], ["Esme", "Asli Arslan", ""]]}, {"id": "1901.06327", "submitter": "Mahmood Rashid Ph.D.", "authors": "Mahmood A. Rashid, Krishneel Deo, Divnesh Prasad, Kunal Singh, Sarvesh\n  Chand, Mansour Assaf", "title": "TEduChain: A Platform for Crowdsourcing Tertiary Education Fund using\n  Blockchain Technology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Blockchain is an emerging technology framework for creating and storing\ntransaction in distributed ledgers with a high degree of security and\nreliability. In this paper we present a blockchain-based platform to create and\nstore contracts in between students and their higher education sponsors. The\nsponsorship might be in any form, such as scholarship, donation or loan. The\nfund will be arranged and managed by a group of competitive agents\n(Fundraisers) who will hold the distributed ledgers and act as miners in the\nblockchain network.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 16:29:04 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 21:20:27 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Rashid", "Mahmood A.", ""], ["Deo", "Krishneel", ""], ["Prasad", "Divnesh", ""], ["Singh", "Kunal", ""], ["Chand", "Sarvesh", ""], ["Assaf", "Mansour", ""]]}, {"id": "1901.06483", "submitter": "Vivek Kumar Mr.", "authors": "Vivek Kumar, Manuel Mazzara, Maj. Gen. (Rtd.) Angelo Messina, JooYoung\n  Lee", "title": "A Conjoint Application of Data Mining Techniques for Analysis of Global\n  Terrorist Attacks -- Prevention and Prediction for Combating Terrorism", "comments": "13 pages, 5 Figures, 7 Tables, Proceedings of 6th International\n  Conference in Software Engineering for Defense Applications- SEDA 2018,Rome,\n  Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Terrorism has become one of the most tedious problems to deal with and a\nprominent threat to mankind. To enhance counter-terrorism, several research\nworks are developing efficient and precise systems, data mining is not an\nexception. Immense data is floating in our lives, though the scarce\navailability of authentic terrorist attack data in the public domain makes it\ncomplicated to fight terrorism. This manuscript focuses on data mining\nclassification techniques and discusses the role of United Nations in\ncounter-terrorism. It analyzes the performance of classifiers such as Lazy\nTree, Multilayer Perceptron, Multiclass and Na\\\"ive Bayes classifiers for\nobserving the trends for terrorist attacks around the world. The database for\nexperiment purpose is created from different public and open access sources for\nyears 1970-2015 comprising of 156,772 reported attacks causing massive losses\nof lives and property. This work enumerates the losses occurred, trends in\nattack frequency and places more prone to it, by considering the attack\nresponsibilities taken as evaluation class.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 08:37:56 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 18:59:43 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2019 10:08:52 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Kumar", "Vivek", "", "Rtd."], ["Mazzara", "Manuel", "", "Rtd."], ["Gen.", "Maj.", "", "Rtd."], ["Messina", "Angelo", ""], ["Lee", "JooYoung", ""]]}, {"id": "1901.06614", "submitter": "Qingkai Kong", "authors": "Qingkai Kong, Qin Lv, Richard M. Allen", "title": "Earthquake Early Warning and Beyond: Systems Challenges in\n  Smartphone-based Seismic Network", "comments": "6 pages, conference paper, already accepted at hotmobile 2019", "journal-ref": null, "doi": "10.1145/3301293.3302377", "report-no": null, "categories": "cs.SY cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Earthquake Early Warning (EEW) systems can effectively reduce fatalities,\ninjuries, and damages caused by earthquakes. Current EEW systems are mostly\nbased on traditional seismic and geodetic networks, and exist only in a few\ncountries due to the high cost of installing and maintaining such systems. The\nMyShake system takes a different approach and turns people's smartphones into\nportable seismic sensors to detect earthquake-like motions. However, to issue\nEEW messages with high accuracy and low latency in the real world, we need to\naddress a number of challenges related to mobile computing. In this paper, we\nfirst summarize our experience building and deploying the MyShake system, then\nfocus on two key challenges for smartphone-based EEW (sensing heterogeneity and\nuser/system dynamics) and some preliminary exploration. We also discuss other\nchallenges and new research directions associated with smartphone-based seismic\nnetwork.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2019 02:32:52 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Kong", "Qingkai", ""], ["Lv", "Qin", ""], ["Allen", "Richard M.", ""]]}, {"id": "1901.06867", "submitter": "Benjamin Finley", "authors": "Kalevi Kilkki and Benjamin Finley", "title": "In Search of Lost QoS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The area of quality of service (QoS) in communications networks has been the\ntarget of research for already several decades with tens of thousands of\npublished journal and conference papers. However, the practical introduction of\nQoS systems in commercial networks has been limited (with a preference for\nsimple overprovisioning). Despite this dissonance, most influential QoS papers\ndo not discuss this lack of penetration or challenge any of the common\nassumptions used to argue for QoS systems. So far, the few critical QoS papers\nhave had only a minor effect on QoS research and standardization. Therefore,\nthere is a serious risk that QoS will remain an academic research topic without\nsignificant practical relevance. To help elucidate these issues, in this work,\nwe first perform a comprehensive review of QoS including a general overview and\nan analysis of both influential and critical work from the past 30 years. We\nexamine properties such as citations, keywords, and author traits to show that\nQoS has passed through several distinct phases with different topics while\nmaintaining the overall attitude towards the role and objective of QoS systems.\nWe then discuss QoS as a social phenomenon and in the context of current\nnetworking standards. Finally, we propose a QoS scheme based on incentives that\navoids some of the problems identified in critical work, and we provide simple\nrecommendations for network operators. Overall, we hope to spark the community\nto take a fresh look at QoS.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 10:43:23 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Kilkki", "Kalevi", ""], ["Finley", "Benjamin", ""]]}, {"id": "1901.06922", "submitter": "Fabrizio Caruso", "authors": "Stefano Bodrato, Fabrizio Caruso, Giovanni A. Cignoni", "title": "Discovering Eastern European PCs by hacking them. Today", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer science would not be the same without personal computers. In the\nWest the so called PC revolution started in the late '70s and has its roots in\nhobbyists and do-it-yourself clubs. In the following years the diffusion of\nhome and personal computers has made the discipline closer to many people. A\nbit later, to a lesser extent, yet in a similar way, the revolution took place\nalso in East European countries. Today, the scenario of personal computing has\ncompletely changed, however the computers of the '80s are still objects of\nfascination for a number of retrocomputing fans who enjoy using, programming\nand hacking the old 8-bits. The paper highlights the continuity between\nyesterday's hobbyists and today's retrocomputing enthusiasts, particularly\nfocusing on East European PCs. Besides the preservation of old hardware and\nsoftware, the community is engaged in the development of emulators and cross\ncompilers. Such tools can be used for historical investigation, for example to\ntrace the origins of the BASIC interpreters loaded in the ROMs of East European\nPCs.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 13:19:49 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Bodrato", "Stefano", ""], ["Caruso", "Fabrizio", ""], ["Cignoni", "Giovanni A.", ""]]}, {"id": "1901.06991", "submitter": "Ovidiu Banias", "authors": "Ovidiu Banias, Camil Octavian Milincu", "title": "Hybrid Design Tools - Image Quality Assessment of a Digitally Augmented\n  Blackboard Integrated System", "comments": null, "journal-ref": "Informatics 2019, 6(1), 5;\n  https://doi.org/10.3390/informatics6010006", "doi": "10.3390/informatics6010006", "report-no": null, "categories": "cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two decades, Interactive White Boards (IWBs) have been widely\navailable as a pedagogic tool. The usability of these boards for teaching\ndisciplines where complex drawings are needed, we consider debatable in\nmultiple regards. In a previous study, we proposed an alternative to the IWBs\nas a blackboard augmented with a minimum of necessary digital elements. The\ncurrent study continues our previous research on hybrid design tools, analyzing\nthe limitations of the developed hybrid system regarding the perceived quality\nof the images being repeatedly captured, annotated, and reprojected onto the\nboard. We validated the hybrid system by evaluating the quality of the\nprojected and reprojected images over a blackboard, using both objective\nmeasurements and subjective human perception in extensive and realistic case\nstudies. Based on the results achieved in the current research, we conclude\nthat the proposed hybrid system provides good quality support for teaching\ndisciplines that require complex drawings and board interaction.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 16:28:41 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Banias", "Ovidiu", ""], ["Milincu", "Camil Octavian", ""]]}, {"id": "1901.07023", "submitter": "Michael Garvie", "authors": "Michael Garvie and Phil Husbands", "title": "Automatic Synthesis of Totally Self-Checking Circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Totally self-checking (TSC) circuits are synthesised with a grid of computers\nrunning a distributed population based stochastic optimisation algorithm. The\npresented method is the first to automatically synthesise TSC circuits from\narbitrary logic as all previous methods fail to guarantee the checker is\nself-testing (ST) for circuits with limited output codespaces. The circuits\nsynthesised by the presented method have significantly lower overhead than the\npreviously reported best for every one of a set of 11 frequently used\nbenchmarks. Average overhead across the entire set is 23% of duplication and\ncomparison overhead, compared with an average of 69% for the previous best\nreported values across the set. The methodology presented represents a\nbreakthrough in concurrent error detection (CED). The highly efficient, novel\ndesigns produced are tailored to each circuit's function, rather than being\nconstrained by a particular modular CED design methodology. Results are\nsynthesised using two-input gates and are TSC with respect to all gate input\nand output stuck-at faults. The method can be used to add CED with or without\nmodifications to the original logic, and can be generalised to any\nimplementation technology and fault model. An example circuit is analysed and\nrigorously proven to be TSC.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 18:26:46 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Garvie", "Michael", ""], ["Husbands", "Phil", ""]]}, {"id": "1901.07046", "submitter": "Kostantinos Papadamou Mr", "authors": "Kostantinos Papadamou, Antonis Papasavva, Savvas Zannettou, Jeremy\n  Blackburn, Nicolas Kourtellis, Ilias Leontiadis, Gianluca Stringhini, Michael\n  Sirivianos", "title": "Disturbed YouTube for Kids: Characterizing and Detecting Inappropriate\n  Videos Targeting Young Children", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of the most-subscribed YouTube channels target children of\nvery young age. Hundreds of toddler-oriented channels on YouTube feature\ninoffensive, well produced, and educational videos. Unfortunately,\ninappropriate content that targets this demographic is also common. YouTube's\nalgorithmic recommendation system regrettably suggests inappropriate content\nbecause some of it mimics or is derived from otherwise appropriate content.\nConsidering the risk for early childhood development, and an increasing trend\nin toddler's consumption of YouTube media, this is a worrisome problem.\n  In this work, we build a classifier able to discern inappropriate content\nthat targets toddlers on YouTube with 84.3% accuracy, and leverage it to\nperform a first-of-its-kind, large-scale, quantitative characterization that\nreveals some of the risks of YouTube media consumption by young children. Our\nanalysis reveals that YouTube is still plagued by such disturbing videos and\nits currently deployed counter-measures are ineffective in terms of detecting\nthem in a timely manner. Alarmingly, using our classifier we show that young\nchildren are not only able, but likely to encounter disturbing videos when they\nrandomly browse the platform starting from benign videos.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 19:21:19 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 22:06:24 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Papadamou", "Kostantinos", ""], ["Papasavva", "Antonis", ""], ["Zannettou", "Savvas", ""], ["Blackburn", "Jeremy", ""], ["Kourtellis", "Nicolas", ""], ["Leontiadis", "Ilias", ""], ["Stringhini", "Gianluca", ""], ["Sirivianos", "Michael", ""]]}, {"id": "1901.07067", "submitter": "Solomia Fedushko", "authors": "S. S. Fedushko", "title": "Development of verification system of socio-demographic data of virtual\n  community member", "comments": "in Ukrainian", "journal-ref": "Radio Electronics, Computer Science, Control. 2016", "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The important task of developing verification system of data of virtual\ncommunity member on the basis of computer-linguistic analysis of the content of\na large sample of Ukrainian virtual communities is solved. The subject of\nresearch is methods and tools for verification of web-members socio-demographic\ncharacteristics based on computer-linguistic analysis of their communicative\ninteraction results. The aim of paper is to verifying web-user personal data on\nthe basis of computer-linguistic analysis of web-members information tracks.\nThe structure of verification software for web-user profile is designed for a\npractical implementation of assigned tasks. The method of personal data\nverification of web-members by analyzing information track of virtual community\nmember is conducted. For the first time the method for checking the\nauthenticity of web members personal data, which helped to design of\nverification tool for socio-demographic characteristics of web-member is\ndeveloped. The verification system of data of web-members, which forms the\nverified socio-demographic profiles of web-members, is developed as a result of\nconducted experiments. Also the user interface of the developed verification\nsystem web-members data is presented. Effectiveness and efficiency of use of\nthe developed methods and means for solving tasks in web-communities\nadministration is proved by their approbation. The number of false results of\nverification system is 18%.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 20:21:05 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Fedushko", "S. S.", ""]]}, {"id": "1901.07072", "submitter": "Solomia Fedushko", "authors": "S.S. Fedushko, Yu.R. Bekesh", "title": "Positioning services of a travel agency in social networks", "comments": "in Ukrainian", "journal-ref": "Scientific works of DonNTU. Series: Informatics, Cybernetics and\n  Computing, 2014", "doi": null, "report-no": null, "categories": "cs.SI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the methods of forming a travel company customer base by means\nof social networks are observed. These methods are made to involve web-users of\nthe social networks (VK.com and Facebook) for positioning of the service of the\ntravel agency \"New Europe\" on the Internet. The methods of applying the\nmaintenance activities and interests of web-users are also used. So, the main\nmethod of information exchanging in modern network society is on-line social\nnetworks. The rapid development and improvement of such information and\ncommunication technologies is a key factor in the positioning of the travel\nagency brand in the global information space. The absence of time and space\nrestrictions and the speed of spreading of the information among an aim\naudience of social networks create all the conditions for effective\npopularization of the travel agency \"New Europe\" and its service in the\nInternet.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 20:40:05 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Fedushko", "S. S.", ""], ["Bekesh", "Yu. R.", ""]]}, {"id": "1901.07290", "submitter": "Suzan Verberne", "authors": "Peter Burger, Soeradj Kanhai, Alexander Pleijter, Suzan Verberne", "title": "The reach of commercially motivated junk news on Facebook", "comments": "18 pages, 4 figures, submitted pre-print", "journal-ref": "PLoS ONE 2019, 14(8): e0220446", "doi": "10.1371/journal.pone.0220446", "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commercially motivated junk news -- i.e. money-driven, highly shareable\nclickbait with low journalistic production standards -- constitutes a vast and\nlargely unexplored news media ecosystem. Using publicly available Facebook\ndata, we compared the reach of junk news on Facebook pages in the Netherlands\nto the reach of Dutch mainstream news on Facebook. During the period 2013-2017\nthe total number of user interactions with junk news significantly exceeded\nthat with mainstream news. Over 5 Million of the 10 Million Dutch Facebook\nusers have interacted with a junk news post at least once. Junk news Facebook\npages also had a significantly stronger increase in the number of user\ninteractions over time than mainstream news. Since the beginning of 2016 the\naverage number of user interactions per junk news post has consistently\nexceeded the average number of user interactions per mainstream news post.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 13:21:52 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 08:11:53 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Burger", "Peter", ""], ["Kanhai", "Soeradj", ""], ["Pleijter", "Alexander", ""], ["Verberne", "Suzan", ""]]}, {"id": "1901.07308", "submitter": "Herve Suaudeau", "authors": "Herv\\'e Suaudeau (UMR 8119)", "title": "Contribution of Herv{\\'e} Suaudeau for the mission of the French Senate\n  Law Commission on electronic voting on 25 January 2018", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This contribution investigate the compatibility between the use of\ndematerialised voting and Article 3 of the french Constitution, which\nstipulates that 'voting must always be universal, equal and secret'. The IT\nrisk management of electronic voting is managed in a rational way like any\nother risk management areas. We are therefore prepared to accept the use of a\nnon dematerialised technique to cover a significant, well-identified and\nunavoidable risk. However, research has shown that a verifiable dematerialized\nvote intrinsically loses its anonymity. In addition, a study of the history of\nvulnerabilities shows that no terminal in recent years has been able to\nguarantee the anonymity of its connection and that personal voting data has\nalways been potentially exposed. Electronic voting systems are therefore, to\nthe best of our knowledge, unable to provide the required constitutional\nguarantee.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 09:25:55 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 16:24:59 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Suaudeau", "Herv\u00e9", "", "UMR 8119"]]}, {"id": "1901.07333", "submitter": "Jun Hao", "authors": "Jun Hao", "title": "Multi-agent Reinforcement Learning Embedded Game for the Optimization of\n  Building Energy Control and Power System Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.GT cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the current game-theoretic demand-side management methods focus\nprimarily on the scheduling of home appliances, and the related numerical\nexperiments are analyzed under various scenarios to achieve the corresponding\nNash-equilibrium (NE) and optimal results. However, not much work is conducted\nfor academic or commercial buildings. The methods for optimizing\nacademic-buildings are distinct from the optimal methods for home appliances.\nIn my study, we address a novel methodology to control the operation of\nheating, ventilation, and air conditioning system (HVAC). With the development\nof Artificial Intelligence and computer technologies, reinforcement learning\n(RL) can be implemented in multiple realistic scenarios and help people to\nsolve thousands of real-world problems. Reinforcement Learning, which is\nconsidered as the art of future AI, builds the bridge between agents and\nenvironments through Markov Decision Chain or Neural Network and has seldom\nbeen used in power system. The art of RL is that once the simulator for a\nspecific environment is built, the algorithm can keep learning from the\nenvironment. Therefore, RL is capable of dealing with constantly changing\nsimulator inputs such as power demand, the condition of power system and\noutdoor temperature, etc. Compared with the existing distribution power system\nplanning mechanisms and the related game theoretical methodologies, our\nproposed algorithm can plan and optimize the hourly energy usage, and have the\nability to corporate with even shorter time window if needed.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 08:37:38 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Hao", "Jun", ""]]}, {"id": "1901.07584", "submitter": "Fisnik Dalipi", "authors": "Salah Uddin Ahmed, Steinar Aasnass, Fisnik Dalipi, and Knut Hesten", "title": "Analytics-Driven Digital Platform for Regional Growth and Development: A\n  Case Study from Norway", "comments": "The Thirteenth International Conference on Digital Society and\n  eGovernments (ICDS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the growth barometer (Vekstbarometer in Norwegian),\nwhich is a digital platform that provides the development trends in the\nregional context in a visual and user-friendly way. The platform is developed\nto use open data from different sources that is presented mainly in five main\ngroups: goals, premises or prerequisites for growth, industries, growth, and\nexpectations. Furthermore, it also helps to improve decision-making and\ntransparency, as well as provide new knowledge for research and society. The\nplatform uses sensitive and non-sensitive open data. In contrast to other\nsimilar digital platforms from Norway, where the data is presented as raw data\nor with basic level of presentations, our platform is advantageous since it\nprovides a range of options for visualization that makes the statistics more\ncomprehensive.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 19:03:54 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 14:17:06 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Ahmed", "Salah Uddin", ""], ["Aasnass", "Steinar", ""], ["Dalipi", "Fisnik", ""], ["Hesten", "Knut", ""]]}, {"id": "1901.07694", "submitter": "Jonathan Dodge", "authors": "Jonathan Dodge, Q. Vera Liao, Yunfeng Zhang, Rachel K. E. Bellamy and\n  Casey Dugan", "title": "Explaining Models: An Empirical Study of How Explanations Impact\n  Fairness Judgment", "comments": null, "journal-ref": null, "doi": "10.1145/3301275.3302310", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring fairness of machine learning systems is a human-in-the-loop process.\nIt relies on developers, users, and the general public to identify fairness\nproblems and make improvements. To facilitate the process we need effective,\nunbiased, and user-friendly explanations that people can confidently rely on.\nTowards that end, we conducted an empirical study with four types of\nprogrammatically generated explanations to understand how they impact people's\nfairness judgments of ML systems. With an experiment involving more than 160\nMechanical Turk workers, we show that: 1) Certain explanations are considered\ninherently less fair, while others can enhance people's confidence in the\nfairness of the algorithm; 2) Different fairness problems--such as model-wide\nfairness issues versus case-specific fairness discrepancies--may be more\neffectively exposed through different styles of explanation; 3) Individual\ndifferences, including prior positions and judgment criteria of algorithmic\nfairness, impact how people react to different styles of explanation. We\nconclude with a discussion on providing personalized and adaptive explanations\nto support fairness judgments of ML systems.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 02:29:28 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Dodge", "Jonathan", ""], ["Liao", "Q. Vera", ""], ["Zhang", "Yunfeng", ""], ["Bellamy", "Rachel K. E.", ""], ["Dugan", "Casey", ""]]}, {"id": "1901.07876", "submitter": "Kyriaki Kalimeri", "authors": "Kyriaki Kalimeri and Mariano G. Beiro and Andrea Bonanomi and\n  Alessandro Rosina and Ciro Cattuto", "title": "Evaluation of Biases in Self-reported Demographic and Psychometric\n  Information: Traditional versus Facebook-based Surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media in scientific research offer a unique digital observatory of\nhuman behaviours and hence great opportunities to conduct research at large\nscale answering complex sociodemographic questions. We focus on the\nidentification and assessment of biases in social media administered surveys.\nThis study aims to shed light on population, self-selection and behavioural\nbiases, empirically comparing the consistency between self-reported information\ncollected traditionally versus social media administered questionnaires,\nincluding demographic and psychometric attributes. We engaged a demographically\nrepresentative cohort of young adults in Italy (approximately 4,000\nparticipants) in taking a traditionally administered online survey and then,\nafter one year, we invited them to use our ad hoc Facebook application (988\naccepted) where they filled in part of the initial survey. We assess the\nstatistically significant differences indicating population, self-selection,\nand behavioural biases due to the different context in which the questionnaire\nis administered. Our findings suggest that surveys administered on Facebook do\nnot exhibit major biases with respect to traditionally administered surveys\nneither in terms of demographics, nor personality traits. Loyalty, authority,\nand social binding values were higher in the Facebook platform, probably due to\nthe platform's intrinsic social character. We conclude, that Facebook apps are\nvalid research tools for administering demographic and psychometric surveys\nprovided that the entailed biases are taken into consideration. We contribute\nto the characterisation of Facebook apps as a valid scientific tool to\nadminister demographic and psychometric surveys, and to the assessment of\npopulation, self-selection, and behavioural biases in the collected data.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 13:41:02 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Kalimeri", "Kyriaki", ""], ["Beiro", "Mariano G.", ""], ["Bonanomi", "Andrea", ""], ["Rosina", "Alessandro", ""], ["Cattuto", "Ciro", ""]]}, {"id": "1901.07999", "submitter": "Marc Miquel-Ribe", "authors": "Marc Miquel-Rib\\'e, David Laniado", "title": "Wikipedia Cultural Diversity Dataset: A Complete Cartography for 300\n  Language Editions", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the Wikipedia Cultural Diversity dataset. For each\nexisting Wikipedia language edition, the dataset contains a classification of\nthe articles that represent its associated cultural context, i.e. all concepts\nand entities related to the language and to the territories where it is spoken.\nWe describe the methodology we employed to classify articles, and the rich set\nof features that we defined to feed the classifier, and that are released as\npart of the dataset. We present several purposes for which we envision the use\nof this dataset, including detecting, measuring and countering content gaps in\nthe Wikipedia project, and encouraging cross-cultural research in the field of\ndigital humanities.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 16:54:38 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 11:15:51 GMT"}, {"version": "v3", "created": "Fri, 7 Jun 2019 23:46:21 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Miquel-Rib\u00e9", "Marc", ""], ["Laniado", "David", ""]]}, {"id": "1901.08317", "submitter": "Leslie Solorzano", "authors": "Leslie Solorzano, Gabriela M. Almeida, B\\'arbara Mesquita, Diana\n  Martins, Carla Oliveira, Carolina W\\\"ahlby", "title": "Whole slide image registration for the study of tumor heterogeneity", "comments": "MICCAI2018 - Computational Pathology and Ophthalmic Medical Image\n  Analysis - COMPAY", "journal-ref": "vol 11039, 2018, p95-102", "doi": "10.1007/978-3-030-00949-6_12", "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consecutive thin sections of tissue samples make it possible to study local\nvariation in e.g. protein expression and tumor heterogeneity by staining for a\nnew protein in each section. In order to compare and correlate patterns of\ndifferent proteins, the images have to be registered with high accuracy. The\nproblem we want to solve is registration of gigapixel whole slide images (WSI).\nThis presents 3 challenges: (i) Images are very large; (ii) Thin sections\nresult in artifacts that make global affine registration prone to very large\nlocal errors; (iii) Local affine registration is required to preserve correct\ntissue morphology (local size, shape and texture). In our approach we compare\nWSI registration based on automatic and manual feature selection on either the\nfull image or natural sub-regions (as opposed to square tiles). Working with\nnatural sub-regions, in an interactive tool makes it possible to exclude\nregions containing scientifically irrelevant information. We also present a new\nway to visualize local registration quality by a Registration Confidence Map\n(RCM). With this method, intra-tumor heterogeneity and charateristics of the\ntumor microenvironment can be observed and quantified.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 10:02:18 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Solorzano", "Leslie", ""], ["Almeida", "Gabriela M.", ""], ["Mesquita", "B\u00e1rbara", ""], ["Martins", "Diana", ""], ["Oliveira", "Carla", ""], ["W\u00e4hlby", "Carolina", ""]]}, {"id": "1901.08329", "submitter": "Yuanwei Liu", "authors": "Yuanwei Liu, Suzhi Bi, Zhiyuan Shi, and Lajos Hanzo", "title": "When Machine Learning Meets Big Data: A Wireless Communication\n  Perspective", "comments": "This article has been accepted by IEEE Vehicular Technology Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have witnessed an exponential growth in commercial data services, which\nhas lead to the 'big data era'. Machine learning, as one of the most promising\nartificial intelligence tools of analyzing the deluge of data, has been invoked\nin many research areas both in academia and industry. The aim of this article\nis twin-fold. Firstly, we briefly review big data analysis and machine\nlearning, along with their potential applications in next-generation wireless\nnetworks. The second goal is to invoke big data analysis to predict the\nrequirements of mobile users and to exploit it for improving the performance of\n\"social network-aware wireless\". More particularly, a unified big data aided\nmachine learning framework is proposed, which consists of feature extraction,\ndata modeling and prediction/online refinement. The main benefits of the\nproposed framework are that by relying on big data which reflects both the\nspectral and other challenging requirements of the users, we can refine the\nmotivation, problem formulations and methodology of powerful machine learning\nalgorithms in the context of wireless networks. In order to characterize the\nefficiency of the proposed framework, a pair of intelligent practical\napplications are provided as case studies: 1) To predict the positioning of\ndrone-mounted areal base stations (BSs) according to the specific tele-traffic\nrequirements by gleaning valuable data from social networks. 2) To predict the\ncontent caching requirements of BSs according to the users' preferences by\nmining data from social networks. Finally, open research opportunities are\nidentified for motivating future investigations.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 10:20:48 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 18:11:28 GMT"}], "update_date": "2019-12-16", "authors_parsed": [["Liu", "Yuanwei", ""], ["Bi", "Suzhi", ""], ["Shi", "Zhiyuan", ""], ["Hanzo", "Lajos", ""]]}, {"id": "1901.08579", "submitter": "Ross Gruetzemacher", "authors": "Ross Gruetzemacher, David Paradice, Kang Bok Lee", "title": "Forecasting Transformative AI: An Expert Survey", "comments": "11 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformative AI technologies have the potential to reshape critical aspects\nof society in the near future. However, in order to properly prepare policy\ninitiatives for the arrival of such technologies accurate forecasts and\ntimelines are necessary. A survey was administered to attendees of three AI\nconferences during the summer of 2018 (ICML, IJCAI and the HLAI conference).\nThe survey included questions for estimating AI capabilities over the next\ndecade, questions for forecasting five scenarios of transformative AI and\nquestions concerning the impact of computational resources in AI research.\nRespondents indicated a median of 21.5% of human tasks (i.e., all tasks that\nhumans are currently paid to do) can be feasibly automated now, and that this\nfigure would rise to 40% in 5 years and 60% in 10 years. Median forecasts\nindicated a 50% probability of AI systems being capable of automating 90% of\ncurrent human tasks in 25 years and 99% of current human tasks in 50 years. The\nconference of attendance was found to have a statistically significant impact\non all forecasts, with attendees of HLAI providing more optimistic timelines\nwith less uncertainty. These findings suggest that AI experts expect major\nadvances in AI technology to continue over the next decade to a degree that\nwill likely have profound transformative impacts on society.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 18:53:07 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 16:23:00 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Gruetzemacher", "Ross", ""], ["Paradice", "David", ""], ["Lee", "Kang Bok", ""]]}, {"id": "1901.08744", "submitter": "Venkatesh Umaashankar Mr", "authors": "Venkatesh Umaashankar and Girish Shanmugam S", "title": "Ask less - Scale Market Research without Annoying Your Customers", "comments": "Accepted and presented in aisgsc2019\n  (http://www.psgtech.edu/aisgsc2019/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Market research is generally performed by surveying a representative sample\nof customers with questions that includes contexts such as psycho-graphics,\ndemographics, attitude and product preferences. Survey responses are used to\nsegment the customers into various groups that are useful for targeted\nmarketing and communication. Reducing the number of questions asked to the\ncustomer has utility for businesses to scale the market research to a large\nnumber of customers. In this work, we model this task using Bayesian networks.\nWe demonstrate the effectiveness of our approach using an example market\nsegmentation of broadband customers.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 05:43:06 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Umaashankar", "Venkatesh", ""], ["S", "Girish Shanmugam", ""]]}, {"id": "1901.08873", "submitter": "Neil F. Johnson", "authors": "N.F. Johnson, F.J. Gomez-Ruiz, F.J. Rodriguez, L. Quiroga", "title": "Quantum Terrorism: Collective Vulnerability of Global Quantum Systems", "comments": "Working paper. Comments welcome to neiljohnson@gwu.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CY physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The major imminent investments in quantum technologies will bring concepts\nlike a global quantum Internet and quantum Internet-of-Things, closer to\nreality. Our findings reveal a new form of vulnerability that will enable\nhostile groups of quantum-enabled adversaries to inflict maximal disruption on\nthe global quantum state in such systems. These attacks will be practically\nimpossible to detect since they introduce no change in the Hamiltonian and no\nloss of purity; they require no real-time communication; and they can be over\nwithin a second. We also predict that such attacks will be amplified by the\nstatistical character of modern extremist, insurgent and terrorist groups. A\ncountermeasure could be to embed future quantum technologies within redundant\nclassical networks.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 13:51:05 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Johnson", "N. F.", ""], ["Gomez-Ruiz", "F. J.", ""], ["Rodriguez", "F. J.", ""], ["Quiroga", "L.", ""]]}, {"id": "1901.08997", "submitter": "Kerry Liu", "authors": "Jingxian Liu, Ke Xiong, Pingyi Fan, Zhangdui Zhong and Khaled Ben\n  Letaief", "title": "Optimal Design of SWIPT-Aware Fog Computing Networks", "comments": "Submitted to IEEE conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a simultaneous wireless information and power transfer\n(SWIPT)-aware fog computing network, where a multiple antenna fog function\nintegrated hybrid access point (F-HAP) transfers information and energy to\nmultiple heterogeneous single-antenna sensors and also helps some of them\nfulfill computing tasks. By jointly optimizing energy and information\nbeamforming designs at the F-HAP, the bandwidth allocation and the computation\noffloading distribution, an optimization problem is formulated to minimize the\nrequired energy under communication and computation requirements, as well as\nenergy harvesting constraints. Two optimal designs, i.e., fixed offloading time\n(FOT) and optimized offloading time (OOT) designs, are proposed. As both\ndesigns get involved in solving non-convex problems, there are no known\nsolutions to them. Therefore, for the FOT design, the semidefinite relaxation\n(SDR) is adopted to solve it. It is theoretically proved that the rank-one\nconstraints are always satisfied, so the global optimal solution is guaranteed.\nFor the OOT design, since its non-convexity is hard to deal with, a penalty\ndual decomposition (PDD)-based algorithm is proposed, which is able to achieve\na suboptimal solution. The computational complexity for two designs are\nanalyzed. Numerical results show that the partial offloading mode is superior\nto binary benchmark modes. It is also shown that if the system is with strong\nenough computing capability, the OOT design is suggested to achieve lower\nrequired energy; Otherwise, the FOT design is preferred to achieve a relatively\nlow computation complexity.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 17:33:05 GMT"}], "update_date": "2019-01-28", "authors_parsed": [["Liu", "Jingxian", ""], ["Xiong", "Ke", ""], ["Fan", "Pingyi", ""], ["Zhong", "Zhangdui", ""], ["Letaief", "Khaled Ben", ""]]}, {"id": "1901.09084", "submitter": "Zhengbo Zou", "authors": "Zhengbo Zou and Semiha Ergan", "title": "Leveraging Data Driven Approaches to Quantify the Impact of Construction\n  Projects on Urban Quality of Life", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to the World Bank, more than half of the world's population now\nlives in cities, creating burdens on the degraded city infrastructures and\ndriving up the demand for new ones. Construction sites are abundant in already\ndense cities and have unavoidable impacts on surrounding environments and\nresidents. However, such impacts were rarely quantified and made available to\nconstruction teams and local agencies to inform their planning decisions. A\nchallenge in achieving this was the lack of availability of data that can\nprovide insights about how urban residents respond to changes in their\nenvironment due to construction projects. Wider availability of data from city\nagencies nowadays provides opportunities for having such analysis possible.\nThis paper provides the details of a generic data-driven approach that enables\nthe analysis of impact of construction projects on quality of life in urban\nsettings through the quantification of change on widely accepted quality of\nlife indicators in cities. This paper also evaluated the approach using data\nfrom publicly construction projects' information and open city data portals\nfrom New York City. Historical 311 Service Requests along with 27 road\nreconstruction projects were used as testbeds. The results showed that 61% of\nthe projects analyzed in this testbed experienced higher 311 requests after the\ncommencement of construction, with main complaints of 'noise', 'air quality',\nand 'sewer' at the beginning of construction, and 'sanitation' and 'waste'\ntowards the end. Prediction models, built using regression machine learning\nalgorithms, achieved an R-Squared value of 0.67. The approach is capable of\nproviding insights for government agencies and construction companies to take\nproactive actions based on expected complaint types through different phases of\nconstruction.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 21:12:55 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 20:48:31 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 19:57:03 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Zou", "Zhengbo", ""], ["Ergan", "Semiha", ""]]}, {"id": "1901.09264", "submitter": "Luis Daniel Ib\\'a\\~nez", "authors": "Eddy Maddalena and Luis-Daniel Ib\\'a\\~nez and Elena Simperl", "title": "On the mapping of Points of Interest through StreetView imagery and paid\n  crowdsourcing", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of volunteers has emerged as low-cost alternative to generate\naccurate geographical information, an approach known as Volunteered Geographic\nInformation (VGI). However, VGI is limited by the number and availability of\nvolunteers in the area to be mapped, hindering scalability for large areas and\nmaking difficult to map within a time-frame. Fortunately, the availability of\nstreet-view imagery enables the virtual exploration of urban environments,\nmaking possible the recruitment of contributors not necessarily located in the\narea to be mapped. In this paper, we describe the design, implementation, and\nevaluation of the Virtual City Explorer (VCE), a system to collect the\ncoordinates of Points of Interest within a bounded area on top of a street view\nservice with the use of paid crowdworkers. Our evaluation suggests that paid\ncrowdworkers are effective for finding PoIs, and cover almost all the area.\nWith respect to completeness, our approach does not find all PoIs found by\nexperts or VGI communities, but is able to find PoIs that were not found by\nthem, suggesting complementarity. We also studied the impact of making PoIs\nalready discovered by a certain number of workers \\emph{taboo} for incoming\nworkers, finding that it encourages more exploration from workers , increase\nthe number of detected PoIs , and reduce costs.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 18:53:20 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Maddalena", "Eddy", ""], ["Ib\u00e1\u00f1ez", "Luis-Daniel", ""], ["Simperl", "Elena", ""]]}, {"id": "1901.09286", "submitter": "Alan Sherman", "authors": "Alan T. Sherman, Linda Oliva, Enis Golaszewski, Dhananjay Phatak,\n  Travis Scheponik, Geoffrey L. Herman, Dong San Choi, Spencer E. Offenberger,\n  Peter Peterson, Josiah Dykstra, Gregory V. Bard, Ankur Chattopadhyay, Filipo\n  Sharevski, Rakesh Verma, Ryan Vrecenar", "title": "The CATS Hackathon: Creating and Refining Test Items for Cybersecurity\n  Concept Inventories", "comments": "Submitted to IEEE Secuirty & Privacy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For two days in February 2018, 17 cybersecurity educators and professionals\nfrom government and industry met in a \"hackathon\" to refine existing draft\nmultiple-choice test items, and to create new ones, for a Cybersecurity Concept\nInventory (CCI) and Cybersecurity Curriculum Assessment (CCA) being developed\nas part of the Cybersecurity Assessment Tools (CATS) Project. We report on the\nresults of the CATS Hackathon, discussing the methods we used to develop test\nitems, highlighting the evolution of a sample test item through this process,\nand offering suggestions to others who may wish to organize similar hackathons.\n  Each test item embodies a scenario, question stem, and five answer choices.\nDuring the Hackathon, participants organized into teams to (1) Generate new\nscenarios and question stems, (2) Extend CCI items into CCA items, and generate\nnew answer choices for new scenarios and stems, and (3) Review and refine draft\nCCA test items.\n  The CATS Project provides rigorous evidence-based instruments for assessing\nand evaluating educational practices; these instruments can help identify\npedagogies and content that are effective in teaching cybersecurity. The CCI\nmeasures how well students understand basic concepts in\ncybersecurity---especially adversarial thinking---after a first course in the\nfield. The CCA measures how well students understand core concepts after\ncompleting a full cybersecurity curriculum.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 22:15:01 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Sherman", "Alan T.", ""], ["Oliva", "Linda", ""], ["Golaszewski", "Enis", ""], ["Phatak", "Dhananjay", ""], ["Scheponik", "Travis", ""], ["Herman", "Geoffrey L.", ""], ["Choi", "Dong San", ""], ["Offenberger", "Spencer E.", ""], ["Peterson", "Peter", ""], ["Dykstra", "Josiah", ""], ["Bard", "Gregory V.", ""], ["Chattopadhyay", "Ankur", ""], ["Sharevski", "Filipo", ""], ["Verma", "Rakesh", ""], ["Vrecenar", "Ryan", ""]]}, {"id": "1901.09498", "submitter": "Adele Lu Jia", "authors": "Adele Lu Jia, Xiaoxue Shen, Siqi Shen, Jun Xu", "title": "User Donations in a Crowdsourced Video System", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourced video systems like YouTube and Twitch.tv have been a major\ninternet phenomenon and are nowadays entertaining over a billion users. In\naddition to video sharing and viewing, over the years they have developed new\nfeatures to boost the community engagement and some managed to attract users to\ndonate, to the community as well as to other users. User donation directly\nreflects and influences user engagement in the community, and has a great\nimpact on the success of such systems. Nevertheless, user donations in\ncrowdsourced video systems remain trade secrets for most companies and to date\nare still unexplored. In this work, we attempt to fill this gap, and we obtain\nand provide a publicly available dataset on user donations in one crowdsourced\nvideo system named BiliBili. Based on information on nearly 40 thousand\ndonators, we examine the dynamics of user donations and their social\nrelationships, we quantitively reveal the factors that potentially impact user\ndonation, and we adopt machine-learned classifiers and network representation\nlearning models to timely and accurately predict the destinations of the\nmajority and the individual donations.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 03:14:07 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Jia", "Adele Lu", ""], ["Shen", "Xiaoxue", ""], ["Shen", "Siqi", ""], ["Xu", "Jun", ""]]}, {"id": "1901.09659", "submitter": "Nandana Sengupta", "authors": "Nandana Sengupta, Nati Srebro and James Evans", "title": "Simple Surveys: Response Retrieval Inspired by Recommendation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, the use of simple rating and comparison surveys has\nproliferated on social and digital media platforms to fuel recommendations.\nThese simple surveys and their extrapolation with machine learning algorithms\nshed light on user preferences over large and growing pools of items, such as\nmovies, songs and ads. Social scientists have a long history of measuring\nperceptions, preferences and opinions, often over smaller, discrete item sets\nwith exhaustive rating or ranking surveys. This paper introduces simple surveys\nfor social science application. We ran experiments to compare the predictive\naccuracy of both individual and aggregate comparative assessments using four\ntypes of simple surveys: pairwise comparisons and ratings on 2, 5 and\ncontinuous point scales in three distinct contexts: perceived Safety of Google\nStreetview Images, Likeability of Artwork, and Hilarity of Animal GIFs. Across\ncontexts, we find that continuous scale ratings best predict individual\nassessments but consume the most time and cognitive effort. Binary choice\nsurveys are quick and perform best to predict aggregate assessments, useful for\ncollective decision tasks, but poorly predict personalized preferences, for\nwhich they are currently used by Netflix to recommend movies. Pairwise\ncomparisons, by contrast, perform well to predict personal assessments, but\npoorly predict aggregate assessments despite being widely used to crowdsource\nideas and collective preferences. We demonstrate how findings from these\nsurveys can be visualized in a low-dimensional space that reveals distinct\nrespondent interpretations of questions asked in each context. We conclude by\nreflecting on differences between sparse, incomplete simple surveys and their\ntraditional survey counterparts in terms of efficiency, information elicited\nand settings in which knowing less about more may be critical for social\nscience.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 05:18:11 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Sengupta", "Nandana", ""], ["Srebro", "Nati", ""], ["Evans", "James", ""]]}, {"id": "1901.09735", "submitter": "Emiliano De Cristofaro", "authors": "Alexandros Mittos, Savvas Zannettou, Jeremy Blackburn, Emiliano De\n  Cristofaro", "title": "\"And We Will Fight For Our Race!\" A Measurement Study of Genetic Testing\n  Conversations on Reddit and 4chan", "comments": "This is the full version of the paper, with same title, appearing in\n  the 14th AAAI Conference on Web and Social Media (ICWSM 2020). Please cite\n  the ICWSM version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress in genomics has enabled the emergence of a booming market for\n\"direct-to-consumer\" genetic testing. Nowadays, companies like 23andMe and\nAncestryDNA provide affordable health, genealogy, and ancestry reports, and\nhave already tested tens of millions of customers. At the same time, alt- and\nfar-right groups have also taken an interest in genetic testing, using them to\nattack minorities and prove their genetic \"purity.\" In this paper, we present a\nmeasurement study shedding light on how genetic testing is being discussed on\nWeb communities in Reddit and 4chan. We collect 1.3M comments posted over 27\nmonths on the two platforms, using a set of 280 keywords related to genetic\ntesting. We then use NLP and computer vision tools to identify trends, themes,\nand topics of discussion.\n  Our analysis shows that genetic testing attracts a lot of attention on Reddit\nand 4chan, with discussions often including highly toxic language expressed\nthrough hateful, racist, and misogynistic comments. In particular, on 4chan's\npolitically incorrect board (/pol/), content from genetic testing conversations\ninvolves several alt-right personalities and openly antisemitic rhetoric, often\nconveyed through memes. Finally, we find that discussions build around user\ngroups, from technology enthusiasts to communities promoting fringe political\nviews.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 15:29:22 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 16:45:49 GMT"}, {"version": "v3", "created": "Fri, 17 May 2019 21:05:03 GMT"}, {"version": "v4", "created": "Mon, 29 Jul 2019 16:03:38 GMT"}, {"version": "v5", "created": "Fri, 4 Oct 2019 12:59:30 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Mittos", "Alexandros", ""], ["Zannettou", "Savvas", ""], ["Blackburn", "Jeremy", ""], ["De Cristofaro", "Emiliano", ""]]}, {"id": "1901.09790", "submitter": "Azzeddine Benabbou", "authors": "Azzeddine Benabbou (Heudiasyc), Domitile Lourdeaux (Heudiasyc),\n  Dominique Lenne (Heudiasyc)", "title": "A model for prohibition and obligation dilemmas generation in virtual\n  environments", "comments": "in French", "journal-ref": "Sciences et Technologies de l'Information et de la Communication\n  pour l'{\\'E}ducation et la Formation, ATIEF, 2018", "doi": "10.23709/sticef.25.1.4", "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the project Maccoy Critical, we would like to train individuals, in\nvirtual environments, to handle critical situations such as dilemmas. These\nlatter refer to situations where there is no ``good'' solution. In other words,\nsituations that lead to negative consequences whichever choice is made. Our\nobjective is to use Knowledge Models to extract necessary properties for\ndilemmas to emerge. To do so, our approach consists in developing a Scenario\nOrchestration System that generates dilemma situations dynamically without\nhaving to write them beforehand. In this paper we present this approach and\nexpose a proof of concept of the generation process.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 08:06:52 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Benabbou", "Azzeddine", "", "Heudiasyc"], ["Lourdeaux", "Domitile", "", "Heudiasyc"], ["Lenne", "Dominique", "", "Heudiasyc"]]}, {"id": "1901.09791", "submitter": "Jun Wang", "authors": "Jun Wang, Sujoy Sikdar, Tyler Shepherd, Zhibing Zhao, Chunheng Jiang\n  and Lirong Xia", "title": "Practical Algorithms for Multi-Stage Voting Rules with Parallel\n  Universes Tiebreaking", "comments": "arXiv admin note: substantial text overlap with arXiv:1805.06992", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  STV and ranked pairs (RP) are two well-studied voting rules for group\ndecision-making. They proceed in multiple rounds, and are affected by how ties\nare broken in each round. However, the literature is surprisingly vague about\nhow ties should be broken. We propose the first algorithms for computing the\nset of alternatives that are winners under some tiebreaking mechanism under STV\nand RP, which is also known as parallel-universes tiebreaking (PUT).\nUnfortunately, PUT-winners are NP-complete to compute under STV and RP, and\nstandard search algorithms from AI do not apply. We propose multiple DFS-based\nalgorithms along with pruning strategies, heuristics, sampling and machine\nlearning to prioritize search direction to significantly improve the\nperformance. We also propose novel ILP formulations for PUT-winners under STV\nand RP, respectively. Experiments on synthetic and real-world data show that\nour algorithms are overall faster than ILP.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 21:41:39 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Wang", "Jun", ""], ["Sikdar", "Sujoy", ""], ["Shepherd", "Tyler", ""], ["Zhao", "Zhibing", ""], ["Jiang", "Chunheng", ""], ["Xia", "Lirong", ""]]}, {"id": "1901.09804", "submitter": "Yefim Shulman", "authors": "Yefim Shulman, Joachim Meyer", "title": "Is Privacy Controllable?", "comments": "The final publication will be available at Springer via\n  http://dx.doi.org/ [in press]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major views of privacy associates privacy with the control over\ninformation. This gives rise to the question how controllable privacy actually\nis. In this paper, we adapt certain formal methods of control theory and\ninvestigate the implications of a control theoretic analysis of privacy. We\nlook at how control and feedback mechanisms have been studied in the privacy\nliterature. Relying on the control theoretic framework, we develop a simplistic\nconceptual control model of privacy, formulate privacy controllability issues\nand suggest directions for possible research.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 16:57:31 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Shulman", "Yefim", ""], ["Meyer", "Joachim", ""]]}, {"id": "1901.10124", "submitter": "Shubham Atreja", "authors": "Shanu Kumar, Shubham Atreja, Anjali Singh, Mohit Jain", "title": "Adversarial Adaptation of Scene Graph Models for Understanding Civic\n  Issues", "comments": "Accepted at WWW'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citizen engagement and technology usage are two emerging trends driven by\nsmart city initiatives. Governments around the world are adopting technology\nfor faster resolution of civic issues. Typically, citizens report issues, such\nas broken roads, garbage dumps, etc. through web portals and mobile apps, in\norder for the government authorities to take appropriate actions. Several\nmediums -- text, image, audio, video -- are used to report these issues.\nThrough a user study with 13 citizens and 3 authorities, we found that image is\nthe most preferred medium to report civic issues. However, analyzing civic\nissue related images is challenging for the authorities as it requires manual\neffort. Moreover, previous works have been limited to identifying a specific\nset of issues from images. In this work, given an image, we propose to generate\na Civic Issue Graph consisting of a set of objects and the semantic relations\nbetween them, which are representative of the underlying civic issue. We also\nrelease two multi-modal (text and images) datasets, that can help in further\nanalysis of civic issues from images. We present a novel approach for\nadversarial training of existing scene graph models that enables the use of\nscene graphs for new applications in the absence of any labelled training data.\nWe conduct several experiments to analyze the efficacy of our approach, and\nusing human evaluation, we establish the appropriateness of our model at\nrepresenting different civic issues.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 06:02:45 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Kumar", "Shanu", ""], ["Atreja", "Shubham", ""], ["Singh", "Anjali", ""], ["Jain", "Mohit", ""]]}, {"id": "1901.10268", "submitter": "Zhen Xue", "authors": "Wei Cui, Zhen Xue and Khanh-Phuong Thai", "title": "Performance comparison of an AI-based Adaptive Learning System in China", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive learning systems stand apart from traditional learning systems by\noffering a personalized learning experience to students according to their\ndifferent knowledge states. Adaptive systems collect and analyse students'\nbehavior data, update learner profiles, then accordingly provide timely\nindividualized feedback to each student. Such interactions between the learning\nsystem and students can improve the engagement of students and the efficiency\nof learning. This paper evaluates the effectiveness of an adaptive learning\nsystem, \"Yixue Squirrel AI\" (or Yixue), on English and math learning in middle\nschool. The effectiveness of the Yixue's math and English learning systems is\nrespectively compared against (1) traditional classroom math instruction\nconducted by expert human teachers and (2) BOXFiSH, another adaptive learning\nplatform for English language learning. Results suggest that students achieved\nbetter performance using Yixue adaptive learning system than both traditional\nclassroom instruction by expert teachers and another adaptive learning\nplatform.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 13:21:03 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Cui", "Wei", ""], ["Xue", "Zhen", ""], ["Thai", "Khanh-Phuong", ""]]}, {"id": "1901.10312", "submitter": "Aythami Morales", "authors": "Alejandro Acien, Aythami Morales, Ruben Vera-Rodriguez, and Julian\n  Fierrez", "title": "MultiLock: Mobile Active Authentication based on Multiple Biometric and\n  Behavioral Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we evaluate mobile active authentication based on an ensemble\nof biometrics and behavior-based profiling signals. We consider seven different\ndata channels and their combination. Touch dynamics (touch gestures and\nkeystroking), accelerometer, gyroscope, WiFi, GPS location and app usage are\nall collected during human-mobile interaction to authenticate the users. We\nevaluate two approaches: one-time authentication and active authentication. In\none-time authentication, we employ the information of all channels available\nduring one session. For active authentication we take advantage of mobile user\nbehavior across multiple sessions by updating a confidence value of the\nauthentication score. Our experiments are conducted on the semi-uncontrolled\nUMDAA-02 database. This database comprises smartphone sensor signals acquired\nduring natural human-mobile interaction. Our results show that different traits\ncan be complementary and multimodal systems clearly increase the performance\nwith accuracies ranging from 82.2% to 97.1% depending on the authentication\nscenario.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 14:39:37 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Acien", "Alejandro", ""], ["Morales", "Aythami", ""], ["Vera-Rodriguez", "Ruben", ""], ["Fierrez", "Julian", ""]]}, {"id": "1901.10437", "submitter": "Piotr Sapiezynski", "authors": "Piotr Sapiezynski, Wesley Zeng, Ronald E. Robertson, Alan Mislove,\n  Christo Wilson", "title": "Quantifying the Impact of User Attention on Fair Group Representation in\n  Ranked Lists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a novel metric for auditing group fairness in\nranked lists. Our approach offers two benefits compared to the state of the\nart. First, we offer a blueprint for modeling of user attention. Rather than\nassuming a logarithmic loss in importance as a function of the rank, we can\naccount for varying user behaviors through parametrization. For example, we\nexpect a user to see more items during a viewing of a social media feed than\nwhen they inspect the results list of a single web search query. Second, we\nallow non-binary protected attributes to enable investigating inherently\ncontinuous attributes (\\eg political alignment on the liberal to conservative\nspectrum) as well as to facilitate measurements across aggregated sets of\nsearch results, rather than separately for each result list. By combining these\ntwo elements into our metric, we are able to better address the human factors\ninherent in this problem. We measure the whole sociotechnical system,\nconsisting of a ranking algorithm and individuals using it, instead of\nexclusively focusing on the ranking algorithm. Finally, we use our metric to\nperform three simulated fairness audits. We show that determining fairness of a\nranked output necessitates knowledge (or a model) of the end-users of the\nparticular service. Depending on their attention distribution function, a fixed\nranking of results can appear biased both in favor and against a protected\ngroup.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 18:25:54 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 05:42:50 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Sapiezynski", "Piotr", ""], ["Zeng", "Wesley", ""], ["Robertson", "Ronald E.", ""], ["Mislove", "Alan", ""], ["Wilson", "Christo", ""]]}, {"id": "1901.10443", "submitter": "Vijay Keswani", "authors": "L. Elisa Celis and Vijay Keswani", "title": "Improved Adversarial Learning for Fair Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by concerns that machine learning algorithms may introduce\nsignificant bias in classification models, developing fair classifiers has\nbecome an important problem in machine learning research. One important\nparadigm towards this has been providing algorithms for adversarially learning\nfair classifiers (Zhang et al., 2018; Madras et al., 2018). We formulate the\nadversarial learning problem as a multi-objective optimization problem and find\nthe fair model using gradient descent-ascent algorithm with a modified gradient\nupdate step, inspired by the approach of Zhang et al., 2018. We provide\ntheoretical insight and guarantees that formalize the heuristic arguments\npresented previously towards taking such an approach. We test our approach\nempirically on the Adult dataset and synthetic datasets and compare against\nstate of the art algorithms (Celis et al., 2018; Zhang et al., 2018; Zafar et\nal., 2017). The results show that our models and algorithms have comparable or\nbetter accuracy than other algorithms while performing better in terms of\nfairness, as measured using statistical rate or false discovery rate.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 18:42:49 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Celis", "L. Elisa", ""], ["Keswani", "Vijay", ""]]}, {"id": "1901.10501", "submitter": "Hao Wang", "authors": "Hao Wang, Berk Ustun, Flavio P. Calmon", "title": "Repairing without Retraining: Avoiding Disparate Impact with\n  Counterfactual Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the performance of a machine learning model varies over groups defined\nby sensitive attributes (e.g., gender or ethnicity), the performance disparity\ncan be expressed in terms of the probability distributions of the input and\noutput variables over each group. In this paper, we exploit this fact to reduce\nthe disparate impact of a fixed classification model over a population of\ninterest. Given a black-box classifier, we aim to eliminate the performance gap\nby perturbing the distribution of input variables for the disadvantaged group.\nWe refer to the perturbed distribution as a counterfactual distribution, and\ncharacterize its properties for common fairness criteria. We introduce a\ndescent algorithm to learn a counterfactual distribution from data. We then\ndiscuss how the estimated distribution can be used to build a data preprocessor\nthat can reduce disparate impact without training a new model. We validate our\napproach through experiments on real-world datasets, showing that it can repair\ndifferent forms of disparity without a significant drop in accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 19:21:10 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 15:42:11 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Wang", "Hao", ""], ["Ustun", "Berk", ""], ["Calmon", "Flavio P.", ""]]}, {"id": "1901.10527", "submitter": "Yi Liu", "authors": "Yi Liu, Jiawen Peng, Zhihao Yu", "title": "Big Data Platform Architecture Under The Background of Financial\n  Technology", "comments": "4 pages, 3 figures, 2018 International Conference on Big Data\n  Engineering and Technology", "journal-ref": null, "doi": "10.1145/3297730.3297743", "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of the concept of financial technology, financial and\ntechnology gradually in-depth integration, scientific and technological means\nto become financial product innovation, improve financial efficiency and reduce\nfinancial transaction costs an important driving force. In this context, the\nnew technology platform is from the business philosophy, business model,\ntechnical means, sales, internal management, and other dimensions to re-shape\nthe financial industry. In this paper, the existing big data platform\narchitecture technology innovation, adding space-time data elements, combined\nwith the insurance industry for practical analysis, put forward a meaningful\nproduct circle and customer circle.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2019 02:39:26 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Liu", "Yi", ""], ["Peng", "Jiawen", ""], ["Yu", "Zhihao", ""]]}, {"id": "1901.10530", "submitter": "Manik Sharma Dr.", "authors": "M. Sharma, G. Singh and R. Singh", "title": "An Advanced Conceptual Diagnostic Healthcare Framework for Diabetes and\n  Cardiovascular Disorders", "comments": "11 PAGES", "journal-ref": "EAI ENDORSED TRANSACTIONS ON SCALABLE INFORMATION SYSTEMS 5.18\n  (2018)", "doi": "10.4108/eai.19-6-2018.154828", "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data mining along with emerging computing techniques have astonishingly\ninfluenced the healthcare industry. Researchers have used different Data Mining\nand Internet of Things (IoT) for enrooting a programmed solution for diabetes\nand heart patients. However, still, more advanced and united solution is needed\nthat can offer a therapeutic opinion to individual diabetic and cardio\npatients. Therefore, here, a smart data mining and IoT (SMDIoT) based advanced\nhealthcare system for proficient diabetes and cardiovascular diseases have been\nproposed. The hybridization of data mining and IoT with other emerging\ncomputing techniques is supposed to give an effective and economical solution\nto diabetes and cardio patients. SMDIoT hybridized the ideas of data mining,\nInternet of Things, chatbots, contextual entity search (CES), bio-sensors,\nsemantic analysis and granular computing (GC). The bio-sensors of the proposed\nsystem assist in getting the current and precise status of the concerned\npatients so that in case of an emergency, the needful medical assistance can be\nprovided. The novelty lies in the hybrid framework and the adequate support of\nchatbots, granular computing, context entity search and semantic analysis. The\npractical implementation of this system is very challenging and costly.\nHowever, it appears to be more operative and economical solution for diabetes\nand cardio patients.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 15:46:24 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Sharma", "M.", ""], ["Singh", "G.", ""], ["Singh", "R.", ""]]}, {"id": "1901.10553", "submitter": "Zhoutong Wang", "authors": "Zhoutong Wang, Qianhui Liang, Fabio Duarte, Fan Zhang, Louis Charron,\n  Lenna Johnsen, Bill Cai, Carlo Ratti", "title": "Quantifying Legibility of Indoor Spaces Using Deep Convolutional Neural\n  Networks: Case Studies in Train Stations", "comments": "20 pages, 19 figures, 7 tables", "journal-ref": null, "doi": "10.1016/j.buildenv.2019.04.035", "report-no": null, "categories": "cs.CY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Legibility is the extent to which a space can be easily recognized.\nEvaluating legibility is particularly desirable in indoor spaces, since it has\na large impact on human behavior and the efficiency of space utilization.\nHowever, indoor space legibility has only been studied through survey and\ntrivial simulations and lacks reliable quantitative measurement. We utilized a\nDeep Convolutional Neural Network (DCNN), which is structurally similar to a\nhuman perception system, to model legibility in indoor spaces. To implement the\nmodeling of legibility for any indoor spaces, we designed an end-to-end\nprocessing pipeline from indoor data retrieving to model training to spatial\nlegibility analysis. Although the model performed very well (98% top-1\naccuracy) overall, there are still discrepancies in accuracy among different\nspaces, reflecting legibility differences. To prove the validity of the\npipeline, we deployed a survey on Amazon Mechanical Turk, collecting 4,015\nsamples. The human samples showed a similar behavior pattern and mechanism as\nthe DCNN models. Further, we used model results to visually explain legibility\nin different architectural programs, building age, building style, visual\nclusterings of spaces and visual explanations for building age and\narchitectural functions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 14:52:07 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Wang", "Zhoutong", ""], ["Liang", "Qianhui", ""], ["Duarte", "Fabio", ""], ["Zhang", "Fan", ""], ["Charron", "Louis", ""], ["Johnsen", "Lenna", ""], ["Cai", "Bill", ""], ["Ratti", "Carlo", ""]]}, {"id": "1901.10555", "submitter": "Otmane Azeroual", "authors": "Otmane Azeroual and Horst Theel", "title": "The Effects of Using Business Intelligence Systems on an Excellence\n  Management and Decision-Making Process by Start-Up Companies: A Case Study", "comments": "4(3), pp.30-40", "journal-ref": "International Journal of Management Science and Business\n  Administration, Inovatus Usluge Ltd., 2018", "doi": "10.18775/ijmsba.1849-5664-5419.2014.43.1004", "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The rapid increase in data volumes in companies has meant that momentous and\ncomprehensive information gathering is barely possible by manual means.\nBusiness intelligence solutions can help here. They provide tools with\nappropriate technologies to assist with the collection, integration, storage,\nediting, and analysis of existing data. While almost only large companies were\ninterested in this topic a few years ago, it has meanwhile also become\nnecessary for start-up companies, and so the market for business intelligence\nhas been growing for years. This article focuses on the general potentials of\nusing BI in start-ups. First, will be examined which providers of BI solutions\nthat are suitable for start-ups and what opportunities exist for implementing\nBI systems in start-ups. Then it will be shown to what extent BI has prevailed\nin start-ups, in which areas the techniques of BI are used in start-ups and\nwhat purpose BI has in start-ups. Finally, the success factors for BI projects\nin start-ups are considered.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 13:16:54 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Azeroual", "Otmane", ""], ["Theel", "Horst", ""]]}, {"id": "1901.10566", "submitter": "Sherri Rose", "authors": "Anna Zink and Sherri Rose", "title": "Fair Regression for Health Care Spending", "comments": "30 pages, 3 figures", "journal-ref": "Biometrics (2020)", "doi": "10.1111/biom.13206", "report-no": null, "categories": "stat.AP cs.CY stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution of health care payments to insurance plans has substantial\nconsequences for social policy. Risk adjustment formulas predict spending in\nhealth insurance markets in order to provide fair benefits and health care\ncoverage for all enrollees, regardless of their health status. Unfortunately,\ncurrent risk adjustment formulas are known to underpredict spending for\nspecific groups of enrollees leading to undercompensated payments to health\ninsurers. This incentivizes insurers to design their plans such that\nindividuals in undercompensated groups will be less likely to enroll, impacting\naccess to health care for these groups. To improve risk adjustment formulas for\nundercompensated groups, we expand on concepts from the statistics, computer\nscience, and health economics literature to develop new fair regression methods\nfor continuous outcomes by building fairness considerations directly into the\nobjective function. We additionally propose a novel measure of fairness while\nasserting that a suite of metrics is necessary in order to evaluate risk\nadjustment formulas more fully. Our data application using the IBM MarketScan\nResearch Databases and simulation studies demonstrate that these new fair\nregression methods may lead to massive improvements in group fairness (e.g.,\n98%) with only small reductions in overall fit (e.g., 4%).\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 04:06:50 GMT"}, {"version": "v2", "created": "Sat, 13 Jul 2019 06:11:28 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Zink", "Anna", ""], ["Rose", "Sherri", ""]]}, {"id": "1901.10578", "submitter": "Solomia Fedushko", "authors": "Yuriy Syerov, Solomia Fedushko", "title": "The Computer-Linguistic Analysis of Socio-Demographic Profile of Virtual\n  Community Member", "comments": null, "journal-ref": "International Journal of Computer Science and Business\n  Informatics, 2013", "doi": null, "report-no": null, "categories": "cs.CY cs.NI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers the current problem of investigation and development\nof computerlinguistic analysis of socio-demographic profile of virtual\ncommunity member. Webmembers' socio-demographic characteristics' profile\nvalidation based on analysis of sociodemographic characteristics. The\ntopicality of the paper is determined by the necessity to identify the\nweb-community member by means of computer-linguistic analysis of their\ninformation track. The formal model of basic socio-demographic characteristics\nof virtual communities' member is formed. The structural model of\nlingvo-communicative indicators of socio-demographic characteristics of the\nweb-members and common algorithm of the formation of lingvo-communicative\nindicators based on processing training sample are developed. Types of the\ncomputer-linguistic analysis of indicative characteristics are studied and\nclassifications of lingvo-communicative indicators of gender, age and sphere of\nactivities of web-community member is established. Also, the formal model of\nthe basic socio-demographic characteristics of web-communities' member is\nintroduced.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 20:27:30 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Syerov", "Yuriy", ""], ["Fedushko", "Solomia", ""]]}, {"id": "1901.10579", "submitter": "Bestoun Ahmed Dr.", "authors": "Bestoun S. Ahmed, Miroslav Bures, Karel Frajtak, Tomas Cerny", "title": "Aspects of Quality in Internet of Things (IoT) Solutions: A Systematic\n  Mapping Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) is an emerging technology that has the promising\npower to change our future. Due to the market pressure, IoT systems may be\nreleased without sufficient testing. However, it is no longer acceptable to\nrelease IoT systems to the market without assuring the quality. As in the case\nof new technologies, the quality assurance process is a challenging task. This\npaper shows the results of the first comprehensive and systematic mapping study\nto structure and categories the research evidence in the literature starting in\n2009 when the early publication of IoT papers for IoT quality assurance\nappeared. The conducted research is based on the most recent guidelines on how\nto perform systematic mapping studies. A set of research questions is defined\ncarefully regarding the quality aspects of the IoT. Based on these questions, a\nlarge number of evidence and research papers is considered in the study (478\npapers). We have extracted and analyzed different levels of information from\nthose considered papers. Also, we have classified the topics addressed in those\npapers into categories based on the quality aspects. The study results carry\nout different areas that require more work and investigation in the context of\nIoT quality assurance. The results of the study can help in a further\nunderstanding of the research gaps. Moreover, the results show a roadmap for\nfuture research directions.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 12:39:24 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Ahmed", "Bestoun S.", ""], ["Bures", "Miroslav", ""], ["Frajtak", "Karel", ""], ["Cerny", "Tomas", ""]]}, {"id": "1901.10580", "submitter": "Milan Jain", "authors": "Milan Jain, Mridula Gupta, Amarjeet Singh, and Vikas Chandan", "title": "Beyond Control: Enabling Smart Thermostats For Leakage Detection", "comments": "21 pages", "journal-ref": "Proceedings of the ACM on Interactive, Mobile, Wearable, and\n  Ubiquitous Technologies (IMWUT) 2019", "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart thermostats, with multiple sensory abilities, are becoming pervasive\nand ubiquitous, in both residential and commercial buildings. By analyzing\noccupants' behavior, adjusting set temperature automatically, and adapting to\ntemporal and spatial changes in the atmosphere, smart thermostats can maximize\nboth - energy savings and user comfort. In this paper, we study smart\nthermostats for refrigerant leakage detection. Retail outlets, such as\nmilk-booths and quick service restaurants set up cold-rooms to store perishable\nitems. In each room, a refrigeration unit (akin to air-conditioners) is used to\nmaintain a suitable temperature for the stored products. Often, refrigerant\nleaks through the coils (or valves) of the refrigeration unit which slowly\ndiminishes the cooling capacity of the refrigeration unit while allowing it to\nbe functional. Such leaks waste significant energy, risk occupants' health, and\nimpact the quality of stored perishable products. While store managers usually\nfail to sense the early symptoms of such leaks, current techniques to report\nrefrigerant leakage are often not scalable. We propose Greina - to continuously\nmonitor the readily available ambient information from the thermostat and\ntimely report such leaks. We evaluate our approach on 74 outlets of a retail\nenterprise and results indicate that Greina can report the leakage a week in\nadvance when compared to manual reporting.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 21:18:00 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Jain", "Milan", ""], ["Gupta", "Mridula", ""], ["Singh", "Amarjeet", ""], ["Chandan", "Vikas", ""]]}, {"id": "1901.10581", "submitter": "Morteza Taiebat", "authors": "Morteza Taiebat, Austin L. Brown, Hannah R. Safford, Shen Qu, Ming Xu", "title": "A Review on Energy, Environmental, and Sustainability Implications of\n  Connected and Automated Vehicles", "comments": null, "journal-ref": "Environmental Science & Technology, 2018, 52(20), 11449-11465", "doi": "10.1021/acs.est.8b00127", "report-no": null, "categories": "cs.CY econ.GN q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Connected and automated vehicles (CAVs) are poised to reshape transportation\nand mobility by replacing humans as the driver and service provider. While the\nprimary stated motivation for vehicle automation is to improve safety and\nconvenience of road mobility, this transformation also provides a valuable\nopportunity to improve vehicle energy efficiency and reduce emissions in the\ntransportation sector. Progress in vehicle efficiency and functionality,\nhowever, does not necessarily translate to net positive environmental outcomes.\nHere we examine the interactions between CAV technology and the environment at\nfour levels of increasing complexity: vehicle, transportation system, urban\nsystem, and society. We find that environmental impacts come from\nCAV-facilitated transformations at all four levels, rather than from CAV\ntechnology directly. We anticipate net positive environmental impacts at the\nvehicle, transportation system, and urban system levels, but expect greater\nvehicle utilization and shifts in travel patterns at the society level to\noffset some of these benefits. Focusing on the vehicle-level improvements\nassociated with CAV technology is likely to yield excessively optimistic\nestimates of environmental benefits. Future research and policy efforts should\nstrive to clarify the extent and possible synergetic effects from a systems\nlevel in order to envisage and address concerns regarding the short- and\nlong-term sustainable adoption of CAV technology.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 03:20:24 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 05:05:42 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Taiebat", "Morteza", ""], ["Brown", "Austin L.", ""], ["Safford", "Hannah R.", ""], ["Qu", "Shen", ""], ["Xu", "Ming", ""]]}, {"id": "1901.10582", "submitter": "Nikos Fotiou", "authors": "Nikos Fotiou, George C. Polyzos", "title": "Smart contracts for the Internet of Things: opportunities and challenges", "comments": "Proc. of the European Conference on Networks and Communications\n  (EUCNC), Ljubljana, Slovenia, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the Internet of Things (IoT), Things are expected to live in different\n\"domains\" and \"contexts\" during their lifetime. Information generated by and\nassociated with Things should be manageable by multiple, diverse stakeholders\naccordingly. Moreover, the scope of the information related to Things can range\nfrom private and confidential to public and auditable. Identification,\nsecurity, and interoperability in this vivid environment are expected to be\nchallenging. In this paper we discuss how smart contracts and blockchain\ntechnologies create the potential for a viable solution. To this end, we\npresent smart contract-based solutions that improve security and information\nmanagement, we identify new opportunities and challenges, and we provide\nsecurity recommendations and guidelines.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 10:48:16 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Fotiou", "Nikos", ""], ["Polyzos", "George C.", ""]]}, {"id": "1901.10583", "submitter": "Vithya Yogarajan", "authors": "Vithya Yogarajan, Bernhard Pfahringer and Michael Mayo", "title": "Automatic end-to-end De-identification: Is high accuracy the only\n  metric?", "comments": "17 pages, 1 figure, 7 tables, review journal paper", "journal-ref": "Applied Artificial Intelligence, 2020", "doi": "10.1080/08839514.2020.1718343", "report-no": "04-Feb-2020", "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  De-identification of electronic health records (EHR) is a vital step towards\nadvancing health informatics research and maximising the use of available data.\nIt is a two-step process where step one is the identification of protected\nhealth information (PHI), and step two is replacing such PHI with surrogates.\nDespite the recent advances in automatic de-identification of EHR, significant\nobstacles remain if the abundant health data available are to be used to the\nfull potential. Accuracy in de-identification could be considered a necessary,\nbut not sufficient condition for the use of EHR without individual patient\nconsent. We present here a comprehensive review of the progress to date, both\nthe impressive successes in achieving high accuracy and the significant risks\nand challenges that remain. To best of our knowledge, this is the first paper\nto present a complete picture of end-to-end automatic de-identification. We\nreview 18 recently published automatic de-identification systems -designed to\nde-identify EHR in the form of free text- to show the advancements made in\nimproving the overall accuracy of the system, and in identifying individual\nPHI. We argue that despite the improvements in accuracy there remain challenges\nin surrogate generation and replacements of identified PHIs, and the risks\nposed to patient protection and privacy.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 21:51:40 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Yogarajan", "Vithya", ""], ["Pfahringer", "Bernhard", ""], ["Mayo", "Michael", ""]]}, {"id": "1901.10794", "submitter": "Radhesh Krishnan Konoth", "authors": "Radhesh Krishnan Konoth, Rolf van Wegberg, Veelasha Moonsamy and\n  Herbert Bos", "title": "Malicious cryptocurrency miners: Status and Outlook", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, we examine the behavior and profitability of modern malware\nthat mines cryptocurrency. Unlike previous studies, we look at the\ncryptocurrency market as a whole, rather than just Bitcoin. We not only\nconsider PCs, but also mobile phones, and IoT devices. In the past few years,\ncriminals have attacked all these platforms for the purpose of cryptocurrency\nmining. The question is: how much money do they make? It is common knowledge\nthat mining Bitcoin is now very difficult, so why do the criminals even target\nlow-end devices for mining purposes? By analyzing the most important families\nof malicious cryptocurrency miners that were active between 2014 and 2017, we\nare able to report how they work, which currency they mine, and how profitable\nit is to do so. We will see that the evolution of the cryptocurrency market\nwith many new cryptocurrencies that are still CPU minable and offer better\nprivacy to criminals and have contributed to making mining malware attractive\nagain -- with attackers generating a continuous stream of profit that in some\ncases may reach in the millions.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 18:34:26 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Konoth", "Radhesh Krishnan", ""], ["van Wegberg", "Rolf", ""], ["Moonsamy", "Veelasha", ""], ["Bos", "Herbert", ""]]}, {"id": "1901.10837", "submitter": "Ziyuan Zhong", "authors": "Alexandre Louis Lamy, Ziyuan Zhong, Aditya Krishna Menon, Nakul Verma", "title": "Noise-tolerant fair classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairness-aware learning involves designing algorithms that do not\ndiscriminate with respect to some sensitive feature (e.g., race or gender).\nExisting work on the problem operates under the assumption that the sensitive\nfeature available in one's training sample is perfectly reliable. This\nassumption may be violated in many real-world cases: for example, respondents\nto a survey may choose to conceal or obfuscate their group identity out of fear\nof potential discrimination. This poses the question of whether one can still\nlearn fair classifiers given noisy sensitive features. In this paper, we answer\nthe question in the affirmative: we show that if one measures fairness using\nthe mean-difference score, and sensitive features are subject to noise from the\nmutually contaminated learning model, then owing to a simple identity we only\nneed to change the desired fairness-tolerance. The requisite tolerance can be\nestimated by leveraging existing noise-rate estimators from the label noise\nliterature. We finally show that our procedure is empirically effective on two\ncase-studies involving sensitive feature censoring.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 14:11:05 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 18:08:36 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2019 07:07:43 GMT"}, {"version": "v4", "created": "Thu, 9 Jan 2020 14:44:05 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Lamy", "Alexandre Louis", ""], ["Zhong", "Ziyuan", ""], ["Menon", "Aditya Krishna", ""], ["Verma", "Nakul", ""]]}, {"id": "1901.11053", "submitter": "Markus Steinberg", "authors": "Markus D. Steinberg", "title": "Software solutions for form-based collection of data and the semantic\n  enrichment of form data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data collection is an important part of many citizen science projects as well\nas other fields of research, particularly in life sciences. Mobile applications\nwith form-based surveys are increasingly used to support this, due to the large\nnumber of mobile devices and their growing number of built-in sensors. Since\nthe composition of form-based surveys from scratch can be a tedious task,\nmultiple tools have been published that can help with their design and\ndistribution as well as the data collection via mobile devices and the data\nstorage. Some even support simple data analysis. With this increasing number of\nsoftware options project leaders will often face the question, which tool is\nmost suitable for their current use case.\n  With that in mind, this project pursues two main objectives:\n  1. To present an overview of a selection of survey design tools and their\ncapabilities in order to provide a clear foundation for such a decision.\n  2. To examine if any tool provides the capability to collect and export data\nin a way that can easily be used and interpreted by other applications or\npersons. This aspect includes the supply of metadata about the data collection\nprocess and the data itself, information about the meaning of the data as well\nas an export format that can easily be processed.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 19:19:02 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Steinberg", "Markus D.", ""]]}, {"id": "1901.11162", "submitter": "Jane Im", "authors": "Jane Im, Eshwar Chandrasekharan, Jackson Sargent, Paige Lighthammer,\n  Taylor Denby, Ankit Bhargava, Libby Hemphill, David Jurgens, Eric Gilbert", "title": "Still out there: Modeling and Identifying Russian Troll Accounts on\n  Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There is evidence that Russia's Internet Research Agency attempted to\ninterfere with the 2016 U.S. election by running fake accounts on Twitter -\noften referred to as \"Russian trolls\". In this work, we: 1) develop machine\nlearning models that predict whether a Twitter account is a Russian troll\nwithin a set of 170K control accounts; and, 2) demonstrate that it is possible\nto use this model to find active accounts on Twitter still likely acting on\nbehalf of the Russian state. Using both behavioral and linguistic features, we\nshow that it is possible to distinguish between a troll and a non-troll with a\nprecision of 78.5% and an AUC of 98.9%, under cross-validation. Applying the\nmodel to out-of-sample accounts still active today, we find that up to 2.6% of\ntop journalists' mentions are occupied by Russian trolls. These findings imply\nthat the Russian trolls are very likely still active today. Additional analysis\nshows that they are not merely software-controlled bots, and manage their\nonline identities in various complex ways. Finally, we argue that if it is\npossible to discover these accounts using externally - accessible data, then\nthe platforms - with access to a variety of private internal signals - should\nsucceed at similar or better rates.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 01:14:28 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Im", "Jane", ""], ["Chandrasekharan", "Eshwar", ""], ["Sargent", "Jackson", ""], ["Lighthammer", "Paige", ""], ["Denby", "Taylor", ""], ["Bhargava", "Ankit", ""], ["Hemphill", "Libby", ""], ["Jurgens", "David", ""], ["Gilbert", "Eric", ""]]}, {"id": "1901.11281", "submitter": "Vincent Labatut", "authors": "Etienne Papegnies (LIA), Vincent Labatut (LIA), Richard Dufour (LIA),\n  Georges Linares (LIA)", "title": "Conversational Networks for Automatic Online Moderation", "comments": null, "journal-ref": "IEEE Transactions on Computational Social Systems, 2019,\n  https://ieeexplore.ieee.org/document/8629298", "doi": "10.1109/tcss.2018.2887240", "report-no": null, "categories": "cs.IR cs.CL cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moderation of user-generated content in an online community is a challenge\nthat has great socio-economical ramifications. However, the costs incurred by\ndelegating this work to human agents are high. For this reason, an automatic\nsystem able to detect abuse in user-generated content is of great interest.\nThere are a number of ways to tackle this problem, but the most commonly seen\nin practice are word filtering or regular expression matching. The main\nlimitations are their vulnerability to intentional obfuscation on the part of\nthe users, and their context-insensitive nature. Moreover, they are\nlanguage-dependent and may require appropriate corpora for training. In this\npaper, we propose a system for automatic abuse detection that completely\ndisregards message content. We first extract a conversational network from raw\nchat logs and characterize it through topological measures. We then use these\nas features to train a classifier on our abuse detection task. We thoroughly\nassess our system on a dataset of user comments originating from a French\nMassively Multiplayer Online Game. We identify the most appropriate network\nextraction parameters and discuss the discriminative power of our features,\nrelatively to their topological and temporal nature. Our method reaches an\nF-measure of 83.89 when using the full feature set, improving on existing\napproaches. With a selection of the most discriminative features, we\ndramatically cut computing time while retaining most of the performance\n(82.65).\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 09:23:57 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Papegnies", "Etienne", "", "LIA"], ["Labatut", "Vincent", "", "LIA"], ["Dufour", "Richard", "", "LIA"], ["Linares", "Georges", "", "LIA"]]}, {"id": "1901.11408", "submitter": "Daniel Schwabe", "authors": "Daniel Schwabe, Carlos Laufer and Antonio Busson", "title": "Building Knowledge Graphs About Political Agents in the Age of\n  Misinformation", "comments": "arXiv admin note: substantial text overlap with arXiv:1804.06015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the construction of a Knowledge Graph about relations\nbetween agents in a political system. It discusses the main modeling\nchallenges, with emphasis on the issue of trust and provenance. Implementation\ndecisions are also presented\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 22:02:24 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Schwabe", "Daniel", ""], ["Laufer", "Carlos", ""], ["Busson", "Antonio", ""]]}]