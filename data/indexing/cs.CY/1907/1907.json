[{"id": "1907.00089", "submitter": "Ramin Mohammadi", "authors": "Ramin Mohammadi, Sarthak Jain, Stephen Agboola, Ramya Palacholla,\n  Sagar Kamarthi, Byron C. Wallace", "title": "Learning to Identify Patients at Risk of Uncontrolled Hypertension Using\n  Electronic Health Records Data", "comments": "Accepted at The AMIA informatics summit 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypertension is a major risk factor for stroke, cardiovascular disease, and\nend-stage renal disease, and its prevalence is expected to rise dramatically.\nEffective hypertension management is thus critical. A particular priority is\ndecreasing the incidence of uncontrolled hypertension. Early identification of\npatients at risk for uncontrolled hypertension would allow targeted use of\npersonalized, proactive treatments. We develop machine learning models\n(logistic regression and recurrent neural networks) to stratify patients with\nrespect to the risk of exhibiting uncontrolled hypertension within the coming\nthree-month period. We trained and tested models using EHR data from 14,407 and\n3,009 patients, respectively. The best model achieved an AUROC of 0.719,\noutperforming the simple, competitive baseline of relying prediction based on\nthe last BP measure alone (0.634). Perhaps surprisingly, recurrent neural\nnetworks did not outperform a simple logistic regression for this task,\nsuggesting that linear models should be included as strong baselines for\npredictive tasks using EHR\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 21:33:40 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Mohammadi", "Ramin", ""], ["Jain", "Sarthak", ""], ["Agboola", "Stephen", ""], ["Palacholla", "Ramya", ""], ["Kamarthi", "Sagar", ""], ["Wallace", "Byron C.", ""]]}, {"id": "1907.00181", "submitter": "Anders Edelbo Lillie", "authors": "Anders Edelbo Lillie and Emil Refsgaard Middelboe", "title": "Fake News Detection using Stance Classification: A Survey", "comments": "19 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper surveys and presents recent academic work carried out within the\nfield of stance classification and fake news detection. Echo chambers and the\nmodel organism problem are examples that pose challenges to acquire data with\nhigh quality, due to opinions being polarised in microblogs. Nevertheless it is\nshown that several machine learning approaches achieve promising results in\nclassifying stance. Some use crowd stance for fake news detection, such as the\napproach in [Dungs et al., 2018] using Hidden Markov Models. Furthermore\nfeature engineering have significant importance in several approaches, which is\nshown in [Aker et al., 2017]. This paper additionally includes a proposal of a\nsystem implementation based on the presented survey.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 11:00:22 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Lillie", "Anders Edelbo", ""], ["Middelboe", "Emil Refsgaard", ""]]}, {"id": "1907.00332", "submitter": "Vidyasagar Sadhu", "authors": "Gabriel Salles-Loustau, Vidyasagar Sadhu, Dario Pompili, Saman Zonouz,\n  Vincent Sritapan", "title": "Secure Mobile Technologies for Proactive Critical Infrastructure\n  Situational Awareness", "comments": "6 pages, IEEE HST 2016", "journal-ref": "2016 IEEE Symposium on Technologies for Homeland Security (HST),\n  Waltham, pp. 1-6", "doi": "10.1109/THS.2016.7568966", "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trustworthy operation of our national critical infrastructures, such as the\nelectricity grid, against adversarial parties and accidental failures requires\nconstant and secure monitoring capabilities. In this paper, Eyephone is\npresented to leverage secure smartphone sensing and data acquisition\ncapabilities and enable pervasive sensing of the national critical\ninfrastructures. The reported information by the smartphone users will notify\nthe control center operators about particular accidental or malicious remote\ncritical infrastructure incidents. The reporting will be proactive regarding\npotentially upcoming failures given the system's current risky situation, e.g.,\na tree close to fall on a power grid transmission line. The information will\ninclude various modalities such as images, video, audio, time and location.\nEyephone will use system-wide information flow analysis and policy enforcement\nto prevent user privacy violations during the incident reportings. A working\nproof-of-concept prototype of Eyephone is implemented. Our results show that\nEyephone allows secure and effective use of smartphones for real-time\nsituational awareness of our national critical infrastructures.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 07:03:05 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Salles-Loustau", "Gabriel", ""], ["Sadhu", "Vidyasagar", ""], ["Pompili", "Dario", ""], ["Zonouz", "Saman", ""], ["Sritapan", "Vincent", ""]]}, {"id": "1907.00435", "submitter": "Janny Zhang", "authors": "Aarash Heydari, Janny Zhang, Shaan Appel, Xinyi Wu, Gireeja Ranade", "title": "YouTube Chatter: Understanding Online Comments Discourse on\n  Misinformative and Political YouTube Videos", "comments": "32 pages, 23 figures. Primary contributors: Aarash Heydari and Janny\n  Zhang. These authors contributed equally to the work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct a preliminary analysis of comments on political YouTube content\ncontaining misinformation in comparison to comments on trustworthy or\napolitical videos, labelling the bias and factual ratings of our channels\naccording to Media Bias Fact Check where applicable. One of our most\ninteresting discoveries is that especially-polarized or misinformative\npolitical channels (Left-Bias, Right-Bias, PragerU, Conspiracy-Pseudoscience,\nand Questionable Source) generate 7.5x more comments per view and 10.42x more\nreplies per view than apolitical or Pro-Science channels; in particular,\nConspiracy-Pseudoscience and Questionable Sources generate 8.3x more comments\nper view and 11.0x more replies per view than apolitical and Pro-Science\nchannels. We also compared average thread lengths, average comment lengths, and\nprofanity rates across channels, and present simple machine learning\nclassifiers for predicting the bias category of a video based on these\nstatistics.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 19:14:17 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 18:04:08 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Heydari", "Aarash", ""], ["Zhang", "Janny", ""], ["Appel", "Shaan", ""], ["Wu", "Xinyi", ""], ["Ranade", "Gireeja", ""]]}, {"id": "1907.00488", "submitter": "Jaimie Murdock", "authors": "Jaimie Murdock", "title": "Topic Modeling the Reading and Writing Behavior of Information Foragers", "comments": "Accepted Ph.D. dissertation, Indiana University, Informatics (Complex\n  Systems) and Cognitive Science, June 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.DL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The general problem of \"information foraging\" in an environment about which\nagents have incomplete information has been explored in many fields, including\ncognitive psychology, neuroscience, economics, finance, ecology, and computer\nscience. In all of these areas, the searcher aims to enhance future performance\nby surveying enough of existing knowledge to orient themselves in the\ninformation space. Individuals can be viewed as conducting a cognitive search\nin which they must balance exploration of ideas that are novel to them against\nexploitation of knowledge in domains in which they are already expert.\n  In this dissertation, I present several case studies that demonstrate how\nreading and writing behaviors interact to construct personal knowledge bases.\nThese studies use LDA topic modeling to represent the information environment\nof the texts each author read and wrote. Three studies revolve around Charles\nDarwin. Darwin left detailed records of every book he read for 23 years, from\ndisembarking from the H.M.S. Beagle to just after publication of The Origin of\nSpecies. Additionally, he left copies of his drafts before publication. I\ncharacterize his reading behavior, then show how that reading behavior\ninteracted with the drafts and subsequent revisions of The Origin of Species,\nand expand the dataset to include later readings and writings. Then, through a\nstudy of Thomas Jefferson's correspondence, I expand the study to non-book\ndata. Finally, through an examination of neuroscience citation data, I move\nfrom individual behavior to collective behavior in constructing an information\nenvironment. Together, these studies reveal \"the interplay between individual\nand collective phenomena where innovation takes place\" (Tria et al. 2014).\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 22:40:37 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Murdock", "Jaimie", ""]]}, {"id": "1907.00498", "submitter": "Evangelos Pournaras", "authors": "Evangelos Pournaras", "title": "Proof of Witness Presence: Blockchain Consensus for Augmented Democracy\n  in Smart Cities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart Cities evolve into complex and pervasive urban environments with a\ncitizens' mandate to meet sustainable development goals. Repositioning\ndemocratic values of citizens' choices in these complex ecosystems has turned\nout to be imperative in an era of social media filter bubbles, fake news and\nopportunities for manipulating electoral results with such means. This paper\nintroduces a new paradigm of augmented democracy that promises actively\nengaging citizens in a more informed decision-making augmented into public\nurban space. The proposed concept is inspired by a digital revive of the\nAncient Agora of Athens, an arena of public discourse, a Polis where citizens\nassemble to actively deliberate and collectively decide about public matters.\nThe core contribution of the proposed paradigm is the concept of proving\nwitness presence: making decision-making subject of providing secure evidence\nand testifying for choices made in the physical space. This paper shows how the\nchallenge of proving witness presence can be tackled with blockchain consensus\nto empower citizens' trust and overcome security vulnerabilities of GPS\nlocalization. Moreover, a novel platform for collective decision-making and\ncrowd-sensing in urban space is introduced: Smart Agora. It is shown how\nreal-time collective measurements over citizens' choices can be made in a fully\ndecentralized and privacy-preserving way. Witness presence is tested by\ndeploying a decentralized system for crowd-sensing the sustainable use of\ntransport means. Furthermore, witness presence of cycling risk is validated\nusing official accident data from public authorities, which are compared\nagainst wisdom of the crowd. The paramount role of dynamic consensus,\nself-governance and ethically aligned artificial intelligence in the augmented\ndemocracy paradigm is outlined.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 23:46:30 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 19:24:32 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2020 22:07:40 GMT"}, {"version": "v4", "created": "Wed, 8 Jul 2020 22:48:11 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Pournaras", "Evangelos", ""]]}, {"id": "1907.00510", "submitter": "Amir Karami", "authors": "Amir Karami, Suzanne C. Swan, Cynthia Nicole White, Kayla Ford", "title": "Hidden in Plain Sight For Too Long: Using Text Mining Techniques to\n  Shine a Light on Workplace Sexism and Sexual Harassment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: The goal of this study is to understand how people experience\nsexism and sexual harassment in the workplace by discovering themes in 2,362\nexperiences posted on the Everyday Sexism Project's website everydaysexism.com.\nMethod: This study used both quantitative and qualitative methods. The\nquantitative method was a computational framework to collect and analyze a\nlarge number of workplace sexual harassment experiences. The qualitative method\nwas the analysis of the topics generated by a text mining method. Results:\nTwenty-three topics were coded and then grouped into three overarching themes\nfrom the sex discrimination and sexual harassment literature. The Sex\nDiscrimination theme included experiences in which women were treated\nunfavorably due to their sex, such as being passed over for promotion, denied\nopportunities, paid less than men, and ignored or talked over in meetings. The\nSex Discrimination and Gender harassment theme included stories about sex\ndiscrimination and gender harassment, such as sexist hostility behaviors\nranging from insults and jokes invoking misogynistic stereotypes to bullying\nbehavior. The last theme, Unwanted Sexual Attention, contained stories\ndescribing sexual comments and behaviors used to degrade women. Unwanted\ntouching was the highest weighted topic, indicating how common it was for\nwebsite users to endure being touched, hugged or kissed, groped, and grabbed.\nConclusions: This study illustrates how researchers can use automatic processes\nto go beyond the limits of traditional research methods and investigate\nnaturally occurring large scale datasets on the internet to achieve a better\nunderstanding of everyday workplace sexism experiences.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 01:48:49 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Karami", "Amir", ""], ["Swan", "Suzanne C.", ""], ["White", "Cynthia Nicole", ""], ["Ford", "Kayla", ""]]}, {"id": "1907.00782", "submitter": "Jun Zhao", "authors": "Ning Wang, Xiaokui Xiao, Yin Yang, Jun Zhao, Siu Cheung Hui, Hyejin\n  Shin, Junbum Shin, Ge Yu", "title": "Collecting and Analyzing Multidimensional Data with Local Differential\n  Privacy", "comments": "12-Page Full Paper in Proceedings of the 2019 IEEE International\n  Conference on Data Engineering (ICDE). arXiv admin note: text overlap with\n  arXiv:1606.05053", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local differential privacy (LDP) is a recently proposed privacy standard for\ncollecting and analyzing data, which has been used, e.g., in the Chrome\nbrowser, iOS and macOS. In LDP, each user perturbs her information locally, and\nonly sends the randomized version to an aggregator who performs analyses, which\nprotects both the users and the aggregator against private information leaks.\nAlthough LDP has attracted much research attention in recent years, the\nmajority of existing work focuses on applying LDP to complex data and/or\nanalysis tasks. In this paper, we point out that the fundamental problem of\ncollecting multidimensional data under LDP has not been addressed sufficiently,\nand there remains much room for improvement even for basic tasks such as\ncomputing the mean value over a single numeric attribute under LDP. Motivated\nby this, we first propose novel LDP mechanisms for collecting a numeric\nattribute, whose accuracy is at least no worse (and usually better) than\nexisting solutions in terms of worst-case noise variance. Then, we extend these\nmechanisms to multidimensional data that can contain both numeric and\ncategorical attributes, where our mechanisms always outperform existing\nsolutions regarding worst-case noise variance. As a case study, we apply our\nsolutions to build an LDP-compliant stochastic gradient descent algorithm\n(SGD), which powers many important machine learning tasks. Experiments using\nreal datasets confirm the effectiveness of our methods, and their advantages\nover existing solutions.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 13:33:43 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Wang", "Ning", ""], ["Xiao", "Xiaokui", ""], ["Yang", "Yin", ""], ["Zhao", "Jun", ""], ["Hui", "Siu Cheung", ""], ["Shin", "Hyejin", ""], ["Shin", "Junbum", ""], ["Yu", "Ge", ""]]}, {"id": "1907.00850", "submitter": "Bruno Lepri", "authors": "Andrea Beretta, Massimo Zancanaro, Bruno Lepri", "title": "Following wrong suggestions: self-blame in human and computer scenarios", "comments": "To be published in the Proceedings of IFIP Conference on\n  Human-Computer Interaction (INTERACT)2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the specific experience of following a suggestion by\nan intelligent machine that has a wrong outcome and the emotions people feel.\nBy adopting a typical task employed in studies on decision-making, we presented\nparticipants with two scenarios in which they follow a suggestion and have a\nwrong outcome by either an expert human being or an intelligent machine. We\nfound a significant decrease in the perceived responsibility on the wrong\nchoice when the machine offers the suggestion. At present, few studies have\ninvestigated the negative emotions that could arise from a bad outcome after\nfollowing the suggestion given by an intelligent system, and how to cope with\nthe potential distrust that could affect the long-term use of the system and\nthe cooperation. This preliminary research has implications in the study of\ncooperation and decision making with intelligent machines. Further research may\naddress how to offer the suggestion in order to better cope with user's\nself-blame.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 15:17:03 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Beretta", "Andrea", ""], ["Zancanaro", "Massimo", ""], ["Lepri", "Bruno", ""]]}, {"id": "1907.00934", "submitter": "Lukas Daniel Klausner", "authors": "Angelika Adensamer and Lukas Daniel Klausner", "title": "Ich wei{\\ss}, was du n\\\"achsten Sommer getan haben wirst: Predictive\n  Policing in \\\"Osterreich", "comments": "14 pages, in German", "journal-ref": "juridikum 3/2019, 2019, 419-431", "doi": "10.33196/juridikum201903041901", "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive policing is a data-based, predictive analytical technique used in\nlaw enforcement. In this paper, we give an overview of the current situation in\nAustria and discuss technical, sociopolitical and legal questions raised by the\nuse of PP, such as the lack of awareness of discriminatory structures in\nsociety, the biases in data underlying PP and the lack of reflection on the\nbasic premises and feedback mechanisms of PP. Violations of fundamental rights\nwithout cause are not allowed by the Austrian Code of Criminal Procedure\n(Strafproze{\\ss}ordnung, StPO), the Security Police Act\n(Sicherheitspolizeigesetz, SPG) or the Act concerning Police Protection of the\nState (Polizeiliches Staatsschutzgesetz, PStSG); the principle of allowing\npolice intervention only on the basis of concrete threats or suspicion must\nremain absolute. Considering the numerous problems (not least from the point of\nview of legal policy), we conclude that the use of PP should be eschewed and\nthat resources and planning should instead be focussed on solving the social\nproblems which actually cause crime.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 17:13:56 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 12:06:49 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Adensamer", "Angelika", ""], ["Klausner", "Lukas Daniel", ""]]}, {"id": "1907.01040", "submitter": "Niki Kilbertus", "authors": "Niki Kilbertus, Philip J. Ball, Matt J. Kusner, Adrian Weller, Ricardo\n  Silva", "title": "The Sensitivity of Counterfactual Fairness to Unmeasured Confounding", "comments": "published at UAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal approaches to fairness have seen substantial recent interest, both\nfrom the machine learning community and from wider parties interested in\nethical prediction algorithms. In no small part, this has been due to the fact\nthat causal models allow one to simultaneously leverage data and expert\nknowledge to remove discriminatory effects from predictions. However, one of\nthe primary assumptions in causal modeling is that you know the causal graph.\nThis introduces a new opportunity for bias, caused by misspecifying the causal\nmodel. One common way for misspecification to occur is via unmeasured\nconfounding: the true causal effect between variables is partially described by\nunobserved quantities. In this work we design tools to assess the sensitivity\nof fairness measures to this confounding for the popular class of non-linear\nadditive noise models (ANMs). Specifically, we give a procedure for computing\nthe maximum difference between two counterfactually fair predictors, where one\nhas become biased due to confounding. For the case of bivariate confounding our\ntechnique can be swiftly computed via a sequence of closed-form updates. For\nmultivariate confounding we give an algorithm that can be efficiently solved\nvia automatic differentiation. We demonstrate our new sensitivity analysis\ntools in real-world fairness scenarios to assess the bias arising from\nconfounding.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 19:47:40 GMT"}], "update_date": "2019-08-17", "authors_parsed": [["Kilbertus", "Niki", ""], ["Ball", "Philip J.", ""], ["Kusner", "Matt J.", ""], ["Weller", "Adrian", ""], ["Silva", "Ricardo", ""]]}, {"id": "1907.01463", "submitter": "Matthew McDermott", "authors": "Matthew B.A. McDermott (1), Shirly Wang (2), Nikki Marinsek (3),\n  Rajesh Ranganath (4), Marzyeh Ghassemi (2 and 5), Luca Foschini (3) ((1)\n  Massachusetts Institute of Technology, (2) University of Toronto, (3)\n  Evidation Health, Inc., (4) New York University, (5) Vector Institute)", "title": "Reproducibility in Machine Learning for Health", "comments": "Presented at the ICLR 2019 Reproducibility in Machine Learning\n  Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms designed to characterize, monitor, and intervene\non human health (ML4H) are expected to perform safely and reliably when\noperating at scale, potentially outside strict human supervision. This\nrequirement warrants a stricter attention to issues of reproducibility than\nother fields of machine learning.\n  In this work, we conduct a systematic evaluation of over 100 recently\npublished ML4H research papers along several dimensions related to\nreproducibility. We find that the field of ML4H compares poorly to more\nestablished machine learning fields, particularly concerning data and code\naccessibility. Finally, drawing from success in other fields of science, we\npropose recommendations to data providers, academic publishers, and the ML4H\nresearch community in order to promote reproducible research moving forward.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 15:46:46 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["McDermott", "Matthew B. A.", "", "2 and 5"], ["Wang", "Shirly", "", "2 and 5"], ["Marinsek", "Nikki", "", "2 and 5"], ["Ranganath", "Rajesh", "", "2 and 5"], ["Ghassemi", "Marzyeh", "", "2 and 5"], ["Foschini", "Luca", ""]]}, {"id": "1907.01536", "submitter": "Taha Yasseri", "authors": "Bertie Vidgen and Taha Yasseri", "title": "What, When and Where of petitions submitted to the UK Government during\n  a time of chaos", "comments": "Preprint; under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SI physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In times marked by political turbulence and uncertainty, as well as\nincreasing divisiveness and hyperpartisanship, Governments need to use every\ntool at their disposal to understand and respond to the concerns of their\ncitizens. We study issues raised by the UK public to the Government during\n2015-2017 (surrounding the UK EU-membership referendum), mining public opinion\nfrom a dataset of 10,950 petitions (representing 30.5 million signatures). We\nextract the main issues with a ground-up natural language processing (NLP)\nmethod, latent Dirichlet allocation (LDA). We then investigate their temporal\ndynamics and geographic features. We show that whilst the popularity of some\nissues is stable across the two years, others are highly influenced by external\nevents, such as the referendum in June 2016. We also study the relationship\nbetween petitions' issues and where their signatories are geographically\nlocated. We show that some issues receive support from across the whole country\nbut others are far more local. We then identify six distinct clusters of\nconstituencies based on the issues which constituents sign. Finally, we\nvalidate our approach by comparing the petitions' issues with the top issues\nreported in Ipsos MORI survey data. These results show the huge power of\ncomputationally analyzing petitions to understand not only what issues citizens\nare concerned about but also when and from where.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 17:40:40 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Vidgen", "Bertie", ""], ["Yasseri", "Taha", ""]]}, {"id": "1907.01591", "submitter": "Zachary Pardos", "authors": "Zachary A. Pardos and Weijie Jiang", "title": "Combating the Filter Bubble: Designing for Serendipity in a University\n  Course Recommendation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering based algorithms, including Recurrent Neural Networks\n(RNN), tend towards predicting a perpetuation of past observed behavior. In a\nrecommendation context, this can lead to an overly narrow set of suggestions\nlacking in serendipity and inadvertently placing the user in what is known as a\n\"filter bubble.\" In this paper, we grapple with the issue of the filter bubble\nin the context of a course recommendation system in production at a public\nuniversity. Most universities in the United States encourage students to\nexplore developing interests while simultaneously advising them to adhere to\ncourse taking norms which progress them towards graduation. These competing\nobjectives, and the stakes involved for students, make this context a\nparticularly meaningful one for investigating real-world recommendation\nstrategies. We introduce a novel modification to the skip-gram model applied to\nnine years of historic course enrollment sequences to learn course vector\nrepresentations used to diversify recommendations based on similarity to a\nstudent's specified favorite course. This model, which we call multifactor2vec,\nis intended to improve the semantics of the primary token embedding by also\nlearning embeddings of potentially conflated factors of the token (e.g.,\ninstructor). Our offline testing found this model improved accuracy and recall\non our course similarity and analogy validation sets over a standard skip-gram.\nIncorporating course catalog description text resulted in further improvements.\nWe compare the performance of these models to the system's existing RNN-based\nrecommendations with a user study of undergraduates (N = 70) rating six\ncharacteristics of their course recommendations. Results of the user study show\na dramatic lack of novelty in RNN recommendations and depict the characteristic\ntrade-offs that make serendipity difficult to achieve.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 19:17:43 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Pardos", "Zachary A.", ""], ["Jiang", "Weijie", ""]]}, {"id": "1907.01671", "submitter": "Vivek Singh", "authors": "Vivek K. Singh and Ishaan Singh", "title": "Quantifying Algorithmic Biases over Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms now permeate multiple aspects of human lives and multiple recent\nresults have reported that these algorithms may have biases pertaining to\ngender, race, and other demographic characteristics. The metrics used to\nquantify such biases have still focused on a static notion of algorithms.\nHowever, algorithms evolve over time. For instance, Tay (a conversational bot\nlaunched by Microsoft) was arguably not biased at its launch but quickly became\nbiased, sexist, and racist over time. We suggest a set of intuitive metrics to\nstudy the variations in biases over time and present the results for a case\nstudy for genders represented in images resulting from a Twitter image search\nfor #Nurse and #Doctor over a period of 21 days. Results indicate that biases\nvary significantly over time and the direction of bias could appear to be\ndifferent on different days. Hence, one-shot measurements may not suffice for\nunderstanding algorithmic bias, thus motivating further work on studying biases\nin algorithms over time.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 22:44:12 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Singh", "Vivek K.", ""], ["Singh", "Ishaan", ""]]}, {"id": "1907.01862", "submitter": "Nicolas Kourtellis Ph.D.", "authors": "Costas Iordanou, Nicolas Kourtellis, Juan Miguel Carrascosa, Claudio\n  Soriente, Ruben Cuevas, Nikolaos Laoutaris", "title": "Beyond content analysis: Detecting targeted ads via distributed counting", "comments": "14 pages, 5 Figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to check whether an online advertisement has been targeted is\nessential for resolving privacy controversies and implementing in practice data\nprotection regulations like GDPR, CCPA, and COPPA. In this paper we describe\nthe design, implementation, and deployment of an advertisement auditing system\ncalled iWnder that uses crowdsourcing to reveal in real time whether a display\nadvertisement has been targeted or not. Crowdsourcing simplifies the detection\nof targeted advertising, but requires reporting to a central repository the\nimpressions seen by different users, thereby jeopardising their privacy. We\nbreak this deadlock with a privacy preserving data sharing protocol that allows\niWnder to compute global statistics required to detect targeting, while keeping\nthe advertisements seen by individual users and their browsing history private.\nWe conduct a simulation study to explore the effect of different parameters and\na live validation to demonstrate the accuracy of our approach. Unlike previous\nsolutions, iWnder can even detect indirect targeting, i.e., marketing campaigns\nthat promote a product or service whose description bears no semantic overlap\nwith its targeted audience.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 11:45:12 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 16:15:04 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Iordanou", "Costas", ""], ["Kourtellis", "Nicolas", ""], ["Carrascosa", "Juan Miguel", ""], ["Soriente", "Claudio", ""], ["Cuevas", "Ruben", ""], ["Laoutaris", "Nikolaos", ""]]}, {"id": "1907.01874", "submitter": "Martin Geier", "authors": "Martin Geier (1) and Samarjit Chakraborty (1) ((1) Technical\n  University of Munich)", "title": "Challenges in IT Operations Management at a German University Chair --\n  Ten Years in Retrospect", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last two decades, the majority of German universities adopted\nvarious characteristics of the prevailing North-American academic system,\nresulting in significant changes in several key areas that include, e.g., both\nteaching and research. The universities' internal organizational structures,\nhowever, still follow a traditional, decentralized scheme implementing an\nadditional organizational level -- the Chair -- effectively a \"mini department\"\nwith dedicated staff, budget and infrastructure. Although the Technical\nUniversity of Munich (TUM) has been establishing a more centralized scheme for\nmany administrative tasks over the past decade, the transition from its\ndistributed to a centralized information technology (IT) administration and\ninfrastructure is still an ongoing process. In case of the authors' chair, this\nmigration so far included handing over all network-related operations to the\njoint compute center, consolidating the Chair's legacy server system in terms\nof both hardware architectures and operating systems and, lately, moving\nselected services to replacements operated by Department or University. With\nrequirements, individuals and organizations constantly shifting, this process,\nhowever, is neither close to completion nor particularly unique to TUM. In this\npaper, we will thus share our experiences w.r.t. this IT migration as we\nbelieve both that many of the other German universities might be facing similar\nchallenges and that, in the future, North-American universities - currently not\nimplementing the chair layer and instead relying on a centralized IT\ninfrastructure - could need a more decentralized solution. Hoping that both\nbenefit from this journey, we thus present the design, commissioning and\nevolution of our infrastructure.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 17:42:17 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Geier", "Martin", ""], ["Chakraborty", "Samarjit", ""]]}, {"id": "1907.01921", "submitter": "Justin Edwards", "authors": "Allison Perrone, Justin Edwards", "title": "Chatbots as Unwitting Actors", "comments": null, "journal-ref": null, "doi": "10.1145/3342775.3342799", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chatbots are popular for both task-oriented conversations and unstructured\nconversations with web users. Several different approaches to creating comedy\nand art exist across the field of computational creativity. Despite the\npopularity and ease of use of chatbots, there have not been any attempts by\nartists or comedians to use these systems for comedy performances. We present\ntwo initial attempts to do so from our comedy podcast and call for future work\ntoward both designing chatbots for performance and for performing alongside\nchatbots.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 13:10:10 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Perrone", "Allison", ""], ["Edwards", "Justin", ""]]}, {"id": "1907.02014", "submitter": "Sonam Damani", "authors": "Nitya Raviprakash, Sonam Damani, Ankush Chatterjee, Meghana Joshi,\n  Puneet Agrawal", "title": "Using AI for Economic Upliftment of Handicraft Industry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The handicraft industry is a strong pillar of Indian economy which provides\nlarge-scale employment opportunities to artisans in rural and underprivileged\ncommunities. However, in this era of globalization, diverse modern designs have\nrendered traditional designs old and monotonous, causing an alarming decline of\nhandicraft sales. For this age-old industry to survive the global competition,\nit is imperative to integrate contemporary designs with Indian handicrafts. In\nthis paper, we use novel AI techniques to generate contemporary designs for two\npopular Indian handicrafts - Ikat and Block Print. These techniques were\nsuccessfully employed by communities across India to manufacture and sell\nproducts with greater appeal and revenue. The designs are evaluated to be\nsignificantly more likeable and marketable than the current designs used by\nartisans.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 07:33:42 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Raviprakash", "Nitya", ""], ["Damani", "Sonam", ""], ["Chatterjee", "Ankush", ""], ["Joshi", "Meghana", ""], ["Agrawal", "Puneet", ""]]}, {"id": "1907.02099", "submitter": "Jos\\'e Manuel Dos Santos", "authors": "J. M. D. S. Dos Santos, A. P. Silveira, A. E. S. Trocado", "title": "GeoGebra e situa\\c{c}\\~oes que envolvem modela\\c{c}\\~ao numa abordagem\n  STEAM", "comments": "in Portuguese", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.HO cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In order to implement a STEAM approach including the use of technology,\nnamely the use of interactive mathematics software GeoGebra, in mathematics\nclasses, in the lusophone space, the materials presented here were conceived,\nto be implemented in a first phase among teachers. Later, with the necessary\nadaptations, these tasks will be applied to the students. The tasks deal with\nmodeling situations, in two- and three-dimensional geometric problems, in order\nto apply GeoGebra software in its analysis to illustrate its capabilities. The\ndifferent windows of this software are used, namely the 2D and 3D windows, CAS\nwindow, spreadsheet and extra two dimensional windows in order to study cutting\nplanes in solids and some surfaces. The tasks are presented so that any user,\nregardless of the degree of knowledge they have of the software, can follow\nthem, being supported in scripts with some indications of the tools and\ncommands to use. Designed for the teaching and learning of Mathematics, from a\nSTEAM approach, these tasks allow connections with other Sciences and the Arts,\nand allow the development of projects using and consolidating relevant\nmathematical contents. These tasks are part of the proposals of activities of\nthe participants of the Training Courses for Trainers in GeoGebra for\nPortuguese Speaking Countries, which from 2019 have an impact on the STEAM\napproach. These courses are carried out with the high sponsorship of the\nOrganization of Ibero-American States for Education, Science and Culture (OEI).\nGiven the interest that the tasks have for the users of the Iberian space, as\nwell as their dissemination at a global level, the materials initially\ndeveloped in Portuguese language will be adapted for Spanish and English\nspeakers.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 18:50:02 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Santos", "J. M. D. S. Dos", ""], ["Silveira", "A. P.", ""], ["Trocado", "A. E. S.", ""]]}, {"id": "1907.02106", "submitter": "Rafael S. Gon\\c{c}alves", "authors": "Rafael S. Gon\\c{c}alves, Matthew Horridge, Rui Li, Yu Liu, Mark A.\n  Musen, Csongor I. Nyulas, Evelyn Obamos, Dhananjay Shrouty, and David Temple", "title": "Use of OWL and Semantic Web Technologies at Pinterest", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-30796-7_26", "report-no": null, "categories": "cs.CL cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pinterest is a popular Web application that has over 250 million active\nusers. It is a visual discovery engine for finding ideas for recipes, fashion,\nweddings, home decoration, and much more. In the last year, the company adopted\nSemantic Web technologies to create a knowledge graph that aims to represent\nthe vast amount of content and users on Pinterest, to help both content\nrecommendation and ads targeting. In this paper, we present the engineering of\nan OWL ontology---the Pinterest Taxonomy---that forms the core of Pinterest's\nknowledge graph, the Pinterest Taste Graph. We describe modeling choices and\nenhancements to WebProt\\'eg\\'e that we used for the creation of the ontology.\nIn two months, eight Pinterest engineers, without prior experience of OWL and\nWebProt\\'eg\\'e, revamped an existing taxonomy of noisy terms into an OWL\nontology. We share our experience and present the key aspects of our work that\nwe believe will be useful for others working in this area.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 18:58:49 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Gon\u00e7alves", "Rafael S.", ""], ["Horridge", "Matthew", ""], ["Li", "Rui", ""], ["Liu", "Yu", ""], ["Musen", "Mark A.", ""], ["Nyulas", "Csongor I.", ""], ["Obamos", "Evelyn", ""], ["Shrouty", "Dhananjay", ""], ["Temple", "David", ""]]}, {"id": "1907.02142", "submitter": "Mohammad Mannan", "authors": "Suzan Ali, Tousif Osman, Mohammad Mannan, Amr Youssef", "title": "On Privacy Risks of Public WiFi Captive Portals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open access WiFi hotspots are widely deployed in many public places,\nincluding restaurants, parks, coffee shops, shopping malls, trains, airports,\nhotels, and libraries. While these hotspots provide an attractive option to\nstay connected, they may also track user activities and share user/device\ninformation with third-parties, through the use of trackers in their captive\nportal and landing websites. In this paper, we present a comprehensive privacy\nanalysis of 67 unique public WiFi hotspots located in Montreal, Canada, and\nshed some light on the web tracking and data collection behaviors of these\nhotspots. Our study reveals the collection of a significant amount of\nprivacy-sensitive personal data through the use of social login (e.g., Facebook\nand Google) and registration forms, and many instances of tracking activities,\nsometimes even before the user accepts the hotspot's privacy and terms of\nservice policies. Most hotspots use persistent third-party tracking cookies\nwithin their captive portal site; these cookies can be used to follow the\nuser's browsing behavior long after the user leaves the hotspots, e.g., up to\n20 years. Additionally, several hotspots explicitly share (sometimes via HTTP)\nthe collected personal and unique device information with many third-party\ntracking domains.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 21:46:33 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Ali", "Suzan", ""], ["Osman", "Tousif", ""], ["Mannan", "Mohammad", ""], ["Youssef", "Amr", ""]]}, {"id": "1907.02227", "submitter": "Anhong Guo", "authors": "Anhong Guo, Ece Kamar, Jennifer Wortman Vaughan, Hanna Wallach,\n  Meredith Ringel Morris", "title": "Toward Fairness in AI for People with Disabilities: A Research Roadmap", "comments": "ACM ASSETS 2019 Workshop on AI Fairness for People with Disabilities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI technologies have the potential to dramatically impact the lives of people\nwith disabilities (PWD). Indeed, improving the lives of PWD is a motivator for\nmany state-of-the-art AI systems, such as automated speech recognition tools\nthat can caption videos for people who are deaf and hard of hearing, or\nlanguage prediction algorithms that can augment communication for people with\nspeech or cognitive disabilities. However, widely deployed AI systems may not\nwork properly for PWD, or worse, may actively discriminate against them. These\nconsiderations regarding fairness in AI for PWD have thus far received little\nattention. In this position paper, we identify potential areas of concern\nregarding how several AI technology categories may impact particular disability\nconstituencies if care is not taken in their design, development, and testing.\nWe intend for this risk assessment of how various classes of AI might interact\nwith various classes of disability to provide a roadmap for future research\nthat is needed to gather data, test these hypotheses, and build more inclusive\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 05:29:49 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 18:39:41 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Guo", "Anhong", ""], ["Kamar", "Ece", ""], ["Vaughan", "Jennifer Wortman", ""], ["Wallach", "Hanna", ""], ["Morris", "Meredith Ringel", ""]]}, {"id": "1907.02275", "submitter": "Nuno Macedo", "authors": "Nuno Macedo and Alcino Cunha and Jos\\'e Pereira and Renato Carvalho\n  and Ricardo Silva and Ana C. R. Paiva and Miguel S. Ramalho and Daniel Silva", "title": "Sharing and Learning Alloy on the Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Alloy4Fun, a web application that enables online editing and\nsharing of Alloy models and instances, to be used mainly in an educational\ncontext. By introducing the notion of secret paragraphs and commands in the\nmodels, it also allows the distribution and automatic evaluation of simple\nspecification challenges, a useful mechanism that enables students to learn\nrelational logic at their own pace. Alloy4Fun stores all versions of shared and\nanalyzed models, as well as derivation trees that depict how those models\nevolved over time: this wealth of information can be mined by researchers or\ntutors to identify, for example, learning breakdowns in the class or typical\nmistakes made by students and other Alloy users. A beta version of Alloy4Fun\nwas already used in two formal methods courses, and we present some results of\nthis preliminary evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 08:35:32 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Macedo", "Nuno", ""], ["Cunha", "Alcino", ""], ["Pereira", "Jos\u00e9", ""], ["Carvalho", "Renato", ""], ["Silva", "Ricardo", ""], ["Paiva", "Ana C. R.", ""], ["Ramalho", "Miguel S.", ""], ["Silva", "Daniel", ""]]}, {"id": "1907.02434", "submitter": "Dionysis Zindros", "authors": "Dimitris Karakostas and Aggelos Kiayias and Christos Nasikas and\n  Dionysis Zindros", "title": "Cryptocurrency Egalitarianism: A Quantitative Approach", "comments": "29 pages, 4 figures, Tokenomics 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the invention of Bitcoin one decade ago, numerous cryptocurrencies have\nsprung into existence. Among these, proof-of-work is the most common mechanism\nfor achieving consensus, whilst a number of coins have adopted\n\"ASIC-resistance\" as a desirable property, claiming to be more \"egalitarian,\"S\nwhere egalitarianism refers to the power of each coin to participate in the\ncreation of new coins. While proof-of-work consensus dominates the space,\nseveral new cryptocurrencies employ alternative consensus, such as\nproof-of-stake in which block minting opportunities are based on monetary\nownership. A core criticism of proof-of-stake revolves around it being less\negalitarian by making the rich richer, as opposed to proof-of-work in which\neveryone can contribute equally according to their computational power. In this\npaper, we give the first quantitative definition of a cryptocurrency's\n\\emph{egalitarianism}. Based on our definition, we measure the egalitarianism\nof popular cryptocurrencies that (may or may not) employ ASIC-resistance, among\nthem Bitcoin, Ethereum, Litecoin, and Monero. Our simulations show, as\nexpected, that ASIC-resistance increases a cryptocurrency's egalitarianism. We\nalso measure the egalitarianism of a stake-based protocol, Ouroboros, and a\nhybrid proof-of-stake/proof-of-work cryptocurrency, Decred. We show that\nstake-based cryptocurrencies, under correctly selected parameters, can be\nperfectly egalitarian, perhaps contradicting folklore belief.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 14:52:56 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Karakostas", "Dimitris", ""], ["Kiayias", "Aggelos", ""], ["Nasikas", "Christos", ""], ["Zindros", "Dionysis", ""]]}, {"id": "1907.02480", "submitter": "Salvatore Vilella", "authors": "Salvatore Vilella, Daniela Paolotti, Giancarlo Ruffo and Leo Ferres", "title": "News and the city: understanding online press consumption patterns\n  through mobile data", "comments": null, "journal-ref": "EPJ Data Sci. 9, 10 (2020).\n  https://doi.org/10.1140/epjds/s13688-020-00228-9", "doi": "10.1140/s13688-020-00228-9", "report-no": null, "categories": "cs.CY physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The always increasing mobile connectivity affects every aspect of our daily\nlives, including how and when we keep ourselves informed and consult news\nmedia. By studying a DPI (deep packet inspection) dataset, provided by one of\nthe major Chilean telecommunication companies, we investigate how different\ncohorts of the population of Santiago De Chile consume news media content\nthrough their smartphones. We find that some socio-demographic attributes are\nhighly associated to specific news media consumption patterns. In particular,\neducation and age play a significant role in shaping the consumers behaviour\neven in the digital context, in agreement with a large body of literature on\noff-line media distribution channels.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 16:25:37 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 18:23:04 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Vilella", "Salvatore", ""], ["Paolotti", "Daniela", ""], ["Ruffo", "Giancarlo", ""], ["Ferres", "Leo", ""]]}, {"id": "1907.02956", "submitter": "Graham McDonald", "authors": "Graham McDonald, Craig Macdonald, Iadh Ounis", "title": "The FACTS of Technology-Assisted Sensitivity Review", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At least ninety countries implement Freedom of Information laws that state\nthat government documents must be made freely available, or opened, to the\npublic. However, many government documents contain sensitive information, such\nas personal or confidential information. Therefore, all government documents\nthat are opened to the public must first be reviewed to identify, and protect,\nany sensitive information. Historically, sensitivity review has been a\ncompletely manual process. However, with the adoption of born-digital\ndocuments, such as e-mail, human-only sensitivity review is not practical and\nthere is a need for new technologies to assist human sensitivity reviewers. In\nthis paper, we discuss how issues of fairness, accountability, confidentiality,\ntransparency and safety (FACTS) impact technology-assisted sensitivity review.\nMoreover, we outline some important areas of future FACTS research that will\nneed to be addressed within technology-assisted sensitivity review.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 17:56:25 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["McDonald", "Graham", ""], ["Macdonald", "Craig", ""], ["Ounis", "Iadh", ""]]}, {"id": "1907.03047", "submitter": "Victor Molina", "authors": "Victor Molina, Marta Kersten-Oertel, Tristan Glatard", "title": "A Conceptual Marketplace Model for IoT Generated Personal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We propose a decentralized conceptual marketplace model for IoT generated\npersonal data. Our model is based on a thorough analysis of personal data in a\nmarketplace context, with specific focus on the challenges presented by\ncommercializing IoT generated personal data. Our model introduces a novel\nperspective on the commercialization of personal data for a marketplace context\nvia risk evaluation and a data licensing framework. We have designed our model\nto be centered around protecting the privacy and data rights of data generators\nthrough model components that effectively assess and modify transaction risks,\nand formalize transaction agreements by establishing rights of data use and\naccess between buyer and seller. Our model could serve as a blueprint to inform\nthe implementation of a personal data marketplace that respects privacy and\nownership.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 23:16:37 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Molina", "Victor", ""], ["Kersten-Oertel", "Marta", ""], ["Glatard", "Tristan", ""]]}, {"id": "1907.03206", "submitter": "Ben Moews", "authors": "Ben Moews, Jaime R. Argueta Jr., Antonia Gieschen", "title": "Filaments of crime: Informing policing via thresholded ridge estimation", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": "10.1016/j.dss.2021.113518", "report-no": null, "categories": "stat.AP cs.CY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: We introduce a new method for reducing crime in hot spots and\nacross cities through ridge estimation. In doing so, our goal is to explore the\napplication of density ridges to hot spots and patrol optimization, and to\ncontribute to the policing literature in police patrolling and crime reduction\nstrategies.\n  Methods: We make use of the subspace-constrained mean shift algorithm, a\nrecently introduced approach for ridge estimation further developed in\ncosmology, which we modify and extend for geospatial datasets and hot spot\nanalysis. Our experiments extract density ridges of Part I crime incidents from\nthe City of Chicago during the year 2018 and early 2019 to demonstrate the\napplication to current data.\n  Results: Our results demonstrate nonlinear mode-following ridges in agreement\nwith broader kernel density estimates. Using early 2019 incidents with\npredictive ridges extracted from 2018 data, we create multi-run confidence\nintervals and show that our patrol templates cover around 94% of incidents for\n0.1-mile envelopes around ridges, quickly rising to near-complete coverage. We\nalso develop and provide researchers, as well as practitioners, with a\nuser-friendly and open-source software for fast geospatial density ridge\nestimation.\n  Conclusions: We show that ridges following crime report densities can be used\nto enhance patrolling capabilities. Our empirical tests show the stability of\nridges based on past data, offering an accessible way of identifying routes\nwithin hot spots instead of patrolling epicenters. We suggest further research\ninto the application and efficacy of density ridges for patrolling.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 23:59:22 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Moews", "Ben", ""], ["Argueta", "Jaime R.", "Jr."], ["Gieschen", "Antonia", ""]]}, {"id": "1907.03263", "submitter": "Aaditeshwar Seth", "authors": "Aaditeshwar Seth", "title": "Ensuring Responsible Outcomes from Technology", "comments": "Presented as an invited talk at IEEE COMSNETS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We attempt to make two arguments in this essay. First, through a case study\nof a mobile phone based voice-media service we have been running in rural\ncentral India for more than six years, we describe several implementation\ncomplexities we had to navigate towards realizing our intended vision of\nbringing social development through technology. Most of these complexities\narose in the interface of our technology with society, and we argue that even\nother technology providers can create similar processes to manage this\nsocio-technological interface and ensure intended outcomes from their\ntechnology use. We then build our second argument about how to ensure that the\norganizations behind both market driven technologies and those technologies\nthat are adopted by the state, pay due attention towards responsibly managing\nthe socio-technological interface of their innovations. We advocate for the\ntechnology engineers and researchers who work within these organizations, to\ntake up the responsibility and ensure that their labour leads to making the\nworld a better place especially for the poor and marginalized. We outline\npossible governance structures that can give more voice to the technology\ndevelopers to push their organizations towards ensuring that responsible\noutcomes emerge from their technology. We note that the examples we use to\nbuild our arguments are limited to contemporary information and communication\ntechnology (ICT) platforms used directly by end-users to share content with one\nanother, and hence our argument may not generalize to other ICTs in a\nstraightforward manner.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 09:55:20 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Seth", "Aaditeshwar", ""]]}, {"id": "1907.03458", "submitter": "Andreas Nautsch", "authors": "Andreas Nautsch, Catherine Jasserand, Els Kindt, Massimiliano Todisco,\n  Isabel Trancoso, Nicholas Evans", "title": "The GDPR & Speech Data: Reflections of Legal and Technology Communities,\n  First Steps towards a Common Understanding", "comments": null, "journal-ref": "Proc. Interspeech 2019", "doi": null, "report-no": null, "categories": "eess.AS cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Privacy preservation and the protection of speech data is in high demand, not\nleast as a result of recent regulation, e.g. the General Data Protection\nRegulation (GDPR) in the EU. While there has been a period with which to\nprepare for its implementation, its implications for speech data is poorly\nunderstood. This assertion applies to both the legal and technology\ncommunities, and is hardly surprising since there is no universal definition of\n'privacy', let alone a clear understanding of when or how the GDPR applies to\nthe capture, storage and processing of speech data. In aiming to initiate the\ndiscussion that is needed to establish a level of harmonisation that is thus\nfar lacking, this contribution presents some reflections of both legal and\ntechnology communities on the implications of the GDPR as regards speech data.\nThe article outlines the need for taxonomies at the intersection of speech\ntechnology and data privacy - a discussion that is still very much in its\ninfancy - and describes the ways to safeguards and priorities for future\nresearch. In being agnostic to any specific application, the treatment should\nbe of interest to the speech communication community at large.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 08:42:42 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Nautsch", "Andreas", ""], ["Jasserand", "Catherine", ""], ["Kindt", "Els", ""], ["Todisco", "Massimiliano", ""], ["Trancoso", "Isabel", ""], ["Evans", "Nicholas", ""]]}, {"id": "1907.03639", "submitter": "Danah Boyd", "authors": "danah boyd", "title": "Differential Privacy in the 2020 Decennial Census and the Implications\n  for Available Data Products", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In early 2021, the US Census Bureau will begin releasing statistical tables\nbased on the decennial census conducted in 2020. Because of significant changes\nin the data landscape, the Census Bureau is changing its approach to disclosure\navoidance. The confidentiality of individuals represented \"anonymously\" in\nthese statistical tables will be protected by a \"formal privacy\" technique that\nallows the Bureau to mathematically assess the risk of revealing information\nabout individuals in the released statistical tables. The Bureau's approach is\nan implementation of \"differential privacy,\" and it gives a rigorously\ndemonstrated guaranteed level of privacy protection that traditional methods of\ndisclosure avoidance do not. Given the importance of the Census Bureau's\nstatistical tables to democracy, resource allocation, justice, and research,\nconfusion about what differential privacy is and how it might alter or\neliminate data products has rippled through the community of its data users,\nnamely: demographers, statisticians, and census advocates.\n  The purpose of this primer is to provide context to the Census Bureau's\ndecision to use a technique based on differential privacy and to help data\nusers and other census advocates who are struggling to understand what this\nmathematical tool is, why it matters, and how it will affect the Bureau's data\nproducts.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 14:20:38 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["boyd", "danah", ""]]}, {"id": "1907.03702", "submitter": "Nagesh Chandra Kanth", "authors": "Raghav Lakhotia, Chandra Kanth Nagesh, Krishna Madgula", "title": "Identifying Missing Component in the Bechdel Test Using Principal\n  Component Analysis Method", "comments": "8 pages, 6 images, Published in the Proceedings of International\n  Conference on Machine Learning and Applications (ICMLA), 324 - 331, June\n  2019, Copenhagen, Denmark, Recipient of the Best Paper Award", "journal-ref": "World Academy of Science, Engineering and Technology International\n  Journal of Computer and Systems Engineering Vol:13, No:6, 2019", "doi": "10.5281/zenodo.3299335", "report-no": null, "categories": "cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A lot has been said and discussed regarding the rationale and significance of\nthe Bechdel Score. It became a digital sensation in 2013 when Swedish cinemas\nbegan to showcase the Bechdel test score of a film alongside its rating. The\ntest has drawn criticism from experts and the film fraternity regarding its use\nto rate the female presence in a movie. The pundits believe that the score is\ntoo simplified and the underlying criteria of a film to pass the test must\ninclude 1) at least two women, 2) who have at least one dialogue, 3) about\nsomething other than a man, is egregious. In this research, we have considered\na few more parameters which highlight how we represent females in film, like\nthe number of female dialogues in a movie, dialogue genre, and part of speech\ntags in the dialogue. The parameters were missing in the existing criteria to\ncalculate the Bechdel score. The research aims to analyze 342 movies scripts to\ntest a hypothesis if these extra parameters, above with the current Bechdel\ncriteria, are significant in calculating the female representation score. The\nresult of the Principal Component Analysis method concludes that the female\ndialogue content is a key component and should be considered while measuring\nthe representation of women in a work of fiction.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 15:09:35 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 06:52:51 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Lakhotia", "Raghav", ""], ["Nagesh", "Chandra Kanth", ""], ["Madgula", "Krishna", ""]]}, {"id": "1907.03706", "submitter": "Mohamed Khalifa", "authors": "Mohamed Khalifa, Farah Magrabi, and Blanca Gallego", "title": "Developing an Evidence-Based Framework for Grading and Assessment of\n  Predictive Tools for Clinical Decision Support", "comments": "63 pages; 48 pages main text and 15 pages appendix. 6 figures and 12\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: Clinical predictive tools quantify contributions of relevant\npatient characteristics to derive likelihood of diseases or predict clinical\noutcomes. When selecting a predictive tool, for implementation at clinical\npractice or for recommendation in clinical guidelines, clinicians are\nchallenged with an overwhelming and ever growing number of tools, most of which\nhave never been implemented or assessed for comparative effectiveness.\nObjective: To develop a comprehensive framework to Grade and Assess Predictive\ntools (GRASP), and provide clinicians with a standardised, evidence based\nsystem to support their search for and selection of effective tools. Methods: A\nfocused review of literature was conducted to extract criteria along which\ntools should be evaluated. An initial framework was designed and applied to\nassess and grade five tools: LACE Index, Centor Score, Wells Criteria, Modified\nEarly Warning Score, and Ottawa knee rule. After peer review, by expert\nclinicians and healthcare researchers, the framework was revised and the\ngrading of the tools was updated. Results: GRASP framework grades predictive\ntools based on published evidence across three dimensions: 1) Phase of\nevaluation; 2) Level of evidence; and 3) Direction of evidence. The final grade\nof a tool is based on the highest phase of evaluation, supported by the highest\nlevel of positive evidence, or mixed evidence that supports positive\nconclusion. Discussion and Conclusion: the GRASP framework builds on well\nestablished models and widely accepted concepts to provide standardised\nassessment and evidence based grading of predictive tools. Unlike other\nmethods, GRASP is based on the critical appraisal of published evidence\nreporting the predictive tools predictive performance before implementation,\npotential effect and usability during implementation, and their post\nimplementation impact.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 01:00:21 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Khalifa", "Mohamed", ""], ["Magrabi", "Farah", ""], ["Gallego", "Blanca", ""]]}, {"id": "1907.03718", "submitter": "Anubrata Das", "authors": "Anubrata Das, Kunjan Mehta, Matthew Lease", "title": "CobWeb: A Research Prototype for Exploring User Bias in Political\n  Fact-Checking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effect of user bias in fact-checking has not been explored extensively\nfrom a user-experience perspective. We estimate the user bias as a function of\nthe user's perceived reputation of the news sources (e.g., a user with liberal\nbeliefs may tend to trust liberal sources). We build an interface to\ncommunicate the role of estimated user bias in the context of a fact-checking\ntask. We also explore the utility of helping users visualize their detected\nlevel of bias. 80% of the users of our system find that the presence of an\nindicator for user bias is useful in judging the veracity of a political claim.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 16:50:12 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Das", "Anubrata", ""], ["Mehta", "Kunjan", ""], ["Lease", "Matthew", ""]]}, {"id": "1907.03744", "submitter": "Bita Sadeghinasr", "authors": "Bita Sadeghinasr, Armin Akhavan, Qi Wang", "title": "Estimating Commuting Patterns from High Resolution Phone GPS Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of location positioning technologies has generated enormous volumes\nof digital footprints. Translating this big data into understandable trip\npatterns plays a crucial role in estimating infrastructure demands. Previous\nstudies were unable to correctly represent commuting patterns on smaller urban\nscales due to insufficient spatial accuracy. In this study, we investigated if,\nand to what extent, estimated commuting patterns identified from GPS data can\nreplicate the results from transportation surveys and to what degree these\nestimates improve the estimates of trips distribution pattern on census tract\nlevel using higher resolution data. We inferred average daily home-to-work\ntrips by analyzing phone GPS data and compared these patterns with U.S. Census\nsummary tables. We found that trips detected by GPS data highly correlate with\ncensus trips. Furthermore, GPS data is a better proxy for Census tract-pairs\nwith larger numbers of trips.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 18:16:00 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Sadeghinasr", "Bita", ""], ["Akhavan", "Armin", ""], ["Wang", "Qi", ""]]}, {"id": "1907.03827", "submitter": "An Yan", "authors": "An Yan, Bill Howe", "title": "FairST: Equitable Spatial and Temporal Demand Prediction for New\n  Mobility Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Emerging transportation modes, including car-sharing, bike-sharing, and\nride-hailing, are transforming urban mobility but have been shown to reinforce\nsocioeconomic inequities. Spatiotemporal demand prediction models for these new\nmobility regimes must therefore consider fairness as a first-class design\nrequirement. We present FairST, a fairness-aware model for predicting demand\nfor new mobility systems. Our approach utilizes 1D, 2D and 3D convolutions to\nintegrate various urban features and learn the spatial-temporal dynamics of a\nmobility system, but we include fairness metrics as a form of regularization to\nmake the predictions more equitable across demographic groups. We propose two\nnovel spatiotemporal fairness metrics, a region-based fairness gap (RFG) and an\nindividual-based fairness gap (IFG). Both quantify equity in a spatiotemporal\ncontext, but vary by whether demographics are labeled at the region level (RFG)\nor whether population distribution information is available (IFG). Experimental\nresults on real bike share and ride share datasets demonstrate the\neffectiveness of the proposed model: FairST not only reduces the fairness gap\nby more than 80%, but can surprisingly achieve better accuracy than\nstate-of-the-art yet fairness-oblivious methods including LSTMs, ConvLSTMs, and\n3D CNN.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 19:33:16 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Yan", "An", ""], ["Howe", "Bill", ""]]}, {"id": "1907.03834", "submitter": "Daniela Ganelin", "authors": "Daniela Ganelin", "title": "Differences in Online Course Usage and IP Geolocation Bias by Local\n  Economic Profile", "comments": "93 pages Thesis for Master of Engineering at MIT in Electrical\n  Engineering and Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Massive Online Open Courses (MOOCs) have the promise to make\nrigorous higher education accessible to everyone, prior research has shown that\nregistrants tend to come from backgrounds of higher socioeconomic status. In\nthis work, I study geographically granular economic patterns in registration\nfor HarvardX and MITx courses, and in the accuracy of identifying users'\nlocations from their IP addresses. Using ZIP Codes identified by the MaxMind IP\ngeolocation database, I find that per-capita registration rates correlate with\neconomic prosperity and population density. Comparing these ZIP Codes with\nuser-provided mailing addresses, I find evidence of bias in MaxMind\ngeolocation: it makes greater errors, both geographically and economically, for\nusers from more economically distressed areas; it disproportionately geolocates\nusers to prosperous areas; and it underestimates the regressive pattern in MOOC\nregistration. Similar economic biases may affect IP geolocation in other\nacademic, commercial, and legal contexts.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 18:51:52 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Ganelin", "Daniela", ""]]}, {"id": "1907.03841", "submitter": "Mar\\'ia Aurora Mart\\'inez Rey", "authors": "Juan A. Lara, David Lizcano, Mar\\'ia A. Mart\\'inez, Juan Pazos", "title": "The Advent of Technological Singularity: a Formal Metric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Technological Singularity; that is, the possibility of achieving a\nGeneral Artificial Intelligence (AGI) that surpasses human intelligence, is one\nof the vital paradigms of today's humanity. However, until now only opinions\nabout its possibility and/or achievement were issued, therefore, in this work,\na metric is presented, for the first time, to objectively measure the actual\nstate in which the advent of technological singularity is found.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 10:17:39 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Lara", "Juan A.", ""], ["Lizcano", "David", ""], ["Mart\u00ednez", "Mar\u00eda A.", ""], ["Pazos", "Juan", ""]]}, {"id": "1907.03843", "submitter": "Manuel Lopes", "authors": "Pedro Fernandes, Francisco C. Santos, Manuel Lopes", "title": "Norms for Beneficial A.I.: A Computational Analysis of the Societal\n  Value Alignment Problem", "comments": null, "journal-ref": "AI Communications, vol. 33, no. 3-6, pp. 155-171, 2020", "doi": "10.3233/AIC-201502", "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of artificial intelligence (A.I.) based systems is already offering\nsubstantial benefits to the society as a whole. However, these systems may also\nenclose potential conflicts and unintended consequences. Notably, people will\ntend to adopt an A.I. system if it confers them an advantage, at which point\nnon-adopters might push for a strong regulation if that advantage for adopters\nis at a cost for them. Here we propose an agent-based game-theoretical model\nfor these conflicts, where agents may decide to resort to A.I. to use and\nacquire additional information on the payoffs of a stochastic game, striving to\nbring insights from simulation to what has been, hitherto, a mostly\nphilosophical discussion. We frame our results under the current discussion on\nethical A.I. and the conflict between individual and societal gains: the\nsocietal value alignment problem. We test the arising equilibria in the\nadoption of A.I. technology under different norms followed by artificial\nagents, their ensuing benefits, and the emergent levels of wealth inequality.\nWe show that without any regulation, purely selfish A.I. systems will have the\nstrongest advantage, even when a utilitarian A.I. provides significant benefits\nfor the individual and the society. Nevertheless, we show that it is possible\nto develop A.I. systems following human conscious policies that, when\nintroduced in society, lead to an equilibrium where the gains for the adopters\nare not at a cost for non-adopters, thus increasing the overall wealth of the\npopulation and lowering inequality. However, as shown, a self-organised\nadoption of such policies would require external regulation.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 10:18:19 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 18:11:35 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Fernandes", "Pedro", ""], ["Santos", "Francisco C.", ""], ["Lopes", "Manuel", ""]]}, {"id": "1907.03848", "submitter": "Thilo Hagendorff", "authors": "Angela Daly, Thilo Hagendorff, Li Hui, Monique Mann, Vidushi Marda,\n  Ben Wagner, Wei Wang, Saskia Witteborn", "title": "Artificial Intelligence Governance and Ethics: Global Perspectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) is a technology which is increasingly being\nutilised in society and the economy worldwide, and its implementation is\nplanned to become more prevalent in coming years. AI is increasingly being\nembedded in our lives, supplementing our pervasive use of digital technologies.\nBut this is being accompanied by disquiet over problematic and dangerous\nimplementations of AI, or indeed, even AI itself deciding to do dangerous and\nproblematic actions, especially in fields such as the military, medicine and\ncriminal justice. These developments have led to concerns about whether and how\nAI systems adhere, and will adhere to ethical standards. These concerns have\nstimulated a global conversation on AI ethics, and have resulted in various\nactors from different countries and sectors issuing ethics and governance\ninitiatives and guidelines for AI. Such developments form the basis for our\nresearch in this report, combining our international and interdisciplinary\nexpertise to give an insight into what is happening in Australia, China,\nEurope, India and the US.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 07:42:48 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Daly", "Angela", ""], ["Hagendorff", "Thilo", ""], ["Hui", "Li", ""], ["Mann", "Monique", ""], ["Marda", "Vidushi", ""], ["Wagner", "Ben", ""], ["Wang", "Wei", ""], ["Witteborn", "Saskia", ""]]}, {"id": "1907.03869", "submitter": "Roman Yampolskiy", "authors": "Roman V. Yampolskiy", "title": "Unexplainability and Incomprehensibility of Artificial Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainability and comprehensibility of AI are important requirements for\nintelligent systems deployed in real-world domains. Users want and frequently\nneed to understand how decisions impacting them are made. Similarly it is\nimportant to understand how an intelligent system functions for safety and\nsecurity reasons. In this paper, we describe two complementary impossibility\nresults (Unexplainability and Incomprehensibility), essentially showing that\nadvanced AIs would not be able to accurately explain some of their decisions\nand for the decisions they could explain people would not understand some of\nthose explanations.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 21:19:31 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Yampolskiy", "Roman V.", ""]]}, {"id": "1907.03969", "submitter": "Yo Nakawake", "authors": "Yo Nakawake, Kosuke Sato", "title": "Systematic quantitative analyses reveal the folk-zoological knowledge\n  embedded in folktales", "comments": "This document is a preprint. Accepted version of the paper is\n  available at https://www.nature.com/articles/s41599-019-0375-x", "journal-ref": "Palgrave Commun 5, 161 (2019)", "doi": "10.1057/s41599-019-0375-x", "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cultural learning is a unique human capacity essential for a wide range of\nadaptations. Researchers have argued that folktales have the pedagogical\nfunction of transmitting the essential information for the environment. The\nmost important knowledge for foraging and pastoral society is folk-zoological\nknowledge, such as the predator-prey relationship among wild animals, or\nbetween wild and domesticated animals. Here, we analysed the descriptions of\nthe 382 animal folktales using the natural language processing method and\ndescriptive statistics listed in a worldwide tale-type index\n(Aarne-Thompson-Uther type index). Our analyses suggested that first, the\npredator-prey relationship frequently appeared in a co-occurrent animal pair\nwithin a folktale (e.g., cat and mouse or wolf and pig), and second, the motif\nof 'deception', describing the antagonistic behaviour among animals, appeared\nrelatively higher in 'wild and domestic animals' and 'wild animals' than other\ntypes. Furthermore, the motif of 'deception' appeared more frequently in pairs,\ncorresponding to the predator-prey relationship. These results corresponded\nwith the hypothesis that the combination of animal characters and what happens\nin stories represented relationships in the real world. The present study\ndemonstrated that the combination of quantitative methods and qualitative data\nbroaden our understanding of the evolutionary aspects of human cultures.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 03:40:13 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 15:27:00 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Nakawake", "Yo", ""], ["Sato", "Kosuke", ""]]}, {"id": "1907.04002", "submitter": "Yazan Boshmaf", "authors": "Yury Zhauniarovich, Yazan Boshmaf, Husam Al Jawaheri, Mashael Al Sabah", "title": "Characterizing Bitcoin donations to open source software on GitHub", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web-based hosting services for version control, such as GitHub, have made it\neasier for people to develop, share, and donate money to software repositories.\nIn this paper, we study the use of Bitcoin to make donations to open source\nrepositories on GitHub. In particular, we analyze the amount and volume of\ndonations over time, in addition to its relationship to the age and popularity\nof a repository.\n  We scanned over three million repositories looking for donation addresses. We\nthen extracted and analyzed their transactions from Bitcoin's public\nblockchain. Overall, we found a limited adoption of Bitcoin as a payment method\nfor receiving donations, with nearly 44 thousand deposits adding up to only 8.3\nmillion dollars in the last 10 years. We also found weak positive correlation\nbetween the amount of donations in dollars and the popularity of a repository,\nwith highest correlation (r=0.013) associated with number of forks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 06:23:49 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Zhauniarovich", "Yury", ""], ["Boshmaf", "Yazan", ""], ["Jawaheri", "Husam Al", ""], ["Sabah", "Mashael Al", ""]]}, {"id": "1907.04139", "submitter": "Jing Chen", "authors": "Yijie Li, Jing Chen, Bernie Liu", "title": "To save the Environmental degradation: Please pay for ecosystem\n  services!", "comments": "arXiv admin note: substantial text overlap with arXiv:1906.06572", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Project planners should consider the monetary value of ecosystem services in\nthe regional service category and add it to the cost calculation of the project\nas environmental cost. Managers should monitor and regularly maintain the\necological environment within the project area to maintain the benefits of\nenvironmental costs.With regard to the problem of predicting how the model\nchanges over time, we use LSTM to realize dynamic evaluation. The analysis that\nhas been made shows that the monetary value of urban ecological services has\nincreased. The environment is still being damaged. Through neural network\nmemory and learning, the problem can be found, and the model can be modified in\ntime.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 14:51:00 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Li", "Yijie", ""], ["Chen", "Jing", ""], ["Liu", "Bernie", ""]]}, {"id": "1907.04245", "submitter": "Zachary Weinberg", "authors": "Arian Akhavan Niaki, Shinyoung Cho, Zachary Weinberg, Nguyen Phong\n  Hoang, Abbas Razaghpanah, Nicolas Christin, Phillipa Gill", "title": "ICLab: A Global, Longitudinal Internet Censorship Measurement Platform", "comments": "To appear in Proceedings of the 41st IEEE Symposium on Security and\n  Privacy (Oakland 2020). San Francisco, CA. May 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.NI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers have studied Internet censorship for nearly as long as attempts\nto censor contents have taken place. Most studies have however been limited to\na short period of time and/or a few countries; the few exceptions have traded\noff detail for breadth of coverage. Collecting enough data for a comprehensive,\nglobal, longitudinal perspective remains challenging. In this work, we present\nICLab, an Internet measurement platform specialized for censorship research. It\nachieves a new balance between breadth of coverage and detail of measurements,\nby using commercial VPNs as vantage points distributed around the world. ICLab\nhas been operated continuously since late 2016. It can currently detect DNS\nmanipulation and TCP packet injection, and overt \"block pages\" however they are\ndelivered. ICLab records and archives raw observations in detail, making\nretrospective analysis with new techniques possible. At every stage of\nprocessing, ICLab seeks to minimize false positives and manual validation.\n  Within 53,906,532 measurements of individual web pages, collected by ICLab in\n2017 and 2018, we observe blocking of 3,602 unique URLs in 60 countries. Using\nthis data, we compare how different blocking techniques are deployed in\ndifferent regions and/or against different types of content. Our longitudinal\nmonitoring pinpoints changes in censorship in India and Turkey concurrent with\npolitical shifts, and our clustering techniques discover 48 previously unknown\nblock pages. ICLab's broad and detailed measurements also expose other forms of\nnetwork interference, such as surveillance and malware injection.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 15:27:42 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 15:47:01 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Niaki", "Arian Akhavan", ""], ["Cho", "Shinyoung", ""], ["Weinberg", "Zachary", ""], ["Hoang", "Nguyen Phong", ""], ["Razaghpanah", "Abbas", ""], ["Christin", "Nicolas", ""], ["Gill", "Phillipa", ""]]}, {"id": "1907.04435", "submitter": "Fabio Miranda", "authors": "Fabio Miranda, Harish Doraiswamy, Marcos Lage, Luc Wilson, Mondrian\n  Hsieh, Claudio T. Silva", "title": "Shadow Accrual Maps: Efficient Accumulation of City-Scale Shadows Over\n  Time", "comments": "Video: https://www.youtube.com/watch?v=LsZv23d1LyM, Data:\n  https://github.com/ViDA-NYU/shadow-accrual-maps", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics (Volume:\n  25, Issue: 3, Mar. 2019)", "doi": "10.1109/TVCG.2018.2802945", "report-no": null, "categories": "cs.GR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale shadows from buildings in a city play an important role in\ndetermining the environmental quality of public spaces. They can be both\nbeneficial, such as for pedestrians during summer, and detrimental, by\nimpacting vegetation and by blocking direct sunlight. Determining the effects\nof shadows requires the accumulation of shadows over time across different\nperiods in a year. In this paper, we propose a simple yet efficient class of\napproach that uses the properties of sun movement to track the changing\nposition of shadows within a fixed time interval. We use this approach to\nextend two commonly used shadowing techniques, shadow maps and ray tracing, and\ndemonstrate the efficiency of our approach. Our technique is used to develop an\ninteractive visual analysis system, Shadow Profiler, targeted at city planners\nand architects that allows them to test the impact of shadows for different\ndevelopment scenarios. We validate the usefulness of this system through case\nstudies set in Manhattan, a dense borough of New York City.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 22:03:40 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Miranda", "Fabio", ""], ["Doraiswamy", "Harish", ""], ["Lage", "Marcos", ""], ["Wilson", "Luc", ""], ["Hsieh", "Mondrian", ""], ["Silva", "Claudio T.", ""]]}, {"id": "1907.04534", "submitter": "Amanda Askell", "authors": "Amanda Askell, Miles Brundage, Gillian Hadfield", "title": "The Role of Cooperation in Responsible AI Development", "comments": "23 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we argue that competitive pressures could incentivize AI\ncompanies to underinvest in ensuring their systems are safe, secure, and have a\npositive social impact. Ensuring that AI systems are developed responsibly may\ntherefore require preventing and solving collective action problems between\ncompanies. We note that there are several key factors that improve the\nprospects for cooperation in collective action problems. We use this to\nidentify strategies to improve the prospects for industry cooperation on the\nresponsible development of AI.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 06:51:04 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Askell", "Amanda", ""], ["Brundage", "Miles", ""], ["Hadfield", "Gillian", ""]]}, {"id": "1907.04911", "submitter": "Michaela Mila", "authors": "Michaela Hardt, Alvin Rajkomar, Gerardo Flores, Andrew Dai, Michael\n  Howell, Greg Corrado, Claire Cui and Moritz Hardt", "title": "Explaining an increase in predicted risk for clinical alerts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much work aims to explain a model's prediction on a static input. We consider\nexplanations in a temporal setting where a stateful dynamical model produces a\nsequence of risk estimates given an input at each time step. When the estimated\nrisk increases, the goal of the explanation is to attribute the increase to a\nfew relevant inputs from the past. While our formal setup and techniques are\ngeneral, we carry out an in-depth case study in a clinical setting. The goal\nhere is to alert a clinician when a patient's risk of deterioration rises. The\nclinician then has to decide whether to intervene and adjust the treatment.\nGiven a potentially long sequence of new events since she last saw the patient,\na concise explanation helps her to quickly triage the alert. We develop methods\nto lift static attribution techniques to the dynamical setting, where we\nidentify and address challenges specific to dynamics. We then experimentally\nassess the utility of different explanations of clinical alerts through expert\nevaluation.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 20:26:43 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Hardt", "Michaela", ""], ["Rajkomar", "Alvin", ""], ["Flores", "Gerardo", ""], ["Dai", "Andrew", ""], ["Howell", "Michael", ""], ["Corrado", "Greg", ""], ["Cui", "Claire", ""], ["Hardt", "Moritz", ""]]}, {"id": "1907.05162", "submitter": "Chico Q. Camargo", "authors": "Chico Q. Camargo, Jonathan Bright, Graham McNeill, Sridhar Raman,\n  Scott A. Hale", "title": "Estimating Traffic Disruption Patterns with Volunteered Geographic\n  Information", "comments": "2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate understanding and forecasting of traffic is a key contemporary\nproblem for policymakers. Road networks are increasingly congested, yet traffic\ndata is often expensive to obtain, making informed policy-making harder. This\npaper explores the extent to which traffic disruption can be estimated from\nstatic features from the volunteered geographic information site OpenStreetMap\n(OSM). We use OSM features as predictors for linear regressions of counts of\ntraffic disruptions and traffic volume at 6,500 points in the road network\nwithin 112 regions of Oxfordshire, UK. We show that more than half the\nvariation in traffic volume and disruptions can be explained with static\nfeatures alone, and use cross-validation and recursive feature elimination to\nevaluate the predictive power and importance of different land use categories.\nFinally, we show that using OSM's granular point of interest data allows for\nbetter predictions than the aggregate categories typically used in studies of\ntransportation and land use.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 12:51:01 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Camargo", "Chico Q.", ""], ["Bright", "Jonathan", ""], ["McNeill", "Graham", ""], ["Raman", "Sridhar", ""], ["Hale", "Scott A.", ""]]}, {"id": "1907.05234", "submitter": "Chainarong Amornbunchornvej", "authors": "Chainarong Amornbunchornvej, Navaporn Surasvadi, Anon Plangprasopchok,\n  and Suttipong Thajchayapong", "title": "Identifying Linear Models in Multi-Resolution Population Data using\n  Minimum Description Length Principle to Predict Household Income", "comments": "This is the accepted manuscript for publication in TKDD. The R\n  package is available at https://github.com/DarkEyes/MRReg", "journal-ref": "ACM Transactions on Knowledge Discovery from Data (TKDD), 15(2),\n  15 (2021)", "doi": "10.1145/3424670", "report-no": null, "categories": "cs.LG cs.CY stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One shirt size cannot fit everybody, while we cannot make a unique shirt that\nfits perfectly for everyone because of resource limitation. This analogy is\ntrue for the policy making. Policy makers cannot establish a single policy to\nsolve all problems for all regions because each region has its own unique\nissue. In the other extreme, policy makers also cannot create a policy for each\nsmall village due to the resource limitation. Would it be better if we can find\na set of largest regions such that the population of each region within this\nset has common issues and we can establish a single policy for them? In this\nwork, we propose a framework using regression analysis and minimum description\nlength (MDL) to find a set of largest areas that have common indicators, which\ncan be used to predict household incomes efficiently. Given a set of household\nfeatures, and a multi-resolution partition that represents administrative\ndivisions, our framework reports a set C* of largest subdivisions that have a\ncommon model for population-income prediction. We formalize a problem of\nfinding C* and propose the algorithm as a solution. We use both simulation\ndatasets as well as a real-world dataset of Thailand's population household\ninformation to demonstrate our framework performance and application. The\nresults show that our framework performance is better than the baseline\nmethods. We show the results of our method can be used to find indicators of\nincome prediction for many areas in Thailand. By increasing these indicator\nvalues, we expect people in these areas to gain more incomes. Hence, the policy\nmakers can plan to establish the policies by using these indicators in our\nresults as a guideline to solve low-income issues. Our framework can be used to\nsupport policy makers to establish policies regarding any other dependent\nvariable beyond incomes in order to combat poverty and other issues.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 03:08:31 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 15:32:25 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 08:39:38 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Amornbunchornvej", "Chainarong", ""], ["Surasvadi", "Navaporn", ""], ["Plangprasopchok", "Anon", ""], ["Thajchayapong", "Suttipong", ""]]}, {"id": "1907.05246", "submitter": "Konstantinos Makantasis", "authors": "Konstantinos Makantasis, Maria Kontorinaki, Ioannis Nikolos", "title": "Deep Reinforcement-Learning-based Driving Policy for Autonomous Road\n  Vehicles", "comments": "19 pages. arXiv admin note: substantial text overlap with\n  arXiv:1905.09046", "journal-ref": null, "doi": "10.1049/iet-its.2019.0249", "report-no": null, "categories": "cs.RO cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work the problem of path planning for an autonomous vehicle that\nmoves on a freeway is considered. The most common approaches that are used to\naddress this problem are based on optimal control methods, which make\nassumptions about the model of the environment and the system dynamics. On the\ncontrary, this work proposes the development of a driving policy based on\nreinforcement learning. In this way, the proposed driving policy makes minimal\nor no assumptions about the environment, since a priori knowledge about the\nsystem dynamics is not required. Driving scenarios where the road is occupied\nboth by autonomous and manual driving vehicles are considered. To the best of\nour knowledge, this is one of the first approaches that propose a reinforcement\nlearning driving policy for mixed driving environments. The derived\nreinforcement learning policy, firstly, is compared against an optimal policy\nderived via dynamic programming, and, secondly, its efficiency is evaluated\nunder realistic scenarios generated by the established SUMO microscopic traffic\nflow simulator. Finally, some initial results regarding the effect of\nautonomous vehicles' behavior on the overall traffic flow are presented.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 11:44:09 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 09:22:25 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Makantasis", "Konstantinos", ""], ["Kontorinaki", "Maria", ""], ["Nikolos", "Ioannis", ""]]}, {"id": "1907.05442", "submitter": "Farig Sadeque", "authors": "Farig Sadeque, Steven Bethard", "title": "Predicting engagement in online social networks: Challenges and\n  opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the introduction of social media, user participation or engagement has\nreceived little research attention. In this survey article, we establish the\nnotion of participation in social media and main challenges that researchers\nmay face while exploring this phenomenon. We surveyed a handful of research\narticles that had been done in this area, and tried to extract, analyze and\nsummarize the techniques performed by the researchers. We classified these\nworks based on our task definitions, and explored the machine learning models\nthat have been used for any kind of participation prediction. We also explored\nthe vast amount of features that have been proven useful, and classified them\ninto categories for better understanding and ease of re-implementation. We have\nfound that the success of a technique mostly depends on the type of the network\nthat has been researched on, and there is no universal machine learning\nalgorithm or feature sets that works reasonably well in all types of social\nmedia. There is a lack of attempts in implementing state-of-the-art machine\nlearning techniques like neural networks, and the possibility of transfer\nlearning and domain adaptation has not been explored.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 18:27:40 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Sadeque", "Farig", ""], ["Bethard", "Steven", ""]]}, {"id": "1907.05447", "submitter": "John Hooker", "authors": "Tae Wan Kim, Thomas Donaldson, John Hooker", "title": "Grounding Value Alignment with Ethical Principles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important step in the development of value alignment (VA) systems in AI is\nunderstanding how values can interrelate with facts. Designers of future VA\nsystems will need to utilize a hybrid approach in which ethical reasoning and\nempirical observation interrelate successfully in machine behavior. In this\narticle we identify two problems about this interrelation that have been\noverlooked by AI discussants and designers. The first problem is that many AI\ndesigners commit inadvertently a version of what has been called by moral\nphilosophers the \"naturalistic fallacy,\" that is, they attempt to derive an\n\"ought\" from an \"is.\" We illustrate when and why this occurs. The second\nproblem is that AI designers adopt training routines that fail fully to\nsimulate human ethical reasoning in the integration of ethical principles and\nfacts. Using concepts of quantified modal logic, we proceed to offer an\napproach that promises to simulate ethical reasoning in humans by connecting\nethical principles on the one hand and propositions about states of affairs on\nthe other.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 18:55:47 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Kim", "Tae Wan", ""], ["Donaldson", "Thomas", ""], ["Hooker", "John", ""]]}, {"id": "1907.05702", "submitter": "Sudhir Routray", "authors": "Sudhir K. Routray, Susanta K. Sarangi, and Abhishek Javali", "title": "Smart Cities: The Hopes and Hypes", "comments": "Conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart cities are being planned for several advanced applications and services\nfor the inhabitants. Smart cities initiative promise many new services which\nare not possible in the traditional city frameworks. In the smart city\nframework, the basic aim is to provide all the essential services through\nsensor based systems which does not need much human intervention. This system\nis designed to operate on its own in a self-organizing manner. Therefore, the\nhopes are really big from the smart cities to enhance the quality of lives and\nthe economy. However, some of the promises in the smart cities are very much\nover hyped. In this article, we analyse the realities of the smart cities and\ntheir practical significances based on the technological aspects of these\nprojects. We also address the false promises that are around which are just the\nhypes. We clarify these hypes with appropriate logical explanations.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 12:38:59 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Routray", "Sudhir K.", ""], ["Sarangi", "Susanta K.", ""], ["Javali", "Abhishek", ""]]}, {"id": "1907.05811", "submitter": "Bernadette Spieler", "authors": "Bernadette Spieler and Vesna Krnjic and Wolfgang Slany", "title": "Girls Create Games: Lessons Learned", "comments": "10 pages, 11 Figures, 13th European Conference on Game Based Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies from all over the world show that more boys than girls play\nvideo games. The numbers are different for mobile gaming apps, where 65% of\nwomen are identified as gamers. Adapting game design activities for academic\npurposes is a widely applied approach at schools or off-school initiatives is\nseen as a promising opportunity for all teenagers to learn to code in an\nentertaining way. This raise the questions do special girls' game-design\npatterns exist, and what can we learn from them? This paper describes a\ngirl-only intervention where girls were asked to create their own games. This\n\"Girls' Coding Week\" was designed as an off-school event and took place during\nsummer 2018 with 13 girls between 11 to 14 years old. To explain the basic\nsteps of programming and to create personalized games, the visual coding app\nPocket Code, an app developed at Graz University of Technology, was used. The\ngirls created their own games with the help of a storyboard after receiving all\nimportant information about coding. Qualitative and quantitative data was\ncollected through open interviews, as well as created artefacts and surveys\nwhich refer to motivational aspects. The findings show that gaming elements\nfemale teenagers tend to like, create, and play, mostly follow stereotypical\nexpectations. In contrast to our experiences in heterogeneous course settings,\nthis was not seen as something negative by girls. Furthermore, the findings\nprovided evidence for game-making environments for girls. Subsequently, the\nresults contributed to the development of new featured games to be used in our\napp to inspire female users around the world to code their own games. The\nauthors argue that by understanding these differences in game design, we can\nsupport girls so that they become game designers and thereby more interested in\ncoding.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 15:59:44 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 07:33:26 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Spieler", "Bernadette", ""], ["Krnjic", "Vesna", ""], ["Slany", "Wolfgang", ""]]}, {"id": "1907.05846", "submitter": "Taha Hassan", "authors": "Taha Hassan, Bob Edmison, Larry Cox II, Matthew Louvet, Daron Williams", "title": "Exploring the context of course rankings on online academic forums", "comments": "ASONAM '19. arXiv admin note: substantial text overlap with\n  arXiv:1905.02272", "journal-ref": "Proceedings of the 2019 IEEE/ACM International Conference on\n  Advances in Social Network Analysis and Mining (ASONAM '19)", "doi": "10.1145/3341161.3342954", "report-no": null, "categories": "cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  University students routinely use the tools provided by online course ranking\nforums to share and discuss their satisfaction with the quality of instruction\nand content in a wide variety of courses. Student perception of the efficacy of\npedagogies employed in a course is a reflection of a multitude of decisions by\nprofessors, instructional designers and university administrators. This\ncomplexity has motivated a large body of research on the utility, reliability,\nand behavioral correlates of course rankings. There is, however, little\ninvestigation of the (potential) implicit student bias on these forums towards\ndesirable course outcomes at the institution level. To that end, we examine the\nconnection between course outcomes (student-reported GPA) and the overall\nranking of the primary course instructor, as well as rating disparity by nature\nof course outcomes, based on data from two popular academic rating forums. Our\nexperiments with ranking data about over ten thousand courses taught at\nVirginia Tech and its 25 SCHEV-approved peer institutions indicate that there\nis a discernible albeit complex bias towards course outcomes in the professor\nratings registered by students.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 18:36:43 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Hassan", "Taha", ""], ["Edmison", "Bob", ""], ["Cox", "Larry", "II"], ["Louvet", "Matthew", ""], ["Williams", "Daron", ""]]}, {"id": "1907.06058", "submitter": "Maria Bampa", "authors": "Maria Bampa and Panagiotis Papapetrou", "title": "Aggregate-Eliminate-Predict: Detecting Adverse Drug Events from\n  Heterogeneous Electronic Health Records", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of detecting adverse drug events in electronic\nhealthcare records. The challenge in this work is to aggregate heterogeneous\ndata types involving diagnosis codes, drug codes, as well as lab measurements.\nAn earlier framework proposed for the same problem demonstrated promising\npredictive performance for the random forest classifier by using only lab\nmeasurements as data features. We extend this framework, by additionally\nincluding diagnosis and drug prescription codes, concurrently. In addition, we\nemploy a recursive feature selection mechanism on top, that extracts the top-k\nmost important features. Our experimental evaluation on five medical datasets\nof adverse drug events and six different classifiers, suggests that the\nintegration of these additional features provides substantial and statistically\nsignificant improvements in terms of AUC, while employing medically relevant\nfeatures.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 11:46:19 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Bampa", "Maria", ""], ["Papapetrou", "Panagiotis", ""]]}, {"id": "1907.06130", "submitter": "Filippo Menczer", "authors": "Xiaodan Lou, Alessandro Flammini, Filippo Menczer", "title": "Manipulating the Online Marketplace of Ideas", "comments": "25 pages, 8 figures, 80 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media, the modern marketplace of ideas, is vulnerable to manipulation.\nDeceptive inauthentic actors impersonate humans to amplify misinformation and\ninfluence public opinions. Little is known about the large-scale consequences\nof such operations, due to the ethical challenges posed by online experiments\nthat manipulate human behavior. Here we introduce a model of information\nspreading where agents prefer quality information but have limited attention.\nWe evaluate the impact of manipulation strategies aimed at degrading the\noverall quality of the information ecosystem. The model reproduces empirical\npatterns about amplification of low-quality information. We find that\ninfiltrating a critical fraction of the network is more damaging than\ngenerating attention-grabbing content or targeting influentials. We discuss\ncountermeasures suggested by these insights to increase the resilience of\nsocial media users to manipulation, and legal issues arising from regulations\naimed at protecting human speech from suppression by inauthentic actors.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 21:12:08 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 01:01:00 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Lou", "Xiaodan", ""], ["Flammini", "Alessandro", ""], ["Menczer", "Filippo", ""]]}, {"id": "1907.06260", "submitter": "Stephen Pfohl", "authors": "Stephen Pfohl, Tony Duan, Daisy Yi Ding, Nigam H. Shah", "title": "Counterfactual Reasoning for Fair Clinical Risk Prediction", "comments": "Machine Learning for Healthcare 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of machine learning systems to support decision making in healthcare\nraises questions as to what extent these systems may introduce or exacerbate\ndisparities in care for historically underrepresented and mistreated groups,\ndue to biases implicitly embedded in observational data in electronic health\nrecords. To address this problem in the context of clinical risk prediction\nmodels, we develop an augmented counterfactual fairness criteria to extend the\ngroup fairness criteria of equalized odds to an individual level. We do so by\nrequiring that the same prediction be made for a patient, and a counterfactual\npatient resulting from changing a sensitive attribute, if the factual and\ncounterfactual outcomes do not differ. We investigate the extent to which the\naugmented counterfactual fairness criteria may be applied to develop fair\nmodels for prolonged inpatient length of stay and mortality with observational\nelectronic health records data. As the fairness criteria is ill-defined without\nknowledge of the data generating process, we use a variational autoencoder to\nperform counterfactual inference in the context of an assumed causal graph.\nWhile our technique provides a means to trade off maintenance of fairness with\nreduction in predictive performance in the context of a learned generative\nmodel, further work is needed to assess the generality of this approach.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 18:44:09 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Pfohl", "Stephen", ""], ["Duan", "Tony", ""], ["Ding", "Daisy Yi", ""], ["Shah", "Nigam H.", ""]]}, {"id": "1907.06279", "submitter": "Seyedeh Zahra Razavi", "authors": "S. Zahra Razavi, Lenhart K. Schubert, Kimberly A. Van Orden, and\n  Mohammad Rafayet Ali", "title": "Discourse Behavior of Older Adults Interacting With a Dialogue Agent\n  Competent in Multiple Topics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present some results concerning the dialogue behavior and inferred\nsentiment of a group of older adults interacting with a computer-based avatar.\nOur avatar is unique in its ability to hold natural dialogues on a wide range\nof everyday topics---27 topics in three groups, developed with the help of\ngerontologists. The three groups vary in ``degrees of intimacy\", and as such in\ndegrees of difficulty for the user. Each participant interacted with the avatar\nfor 7-9 sessions over a period of 3-4 weeks; analysis of the dialogues reveals\ncorrelations such as greater verbosity for more difficult topics, increasing\nverbosity with successive sessions, especially for more difficult topics,\nstronger sentiment on topics concerned with life goals rather than routine\nactivities, and stronger self-disclosure for more intimate topics. In addition\nto their intrinsic interest, these results also reflect positively on the\nsophistication of our dialogue system.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 20:41:46 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Razavi", "S. Zahra", ""], ["Schubert", "Lenhart K.", ""], ["Van Orden", "Kimberly A.", ""], ["Ali", "Mohammad Rafayet", ""]]}, {"id": "1907.06360", "submitter": "Aaditeshwar Seth", "authors": "Aaditeshwar Seth", "title": "The Elusive Model of Technology, Media, Social Development, and\n  Financial Sustainability", "comments": "Case study prepared for a forthcoming book - Next Frontier Solutions:\n  Harnessing Technology for Social Good", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We recount in this essay the decade-long story of Gram Vaani, a social\nenterprise with a vision to build appropriate ICTs (Information and\nCommunication Technologies) for participatory media in rural and low-income\nsettings, to bring about social development and community empowerment. Other\nsocial enterprises will relate to the learning gained and the strategic pivots\nthat Gram Vaani had to undertake to survive and deliver on its mission, while\nsearching for a robust financial sustainability model. While we believe the\nideal model still remains elusive, we conclude this essay with an open question\nabout the reason to differentiate between different kinds of enterprises -\ncommercial or social, for-profit or not-for-profit - and argue that all\nenterprises should have an ethical underpinning to their work.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 08:20:02 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Seth", "Aaditeshwar", ""]]}, {"id": "1907.06520", "submitter": "Tim Libert", "authors": "Elena Maris, Timothy Libert, Jennifer Henrichsen", "title": "Tracking sex: The implications of widespread sexual data leakage and\n  tracking on porn websites", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores tracking and privacy risks on pornography websites. Our\nanalysis of 22,484 pornography websites indicated that 93% leak user data to a\nthird party. Tracking on these sites is highly concentrated by a handful of\nmajor companies, which we identify. We successfully extracted privacy policies\nfor 3,856 sites, 17% of the total. The policies were written such that one\nmight need a two-year college education to understand them. Our content\nanalysis of the sample's domains indicated 44.97% of them expose or suggest a\nspecific gender/sexual identity or interest likely to be linked to the user. We\nidentify three core implications of the quantitative results: 1) the\nunique/elevated risks of porn data leakage versus other types of data, 2) the\nparticular risks/impact for vulnerable populations, and 3) the complications of\nproviding consent for porn site users and the need for affirmative consent in\nthese online sexual interactions.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 14:31:57 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Maris", "Elena", ""], ["Libert", "Timothy", ""], ["Henrichsen", "Jennifer", ""]]}, {"id": "1907.06538", "submitter": "Tapajit Dey", "authors": "Tapajit Dey, Yuxing Ma, Audris Mockus", "title": "Patterns of Effort Contribution and Demand and User Classification based\n  on Participation Patterns in NPM Ecosystem", "comments": "10 pages, 5 Tables, 2 Figures, Accepted in The 15th International\n  Conference on Predictive Models and Data Analytics in Software Engineering\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Open source requires participation of volunteer and commercial\ndevelopers (users) in order to deliver functional high-quality components.\nDevelopers both contribute effort in the form of patches and demand effort from\nthe component maintainers to resolve issues reported against it. Aim: Identify\nand characterize patterns of effort contribution and demand throughout the open\nsource supply chain and investigate if and how these patterns vary with\ndeveloper activity; identify different groups of developers; and predict\ndevelopers' company affiliation based on their participation patterns. Method:\n1,376,946 issues and pull-requests created for 4433 NPM packages with over\n10,000 monthly downloads and full (public) commit activity data of the 272,142\nissue creators is obtained and analyzed and dependencies on NPM packages are\nidentified. Fuzzy c-means clustering algorithm is used to find the groups among\nthe users based on their effort contribution and demand patterns, and Random\nForest is used as the predictive modeling technique to identify their company\naffiliations. Result: Users contribute and demand effort primarily from\npackages that they depend on directly with only a tiny fraction of\ncontributions and demand going to transitive dependencies. A significant\nportion of demand goes into packages outside the users' respective supply\nchains (constructed based on publicly visible version control data). Three and\ntwo different groups of users are observed based on the effort demand and\neffort contribution patterns respectively. The Random Forest model used for\nidentifying the company affiliation of the users gives a AUC-ROC value of 0.68.\nConclusion: Our results give new insights into effort demand and supply at\ndifferent parts of the supply chain of the NPM ecosystem and its users and\nsuggests the need to increase visibility further upstream.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 15:08:47 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Dey", "Tapajit", ""], ["Ma", "Yuxing", ""], ["Mockus", "Audris", ""]]}, {"id": "1907.06809", "submitter": "Aaditeshwar Seth", "authors": "Aaditeshwar Seth", "title": "Ethical Underpinnings in the Design and Management of ICT Projects", "comments": "Presented at the ACM HCAI Summer School, India 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With a view towards understanding why undesirable outcomes often arise in ICT\nprojects, we draw attention to three aspects in this essay. First, we present\nseveral examples to show that incorporating an ethical framework in the design\nof an ICT system is not sufficient in itself, and that ethics need to guide the\ndeployment and ongoing management of the projects as well. We present a\nframework that brings together the objectives, design, and deployment\nmanagement of ICT projects as being shaped by a common underlying ethical\nsystem. Second, we argue that power-based equality should be incorporated as a\nkey underlying ethical value in ICT projects, to ensure that the project does\nnot reinforce inequalities in power relationships between the actors directly\nor indirectly associated with the project. We present a method to model ICT\nprojects to make legible its influence on the power relationships between\nvarious actors in the ecosystem. Third, we discuss that the ethical values\nunderlying any ICT project ultimately need to be upheld by the project teams,\nwhere certain factors like political ideologies or dispersed teams may affect\nthe rigour with which these ethical values are followed. These three aspects of\nhaving an ethical underpinning to the design and management of ICT projects,\nthe need for having a power-based equality principle for ICT projects, and the\nimportance of socialization of the project teams, needs increasing attention in\ntoday's age of ICT platforms where millions and billions of users interact on\nthe same platform but which are managed by only a few people.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 02:30:03 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Seth", "Aaditeshwar", ""]]}, {"id": "1907.06837", "submitter": "Shalini Pandey", "authors": "Shalini Pandey, George Karypis", "title": "A Self-Attentive model for Knowledge Tracing", "comments": "International Conference on Education Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge tracing is the task of modeling each student's mastery of knowledge\nconcepts (KCs) as (s)he engages with a sequence of learning activities. Each\nstudent's knowledge is modeled by estimating the performance of the student on\nthe learning activities. It is an important research area for providing a\npersonalized learning platform to students. In recent years, methods based on\nRecurrent Neural Networks (RNN) such as Deep Knowledge Tracing (DKT) and\nDynamic Key-Value Memory Network (DKVMN) outperformed all the traditional\nmethods because of their ability to capture complex representation of human\nlearning. However, these methods face the issue of not generalizing well while\ndealing with sparse data which is the case with real-world data as students\ninteract with few KCs. In order to address this issue, we develop an approach\nthat identifies the KCs from the student's past activities that are\n\\textit{relevant} to the given KC and predicts his/her mastery based on the\nrelatively few KCs that it picked. Since predictions are made based on\nrelatively few past activities, it handles the data sparsity problem better\nthan the methods based on RNN. For identifying the relevance between the KCs,\nwe propose a self-attention based approach, Self Attentive Knowledge Tracing\n(SAKT). Extensive experimentation on a variety of real-world dataset shows that\nour model outperforms the state-of-the-art models for knowledge tracing,\nimproving AUC by 4.43% on average.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 04:47:35 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Pandey", "Shalini", ""], ["Karypis", "George", ""]]}, {"id": "1907.06848", "submitter": "Zejie Zhou", "authors": "Zejie Zhou, Boleslaw K. Szymanski, Jianxi Gao", "title": "Modeling competitive evolution of multiple languages", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0232888", "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing evidence demonstrates that in many places language coexistence has\nbecome ubiquitous and essential for supporting language and cultural diversity\nand associated with its financial and economic benefits. The competitive\nevolution among multiple languages determines the evolution outcome, either\ncoexistence, decline, or extinction. Here, we extend the Abrams-Strogatz model\nof language competition to multiple languages and then validate it by analyzing\nthe behavioral transitions of language usage over the recent several decades in\nSingapore and Hong Kong. In each case, we estimate from data the model\nparameters that measure each language utility for its speakers and the strength\nof two biases, the majority preference for their language, and the minority\naversion to it. The values of these two biases decide which language is the\nfastest growing in the competition and what would be the stable state of the\nsystem. We also study the system convergence time to stable states and discover\nthe existence of tipping points with multiple attractors. Moreover, the\ncritical slowdown of convergence to the stable fractions of language users\nappears near and peaks at the tipping points, signaling when the system\napproaches them. Our analysis furthers our understanding of multiple language\nevolution and the role of tipping points in behavioral transitions. These\ninsights may help to protect languages from extinction and retain the language\nand cultural diversity.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 05:31:35 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Zhou", "Zejie", ""], ["Szymanski", "Boleslaw K.", ""], ["Gao", "Jianxi", ""]]}, {"id": "1907.06860", "submitter": "Jianlin Shi", "authors": "Jianlin Shi, Kevin Graves, John F. Hurdle", "title": "A generic rule-based system for clinical trial patient selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The n2c2 2018 Challenge task 1 aimed to identify patients who meet lists of\nheterogeneous inclusion/exclusion criteria for a hypothetical clinical trial.\nWe demonstrate a generic rule-based natural language pipeline can support this\ntask with decent performance (the average F1 score on the test set is 0.89,\nranked the 8th out of 45 teams ).\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 06:43:34 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Shi", "Jianlin", ""], ["Graves", "Kevin", ""], ["Hurdle", "John F.", ""]]}, {"id": "1907.06929", "submitter": "Antonio Luca Alfeo", "authors": "Antonio L. Alfeo, Mario G. C. A. Cimino, Bruno Lepri, Alex 'Sandy'\n  Pentland, Gigliola Vaglini", "title": "Assessing Refugees' Integration via Spatio-temporal Similarities of\n  Mobility and Calling Behaviors", "comments": "https://ieeexplore.ieee.org/document/8758458", "journal-ref": "IEEE Transactions on Computational Social Systems, pp 1 - 13,\n  Electronic ISSN: 2329-924X, Date of Publication: 09 July 2019", "doi": "10.1109/TCSS.2019.2923216", "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In Turkey the increasing tension, due to the presence of 3.4 million Syrian\nrefugees, demands the formulation of effective integration policies. Moreover,\ntheir design requires tools aimed at understanding the integration of refugees\ndespite the complexity of this phenomenon. In this work, we propose a set of\nmetrics aimed at providing insights and assessing the integration of Syrians\nrefugees, by analyzing a real-world Call Details Records (CDRs) dataset\nincluding calls from refugees and locals in Turkey throughout 2017.\nSpecifically, we exploit the similarity between refugees' and locals' spatial\nand temporal behaviors, in terms of communication and mobility in order to\nassess integration dynamics. Together with the already known methods for data\nanalysis, we use a novel computational approach to analyze spatiotemporal\npatterns: Computational Stigmergy, a bio-inspired scalar and temporal\naggregation of samples. Computational Stigmergy associates each sample to a\nvirtual pheromone deposit (mark). Marks in spatiotemporal proximity are\naggregated into functional structures called trails, which summarize the\nspatiotemporal patterns in data and allows computing the similarity between\ndifferent patterns. According to our results, collective mobility and\nbehavioral similarity with locals have great potential as measures of\nintegration, since they are: (i) correlated with the amount of interaction with\nlocals; (ii) an effective proxy for refugee's economic capacity, thus refugee's\npotential employment; and (iii) able to capture events that may disrupt the\nintegration phenomena, such as social tensions.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 10:20:49 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Alfeo", "Antonio L.", ""], ["Cimino", "Mario G. C. A.", ""], ["Lepri", "Bruno", ""], ["Pentland", "Alex 'Sandy'", ""], ["Vaglini", "Gigliola", ""]]}, {"id": "1907.07009", "submitter": "Moreno Marzolla", "authors": "Moreno Marzolla and Raffaela Mirandola", "title": "Gender Balance in Computer Science and Engineering in Italian\n  Universities", "comments": null, "journal-ref": "Proceedings of the 13th European Conference on Software\n  Architecture - Volume 2 Pages 82-87, Paris, France, September 09-13, 2019", "doi": "10.1145/3344948.3344966", "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple studies have shown that gender balance in the fields of Science,\nTechnology, Engineering and Maths -- and in particular in ICT -- is still far\nto be achieved. Several initiatives have been recently taken to increase the\nwomen participation, but it is difficult, at present, to evaluate their impact\nand their potential of changing the situation. This paper contributes to the\ndiscussion by presenting a descriptive analysis of the gender balance in\nComputer Science and Computer Engineering in Italian Universities.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 13:55:55 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Marzolla", "Moreno", ""], ["Mirandola", "Raffaela", ""]]}, {"id": "1907.07032", "submitter": "Arunesh Mathur", "authors": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini,\n  Jonathan Mayer, Marshini Chetty, Arvind Narayanan", "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites", "comments": "32 pages, 11 figures, ACM Conference on Computer-Supported\n  Cooperative Work and Social Computing (CSCW 2019)", "journal-ref": "Proceedings of the ACM Human-Computer Interaction, Vol. 3, CSCW,\n  Article 81 (November 2019)", "doi": "10.1145/3359183", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dark patterns are user interface design choices that benefit an online\nservice by coercing, steering, or deceiving users into making unintended and\npotentially harmful decisions. We present automated techniques that enable\nexperts to identify dark patterns on a large set of websites. Using these\ntechniques, we study shopping websites, which often use dark patterns to\ninfluence users into making more purchases or disclosing more information than\nthey would otherwise. Analyzing ~53K product pages from ~11K shopping websites,\nwe discover 1,818 dark pattern instances, together representing 15 types and 7\nbroader categories. We examine these dark patterns for deceptive practices, and\nfind 183 websites that engage in such practices. We also uncover 22 third-party\nentities that offer dark patterns as a turnkey solution. Finally, we develop a\ntaxonomy of dark pattern characteristics that describes the underlying\ninfluence of the dark patterns and their potential harm on user\ndecision-making. Based on our findings, we make recommendations for\nstakeholders including researchers and regulators to study, mitigate, and\nminimize the use of these patterns.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 14:29:55 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 12:22:56 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Mathur", "Arunesh", ""], ["Acar", "Gunes", ""], ["Friedman", "Michael J.", ""], ["Lucherini", "Elena", ""], ["Mayer", "Jonathan", ""], ["Chetty", "Marshini", ""], ["Narayanan", "Arvind", ""]]}, {"id": "1907.07080", "submitter": "Luis Guillermo Natera Orozco", "authors": "Luis Natera, Federico Battiston, Gerardo I\\~niguez, Michael Szell", "title": "Data-driven strategies for optimal bicycle network growth", "comments": "Main text (12 pages, 4 figures) + Supplementary Information (5 pages,\n  5 figures, 1 table)", "journal-ref": "R. Soc. open sci. 7, 201130 (2020)", "doi": "10.1098/rsos.201130", "report-no": null, "categories": "physics.soc-ph cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban transportation networks, from sidewalks and bicycle paths to streets\nand rail lines, provide the backbone for movement and socioeconomic life in\ncities. These networks can be understood as layers of a larger multiplex\ntransport network. Because most cities are car-centric, the most developed\nlayer is typically the street layer, while other layers can be highly\ndisconnected. To make urban transport sustainable, cities are increasingly\ninvesting to develop their bicycle networks. However, given the usually patchy\nnature of the bicycle network layer, it is yet unclear how to extend it\ncomprehensively and effectively given a limited budget. Here we develop\ndata-driven, algorithmic network growth strategies and apply them to cities\naround the world, showing that small but focused investments allow to\nsignificantly increase the connectedness and directness of urban bicycle\nnetworks. We motivate the development of our algorithms with a network\ncomponent analysis and with multimodal urban fingerprints that reveal different\nclasses of cities depending on the connectedness between different network\nlayers. We introduce two greedy algorithms to add the most critical missing\nlinks in the bicycle layer: The first algorithm connects the two largest\nconnected components, the second algorithm connects the largest with the\nclosest component. We show that these algorithms outmatch both a random\napproach and a baseline minimum investment strategy that connects the closest\ncomponents ignoring size. Our computational approach outlines novel pathways\nfrom car-centric towards sustainable cities by taking advantage of urban data\navailable on a city-wide scale. It is a first step towards a quantitative\nconsolidation of bicycle infrastructure development that can become valuable\nfor urban planners and stakeholders.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 15:49:48 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Natera", "Luis", ""], ["Battiston", "Federico", ""], ["I\u00f1iguez", "Gerardo", ""], ["Szell", "Michael", ""]]}, {"id": "1907.07120", "submitter": "Nguyen Phong Hoang", "authors": "Nguyen Phong Hoang, Sadie Doreen, Michalis Polychronakis", "title": "Measuring I2P Censorship at a Global Scale", "comments": "To appear in Proceedings of the 9th USENIX Workshop on Free and Open\n  Communications on the Internet (FOCI '19). San Francisco, CA. May 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.NI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalence of Internet censorship has prompted the creation of several\nmeasurement platforms for monitoring filtering activities. An important\nchallenge faced by these platforms revolves around the trade-off between depth\nof measurement and breadth of coverage. In this paper, we present an\nopportunistic censorship measurement infrastructure built on top of a network\nof distributed VPN servers run by volunteers, which we used to measure the\nextent to which the I2P anonymity network is blocked around the world. This\ninfrastructure provides us with not only numerous and geographically diverse\nvantage points, but also the ability to conduct in-depth measurements across\nall levels of the network stack. Using this infrastructure, we measured at a\nglobal scale the availability of four different I2P services: the official\nhomepage, its mirror site, reseed servers, and active relays in the network.\nWithin a period of one month, we conducted a total of 54K measurements from\n1.7K network locations in 164 countries. With different techniques for\ndetecting domain name blocking, network packet injection, and block pages, we\ndiscovered I2P censorship in five countries: China, Iran, Oman, Qatar, and\nKuwait. Finally, we conclude by discussing potential approaches to circumvent\ncensorship on I2P.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 16:57:22 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Hoang", "Nguyen Phong", ""], ["Doreen", "Sadie", ""], ["Polychronakis", "Michalis", ""]]}, {"id": "1907.07245", "submitter": "Lars Brenna", "authors": "Lars Brenna, H{\\aa}vard D. Johansen and Dag Johansen", "title": "A Survey of Automatic Methods for Nutritional Assessment", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nutritional assessment is key in order to make decisions about the nature and\ncause of nutrition related health issues that affect an individual. The\nsystematic process of collecting and interpreting relevant nutrition\ninformation, however, is still in its technological infancy. Despite\ntechnological advances in storage and analysis of nutritional data, methods for\ncollecting data are largely unchanged over the past two decades. It is well\ndocumented that these methods have issues that cause under-reporting.\nMeanwhile, new developments in wearable biometric logging devices have seen\nincreased traction among individuals. This is sometimes referred to as the\nQuantified Self movement. One part of this movement is the development of\ntechnological means for objectively collecting nutritional data. Nutritional\nassessment, however, is about to be heavily impacted by emerging computer\nscience technologies, and this survey provides an overview of promising\ntechnology approaches supporting nutritional assessment. Both academic and\ncommercial systems are reviewed and categorized.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 21:19:02 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Brenna", "Lars", ""], ["Johansen", "H\u00e5vard D.", ""], ["Johansen", "Dag", ""]]}, {"id": "1907.07275", "submitter": "John Cook", "authors": "John Cook, Rishab Nithyanand and Zubair Shafiq", "title": "Inferring Tracker-Advertiser Relationships in the Online Advertising\n  Ecosystem using Header Bidding", "comments": "18 pages, 2 figures, Privacy Enhancing Technologies Symposium (2020)", "journal-ref": null, "doi": "10.2478/popets-2020-0001", "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online advertising relies on trackers and data brokers to show targeted ads\nto users. To improve targeting, different entities in the intricately\ninterwoven online advertising and tracking ecosystems are incentivized to share\ninformation with each other through client-side or server-side mechanisms.\nInferring data sharing between entities, especially when it happens at the\nserver-side, is an important and challenging research problem. In this paper,\nwe introduce KASHF: a novel method to infer data sharing relationships between\nadvertisers and trackers by studying how an advertiser's bidding behavior\nchanges as we manipulate the presence of trackers. We operationalize this\ninsight by training an interpretable machine learning model that uses the\npresence of trackers as features to predict the bidding behavior of an\nadvertiser. By analyzing the machine learning model, we are able to infer\nrelationships between advertisers and trackers irrespective of whether data\nsharing occurs at the client-side or the server-side. We are also able to\nidentify several server-side data sharing relationships that are validated\nexternally but are not detected by client-side cookie syncing.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 22:15:00 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 20:08:25 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Cook", "John", ""], ["Nithyanand", "Rishab", ""], ["Shafiq", "Zubair", ""]]}, {"id": "1907.07475", "submitter": "M\\'arton Karsai", "authors": "M\\'arton Karsai", "title": "Computational Human Dynamics", "comments": "Habilitation thesis, ENS Lyon (2019) - 148 pages, 49 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis summarises my scientific contributions in the domain of network\nscience, human dynamics and computational social science. These contributions\nare associated to computer science, physics, statistics, and applied\nmathematics. The goal of this thesis is twofold, on one hand to write a concise\nsummary of my most interesting scientific contributions, and on the other hand\nto provide an up-to-date view and perspective about my field. I start my\ndissertation with an introduction to position the reader on the landscape of my\nfield and to put in perspective my contributions. In the second chapter I\nconcentrate on my works on bursty human dynamics, addressing heterogeneous\ntemporal characters of human actions and interactions. Next, I discuss my\ncontributions to the field of temporal networks and give a synthesises of my\nworks on various methods of the representation, characterisation, and modelling\nof time-varying structures. Finally, I discuss my works on the data-driven\nobservations and modelling of collective social phenomena. There, I summarise\nstudies on the static observations of emergent patterns of socioeconomic\ninequalities and their correlations with social-communication networks, and\nwith linguistic patterns. I also discuss dynamic observations and modelling of\nsocial contagion processes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 12:37:52 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 05:58:10 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Karsai", "M\u00e1rton", ""]]}, {"id": "1907.07493", "submitter": "Carl M\\\"orch", "authors": "Carl-Maria M\\\"orch, Abhishek Gupta, Brian L. Mishara", "title": "Canada Protocol: an ethical checklist for the use of Artificial\n  Intelligence in Suicide Prevention and Mental Health", "comments": "Submitted to CRISIS (The Journal of Crisis Intervention and Suicide\n  Prevention), Hogrefe", "journal-ref": null, "doi": "10.1016/j.artmed.2020.101934", "report-no": null, "categories": "cs.CY cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction: To improve current public health strategies in suicide\nprevention and mental health, governments, researchers and private companies\nincreasingly use information and communication technologies, and more\nspecifically Artificial Intelligence and Big Data. These technologies are\npromising but raise ethical challenges rarely covered by current legal systems.\nIt is essential to better identify, and prevent potential ethical risks.\nObjectives: The Canada Protocol - MHSP is a tool to guide and support\nprofessionals, users, and researchers using AI in mental health and suicide\nprevention. Methods: A checklist was constructed based upon ten international\nreports on AI and ethics and two guides on mental health and new technologies.\n329 recommendations were identified, of which 43 were considered as applicable\nto Mental Health and AI. The checklist was validated, using a two round Delphi\nConsultation. Results: 16 experts participated in the first round of the Delphi\nConsultation and 8 participated in the second round. Of the original 43 items,\n38 were retained. They concern five categories: \"Description of the Autonomous\nIntelligent System\" (n=8), \"Privacy and Transparency\" (n=8), \"Security\" (n=6),\n\"Health-Related Risks\" (n=8), \"Biases\" (n=8). The checklist was considered\nrelevant by most users, and could need versions tailored to each category of\ntarget users.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 13:14:13 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["M\u00f6rch", "Carl-Maria", ""], ["Gupta", "Abhishek", ""], ["Mishara", "Brian L.", ""]]}, {"id": "1907.07498", "submitter": "Jukka Ruohonen", "authors": "Kalle Hjerppe and Jukka Ruohonen and Ville Lepp\\\"anen", "title": "The General Data Protection Regulation: Requirements, Architectures, and\n  Constraints", "comments": "Forthcoming in the 27th IEEE International Requirements Engineering\n  Conference (RE'19), Jeju Island, IEEE", "journal-ref": null, "doi": "10.1109/RE.2019.00036", "report-no": null, "categories": "cs.SE cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The General Data Protection Regulation (GDPR) in the European Union is the\nmost famous recently enacted privacy regulation. Despite of the regulation's\nlegal, political, and technological ramifications, relatively little research\nhas been carried out for better understanding the GDPR's practical implications\nfor requirements engineering and software architectures. Building on a grounded\ntheory approach with close ties to the Finnish software industry, this paper\ncontributes to the sealing of this gap in previous research. Three questions\nare asked and answered in the context of software development organizations.\nFirst, the paper elaborates nine practical constraints under which many small\nand medium-sized enterprises (SMEs) often operate when implementing solutions\nthat address the new regulatory demands. Second, the paper elicits nine\nregulatory requirements from the GDPR for software architectures. Third, the\npaper presents an implementation for a software architecture that complies both\nwith the requirements elicited and the constraints elaborated.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 13:19:47 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Hjerppe", "Kalle", ""], ["Ruohonen", "Jukka", ""], ["Lepp\u00e4nen", "Ville", ""]]}, {"id": "1907.07514", "submitter": "Peter Cotton", "authors": "Peter Cotton", "title": "Self Organizing Supply Chains for Micro-Prediction: Present and Future\n  uses of the ROAR Protocol", "comments": "Thirty-sixth International Conference on Machine Learning Workshop on\n  AI in Finance: Applications and Infrastructure for Multi-Agent Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multi-agent system is trialed as a means of crowd-sourcing inexpensive but\nhigh quality streams of predictions. Each agent is a microservice embodying\nstatistical models and endowed with economic self-interest. The ability to fork\nand modify simple agents is granted to a large number of employees in a firm\nand empirical lessons are reported. We suggest that one plausible trajectory\nfor this project is the creation of a Prediction Web.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 13:40:15 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Cotton", "Peter", ""]]}, {"id": "1907.07660", "submitter": "Lynn Kaack", "authors": "Lynn H. Kaack and George H. Chen and M. Granger Morgan", "title": "Truck Traffic Monitoring with Satellite Images", "comments": "31 pages, 15 figures, to be published in ACM SIGCAS Conference on\n  Computing and Sustainable Societies (COMPASS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The road freight sector is responsible for a large and growing share of\ngreenhouse gas emissions, but reliable data on the amount of freight that is\nmoved on roads in many parts of the world are scarce. Many low- and\nmiddle-income countries have limited ground-based traffic monitoring and\nfreight surveying activities. In this proof of concept, we show that we can use\nan object detection network to count trucks in satellite images and predict\naverage annual daily truck traffic from those counts. We describe a complete\nmodel, test the uncertainty of the estimation, and discuss the transfer to\ndeveloping countries.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 17:45:40 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Kaack", "Lynn H.", ""], ["Chen", "George H.", ""], ["Morgan", "M. Granger", ""]]}, {"id": "1907.07725", "submitter": "Marc-Andr\\'e Kaufhold", "authors": "Marc-Andr\\'e Kaufhold, Christian Reuter, Thomas Ludwig", "title": "Cross-Media Usage of Social Big Data for Emergency Services and\n  Volunteer Communities: Approaches, Development and Challenges of\n  Multi-Platform Social Media Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of social media is ubiquitous and nowadays well-established in our\neveryday life, but increasingly also before, during or after emergencies. The\nproduced data is spread across several types of social media and can be used by\ndifferent actors, such as emergency services or volunteer communities. There\nare already systems available that support the process of gathering, analysing\nand distributing information through social media. However, dependent on the\ngoal of analysis, the analysis methods and available systems are limited based\non technical or business-oriented restrictions. This paper presents the design\nof a cross-platform Social Media API, which was integrated and evaluated within\nmultiple emergency scenarios. Based on the lessons learned, we outline the core\nchallenges from the practical development and theoretical findings, focusing\n(1) cross-platform gathering and data management, (2) trustability and\ninformation quality, (3) tailorability and adjustable data operations, and (4)\nqueries, performance, and technical development.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 19:00:05 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Kaufhold", "Marc-Andr\u00e9", ""], ["Reuter", "Christian", ""], ["Ludwig", "Thomas", ""]]}, {"id": "1907.07757", "submitter": "Fan Yang", "authors": "Fan Yang, Shiva K. Pentyala, Sina Mohseni, Mengnan Du, Hao Yuan, Rhema\n  Linder, Eric D. Ragan, Shuiwang Ji, Xia Hu", "title": "XFake: Explainable Fake News Detector with Visualizations", "comments": "4 pages, WebConf'2019 Demo", "journal-ref": null, "doi": "10.1145/3308558.3314119", "report-no": null, "categories": "cs.CY cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this demo paper, we present the XFake system, an explainable fake news\ndetector that assists end-users to identify news credibility. To effectively\ndetect and interpret the fakeness of news items, we jointly consider both\nattributes (e.g., speaker) and statements. Specifically, MIMIC, ATTN and PERT\nframeworks are designed, where MIMIC is built for attribute analysis, ATTN is\nfor statement semantic analysis and PERT is for statement linguistic analysis.\nBeyond the explanations extracted from the designed frameworks, relevant\nsupporting examples as well as visualization are further provided to facilitate\nthe interpretation. Our implemented system is demonstrated on a real-world\ndataset crawled from PolitiFact, where thousands of verified political news\nhave been collected.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 18:29:58 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Yang", "Fan", ""], ["Pentyala", "Shiva K.", ""], ["Mohseni", "Sina", ""], ["Du", "Mengnan", ""], ["Yuan", "Hao", ""], ["Linder", "Rhema", ""], ["Ragan", "Eric D.", ""], ["Ji", "Shuiwang", ""], ["Hu", "Xia", ""]]}, {"id": "1907.07758", "submitter": "Massimo Franceschet", "authors": "Massimo Franceschet", "title": "HITS hits art", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The blockchain art market is partitioned around the roles of artists and\ncollectors and highly concentrated among few prominent figures. We hence\npropose to adapt Kleinberg's authority/hub HITS method to rate artists and\ncollectors in the art context. This seems a reasonable choice since the\noriginal method deftly defines its scores in terms of a mutual recursive\nrelationship between authorities/artists - the miners of information/art, and\nhubs/collectors - the assemblers of such information/art.\n  We evaluated the proposed method on the collector-artist network of SuperRare\ngallery, the major crypto art marketplace. We found that the proposed artist\nand collector metrics are weakly correlated with other network science metrics\nlike degree and strength. This hints the possibility of coupling different\nmeasures in order to profile active users of the gallery and suggests\ninvestment strategies with different risk/reward ratios for collectors as well\nas marketing strategies with different targets for artists.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 09:49:23 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 16:30:52 GMT"}, {"version": "v3", "created": "Sun, 12 Apr 2020 10:26:48 GMT"}, {"version": "v4", "created": "Mon, 31 May 2021 10:34:25 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Franceschet", "Massimo", ""]]}, {"id": "1907.07759", "submitter": "Bin Guo", "authors": "Bin Guo, Yasan Ding, Yueheng Sun, Shuai Ma, Ke Li", "title": "The Mass, Fake News, and Cognition Security", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide spread of fake news in social networks is posing threats to social\nstability, economic development and political democracy etc. Numerous studies\nhave explored the effective detection approaches of online fake news, while few\nworks study the intrinsic propagation and cognition mechanisms of fake news.\nSince the development of cognitive science paves a promising way for the\nprevention of fake news, we present a new research area called Cognition\nSecurity (CogSec), which studies the potential impacts of fake news to human\ncognition, ranging from misperception, untrusted knowledge acquisition,\ntargeted opinion/attitude formation, to biased decision making, and\ninvestigates the effective ways for fake news debunking. CogSec is a\nmultidisciplinary research field that leverages knowledge from social science,\npsychology, cognition science, neuroscience, AI and computer science. We first\npropose related definitions to characterize CogSec and review the literature\nhistory. We further investigate the key research challenges and techniques of\nCogSec, including human-content cognition mechanism, social influence and\nopinion diffusion, fake news detection and malicious bot detection. Finally, we\nsummarize the open issues and future research directions, such as early\ndetection of fake news, explainable fake news debunking, social contagion and\ndiffusion models of fake news, and so on.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 22:40:35 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Guo", "Bin", ""], ["Ding", "Yasan", ""], ["Sun", "Yueheng", ""], ["Ma", "Shuai", ""], ["Li", "Ke", ""]]}, {"id": "1907.07760", "submitter": "Georgios Mylonas", "authors": "Georgios Mylonas, Dimitrios Amaxilatis, Stelios Tsampas, Lidia Pocero,\n  Joakim Gunneriusson", "title": "A Methodology for Saving Energy in Educational Buildings Using an IoT\n  Infrastructure", "comments": "To appear in the 10th International Conference on Information,\n  Intelligence, Systems and Applications (IISA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A considerable part of recent research in smart cities and IoT has focused on\nachieving energy savings in buildings and supporting aspects related to\nsustainability. In this context, the educational community is one of the most\nimportant ones to consider, since school buildings constitute a large part of\nnon-residential buildings, while also educating students on sustainability\nmatters is an investment for the future. In this work, we discuss a methodology\nfor achieving energy savings in schools based on the utilization of data\nproduced by an IoT infrastructure installed inside school buildings and related\neducational scenarios. We present the steps comprising this methodology in\ndetail, along with a set of tangible results achieved within the GAIA project.\nWe also showcase how an IoT infrastructure can support activities in an\neducational setting and produce concrete outcomes, with typical levels of 20%\nenergy savings.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 08:12:34 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Mylonas", "Georgios", ""], ["Amaxilatis", "Dimitrios", ""], ["Tsampas", "Stelios", ""], ["Pocero", "Lidia", ""], ["Gunneriusson", "Joakim", ""]]}, {"id": "1907.07762", "submitter": "Eug\\^enio Pacceli Reis Da Fonseca", "authors": "Eug\\^enio Pacceli Reis da Fonseca, Evandro Caldeira, Heitor Soares\n  Ramos Filho, Leonardo Barbosa e Oliveira, Adriano C\\'esar Machado Pereira,\n  Pierre Santos Vilela", "title": "Agro 4.0: A Green Information System for Sustainable Agroecosystem\n  Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agriculture is one of the most critical activities developed today by\nhumankind and is in constant technical evolution to supply food and other\nessential products to everlasting and increasing demand. New machines, seeds,\nand fertilizers were developed to increase the productivity of cultivated\nareas. It is estimated that by 2050 we will have a population of 9 billion\npeople and the production of food to meet this demand must occur sustainably.\nTo achieve this goal, it is paramount the adoption of sustainable management\ntechniques for agroecosystems. However, this is a complex task due to a large\nnumber of variables involved. One of the solutions for the handling and\ntreatment of such diverse data is the use of Green IS. In this work, we adopt a\nmethodology called Indicators of Sustainability in Agroecosystems (Indicadores\nde Sustentabilidade em Agroecossistemas -- ISA), implement an information\nsystem based on it and apply Data Science techniques over the gathered data -\nfrom 100 real rural properties - to compute which are the most relevant ISA\nIndicators for the final ISA Sustainability Index Score. As a result, we have\ndeveloped a set of tools for data collection, processing, visualization, and\nanalysis of the sustainability of a rural property or region, following the ISA\nmethodology. We also have that with only 7 of the 21 Indicators present in ISA\nwe can identify the level of sustainability in more than 90% of cases, allowing\nfor a new discussion about shrinking the amount of data needed for the\ncomputation of ISA, or remodelling the final computation of the Sustainability\nIndex so other Indicators can be more expressive. Users of the solutions\ndeveloped in this work can identify best practices for sustainability in\nparticipating agroecosystems.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 04:29:26 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["da Fonseca", "Eug\u00eanio Pacceli Reis", ""], ["Caldeira", "Evandro", ""], ["Filho", "Heitor Soares Ramos", ""], ["Oliveira", "Leonardo Barbosa e", ""], ["Pereira", "Adriano C\u00e9sar Machado", ""], ["Vilela", "Pierre Santos", ""]]}, {"id": "1907.07765", "submitter": "Hina Binte Haq", "authors": "Hina Binte Haq, Ronan McDermott, and Syed Taha Ali", "title": "Pakistan's Internet Voting Experiment", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pakistan recently conducted small-scale trials of a remote Internet voting\nsystem for overseas citizens. In this contribution, we report on the\nexperience: we document the unique combination of sociopolitical, legal, and\ninstitutional factors motivating this exercise. We describe the system and it's\nreported vulnerabilities, and we also highlight new issues pertaining to\nmateriality. If this system is deployed in the next general elections, as seems\nlikely, this development would constitute the largest enfranchised diaspora in\nthe world. Our goal in this paper, therefore, is to provide comprehensive\ninsight into Pakistan's experiment with Internet voting, emphasize outstanding\nchallenges, and identify directions for future research.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 19:48:40 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Haq", "Hina Binte", ""], ["McDermott", "Ronan", ""], ["Ali", "Syed Taha", ""]]}, {"id": "1907.07771", "submitter": "Roman Yampolskiy", "authors": "Peter J. Scott, Roman V. Yampolskiy", "title": "Classification Schemas for Artificial Intelligence Failures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine historical failures of artificial intelligence (AI)\nand propose a classification scheme for categorizing future failures. By doing\nso we hope that (a) the responses to future failures can be improved through\napplying a systematic classification that can be used to simplify the choice of\nresponse and (b) future failures can be reduced through augmenting development\nlifecycles with targeted risk assessments.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 16:05:44 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Scott", "Peter J.", ""], ["Yampolskiy", "Roman V.", ""]]}, {"id": "1907.07772", "submitter": "Patrick Gikunda Mr.", "authors": "Patrick Kinyua Gikunda", "title": "Modern CNNs for IoT Based Farms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent introduction of ICT in agriculture has brought a number of changes in\nthe way farming is done. This means use of Internet of Things(IoT), Cloud\nComputing(CC), Big Data (BD) and automation to gain better control over the\nprocess of farming. As the use of these technologies in farms has grown\nexponentially with massive data production, there is need to develop and use\nstate-of-the-art tools in order to gain more insight from the data within\nreasonable time. In this paper, we present an initial understanding of\nConvolutional Neural Network (CNN), the recent architectures of\nstate-of-the-art CNN and their underlying complexities. Then we propose a\nclassification taxonomy tailored for agricultural application of CNN. Finally,\nwe present a comprehensive review of research dedicated to applications of\nstate-of-the-art CNNs in agricultural production systems. Our contribution is\nin two-fold. First, for end users of agricultural deep learning tools, our\nbenchmarking finding can serve as a guide to selecting appropriate architecture\nto use. Second, for agricultural software developers of deep learning tools,\nour in-depth analysis explains the state-of-the-art CNN complexities and points\nout possible future directions to further optimize the running performance.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 19:28:46 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Gikunda", "Patrick Kinyua", ""]]}, {"id": "1907.07892", "submitter": "Igor Rubinov", "authors": "Alexa Hagerty and Igor Rubinov", "title": "Global AI Ethics: A Review of the Social Impacts and Ethical\n  Implications of Artificial Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ethical implications and social impacts of artificial intelligence have\nbecome topics of compelling interest to industry, researchers in academia, and\nthe public. However, current analyses of AI in a global context are biased\ntoward perspectives held in the U.S., and limited by a lack of research,\nespecially outside the U.S. and Western Europe.\n  This article summarizes the key findings of a literature review of recent\nsocial science scholarship on the social impacts of AI and related technologies\nin five global regions. Our team of social science researchers reviewed more\nthan 800 academic journal articles and monographs in over a dozen languages.\n  Our review of the literature suggests that AI is likely to have markedly\ndifferent social impacts depending on geographical setting. Likewise,\nperceptions and understandings of AI are likely to be profoundly shaped by\nlocal cultural and social context.\n  Recent research in U.S. settings demonstrates that AI-driven technologies\nhave a pattern of entrenching social divides and exacerbating social\ninequality, particularly among historically-marginalized groups. Our literature\nreview indicates that this pattern exists on a global scale, and suggests that\nlow- and middle-income countries may be more vulnerable to the negative social\nimpacts of AI and less likely to benefit from the attendant gains.\n  We call for rigorous ethnographic research to better understand the social\nimpacts of AI around the world. Global, on-the-ground research is particularly\ncritical to identify AI systems that may amplify social inequality in order to\nmitigate potential harms. Deeper understanding of the social impacts of AI in\ndiverse social settings is a necessary precursor to the development,\nimplementation, and monitoring of responsible and beneficial AI technologies,\nand forms the basis for meaningful regulation of these technologies.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 06:34:08 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Hagerty", "Alexa", ""], ["Rubinov", "Igor", ""]]}, {"id": "1907.07946", "submitter": "Yasuko Kawahata", "authors": "Yasuko Kawahata, Akira Ishii", "title": "Consensus formation Online using Sociophysics method", "comments": null, "journal-ref": "GDN2019(19th International Conference on Group Decision and\n  Negotiation in 2019 a Joint GDN-EWG/BOR meeting) Proceedings", "doi": null, "report-no": null, "categories": "cs.SI cs.CY physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Consensus formation and difference of opinion have long been the subject of\nresearch. However, relevant laws and systems within society are being updated\nto reflect the changes in information networks. Online environment has come to\nfulfill a major role as a real and concrete place of opposing opinions and\nconsensus formation. In the future, quantitative findings on consensus\nformation, and findings on relevant trends, must be summarized, and\nquantitative research related to trends likely to give rise to social and\neconomic risk is required. Thus, the potential for comparing research related\nto consensus formation using actual data and an approach using a mathematical\nmodel was first investigated.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 09:27:40 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Kawahata", "Yasuko", ""], ["Ishii", "Akira", ""]]}, {"id": "1907.07962", "submitter": "Camille Roth", "authors": "Agathe Baltzer, M\\'arton Karsai, Camille Roth", "title": "Interactional and Informational Attention on Twitter", "comments": "16 pages, 6 figures", "journal-ref": "Information 2019, 10(8), 250", "doi": "10.3390/info10080250", "report-no": null, "categories": "cs.SI cs.CY physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter may be considered as a decentralized social information processing\nplatform whose users constantly receive their followees' information feeds,\nwhich they may in turn dispatch to their followers. This decentralization is\nnot devoid of hierarchy and heterogeneity, both in terms of activity and\nattention. In particular, we appraise the distribution of attention at the\ncollective and individual level, which exhibits the existence of attentional\nconstraints and focus effects. We observe that most users usually concentrate\ntheir attention on a limited core of peers and topics, and discuss the\nrelationship between interactional and informational attention processes -- all\nof which, we suggest, may be useful to refine influence models by enabling the\nconsideration of differential attention likelihood depending on users, their\nactivity levels and peers' positions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 10:13:04 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Baltzer", "Agathe", ""], ["Karsai", "M\u00e1rton", ""], ["Roth", "Camille", ""]]}, {"id": "1907.08083", "submitter": "Azqa Nadeem", "authors": "Azqa Nadeem, and Marianne Junger", "title": "Laptop Theft in a University Setting can be Avoided with Warnings", "comments": "The results in this paper are erroneous. Due to selection bias, the\n  results aren't actually statistically significant", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Laptops have become an indispensable asset in today's digital age. They often\ncontain highly sensitive information, such as credentials and confidential\ndocuments. As a result, the value of a laptop is an accumulation of the value\nof both the physical device itself and the cyber assets it contains, making it\na lucrative target for theft. Educational institutions have a large population\nof potential victims of laptop theft. To mitigate this risk, we investigate\nwhether a simple warning sign can reduce the opportunity for potential\noffenders. To this end, we have conducted an empirical study to observe the\nprevalence of students/staff leaving their laptops unattended at a university\nstudy hall at the Delft University of Technology in the Netherlands, both with\nand without a warning sign. We observed 148 out of 220 subjects leaving their\nlaptops unattended in just three weeks. The results also showed that without\nthe warning banner, 75.5% (83 out of 110) of subjects left their laptops\nunattended and with the warning, only 59.1% (65 out of 110) of subjects showed\nthe same behavior, which is a significant reduction of 16.4%. In addition, a\nqualitative analysis was performed on the responses of subjects who left their\nlaptops unattended after the warning banner was placed. The results showed a\nmix of convenience, and a blind trust on the safety of the faculty. In\nconclusion, a simple banner was effective in reducing the opportunity for\nlaptop theft. However, the percentage of laptops left unattended was still high\neven after the introduction of the banner.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 14:36:52 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 17:06:57 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Nadeem", "Azqa", ""], ["Junger", "Marianne", ""]]}, {"id": "1907.08228", "submitter": "Indira Sen", "authors": "Indira Sen, Fabian Floeck, Katrin Weller, Bernd Weiss, Claudia Wagner", "title": "TED-On: A Total Error Framework for Digital Traces of Human Behavior on\n  Online Platforms", "comments": "20 pages, 2 figures, Longer version of paper set to appear in Public\n  Opinion Quarterly. Updating terminology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peoples' activities and opinions recorded as digital traces online,\nespecially on social media and other web-based platforms, offer increasingly\ninformative pictures of the public. They promise to allow inferences about\npopulations beyond the users of the platforms on which the traces are recorded,\nrepresenting real potential for the Social Sciences and a complement to\nsurvey-based research. But the use of digital traces brings its own\ncomplexities and new error sources to the research enterprise. Recently,\nresearchers have begun to discuss the errors that can occur when digital traces\nare used to learn about humans and social phenomena. This article synthesizes\nthis discussion and proposes a systematic way to categorize potential errors,\ninspired by the Total Survey Error (TSE) Framework developed for survey\nmethodology. We introduce a conceptual framework to diagnose, understand, and\ndocument errors that may occur in studies based on such digital traces. While\nthere are clear parallels to the well-known error sources in the TSE framework,\nthe new \"Total Error Framework for Digital Traces of Human Behavior on Online\nPlatforms\" (TED-On) identifies several types of error that are specific to the\nuse of digital traces. By providing a standard vocabulary to describe these\nerrors, the proposed framework is intended to advance communication and\nresearch concerning the use of digital traces in scientific social research.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 18:18:48 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 16:42:26 GMT"}, {"version": "v3", "created": "Thu, 5 Dec 2019 18:03:44 GMT"}, {"version": "v4", "created": "Thu, 3 Jun 2021 16:52:11 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Sen", "Indira", ""], ["Floeck", "Fabian", ""], ["Weller", "Katrin", ""], ["Weiss", "Bernd", ""], ["Wagner", "Claudia", ""]]}, {"id": "1907.08489", "submitter": "Ning Wu", "authors": "Jingyuan Wang, Ning Wu, Wayne Xin Zhao, Fanzhang Peng and Xin Lin", "title": "Empowering A* Search Algorithms with Neural Networks for Personalized\n  Route Recommendation", "comments": "9 pages, 25TH ACM SIGKDD Conference On Knowledge Discovery And Data\n  Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized Route Recommendation (PRR) aims to generate user-specific route\nsuggestions in response to users' route queries. Early studies cast the PRR\ntask as a pathfinding problem on graphs, and adopt adapted search algorithms by\nintegrating heuristic strategies. Although these methods are effective to some\nextent, they require setting the cost functions with heuristics. In addition,\nit is difficult to utilize useful context information in the search procedure.\nTo address these issues, we propose using neural networks to automatically\nlearn the cost functions of a classic heuristic algorithm, namely A* algorithm,\nfor the PRR task. Our model consists of two components. First, we employ\nattention-based Recurrent Neural Networks (RNN) to model the cost from the\nsource to the candidate location by incorporating useful context information.\nInstead of learning a single cost value, the RNN component is able to learn a\ntime-varying vectorized representation for the moving state of a user. Second,\nwe propose to use a value network for estimating the cost from a candidate\nlocation to the destination. For capturing structural characteristics, the\nvalue network is built on top of improved graph attention networks by\nincorporating the moving state of a user and other context information. The two\ncomponents are integrated in a principled way for deriving a more accurate cost\nof a candidate location. Extensive experiment results on three real-world\ndatasets have shown the effectiveness and robustness of the proposed model.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 12:47:00 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Wang", "Jingyuan", ""], ["Wu", "Ning", ""], ["Zhao", "Wayne Xin", ""], ["Peng", "Fanzhang", ""], ["Lin", "Xin", ""]]}, {"id": "1907.08699", "submitter": "Heinrich S\\\"obke", "authors": "Heinrich S\\\"obke and Andrea L\\\"uck", "title": "Elementary Interactions An Approach in Decision Tool Development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Criteria Decision Analysis (MCDA) is an established methodology to\nsupport decision making of multi-objective problems. For conducting a MCDA, in\nmost cases a set of objectives (SOO) is required which consists of a\nhierarchical structure with objectives, criteria and indicators. The\ndevelopment of a SOO may require high organizational effort. This article\nintroduces elementary interactions as a key paradigm for the development of a\nSOO. Elementary interactions are self-contained information requests that can\nbe answered with little cognitive effort, which are made and processed with the\nhelp of a web platform. Each elementary interaction contributes to the stepwise\ndevelopment of a SOO. Based on the hypothesis that a SOO can be developed\nexclusively with elementary interactions, a platform concept is described.\nEssential components of the platform are a Model Aggregator, an Elementary\nInteraction Stream Generator, a Participant Manager and a Discussion Forum. The\nplatform concept has been evaluated in a pilot study using a web-based\nprototype. In summary, the proposed concept demonstrates the potential to\nadvance the development of sets of objectives for MCDA applications: (1) The\nplatform concept does not restrict the application domain, (2) it is intended\nto work with little administration efforts, (3) it lowers the organizational\neffort for developing a SOO. (3) it supports the further development of an\nexisting SOO in the event of significant changes in external conditions. (4)\nThe development process of the SOO can be recorded by the platform and thus\nbecomes retraceable. The reproducibility may have a positive effect on the\nspread of MCDA applications. The traceability and the use of elementary\ninteractions make the platform appear to be a suitable medium for Citizen\nScience-based approaches to the development of MCDA applications.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 21:10:33 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["S\u00f6bke", "Heinrich", ""], ["L\u00fcck", "Andrea", ""]]}, {"id": "1907.08849", "submitter": "Joaquin Garcia-Alfaro", "authors": "Michel Barbeau, Georg Carle, Joaquin Garcia-Alfaro, Vicen\\c{c} Torra", "title": "Next Generation Resilient Cyber-Physical Systems", "comments": "6 pages, 45 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-Physical Systems (CPS) consist of distributed engineered environments\nwhere the monitoring and surveillance tasks are governed by tightly integrated\ncomputing, communication and control technologies. CPS are omnipresent in our\neveryday life. Hacking and failures of such systems have impact on critical\nservices with potentially significant and lasting consequences. In this paper,\nwe review which requirements a CPS must meet to address the challenges of\ntomorrow. Two key challenges are understanding and reinforcing the resilience\nof CPS.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 17:50:22 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 19:04:29 GMT"}, {"version": "v3", "created": "Fri, 8 Nov 2019 15:16:01 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Barbeau", "Michel", ""], ["Carle", "Georg", ""], ["Garcia-Alfaro", "Joaquin", ""], ["Torra", "Vicen\u00e7", ""]]}, {"id": "1907.08873", "submitter": "Emiliano De Cristofaro", "authors": "Despoina Chatzakou, Ilias Leontiadis, Jeremy Blackburn, Emiliano De\n  Cristofaro, Gianluca Stringhini, Athena Vakali, and Nicolas Kourtellis", "title": "Detecting Cyberbullying and Cyberaggression in Social Media", "comments": "To appear in ACM Transactions on the Web (TWEB)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyberbullying and cyberaggression are increasingly worrisome phenomena\naffecting people across all demographics. More than half of young social media\nusers worldwide have been exposed to such prolonged and/or coordinated digital\nharassment. Victims can experience a wide range of emotions, with negative\nconsequences such as embarrassment, depression, isolation from other community\nmembers, which embed the risk to lead to even more critical consequences, such\nas suicide attempts.\n  In this work, we take the first concrete steps to understand the\ncharacteristics of abusive behavior in Twitter, one of today's largest social\nmedia platforms. We analyze 1.2 million users and 2.1 million tweets, comparing\nusers participating in discussions around seemingly normal topics like the NBA,\nto those more likely to be hate-related, such as the Gamergate controversy, or\nthe gender pay inequality at the BBC station. We also explore specific\nmanifestations of abusive behavior, i.e., cyberbullying and cyberaggression, in\none of the hate-related communities (Gamergate). We present a robust\nmethodology to distinguish bullies and aggressors from normal Twitter users by\nconsidering text, user, and network-based attributes. Using various\nstate-of-the-art machine learning algorithms, we classify these accounts with\nover 90% accuracy and AUC. Finally, we discuss the current status of Twitter\nuser accounts marked as abusive by our methodology, and study the performance\nof potential mechanisms that can be used by Twitter to suspend users in the\nfuture.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 22:24:44 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Chatzakou", "Despoina", ""], ["Leontiadis", "Ilias", ""], ["Blackburn", "Jeremy", ""], ["De Cristofaro", "Emiliano", ""], ["Stringhini", "Gianluca", ""], ["Vakali", "Athena", ""], ["Kourtellis", "Nicolas", ""]]}, {"id": "1907.08922", "submitter": "Melvin Wevers", "authors": "Melvin Wevers", "title": "Using Word Embeddings to Examine Gender Bias in Dutch Newspapers,\n  1950-1990", "comments": "6 pages with appendix. Published in Proceedings of the 1st\n  International Workshop on Computational Approaches to Historical Language\n  Change 2019 co-organized with ACL2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contemporary debates on filter bubbles and polarization in public and social\nmedia raise the question to what extent news media of the past exhibited\nbiases. This paper specifically examines bias related to gender in six Dutch\nnational newspapers between 1950 and 1990. We measure bias related to gender by\ncomparing local changes in word embedding models trained on newspapers with\ndivergent ideological backgrounds. We demonstrate clear differences in gender\nbias and changes within and between newspapers over time. In relation to themes\nsuch as sexuality and leisure, we see the bias moving toward women, whereas,\ngenerally, the bias shifts in the direction of men, despite growing female\nemployment number and feminist movements. Even though Dutch society became less\nstratified ideologically (depillarization), we found an increasing divergence\nin gender bias between religious and social-democratic on the one hand and\nliberal newspapers on the other. Methodologically, this paper illustrates how\nword embeddings can be used to examine historical language change. Future work\nwill investigate how fine-tuning deep contextualized embedding models, such as\nELMO, might be used for similar tasks with greater contextual information.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 06:58:22 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Wevers", "Melvin", ""]]}, {"id": "1907.09335", "submitter": "Joao Meirelles", "authors": "William Wills, Joao Meirelles, Vivien Green Baptista, Gabriel Cury,\n  Pablo Cerdeira", "title": "A Simple Sinuosity-Based Method using GPS data to Support Mitigation\n  Policies for Public Buses GHG Emissions", "comments": "15 pages, 7 figures, Presented in the 1st Latin American SDEWES\n  Conference 2018 Rio de Janeiro", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is clear by now that climate change mitigation relies on our capacity to\nguide urban systems towards a low-carbon phase and that the urban\ntransportation sector plays a major role in this transition. It is estimated\nthat around 30% of total CO2 emissions worldwide come from the urban\ntransportation sector. Regardless of its importance, detailed estimations of\ntransport-related emissions in cities are still rare to find, hindering our\ncapacity to understand and reduce them. This work aims to develop a replicable\nand fast method for GHG estimation from GPS (Global Positioning System) data\nand to introduce a simple sinuosity-based algorithm for such. We applied the\nmethod for 1 year of GPS data in the city of Rio de Janeiro. Our results were\ncompared to top-down estimations from fuel consumption and proved to be valid\nafter a simple data filling process. Our GPS-based approach allowed for much\nfiner spatial and temporal descriptions of emissions and we further showed\npossible policy insights that can be extracted from the estimated emissions\nbased on the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 14:23:08 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 19:45:46 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Wills", "William", ""], ["Meirelles", "Joao", ""], ["Baptista", "Vivien Green", ""], ["Cury", "Gabriel", ""], ["Cerdeira", "Pablo", ""]]}, {"id": "1907.09600", "submitter": "Lorenzo A. Rossi", "authors": "Lorenzo A. Rossi, Chad Shawber, Janet Munu and Finly Zachariah", "title": "Evaluation of Embeddings of Laboratory Test Codes for Patients at a\n  Cancer Center", "comments": "2019 KDD Workshop on Applied Data Science for Healthcare (DSHealth,\n  August 2019, Anchorage, AK). Make sure you have downloaded the latest version\n  with the link to the DSHealth2019_loinc_embeddings GitHub repository:\n  https://github.com/elleros/DSHealth2019_loinc_embeddings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Laboratory test results are an important and generally high dimensional\ncomponent of a patient's Electronic Health Record (EHR). We train embedding\nrepresentations (via Word2Vec and GloVe) for LOINC codes of laboratory tests\nfrom the EHRs of about 80,000 patients at a cancer center. To include\ninformation about lab test outcomes, we also train embeddings on the\nconcatenation of a LOINC code with a symbol indicating normality or abnormality\nof the result. We observe several clinically meaningful similarities among\nLOINC embeddings trained over our data. For the embeddings of the concatenation\nof LOINCs with abnormality codes, we evaluate the performance for mortality\nprediction tasks and the ability to preserve ordinality properties: i.e. a lab\ntest with normal outcome should be more similar to an abnormal one than to the\na very abnormal one.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 21:58:40 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 15:29:44 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Rossi", "Lorenzo A.", ""], ["Shawber", "Chad", ""], ["Munu", "Janet", ""], ["Zachariah", "Finly", ""]]}, {"id": "1907.09726", "submitter": "Serhiy Semerikov", "authors": "Oleksandr Teplytskyi, Illia Teplytskyi, Serhiy Semerikov, Vladimir\n  Soloviev", "title": "Training future teachers in natural sciences and mathematics by means of\n  computer simulation: a social constructivist approach", "comments": "278 pages, in Ukrainian", "journal-ref": "Theory and methods of learning fundamental disciplines in high\n  school 10 (2015)", "doi": null, "report-no": null, "categories": "physics.ed-ph cs.CY math.HO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The monograph defines the conditions of training of future teachers in\nnatural sciences and mathematics by means of computer simulation, developed a\nstructural-functional model of training, selected socio-constructivist forms of\norganization, methods and learning tools of computer modeling for future\nteachers of natural and mathematical disciplines.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 07:25:54 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Teplytskyi", "Oleksandr", ""], ["Teplytskyi", "Illia", ""], ["Semerikov", "Serhiy", ""], ["Soloviev", "Vladimir", ""]]}, {"id": "1907.09745", "submitter": "Junjie Huang", "authors": "Junjie Huang, Tiejian Luo", "title": "Computing Lens for Exploring the Historical People's Social Network", "comments": "accepted at SoNet 2018", "journal-ref": "International Conference on Future Internet of Things and Cloud\n  Workshops (FiCloudW) 2018", "doi": "10.1109/w-ficloud.2018.00021", "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical social research topic is to figure out the influential people's\nrelationship and its weights. It is very tedious for social scientists to solve\nthose problems by studying massive literature. Digital humanities bring a new\nway to a social subject. In this paper, we propose a framework for social\nscientists to find out ancient figures' power and their camp. The core of our\nframework consists of signed graph model and novel group partition algorithm.\nWe validate and verify our solution by China Biographical Database Project\n(CBDB) dataset. The analytic results on a case study demonstrate the\neffectiveness of our framework, which gets information that consists with the\nliterature's facts and social scientists' viewpoints.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 08:18:41 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Huang", "Junjie", ""], ["Luo", "Tiejian", ""]]}, {"id": "1907.10322", "submitter": "Simon Tamayo Giraldo", "authors": "Sarah Manard (CAOR), Nicolas Vergos (CAOR), Simon Tamayo (CAOR),\n  Fr\\'ed\\'eric Fontane (CAOR)", "title": "Electronic health record in the era of industry 4.0: the French example", "comments": null, "journal-ref": "International Conference e-Health 2019, Jul 2019, Porto, Portugal", "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent implementation of the Electronic Health Record (EHR) in France is\npart of a more general process of digitizing information flows, as the world\nenters the fourth industrial revolution in a phenomenon known as Industry 4.0.\nBehind this concept lies the concern to allow Man to remain permanently in\ncontrol of his destiny, despite an increasingly interconnected world (Internet\nof Things, cooperative robots, augmented reality, etc.). Accordingly, the\nimplementation of EHR must guarantee the respect for the private life of each\ncitizen. From this perspective, healthcare professionals will therefore have to\nconstantly ensure the protection of medical confidentiality during Electronic\nData Interchange (EDI). This paper summarises the current state of the use of\nEHR in France. Based on a survey conducted by the European Commission to assess\nthe deployment of digitalisation in the health sector in EU countries, this\narticle aims to highlight the opportunities and perspectives that Industry 4.0\ncould bring to the health sector in France. However, this study also identifies\na number of limits related to the application of such a system, the first of\nwhich is cyber threat or transhumanism. To this end, a SWOT matrix identifies\nthe strengths and weaknesses related to the implementation of the French EHR.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 09:24:24 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 09:00:12 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Manard", "Sarah", "", "CAOR"], ["Vergos", "Nicolas", "", "CAOR"], ["Tamayo", "Simon", "", "CAOR"], ["Fontane", "Fr\u00e9d\u00e9ric", "", "CAOR"]]}, {"id": "1907.10384", "submitter": "Chirag Raman", "authors": "Chirag Raman, Hayley Hung", "title": "Towards automatic estimation of conversation floors within F-formations", "comments": "8th International Conference on Affective Computing & Intelligent\n  Interaction EMERGent Workshop, 7 pages, 4 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of free-standing conversing groups has received significant\nattention in recent years. In the absence of a formal definition, most studies\noperationalize the notion of a conversation group either through a spatial or a\ntemporal lens. Spatially, the most commonly used representation is the\nF-formation, defined by social scientists as the configuration in which people\narrange themselves to sustain an interaction. However, the use of this\nrepresentation is often accompanied with the simplifying assumption that a\nsingle conversation occurs within an F-formation. Temporally, various\ncategories have been used to organize conversational units; these include,\namong others, turn, topic, and floor. Some of these concepts are hard to define\nobjectively by themselves. The present work constitutes an initial exploration\ninto unifying these perspectives by primarily posing the question: can we use\nthe observation of simultaneous speaker turns to infer whether multiple\nconversation floors exist within an F-formation? We motivate a metric for the\nexistence of distinct conversation floors based on simultaneous speaker turns,\nand provide an analysis using this metric to characterize conversations across\nF-formations of varying cardinality. We contribute two key findings: firstly,\nat the average speaking turn duration of about two seconds for humans, there is\nevidence for the existence of multiple floors within an F-formation; and\nsecondly, an increase in the cardinality of an F-formation correlates with a\ndecrease in duration of simultaneous speaking turns.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 09:16:05 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 09:31:16 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Raman", "Chirag", ""], ["Hung", "Hayley", ""]]}, {"id": "1907.10385", "submitter": "Luisito Lacatan", "authors": "Yolanda D Austria, Luisito L. Lacatan, John Gregory D Funtera, Shawn\n  C. Garcia, Jonet H. Montenegro, Laymar T. Santilleces", "title": "Face Recognition for Motorcycle Engine Ignition with Messaging System", "comments": "9 pages, 9 figures, 2017 1st International Conference on Redesigning,\n  Re-engineering Academic Direction for Global Competitiveness", "journal-ref": "2018 International Journal of Computing Sciences Research (IJCSR)", "doi": "10.25147/ijcsr.2017.001.1.11", "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this current world where technology is growing up day by day and\nscientific researchers are presenting new era of discoveries, the need for\nsecurity is also increasing in all areas. At present, the vehicle usage is\nbasic necessity for everyone. Simultaneously, protecting the vehicle against\ntheft is also very important. Traditional vehicle security system depends on\nmany sensors and cost is also high. When the vehicle is stolen, no more\nresponse or alternative could be available to help the owner of the vehicle to\nfind it back. The main goal of this paper is to protect the vehicle from any\nunauthorized access, using fast, easy-to-use, clear, reliable and economical\nface recognition technique. An efficient automotive security system is\nimplemented for anti-theft using an embedded system for starting the engine by\nthe use of face recognition and integrated with Global Positioning System (GPS)\nand Global System for Mobile Communication (GSM). This proposed work is an\nattempt to design and develop a smart anti-theft system that uses Face\nrecognition, GPS and GSM system to prevent theft and to determine the exact\nlocation of vehicle.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 14:26:08 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Austria", "Yolanda D", ""], ["Lacatan", "Luisito L.", ""], ["Funtera", "John Gregory D", ""], ["Garcia", "Shawn C.", ""], ["Montenegro", "Jonet H.", ""], ["Santilleces", "Laymar T.", ""]]}, {"id": "1907.10401", "submitter": "Camille Roth", "authors": "Camille Roth (CAMS, CMB)", "title": "Algorithmic Distortion of Informational Landscapes", "comments": null, "journal-ref": "Intellectica, In press", "doi": null, "report-no": null, "categories": "cs.CY cs.IR cs.LG cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The possible impact of algorithmic recommendation on the autonomy and free\nchoice of Internet users is being increasingly discussed, especially in terms\nof the rendering of information and the structuring of interactions. This paper\naims at reviewing and framing this issue along a double dichotomy. The first\none addresses the discrepancy between users' intentions and actions (1) under\nsome algorithmic influence and (2) without it. The second one distinguishes\nalgorithmic biases on (1) prior information rearrangement and (2) posterior\ninformation arrangement. In all cases, we focus on and differentiate situations\nwhere algorithms empirically appear to expand the cognitive and social horizon\nof users, from those where they seem to limit that horizon. We additionally\nsuggest that these biases may not be properly appraised without taking into\naccount the underlying social processes which algorithms are building upon.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 08:13:18 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Roth", "Camille", "", "CAMS, CMB"]]}, {"id": "1907.10407", "submitter": "Joseph Attia", "authors": "Joseph Attia", "title": "Evaluating the Effectiveness of Common Technical Trading Models", "comments": "43 pages. arXiv admin note: text overlap with arXiv:1902.00786", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  How effective are the most common trading models? The answer may help\ninvestors realize upsides to using each model, act as a segue for investors\ninto more complex financial analysis and machine learning, and to increase\nfinancial literacy amongst students. Creating original versions of popular\nmodels, like linear regression, K-Nearest Neighbor, and moving average\ncrossovers, we can test how each model performs on the most popular stocks and\nlargest indexes. With the results for each, we can compare the models, and\nunderstand which model reliably increases performance. The trials showed that\nwhile all three models reduced losses on stocks with strong overall downward\ntrends, the two machine learning models did not work as well to increase\nprofits. Moving averages crossovers outperformed a continuous investment every\ntime, although did result in a more volatile investment as well. Furthermore,\nonce finished creating the program that implements moving average crossover,\nwhat are the optimal periods to use? A massive test consisting of 169,880\ntrials, showed the best periods to use to increase investment performance\n(5,10) and to decrease volatility (33,44). In addition, the data showed\nnumerous trends such as a smaller short SMA period is accompanied by higher\nperformance. Plotting volatility against performance shows that the high risk,\nhigh reward saying holds true and shows that for investments, as the volatility\nincreases so does its performance.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 03:21:16 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Attia", "Joseph", ""]]}, {"id": "1907.10424", "submitter": "Andrea Polonioli PhD", "authors": "Ciro Greco, Andrea Polonioli and Jacopo Tagliabue", "title": "Less (Data) Is More: Why Small Data Holds the Key to the Future of\n  Artificial Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The claims that big data holds the key to enterprise successes and that\nArtificial Intelligence is going to replace humanity have become increasingly\nmore popular over the past few years, both in academia and in the industry.\nHowever, while these claims may indeed capture some truth, they have also been\nmassively oversold, or so we contend here. The goal of this paper is two-fold.\nFirst, we provide a qualified defence of the value of less data within the\ncontext of AI. This is done by carefully reviewing two distinct problems for\nbig data driven AI, namely a) the limited track record of Deep Learning in key\nareas such as Natural Language Processing, b) the regulatory and business\nsignificance of being able to learn from few data points. Second, we briefly\nsketch what we refer to as a case of AI with humans and for humans, namely an\nAI paradigm whereby the systems we build are privacy-oriented and focused on\nhuman-machine collaboration, not competition. Combining our claims above, we\nconclude that when seen through the lens of cognitively inspired AI, the bright\nfuture of the discipline is about less data, not more, and more humans, not\nfewer.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 15:57:00 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Greco", "Ciro", ""], ["Polonioli", "Andrea", ""], ["Tagliabue", "Jacopo", ""]]}, {"id": "1907.10429", "submitter": "Roufaida Laidi", "authors": "Roufaida Laidi, Djamel Djenouri, Marc Ringel", "title": "Commercial Technologies for Advanced Light Control in Smart Building\n  Energy Management Systems: A Comparative Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates the economic, social, and environmental impact of\nadopting different smart lighting architectures for home automation in two\ngeographical and regulatory regions: Algiers, Algeria, and Stuttgart, Germany.\nLighting consumes a considerable amount of energy, and devices for smart\nlight-ing solutions are among the most purchased smart home devices. As\ncommercial-ized solutions come with variant features, we empirically evaluate\nthrough this study the impact of each one of the energy-related features and\nprovide insights on those that have higher energy saving contribution. The\nstudy started by investigating the state-of-the-art of commercialized ICT-based\nlight control solutions, which allowed the extraction of the energy-related\nfeatures. Based on the outcomes of this study, we generated simulation\nscenarios and selected evaluations metrics to evaluate the impact of dimming,\ndaylight harvesting, scheduling, and motion detection. The simulation study has\nbeen conducted using \\textit{EnergyPlus} simulation tool, which enables\nfine-grained realistic evaluation. The results show that adopting smart\nlighting technologies have a payback period of few years, and that the use of\nthese technologies has positive economic and societal impacts, as well as on\nthe environment by considerably reducing gas emissions. However, this positive\ncontribution is highly sensitive to the geographical location, energy prices,\nand the occupancy profile.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 07:31:51 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Laidi", "Roufaida", ""], ["Djenouri", "Djamel", ""], ["Ringel", "Marc", ""]]}, {"id": "1907.10508", "submitter": "Alexantrou Serb", "authors": "Alexander Serb, Themistoklis Prodromakis", "title": "A system of different layers of abstraction for artificial intelligence", "comments": "12 pages, 1 figure, 1 table, submitted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of artificial intelligence (AI) represents an enormous endeavour of\nhumankind that is currently transforming our societies down to their very\nfoundations. Its task, building truly intelligent systems, is underpinned by a\nvast array of subfields ranging from the development of new electronic\ncomponents to mathematical formulations of highly abstract and complex\nreasoning. This breadth of subfields renders it often difficult to understand\nhow they all fit together into a bigger picture and hides the multi-faceted,\nmulti-layered conceptual structure that in a sense can be said to be what AI\ntruly is. In this perspective we propose a system of five levels/layers of\nabstraction that underpin many AI implementations. We further posit that each\nlayer is subject to a complexity-performance trade-off whilst different layers\nare interlocked with one another in a control-complexity trade-off. This\noverview provides a conceptual map that can help to identify how and where\ninnovation should be targeted in order to achieve different levels of\nfunctionality, assure them for safety, optimise performance under various\noperating constraints and map the opportunity space for social and economic\nexploitation.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 08:09:14 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Serb", "Alexander", ""], ["Prodromakis", "Themistoklis", ""]]}, {"id": "1907.10554", "submitter": "Guanglin Tang", "authors": "Guanglin Tang, Yulong Yan, Chenyang Shen, Xun Jia, Meyer Zinn,\n  Zipalkumar Trivedi, Alicia Yingling, Kenneth Westover, Steve Jiang", "title": "Development of a Real-time Indoor Location System using Bluetooth Low\n  Energy Technology and Deep Learning to Facilitate Clinical Applications", "comments": "20 pages, 6 figures, submitted to Physics in Medicine & Biology", "journal-ref": null, "doi": "10.1002/mp.14198", "report-no": null, "categories": "eess.SP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An indoor, real-time location system (RTLS) can benefit both hospitals and\npatients by improving clinical efficiency through data-driven optimization of\nprocedures. Bluetooth-based RTLS systems are cost-effective but lack accuracy\nand robustness because Bluetooth signal strength is subject to fluctuation. We\ndeveloped a machine learning-based solution using a Long Short-Term Memory\n(LSTM) network followed by a Multilayer Perceptron classifier and a posterior\nconstraint algorithm to improve RTLS performance. Training and validation\ndatasets showed that most machine learning models perform well in classifying\nindividual location zones, although LSTM was most reliable. However, when faced\nwith data indicating cross-zone trajectories, all models showed erratic zone\nswitching. Thus, we implemented a history-based posterior constraint algorithm\nto reduce the variability in exchange for a slight decrease in responsiveness.\nThis network increases robustness at the expense of latency. When latency is\nless of a concern, we computed the latency-corrected accuracy which is 100% for\nour testing data, significantly improved from LSTM without constraint which is\n96.2%. The balance between robustness and responsiveness can be considered and\nadjusted on a case-by-case basis, according to the specific needs of downstream\nclinical applications. This system was deployed and validated in an academic\nmedical center. Industry best practices enabled system scaling without\nsubstantial compromises to performance or cost.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 16:47:47 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 16:11:35 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Tang", "Guanglin", ""], ["Yan", "Yulong", ""], ["Shen", "Chenyang", ""], ["Jia", "Xun", ""], ["Zinn", "Meyer", ""], ["Trivedi", "Zipalkumar", ""], ["Yingling", "Alicia", ""], ["Westover", "Kenneth", ""], ["Jiang", "Steve", ""]]}, {"id": "1907.10597", "submitter": "Roy Schwartz", "authors": "Roy Schwartz, Jesse Dodge, Noah A. Smith, Oren Etzioni", "title": "Green AI", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.CV cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computations required for deep learning research have been doubling every\nfew months, resulting in an estimated 300,000x increase from 2012 to 2018 [2].\nThese computations have a surprisingly large carbon footprint [38]. Ironically,\ndeep learning was inspired by the human brain, which is remarkably energy\nefficient. Moreover, the financial cost of the computations can make it\ndifficult for academics, students, and researchers, in particular those from\nemerging economies, to engage in deep learning research.\n  This position paper advocates a practical solution by making efficiency an\nevaluation criterion for research alongside accuracy and related measures. In\naddition, we propose reporting the financial cost or \"price tag\" of developing,\ntraining, and running models to provide baselines for the investigation of\nincreasingly efficient methods. Our goal is to make AI both greener and more\ninclusive---enabling any inspired undergraduate with a laptop to write\nhigh-quality research papers. Green AI is an emerging focus at the Allen\nInstitute for AI.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 19:36:18 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 02:54:44 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 20:09:57 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Schwartz", "Roy", ""], ["Dodge", "Jesse", ""], ["Smith", "Noah A.", ""], ["Etzioni", "Oren", ""]]}, {"id": "1907.10672", "submitter": "Jos\\'e Gonz\\'alez Caba\\~nas", "authors": "\\'Angel Cuevas, Jos\\'e Gonz\\'alez Caba\\~nas, Aritz Arrate, Rub\\'en\n  Cuevas", "title": "Does Facebook Use Sensitive Data for Advertising Purposes? Worldwide\n  Analysis and GDPR Impact", "comments": "6 pages, 3 figures, 3 tables. arXiv admin note: text overlap with\n  arXiv:1802.05030", "journal-ref": "Communications of the ACM 64 (1) (2021) 62-69", "doi": "10.1145/3426361", "report-no": null, "categories": "cs.SI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent European General Data Protection Regulation (GDPR) and other data\nprotection regulations restrict the processing of some categories of personal\ndata (health, political orientation, sexual preferences, religious beliefs,\nethnic origin, etc.) due to the privacy risks associated to such information.\nThe GDPR refers to these categories as sensitive personal data. This paper\nquantifies the portion of Facebook (FB) users, across 197 countries, who are\nlabeled with advertising interests linked to potentially sensitive personal\ndata. Our study reveals that Facebook labels 67% of users with potential\nsensitive interests. This corresponds to 22% of the population in the referred\n197 countries. Moreover, our work shows that the GDPR enforcement had a\nnegligible impact in this context since the portion of FB users labeled with\nsensitive interests in the European Union remains almost the same 5 months\nbefore and 9 months after the GDPR was enacted. The paper also illustrates\npotential risks associated to the use of sensitive interests. For instance, we\nquantify the portion of FB users labelled with the interest \"Homosexuality\" in\ncountries where being gay may be punished with the death penalty. The last\ncontribution is the implementation of a web browser extension that allows FB\nusers removing in a simple way the potentially sensitive interests FB has\nassigned them.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 09:45:35 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Cuevas", "\u00c1ngel", ""], ["Caba\u00f1as", "Jos\u00e9 Gonz\u00e1lez", ""], ["Arrate", "Aritz", ""], ["Cuevas", "Rub\u00e9n", ""]]}, {"id": "1907.10855", "submitter": "William Buchanan Prof", "authors": "Shane Murnion, William J. Buchanan, Adrian Smales and Gordon Russell", "title": "Machine learning and semantic analysis of in-game chat for cyberbullying", "comments": null, "journal-ref": "Computers & Security 76 (2018): 197-213", "doi": "10.1016/j.cose.2018.02.016", "report-no": null, "categories": "cs.CR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One major problem with cyberbullying research is the lack of data, since\nresearchers are traditionally forced to rely on survey data where victims and\nperpetrators self-report their impressions. In this paper, an automatic data\ncollection system is presented that continuously collects in-game chat data\nfrom one of the most popular online multi-player games: World of Tanks. The\ndata was collected and combined with other information about the players from\navailable online data services. It presents a scoring scheme to enable\nidentification of cyberbullying based on current research. Classification of\nthe collected data was carried out using simple feature detection with SQL\ndatabase queries and compared to classification from AI-based sentiment text\nanalysis services that have recently become available and further against\nmanually classified data using a custom-built classification client built for\nthis paper. The simple SQL classification proved to be quite useful at\nidentifying some features of toxic chat such as the use of bad language or\nracist sentiments, however the classification by the more sophisticated online\nsentiment analysis services proved to be disappointing. The results were then\nexamined for insights into cyberbullying within this game and it was shown that\nit should be possible to reduce cyberbullying within the World of Tanks game by\na significant factor by simply freezing the player's ability to communicate\nthrough the in-game chat function for a short period after the player is killed\nwithin a match. It was also shown that very new players are much less likely to\nengage in cyberbullying, suggesting that it may be a learned behaviour from\nother players.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 06:35:49 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Murnion", "Shane", ""], ["Buchanan", "William J.", ""], ["Smales", "Adrian", ""], ["Russell", "Gordon", ""]]}, {"id": "1907.11057", "submitter": "Andreas Veneris", "authors": "Andreas Veneris, Andreas Park", "title": "Special Drawing Rights in a New Decentralized Century", "comments": "4 pages, IMF Georgetown", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unfulfilled expectations from macro-economic initiatives during the Great\nRecession and the massive shift into globalization echo today with political\nupheaval, anti-establishment propaganda, and looming trade/currency wars that\nthreaten domestic and international value chains. Once stable entities like the\nEU now look fragile and political instability in the US presents unprecedented\nchallenges to an International Monetary System (IMS) that predominantly relies\non the USD and EUR as reserve currencies. In this environment, it is critical\nfor an international organization mandated to ensure stability to plan and act\nahead. This paper argues that Decentralized Ledger-based technology (DLT) is\nkey for the International Monetary Fund (IMF) to mitigate some of those risks,\npromote stability and safeguard world prosperity. Over the last two years, DLT\nhas made headline news globally and created a worldwide excitement not seen\nsince the internet entered the mainstream. The rapid adoption and open-to-all\nphilosophy of DLT has already redefined global socioeconomics, promises to\nshake up the world of commerce/finance and challenges the workings of central\ngovernments/regulators. This paper examines DLT core premises and proposes a\ntwo-step approach for the IMF to expand Special Drawing Rights (SDR) into that\nsphere so as to become the originally envisioned numeraire and reserve currency\nfor cross-border transactions in this new decentralized century.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 00:52:04 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Veneris", "Andreas", ""], ["Park", "Andreas", ""]]}, {"id": "1907.11073", "submitter": "Michael Bommarito II", "authors": "Ethan Bommarito, Michael Bommarito", "title": "An Empirical Analysis of the Python Package Index (PyPI)", "comments": "15 pages, 2 figures, 20 tables; initial draft for public comment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CY physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research, we provide a comprehensive empirical summary of the Python\nPackage Repository, PyPI, including both package metadata and source code\ncovering 178,592 packages, 1,745,744 releases, 76,997 contributors, and\n156,816,750 import statements. We provide counts and trends for packages,\nreleases, dependencies, category classifications, licenses, and package\nimports, as well as authors, maintainers, and organizations. As one of the\nlargest and oldest software repositories as of publication, PyPI provides\ninsight not just into the Python ecosystem today, but also trends in software\ndevelopment and licensing more broadly over time. Within PyPI, we find that the\ngrowth of the repository has been robust under all measures, with a compound\nannual growth rate of 47% for active packages, 39% for new authors, and 61% for\nnew import statements over the last 15 years. As with many similar social\nsystems, we find a number of highly right-skewed distributions, including the\ndistribution of releases per package, packages and releases per author, imports\nper package, and size per package and release. However, we also find that most\npackages are contributed by single individuals, not multiple individuals or\norganizations. The data, methods, and calculations herein provide an anchor for\npublic discourse on PyPI and serve as a foundation for future research on the\nPython software ecosystem.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 14:11:32 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 01:59:41 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Bommarito", "Ethan", ""], ["Bommarito", "Michael", ""]]}, {"id": "1907.11274", "submitter": "Aviv Ovadya", "authors": "Aviv Ovadya, Jess Whittlestone", "title": "Reducing malicious use of synthetic media research: Considerations and\n  potential release practices for machine learning", "comments": "11 pages. Language fixes and tweaks for clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to facilitate nuanced discussion around research\nnorms and practices to mitigate the harmful impacts of advances in machine\nlearning (ML). We focus particularly on the use of ML to create \"synthetic\nmedia\" (e.g. to generate or manipulate audio, video, images, and text), and the\nquestion of what publication and release processes around such research might\nlook like, though many of the considerations discussed will apply to ML\nresearch more broadly. We are not arguing for any specific approach on when or\nhow research should be distributed, but instead try to lay out some useful\ntools, analogies, and options for thinking about these issues.\n  We begin with some background on the idea that ML research might be misused\nin harmful ways, and why advances in synthetic media, in particular, are\nraising concerns. We then outline in more detail some of the different paths to\nharm from ML research, before reviewing research risk mitigation strategies in\nother fields and identifying components that seem most worth emulating in the\nML and synthetic media research communities. Next, we outline some important\ndimensions of disagreement on these issues which risk polarizing conversations.\n  Finally, we conclude with recommendations, suggesting that the machine\nlearning community might benefit from: working with subject matter experts to\nincrease understanding of the risk landscape and possible mitigation\nstrategies; building a community and norms around understanding the impacts of\nML research, e.g. through regular workshops at major conferences; and\nestablishing institutions and systems to support release practices that would\notherwise be onerous and error-prone.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 18:51:45 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 02:01:40 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Ovadya", "Aviv", ""], ["Whittlestone", "Jess", ""]]}, {"id": "1907.11498", "submitter": "Mohammed Khwaja", "authors": "Mohammed Khwaja and Aleksandar Matic", "title": "Personality is Revealed During Weekends: Towards Data Minimisation for\n  Smartphone Based Personality Classification", "comments": null, "journal-ref": "In 17th IFIP Conference on Human-Computer Interaction (INTERACT).\n  Springer, 2019", "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous literature has explored automatic personality modelling using\nsmartphone data for its potential to personalise mobile services. Although\npassive modelling of personality removes the burden of completing lengthy\nquestionnaires, the fact that such models typically require a few weeks or\nmonths of personal data can negatively impact user's engagement. In this study,\nwe explore the feasibility of reducing the duration of data collection in the\ncontext of personality classification. We found that only one or two weekends\ncan suffice for achieving state-of-the-art accuracy between 66% and 71% for\nclassifying the five personality traits. These results provide lessons for\npracticing \"data minimisation\" - a key principle of privacy laws.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 11:40:29 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 00:21:32 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Khwaja", "Mohammed", ""], ["Matic", "Aleksandar", ""]]}, {"id": "1907.11523", "submitter": "Mohamed Khalifa", "authors": "Mohamed Khalifa, Farah Magrabi, Blanca Gallego", "title": "Evaluating the Impact of Using GRASP Framework on Clinicians and\n  Healthcare Professionals Decisions in Selecting Clinical Predictive Tools", "comments": "42 pages, 9 figures, and 13 tables. arXiv admin note: text overlap\n  with arXiv:1907.03706, arXiv:1907.11524", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background. When selecting predictive tools, clinicians and healthcare\nprofessionals are challenged with an overwhelming number of tools, most of\nwhich have never been evaluated for comparative effectiveness. To overcome this\nchallenge, the authors developed and validated an evidence-based framework for\ngrading and assessment of predictive tools (GRASP), based on the critical\nappraisal of published evidence. Methods. To examine GRASP impact on\nprofessionals decisions, a controlled experiment was conducted through an\nonline survey. Randomising two groups of tools and two scenarios; participants\nwere asked to select the best tools; most validated or implemented, with and\nwithout GRASP. A wide group of international participants were invited. Task\ncompletion time, rate of correct decisions, rate of objective vs subjective\ndecisions, and level of decisional conflict were measured. Results. Valid\nresponses received were 194. Compared to not using the framework, GRASP\nsignificantly increased correct decisions by 64% (T=8.53, p<0.001), increased\nobjective decision making by 32% (T=9.24, p<0.001), and decreased subjective\ndecision making; based on guessing and based on prior knowledge or experience\nby 20% (T=-5.47, p<0.001) and 8% (T=-2.99, p=0.003) respectively. GRASP\nsignificantly decreased decisional conflict; increasing confidence and\nsatisfaction of participants with their decisions by 11% (T=4.27, p<0.001) and\n13% (T=4.89, p<0.001) respectively. GRASP decreased task completion time by 52%\n(T=-0.87, p=0.384). The average system usability scale of GRASP was very good;\n72.5%, and 88% of participants found GRASP useful. Discussion and Conclusions.\nUsing GRASP has positively supported and significantly improved evidence-based\ndecision making and increased accuracy and efficiency of selecting predictive\ntools.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 02:32:01 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Khalifa", "Mohamed", ""], ["Magrabi", "Farah", ""], ["Gallego", "Blanca", ""]]}, {"id": "1907.11524", "submitter": "Mohamed Khalifa", "authors": "Mohamed Khalifa, Farah Magrabi, Blanca Gallego", "title": "Validating and Updating GRASP: A New Evidence-Based Framework for\n  Grading and Assessment of Clinical Predictive Tools", "comments": "arXiv admin note: substantial text overlap with arXiv:1907.03706,\n  arXiv:1907.11523", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: When selecting predictive tools, for implementation in clinical\npractice or for recommendation in guidelines, clinicians are challenged with an\noverwhelming and ever-growing number of tools. Many of these have never been\nimplemented or evaluated for comparative effectiveness. The authors developed\nan evidence-based framework for grading and assessment of predictive tools\n(GRASP), based on critical appraisal of published evidence. The objective of\nthis study is to validate, update GRASP, and evaluate its reliability. Methods:\nWe aimed at validating and updating GRASP through surveying a wide\ninternational group of experts then evaluating GRASP reliability. Results: Out\nof 882 invited experts, 81 valid responses were received. Experts overall\nstrongly agreed to GRASP evaluation criteria of predictive tools (4.35/5).\nExperts strongly agreed to six criteria; predictive performance (4.87/5),\npredictive performance levels (4.44/5), usability (4.68/5), potential effect\n(4.61/5), post-implementation impact (4.78/5) and evidence direction (4.26/5).\nExperts somewhat agreed to one criterion; post-implementation impact levels\n(4.16/5). Experts were neutral about one criterion; usability is higher than\npotential effect (2.97/5). Experts also provided recommendations to six\nopen-ended questions regarding adding, removing or changing evaluation\ncriteria. The GRASP concept and its detailed report were updated then the\ninterrater reliability of GRASP was tested and found to be reliable. Discussion\nand Conclusion: The GRASP framework grades predictive tools based on the\ncritical appraisal of the published evidence across three dimensions: 1) Phase\nof evaluation; 2) Level of evidence; and 3) Direction of evidence. The final\ngrade of a tool is based on the highest phase of evaluation, supported by the\nhighest level of positive evidence, or mixed evidence that supports positive\nconclusion.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 02:49:19 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Khalifa", "Mohamed", ""], ["Magrabi", "Farah", ""], ["Gallego", "Blanca", ""]]}, {"id": "1907.11624", "submitter": "Hansi Zhang", "authors": "Hansi Zhang, Christopher Wheldon, Adam G. Dunn, Cui Tao, Jinhai Huo,\n  Rui Zhang, Mattia Prosperi, Yi Guo, Jiang Bian", "title": "Mining Twitter to Assess the Determinants of Health Behavior towards\n  Human Papillomavirus Vaccination in the United States", "comments": "6 figures, 5 tables, Journal of the American Medical Informatics\n  Association, Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives To test the feasibility of using Twitter data to assess\ndeterminants of consumers' health behavior towards Human papillomavirus (HPV)\nvaccination informed by the Integrated Behavior Model (IBM).\n  Methods We used three Twitter datasets spanning from 2014 to 2018. We\npreprocessed and geocoded the tweets, and then built a rule-based model that\nclassified each tweet into either promotional information or consumers'\ndiscussions. We applied topic modeling to discover major themes, and\nsubsequently explored the associations between the topics learned from\nconsumers' discussions and the responses of HPV-related questions in the Health\nInformation National Trends Survey (HINTS).\n  Results We collected 2,846,495 tweets and analyzed 335,681 geocoded tweets.\nThrough topic modeling, we identified 122 high-quality topics. The most\ndiscussed consumer topic is \"cervical cancer screening\"; while in promotional\ntweets, the most popular topic is to increase awareness of \"HPV causes cancer\".\n87 out of the 122 topics are correlated between promotional information and\nconsumers' discussions. Guided by IBM, we examined the alignment between our\nTwitter findings and the results obtained from HINTS. 35 topics can be mapped\nto HINTS questions by keywords, 112 topics can be mapped to IBM constructs, and\n45 topics have statistically significant correlations with HINTS responses in\nterms of geographic distributions.\n  Conclusion Not only mining Twitter to assess consumers' health behaviors can\nobtain results comparable to surveys but can yield additional insights via a\ntheory-driven approach. Limitations exist, nevertheless, these encouraging\nresults impel us to develop innovative ways of leveraging social media in the\nchanging health communication landscape.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 18:51:51 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Zhang", "Hansi", ""], ["Wheldon", "Christopher", ""], ["Dunn", "Adam G.", ""], ["Tao", "Cui", ""], ["Huo", "Jinhai", ""], ["Zhang", "Rui", ""], ["Prosperi", "Mattia", ""], ["Guo", "Yi", ""], ["Bian", "Jiang", ""]]}, {"id": "1907.12090", "submitter": "Yasushi Ota", "authors": "Yasushi Ota and Naoki Mizutani", "title": "Estimating Parameters in Mathematical Model for Societal Booms through\n  Bayesian Inference Approach", "comments": "14pages with 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, based on our previous study, we examined the mathematical\nproperties, especially the stability of the equilibrium for our proposed\nmathematical model. By means of the results of the stability in this study, we\nalso used actual data representing transient booms and resurgent booms, and\nconducted parameter estimation for our proposed model using Bayesian inference.\nIn addition, we conducted a model fitting to five actual data. By this study,\nwe reconfirmed that we can express the resurgences or minute vibrations of\nactual data by means of our proposed model.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 14:37:56 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 15:19:06 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Ota", "Yasushi", ""], ["Mizutani", "Naoki", ""]]}, {"id": "1907.12221", "submitter": "William Buchanan Prof", "authors": "Simon Dyson, William J Buchanan, Liam Bell", "title": "The Challenges of Investigating Cryptocurrencies and Blockchain Related\n  Crime", "comments": null, "journal-ref": "The Journal of The British Blockchain Association, 1(2), 5779,\n  2018", "doi": "10.31585/jbba-1-2-(8)2018", "report-no": null, "categories": "cs.CY cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We increasingly live in a world where there is a balance between the rights\nto privacy and the requirements for consent, and the rights of society to\nprotect itself. Within this world, there is an ever-increasing requirement to\nprotect the identities involved within financial transactions, but this makes\nthings increasingly difficult for law enforcement agencies, especially in terms\nof financial fraud and money laundering. This paper reviews the\nstate-of-the-art in terms of the methods of privacy that are being used within\ncryptocurrency transactions, and in the challenges that law enforcement face.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 05:54:27 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Dyson", "Simon", ""], ["Buchanan", "William J", ""], ["Bell", "Liam", ""]]}, {"id": "1907.12370", "submitter": "Henry M. Kim", "authors": "Shivam Saxena, Hany Farag, Aidan Brookson, Hjalmar Turesson, Henry M.\n  Kim", "title": "Design and Field Implementation of Blockchain Based Renewable Energy\n  Trading in Residential Communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.CY cs.SY eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a peer to peer (P2P), blockchain based energy trading\nmarket platform for residential communities with the objective of reducing\noverall community peak demand and household electricity bills. Smart homes\nwithin the community place energy bids for its available distributed energy\nresources (DERs) for each discrete trading period during a day, and a double\nauction mechanism is used to clear the market and compute the market clearing\nprice (MCP). The marketplace is implemented on a permissioned blockchain\ninfrastructure, where bids are stored to the immutable ledger and smart\ncontracts are used to implement the MCP calculation and award service contracts\nto all winning bids. Utilizing the blockchain obviates the need for a trusted,\ncentralized auctioneer, and eliminates vulnerability to a single point of\nfailure. Simulation results show that the platform enables a community peak\ndemand reduction of 46%, as well as a weekly savings of 6%. The platform is\nalso tested at a real-world Canadian microgrid using the Hyperledger Fabric\nblockchain framework, to show the end to end connectivity of smart home DERs to\nthe platform.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 06:53:26 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Saxena", "Shivam", ""], ["Farag", "Hany", ""], ["Brookson", "Aidan", ""], ["Turesson", "Hjalmar", ""], ["Kim", "Henry M.", ""]]}, {"id": "1907.12386", "submitter": "Kaylee Bodner", "authors": "P. Murali Doraiswamy, Charlotte Blease, Kaylee Bodner", "title": "Artificial Intelligence and the Future of Psychiatry: Insights from a\n  Global Physician Survey", "comments": "30 pages, 7 tables, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Futurists have predicted that new technologies, embedded with artificial\nintelligence (AI) and machine learning (ML), will lead to substantial job loss\nin many sectors disrupting many aspects of healthcare. Mental health appears\nripe for such disruption given the global illness burden, stigma, and shortage\nof care providers. Using Sermo, a global networking platform open to verified\nand licensed physicians, we measured the opinions of psychiatrists about the\nlikelihood that future autonomous technology (referred to as AI/ML) would be\nable to fully replace the average psychiatrist in performing 10 key tasks (e.g.\nmental status exam, suicidality assessment, treatment planning) carried out in\nmental health care. Survey respondents were 791 psychiatrists from 22\ncountries. Only 3.8% of respondents felt that AI/ML was likely to replace a\nhuman clinician for providing empathetic care. Documenting (e.g. updating\nmedical records) and synthesizing information to reach a diagnosis were the two\ntasks where a majority predicted that future AI/ML would replace human doctors.\nAbout 1 in 2 doctors believed their jobs could be changed substantially by\nfuture AI/ML. However, female and US-based doctors were more uncertain that the\npossible benefits of AI would outweigh potential risks, versus their male and\nglobal counterparts. To our knowledge, this is the first global survey to seek\nthe opinions of physicians on the impact of autonomous AI/ML on the future of\npsychiatry. Our findings provide compelling insights into how physicians think\nabout intelligent technologies which may better help us integrate such tools\nand reskill doctors, as needed, to enhance mental health care.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 12:41:50 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Doraiswamy", "P. Murali", ""], ["Blease", "Charlotte", ""], ["Bodner", "Kaylee", ""]]}, {"id": "1907.12393", "submitter": "The Anh Han", "authors": "The Anh Han, Luis Moniz Pereira, Francisco C. Santos and Tom Lenaerts", "title": "To regulate or not: a social dynamics analysis of the race for AI\n  supremacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid technological advancements in AI as well as the growing deployment of\nintelligent technologies in new application domains are currently driving the\ncompetition between businesses, nations and regions. This race for\ntechnological supremacy creates a complex ecology of choices that may lead to\nnegative consequences, in particular, when ethical and safety procedures are\nunderestimated or even ignored. As a consequence, different actors are urging\nto consider both the normative and social impact of these technological\nadvancements. As there is no easy access to data describing this AI race,\ntheoretical models are necessary to understand its dynamics, allowing for the\nidentification of when, how and which procedures need to be put in place to\nfavour outcomes beneficial for all. We show that, next to the risks of setbacks\nand being reprimanded for unsafe behaviour, the time-scale in which AI\nsupremacy can be achieved plays a crucial role. When this supremacy can be\nachieved in a short term, those who completely ignore the safety precautions\nare bound to win the race but at a cost to society, apparently requiring\nregulatory actions. Our analysis reveals that blindly imposing regulations may\nnot have anticipated effect as only for specific conditions a dilemma arises\nbetween what individually preferred and globally beneficial. Similar\nobservations can be made for the long-term development case. Yet different from\nthe short term situation, certain conditions require the promotion of\nrisk-taking as opposed to compliance to safety regulations in order to improve\nsocial welfare. These results remain robust when two or several actors are\ninvolved in the race and when collective rather than individual setbacks are\nproduced by risk-taking behaviour. When defining codes of conduct and\nregulatory policies for AI, a clear understanding about the time-scale of the\nrace is required.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 14:59:27 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 16:48:16 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Han", "The Anh", ""], ["Pereira", "Luis Moniz", ""], ["Santos", "Francisco C.", ""], ["Lenaerts", "Tom", ""]]}, {"id": "1907.12457", "submitter": "Mariano Leva", "authors": "Daniele Sora, Massimo Meceella, Francesco Leotta, Giuseppe Bracone,\n  Daniele Buonanno, Mario Caruso, Adriano Cerocchi and Mariano Leva", "title": "Micro-accounting for optimizing and saving energy in smart buildings", "comments": null, "journal-ref": "Lecture Notes in Business Information Processing 2016", "doi": "10.1007/978-3-319-39564-7_15", "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy management, and in particular its optimization, is one of the hot\ntrends in the current days, both at the enterprise level (optimization of whole\ncorporate/government buildings) and single citizens' homes. The current trend\nis to provide knowledge about the micro(scopic) energy consumption. This allows\nto save energy, but also to optimize the dfferent energy sources (e.g., solar\nvs. traditional one) in case of a mixed architecture. In this work, after\nbriefly introducing our specific platform for smart environments able to\nmicro-account energy consumption of devices, we present two case studies of its\nutilization: energy saving in offices and smart switching among dfferent energy\nsources.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 14:37:21 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Sora", "Daniele", ""], ["Meceella", "Massimo", ""], ["Leotta", "Francesco", ""], ["Bracone", "Giuseppe", ""], ["Buonanno", "Daniele", ""], ["Caruso", "Mario", ""], ["Cerocchi", "Adriano", ""], ["Leva", "Mariano", ""]]}, {"id": "1907.12549", "submitter": "Wei Yan Ph.D.", "authors": "Wei Yan", "title": "Augmented Reality Applied to LEGO Construction: AR-based Building\n  Instructions with High Accuracy & Precision and Realistic Object-Hand\n  Occlusions", "comments": "Accompanying project video:\n  https://www.youtube.com/watch?v=7JDW_lDv7FU and LinkedIn article:\n  https://www.linkedin.com/pulse/augmented-reality-lego-construction-iphone-wei-yan/\n  In version 4, algorithms are explained in more detail, more evaluations are\n  added, and references are updated, while the results of the performance of AR\n  instruction are unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BRICKxAR is a novel Augmented Reality (AR) instruction method for\nconstruction toys such as LEGO. With BRICKxAR, physical LEGO construction is\nguided by virtual bricks. Compared with the state-of-the-art, accuracy of the\nvirtual - physical model alignment is significantly improved through a new\ndesign of marker-based registration, which can achieve an average error less\nthan 1mm throughout the model. Realistic object occlusion is accomplished to\nreveal the true spatial relationship between physical and virtual bricks. LEGO\nplayers' hand detection and occlusion are realized to visualize the correct\nspatial relationship between real hands and virtual bricks, and allow virtual\nbricks to be \"grasped\" by real hands. The integration of these features makes\nAR instructions possible for small-parts assembly, validated through a working\nAR prototype for constructing LEGO Arc de Triomphe, quantitative measures of\nthe accuracies of registration and occlusions, and heuristic evaluation of AR\ninstruction features.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 17:44:14 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 17:22:15 GMT"}, {"version": "v3", "created": "Sun, 11 Aug 2019 21:19:53 GMT"}, {"version": "v4", "created": "Sun, 20 Dec 2020 00:29:10 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Yan", "Wei", ""]]}, {"id": "1907.12567", "submitter": "Christopher McLaughlin Danforth", "authors": "Laura Jennings, Christopher M. Danforth, Peter Sheridan Dodds,\n  Elizabeth Pinel, Lizzy Pope", "title": "Exploring Perceptions of Veganism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This project examined perceptions of the vegan lifestyle using surveys and\nsocial media to explore barriers to choosing veganism. A survey of 510\nindividuals indicated that non-vegans did not believe veganism was as healthy\nor difficult as vegans. In a second analysis, Instagram posts using #vegan\nsuggest content is aimed primarily at the female vegan community. Finally,\nsentiment analysis of roughly 5 million Twitter posts mentioning 'vegan' found\nveganism to be portrayed in a more positive light compared to other topics.\nResults suggest non-vegans' lack of interest in veganism is driven by\nnon-belief in the health benefits of the diet.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 18:00:02 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Jennings", "Laura", ""], ["Danforth", "Christopher M.", ""], ["Dodds", "Peter Sheridan", ""], ["Pinel", "Elizabeth", ""], ["Pope", "Lizzy", ""]]}, {"id": "1907.12625", "submitter": "Carlos Sarraute PhD", "authors": "Ariel Futoransky, Carlos Sarraute, Ariel Waissbein, Daniel Fernandez,\n  Matias Travizano, Martin Minnoni", "title": "Secure Exchange of Digital Goods in a Decentralized Data Marketplace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We are tackling the problem of trading real-world private information using\nonly cryptographic protocols and a public blockchain to guarantee honest\ntransactions. In this project, we consider three types of agents --buyers,\nsellers and notaries-- interacting in a decentralized privacy-preserving data\nmarketplace (dPDM) such as the Wibson data marketplace. This framework offers\ninfrastructure and financial incentives for individuals to securely sell\npersonal information while preserving personal privacy. Here we provide an\nefficient cryptographic primitive for the secure exchange of data in a dPDM,\nwhich occurs as an atomic operation wherein the data buyer gets access to the\ndata and the data seller gets paid simultaneously.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 20:21:00 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Futoransky", "Ariel", ""], ["Sarraute", "Carlos", ""], ["Waissbein", "Ariel", ""], ["Fernandez", "Daniel", ""], ["Travizano", "Matias", ""], ["Minnoni", "Martin", ""]]}, {"id": "1907.12647", "submitter": "Arpan Sainju", "authors": "Arpan Sainju and Zhe Jiang", "title": "Mapping road safety features from streetview imagery: A deep learning\n  approach", "comments": "17 pages, 16 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each year, around 6 million car accidents occur in the U.S. on average. Road\nsafety features (e.g., concrete barriers, metal crash barriers, rumble strips)\nplay an important role in preventing or mitigating vehicle crashes. Accurate\nmaps of road safety features is an important component of safety management\nsystems for federal or state transportation agencies, helping traffic engineers\nidentify locations to invest on safety infrastructure. In current practice,\nmapping road safety features is largely done manually (e.g., observations on\nthe road or visual interpretation of streetview imagery), which is both\nexpensive and time consuming. In this paper, we propose a deep learning\napproach to automatically map road safety features from streetview imagery.\nUnlike existing Convolutional Neural Networks (CNNs) that classify each image\nindividually, we propose to further add Recurrent Neural Network (Long Short\nTerm Memory) to capture geographic context of images (spatial autocorrelation\neffect along linear road network paths). Evaluations on real world streetview\nimagery show that our proposed model outperforms several baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 20:38:33 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Sainju", "Arpan", ""], ["Jiang", "Zhe", ""]]}, {"id": "1907.12649", "submitter": "Michalis Pachilakis", "authors": "Michalis Pachilakis and Panagiotis Papadopoulos and Evangelos P.\n  Markatos and Nicolas Kourtellis", "title": "No More Chasing Waterfalls: A Measurement Study of the Header Bidding\n  Ad-Ecosystem", "comments": "14 pages, 24 Figures, 56 References, 1 Table. Accepted in ACM IMC\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Header Bidding (HB) has gained popularity among web\npublishers, challenging the status quo in the ad ecosystem. Contrary to the\ntraditional waterfall standard, HB aims to give back to publishers control of\ntheir ad inventory, increase transparency, fairness and competition among\nadvertisers, resulting in higher ad-slot prices. Although promising, little is\nknown about how this ad protocol works: What are HB's possible implementations,\nwho are the major players, and what is its network and UX overhead? To address\nthese questions, we design and implement HBDetector: a novel methodology to\ndetect HB auctions on a website at real time. By crawling 35,000 top Alexa\nwebsites, we collect and analyze a dataset of 800k auctions. We find that: (i)\n14.28% of top websites utilize HB. (ii) Publishers prefer to collaborate with a\nfew Demand Partners who also dominate the waterfall market. (iii) HB latency\ncan be significantly higher (up to 3x in median case) than waterfall.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 09:45:08 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 15:23:41 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Pachilakis", "Michalis", ""], ["Papadopoulos", "Panagiotis", ""], ["Markatos", "Evangelos P.", ""], ["Kourtellis", "Nicolas", ""]]}, {"id": "1907.12652", "submitter": "Christin Seifert", "authors": "Andrea Papenmeier and Gwenn Englebienne and Christin Seifert", "title": "How model accuracy and explanation fidelity influence user trust", "comments": "AI IJCAI Workshop on Explainable Artificial Intelligence (X-AI) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning systems have become popular in fields such as marketing,\nfinancing, or data mining. While they are highly accurate, complex machine\nlearning systems pose challenges for engineers and users. Their inherent\ncomplexity makes it impossible to easily judge their fairness and the\ncorrectness of statistically learned relations between variables and classes.\nExplainable AI aims to solve this challenge by modelling explanations alongside\nwith the classifiers, potentially improving user trust and acceptance. However,\nusers should not be fooled by persuasive, yet untruthful explanations. We\ntherefore conduct a user study in which we investigate the effects of model\naccuracy and explanation fidelity, i.e. how truthfully the explanation\nrepresents the underlying model, on user trust. Our findings show that accuracy\nis more important for user trust than explainability. Adding an explanation for\na classification result can potentially harm trust, e.g. when adding\nnonsensical explanations. We also found that users cannot be tricked by\nhigh-fidelity explanations into having trust for a bad classifier. Furthermore,\nwe found a mismatch between observed (implicit) and self-reported (explicit)\ntrust.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 09:22:16 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Papenmeier", "Andrea", ""], ["Englebienne", "Gwenn", ""], ["Seifert", "Christin", ""]]}, {"id": "1907.12669", "submitter": "Muhammad Aurangzeb Ahmad", "authors": "Muhammad Aurangzeb Ahmad, Carly Eckert, Ankur Teredesai", "title": "The Challenge of Imputation in Explainable Artificial Intelligence\n  Models", "comments": "The IJCAI-19 Workshop on Artificial Intelligence Safety", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable models in Artificial Intelligence are often employed to ensure\ntransparency and accountability of AI systems. The fidelity of the explanations\nare dependent upon the algorithms used as well as on the fidelity of the data.\nMany real world datasets have missing values that can greatly influence\nexplanation fidelity. The standard way to deal with such scenarios is\nimputation. This can, however, lead to situations where the imputed values may\ncorrespond to a setting which refer to counterfactuals. Acting on explanations\nfrom AI models with imputed values may lead to unsafe outcomes. In this paper,\nwe explore different settings where AI models with imputation can be\nproblematic and describe ways to address such scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 22:06:21 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Ahmad", "Muhammad Aurangzeb", ""], ["Eckert", "Carly", ""], ["Teredesai", "Ankur", ""]]}, {"id": "1907.12698", "submitter": "Antonio Mora Dr.", "authors": "A.M. Mora and A.I. Esparcia-Alc\\'azar", "title": "EVO* 2019 -- Late-Breaking Abstracts Volume", "comments": "LBAs accepted in EVO* 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the Late-Breaking Abstracts submitted to the EVO* 2019\nConference, that took place in Leipzig, from 24 to 26 of April. These papers\nwhere presented as short talks and also at the poster session of the conference\ntogether with other regular submissions. All of them present ongoing research\nand preliminary results investigating on the application of different\napproaches of Evolutionary Computation to different problems, most of them real\nworld ones.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 01:42:42 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Mora", "A. M.", ""], ["Esparcia-Alc\u00e1zar", "A. I.", ""]]}, {"id": "1907.12737", "submitter": "Hamed Vahdat-Nejad", "authors": "Hamed Vahdat-Nejad, Zahra Mazhar Farimani, Arezoo Tavakolifar", "title": "Social Internet of Things and New Generation Computing -- A Survey", "comments": "IoT, Social computing, Survey", "journal-ref": null, "doi": "10.1007/978-3-030-24513-9_8", "report-no": null, "categories": "cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social Internet of Things (SIoT) tries to overcome the challenges of Internet\nof Things (IoT) such as scalability, trust and discovery of resources, by\ninspiration from social computing. This survey aims to investigate the research\ndone on SIoT from two perspectives including application domain and the\nintegration to the new computing models. For this, a two-dimensional framework\nis proposed and the projects are investigated, accordingly. The first dimension\nconsiders and classifies available research from the application domain\nperspective and the second dimension performs the same from the integration to\nnew computing models standpoint. The aim is to technically describe SIoT, to\nclassify related research, to foster the dissemination of state-of-the-art, and\nto discuss open research directions in this field.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 18:49:43 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Vahdat-Nejad", "Hamed", ""], ["Farimani", "Zahra Mazhar", ""], ["Tavakolifar", "Arezoo", ""]]}, {"id": "1907.13061", "submitter": "Pamela Gay", "authors": "Alison Reiheld, Pamela L. Gay", "title": "Coercion, Consent, and Participation in Citizen Science", "comments": "submitted to journal of Science and Engineering Ethics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY physics.ed-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout history, everyday people have contributed to science through a\nmyriad of volunteer activities. This early participation required training and\noften involved mentorship from scientists or senior citizen scientists (or, as\nthey were often called, gentleman scientists). During this learning process,\nparticipants learned how they and their data would be used both to advance\nscience, and in some cases, advance the careers of professional collaborators.\nModern, online citizen science, allows participation with just a few clicks,\nand people may participate without understanding what they are contributing to.\nToo often, they happily see what they are doing as the privilege of painting\nTom Sawyer's fence without realizing they are actually being used as merely a\nmeans to a scientific end. This paper discusses the ethical dilemmas that\nplague modern citizen science, including: the issues of informed consent, such\nas not requiring logins; the issues of coercion inherent in mandatory classroom\nassignments requiring data submission; and the issues of using people merely as\na means to an end that are inherent in technonationalism, and projects that do\nnot provide utility to the users beyond the knowledge they helped. This work is\ntested within the context of astronomy citizen science.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 03:48:21 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Reiheld", "Alison", ""], ["Gay", "Pamela L.", ""]]}]