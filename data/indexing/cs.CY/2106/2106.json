[{"id": "2106.00236", "submitter": "Patrick Kelley", "authors": "Sunny Consolvo, Patrick Gage Kelley, Tara Matthews, Kurt Thomas, Lee\n  Dunn, Elie Bursztein", "title": "\"Why wouldn't someone think of democracy as a target?\": Security\n  practices & challenges of people involved with U.S. political campaigns", "comments": "18 pages, 2 tables, one ancillary file with 4 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  People who are involved with political campaigns face increased digital\nsecurity threats from well-funded, sophisticated attackers, especially\nnation-states. Improving political campaign security is a vital part of\nprotecting democracy. To identify campaign security issues, we conducted\nqualitative research with 28 participants across the U.S. political spectrum to\nunderstand the digital security practices, challenges, and perceptions of\npeople involved in campaigns. A main, overarching finding is that a unique\ncombination of threats, constraints, and work culture lead people involved with\npolitical campaigns to use technologies from across platforms and domains in\nways that leave them--and democracy--vulnerable to security attacks. Sensitive\ndata was kept in a plethora of personal and work accounts, with ad hoc adoption\nof strong passwords, two-factor authentication, encryption, and access\ncontrols. No individual company, committee, organization, campaign, or academic\ninstitution can solve the identified problems on their own. To this end, we\nprovide an initial understanding of this complex problem space and\nrecommendations for how a diverse group of experts can begin working together\nto improve security for political campaigns.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 05:27:51 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Consolvo", "Sunny", ""], ["Kelley", "Patrick Gage", ""], ["Matthews", "Tara", ""], ["Thomas", "Kurt", ""], ["Dunn", "Lee", ""], ["Bursztein", "Elie", ""]]}, {"id": "2106.00321", "submitter": "Felix Thiel", "authors": "Felix J. Thiel, Anthony Steed", "title": "A Way to a Universal VR Accessibility Toolkit", "comments": "This work was presented at the ACM CHI 2021 Workshop on Design and\n  Creation of Inclusive User Interactions Through Immersive Media.\n  https://sites.google.com/view/acm-chi-iicw21/home", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Reality (VR) has become more and more popular with dropping prices\nfor systems and a growing number of users. However, the issue of accessibility\nin VR has been hardly addressed so far and no uniform approach or standard\nexists at this time. In this position paper, we propose a customisable toolkit\nimplemented at the system-level and discuss the potential benefits of this\napproach and challenges that will need to be overcome for a successful\nimplementation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 08:50:46 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Thiel", "Felix J.", ""], ["Steed", "Anthony", ""]]}, {"id": "2106.00326", "submitter": "Kimon Kieslich", "authors": "Kimon Kieslich, Birte Keller, Christopher Starke", "title": "AI-Ethics by Design. Evaluating Public Perception on the Importance of\n  Ethical Design Principles of AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the immense societal importance of ethically designing artificial\nintelligence (AI), little research on the public perceptions of ethical AI\nprinciples exists. This becomes even more striking when considering that\nethical AI development has the aim to be human-centric and of benefit for the\nwhole society. In this study, we investigate how ethical principles\n(explainability, fairness, security, accountability, accuracy, privacy, machine\nautonomy) are weighted in comparison to each other. This is especially\nimportant, since simultaneously considering ethical principles is not only\ncostly, but sometimes even impossible, as developers must make specific\ntrade-off decisions. In this paper, we give first answers on the relative\nimportance of ethical principles given a specific use case - the use of AI in\ntax fraud detection. The results of a large conjoint survey (n=1099) suggest\nthat, by and large, German respondents found the ethical principles equally\nimportant. However, subsequent cluster analysis shows that different preference\nmodels for ethically designed systems exist among the German population. These\nclusters substantially differ not only in the preferred attributes, but also in\nthe importance level of the attributes themselves. We further describe how\nthese groups are constituted in terms of sociodemographics as well as opinions\non AI. Societal implications as well as design challenges are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 09:01:14 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Kieslich", "Kimon", ""], ["Keller", "Birte", ""], ["Starke", "Christopher", ""]]}, {"id": "2106.00356", "submitter": "Joel Persson", "authors": "Amray Schwabe, Joel Persson and Stefan Feuerriegel", "title": "Predicting COVID-19 Spread from Large-Scale Mobility Data", "comments": "9 pages, 3 figures. Accepted for publication in KDD '21: 27th ACM\n  SIGKDD Conference on Knowledge Discovery and Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To manage the COVID-19 epidemic effectively, decision-makers in public health\nneed accurate forecasts of case numbers. A potential near real-time predictor\nof future case numbers is human mobility; however, research on the predictive\npower of mobility is lacking. To fill this gap, we introduce a novel model for\nepidemic forecasting based on mobility data, called mobility marked Hawkes\nmodel. The proposed model consists of three components: (1) A Hawkes process\ncaptures the transmission dynamics of infectious diseases. (2) A mark modulates\nthe rate of infections, thus accounting for how the reproduction number R\nvaries across space and time. The mark is modeled using a regularized Poisson\nregression based on mobility covariates. (3) A correction procedure\nincorporates new cases seeded by people traveling between regions. Our model\nwas evaluated on the COVID-19 epidemic in Switzerland. Specifically, we used\nmobility data from February through April 2020, amounting to approximately 1.5\nbillion trips. Trip counts were derived from large-scale telecommunication\ndata, i.e., cell phone pings from the Swisscom network, the largest\ntelecommunication provider in Switzerland. We compared our model against\nvarious state-of-the-art baselines in terms of out-of-sample root mean squared\nerror. We found that our model outperformed the baselines by 15.52%. The\nimprovement was consistently achieved across different forecast horizons\nbetween 5 and 21 days. In addition, we assessed the predictive power of\nconventional point of interest data, confirming that telecommunication data is\nsuperior. To the best of our knowledge, our work is the first to predict the\nspread of COVID-19 from telecommunication data. Altogether, our work\ncontributes to previous research by developing a scalable early warning system\nfor decision-makers in public health tasked with controlling the spread of\ninfectious diseases.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 10:05:02 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Schwabe", "Amray", ""], ["Persson", "Joel", ""], ["Feuerriegel", "Stefan", ""]]}, {"id": "2106.00365", "submitter": "Verity Allan", "authors": "Verity Allan, Caitriona Leedham", "title": "Scientific Computing in the Cavendish Laboratory and the pioneering\n  women Computors", "comments": "11 pages, 8 figures, submitted to IEEE Annals in the History of\n  Computing, (C) IEEE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY astro-ph.IM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The use of computers and the role of women in radio astronomy and X-ray\ncrystallography research at the Cavendish Laboratory between 1949 and 1975 have\nbeen investigated. We recorded examples of when computers were used, what they\nwere used for and who used them from hundreds of papers published during these\nyears. The use of the EDSAC, EDSAC 2 and TITAN computers was found to increase\nconsiderably over this time-scale and they were used for a diverse range of\napplications. The majority of references to computer operators and programmers\nreferred to women, 57% for astronomy and 62% for crystallography, in contrast\nto a very small proportion, 4% and 13% respectively, of female authors of\npapers.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 10:17:37 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Allan", "Verity", ""], ["Leedham", "Caitriona", ""]]}, {"id": "2106.00461", "submitter": "Paolo Bajardi", "authors": "Elvio G. Amparore and Alan Perotti and Paolo Bajardi", "title": "To trust or not to trust an explanation: using LEAF to evaluate local\n  linear XAI methods", "comments": "16 pages, 8 figures", "journal-ref": "PeerJ Computer Science 7:e479 (2021)", "doi": "10.7717/peerj-cs.479", "report-no": null, "categories": "cs.AI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The main objective of eXplainable Artificial Intelligence (XAI) is to provide\neffective explanations for black-box classifiers. The existing literature lists\nmany desirable properties for explanations to be useful, but there is no\nconsensus on how to quantitatively evaluate explanations in practice. Moreover,\nexplanations are typically used only to inspect black-box models, and the\nproactive use of explanations as a decision support is generally overlooked.\nAmong the many approaches to XAI, a widely adopted paradigm is Local Linear\nExplanations - with LIME and SHAP emerging as state-of-the-art methods. We show\nthat these methods are plagued by many defects including unstable explanations,\ndivergence of actual implementations from the promised theoretical properties,\nand explanations for the wrong label. This highlights the need to have standard\nand unbiased evaluation procedures for Local Linear Explanations in the XAI\nfield. In this paper we address the problem of identifying a clear and\nunambiguous set of metrics for the evaluation of Local Linear Explanations.\nThis set includes both existing and novel metrics defined specifically for this\nclass of explanations. All metrics have been included in an open Python\nframework, named LEAF. The purpose of LEAF is to provide a reference for end\nusers to evaluate explanations in a standardised and unbiased way, and to guide\nresearchers towards developing improved explainable techniques.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 13:14:12 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Amparore", "Elvio G.", ""], ["Perotti", "Alan", ""], ["Bajardi", "Paolo", ""]]}, {"id": "2106.00467", "submitter": "Daniele Regoli", "authors": "Alessandro Castelnovo, Riccardo Crupi, Greta Greco, Daniele Regoli", "title": "The zoo of Fairness metrics in Machine Learning", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the problem of addressing fairness in Machine Learning (ML)\nand automatic decision-making has attracted a lot of attention in the\nscientific communities dealing with Artificial Intelligence. A plethora of\ndifferent definitions of fairness in ML have been proposed, that consider\ndifferent notions of what is a \"fair decision\" in situations impacting\nindividuals in the population. The precise differences, implications and\n\"orthogonality\" between these notions have not yet been fully analyzed in the\nliterature. In this work, we try to make some order out of this zoo of\ndefinitions.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 13:19:30 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 21:34:17 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Castelnovo", "Alessandro", ""], ["Crupi", "Riccardo", ""], ["Greco", "Greta", ""], ["Regoli", "Daniele", ""]]}, {"id": "2106.00647", "submitter": "Laura Alessandretti", "authors": "Matthieu Nadini, Laura Alessandretti, Flavio Di Giacinto, Mauro\n  Martino, Luca Maria Aiello, Andrea Baronchelli", "title": "Mapping the NFT revolution: market trends, trade networks and visual\n  features", "comments": "Working paper, comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CY physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non Fungible Tokens (NFTs) are digital assets that represent objects like\nart, videos, in-game items and music. They are traded online, often with\ncryptocurrency, and they are generally encoded as smart contracts on a\nblockchain. Media and public attention towards NFTs has exploded in 2021, when\nthe NFT art market has experienced record sales while celebrated new star\nartists. However, little is known about the overall structure and evolution of\nthe NFT market. Here, we analyse data concerning 6.1 million trades of 4.7\nmillion NFTs generating a total trading volume of 935 millions US dollars. Our\ndata are obtained primarily from the Ethereum and WAX blockchains and cover the\nperiod between June 23, 2017 and April 27, 2021. First, we characterize the\nstatistical properties of the market. Second, we build the network of\ninteractions and show that traders have bursts of activity followed by inactive\nperiods, and typically specialize on NFTs associated to similar objects. Third,\nwe cluster objects associated to NFTs according to their visual features and\nshow that NFTs within the same category tend to be visually homogeneous.\nFinally, we investigate the predictability of NFT sales. We use simple machine\nlearning algorithms and find that prices can be best predicted by the sale\nhistory of the NFT collection, but also by some features describing the\nproperties of the associated object (e.g., visual features of digital images).\nWe anticipate that our analysis will be of interest to both researchers and\npractitioners and will spark further research on the NFT production, adoption\nand trading in different contexts.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 17:25:32 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 16:22:52 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Nadini", "Matthieu", ""], ["Alessandretti", "Laura", ""], ["Di Giacinto", "Flavio", ""], ["Martino", "Mauro", ""], ["Aiello", "Luca Maria", ""], ["Baronchelli", "Andrea", ""]]}, {"id": "2106.00660", "submitter": "Ilia Shumailov", "authors": "David Khachaturov, Ilia Shumailov, Yiren Zhao, Nicolas Papernot, Ross\n  Anderson", "title": "Markpainting: Adversarial Machine Learning meets Inpainting", "comments": "Proceedings of the 38th International Conference on Machine Learning\n  (ICML 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inpainting is a learned interpolation technique that is based on generative\nmodeling and used to populate masked or missing pieces in an image; it has wide\napplications in picture editing and retouching. Recently, inpainting started\nbeing used for watermark removal, raising concerns. In this paper we study how\nto manipulate it using our markpainting technique. First, we show how an image\nowner with access to an inpainting model can augment their image in such a way\nthat any attempt to edit it using that model will add arbitrary visible\ninformation. We find that we can target multiple different models\nsimultaneously with our technique. This can be designed to reconstitute a\nwatermark if the editor had been trying to remove it. Second, we show that our\nmarkpainting technique is transferable to models that have different\narchitectures or were trained on different datasets, so watermarks created\nusing it are difficult for adversaries to remove. Markpainting is novel and can\nbe used as a manipulation alarm that becomes visible in the event of\ninpainting.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 17:45:52 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Khachaturov", "David", ""], ["Shumailov", "Ilia", ""], ["Zhao", "Yiren", ""], ["Papernot", "Nicolas", ""], ["Anderson", "Ross", ""]]}, {"id": "2106.00717", "submitter": "Aymen Hamrouni", "authors": "Aymen Hamrouni, Hakim Ghazzai, Turki Alelyani, Yehia Massoud", "title": "Low Complexity Recruitment for Collaborative Mobile Crowdsourcing Using\n  Graph Neural Networks", "comments": "16 pages, 20 figures, 2 tables. Accepted for publications in IEEE\n  Internet-of-things Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Collaborative Mobile crowdsourcing (CMCS) allows entities, e.g., local\nauthorities or individuals, to hire a team of workers from the crowd of\nconnected people, to execute complex tasks. In this paper, we investigate two\ndifferent CMCS recruitment strategies allowing task requesters to form teams of\nsocially connected and skilled workers: i) a platform-based strategy where the\nplatform exploits its own knowledge about the workers to form a team and ii) a\nleader-based strategy where the platform designates a group leader that\nrecruits its own suitable team given its own knowledge about its Social Network\n(SN) neighbors. We first formulate the recruitment as an Integer Linear Program\n(ILP) that optimally forms teams according to four fuzzy-logic-based criteria:\nlevel of expertise, social relationship strength, recruitment cost, and\nrecruiter's confidence level. To cope with NP-hardness, we design a novel\nlow-complexity CMCS recruitment approach relying on Graph Neural Networks\n(GNNs), specifically graph embedding and clustering techniques, to shrink the\nworkers' search space and afterwards, exploiting a meta-heuristic genetic\nalgorithm to select appropriate workers. Simulation results applied on a\nreal-world dataset illustrate the performance of both proposed CMCS recruitment\napproaches. It is shown that our proposed low-complexity GNN-based recruitment\nalgorithm achieves close performances to those of the baseline ILP with\nsignificant computational time saving and ability to operate on large-scale\nmobile crowdsourcing platforms. It is also shown that compared to the\nleader-based strategy, the platform-based strategy recruits a more skilled team\nbut with lower SN relationships and higher cost.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 18:24:02 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Hamrouni", "Aymen", ""], ["Ghazzai", "Hakim", ""], ["Alelyani", "Turki", ""], ["Massoud", "Yehia", ""]]}, {"id": "2106.00772", "submitter": "Mohamed Nafea", "authors": "Sajad Khodadadian, Mohamed Nafea, AmirEmad Ghassami, Negar Kiyavash", "title": "Information Theoretic Measures for Fairness-aware Feature Selection", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning algorithms are increasingly used for consequential decision\nmaking regarding individuals based on their relevant features. Features that\nare relevant for accurate decisions may however lead to either explicit or\nimplicit forms of discrimination against unprivileged groups, such as those of\ncertain race or gender. This happens due to existing biases in the training\ndata, which are often replicated or even exacerbated by the learning algorithm.\nIdentifying and measuring these biases at the data level is a challenging\nproblem due to the interdependence among the features, and the decision\noutcome. In this work, we develop a framework for fairness-aware feature\nselection which takes into account the correlation among the features and the\ndecision outcome, and is based on information theoretic measures for the\naccuracy and discriminatory impacts of features. In particular, we first\npropose information theoretic measures which quantify the impact of different\nsubsets of features on the accuracy and discrimination of the decision\noutcomes. We then deduce the marginal impact of each feature using Shapley\nvalue function; a solution concept in cooperative game theory used to estimate\nmarginal contributions of players in a coalitional game. Finally, we design a\nfairness utility score for each feature (for feature selection) which\nquantifies how this feature influences accurate as well as nondiscriminatory\ndecisions. Our framework depends on the joint statistics of the data rather\nthan a particular classifier design. We examine our proposed framework on real\nand synthetic data to evaluate its performance.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 20:11:54 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 06:48:21 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Khodadadian", "Sajad", ""], ["Nafea", "Mohamed", ""], ["Ghassami", "AmirEmad", ""], ["Kiyavash", "Negar", ""]]}, {"id": "2106.01013", "submitter": "Martino Trevisan Dr", "authors": "Martino Trevisan, Luca Vassio, Danilo Giordano", "title": "Debate on Online Social Networks at the Time of COVID-19: An Italian\n  Case Study", "comments": null, "journal-ref": "TREVISAN, Martino; VASSIO, Luca; GIORDANO, Danilo. Debate on\n  online social networks at the time of COVID-19: An Italian case study. Online\n  Social Networks and Media, 2021, 23: 100136", "doi": "10.1016/j.osnem.2021.100136", "report-no": null, "categories": "cs.SI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic is not only having a heavy impact on healthcare but\nalso changing people's habits and the society we live in. Countries such as\nItaly have enforced a total lockdown lasting several months, with most of the\npopulation forced to remain at home. During this time, online social networks,\nmore than ever, have represented an alternative solution for social life,\nallowing users to interact and debate with each other. Hence, it is of\nparamount importance to understand the changing use of social networks brought\nabout by the pandemic. In this paper, we analyze how the interaction patterns\naround popular influencers in Italy changed during the first six months of\n2020, within Instagram and Facebook social networks. We collected a large\ndataset for this group of public figures, including more than 54 million\ncomments on over 140 thousand posts for these months. We analyze and compare\nengagement on the posts of these influencers and provide quantitative figures\nfor aggregated user activity. We further show the changes in the patterns of\nusage before and during the lockdown, which demonstrated a growth of activity\nand sizable daily and weekly variations. We also analyze the user sentiment\nthrough the psycholinguistic properties of comments, and the results testified\nthe rapid boom and disappearance of topics related to the pandemic. To support\nfurther analyses, we release the anonymized dataset.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 08:25:19 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Trevisan", "Martino", ""], ["Vassio", "Luca", ""], ["Giordano", "Danilo", ""]]}, {"id": "2106.01033", "submitter": "Kunwoo Park", "authors": "Kunwoo Park, Zhufeng Pan, and Jungseock Joo", "title": "Who Blames or Endorses Whom? Entity-to-Entity Directed Sentiment\n  Extraction in News Text", "comments": "Published in Findings of ACL 2021 (Long paper). The manuscript is\n  slightly revised after the camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Understanding who blames or supports whom in news text is a critical research\nquestion in computational social science. Traditional methods and datasets for\nsentiment analysis are, however, not suitable for the domain of political text\nas they do not consider the direction of sentiments expressed between entities.\nIn this paper, we propose a novel NLP task of identifying directed sentiment\nrelationship between political entities from a given news document, which we\ncall directed sentiment extraction. From a million-scale news corpus, we\nconstruct a dataset of news sentences where sentiment relations of political\nentities are manually annotated. We present a simple but effective approach for\nutilizing a pretrained transformer, which infers the target class by predicting\nmultiple question-answering tasks and combining the outcomes. We demonstrate\nthe utility of our proposed method for social science research questions by\nanalyzing positive and negative opinions between political entities in two\nmajor events: 2016 U.S. presidential election and COVID-19. The newly proposed\nproblem, data, and method will facilitate future studies on interdisciplinary\nNLP methods and applications.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 09:02:14 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 07:32:36 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Park", "Kunwoo", ""], ["Pan", "Zhufeng", ""], ["Joo", "Jungseock", ""]]}, {"id": "2106.01070", "submitter": "Viet Anh Nguyen", "authors": "Nian Si and Karthyek Murthy and Jose Blanchet and Viet Anh Nguyen", "title": "Testing Group Fairness via Optimal Transport Projections", "comments": null, "journal-ref": "International Conference on Machine Learning 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a statistical testing framework to detect if a given machine\nlearning classifier fails to satisfy a wide range of group fairness notions.\nThe proposed test is a flexible, interpretable, and statistically rigorous tool\nfor auditing whether exhibited biases are intrinsic to the algorithm or due to\nthe randomness in the data. The statistical challenges, which may arise from\nmultiple impact criteria that define group fairness and which are discontinuous\non model parameters, are conveniently tackled by projecting the empirical\nmeasure onto the set of group-fair probability models using optimal transport.\nThis statistic is efficiently computed using linear programming and its\nasymptotic distribution is explicitly obtained. The proposed framework can also\nbe used to test for testing composite fairness hypotheses and fairness with\nmultiple sensitive attributes. The optimal transport testing formulation\nimproves interpretability by characterizing the minimal covariate perturbations\nthat eliminate the bias observed in the audit.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 10:51:39 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Si", "Nian", ""], ["Murthy", "Karthyek", ""], ["Blanchet", "Jose", ""], ["Nguyen", "Viet Anh", ""]]}, {"id": "2106.01315", "submitter": "Jing Ma", "authors": "Jing Ma, Yushun Dong, Zheng Huang, Daniel Mietchen, Jundong Li", "title": "Assessing the Causal Impact of COVID-19 Related Policies on Outbreak\n  Dynamics: A Case Study in the US", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To mitigate the spread of COVID-19 pandemic, decision-makers and public\nauthorities have announced various non-pharmaceutical policies. Analyzing the\ncausal impact of these policies in reducing the spread of COVID-19 is important\nfor future policy-making. The main challenge here is the existence of\nunobserved confounders (e.g., vigilance of residents). Besides, as the\nconfounders may be time-varying during COVID-19 (e.g., vigilance of residents\nchanges in the course of the pandemic), it is even more difficult to capture\nthem. In this paper, we study the problem of assessing the causal effects of\ndifferent COVID-19 related policies on the outbreak dynamics in different\ncounties at any given time period. To this end, we integrate data about\ndifferent COVID-19 related policies (treatment) and outbreak dynamics (outcome)\nfor different United States counties over time and analyze them with respect to\nvariables that can infer the confounders, including the covariates of different\ncounties, their relational information and historical information. Based on\nthese data, we develop a neural network based causal effect estimation\nframework which leverages above information in observational data and learns\nthe representations of time-varying (unobserved) confounders. In this way, it\nenables us to quantify the causal impact of policies at different\ngranularities, ranging from a category of policies with a certain goal to a\nspecific policy type in this category. Besides, experimental results also\nindicate the effectiveness of our proposed framework in capturing the\nconfounders for quantifying the causal impact of different policies. More\nspecifically, compared with several baseline methods, our framework captures\nthe outbreak dynamics more accurately, and our assessment of policies is more\nconsistent with existing epidemiological studies of COVID-19.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 00:40:24 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Ma", "Jing", ""], ["Dong", "Yushun", ""], ["Huang", "Zheng", ""], ["Mietchen", "Daniel", ""], ["Li", "Jundong", ""]]}, {"id": "2106.01325", "submitter": "David Lindner", "authors": "David Lindner and Hoda Heidari and Andreas Krause", "title": "Addressing the Long-term Impact of ML Decisions via Policy Regret", "comments": "Accepted to IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) increasingly informs the allocation of opportunities to\nindividuals and communities in areas such as lending, education, employment,\nand beyond. Such decisions often impact their subjects' future characteristics\nand capabilities in an a priori unknown fashion. The decision-maker, therefore,\nfaces exploration-exploitation dilemmas akin to those in multi-armed bandits.\nFollowing prior work, we model communities as arms. To capture the long-term\neffects of ML-based allocation decisions, we study a setting in which the\nreward from each arm evolves every time the decision-maker pulls that arm. We\nfocus on reward functions that are initially increasing in the number of pulls\nbut may become (and remain) decreasing after a certain point. We argue that an\nacceptable sequential allocation of opportunities must take an arm's potential\nfor growth into account. We capture these considerations through the notion of\npolicy regret, a much stronger notion than the often-studied external regret,\nand present an algorithm with provably sub-linear policy regret for\nsufficiently long time horizons. We empirically compare our algorithm with\nseveral baselines and find that it consistently outperforms them, in particular\nfor long time horizons.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 17:38:10 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Lindner", "David", ""], ["Heidari", "Hoda", ""], ["Krause", "Andreas", ""]]}, {"id": "2106.01589", "submitter": "Hongyuan Diao", "authors": "Hongyuan Diao, Fuzhong Nian, Xuelong Yu, Xirui Liu and Xinhao Liu", "title": "The Emotion coding and Propagation based on improved Genetic algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational communication research on information has been prevalent in\nrecent years, as people are progressively inquisitive in social behavior and\npublic opinion. Nevertheless, it is of great significance to analyze the\ndirection of predominant sentiment from the sentiment communication\nperspective. In this paper, the information emotion propagation model is\nestablished by introducing revamp genetic algorithms into information emotion.\nIn the process of information dissemination, both the information emotions and\nthe network emotions are dynamic. For this model, the information emotions and\nthe network nodes emotions are quantified as binary codes. The convergence\neffects, crossover and mutation algorithms are introduced. These factors all\nact on the transmission process via dynamic propagation rate, and the improved\ngenetic algorithm also acts on the emotion transmission. In particular, the\nlatter two algorithms are different from the existing biological domain. Based\non the existing research results in other manuscripts, we perform simulation\ndescribed above on the hybrid network. The simulation results demonstrate that\nthe trend approximate to the actual data. As a result, our work can prove that\nour proposed model is essentially consistent with the actual emotion\ntransmission phenomenon.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 04:21:06 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Diao", "Hongyuan", ""], ["Nian", "Fuzhong", ""], ["Yu", "Xuelong", ""], ["Liu", "Xirui", ""], ["Liu", "Xinhao", ""]]}, {"id": "2106.01601", "submitter": "Jiao Sun", "authors": "Jiao Sun and Nanyun Peng", "title": "Men Are Elected, Women Are Married: Events Gender Bias on Wikipedia", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activities can be seen as sequences of events, which are crucial to\nunderstanding societies. Disproportional event distribution for different\ndemographic groups can manifest and amplify social stereotypes, and potentially\njeopardize the ability of members in some groups to pursue certain goals. In\nthis paper, we present the first event-centric study of gender biases in a\nWikipedia corpus. To facilitate the study, we curate a corpus of career and\npersonal life descriptions with demographic information consisting of 7,854\nfragments from 10,412 celebrities. Then we detect events with a\nstate-of-the-art event detection model, calibrate the results using\nstrategically generated templates, and extract events that have asymmetric\nassociations with genders. Our study discovers that the Wikipedia pages tend to\nintermingle personal life events with professional events for females but not\nfor males, which calls for the awareness of the Wikipedia community to\nformalize guidelines and train the editors to mind the implicit biases that\ncontributors carry. Our work also lays the foundation for future works on\nquantifying and discovering event biases at the corpus level.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 05:22:16 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Sun", "Jiao", ""], ["Peng", "Nanyun", ""]]}, {"id": "2106.01784", "submitter": "Ben Green", "authors": "Ben Green", "title": "The Contestation of Tech Ethics: A Sociotechnical Approach to Ethics and\n  Technology in Action", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent controversies related to topics such as fake news, privacy, and\nalgorithmic bias have prompted increased public scrutiny of digital\ntechnologies and soul-searching among many of the people associated with their\ndevelopment. In response, the tech industry, academia, civil society, and\ngovernments have rapidly increased their attention to \"ethics\" in the design\nand use of digital technologies (\"tech ethics\"). Yet almost as quickly as\nethics discourse has proliferated across the world of digital technologies, the\nlimitations of these approaches have also become apparent: tech ethics is vague\nand toothless, is subsumed into corporate logics and incentives, and has a\nmyopic focus on individual engineers and technology design rather than on the\nstructures and cultures of technology production. As a result of these\nlimitations, many have grown skeptical of tech ethics and its proponents,\ncharging them with \"ethics-washing\": promoting ethics research and discourse to\ndefuse criticism and government regulation without committing to ethical\nbehavior. By looking at how ethics has been taken up in both science and\nbusiness in superficial and depoliticizing ways, I recast tech ethics as a\nterrain of contestation where the central fault line is not whether it is\ndesirable to be ethical, but what \"ethics\" entails and who gets to define it.\nThis framing highlights the significant limits of current approaches to tech\nethics and the importance of studying the formulation and real-world effects of\ntech ethics. In order to identify and develop more rigorous strategies for\nreforming digital technologies and the social relations that they mediate, I\ndescribe a sociotechnical approach to tech ethics, one that reflexively applies\nmany of tech ethics' own lessons regarding digital technologies to tech ethics\nitself.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 12:16:08 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Green", "Ben", ""]]}, {"id": "2106.02094", "submitter": "James Kaufman", "authors": "Vishrawas Gopalakrishnan, Sayali Navalekar, Pan Ding, Ryan Hooley,\n  Jacob Miller, Raman Srinivasan, Ajay Deshpande, Xuan Liu, Simone Bianco,\n  James H. Kaufman", "title": "Adaptive Epidemic Forecasting and Community Risk Evaluation of COVID-19", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pandemic control measures like lock-down, restrictions on restaurants and\ngatherings, social-distancing have shown to be effective in curtailing the\nspread of COVID-19. However, their sustained enforcement has negative economic\neffects. To craft strategies and policies that reduce the hardship on the\npeople and the economy while being effective against the pandemic, authorities\nneed to understand the disease dynamics at the right geo-spatial granularity.\nConsidering factors like the hospitals' ability to handle the fluctuating\ndemands, evaluating various reopening scenarios, and accurate forecasting of\ncases are vital to decision making. Towards this end, we present a flexible\nend-to-end solution that seamlessly integrates public health data with tertiary\nclient data to accurately estimate the risk of reopening a community. At its\ncore lies a state-of-the-art prediction model that auto-captures changing\ntrends in transmission and mobility. Benchmarking against various published\nbaselines confirm the superiority of our forecasting algorithm. Combined with\nthe ability to extend to multiple client-specific requirements and perform\ndeductive reasoning through counter-factual analysis, this solution provides\nactionable insights to multiple client domains ranging from government to\neducational institutions, hospitals, and commercial establishments.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 19:26:37 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Gopalakrishnan", "Vishrawas", ""], ["Navalekar", "Sayali", ""], ["Ding", "Pan", ""], ["Hooley", "Ryan", ""], ["Miller", "Jacob", ""], ["Srinivasan", "Raman", ""], ["Deshpande", "Ajay", ""], ["Liu", "Xuan", ""], ["Bianco", "Simone", ""], ["Kaufman", "James H.", ""]]}, {"id": "2106.02167", "submitter": "Nguyen Phong Hoang", "authors": "Nguyen Phong Hoang, Arian Akhavan Niaki, Jakub Dalek, Jeffrey Knockel,\n  Pellaeon Lin, Bill Marczak, Masashi Crete-Nishihata, Phillipa Gill, Michalis\n  Polychronakis", "title": "How Great is the Great Firewall? Measuring China's DNS Censorship", "comments": "To appear at the 30th USENIX Security Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.NI cs.SI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The DNS filtering apparatus of China's Great Firewall (GFW) has evolved\nconsiderably over the past two decades. However, most prior studies of China's\nDNS filtering were performed over short time periods, leading to unnoticed\nchanges in the GFW's behavior. In this study, we introduce GFWatch, a\nlarge-scale, longitudinal measurement platform capable of testing hundreds of\nmillions of domains daily, enabling continuous monitoring of the GFW's DNS\nfiltering behavior.\n  We present the results of running GFWatch over a nine-month period, during\nwhich we tested an average of 411M domains per day and detected a total of 311K\ndomains censored by GFW's DNS filter. To the best of our knowledge, this is the\nlargest number of domains tested and censored domains discovered in the\nliterature. We further reverse engineer regular expressions used by the GFW and\nfind 41K innocuous domains that match these filters, resulting in overblocking\nof their content. We also observe bogus IPv6 and globally routable IPv4\naddresses injected by the GFW, including addresses owned by US companies, such\nas Facebook, Dropbox, and Twitter.\n  Using data from GFWatch, we studied the impact of GFW blocking on the global\nDNS system. We found 77K censored domains with DNS resource records polluted in\npopular public DNS resolvers, such as Google and Cloudflare. Finally, we\npropose strategies to detect poisoned responses that can (1) sanitize poisoned\nDNS records from the cache of public DNS resolvers, and (2) assist in the\ndevelopment of circumvention tools to bypass the GFW's DNS censorship.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 22:59:27 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Hoang", "Nguyen Phong", ""], ["Niaki", "Arian Akhavan", ""], ["Dalek", "Jakub", ""], ["Knockel", "Jeffrey", ""], ["Lin", "Pellaeon", ""], ["Marczak", "Bill", ""], ["Crete-Nishihata", "Masashi", ""], ["Gill", "Phillipa", ""], ["Polychronakis", "Michalis", ""]]}, {"id": "2106.02283", "submitter": "Maximilian Hils", "authors": "Maximilian Hils, Daniel W. Woods, Rainer B\\\"ohme (University of\n  Innsbruck)", "title": "Privacy Preference Signals: Past, Present and Future", "comments": null, "journal-ref": "Proceedings on Privacy Enhancing Technologies 2021", "doi": "10.2478/popets-2021-0069", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Privacy preference signals are digital representations of how users want\ntheir personal data to be processed. Such signals must be adopted by both the\nsender (users) and intended recipients (data processors). Adoption represents a\ncoordination problem that remains unsolved despite efforts dating back to the\n1990s. Browsers implemented standards like the Platform for Privacy Preferences\n(P3P) and Do Not Track (DNT), but vendors profiting from personal data faced\nfew incentives to receive and respect the expressed wishes of data subjects. In\nthe wake of recent privacy laws, a coalition of AdTech firms published the\nTransparency and Consent Framework (TCF), which defines an opt-in consent\nsignal. This paper integrates post-GDPR developments into the wider history of\nprivacy preference signals. Our main contribution is a high-frequency\nlongitudinal study describing how TCF signal gained dominance as of February\n2021. We explore which factors correlate with adoption at the website level.\nBoth the number of third parties on a website and the presence of Google Ads\nare associated with higher adoption of TCF. Further, we show that vendors acted\nas early adopters of TCF 2.0 and provide two case-studies describing how\nConsent Management Providers shifted existing customers to TCF 2.0. We sketch\nways forward for a pro-privacy signal.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 06:39:20 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 00:22:35 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 08:53:05 GMT"}, {"version": "v4", "created": "Wed, 14 Jul 2021 10:48:17 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Hils", "Maximilian", "", "University of\n  Innsbruck"], ["Woods", "Daniel W.", "", "University of\n  Innsbruck"], ["B\u00f6hme", "Rainer", "", "University of\n  Innsbruck"]]}, {"id": "2106.02359", "submitter": "Zhijing Jin", "authors": "Zhijing Jin, Geeticka Chauhan, Brian Tse, Mrinmaya Sachan, Rada\n  Mihalcea", "title": "How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social\n  Impact", "comments": "Findings of ACL 2021; also accepted at the NLP for Positive Impact\n  workshop@ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have seen many breakthroughs in natural language processing\n(NLP), transitioning it from a mostly theoretical field to one with many\nreal-world applications. Noting the rising number of applications of other\nmachine learning and AI techniques with pervasive societal impact, we\nanticipate the rising importance of developing NLP technologies for social\ngood. Inspired by theories in moral philosophy and global priorities research,\nwe aim to promote a guideline for social good in the context of NLP. We lay the\nfoundations via the moral philosophy definition of social good, propose a\nframework to evaluate the direct and indirect real-world impact of NLP tasks,\nand adopt the methodology of global priorities research to identify priority\ncauses for NLP research. Finally, we use our theoretical framework to provide\nsome practical guidelines for future NLP research for social good. Our data and\ncode are available at http://github.com/zhijing-jin/nlp4sg_acl2021. In\naddition, we curate a list of papers and resources on NLP for social good at\nhttps://github.com/zhijing-jin/NLP4SocialGood_Papers.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 09:17:15 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 19:26:38 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Jin", "Zhijing", ""], ["Chauhan", "Geeticka", ""], ["Tse", "Brian", ""], ["Sachan", "Mrinmaya", ""], ["Mihalcea", "Rada", ""]]}, {"id": "2106.02522", "submitter": "Jichang Zhao", "authors": "Junran Wu, Ke Xu, Xueyuan Chen, Shangzhe Li and Jichang Zhao", "title": "Price graphs: Utilizing the structural information of financial time\n  series for stock prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Great research efforts have been devoted to exploiting deep neural networks\nin stock prediction. While long-range dependencies and chaotic property are\nstill two major issues that lower the performance of state-of-the-art deep\nlearning models in forecasting future price trends. In this study, we propose a\nnovel framework to address both issues. Specifically, in terms of transforming\ntime series into complex networks, we convert market price series into graphs.\nThen, structural information, referring to associations among temporal points\nand the node weights, is extracted from the mapped graphs to resolve the\nproblems regarding long-range dependencies and the chaotic property. We take\ngraph embeddings to represent the associations among temporal points as the\nprediction model inputs. Node weights are used as a priori knowledge to enhance\nthe learning of temporal attention. The effectiveness of our proposed framework\nis validated using real-world stock data, and our approach obtains the best\nperformance among several state-of-the-art benchmarks. Moreover, in the\nconducted trading simulations, our framework further obtains the highest\ncumulative profits. Our results supplement the existing applications of complex\nnetwork methods in the financial realm and provide insightful implications for\ninvestment applications regarding decision support in financial markets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 14:46:08 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 10:00:13 GMT"}, {"version": "v3", "created": "Fri, 16 Jul 2021 02:56:03 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Wu", "Junran", ""], ["Xu", "Ke", ""], ["Chen", "Xueyuan", ""], ["Li", "Shangzhe", ""], ["Zhao", "Jichang", ""]]}, {"id": "2106.02596", "submitter": "Svetlana Kiritchenko", "authors": "Kathleen C. Fraser, Isar Nejadgholi, Svetlana Kiritchenko", "title": "Understanding and Countering Stereotypes: A Computational Approach to\n  the Stereotype Content Model", "comments": "In Proceedings of the Joint Conference of the 59th Annual Meeting of\n  the Association for Computational Linguistics and the 11th International\n  Joint Conference on Natural Language Processing (ACL-IJCNLP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stereotypical language expresses widely-held beliefs about different social\ncategories. Many stereotypes are overtly negative, while others may appear\npositive on the surface, but still lead to negative consequences. In this work,\nwe present a computational approach to interpreting stereotypes in text through\nthe Stereotype Content Model (SCM), a comprehensive causal theory from social\npsychology. The SCM proposes that stereotypes can be understood along two\nprimary dimensions: warmth and competence. We present a method for defining\nwarmth and competence axes in semantic embedding space, and show that the four\nquadrants defined by this subspace accurately represent the warmth and\ncompetence concepts, according to annotated lexicons. We then apply our\ncomputational SCM model to textual stereotype data and show that it compares\nfavourably with survey-based studies in the psychological literature.\nFurthermore, we explore various strategies to counter stereotypical beliefs\nwith anti-stereotypes. It is known that countering stereotypes with\nanti-stereotypical examples is one of the most effective ways to reduce biased\nthinking, yet the problem of generating anti-stereotypes has not been\npreviously studied. Thus, a better understanding of how to generate realistic\nand effective anti-stereotypes can contribute to addressing pressing societal\nconcerns of stereotyping, prejudice, and discrimination.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 16:53:37 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Fraser", "Kathleen C.", ""], ["Nejadgholi", "Isar", ""], ["Kiritchenko", "Svetlana", ""]]}, {"id": "2106.02708", "submitter": "Venkata Sriram Siddhardh Nadendla", "authors": "Sainath Sanga and Venkata Sriram Siddhardh Nadendla", "title": "On the Design of Strategic Task Recommendations for Sustainable\n  Crowdsourcing-Based Content Moderation", "comments": "Presented at International Workshop on Autonomous Agents for Social\n  Good (AASG), May 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.CY cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing-based content moderation is a platform that hosts content\nmoderation tasks for crowd workers to review user submissions (e.g. text,\nimages and videos) and make decisions regarding the admissibility of the posted\ncontent, along with a gamut of other tasks such as image labeling and\nspeech-to-text conversion. In an attempt to reduce cognitive overload at the\nworkers and improve system efficiency, these platforms offer personalized task\nrecommendations according to the worker's preferences. However, the current\nstate-of-the-art recommendation systems disregard the effects on worker's\nmental health, especially when they are repeatedly exposed to content\nmoderation tasks with extreme content (e.g. violent images, hate-speech). In\nthis paper, we propose a novel, strategic recommendation system for the\ncrowdsourcing platform that recommends jobs based on worker's mental status.\nSpecifically, this paper models interaction between the crowdsourcing\nplatform's recommendation system (leader) and the worker (follower) as a\nBayesian Stackelberg game where the type of the follower corresponds to the\nworker's cognitive atrophy rate and task preferences. We discuss how rewards\nand costs should be designed to steer the game towards desired outcomes in\nterms of maximizing the platform's productivity, while simultaneously improving\nthe working conditions of crowd workers.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 20:35:14 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Sanga", "Sainath", ""], ["Nadendla", "Venkata Sriram Siddhardh", ""]]}, {"id": "2106.02961", "submitter": "Giovanni Colavizza", "authors": "Giovanni Colavizza", "title": "Meta-research on COVID-19: An overview of the early trends", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  COVID-19 is having a dramatic impact on research and researchers. The\npandemic has underlined the severity of known challenges in research and\nsurfaced new ones, but also accelerated the adoption of innovations and\nmanifested new opportunities. This review considers early trends emerging from\nmeta-research on COVID-19. In particular, it focuses on the following topics:\ni) mapping COVID-19 research; ii) data and machine learning; iii) research\npractices including open access and open data, reviewing, publishing and\nfunding; iv) communicating research to the public; v) the impact of COVID-19 on\nresearchers, in particular with respect to gender and career trajectories. This\noverview finds that most early meta-research on COVID-19 has been reactive and\nfocused on short-term questions, while more recently a shift to consider the\nlong-term consequences of COVID-19 is taking place. Based on these findings,\nthe author speculates that some aspects of doing research during COVID-19 are\nmore likely to persist than others. These include: the shift to virtual for\nacademic events such as conferences; the use of openly accessible pre-prints;\nthe `datafication' of scholarly literature and consequent broader adoption of\nmachine learning in science communication; the public visibility of research\nand researchers on social and online media.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 20:50:43 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Colavizza", "Giovanni", ""]]}, {"id": "2106.03060", "submitter": "Huansheng Ning Prof", "authors": "Sahraoui Dhelim, Liming Luke Chen, Nyothiri Aung, Wenyin Zhang,\n  Huansheng Ning", "title": "Big-Five, MPTI, Eysenck or HEXACO: The Ideal Personality Model for\n  Personality-aware Recommendation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personality-aware recommendation systems have been proven to achieve high\naccuracy compared to conventional recommendation systems. In addition to that,\npersonality-aware recommendation systems could help alleviate cold start and\ndata sparsity problems. Most of the existing works use Big-Five personality\nmodel to represent the user's personality, this is due to the popularity of\nBig-Five model in the literature of psychology. However, from personality\ncomputing perspective, the choice of the most suitable personality model that\nsatisfy the requirements of the recommendation application and the recommended\ncontent type still needs further investigation. In this paper, we study and\ncompare four personality-aware recommendation systems based on different\npersonality models, namely Big-Five, Eysenck and HEXACO from the personality\ntraits theory, and Myers-Briggs Type Indicator (MPTI) from the personality\ntypes theory. Following that, we propose a hybrid personality model for\nrecommendation that takes advantage of the personality traits models, as well\nas the personality types models. Through extensive experiments on\nrecommendation dataset, we prove the efficiency of the proposed model,\nespecially in cold start settings.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 08:17:55 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Dhelim", "Sahraoui", ""], ["Chen", "Liming Luke", ""], ["Aung", "Nyothiri", ""], ["Zhang", "Wenyin", ""], ["Ning", "Huansheng", ""]]}, {"id": "2106.03148", "submitter": "Jianjun Zhou", "authors": "Pan Deng, Jianjun Zhou, Jing Lyu, Zitong Zhao", "title": "Assessing Attendance by Peer Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Attendance rate is an important indicator of students' study motivation,\nbehavior and Psychological status; However, the heterogeneous nature of student\nattendance rates due to the course registration difference or the\nonline/offline difference in a blended learning environment makes it\nchallenging to compare attendance rates. In this paper, we propose a novel\nmethod called Relative Attendance Index (RAI) to measure attendance rates,\nwhich reflects students' efforts on attending courses. While traditional\nattendance focuses on the record of a single person or course, relative\nattendance emphasizes peer attendance information of relevant individuals or\ncourses, making the comparisons of attendance more justified. Experimental\nresults on real-life data show that RAI can indeed better reflect student\nengagement.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 15:00:40 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Deng", "Pan", ""], ["Zhou", "Jianjun", ""], ["Lyu", "Jing", ""], ["Zhao", "Zitong", ""]]}, {"id": "2106.03386", "submitter": "Felix Beierle", "authors": "Felix Beierle, Johannes Schobel, Carsten Vogel, Johannes Allgaier,\n  Lena Mulansky, Fabian Haug, Julian Haug, Winfried Schlee, Marc Holfelder,\n  Michael Stach, Marc Schickler, Harald Baumeister, Caroline Cohrdes, J\\\"urgen\n  Deckert, Lorenz Deserno, Johanna-Sophie Edler, Felizitas A. Eichner, Helmut\n  Greger, Grit Hein, Peter Heuschmann, Dennis John, Hans A. Kestler, Dagmar\n  Krefting, Berthold Langguth, Patrick Meybohm, Thomas Probst, Manfred\n  Reichert, Marcel Romanos, Stefan St\\\"ork, Yannik Terhorst, Martin Wei{\\ss},\n  R\\\"udiger Pryss", "title": "Corona Health -- A Study- and Sensor-based Mobile App Platform Exploring\n  Aspects of the COVID-19 Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical and mental well-being during the COVID-19 pandemic is typically\nassessed via surveys, which might make it difficult to conduct longitudinal\nstudies and might lead to data suffering from recall bias. Ecological momentary\nassessment (EMA) driven smartphone apps can help alleviate such issues,\nallowing for in situ recordings. Implementing such an app is not trivial,\nnecessitates strict regulatory and legal requirements, and requires short\ndevelopment cycles to appropriately react to abrupt changes in the pandemic.\nBased on an existing app framework, we developed Corona Health, an app that\nserves as a platform for deploying questionnaire-based studies in combination\nwith recordings of mobile sensors. In this paper, we present the technical\ndetails of Corona Health and provide first insights into the collected data.\nThrough collaborative efforts from experts from public health, medicine,\npsychology, and computer science, we released Corona Health publicly on Google\nPlay and the Apple App Store (in July, 2020) in 8 languages and attracted 7,290\ninstallations so far. Currently, five studies related to physical and mental\nwell-being are deployed and 17,241 questionnaires have been filled out. Corona\nHealth proves to be a viable tool for conducting research related to the\nCOVID-19 pandemic and can serve as a blueprint for future EMA-based studies.\nThe data we collected will substantially improve our knowledge on mental and\nphysical health states, traits and trajectories as well as its risk and\nprotective factors over the course of the COVID-19 pandemic and its diverse\nprevention measures.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 07:26:59 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 12:26:47 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Beierle", "Felix", ""], ["Schobel", "Johannes", ""], ["Vogel", "Carsten", ""], ["Allgaier", "Johannes", ""], ["Mulansky", "Lena", ""], ["Haug", "Fabian", ""], ["Haug", "Julian", ""], ["Schlee", "Winfried", ""], ["Holfelder", "Marc", ""], ["Stach", "Michael", ""], ["Schickler", "Marc", ""], ["Baumeister", "Harald", ""], ["Cohrdes", "Caroline", ""], ["Deckert", "J\u00fcrgen", ""], ["Deserno", "Lorenz", ""], ["Edler", "Johanna-Sophie", ""], ["Eichner", "Felizitas A.", ""], ["Greger", "Helmut", ""], ["Hein", "Grit", ""], ["Heuschmann", "Peter", ""], ["John", "Dennis", ""], ["Kestler", "Hans A.", ""], ["Krefting", "Dagmar", ""], ["Langguth", "Berthold", ""], ["Meybohm", "Patrick", ""], ["Probst", "Thomas", ""], ["Reichert", "Manfred", ""], ["Romanos", "Marcel", ""], ["St\u00f6rk", "Stefan", ""], ["Terhorst", "Yannik", ""], ["Wei\u00df", "Martin", ""], ["Pryss", "R\u00fcdiger", ""]]}, {"id": "2106.03673", "submitter": "Karen Levy", "authors": "Karen Levy, Kyla Chasalow, Sarah Riley", "title": "Algorithms and Decision-Making in the Public Sector", "comments": null, "journal-ref": "Annual Review of Law and Social Science, Vol. 17 (2021)", "doi": "10.1146/annurev-lawsocsci-041221-023808", "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This article surveys the use of algorithmic systems to support\ndecision-making in the public sector. Governments adopt, procure, and use\nalgorithmic systems to support their functions within several contexts --\nincluding criminal justice, education, and benefits provision -- with important\nconsequences for accountability, privacy, social inequity, and public\nparticipation in decision-making. We explore the social implications of\nmunicipal algorithmic systems across a variety of stages, including problem\nformulation, technology acquisition, deployment, and evaluation. We highlight\nseveral open questions that require further empirical research.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 14:53:04 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 02:33:35 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Levy", "Karen", ""], ["Chasalow", "Kyla", ""], ["Riley", "Sarah", ""]]}, {"id": "2106.03750", "submitter": "Himanshu Thapliyal", "authors": "Amit Degada, Himanshu Thapliyal, Saraju P. Mohanty", "title": "Smart Village: An IoT Based Digital Transformation", "comments": "5 Pages, Conference: IEEE 7th World Forum on Internet of Things\n  (WF-IoT), New Orleans, June 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.NI cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Almost 46% of the world's population resides in a rural landscape. Smart\nvillages, alongside smart cities, are in need of time for future economic\ngrowth, improved agriculture, better health, and education. The smart village\nis a concept that improves the traditional rural aspects with the help of\ndigital transformation. The smart village is built up using heterogeneous\ndigital technologies pillared around the Internet-of-Thing (IoT). There exist\nmany opportunities in research to design a low-cost, secure, and efficient\ntechnical ecosystem. This article identifies the key application areas, where\nthe IoT can be applied in the smart village. The article also presents a\ncomparative study of communication technology options.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 16:16:33 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Degada", "Amit", ""], ["Thapliyal", "Himanshu", ""], ["Mohanty", "Saraju P.", ""]]}, {"id": "2106.03924", "submitter": "Gabriele Etta", "authors": "Gabriele Etta, Matteo Cinelli, Alessandro Galeazzi, Carlo Michele\n  Valensise, Mauro Conti, Walter Quattrociocchi", "title": "News consumption and social media regulations policy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Users online tend to consume information adhering to their system of beliefs\nand to ignore dissenting information. During the COVID-19 pandemic, users get\nexposed to a massive amount of information about a new topic having a high\nlevel of uncertainty. In this paper, we analyze two social media that enforced\nopposite moderation methods, Twitter and Gab, to assess the interplay between\nnews consumption and content regulation concerning COVID-19. We compare the two\nplatforms on about three million pieces of content analyzing user interaction\nwith respect to news articles. We first describe users' consumption patterns on\nthe two platforms focusing on the political leaning of news outlets. Finally,\nwe characterize the echo chamber effect by modeling the dynamics of users'\ninteraction networks. Our results show that the presence of moderation pursued\nby Twitter produces a significant reduction of questionable content, with a\nconsequent affiliation towards reliable sources in terms of engagement and\ncomments. Conversely, the lack of clear regulation on Gab results in the\ntendency of the user to engage with both types of content, showing a slight\npreference for the questionable ones which may account for a\ndissing/endorsement behavior. Twitter users show segregation towards reliable\ncontent with a uniform narrative. Gab, instead, offers a more heterogeneous\nstructure where users, independently of their leaning, follow people who are\nslightly polarized towards questionable news.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 19:26:32 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Etta", "Gabriele", ""], ["Cinelli", "Matteo", ""], ["Galeazzi", "Alessandro", ""], ["Valensise", "Carlo Michele", ""], ["Conti", "Mauro", ""], ["Quattrociocchi", "Walter", ""]]}, {"id": "2106.03965", "submitter": "Somalee Datta", "authors": "Sanjay Malunjkar, Susan Weber, Somalee Datta", "title": "A highly scalable repository of waveform and vital signs data from\n  bedside monitoring devices", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CY cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The advent of cost effective cloud computing over the past decade and\never-growing accumulation of high-fidelity clinical data in a modern hospital\nsetting is leading to new opportunities for translational medicine. Machine\nlearning is driving the appetite of the research community for various types of\nsignal data such as patient vitals. Health care systems, however, are ill\nsuited for massive processing of large volumes of data. In addition, due to the\nsheer magnitude of the data being collected, it is not feasible to retain all\nof the data in health care systems in perpetuity. This gold mine of information\ngets purged periodically thereby losing invaluable future research\nopportunities. We have developed a highly scalable solution that: a) siphons\noff patient vital data on a nightly basis from on-premises bio-medical systems\nto a cloud storage location as a permanent archive, b) reconstructs the\ndatabase in the cloud, c) generates waveforms, alarms and numeric data in a\nresearch-ready format, and d) uploads the processed data to a storage location\nin the cloud ready for research.\n  The data is de-identified and catalogued such that it can be joined with\nElectronic Medical Records (EMR) and other ancillary data types such as\nelectroencephalogram (EEG), radiology, video monitoring etc. This technique\neliminates the research burden from health care systems. This highly scalable\nsolution is used to process high density patient monitoring data aggregated by\nthe Philips Patient Information Center iX (PIC iX) hospital surveillance system\nfor archival storage in the Philips Data Warehouse Connect enterprise-level\ndatabase. The solution is part of a broader platform that supports a secure\nhigh performance clinical data science platform.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 20:59:58 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Malunjkar", "Sanjay", ""], ["Weber", "Susan", ""], ["Datta", "Somalee", ""]]}, {"id": "2106.04420", "submitter": "Harshavardhan Kamarthi", "authors": "Harshavardhan Kamarthi, Alexander Rodr\\'iguez, B. Aditya Prakash", "title": "Back2Future: Leveraging Backfill Dynamics for Improving Real-time\n  Predictions in Future", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In real-time forecasting in public health, data collection is a non-trivial\nand demanding task. Often after initially released, it undergoes several\nrevisions later (maybe due to human or technical constraints) - as a result, it\nmay take weeks until the data reaches to a stable value. This so-called\n'backfill' phenomenon and its effect on model performance has been barely\nstudied in the prior literature. In this paper, we introduce the multi-variate\nbackfill problem using COVID-19 as the motivating example. We construct a\ndetailed dataset composed of relevant signals over the past year of the\npandemic. We then systematically characterize several patterns in backfill\ndynamics and leverage our observations for formulating a novel problem and\nneural framework Back2Future that aims to refines a given model's predictions\nin real-time. Our extensive experiments demonstrate that our method refines the\nperformance of top models for COVID-19 forecasting, in contrast to non-trivial\nbaselines, yielding 18% improvement over baselines, enabling us obtain a new\nSOTA performance. In addition, we show that our model improves model evaluation\ntoo; hence policy-makers can better understand the true accuracy of forecasting\nmodels in real-time.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 14:48:20 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Kamarthi", "Harshavardhan", ""], ["Rodr\u00edguez", "Alexander", ""], ["Prakash", "B. Aditya", ""]]}, {"id": "2106.04511", "submitter": "Deepak Kumar", "authors": "Deepak Kumar, Patrick Gage Kelley, Sunny Consolvo, Joshua Mason, Elie\n  Bursztein, Zakir Durumeric, Kurt Thomas, Michael Bailey", "title": "Designing Toxic Content Classification for a Diversity of Perspectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we demonstrate how existing classifiers for identifying toxic\ncomments online fail to generalize to the diverse concerns of Internet users.\nWe survey 17,280 participants to understand how user expectations for what\nconstitutes toxic content differ across demographics, beliefs, and personal\nexperiences. We find that groups historically at-risk of harassment - such as\npeople who identify as LGBTQ+ or young adults - are more likely to to flag a\nrandom comment drawn from Reddit, Twitter, or 4chan as toxic, as are people who\nhave personally experienced harassment in the past. Based on our findings, we\nshow how current one-size-fits-all toxicity classification algorithms, like the\nPerspective API from Jigsaw, can improve in accuracy by 86% on average through\npersonalized model tuning. Ultimately, we highlight current pitfalls and new\ndesign directions that can improve the equity and efficacy of toxic content\nclassifiers for all users.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 16:45:15 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Kumar", "Deepak", ""], ["Kelley", "Patrick Gage", ""], ["Consolvo", "Sunny", ""], ["Mason", "Joshua", ""], ["Bursztein", "Elie", ""], ["Durumeric", "Zakir", ""], ["Thomas", "Kurt", ""], ["Bailey", "Michael", ""]]}, {"id": "2106.04569", "submitter": "Nataniel Ruiz", "authors": "Nataniel Ruiz, Adam Kortylewski, Weichao Qiu, Cihang Xie, Sarah Adel\n  Bargal, Alan Yuille, Stan Sclaroff", "title": "Simulated Adversarial Testing of Face Recognition Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most machine learning models are validated and tested on fixed datasets. This\ncan give an incomplete picture of the capabilities and weaknesses of the model.\nSuch weaknesses can be revealed at test time in the real world. The risks\ninvolved in such failures can be loss of profits, loss of time or even loss of\nlife in certain critical applications. In order to alleviate this issue,\nsimulators can be controlled in a fine-grained manner using interpretable\nparameters to explore the semantic image manifold. In this work, we propose a\nframework for learning how to test machine learning algorithms using simulators\nin an adversarial manner in order to find weaknesses in the model before\ndeploying it in critical scenarios. We apply this model in a face recognition\nscenario. We are the first to show that weaknesses of models trained on real\ndata can be discovered using simulated samples. Using our proposed method, we\ncan find adversarial synthetic faces that fool contemporary face recognition\nmodels. This demonstrates the fact that these models have weaknesses that are\nnot measured by commonly used validation datasets. We hypothesize that this\ntype of adversarial examples are not isolated, but usually lie in connected\ncomponents in the latent space of the simulator. We present a method to find\nthese adversarial regions as opposed to the typical adversarial points found in\nthe adversarial example literature.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 17:58:10 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Ruiz", "Nataniel", ""], ["Kortylewski", "Adam", ""], ["Qiu", "Weichao", ""], ["Xie", "Cihang", ""], ["Bargal", "Sarah Adel", ""], ["Yuille", "Alan", ""], ["Sclaroff", "Stan", ""]]}, {"id": "2106.04675", "submitter": "Marios Constantinides", "authors": "Melanie Bancilhon, Marios Constantinides, Edyta Paulina Bogucka, Luca\n  Maria Aiello, Daniele Quercia", "title": "Streetonomics: Quantifying Culture Using Street Names", "comments": "17 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantifying a society's value system is important because it suggests what\npeople deeply care about -- it reflects who they actually are and, more\nimportantly, who they will like to be. This cultural quantification has been\ntypically done by studying literary production. However, a society's value\nsystem might well be implicitly quantified based on the decisions that people\ntook in the past and that were mediated by what they care about. It turns out\nthat one class of these decisions is visible in ordinary settings: it is\nvisible in street names. We studied the names of 4,932 honorific streets in the\ncities of Paris, Vienna, London and New York. We chose these four cities\nbecause they were important centers of cultural influence for the Western world\nin the 20th century. We found that street names greatly reflect the extent to\nwhich a society is gender biased, which professions are considered elite ones,\nand the extent to which a city is influenced by the rest of the world. This way\nof quantifying a society's value system promises to inform new methodologies in\nDigital Humanities; makes it possible for municipalities to reflect on their\npast to inform their future; and informs the design of everyday's educational\ntools that promote historical awareness in a playful way.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 20:42:42 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 07:37:01 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Bancilhon", "Melanie", ""], ["Constantinides", "Marios", ""], ["Bogucka", "Edyta Paulina", ""], ["Aiello", "Luca Maria", ""], ["Quercia", "Daniele", ""]]}, {"id": "2106.04688", "submitter": "Marios Constantinides", "authors": "Edyta Paulina Bogucka, Marios Constantinides, Luca Maria Aiello,\n  Daniele Quercia, Wonyoung So, Melanie Bancilhon", "title": "Cartographic Design of Cultural Maps", "comments": "9 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Throughout history, maps have been used as a tool to explore cities. They\nvisualize a city's urban fabric through its streets, buildings, and points of\ninterest. Besides purely navigation purposes, street names also reflect a\ncity's culture through its commemorative practices. Therefore, cultural maps\nthat unveil socio-cultural characteristics encoded in street names could\npotentially raise citizens' historical awareness. But designing effective\ncultural maps is challenging, not only due to data scarcity but also due to the\nlack of effective approaches to engage citizens with data exploration. To\naddress these challenges, we collected a dataset of 5,000 streets across the\ncities of Paris, Vienna, London, and New York, and built their cultural maps\ngrounded on cartographic storytelling techniques. Through data exploration\nscenarios, we demonstrated how cultural maps engage users and allow them to\ndiscover distinct patterns in the ways these cities are gender-biased,\ncelebrate various professions, and embrace foreign cultures.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 20:53:45 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Bogucka", "Edyta Paulina", ""], ["Constantinides", "Marios", ""], ["Aiello", "Luca Maria", ""], ["Quercia", "Daniele", ""], ["So", "Wonyoung", ""], ["Bancilhon", "Melanie", ""]]}, {"id": "2106.04757", "submitter": "Mustafa Ozdayi", "authors": "Mustafa Safa Ozdayi, Murat Kantarcioglu, Rishabh Iyer", "title": "BiFair: Training Fair Models with Bilevel Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prior studies have shown that, training machine learning models via empirical\nloss minimization to maximize a utility metric (e.g., accuracy), might yield\nmodels that make discriminatory predictions. To alleviate this issue, we\ndevelop a new training algorithm, named BiFair, which jointly minimizes for a\nutility, and a fairness loss of interest. Crucially, we do so without directly\nmodifying the training objective, e.g., by adding regularization terms. Rather,\nwe learn a set of weights on the training dataset, such that, training on the\nweighted dataset ensures both good utility, and fairness. The dataset weights\nare learned in concurrence to the model training, which is done by solving a\nbilevel optimization problem using a held-out validation dataset. Overall, this\napproach yields models with better fairness-utility trade-offs. Particularly,\nwe compare our algorithm with three other state-of-the-art fair training\nalgorithms over three real-world datasets, and demonstrate that, BiFair\nconsistently performs better, i.e., we reach to better values of a given\nfairness metric under same, or higher accuracy. Further, our algorithm is\nscalable. It is applicable both to simple models, such as logistic regression,\nas well as more complex models, such as deep neural networks, as evidenced by\nour experimental analysis.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 22:36:17 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Ozdayi", "Mustafa Safa", ""], ["Kantarcioglu", "Murat", ""], ["Iyer", "Rishabh", ""]]}, {"id": "2106.04906", "submitter": "Edward Oughton", "authors": "Edward J. Oughton and Erik Boch and Julius Kusuma", "title": "Engineering-Economic Evaluation of Diffractive Non-Line-Of-Sight\n  Backhaul (e3nb): A Techno-economic Model for 3D Wireless Backhaul Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CY cs.ET econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing ways to affordably deliver broadband connectivity is one of the\nmajor issues of our time. In challenging deployment locations with irregular\nterrain, fiber optic or traditional Clear-Line-Of-Sight (CLOS) wireless links\ncan be uneconomical to deploy, resulting from the number of required towers\nmaking infrastructure deployment unviable. With the emergence of new research\nfocusing on developing wireless diffractive backhaul technologies to provide\ndiffractive Non-Line-Of-Sight (NLOS) links, this paper evaluates the\nengineering-economic implications of such approaches. To quantify different\ntechnology strategies, a Three-Dimensional (3D) techno-economic assessment\nframework is presented to help prioritize regions for future investment in\nbroadband connectivity, utilizing a combination of remote sensing and viewshed\ngeospatial techniques. Such a method is an essential evaluation step prior to\nbeginning detailed Radio Frequency (RF) Quality of Service engineering but has\nhitherto received less research attention in the literature. This framework is\napplied to assess both Clear-Line-Of-Sight and diffractive Non-Line-Of-Sight\nstrategies for deployment in Peru, as well as the islands of Kalimantan and\nPapua, in Indonesia. The results find that a hybrid strategy combining the use\nof Clear-Line-Of-Sight and diffractive Non-Line-Of-Sight links produces a 15-43\npercent cost-efficiency saving, relative to only using traditional\nClear-Line-Of-Sight wireless backhaul links. The codebase is released\nopensource via the Engineering-Economic Evaluation of Non-Line-of-Sight\nBackhaul (e3nb) repository.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 08:43:46 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Oughton", "Edward J.", ""], ["Boch", "Erik", ""], ["Kusuma", "Julius", ""]]}, {"id": "2106.05027", "submitter": "Keisuke Okamura", "authors": "Keisuke Okamura", "title": "Scientometric engineering: Revealing spatiotemporal citation dynamics\n  via open eprints", "comments": "1+25 pages, 6 figures for main text; 1+24 pages, 14 figures for\n  supplementary information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CY cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever-increasing speed and volume of knowledge production and\nconsumption, scholarly communication systems have been rapidly transformed into\ndigitised and networked open ecosystems, where preprint servers have played a\npivotal role. However, evidence is scarce regarding how this paradigm shift has\naffected the dynamics of collective attention on scientific knowledge. Herein,\nwe address this issue by investigating the citation dynamics of more than 1.5\nmillion eprints on arXiv, the most prominent and oldest eprint archive. The\ndiscipline-average citation history curves are estimated by applying a\nnonlinear regression model to the long-term citation data. The revealed\nspatiotemporal characteristics, including the growth and obsolescence patterns,\nare shown to vary across disciplines, reflecting the different publication and\ncitation practices. The results are used to develop a spatiotemporally\nnormalised citation index, called the $\\gamma$-index, with an approximately\nnormal distribution. It can be used to compare the citational impact of\nindividual papers across disciplines and time periods, providing a less biased\nmeasure of research impact than those widely used in the literature and in\npractice. Further, a stochastic model for the observed spatiotemporal citation\ndynamics is derived, reproducing both the Lognormal Law for the cumulative\ncitation distribution and the time trajectory of average citations in a unified\nformalism.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 12:38:44 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Okamura", "Keisuke", ""]]}, {"id": "2106.05086", "submitter": "Di Zhang Dr.", "authors": "Di Zhang, Joel J. P. C. Rodrigues, Yunkai Zhai, Takuro Sato", "title": "Design and Implementation of 5G eHealth Systems, Technologies, Use Cases\n  and Future Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fifth generation (5G) aims to connect massive devices with even higher\nreliability, lower latency and even faster transmission speed, which are vital\nfor implementing the e-health systems. However, the current efforts on 5G\ne-health systems are still not enough to accomplish its full blueprint. In this\narticle, we first discuss the related technologies from physical layer, upper\nlayer and cross layer perspectives on designing the 5G e-health systems. We\nafterwards elaborate two use cases according to our implementations, i.e., 5G\ne-health systems for remote health and 5G e-health systems for Covid-19\npandemic containment. We finally envision the future research trends and\nchallenges of 5G e-health systems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:05:07 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 05:48:12 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Zhang", "Di", ""], ["Rodrigues", "Joel J. P. C.", ""], ["Zhai", "Yunkai", ""], ["Sato", "Takuro", ""]]}, {"id": "2106.05127", "submitter": "Peizhao Li", "authors": "Hanyu Song, Peizhao Li, Hongfu Liu", "title": "Deep Clustering based Fair Outlier Detection", "comments": "To appear in KDD'2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we focus on the fairness issues regarding unsupervised outlier\ndetection. Traditional algorithms, without a specific design for algorithmic\nfairness, could implicitly encode and propagate statistical bias in data and\nraise societal concerns. To correct such unfairness and deliver a fair set of\npotential outlier candidates, we propose Deep Clustering based Fair Outlier\nDetection (DCFOD) that learns a good representation for utility maximization\nwhile enforcing the learnable representation to be subgroup-invariant on the\nsensitive attribute. Considering the coupled and reciprocal nature between\nclustering and outlier detection, we leverage deep clustering to discover the\nintrinsic cluster structure and out-of-structure instances. Meanwhile, an\nadversarial training erases the sensitive pattern for instances for fairness\nadaptation. Technically, we propose an instance-level weighted representation\nlearning strategy to enhance the joint deep clustering and outlier detection,\nwhere the dynamic weight module re-emphasizes contributions of likely-inliers\nwhile mitigating the negative impact from outliers. Demonstrated by experiments\non eight datasets comparing to 17 outlier detection algorithms, our DCFOD\nmethod consistently achieves superior performance on both the outlier detection\nvalidity and two types of fairness notions in outlier detection.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 15:12:26 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Song", "Hanyu", ""], ["Li", "Peizhao", ""], ["Liu", "Hongfu", ""]]}, {"id": "2106.05184", "submitter": "Pushkal Agarwal", "authors": "Pushkal Agarwal, Aravindh Raman, Kiran Garimella, Damilola Ibosiola,\n  Gareth Tyson, Nishanth Sastry", "title": "Tackling spam in the era of end-to-end encryption: A case study of\n  WhatsApp", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  WhatsApp is a popular messaging app used by over a billion users around the\nglobe. Due to this popularity, spam on WhatsApp is an important issue. Despite\nthis, the distribution of spam via WhatsApp remains understudied by\nresearchers, in part because of the end-to-end encryption offered by the\nplatform. This paper addresses this gap by studying spam on a dataset of 2.6\nmillion messages sent to 5,051 public WhatsApp groups in India over 300 days.\nFirst, we characterise spam content shared within public groups and find that\nnearly 1 in 10 messages is spam. We observe a wide selection of topics ranging\nfrom job ads to adult content, and find that spammers post both URLs and phone\nnumbers to promote material. Second, we inspect the nature of spammers\nthemselves. We find that spam is often disseminated by groups of phone numbers,\nand that spam messages are generally shared for longer duration than non-spam\nmessages. Finally, we devise content and activity based detection algorithms\nthat can counter spam.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 15:52:46 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 07:05:17 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Agarwal", "Pushkal", ""], ["Raman", "Aravindh", ""], ["Garimella", "Kiran", ""], ["Ibosiola", "Damilola", ""], ["Tyson", "Gareth", ""], ["Sastry", "Nishanth", ""]]}, {"id": "2106.05227", "submitter": "Pardis Emami-Naeini", "authors": "Pardis Emami-Naeini, Tiona Francisco, Tadayoshi Kohno, Franziska\n  Roesner", "title": "Understanding Privacy Attitudes and Concerns Towards Remote\n  Communications During the COVID-19 Pandemic", "comments": "To appear at the 17th Symposium on Usable Privacy and Security\n  (SOUPS'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since December 2019, the COVID-19 pandemic has caused people around the world\nto exercise social distancing, which has led to an abrupt rise in the adoption\nof remote communications for working, socializing, and learning from home. As\nremote communications will outlast the pandemic, it is crucial to protect\nusers' security and respect their privacy in this unprecedented setting, and\nthat requires a thorough understanding of their behaviors, attitudes, and\nconcerns toward various aspects of remote communications. To this end, we\nconducted an online study with 220 worldwide Prolific participants. We found\nthat privacy and security are among the most frequently mentioned factors\nimpacting participants' attitude and comfort level with conferencing tools and\nmeeting locations. Open-ended responses revealed that most participants lacked\nautonomy when choosing conferencing tools or using microphone/webcam in their\nremote meetings, which in several cases contradicted their personal privacy and\nsecurity preferences. Based on our findings, we distill several recommendations\non how employers, educators, and tool developers can inform and empower users\nto make privacy-protective decisions when engaging in remote communications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:17:06 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Emami-Naeini", "Pardis", ""], ["Francisco", "Tiona", ""], ["Kohno", "Tadayoshi", ""], ["Roesner", "Franziska", ""]]}, {"id": "2106.05357", "submitter": "Abhishek Santra", "authors": "Abhishek Santra, Kunal Samant, Endrit Memeti, Enamul Karim and Sharma\n  Chakravarthy", "title": "An Extensible Dashboard Architecture For Visualizing Base And Analyzed\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Any data analysis, especially the data sets that may be changing often or in\nreal-time, consists of at least three important synchronized components: i)\nfiguring out what to infer (objectives), ii) analysis or computation of\nobjectives, and iii) understanding of the results which may require drill-down\nand/or visualization. There is a lot of attention paid to the first two of the\nabove components as part of research whereas the understanding as well as\nderiving actionable decisions is quite tricky. Visualization is an important\nstep towards both understanding (even by non-experts) and inferring the actions\nthat need to be taken. As an example, for Covid-19, knowing regions (say, at\nthe county or state level) that have seen a spike or prone to a spike in cases\nin the near future may warrant additional actions with respect to gatherings,\nbusiness opening hours, etc. This paper focuses on an extensible architecture\nfor visualization of base as well as analyzed data. This paper proposes a\nmodular architecture of a dashboard for user-interaction, visualization\nmanagement, and complex analysis of base data. The contributions of this paper\nare: i) extensibility of the architecture providing flexibility to add\nadditional analysis, visualizations, and user interactions without changing the\nworkflow, ii) decoupling of the functional modules to ease and speedup\ndevelopment by different groups, and iii) address efficiency issues for display\nresponse time. This paper uses Multilayer Networks (or MLNs) for analysis. To\nshowcase the above, we present the implementation of a visualization dashboard,\ntermed CoWiz++ (for Covid Wizard), and elaborate on how web-based user\ninteraction and display components are interfaced seamlessly with the back end\nmodules.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 19:45:43 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Santra", "Abhishek", ""], ["Samant", "Kunal", ""], ["Memeti", "Endrit", ""], ["Karim", "Enamul", ""], ["Chakravarthy", "Sharma", ""]]}, {"id": "2106.05498", "submitter": "Suresh Venkatasubramanian", "authors": "Michelle Bao, Angela Zhou, Samantha Zottola, Brian Brubach, Sarah\n  Desmarais, Aaron Horowitz, Kristian Lum, Suresh Venkatasubramanian", "title": "It's COMPASlicated: The Messy Relationship between RAI Datasets and\n  Algorithmic Fairness Benchmarks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Risk assessment instrument (RAI) datasets, particularly ProPublica's COMPAS\ndataset, are commonly used in algorithmic fairness papers due to benchmarking\npractices of comparing algorithms on datasets used in prior work. In many\ncases, this data is used as a benchmark to demonstrate good performance without\naccounting for the complexities of criminal justice (CJ) processes. We show\nthat pretrial RAI datasets contain numerous measurement biases and errors\ninherent to CJ pretrial evidence and due to disparities in discretion and\ndeployment, are limited in making claims about real-world outcomes, making the\ndatasets a poor fit for benchmarking under assumptions of ground truth and\nreal-world impact. Conventional practices of simply replicating previous data\nexperiments may implicitly inherit or edify normative positions without\nexplicitly interrogating assumptions. With context of how interdisciplinary\nfields have engaged in CJ research, algorithmic fairness practices are\nmisaligned for meaningful contribution in the context of CJ, and would benefit\nfrom transparent engagement with normative considerations and values related to\nfairness, justice, and equality. These factors prompt questions about whether\nbenchmarks for intrinsically socio-technical systems like the CJ system can\nexist in a beneficial and ethical way.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 04:59:06 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Bao", "Michelle", ""], ["Zhou", "Angela", ""], ["Zottola", "Samantha", ""], ["Brubach", "Brian", ""], ["Desmarais", "Sarah", ""], ["Horowitz", "Aaron", ""], ["Lum", "Kristian", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "2106.05581", "submitter": "Xiaozan Lyu", "authors": "Xiaozan Lyu and Rodrigo Costas", "title": "Studying the characteristics of scientific communities using\n  individual-level bibliometrics: the case of Big Data research", "comments": null, "journal-ref": "Scientometrics (2021)", "doi": "10.1007/s11192-021-04034-6", "report-no": null, "categories": "cs.DL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unlike most bibliometric studies focusing on publications, taking Big Data\nresearch as a case study, we introduce a novel bibliometric approach to unfold\nthe status of a given scientific community from an individual level\nperspective. We study the academic age, production, and research focus of the\ncommunity of authors active in Big Data research. Artificial Intelligence (AI)\nis selected as a reference area for comparative purposes. Results show that the\nacademic realm of \"Big Data\" is a growing topic with an expanding community of\nauthors, particularly of new authors every year. Compared to AI, Big Data\nattracts authors with a longer academic age, who can be regarded to have\naccumulated some publishing experience before entering the community. Despite\nthe highly skewed distribution of productivity amongst researchers in both\ncommunities, Big Data authors have higher values of both research focus and\nproduction than those of AI. Considering the community size, overall academic\nage, and persistence of publishing on the topic, our results support the idea\nof Big Data as a research topic with attractiveness for researchers. We argue\nthat the community-focused indicators proposed in this study could be\ngeneralized to investigate the development and dynamics of other research\nfields and topics.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 08:17:09 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Lyu", "Xiaozan", ""], ["Costas", "Rodrigo", ""]]}, {"id": "2106.05647", "submitter": "Stefano M. Iacus", "authors": "Michele Vespe, Stefano Maria Iacus, Carlos Santamaria, Francesco\n  Sermi, Spyridon Spyratos", "title": "On the Use of Data from Multiple Mobile Network Operators in Europe to\n  fight COVID-19", "comments": null, "journal-ref": "Data & Policy, 3, E9 (2021)", "doi": "10.1017/dap.2021.9", "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid spread of COVID-19 infections on a global level has highlighted the\nneed for accurate, transparent and timely information regarding collective\nmobility patterns to inform de-escalation strategies as well as to provide\nforecasting capacity for re-escalation policies aiming at addressing further\nwaves of the virus. Such information can be extracted using aggregate\nanonymised data from innovative sources such as mobile positioning data. This\npaper presents lessons learnt and results of a unique Business-to-Government\n(B2G) initiative between several Mobile Network Operators in Europe and the\nEuropean Commission. Mobile positioning data have supported policy makers and\npractitioners with evidence and data-driven knowledge to understand and predict\nthe spread of the disease, the effectiveness of the containment measures, their\nsocio-economic impacts while feeding scenarios at EU scale and in a comparable\nway across countries. The challenges of this data sharing initiative are not\nlimited to data quality, harmonisation, and comparability across countries,\nhowever important they are. Equally essential aspects that need to be addressed\nfrom the onset are related to data privacy, security, fundamental rights and\ncommercial sensitivity.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 10:39:21 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Vespe", "Michele", ""], ["Iacus", "Stefano Maria", ""], ["Santamaria", "Carlos", ""], ["Sermi", "Francesco", ""], ["Spyratos", "Spyridon", ""]]}, {"id": "2106.05725", "submitter": "Federica Bologna", "authors": "Federica Bologna, Angelo Di Iorio, Silvio Peroni, Francesco Poggi", "title": "Academics evaluating academics: a methodology to inform the review\n  process on top of open citations", "comments": "arXiv admin note: substantial text overlap with arXiv:2103.07942", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the past, several works have investigated ways for combining quantitative\nand qualitative methods in research assessment exercises. In this work, we aim\nat introducing a methodology to explore whether citation-based metrics,\ncalculated only considering open bibliographic and citation data, can yield\ninsights on how human peer-review of research assessment exercises is\nconducted. To understand if and what metrics provide relevant information, we\npropose to use a series of machine learning models to replicate the decisions\nof the committees of the research assessment exercises.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 13:09:15 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Bologna", "Federica", ""], ["Di Iorio", "Angelo", ""], ["Peroni", "Silvio", ""], ["Poggi", "Francesco", ""]]}, {"id": "2106.05831", "submitter": "Roberto Ulloa Dr.", "authors": "Roberto Ulloa and Mykola Makhortykh and Aleksandra Urman", "title": "Algorithm Auditing at a Large-Scale: Insights from Search Engine Audits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithm audits have increased in recent years due to a growing need to\nindependently assess the performance of automatically curated services that\nprocess, filter and rank the large and dynamic amount of information available\non the internet. Among several methodologies to perform such audits, virtual\nagents stand out because they offer the possibility of performing systematic\nexperiments simulating human behaviour without the associated costs of\nrecruiting participants. Motivated by the importance of research transparency\nand replicability of results, this paper focuses on the challenges of such an\napproach, and it provides methodological details, recommendations, lessons\nlearned and limitations that researchers should take into consideration when\nsetting up experiments with virtual agents. We demonstrate the successful\nperformance of our research infrastructure in multiple data collections with\ndiverse experimental designs, and point to different changes and strategies\nthat improved the quality of the method. We conclude that virtual agents are a\npromising venue for monitoring the performance of algorithms during longer\nperiods of time, and we hope that this paper serves as a base to widen the\nresearch in this direction.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 15:49:58 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Ulloa", "Roberto", ""], ["Makhortykh", "Mykola", ""], ["Urman", "Aleksandra", ""]]}, {"id": "2106.05903", "submitter": "Scott A. Hale", "authors": "Austin Botelho and Bertie Vidgen and Scott A. Hale", "title": "Deciphering Implicit Hate: Evaluating Automated Detection Algorithms for\n  Multimodal Hate", "comments": "Please note the paper contains examples of hateful content", "journal-ref": "Findings of ACL, 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection and classification of online hate is a difficult task.\nImplicit hate is particularly challenging as such content tends to have unusual\nsyntax, polysemic words, and fewer markers of prejudice (e.g., slurs). This\nproblem is heightened with multimodal content, such as memes (combinations of\ntext and images), as they are often harder to decipher than unimodal content\n(e.g., text alone). This paper evaluates the role of semantic and multimodal\ncontext for detecting implicit and explicit hate. We show that both text- and\nvisual- enrichment improves model performance, with the multimodal model\n(0.771) outperforming other models' F1 scores (0.544, 0.737, and 0.754). While\nthe unimodal-text context-aware (transformer) model was the most accurate on\nthe subtask of implicit hate detection, the multimodal model outperformed it\noverall because of a lower propensity towards false positives. We find that all\nmodels perform better on content with full annotator agreement and that\nmultimodal models are best at classifying the content where annotators\ndisagree. To conduct these investigations, we undertook high-quality annotation\nof a sample of 5,000 multimodal entries. Tweets were annotated for primary\ncategory, modality, and strategy. We make this corpus, along with the codebook,\ncode, and final model, freely available.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 16:29:42 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Botelho", "Austin", ""], ["Vidgen", "Bertie", ""], ["Hale", "Scott A.", ""]]}, {"id": "2106.05964", "submitter": "Anay Mehrotra", "authors": "L. Elisa Celis, Anay Mehrotra, Nisheeth K. Vishnoi", "title": "Fair Classification with Adversarial Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study fair classification in the presence of an omniscient adversary that,\ngiven an $\\eta$, is allowed to choose an arbitrary $\\eta$-fraction of the\ntraining samples and arbitrarily perturb their protected attributes. The\nmotivation comes from settings in which protected attributes can be incorrect\ndue to strategic misreporting, malicious actors, or errors in imputation; and\nprior approaches that make stochastic or independence assumptions on errors may\nnot satisfy their guarantees in this adversarial setting. Our main contribution\nis an optimization framework to learn fair classifiers in this adversarial\nsetting that comes with provable guarantees on accuracy and fairness. Our\nframework works with multiple and non-binary protected attributes, is designed\nfor the large class of linear-fractional fairness metrics, and can also handle\nperturbations besides protected attributes. We prove near-tightness of our\nframework's guarantees for natural hypothesis classes: no algorithm can have\nsignificantly better accuracy and any algorithm with better fairness must have\nlower accuracy. Empirically, we evaluate the classifiers produced by our\nframework for statistical rate on real-world and synthetic datasets for a\nfamily of adversaries.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:56:59 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Celis", "L. Elisa", ""], ["Mehrotra", "Anay", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "2106.06001", "submitter": "Elias Gr\\\"unewald", "authors": "Elias Gr\\\"unewald, Paul Wille, Frank Pallas, Maria C. Borges, Max-R.\n  Ulbricht", "title": "TIRA: An OpenAPI Extension and Toolbox for GDPR Transparency in RESTful\n  Architectures", "comments": "Accepted for publication at the 2021 International Workshop on\n  Privacy Engineering (IWPE'21). This is a preprint manuscript (authors' own\n  version before final copy-editing)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transparency - the provision of information about what personal data is\ncollected for which purposes, how long it is stored, or to which parties it is\ntransferred - is one of the core privacy principles underlying regulations such\nas the GDPR. Technical approaches for implementing transparency in practice\nare, however, only rarely considered. In this paper, we present a novel\napproach for doing so in current, RESTful application architectures and in line\nwith prevailing agile and DevOps-driven practices. For this purpose, we\nintroduce 1) a transparency-focused extension of OpenAPI specifications that\nallows individual service descriptions to be enriched with transparency-related\nannotations in a bottom-up fashion and 2) a set of higher-order tools for\naggregating respective information across multiple, interdependent services and\nfor coherently integrating our approach into automated CI/CD-pipelines.\nTogether, these building blocks pave the way for providing transparency\ninformation that is more specific and at the same time better reflects the\nactual implementation givens within complex service architectures than current,\noverly broad privacy statements.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 18:42:50 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Gr\u00fcnewald", "Elias", ""], ["Wille", "Paul", ""], ["Pallas", "Frank", ""], ["Borges", "Maria C.", ""], ["Ulbricht", "Max-R.", ""]]}, {"id": "2106.06053", "submitter": "Tasos Spiliotopoulos", "authors": "Tasos Spiliotopoulos, Dave Horsfall, Magdalene Ng, Kovila Coopamootoo,\n  Aad van Moorsel, Karen Elliott", "title": "Identifying and Supporting Financially Vulnerable Consumers in a\n  Privacy-Preserving Manner: A Use Case Using Decentralised Identifiers and\n  Verifiable Credentials", "comments": "Published in the ACM CHI 2021 workshop on Designing for New Forms of\n  Vulnerability", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vulnerable individuals have a limited ability to make reasonable financial\ndecisions and choices and, thus, the level of care that is appropriate to be\nprovided to them by financial institutions may be different from that required\nfor other consumers. Therefore, identifying vulnerability is of central\nimportance for the design and effective provision of financial services and\nproducts. However, validating the information that customers share and\nrespecting their privacy are both particularly important in finance and this\nposes a challenge for identifying and caring for vulnerable populations. This\nposition paper examines the potential of the combination of two emerging\ntechnologies, Decentralized Identifiers (DIDs) and Verifiable Credentials\n(VCs), for the identification of vulnerable consumers in finance in an\nefficient and privacy-preserving manner.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 21:05:34 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Spiliotopoulos", "Tasos", ""], ["Horsfall", "Dave", ""], ["Ng", "Magdalene", ""], ["Coopamootoo", "Kovila", ""], ["van Moorsel", "Aad", ""], ["Elliott", "Karen", ""]]}, {"id": "2106.06128", "submitter": "Xiaotian Zhou", "authors": "Xiaotian Zhou and Zhongzhi Zhang", "title": "Maximizing Influence of Leaders in Social Networks", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": "10.1145/3447548.3467229", "report-no": null, "categories": "cs.SI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The operation of adding edges has been frequently used to the study of\nopinion dynamics in social networks for various purposes. In this paper, we\nconsider the edge addition problem for the DeGroot model of opinion dynamics in\na social network with $n$ nodes and $m$ edges, in the presence of a small\nnumber $s \\ll n$ of competing leaders with binary opposing opinions 0 or 1.\nConcretely, we pose and investigate the problem of maximizing the equilibrium\noverall opinion by creating $k$ new edges in a candidate edge set, where each\nedge is incident to a 1-valued leader and a follower node. We show that the\nobjective function is monotone and submodular. We then propose a simple greedy\nalgorithm with an approximation factor $(1-\\frac{1}{e})$ that approximately\nsolves the problem in $O(n^3)$ time. Moreover, we provide a fast algorithm with\na $(1-\\frac{1}{e}-\\epsilon)$ approximation ratio and\n$\\tilde{O}(mk\\epsilon^{-2})$ time complexity for any $\\epsilon>0$, where\n$\\tilde{O}(\\cdot)$ notation suppresses the ${\\rm poly} (\\log n)$ factors.\nExtensive experiments demonstrate that our second approximate algorithm is\nefficient and effective, which scales to large networks with more than a\nmillion nodes.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 02:31:46 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Zhou", "Xiaotian", ""], ["Zhang", "Zhongzhi", ""]]}, {"id": "2106.06487", "submitter": "Qing Ke", "authors": "Qing Ke, Lizhen Liang, Ying Ding, Stephen V. David, Daniel E. Acuna", "title": "A dataset of mentorship in science with semantic and demographic\n  estimations", "comments": "Data can be found at https://doi.org/10.5281/zenodo.4917086", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mentorship in science is crucial for topic choice, career decisions, and the\nsuccess of mentees and mentors. Typically, researchers who study mentorship use\narticle co-authorship and doctoral dissertation datasets. However, available\ndatasets of this type focus on narrow selections of fields and miss out on\nearly career and non-publication-related interactions. Here, we describe\nMENTORSHIP, a crowdsourced dataset of 743176 mentorship relationships among\n738989 scientists across 112 fields that avoids these shortcomings. We enrich\nthe scientists' profiles with publication data from the Microsoft Academic\nGraph and \"semantic\" representations of research using deep learning content\nanalysis. Because gender and race have become critical dimensions when\nanalyzing mentorship and disparities in science, we also provide estimations of\nthese factors. We perform extensive validations of the profile--publication\nmatching, semantic content, and demographic inferences. We anticipate this\ndataset will spur the study of mentorship in science and deepen our\nunderstanding of its role in scientists' career outcomes.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 16:12:15 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Ke", "Qing", ""], ["Liang", "Lizhen", ""], ["Ding", "Ying", ""], ["David", "Stephen V.", ""], ["Acuna", "Daniel E.", ""]]}, {"id": "2106.06677", "submitter": "Shan Jiang", "authors": "Tigran Aslanyan, Shan Jiang", "title": "Examining Passenger Vehicle Miles Traveled and Carbon Emissions in the\n  Boston Metropolitan Area", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With spatial analytic, econometric, and visualization tools, this book\nchapter investigates greenhouse gas emissions for the on-road passenger vehicle\ntransport sector in the Boston metropolitan area in 2014. It compares\ngreenhouse gas emission estimations from both the production-based and\nconsumption-based perspectives with two large-scale administrative datasets:\nthe vehicle odometer readings from individual vehicle annual inspection, and\nthe road inventory data containing road segment level geospatial and traffic\ninformation. Based on spatial econometric models that examine socioeconomic and\nbuilt environment factors contributing to the vehicle miles traveled at the\ncensus tract level, it offers insights to help cities reduce VMT and carbon\nfootprint for passenger vehicle travel. Finally, it recommends a pathway for\ncities and towns in the Boston metropolitan area to curb VMT and mitigate\ncarbon emissions to achieve climate goals of carbon neutrality.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 03:28:58 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Aslanyan", "Tigran", ""], ["Jiang", "Shan", ""]]}, {"id": "2106.06720", "submitter": "Junaid Baber", "authors": "Muhammad Nasir, Maheen Bakhtyar, Junaid Baber, Sadia Lakho, Bilal\n  Ahmed, Waheed Noor", "title": "BIOPAK Flasher: Epidemic disease monitoring and detection in Pakistan\n  using text mining", "comments": "Paper is accepted in SOFTA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Infectious disease outbreak has a significant impact on morbidity, mortality\nand can cause economic instability of many countries. As global trade is\ngrowing, goods and individuals are expected to travel across the border, an\ninfected epidemic area carrier can pose a great danger to his hostile. If a\ndisease outbreak is recognized promptly, then commercial products and travelers\n(traders/visitors) will be effectively vaccinated, and therefore the disease\nstopped. Early detection of outbreaks plays an important role here, and beware\nof the rapid implementation of control measures by citizens, public health\norganizations, and government. Many indicators have valuable information, such\nas online news sources (RSS) and social media sources (Twitter, Facebook) that\ncan be used, but are unstructured and bulky, to extract information about\ndisease outbreaks. Few early warning outbreak systems exist with some\nlimitation of linguistic (Urdu) and covering areas (Pakistan). In Pakistan, few\nchannels are published the outbreak news in Urdu or English. The aim is to\nprocure information from Pakistan's English and Urdu news channels and then\ninvestigate process, integrate, and visualize the disease epidemic. Urdu\nontology is not existed before to match extracted diseases, so we also build\nthat ontology of disease.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 08:55:40 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Nasir", "Muhammad", ""], ["Bakhtyar", "Maheen", ""], ["Baber", "Junaid", ""], ["Lakho", "Sadia", ""], ["Ahmed", "Bilal", ""], ["Noor", "Waheed", ""]]}, {"id": "2106.06844", "submitter": "Hadi Asghari", "authors": "Hadi Asghari, Thomas van Biemen, Martijn Warnier", "title": "Amplifying Privacy: Scaling Up Transparency Research Through Delegated\n  Access Requests", "comments": "Peer-reviewed and presented at IEEE Workshop on Technology and\n  Consumer Protection 2021 (ConPro '21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, numerous studies have used 'data subject access requests' in\na collective manner, to tackle information asymmetries and shed light on data\ncollection and privacy practices of organizations. While successful at\nincreasing transparency, such studies are quite hard to conduct for the simple\nfact that right of access is an individual right. This means that researchers\nhave to recruit participants and guide them through the often-cumbersome\nprocess of access. In this paper, we present an alternative method: to ask\nparticipants to delegate their right of access to the researchers. We discuss\nthe legal grounds for doing this, the advantages it can bring to both\nresearchers and data subjects, and present a procedural and technical design to\nexecute it in a manner that ensures data subjects stay informed and in charge\nduring the process. We tested our method in a pilot study in the Netherlands,\nand found that it creates a win-win for both the researchers and the\nparticipants. We also noted differences in how data controllers from various\nsectors react to such requests and discuss some remaining challenges.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 19:51:55 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Asghari", "Hadi", ""], ["van Biemen", "Thomas", ""], ["Warnier", "Martijn", ""]]}, {"id": "2106.07057", "submitter": "Avijit Ghosh", "authors": "Avijit Ghosh, Aalok Shanbhag", "title": "FairCanary: Rapid Continuous Explainable Fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) models are being used in all facets of today's society\nto make high stake decisions like bail granting or credit lending, with very\nminimal regulations. Such systems are extremely vulnerable to both propagating\nand amplifying social biases, and have therefore been subject to growing\nresearch interest. One of the main issues with conventional fairness metrics is\ntheir narrow definitions which hide the complete extent of the bias by focusing\nprimarily on positive and/or negative outcomes, whilst not paying attention to\nthe overall distributional shape. Moreover, these metrics are often\ncontradictory to each other, are severely restrained by the contextual and\nlegal landscape of the problem, have technical constraints like poor support\nfor continuous outputs, the requirement of class labels, and are not\nexplainable.\n  In this paper, we present Quantile Demographic Drift, which addresses the\nshortcomings mentioned above. This metric can also be used to measure\nintra-group privilege. It is easily interpretable via existing attribution\ntechniques, and also extends naturally to individual fairness via the principle\nof like-for-like comparison. We make this new fairness score the basis of a new\nsystem that is designed to detect bias in production ML models without the need\nfor labels. We call the system FairCanary because of its capability to detect\nbias in a live deployed model and narrow down the alert to the responsible set\nof features, like the proverbial canary in a coal mine.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 17:47:44 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Ghosh", "Avijit", ""], ["Shanbhag", "Aalok", ""]]}, {"id": "2106.07112", "submitter": "Clarice Wang", "authors": "Clarice Wang, Kathryn Wang, Andrew Bian, Rashidul Islam, Kamrun Naher\n  Keya, James Foulds, Shimei Pan", "title": "User Acceptance of Gender Stereotypes in Automated Career\n  Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Currently, there is a surge of interest in fair Artificial Intelligence (AI)\nand Machine Learning (ML) research which aims to mitigate discriminatory bias\nin AI algorithms, e.g. along lines of gender, age, and race. While most\nresearch in this domain focuses on developing fair AI algorithms, in this work,\nwe show that a fair AI algorithm on its own may be insufficient to achieve its\nintended results in the real world. Using career recommendation as a case\nstudy, we build a fair AI career recommender by employing gender debiasing\nmachine learning techniques. Our offline evaluation showed that the debiased\nrecommender makes fairer career recommendations without sacrificing its\naccuracy. Nevertheless, an online user study of more than 200 college students\nrevealed that participants on average prefer the original biased system over\nthe debiased system. Specifically, we found that perceived gender disparity is\na determining factor for the acceptance of a recommendation. In other words,\nour results demonstrate we cannot fully address the gender bias issue in AI\nrecommendations without addressing the gender bias in humans.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 23:27:45 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 14:16:22 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Wang", "Clarice", ""], ["Wang", "Kathryn", ""], ["Bian", "Andrew", ""], ["Islam", "Rashidul", ""], ["Keya", "Kamrun Naher", ""], ["Foulds", "James", ""], ["Pan", "Shimei", ""]]}, {"id": "2106.07226", "submitter": "Federica Lago", "authors": "Federica Lago, Cecilia Pasquini, Rainer B\\\"ohme, H\\'el\\`ene Dumont,\n  Val\\'erie Goffaux and Giulia Boato", "title": "More Real than Real: A Study on Human Visual Perception of Synthetic\n  Faces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep fakes became extremely popular in the last years, also thanks to their\nincreasing realism. Therefore, there is the need to measures human's ability to\ndistinguish between real and synthetic face images when confronted with\ncutting-edge creation technologies. We describe the design and results of a\nperceptual experiment we have conducted, where a wide and diverse group of\nvolunteers has been exposed to synthetic face images produced by\nstate-of-the-art Generative Adversarial Networks (namely, PG-GAN, StyleGAN,\nStyleGAN2). The experiment outcomes reveal how strongly we should call into\nquestion our human ability to discriminate real faces from synthetic ones\ngenerated through modern AI.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 08:27:25 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Lago", "Federica", ""], ["Pasquini", "Cecilia", ""], ["B\u00f6hme", "Rainer", ""], ["Dumont", "H\u00e9l\u00e8ne", ""], ["Goffaux", "Val\u00e9rie", ""], ["Boato", "Giulia", ""]]}, {"id": "2106.07307", "submitter": "Shahid Alam", "authors": "Shahid Alam, Juvariya Khan", "title": "A Recipe for Social Media Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ubiquitous nature of smartphones has significantly increased the use of\nsocial media platforms, such as Facebook, Twitter, TikTok, and LinkedIn, etc.,\namong the public, government, and businesses. Facebook generated ~70 billion\nUSD in 2019 in advertisement revenues alone, a ~27% increase from the previous\nyear. Social media has also played a strong role in outbreaks of social\nprotests responsible for political changes in different countries. As we can\nsee from the above examples, social media plays a big role in business\nintelligence and international politics. In this paper, we present and discuss\na high-level functional intelligence model (recipe) of Social Media Analysis\n(SMA). This model synthesizes the input data and uses operational intelligence\nto provide actionable recommendations. In addition, it also matches the\nsynthesized function of the experiences and learning gained from the\nenvironment. The SMA model presented is independent of the application domain,\nand can be applied to different domains, such as Education, Healthcare and\nGovernment, etc. Finally, we also present some of the challenges faced by SMA\nand how the SMA model presented in this paper solves them.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 11:27:33 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Alam", "Shahid", ""], ["Khan", "Juvariya", ""]]}, {"id": "2106.07432", "submitter": "Inga Ivanova", "authors": "Inga A. Ivanova", "title": "Information exchange, meaning and redundancy generation in anticipatory\n  systems: self-organization of expectations -- the case of Covid-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When studying the evolution of complex systems one refers to model\nrepresentations comprising various descriptive parameters. There is hardly\nresearch where system evolution is described on the base of information flows\nin the system. The paper focuses on the link between the dynamics of\ninformation and system evolution. Information, exchanged between different\nsystem's parts, before being processed is first provided with meaning by the\nsystem. Meanings are generated from the perspective of hindsight, i.e. against\nthe arrow of time. The same information can be differently interpreted by\ndifferent system's parts (i,e,provided with different meanings) so that the\nnumber of options for possible system development is proliferated. Some options\neventually turn into observable system states. So that system evolutionary\ndynamics can be considered as due to information processing within the system.\nThis process is considered here in a model representation. The model under\nstudy is Triple Helix (TH) model, which was earlier used to describe\ninteractions between university, industry and government to foster innovations.\nIn TH model the system is comprised of three interacting parts where each part\nprocess information ina different way. The model is not limited to the sphere\nof innovation and can be used in a broader perspective. Here TH is\nconceptualized in the framework of three compertment model used to describe\ninfectious disease. The paper demonstrates how the dynamics of information and\nmeaning can be incorporated in the description of Covid-19 infectious\npropagation. The results show correspondence of model predictions with\nobservable infection dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 05:01:38 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Ivanova", "Inga A.", ""]]}, {"id": "2106.07483", "submitter": "Kiana Alikhademi", "authors": "Kiana Alikhademi, Brianna Richardson, Emma Drobina, and Juan E.\n  Gilbert", "title": "Can Explainable AI Explain Unfairness? A Framework for Evaluating\n  Explainable AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Many ML models are opaque to humans, producing decisions too complex for\nhumans to easily understand. In response, explainable artificial intelligence\n(XAI) tools that analyze the inner workings of a model have been created.\nDespite these tools' strength in translating model behavior, critiques have\nraised concerns about the impact of XAI tools as a tool for `fairwashing` by\nmisleading users into trusting biased or incorrect models. In this paper, we\ncreated a framework for evaluating explainable AI tools with respect to their\ncapabilities for detecting and addressing issues of bias and fairness as well\nas their capacity to communicate these results to their users clearly. We found\nthat despite their capabilities in simplifying and explaining model behavior,\nmany prominent XAI tools lack features that could be critical in detecting\nbias. Developers can use our framework to suggest modifications needed in their\ntoolkits to reduce issues likes fairwashing.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 15:14:03 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Alikhademi", "Kiana", ""], ["Richardson", "Brianna", ""], ["Drobina", "Emma", ""], ["Gilbert", "Juan E.", ""]]}, {"id": "2106.07504", "submitter": "Ulrich A\\\"ivodji", "authors": "Ulrich A\\\"ivodji, Hiromi Arai, S\\'ebastien Gambs, Satoshi Hara", "title": "Characterizing the risk of fairwashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairwashing refers to the risk that an unfair black-box model can be\nexplained by a fairer model through post-hoc explanations' manipulation.\nHowever, to realize this, the post-hoc explanation model must produce different\npredictions than the original black-box on some inputs, leading to a decrease\nin the fidelity imposed by the difference in unfairness. In this paper, our\nmain objective is to characterize the risk of fairwashing attacks, in\nparticular by investigating the fidelity-unfairness trade-off. First, we\ndemonstrate through an in-depth empirical study on black-box models trained on\nseveral real-world datasets and for several statistical notions of fairness\nthat it is possible to build high-fidelity explanation models with low\nunfairness. For instance, we find that fairwashed explanation models can\nexhibit up to $99.20\\%$ fidelity to the black-box models they explain while\nbeing $50\\%$ less unfair. These results suggest that fidelity alone should not\nbe used as a proxy for the quality of black-box explanations. Second, we show\nthat fairwashed explanation models can generalize beyond the suing group\n(\\emph{i.e.}, data points that are being explained), which will only worsen as\nmore stable fairness methods get developed. Finally, we demonstrate that\nfairwashing attacks can transfer across black-box models, meaning that other\nblack-box models can perform fairwashing without explicitly using their\npredictions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 15:33:17 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["A\u00efvodji", "Ulrich", ""], ["Arai", "Hiromi", ""], ["Gambs", "S\u00e9bastien", ""], ["Hara", "Satoshi", ""]]}, {"id": "2106.07553", "submitter": "Sara Kingsley", "authors": "Sara Kingsley", "title": "A Cognitive Science perspective for learning how to design meaningful\n  user experiences and human-centered technology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper reviews literature in cognitive science, human-computer\ninteraction (HCI) and natural-language processing (NLP) to consider how\nanalogical reasoning (AR) could help inform the design of communication and\nlearning technologies, as well as online communities and digital platforms.\nFirst, analogical reasoning (AR) is defined, and use-cases of AR in the\ncomputing sciences are presented. The concept of schema is introduced, along\nwith use-cases in computing. Finally, recommendations are offered for future\nwork on using analogical reasoning and schema methods in the computing\nsciences.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 15:00:50 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Kingsley", "Sara", ""]]}, {"id": "2106.07677", "submitter": "Aviva Prins", "authors": "Christine Herlihy, Aviva Prins, Aravind Srinivasan, and John Dickerson", "title": "Planning to Fairly Allocate: Probabilistic Fairness in the Restless\n  Bandit Setting", "comments": "27 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restless and collapsing bandits are commonly used to model constrained\nresource allocation in settings featuring arms with action-dependent transition\nprobabilities, such as allocating health interventions among patients [Whittle,\n1988; Mate et al., 2020]. However, state-of-the-art Whittle-index-based\napproaches to this planning problem either do not consider fairness among arms,\nor incentivize fairness without guaranteeing it [Mate et al., 2021].\nAdditionally, their optimality guarantees only apply when arms are indexable\nand threshold-optimal. We demonstrate that the incorporation of hard fairness\nconstraints necessitates the coupling of arms, which undermines the\ntractability, and by extension, indexability of the problem. We then introduce\nProbFair, a probabilistically fair stationary policy that maximizes total\nexpected reward and satisfies the budget constraint, while ensuring a strictly\npositive lower bound on the probability of being pulled at each timestep. We\nevaluate our algorithm on a real-world application, where interventions support\ncontinuous positive airway pressure (CPAP) therapy adherence among obstructive\nsleep apnea (OSA) patients, as well as simulations on a broader class of\nsynthetic transition matrices.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 18:01:08 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Herlihy", "Christine", ""], ["Prins", "Aviva", ""], ["Srinivasan", "Aravind", ""], ["Dickerson", "John", ""]]}, {"id": "2106.07754", "submitter": "Daniele Regoli", "authors": "Riccardo Crupi, Alessandro Castelnovo, Daniele Regoli, Beatriz San\n  Miguel Gonzalez", "title": "Counterfactual Explanations as Interventions in Latent Space", "comments": "34 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explainable Artificial Intelligence (XAI) is a set of techniques that allows\nthe understanding of both technical and non-technical aspects of Artificial\nIntelligence (AI) systems. XAI is crucial to help satisfying the increasingly\nimportant demand of \\emph{trustworthy} Artificial Intelligence, characterized\nby fundamental characteristics such as respect of human autonomy, prevention of\nharm, transparency, accountability, etc. Within XAI techniques, counterfactual\nexplanations aim to provide to end users a set of features (and their\ncorresponding values) that need to be changed in order to achieve a desired\noutcome. Current approaches rarely take into account the feasibility of actions\nneeded to achieve the proposed explanations, and in particular they fall short\nof considering the causal impact of such actions. In this paper, we present\nCounterfactual Explanations as Interventions in Latent Space (CEILS), a\nmethodology to generate counterfactual explanations capturing by design the\nunderlying causal relations from the data, and at the same time to provide\nfeasible recommendations to reach the proposed profile. Moreover, our\nmethodology has the advantage that it can be set on top of existing\ncounterfactuals generator algorithms, thus minimising the complexity of\nimposing additional causal constrains. We demonstrate the effectiveness of our\napproach with a set of different experiments using synthetic and real datasets\n(including a proprietary dataset of the financial domain).\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 20:48:48 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Crupi", "Riccardo", ""], ["Castelnovo", "Alessandro", ""], ["Regoli", "Daniele", ""], ["Gonzalez", "Beatriz San Miguel", ""]]}, {"id": "2106.07909", "submitter": "Gerg\\H{o} Pint\\'er", "authors": "Gerg\\H{o} Pint\\'er and Imre Felde", "title": "Evaluating the Effect of the Financial Status to the Mobility Customs", "comments": null, "journal-ref": null, "doi": "10.3390/ijgi10050328", "report-no": null, "categories": "cs.SI cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we explore the relationship between cellular phone data and\nhousing prices in Budapest, Hungary. We determine mobility indicators from one\nmonths of Call Detail Records (CDR) data, while the property price data are\nused to characterize the socioeconomic status at the Capital of Hungary. First,\nwe validated the proposed methodology by comparing the Home and Work locations\nestimation and the commuting patterns derived from the cellular network dataset\nwith reports of the national mini census. We investigated the statistical\nrelationships between mobile phone indicators, such as Radius of Gyration, the\ndistance between Home and Work locations or the Entropy of visited cells, and\nmeasures of economic status based on housing prices. Our findings show that the\nmobility correlates significantly with the socioeconomic status. We performed\nPrincipal Component Analysis (PCA) on combined vectors of mobility indicators\nin order to characterize the dependence of mobility habits on socioeconomic\nstatus. The results of the PCA investigation showed remarkable correlation of\nhousing prices and mobility customs.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 06:47:05 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Pint\u00e9r", "Gerg\u0151", ""], ["Felde", "Imre", ""]]}, {"id": "2106.08024", "submitter": "Max Maass", "authors": "Max Maass and Marc-Pascal Clement and Matthias Hollick", "title": "Snail Mail Beats Email Any Day: On Effective Operator Security\n  Notifications in the Internet", "comments": "Accepted at The 16th International Conference on Availability,\n  Reliability and Security (ARES '21). Code and data:\n  https://doi.org/10.5281/zenodo.4817463", "journal-ref": null, "doi": "10.1145/3465481.3465743", "report-no": null, "categories": "cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of large-scale internet scanning, misconfigured websites are a\nfrequent cause of data leaks and security incidents. Previous research has\ninvestigated sending automated email notifications to operators of insecure or\ncompromised websites, but has often met with limited success due to challenges\nin address data quality, spam filtering, and operator distrust and disinterest.\nWhile several studies have investigated the design and phrasing of notification\nemails in a bid to increase their effectiveness, the use of other contact\nchannels has remained almost completely unexplored due to the required effort\nand cost. In this paper, we investigate two methods to increase notification\nsuccess: the use of letters as an alternative delivery medium, and the\ndescription of attack scenarios to incentivize remediation. We evaluate these\nfactors as part of a notification campaign utilizing manually-collected address\ninformation from 1359 German website operators and focusing on unintentional\ninformation leaks from web servers. We find that manually collected addresses\nlead to large increases in delivery rates compared to previous work, and\nletters were markedly more effective than emails, increasing remediation rates\nby up to 25 percentage points. Counterintuitively, providing detailed\ndescriptions of possible attacks can actually *decrease* remediation rates,\nhighlighting the need for more research into how notifications are perceived by\nrecipients.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 10:17:59 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Maass", "Max", ""], ["Clement", "Marc-Pascal", ""], ["Hollick", "Matthias", ""]]}, {"id": "2106.08049", "submitter": "Pawel Drozdowski", "authors": "Pawel Drozdowski, Christian Rathgeb, Christoph Busch", "title": "Demographic Fairness in Face Identification: The Watchlist Imbalance\n  Effect", "comments": "dispute over contents of section 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, different researchers have found that the gallery composition of a\nface database can induce performance differentials to facial identification\nsystems in which a probe image is compared against up to all stored reference\nimages to reach a biometric decision. This negative effect is referred to as\n\"watchlist imbalance effect\". In this work, we present a method to\ntheoretically estimate said effect for a biometric identification system given\nits verification performance across demographic groups and the composition of\nthe used gallery. Further, we report results for identification experiments on\ndifferently composed demographic subsets, i.e. females and males, of the public\nacademic MORPH database using the open-source ArcFace face recognition system.\nIt is shown that the database composition has a huge impact on performance\ndifferentials in biometric identification systems, even if performance\ndifferentials are less pronounced in the verification scenario. This study\nrepresents the first detailed analysis of the watchlist imbalance effect which\nis expected to be of high interest for future research in the field of facial\nrecognition.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 11:09:06 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 07:45:48 GMT"}, {"version": "v3", "created": "Wed, 30 Jun 2021 07:20:20 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Drozdowski", "Pawel", ""], ["Rathgeb", "Christian", ""], ["Busch", "Christoph", ""]]}, {"id": "2106.08072", "submitter": "Matthieu Latapy", "authors": "Jules Azad Emery and Matthieu Latapy", "title": "Full Bitcoin Blockchain Data Made Easy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CR cs.CY cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the fact that it is publicly available, collecting and processing the\nfull bitcoin blockchain data is not trivial. Its mere size, history, and other\nfeatures indeed raise quite specific challenges, that we address in this paper.\nThe strengths of our approach are the following: it relies on very basic and\nstandard tools, which makes the procedure reliable and easily reproducible; it\nis a purely lossless procedure ensuring that we catch and preserve all existing\ndata; it provides additional indexing that makes it easy to further process the\nwhole data and select appropriate subsets of it. We present our procedure in\ndetails and illustrate its added value on large-scale use cases, like address\nclustering. We provide an implementation online, as well as the obtained\ndataset.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 12:02:49 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Emery", "Jules Azad", ""], ["Latapy", "Matthieu", ""]]}, {"id": "2106.08177", "submitter": "Shaykh Siddique", "authors": "Shaykh Siddique, Monica Yasmin, Tasnova Bintee Taher, Mushfiqul Alam", "title": "The Reliability and Acceptance of Biometric System in Bangladesh: Users\n  Perspective", "comments": "7 pages, 4 figures, Published with International Journal of Computer\n  Trends and Technology (IJCTT)", "journal-ref": "International Journal of Computer Trends and Technology, 69(6),\n  15-21, June 2021", "doi": "10.14445/22312803/IJCTT-V69I6P103", "report-no": null, "categories": "cs.CR cs.CY cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Biometric systems are the latest technologies of unique identification.\nPeople all over the world prefer to use this unique identification technology\nfor their authentication security. The goal of this research is to evaluate the\nbiometric systems based on system reliability and user satisfaction. As\ntechnology fully depends on personal data, so in terms of the quality and\nreliability of biometric systems, user satisfaction is a principal factor. To\nwalk with the digital era, it is extremely important to assess users' concerns\nabout data security as the systems are conducted the authentication by\nanalyzing users' personal data. The study shows that users are satisfied by\nusing biometric systems rather than other security systems. Besides, hardware\nfailure is a big issue faced by biometric systems users. Finally, a matrix is\ngenerated to compare the performance of popular biometric systems from the\nusers' opinions. As system reliability and user satisfaction are the focused\nissue of this research, biometric service providers can use these phenomena to\nfind what aspect of improvement they need for their services. Also, this study\ncan be a great visualizer for Bangladeshi users, so that they can easily\nrealize which biometric system they have to choose.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 08:46:00 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Siddique", "Shaykh", ""], ["Yasmin", "Monica", ""], ["Taher", "Tasnova Bintee", ""], ["Alam", "Mushfiqul", ""]]}, {"id": "2106.08204", "submitter": "Rogier van de Wetering", "authors": "Rogier van de Wetering", "title": "Achieving digital-driven patient agility in the era of big data", "comments": "13 pages, 1 figure, The 20th IFIP Conference e-Business, e-Services,\n  and e-Society I3E2021. arXiv admin note: text overlap with arXiv:2105.09013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There is still a limited understanding of the necessary skill, talent, and\nexpertise to manage digital technologies as a crucial enabler of the hospitals\nability to adequately sense and respond to patient needs and wishes, i.e.,\npatient agility. Therefore, this investigates how hospital departments can\nleverage a digital dy-namic capability to enable the departments patient\nagility. This study embraces the dynamic capabilities theory, develops a\nresearch model, and tests it accordingly using data from 90 clinical hospital\ndepartments from the Netherlands through an online survey. The model's\nhypothesized relationships are tested using structural equation modeling (SEM).\nThe outcomes demonstrate the significance of digital dynamic capability in\ndeveloping patient sensing and responding capabili-ties that, in turn,\npositively influence patient service performance. Outcomes are very relevant\nfor the hospital practice now, as hospitals worldwide need to trans-form\nhealthcare delivery processes using digital technologies and increase clinical\nproductivity.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 15:08:26 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["van de Wetering", "Rogier", ""]]}, {"id": "2106.08258", "submitter": "Iain Barclay", "authors": "Iain Barclay, Will Abramson", "title": "Identifying Roles, Requirements and Responsibilities in Trustworthy AI\n  Systems", "comments": "Pre-print of paper submitted to Workshop on Reviewable and Auditable\n  Pervasive Systems (WRAPS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Artificial Intelligence (AI) systems are being deployed around the globe in\ncritical fields such as healthcare and education. In some cases, expert\npractitioners in these domains are being tasked with introducing or using such\nsystems, but have little or no insight into what data these complex systems are\nbased on, or how they are put together. In this paper, we consider an AI system\nfrom the domain practitioner's perspective and identify key roles that are\ninvolved in system deployment. We consider the differing requirements and\nresponsibilities of each role, and identify a tension between transparency and\nprivacy that needs to be addressed so that domain practitioners are able to\nintelligently assess whether a particular AI system is appropriate for use in\ntheir domain.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 16:05:10 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Barclay", "Iain", ""], ["Abramson", "Will", ""]]}, {"id": "2106.08259", "submitter": "Falaah Arif Khan", "authors": "Falaah Arif Khan, Eleni Manis, Julia Stoyanovich", "title": "Fairness as Equality of Opportunity: Normative Guidance from Political\n  Philosophy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recent interest in codifying fairness in Automated Decision Systems (ADS) has\nresulted in a wide range of formulations of what it means for an algorithmic\nsystem to be fair. Most of these propositions are inspired by, but inadequately\ngrounded in, political philosophy scholarship. This paper aims to correct that\ndeficit. We introduce a taxonomy of fairness ideals using doctrines of Equality\nof Opportunity (EOP) from political philosophy, clarifying their conceptions in\nphilosophy and the proposed codification in fair machine learning. We arrange\nthese fairness ideals onto an EOP spectrum, which serves as a useful frame to\nguide the design of a fair ADS in a given context.\n  We use our fairness-as-EOP framework to re-interpret the impossibility\nresults from a philosophical perspective, as the in-compatibility between\ndifferent value systems, and demonstrate the utility of the framework with\nseveral real-world and hypothetical examples. Through our EOP-framework we hope\nto answer what it means for an ADS to be fair from a moral and political\nphilosophy standpoint, and to pave the way for similar scholarship from ethics\nand legal experts.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 16:07:58 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Khan", "Falaah Arif", ""], ["Manis", "Eleni", ""], ["Stoyanovich", "Julia", ""]]}, {"id": "2106.08298", "submitter": "Jason R.C. Nurse Dr", "authors": "Suraj Sharma and Joseph Brennan and Jason R. C. Nurse", "title": "StockBabble: A Conversational Financial Agent to support Stock Market\n  Investors", "comments": "CUI 2021 - 3rd Conference on Conversational User Interfaces", "journal-ref": null, "doi": "10.1145/3469595.3469620", "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce StockBabble, a conversational agent designed to support\nunderstanding and engagement with the stock market. StockBabble's value and\nnovelty is in its ability to empower retail investors -- many of which may be\nnew to investing -- and supplement their informational needs using a\nuser-friendly agent. Users have the ability to query information on companies\nto retrieve a general and financial overview of a stock, including accessing\nthe latest news and trading recommendations. They can also request charts which\ncontain live prices and technical investment indicators, and add shares to a\npersonal portfolio to allow performance monitoring over time. To evaluate our\nagent's potential, we conducted a user study with 15 participants. In total,\n73% (11/15) of respondents said that they felt more confident in investing\nafter using StockBabble, and all 15 would consider recommending it to others.\nThese results are encouraging and suggest a wider appeal for such agents.\nMoreover, we believe this research can help to inform the design and\ndevelopment of future intelligent, financial personal assistants.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 17:19:30 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Sharma", "Suraj", ""], ["Brennan", "Joseph", ""], ["Nurse", "Jason R. C.", ""]]}, {"id": "2106.08423", "submitter": "Karishma Sharma", "authors": "Karishma Sharma, Yizhou Zhang, Yan Liu", "title": "COVID-19 Vaccines: Characterizing Misinformation Campaigns and Vaccine\n  Hesitancy on Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vaccine hesitancy and misinformation on social media has increased concerns\nabout COVID-19 vaccine uptake required to achieve herd immunity and overcome\nthe pandemic. However anti-science and political misinformation and\nconspiracies have been rampant throughout the pandemic. For COVID-19 vaccines,\nwe investigate misinformation and conspiracy campaigns and their characteristic\nbehaviours. We identify whether coordinated efforts are used to promote\nmisinformation in vaccine related discussions, and find accounts coordinately\npromoting a `Great Reset' conspiracy group promoting vaccine related\nmisinformation and strong anti-vaccine and anti-social messages such as boycott\nvaccine passports, no lock-downs and masks. We characterize other\nmisinformation communities from the information diffusion structure, and study\nthe large anti-vaccine misinformation community and smaller anti-vaccine\ncommunities, including a far-right anti-vaccine conspiracy group. In comparison\nwith the mainstream and health news, left-leaning group, which are more\npro-vaccine, the right-leaning group is influenced more by the anti-vaccine and\nfar-right misinformation/conspiracy communities. The misinformation communities\nare more vocal either specific to the vaccine discussion or political\ndiscussion, and we find other differences in the characteristic behaviours of\ndifferent communities. Lastly, we investigate misinformation narratives and\ntactics of information distortion that can increase vaccine hesitancy, using\ntopic modeling and comparison with reported vaccine side-effects (VAERS)\nfinding rarer side-effects are more frequently discussed on social media.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 20:32:10 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Sharma", "Karishma", ""], ["Zhang", "Yizhou", ""], ["Liu", "Yan", ""]]}, {"id": "2106.08680", "submitter": "Krithika Ramesh", "authors": "Gauri Gupta, Krithika Ramesh and Sanjay Singh", "title": "Evaluating Gender Bias in Hindi-English Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With language models being deployed increasingly in the real world, it is\nessential to address the issue of the fairness of their outputs. The word\nembedding representations of these language models often implicitly draw\nunwanted associations that form a social bias within the model. The nature of\ngendered languages like Hindi, poses an additional problem to the\nquantification and mitigation of bias, owing to the change in the form of the\nwords in the sentence, based on the gender of the subject. Additionally, there\nis sparse work done in the realm of measuring and debiasing systems for Indic\nlanguages. In our work, we attempt to evaluate and quantify the gender bias\nwithin a Hindi-English machine translation system. We implement a modified\nversion of the existing TGBI metric based on the grammatical considerations for\nHindi. We also compare and contrast the resulting bias measurements across\nmultiple metrics for pre-trained embeddings and the ones learned by our machine\ntranslation model.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 10:35:51 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Gupta", "Gauri", ""], ["Ramesh", "Krithika", ""], ["Singh", "Sanjay", ""]]}, {"id": "2106.08684", "submitter": "Niccol\\`o Di Marco", "authors": "Niccol\\`o Di Marco, Matteo Cinelli, Walter Quattrociocchi", "title": "Infodemics on Youtube: Reliability of Content and Echo Chambers on\n  COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media radically changed how information is consumed and reported.\nMoreover, social networks elicited a disintermediated access to an\nunprecedented amount of content. The world health organization (WHO) coined the\nterm infodemics to identify the information overabundance during an epidemic.\nIndeed, the spread of inaccurate and misleading information may alter behaviors\nand complicate crisis management and health responses. This paper addresses\ninformation diffusion during the COVID-19 pandemic period with a massive data\nanalysis on YouTube. First, we analyze more than 2M users' engagement in 13000\nvideos released by 68 different YouTube channels, with different political bias\nand fact-checking indexes. We then investigate the relationship between each\nuser's political preference and her/his consumption of questionable/reliable\ninformation. Our results, quantified using information theory measures, provide\nevidence for the existence of echo chambers across two dimensions represented\nby the political bias and by the trustworthiness of information channels.\nFinally, we observe that the echo chamber structure cannot be reproduced after\nproperly randomizing the users' interaction patterns.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 10:44:29 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Di Marco", "Niccol\u00f2", ""], ["Cinelli", "Matteo", ""], ["Quattrociocchi", "Walter", ""]]}, {"id": "2106.08710", "submitter": "Jacky Cao", "authors": "Jacky Cao, Kit-Yung Lam, Lik-Hang Lee, Xiaoli Liu, Pan Hui, Xiang Su", "title": "Mobile Augmented Reality: User Interfaces, Frameworks, and Intelligence", "comments": "This work is currently under review in an international journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile Augmented Reality (MAR) integrates computer-generated virtual objects\nwith physical environments for mobile devices. MAR systems enable users to\ninteract with MAR devices, such as smartphones and head-worn wearables, and\nperforms seamless transitions from the physical world to a mixed world with\ndigital entities. These MAR systems support user experiences by using MAR\ndevices to provide universal accessibility to digital contents. Over the past\n20 years, a number of MAR systems have been developed, however, the studies and\ndesign of MAR frameworks have not yet been systematically reviewed from the\nperspective of user-centric design. This article presents the first effort of\nsurveying existing MAR frameworks (count: 37) and further discusses the latest\nstudies on MAR through a top-down approach: 1) MAR applications; 2) MAR\nvisualisation techniques adaptive to user mobility and contexts; 3) systematic\nevaluation of MAR frameworks including supported platforms and corresponding\nfeatures such as tracking, feature extraction plus sensing capabilities; and 4)\nunderlying machine learning approaches supporting intelligent operations within\nMAR systems. Finally, we summarise the development of emerging research fields,\ncurrent state-of-the-art, and discuss the important open challenges and\npossible theoretical and technical directions. This survey aims to benefit both\nresearchers and MAR system developers alike.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 11:26:37 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Cao", "Jacky", ""], ["Lam", "Kit-Yung", ""], ["Lee", "Lik-Hang", ""], ["Liu", "Xiaoli", ""], ["Hui", "Pan", ""], ["Su", "Xiang", ""]]}, {"id": "2106.08729", "submitter": "Joberto Martins Prof. Dr.", "authors": "David S. Barreto and Rafael F. Reale and Joberto S. B. Martins", "title": "Modeling and Accomplishing the BEREC Network Neutrality Policy", "comments": "17 pages, 8 figures, IJNM preprint", "journal-ref": "International Journal of Network Management, vol na, p e2148, 2020", "doi": "10.5281/zenodo.4554025", "report-no": null, "categories": "cs.NI cs.CY cs.PF", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Network neutrality (NN) is a principle of equal treatment of data in network\ninfrastructures with fairness and universality being the primary outcomes of\nthe NN management practice. For networks, the accomplishment of NN management\npractice is essential to deal with heterogeneous user requirements and the\never-increasing data traffic. Current tools and methods address the NN problem\nby detecting network neutrality violations and detecting traffic\ndifferentiation. This paper proposes the NN-PCM (Network Neutrality Policy\nConformance Module) that deploys the BEREC network neutrality policy using a\nbandwidth allocation model (BAM). The NN-PCM new approach allocates bandwidth\nto network users and accomplishes the BEREC NN policy concomitantly. Network\nneutrality is achieved by grouping users with similar traffic requirements in\nclasses and leveraging the bandwidth allocation model's characteristics. The\nconceptual analysis and simulation results indicate that NN-PCM allocates\nbandwidth to users and accomplishes BEREC network neutrality conformance by\ndesign with transparent, non-discriminatory, exceptional, and proportional\nmanagement practices.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 12:11:11 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Barreto", "David S.", ""], ["Reale", "Rafael F.", ""], ["Martins", "Joberto S. B.", ""]]}, {"id": "2106.08737", "submitter": "James Scheibner", "authors": "James Scheibner, Marcello Ienca, Joanna Sleigh and Effy Vayena", "title": "Benefits, Challenges and Contributors to Success for National eHealth\n  Systems Implementation: A Scoping Review", "comments": "28 pages, 4 figures, 2 tables", "journal-ref": "Journal of the American Medical Informatics Association ocab096\n  (2021)", "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our scoping review aims to assess what legal, ethical, and socio-technical\nfactors contribute or inhibit the success of national eHealth system\nimplementations. In addition, our review seeks to describe the characteristics\nand benefits of eHealth systems. We conducted a scoping review of literature\npublished in English between January 2000 and 2020 using a keyword search on\nfive databases; PubMed, Scopus, Web of Science, IEEEXplore, and ProQuest. After\nremoval of duplicates, abstract screening and full-text filtering, 86 articles\nwere included from 8276 search results. We identified 17 stakeholder groups, 6\neHealth Systems areas, and 15 types of legal regimes and standards. In-depth\ntextual analysis revealed challenges mainly in implementation, followed by\nethico-legal and data related aspects. Key factors influencing success include\npromoting trust of the system, ensuring wider acceptance amongst users,\nreconciling the system with legal requirements and ensuring an adaptable\ntechnical platform. Results revealed support for decentralised implementations\nbecause they carry less implementation and engagement challenges than\ncentralised ones. Simultaneously, due to decentralised systems interoperability\nissues, federated implementations (with a set of national standards) might be\npreferable. This study identifies the primary socio-technical, legal and\nethical factors that challenge and contribute to the success of eHealth system\nimplementations. This study also describes the complexities and characteristics\nof existing eHealth implementation programs, and surmises suggested guidance\nfor resolving the identified challenges.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 12:25:54 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Scheibner", "James", ""], ["Ienca", "Marcello", ""], ["Sleigh", "Joanna", ""], ["Vayena", "Effy", ""]]}, {"id": "2106.08812", "submitter": "Han Zhao", "authors": "Han Zhao", "title": "Costs and Benefits of Wasserstein Fair Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world applications of machine learning tools in high-stakes domains are\noften regulated to be fair, in the sense that the predicted target should\nsatisfy some quantitative notion of parity with respect to a protected\nattribute. However, the exact tradeoff between fairness and accuracy with a\nreal-valued target is not clear. In this paper, we characterize the inherent\ntradeoff between statistical parity and accuracy in the regression setting by\nproviding a lower bound on the error of any fair regressor. Our lower bound is\nsharp, algorithm-independent, and admits a simple interpretation: when the\nmoments of the target differ between groups, any fair algorithm has to make a\nlarge error on at least one of the groups. We further extend this result to\ngive a lower bound on the joint error of any (approximately) fair algorithm,\nusing the Wasserstein distance to measure the quality of the approximation. On\nthe upside, we establish the first connection between individual fairness,\naccuracy parity, and the Wasserstein distance by showing that if a regressor is\nindividually fair, it also approximately verifies the accuracy parity, where\nthe gap is given by the Wasserstein distance between the two groups. Inspired\nby our theoretical results, we develop a practical algorithm for fair\nregression through the lens of representation learning, and conduct experiments\non a real-world dataset to corroborate our findings.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 14:24:44 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Zhao", "Han", ""]]}, {"id": "2106.09208", "submitter": "Mingyi Liu", "authors": "Zhongjie Wang and Mingyi Liu and Zhiying Tu and Xiaofei Xu", "title": "External Service Sensing (ESS): Research Framework, Challenges and\n  Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The flourish of web-based services gave birth to the research area\n\\textit{services computing}, a rapidly-expanding academic community since\nnearly 20 years ago. Consensus has been reached on a set of representative\nresearch problems in services computing, such as service selection, service\ncomposition, service recommendation, and service quality prediction. An obvious\nfact is that most services keep constant changes to timely adapt to changes of\nexternal business/technical environment and changes of internal development\nstrategies. However, traditional services computing research does not consider\nsuch changes sufficiently. Many works regard services as \\textit{static}\nentities; this leads to the situation that some proposed models/algorithms do\nnot work in real world. Sensing various types of service changes is of great\nsignificance to the practicability and rationality of services computing\nresearch. In this paper, a new research problem \\textit{External Service\nSensing} (ESS) is defined to cope with various changes in services, and a\nresearch framework of ESS is presented to elaborate the scope and boundary of\nESS. This framework is composed of four orthogonal dimensions: sensing objects,\nsensing contents, sensing channels, and sensing techniques. Each concrete ESS\nproblem is defined by combining different values in these dimensions, and\nexisting research work related to service changes can be well adapted to this\nframework. Real-world case studies demonstrate the soundness of ESS and its\nframework. Finally, some challenges and opportunities in ESS research are\nlisted for researchers in the services computing community. To the best of our\nknowledge, this is the first time to systematically define service\nchange-related research as a standard services computing problem, and thus\nbroadening the research scope of services computing.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 02:12:11 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Wang", "Zhongjie", ""], ["Liu", "Mingyi", ""], ["Tu", "Zhiying", ""], ["Xu", "Xiaofei", ""]]}, {"id": "2106.09407", "submitter": "Konrad Kollnig", "authors": "Konrad Kollnig, Reuben Binns, Pierre Dewitte, Max Van Kleek, Ge Wang,\n  Daniel Omeiza, Helena Webb, Nigel Shadbolt", "title": "A Fait Accompli? An Empirical Study into the Absence of Consent to\n  Third-Party Tracking in Android Apps", "comments": "This paper will be presented at the 7th Symposium on Usable Privacy\n  and Security (SOUPS 2021), 8th-10th August 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Third-party tracking allows companies to collect users' behavioural data and\ntrack their activity across digital devices. This can put deep insights into\nusers' private lives into the hands of strangers, and often happens without\nusers' awareness or explicit consent. EU and UK data protection law, however,\nrequires consent, both 1) to access and store information on users' devices and\n2) to legitimate the processing of personal data as part of third-party\ntracking, as we analyse in this paper.\n  This paper further investigates whether and to what extent consent is\nimplemented in mobile apps. First, we analyse a representative sample of apps\nfrom the Google Play Store. We find that most apps engage in third-party\ntracking, but few obtained consent before doing so, indicating potentially\nwidespread violations of EU and UK privacy law. Second, we examine the most\ncommon third-party tracking libraries in detail. While most acknowledge that\nthey rely on app developers to obtain consent on their behalf, they typically\nfail to put in place robust measures to ensure this: disclosure of consent\nrequirements is limited; default consent implementations are lacking; and\ncompliance guidance is difficult to find, hard to read, and poorly maintained.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 11:44:49 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 07:00:40 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Kollnig", "Konrad", ""], ["Binns", "Reuben", ""], ["Dewitte", "Pierre", ""], ["Van Kleek", "Max", ""], ["Wang", "Ge", ""], ["Omeiza", "Daniel", ""], ["Webb", "Helena", ""], ["Shadbolt", "Nigel", ""]]}, {"id": "2106.09580", "submitter": "Kate Donahue", "authors": "Kate Donahue and Jon Kleinberg", "title": "Optimality and Stability in Federated Learning: A Game-theoretic\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CY cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning is a distributed learning paradigm where multiple agents,\neach only with access to local data, jointly learn a global model. There has\nrecently been an explosion of research aiming not only to improve the accuracy\nrates of federated learning, but also provide certain guarantees around social\ngood properties such as total error. One branch of this research has taken a\ngame-theoretic approach, and in particular, prior work has viewed federated\nlearning as a hedonic game, where error-minimizing players arrange themselves\ninto federating coalitions. This past work proves the existence of stable\ncoalition partitions, but leaves open a wide range of questions, including how\nfar from optimal these stable solutions are. In this work, we motivate and\ndefine a notion of optimality given by the average error rates among federating\nagents (players). First, we provide and prove the correctness of an efficient\nalgorithm to calculate an optimal (error minimizing) arrangement of players.\nNext, we analyze the relationship between the stability and optimality of an\narrangement. First, we show that for some regions of parameter space, all\nstable arrangements are optimal (Price of Anarchy equal to 1). However, we show\nthis is not true for all settings: there exist examples of stable arrangements\nwith higher cost than optimal (Price of Anarchy greater than 1). Finally, we\ngive the first constant-factor bound on the performance gap between stability\nand optimality, proving that the total error of the worst stable solution can\nbe no higher than 9 times the total error of an optimal solution (Price of\nAnarchy bound of 9).\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 15:03:51 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Donahue", "Kate", ""], ["Kleinberg", "Jon", ""]]}, {"id": "2106.09694", "submitter": "Naroa Coretti Sanchez", "authors": "Naroa Coretti S\\'anchez, I\\~nigo Martinez, Luis Alonso Pastor, Kent\n  Larson", "title": "Simulation study on the fleet performance of shared autonomous bicycles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rethinking cities is now more imperative than ever, as society faces global\nchallenges such as population growth and climate change. The design of cities\ncan not be abstracted from the design of its mobility system, and, therefore,\nefficient solutions must be found to transport people and goods throughout the\ncity in an ecological way. An autonomous bicycle-sharing system would combine\nthe most relevant benefits of vehicle sharing, electrification, autonomy, and\nmicro-mobility, increasing the efficiency and convenience of bicycle-sharing\nsystems and incentivizing more people to bike and enjoy their cities in an\nenvironmentally friendly way. Due to the uniqueness and radical novelty of\nintroducing autonomous driving technology into bicycle-sharing systems and the\ninherent complexity of these systems, there is a need to quantify the potential\nimpact of autonomy on fleet performance and user experience. This paper\npresents an ad-hoc agent-based simulator that provides an in-depth\nunderstanding of the fleet behavior of autonomous bicycle-sharing systems in\nrealistic scenarios, including a rebalancing system based on demand prediction.\nIn addition, this work describes the impact of different parameters on system\nefficiency and service quality and quantifies the extent to which an autonomous\nsystem would outperform current bicycle-sharing schemes. The obtained results\nshow that with a fleet size three and a half times smaller than a station-based\nsystem and eight times smaller than a dockless system, an autonomous system can\nprovide overall improved performance and user experience even with no\nrebalancing. These findings indicate that the remarkable efficiency of an\nautonomous bicycle-sharing system could compensate for the additional cost of\nautonomous bicycles.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:47:08 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 15:07:57 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["S\u00e1nchez", "Naroa Coretti", ""], ["Martinez", "I\u00f1igo", ""], ["Pastor", "Luis Alonso", ""], ["Larson", "Kent", ""]]}, {"id": "2106.09933", "submitter": "Awad Abdelhalim", "authors": "Awad Abdelhalim, Linda Bailey, Emily Dalphy, Kelli Raboy", "title": "Data Enforced: An Exploratory Impact Analysis of Automated Speed\n  Enforcement in the District of Columbia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2015, the District of Columbia framed a Vision Zero mission and action\nplan, with a target of achieving zero traffic fatalities by 2024. This study\nexamines the impacts of Automated Speed Enforcement (ASE) and its role in\nachieving the goals of Vision Zero. Independent datasets containing detailed\ninformation about traffic crashes, ASE camera locations, and citation records,\nand driving speeds across the District's streets were collected, combined, and\nanalyzed to identify patterns and trends in crashes, speed limit violations,\nand speeding behavior before and after the ASE camera installation. The results\nof this exploratory analysis confirm the safety benefits of ASE systems in\nWashington, D.C. The study also provides a blueprint for the different means of\nevaluating the short-term impact of ASE systems using different data sources\nwhich can aid practitioners in better evaluating existing systems and support\nthe decision-making process regarding future installations.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 06:18:08 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Abdelhalim", "Awad", ""], ["Bailey", "Linda", ""], ["Dalphy", "Emily", ""], ["Raboy", "Kelli", ""]]}, {"id": "2106.09937", "submitter": "Noble Mathews", "authors": "Noble Saji Mathews, Sridhar Chimalakonda", "title": "Detox Browser -- Towards Filtering Sensitive Content On the Web", "comments": "6 pages, 2 figures, CSCW", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The annual consumption of web-based resources is increasing at a very fast\nrate, mainly due to an increase in affordability and accessibility of the\ninternet. Many are relying on the web to get diverse perspectives, but at the\nsame time, it can expose them to content that is harmful to their mental\nwell-being. Catchy headlines and emotionally charged articles increase the\nnumber of readers which in turn increases ad revenue for websites. When a user\nconsumes a large quantity of negative content, it adversely impacts the user's\nhappiness and has a significant impact on his/her mood and state of mind. Many\nstudies carried out during the COVID-19 pandemic has shown that people across\nthe globe irrespective of their country of origin have experienced higher\nlevels of anxiety and depression. Web filters can help in constructing a\ndigital environment that is more suitable for people prone to depression,\nanxiety and stress. A significant amount of work has been done in the field of\nweb filtering, but there has been limited focus on helping Highly Sensitive\nPersons (HSP's) or those with stress disorders induced by trauma. Through this\npaper, we propose detox Browser, a simple tool that enables end-users to tune\nout of or control their exposure to topics that can affect their mental well\nbeing. The extension makes use of sentiment analysis and keywords to filter out\nflagged content from google search results and warns users if any blacklisted\ntopics are detected when navigating across websites\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 06:28:17 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Mathews", "Noble Saji", ""], ["Chimalakonda", "Sridhar", ""]]}, {"id": "2106.09981", "submitter": "Junda Wang", "authors": "Junda Wang, Xupin Zhang, Jiebo Luo", "title": "How COVID-19 Have Changed Crowdfunding: Evidence From GoFundMe", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the long-term effects of COVID-19 are yet to be determined, its\nimmediate impact on crowdfunding is nonetheless significant. This study takes a\ncomputational approach to more deeply comprehend this change. Using a unique\ndata set of all the campaigns published over the past two years on GoFundMe, we\nexplore the factors that have led to the successful funding of a crowdfunding\nproject. In particular, we study a corpus of crowdfunded projects, analyzing\ncover images and other variables commonly present on crowdfunding sites.\nFurthermore, we construct a classifier and a regression model to assess the\nsignificance of features based on XGBoost. In addition, we employ\ncounterfactual analysis to investigate the causality between features and the\nsuccess of crowdfunding. More importantly, sentiment analysis and the paired\nsample t-test are performed to examine the differences in crowdfunding\ncampaigns before and after the COVID-19 outbreak that started in March 2020.\nFirst, we note that there is significant racial disparity in crowdfunding\nsuccess. Second, we find that sad emotion expressed through the campaign's\ndescription became significant after the COVID-19 outbreak. Considering all\nthese factors, our findings shed light on the impact of COVID-19 on\ncrowdfunding campaigns.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 08:03:58 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wang", "Junda", ""], ["Zhang", "Xupin", ""], ["Luo", "Jiebo", ""]]}, {"id": "2106.10104", "submitter": "Fareed Sheriff", "authors": "Fareed Sheriff", "title": "ELMOPP: An Application of Graph Theory and Machine Learning to Traffic\n  Light Coordination", "comments": "13 pages, 3 figures, published in Applied Computing and Informatics\n  (2021)", "journal-ref": null, "doi": "10.1108/ACI-07-2020-0035", "report-no": null, "categories": "cs.CE cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traffic light management is a broad subject with various papers published\nthat put forth algorithms to efficiently manage traffic using traffic lights.\nTwo such algorithms are the OAF (oldest arrival first) and ITLC (intelligent\ntraffic light controller) algorithms. However, many traffic light algorithms do\nnot consider future traffic flow and therefore cannot mitigate traffic in such\na way as to reduce future traffic in the present. This paper presents the Edge\nLoad Management and Optimization through Pseudoflow Prediction (ELMOPP)\nalgorithm, which aims to solve problems detailed in previous algorithms;\nthrough machine learning with nested long short-term memory (NLSTM) modules and\ngraph theory, the algorithm attempts to predict the near future using past data\nand traffic patterns to inform its real-time decisions and better mitigate\ntraffic by predicting future traffic flow based on past flow and using those\npredictions to both maximize present traffic flow and decrease future traffic\ncongestion. Furthermore, while ITLC and OAF require the use of GPS\ntransponders; and GPS, speed sensors, and radio, respectively, ELMOPP only uses\ntraffic light camera footage, something that is almost always readily available\nin contrast to GPS and speed sensors. ELMOPP was tested against the ITLC and\nOAF traffic management algorithms using a simulation modeled after the one\npresented in the ITLC paper, a single-intersection simulation, and the\ncollected data supports the conclusion that ELMOPP statistically significantly\noutperforms both algorithms in throughput rate, a measure of how many vehicles\nare able to exit inroads every second.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 20:57:29 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Sheriff", "Fareed", ""]]}, {"id": "2106.10241", "submitter": "Mayana Pereira", "authors": "Mayana Pereira, Meghana Kshirsagar, Sumit Mukherjee, Rahul Dodhia,\n  Juan Lavista Ferres", "title": "An Analysis of the Deployment of Models Trained on Private Tabular\n  Synthetic Data: Unexpected Surprises", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diferentially private (DP) synthetic datasets are a powerful approach for\ntraining machine learning models while respecting the privacy of individual\ndata providers. The effect of DP on the fairness of the resulting trained\nmodels is not yet well understood. In this contribution, we systematically\nstudy the effects of differentially private synthetic data generation on\nclassification. We analyze disparities in model utility and bias caused by the\nsynthetic dataset, measured through algorithmic fairness metrics. Our first set\nof results show that although there seems to be a clear negative correlation\nbetween privacy and utility (the more private, the less accurate) across all\ndata synthesizers we evaluated, more privacy does not necessarily imply more\nbias. Additionally, we assess the effects of utilizing synthetic datasets for\nmodel training and model evaluation. We show that results obtained on synthetic\ndata can misestimate the actual model performance when it is deployed on real\ndata. We hence advocate on the need for defining proper testing protocols in\nscenarios where differentially private synthetic datasets are utilized for\nmodel training and evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 21:00:57 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Pereira", "Mayana", ""], ["Kshirsagar", "Meghana", ""], ["Mukherjee", "Sumit", ""], ["Dodhia", "Rahul", ""], ["Ferres", "Juan Lavista", ""]]}, {"id": "2106.10281", "submitter": "Christopher McLaughlin Danforth", "authors": "Henry H. Wu, Ryan J. Gallagher, Thayer Alshaabi, Jane L. Adams, Joshua\n  R. Minot, Michael V. Arnold, Brooke Foucault Welles, Randall Harp, Peter\n  Sheridan Dodds, Christopher M. Danforth", "title": "Say Their Names: Resurgence in the collective attention toward Black\n  victims of fatal police violence following the death of George Floyd", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY physics.soc-ph", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The murder of George Floyd by police in May 2020 sparked international\nprotests and renewed attention in the Black Lives Matter movement. Here, we\ncharacterize ways in which the online activity following George Floyd's death\nwas unparalleled in its volume and intensity, including setting records for\nactivity on Twitter, prompting the saddest day in the platform's history, and\ncausing George Floyd's name to appear among the ten most frequently used\nphrases in a day, where he is the only individual to have ever received that\nlevel of attention who was not known to the public earlier that same week.\nFurther, we find this attention extended beyond George Floyd and that more\nBlack victims of fatal police violence received attention following his death\nthan during other past moments in Black Lives Matter's history. We place that\nattention within the context of prior online racial justice activism by showing\nhow the names of Black victims of police violence have been lifted and\nmemorialized over the last 12 years on Twitter. Our results suggest that the\n2020 wave of attention to the Black Lives Matter movement centered past\ninstances of police violence in an unprecedented way, demonstrating the impact\nof the movement's rhetorical strategy to \"say their names.\"\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 18:00:00 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wu", "Henry H.", ""], ["Gallagher", "Ryan J.", ""], ["Alshaabi", "Thayer", ""], ["Adams", "Jane L.", ""], ["Minot", "Joshua R.", ""], ["Arnold", "Michael V.", ""], ["Welles", "Brooke Foucault", ""], ["Harp", "Randall", ""], ["Dodds", "Peter Sheridan", ""], ["Danforth", "Christopher M.", ""]]}, {"id": "2106.10328", "submitter": "Christy Dennison", "authors": "Irene Solaiman (1) and Christy Dennison (1) ((1) OpenAI)", "title": "Process for Adapting Language Models to Society (PALMS) with\n  Values-Targeted Datasets", "comments": "Both authors contributed equally. Submitted to NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Language models can generate harmful and biased outputs and exhibit\nundesirable behavior. We propose a Process for Adapting Language Models to\nSociety (PALMS) with Values-Targeted Datasets, an iterative process to\nsignificantly change model behavior by crafting and fine-tuning on a dataset\nthat reflects a predetermined set of target values. We evaluate our process\nusing three metrics: quantitative metrics with human evaluations that score\noutput adherence to a target value, and toxicity scoring on outputs; and\nqualitative metrics analyzing the most common word associated with a given\nsocial category. Through each iteration, we add additional training dataset\nexamples based on observed shortcomings from evaluations. PALMS performs\nsignificantly better on all metrics compared to baseline and control models for\na broad range of GPT-3 language model sizes without compromising capability\nintegrity. We find that the effectiveness of PALMS increases with model size.\nWe show that significantly adjusting language model behavior is feasible with a\nsmall, hand-curated dataset.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 19:38:28 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Solaiman", "Irene", "", "OpenAI"], ["Dennison", "Christy", "", "OpenAI"]]}, {"id": "2106.10331", "submitter": "Nirmalya Thakur", "authors": "Nirmalya Thakur and Chia Y. Han", "title": "Exoskeleton-Based Multimodal Action and Movement Recognition:\n  Identifying and Developing the Optimal Boosted Learning Approach", "comments": null, "journal-ref": "Journal of Advances in Artificial Intelligence and Machine\n  Learning. 2021; Volume 1, Issue 1, Article 4", "doi": null, "report-no": null, "categories": "cs.RO cs.CY cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper makes two scientific contributions to the field of\nexoskeleton-based action and movement recognition. First, it presents a novel\nmachine learning and pattern recognition-based framework that can detect a wide\nrange of actions and movements - walking, walking upstairs, walking downstairs,\nsitting, standing, lying, stand to sit, sit to stand, sit to lie, lie to sit,\nstand to lie, and lie to stand, with an overall accuracy of 82.63%. Second, it\npresents a comprehensive comparative study of different learning approaches -\nRandom Forest, Artificial Neural Network, Decision Tree, Multiway Decision\nTree, Support Vector Machine, k-NN, Gradient Boosted Trees, Decision Stump,\nAuto MLP, Linear Regression, Vector Linear Regression, Random Tree, Na\\\"ive\nBayes, Na\\\"ive Bayes (Kernel), Linear Discriminant Analysis, Quadratic\nDiscriminant Analysis, and Deep Learning applied to this framework. The\nperformance of each of these learning approaches was boosted by using the\nAdaBoost algorithm, and the Cross Validation approach was used for training and\ntesting. The results show that in boosted form, the k- NN classifier\noutperforms all the other boosted learning approaches and is, therefore, the\noptimal learning method for this purpose. The results presented and discussed\nuphold the importance of this work to contribute towards augmenting the\nabilities of exoskeleton-based assisted and independent living of the elderly\nin the future of Internet of Things-based living environments, such as Smart\nHomes. As a specific use case, we also discuss how the findings of our work are\nrelevant for augmenting the capabilities of the Hybrid Assistive Limb\nexoskeleton, a highly functional lower limb exoskeleton.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 19:43:54 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Thakur", "Nirmalya", ""], ["Han", "Chia Y.", ""]]}, {"id": "2106.10339", "submitter": "Fang Liu", "authors": "Dong Wang, Fang Liu", "title": "Privacy-preserving Publication and Sharing of COVID-19 Pandemic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A huge amount of data of various types are collected during the COVID-19\npandemic, the analysis and interpretation of which has been indispensable for\ncurbing the spread of the coronavirus. As the pandemic slows down, the\ncollected data during the pandemic will continue to be rich sources for further\nstudying the pandemic and understanding its impacts on public health,\neconomics, and societies. On the other hand, na\\\"{i}ve release and sharing of\nthe information can be associated with serious privacy concerns. In this paper,\naiming at shedding light on privacy-preserving sharing of pandemic data and\nthus promoting and encouraging more data sharing for research and public use,\nwe examine three common data types -- case surveillance, patient location\nhistories and hot spot maps, and contact tracing networks -- collected during\nthe pandemic and develop and apply privacy-preserving approaches for publishing\nor sharing each data type. We illustrate the applications and examine the\nutility of released privacy-preserving data in examples and experiments at\nvarious levels of privacy guarantees.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 20:07:58 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wang", "Dong", ""], ["Liu", "Fang", ""]]}, {"id": "2106.10352", "submitter": "Ruiqing Ding", "authors": "Ruiqing Ding, Yu Zhou, Jie Xu, Yan Xie, Qiqiang Liang, He Ren, Yixuan\n  Wang, Yanlin Chen, Leye Wang and Man Huang", "title": "Semi-supervised Optimal Transport with Self-paced Ensemble for\n  Cross-hospital Sepsis Early Detection", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The utilization of computer technology to solve problems in medical scenarios\nhas attracted considerable attention in recent years, which still has great\npotential and space for exploration. Among them, machine learning has been\nwidely used in the prediction, diagnosis and even treatment of Sepsis. However,\nstate-of-the-art methods require large amounts of labeled medical data for\nsupervised learning. In real-world applications, the lack of labeled data will\ncause enormous obstacles if one hospital wants to deploy a new Sepsis detection\nsystem. Different from the supervised learning setting, we need to use known\ninformation (e.g., from another hospital with rich labeled data) to help build\na model with acceptable performance, i.e., transfer learning. In this paper, we\npropose a semi-supervised optimal transport with self-paced ensemble framework\nfor Sepsis early detection, called SPSSOT, to transfer knowledge from the other\nthat has rich labeled data. In SPSSOT, we first extract the same clinical\nindicators from the source domain (e.g., hospital with rich labeled data) and\nthe target domain (e.g., hospital with little labeled data), then we combine\nthe semi-supervised domain adaptation based on optimal transport theory with\nself-paced under-sampling to avoid a negative transfer possibly caused by\ncovariate shift and class imbalance. On the whole, SPSSOT is an end-to-end\ntransfer learning method for Sepsis early detection which can automatically\nselect suitable samples from two domains respectively according to the number\nof iterations and align feature space of two domains. Extensive experiments on\ntwo open clinical datasets demonstrate that comparing with other methods, our\nproposed SPSSOT, can significantly improve the AUC values with only 1% labeled\ndata in the target domain in two transfer learning scenarios, MIMIC\n$rightarrow$ Challenge and Challenge $rightarrow$ MIMIC.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 20:54:18 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ding", "Ruiqing", ""], ["Zhou", "Yu", ""], ["Xu", "Jie", ""], ["Xie", "Yan", ""], ["Liang", "Qiqiang", ""], ["Ren", "He", ""], ["Wang", "Yixuan", ""], ["Chen", "Yanlin", ""], ["Wang", "Leye", ""], ["Huang", "Man", ""]]}, {"id": "2106.10427", "submitter": "Jukka Ruohonen", "authors": "Jukka Ruohonen", "title": "Reassessing Measures for Press Freedom", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a newly refound interest in press freedom in the face of\nvarious global scandals, transformation of media, technological change,\nobstacles to deliberative democracy, and other factors. Press freedom is\nfrequently used also as an explanatory factor in comparative empirical\nresearch. However, validations of existing measurement instruments on press\nfreedom have been far and few between. Given these points, this paper evaluates\neight cross-country instruments on press freedom in 147 countries between 2001\nand 2020, replicating an earlier study with a comparable research setup. The\nmethodology is based on principal component analysis and multi-level regression\nmodeling. According to the results, the construct (convergence) validity of the\ninstruments is good; they all measure the same underlying semi-narrow\ndefinition for press freedom elaborated in the paper. In addition, any of the\nindices seems suitable to be used interchangeability in empirical research.\nLimitations and future research directions are further discussed.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 05:27:45 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ruohonen", "Jukka", ""]]}, {"id": "2106.10734", "submitter": "Jiyue Huang", "authors": "Jiyue Huang, Chi Hong, Lydia Y. Chen, Stefanie Roos", "title": "Is Shapley Value fair? Improving Client Selection for Mavericks in\n  Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Shapley Value is commonly adopted to measure and incentivize client\nparticipation in federated learning. In this paper, we show -- theoretically\nand through simulations -- that Shapley Value underestimates the contribution\nof a common type of client: the Maverick. Mavericks are clients that differ\nboth in data distribution and data quantity and can be the sole owners of\ncertain types of data. Selecting the right clients at the right moment is\nimportant for federated learning to reduce convergence times and improve\naccuracy. We propose FedEMD, an adaptive client selection strategy based on the\nWasserstein distance between the local and global data distributions. As FedEMD\nadapts the selection probability such that Mavericks are preferably selected\nwhen the model benefits from improvement on rare classes, it consistently\nensures the fast convergence in the presence of different types of Mavericks.\nCompared to existing strategies, including Shapley Value-based ones, FedEMD\nimproves the convergence of neural network classifiers by at least 26.9% for\nFedAvg aggregation compared with the state of the art.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 18:01:15 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Huang", "Jiyue", ""], ["Hong", "Chi", ""], ["Chen", "Lydia Y.", ""], ["Roos", "Stefanie", ""]]}, {"id": "2106.10826", "submitter": "Yada Pruksachatkun Ms.", "authors": "Yada Pruksachatkun and Satyapriya Krishna and Jwala Dhamala and Rahul\n  Gupta and Kai-Wei Chang", "title": "Does Robustness Improve Fairness? Approaching Fairness with Word\n  Substitution Robustness Methods for Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing bias mitigation methods to reduce disparities in model outcomes\nacross cohorts have focused on data augmentation, debiasing model embeddings,\nor adding fairness-based optimization objectives during training. Separately,\ncertified word substitution robustness methods have been developed to decrease\nthe impact of spurious features and synonym substitutions on model predictions.\nWhile their end goals are different, they both aim to encourage models to make\nthe same prediction for certain changes in the input. In this paper, we\ninvestigate the utility of certified word substitution robustness methods to\nimprove equality of odds and equality of opportunity on multiple text\nclassification tasks. We observe that certified robustness methods improve\nfairness, and using both robustness and bias mitigation methods in training\nresults in an improvement in both fronts\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 03:20:44 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Pruksachatkun", "Yada", ""], ["Krishna", "Satyapriya", ""], ["Dhamala", "Jwala", ""], ["Gupta", "Rahul", ""], ["Chang", "Kai-Wei", ""]]}, {"id": "2106.10910", "submitter": "Fotios Lazarinis", "authors": "Fotis Lazarinis, Dimitris Kanellopoulos", "title": "Fostering Student Engagement in a Mobile Formative Assessment System for\n  High-School Economics", "comments": "10 pages, 3 images", "journal-ref": "Social Education Research, 2021", "doi": "10.37256/ser.222021747", "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a mobile learning environment, students can learn via mobile devices\nwithout being limited by time and space. Therefore, it is vital to develop\ntools to assist students to learn and assess their knowledge in such\nenvironments. This paper presents a tool/application for formative\nself-assessment. The tool supports the selection of questions based on\nuser-defined criteria concerning (1) the difficulty level; (2) the associated\nconcepts; and (3) the purposes of the test taker. The main purpose of the\npresented tool is to better support the learning aims of the participants and\nto increase their engagement in the learning process. The focus of this study\nis to evaluate the tool using quizzes in Microeconomics to realize its\npotential in this specific domain. Teachers and students were involved in the\nexperiments conducted. The experiments demonstrated that the presented tool is\nusable; it motivates the students and improves their understanding\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 08:15:49 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Lazarinis", "Fotis", ""], ["Kanellopoulos", "Dimitris", ""]]}, {"id": "2106.10925", "submitter": "Laila El-Hamamsy", "authors": "J\\'er\\^ome Brender, Laila El-Hamamsy, Barbara Bruno, Fr\\'ed\\'erique\n  Chessel-Lazzarotto, Jessica Dehler Zufferey, Francesco Mondada", "title": "Investigating the role of educational robotics in formal mathematics\n  education: the case of geometry for 15-year-old students", "comments": "To appear in the proceedings of the Sixteenth European Conference on\n  Technology Enhanced Learning (2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Research has shown that Educational Robotics (ER) enhances student\nperformance, interest, engagement and collaboration. However, until now, the\nadoption of robotics in formal education has remained relatively scarce. Among\nother causes, this is due to the difficulty of determining the alignment of\neducational robotic learning activities with the learning outcomes envisioned\nby the curriculum, as well as their integration with traditional, non-robotics\nlearning activities that are well established in teachers' practices. This work\ninvestigates the integration of ER into formal mathematics education, through a\nquasi-experimental study employing the Thymio robot and Scratch programming to\nteach geometry to two classes of 15-year-old students, for a total of 26\nparticipants. Three research questions were addressed: (1) Should an ER-based\ntheoretical lecture precede, succeed or replace a traditional theoretical\nlecture? (2) What is the students' perception of and engagement in the ER-based\nlecture and exercises? (3) Do the findings differ according to students' prior\nappreciation of mathematics? The results suggest that ER activities are as\nvalid as traditional ones in helping students grasp the relevant theoretical\nconcepts. Robotics activities seem particularly beneficial during exercise\nsessions: students freely chose to do exercises that included the robot, rated\nthem as significantly more interesting and useful than their traditional\ncounterparts, and expressed their interest in introducing ER in other\nmathematics lectures. Finally, results were generally consistent between the\nstudents that like and did not like mathematics, suggesting the use of robotics\nas a means to broaden the number of students engaged in the discipline.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 08:53:39 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Brender", "J\u00e9r\u00f4me", ""], ["El-Hamamsy", "Laila", ""], ["Bruno", "Barbara", ""], ["Chessel-Lazzarotto", "Fr\u00e9d\u00e9rique", ""], ["Zufferey", "Jessica Dehler", ""], ["Mondada", "Francesco", ""]]}, {"id": "2106.11000", "submitter": "Jukka Ruohonen", "authors": "Jukka Ruohonen", "title": "A Comparative Study of Online Disinformation and Offline Protests", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In early 2021 the United States Capitol in Washington was stormed during a\nriot and violent attack. Although the storming was merely an instance in a long\nsequence of events, it provided a testimony for many observers who had claimed\nthat online actions, including the propagation of disinformation, have offline\nconsequences. Soon after, a number of papers have been published about the\nrelation between online disinformation and offline violence, among other\nrelated relations. Hitherto, the effects upon political protests have been\nunexplored. This paper thus evaluates such effects with a time series\ncross-sectional sample of 125 countries in a period between 2000 and 2019. The\nresults are mixed. Based on Bayesian multi-level regression modeling, (i) there\nindeed is an effect between online disinformation and offline protests, but the\neffect is partially meditated by political polarization. The results are\nclearer in a sample of countries belonging to the European Economic Area. With\nthis sample, (ii) offline protest counts increase from online disinformation\ndisseminated by domestic governments, political parties, and politicians as\nwell as by foreign governments. Furthermore, (iii) Internet shutdowns and\ngovernmental monitoring of social media tend to decrease the counts. With these\nresults, the paper contributes to the blossoming disinformation research by\nmodeling the impact of disinformation upon offline phenomenon. The contribution\nis important due to the various policy measures planned or already enacted.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 11:40:00 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 15:52:04 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Ruohonen", "Jukka", ""]]}, {"id": "2106.11021", "submitter": "Marc Canellas", "authors": "Marc Canellas", "title": "Defending IEEE Software Standards in Federal Criminal Court", "comments": "13 pages, 0 figures, 1 table", "journal-ref": "Computer, vol. 54, no. 6, pp. 14--23, Jun. 2021", "doi": "10.1109/MC.2020.3038630", "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  IEEE's 1012 Standard for independent software and hardware verification and\nvalidation (IV&V) is under attack in U.S. federal criminal court. As software\nspreads through the criminal legal system, scientists, engineers, and IEEE have\nan essential role in ensuring courts understand and respect IEEE 1012 and IV&V.\nIf scientists, engineers, and IEEE do not engage, courts will continue to allow\nunreliable scientific evidence to deprive people of their life and liberty.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 02:40:01 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Canellas", "Marc", ""]]}, {"id": "2106.11022", "submitter": "Roel Dobbe", "authors": "Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz", "title": "Hard Choices in Artificial Intelligence", "comments": "Pre-print. Shorter versions published at Neurips 2019 Workshop on AI\n  for Social Good and Conference on AI, Ethics and Society 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As AI systems are integrated into high stakes social domains, researchers now\nexamine how to design and operate them in a safe and ethical manner. However,\nthe criteria for identifying and diagnosing safety risks in complex social\ncontexts remain unclear and contested. In this paper, we examine the vagueness\nin debates about the safety and ethical behavior of AI systems. We show how\nthis vagueness cannot be resolved through mathematical formalism alone, instead\nrequiring deliberation about the politics of development as well as the context\nof deployment. Drawing from a new sociotechnical lexicon, we redefine vagueness\nin terms of distinct design challenges at key stages in AI system development.\nThe resulting framework of Hard Choices in Artificial Intelligence (HCAI)\nempowers developers by 1) identifying points of overlap between design\ndecisions and major sociotechnical challenges; 2) motivating the creation of\nstakeholder feedback channels so that safety issues can be exhaustively\naddressed. As such, HCAI contributes to a timely debate about the status of AI\ndevelopment in democratic societies, arguing that deliberation should be the\ngoal of AI Safety, not just the procedure by which it is ensured.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 09:49:34 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Dobbe", "Roel", ""], ["Gilbert", "Thomas Krendl", ""], ["Mintz", "Yonatan", ""]]}, {"id": "2106.11023", "submitter": "Jawad Haqbeen", "authors": "Jawad Haqbeen, Takayuki Ito, Sofia Sahab, Rafik Hadfi, Shun Okuhara,\n  Nasim Saba, Murataza Hofaini, Umar Baregzai", "title": "A Contribution to COVID-19 Prevention through Crowd Collaboration using\n  Conversational AI & Social Platforms", "comments": "To appear as a workshop paper at AI4SG 2020. 6 pages, 4 figures, 1\n  table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  COVID-19 Prevention, which combines the soft approaches and best practices\nfor public health safety, is the only recommended solution from the health\nscience and management society side considering the pandemic era. In an attempt\nto evaluate the validity of such claims in a conflict and COVID-19-affected\ncountry like Afghanistan, we conducted a large-scale digital social experiment\nusing conversational AI and social platforms from an info-epidemiology and an\ninfoveillance perspective. This served as a means to uncover an underling\ntruth, give large-scale facilitation support, extend the soft impact of\ndiscussion to multiple sites, collect, diverge, converge and evaluate a large\namount of opinions and concerns from health experts, patients and local people,\ndeliberate on the data collected and explore collective prevention approaches\nof COVID-19. Finally, this paper shows that deciding a prevention measure that\nmaximizes the probability of finding the ground truth is intrinsically\ndifficult without utilizing the support of an AI-enabled discussion systems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 04:46:42 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Haqbeen", "Jawad", ""], ["Ito", "Takayuki", ""], ["Sahab", "Sofia", ""], ["Hadfi", "Rafik", ""], ["Okuhara", "Shun", ""], ["Saba", "Nasim", ""], ["Hofaini", "Murataza", ""], ["Baregzai", "Umar", ""]]}, {"id": "2106.11025", "submitter": "Benjamin Leiding", "authors": "Benjamin Leiding", "title": "Come back when you are charged! Self-Organized Charging for Electric\n  Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dwindling nonrenewable fuel reserves, progressing severe environmental\npollution, and accelerating climate change require society to reevaluate\nexisting transportation concepts. While electric vehicles (EVs) have become\nmore popular and slowly gain widespread adoption, the corresponding battery\ncharging infrastructures still limits EVs' use in our everyday life. This is\nespecially true for EV owners that do not have the option to operate charging\nhardware, such as wall boxes, at their premises. Charging an EV without an\nat-home wall box is time-consuming since the owner has to drive to the charger,\ncharge the vehicle while waiting nearby, and finally drive back home. Thus, a\nconvenient and easy-to-use solution is required to overcome the issue and\nincentivize EVs for daily commuters. Therefore, we propose an ecosystem and a\nservice platform for (semi-)autonomous electric vehicles that allow them to\nutilize their \"free\"-time, e.g., at night, to access public and private\ncharging infrastructure, charge their batteries, and get back home before the\nowner needs the car again. To do so, we utilize the concept of the\nMachine-to-Everything Economy (M2X Economy) and outline a decentralized\necosystem for smart machines that transact, interact and collaborate via\nblockchain-based smart contracts to enable a convenient battery charging\nmarketplace for (semi-)autonomous EVs.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 08:36:16 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Leiding", "Benjamin", ""]]}, {"id": "2106.11027", "submitter": "Suchithra Rajendran", "authors": "Suchithra Rajendran and Aidan Harper", "title": "Simulation-based Algorithm for Determining Best Package Delivery\n  Alternatives under Three Criteria: Time, Cost and Sustainability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.GT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the significant rise in demand for same-day instant deliveries, several\ncourier services are exploring alternatives to transport packages in a cost-\nand time-effective, as well as, sustainable manner. Motivated by a real-life\ncase study, this paper focuses on developing a simulation algorithm that\nassists same-day package delivery companies to serve customers instantly. The\nproposed recommender system provides the best solution with respect to three\ncriteria: cost, time, and sustainability, considering the variation in travel\ntime and cost parameters. The decision support tool provides recommendations on\nthe best alternative for transporting products based on factors, such as source\nand destination locations, time of the day, package weight, and volume. Besides\nconsidering existing new technologies like electric-assisted cargo bikes, we\nalso analyze the impact of emerging methods of deliveries, such as robots and\nair taxis. Finally, this paper also considers the best delivery alternative\nduring the presence of a pandemic, such as COVID-19. For the purpose of\nillustrating our approach, we consider the delivery options in New York City.\nWe believe that the proposed tool is the first to provide solutions to courier\ncompanies considering evolving modes of transportation and under logistics\ndisruptions due to pandemic.\n  Keywords: Instant package delivery; Courier services; Simulation algorithm;\nRecommender system; Emerging technologies; COVID-19 pandemic.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 18:17:09 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Rajendran", "Suchithra", ""], ["Harper", "Aidan", ""]]}, {"id": "2106.11029", "submitter": "Shishir Adhikari", "authors": "Shishir Adhikari, Akshay Uppal, Robin Mermelstein, Tanya Berger-Wolf,\n  Elena Zheleva", "title": "Understanding the Dynamics between Vaping and Cannabis Legalization\n  Using Twitter Opinions", "comments": "Published at ICWSM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Cannabis legalization has been welcomed by many U.S. states but its role in\nescalation from tobacco e-cigarette use to cannabis vaping is unclear.\nMeanwhile, cannabis vaping has been associated with new lung diseases and\nrising adolescent use. To understand the impact of cannabis legalization on\nescalation, we design an observational study to estimate the causal effect of\nrecreational cannabis legalization on the development of pro-cannabis attitude\nfor e-cigarette users. We collect and analyze Twitter data which contains\nopinions about cannabis and JUUL, a very popular e-cigarette brand. We use\nweakly supervised learning for personal tweet filtering and classification for\nstance detection. We discover that recreational cannabis legalization policy\nhas an effect on increased development of pro-cannabis attitudes for users\nalready in favor of e-cigarettes.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 15:34:20 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Adhikari", "Shishir", ""], ["Uppal", "Akshay", ""], ["Mermelstein", "Robin", ""], ["Berger-Wolf", "Tanya", ""], ["Zheleva", "Elena", ""]]}, {"id": "2106.11030", "submitter": "Mohamad Kassem Prof", "authors": "Dean Douglas, Graham Kelly and Mohamad Kassem", "title": "BIM, Digital Twin and Cyber-Physical Systems: crossing and blurring\n  boundaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Digital Twin in construction and the built environment have started to\nattract the attention of researchers and practitioners in recent times. Its\nanticipated value proposition is focussed on its capability of generating new\nunderstanding and insights into an asset at all stages of its life cycle,\nexploiting diverse data sets from a multitude of sources and professions, in\nreal or near real-time. However, there is still a significant debate about the\ndelineation (i.e. communalities and differences) between digital twin and other\nrelated concepts, particularly Building Information Modelling (BIM) and\nCyber-Physical Systems (CPS). To date, this debate has been confined to social\nmedia discussions, insights blogs and position papers. This paper addresses\nthis challenge using a systematic review. The aim is to investigate\ncommunalities and differences between the three concepts, Digital Twin, BIM and\nCPS. The results of this paper are expected to foster the discussion around\nthis theme within construction and the built environment.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 09:24:32 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Douglas", "Dean", ""], ["Kelly", "Graham", ""], ["Kassem", "Mohamad", ""]]}, {"id": "2106.11031", "submitter": "John Grundy", "authors": "John C. Grundy", "title": "A Graduate Course on E-commerce Information Systems Engineering", "comments": "Preprint to appear in", "journal-ref": "Journal of Informatics Education and Research, IAIM Press, Vol. 2,\n  No. 2. 2000", "doi": null, "report-no": null, "categories": "cs.CY cs.GL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Interest in developing and deploying E-commerce systems has grown greatly\nover the past few years, leading to a major shortage of qualified developers.\nThis paper describes the development of a graduate course in E-commerce\neducation that has been implemented as part of an Honours degree programme and\na first year Masters offering for a traditional Information Systems programme.\nThe aim of this course is to give graduate students a broad grounding and\nholistic set of skills to enable them to begin successful E-commerce systems\ndevelopment. This paper provides a background and context for this course, an\noverview and discussion of the course content, delivery methods and practical\nfocus, and discusses experience in running and evolving the course over the\npast two years. We hope that our experiences and approaches will be useful for\nothers who are planning similar courses focusing on E-commerce systems\nengineering.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 23:05:22 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Grundy", "John C.", ""]]}, {"id": "2106.11032", "submitter": "Seth Poulsen", "authors": "Seth Poulsen, Mahesh Viswanathan, Geoffrey L. Herman, Matthew West", "title": "Proof Blocks: Autogradeable Scaffolding Activities for Learning to Write\n  Proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proof Blocks is a software tool which enables students to write proofs by\ndragging and dropping prewritten proof lines into the correct order. These\nproofs can be graded completely automatically, enabling students to receive\nrapid feedback on how they are doing with their proofs. When constructing a\nproblem, the instructor specifies the dependency graph of the lines of the\nproof, so that any correct arrangement of the lines can receive full credit.\nThis innovation can improve assessment tools by increasing the types of\nquestions we can ask students about proofs, and can give greater access to\nproof knowledge by increasing the amount that students can learn on their own\nwith the help of a computer.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:03:58 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Poulsen", "Seth", ""], ["Viswanathan", "Mahesh", ""], ["Herman", "Geoffrey L.", ""], ["West", "Matthew", ""]]}, {"id": "2106.11033", "submitter": "Stephan Stahlschmidt", "authors": "Axel Oberschelp, Stephan Stahlschmidt", "title": "Gr\\\"o{\\ss}e als Erfolgsgarant? Zur Bedeutung der Organisationstruktur\n  f\\\"ur die Einwerbung von Drittmitteln der Deutschen Forschungsgemeinschaft", "comments": "in German", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research funding through third-party financing is of considerable importance\nfor the German science system. The funds of the German Research Foundation\n(DFG) serve as the central focus due to the high reputation of the foundation.\nHowever, it has not been clarified yet to what extent the chances of\nsuccessfully acquiring these funds depend on the structure of the university as\nan institution. The present study analyses DFG funding in the context of\nuniversity research and examines the role of organisational conditions in the\nacquisition of funding. Several factors, such as size of the institution,\nequipment, and teaching activities, are analysed. The empirical study focuses\non four subjects and investigates the correlation between funding success and\nconditional factors using a Bayesian approach. Results reveal the considerable\nrelevance of the factors size as well as provision of academic and non-academic\npersonnel. This implies that the organisational conditions are to be taken into\naccount while evaluating third-party financing success.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 08:47:34 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Oberschelp", "Axel", ""], ["Stahlschmidt", "Stephan", ""]]}, {"id": "2106.11034", "submitter": "Tapani Toivonen Dr.", "authors": "Matti Tedre, Tapani Toivonen, Juho Kaihila, Henriikka Vartiainen,\n  Teemu Valtonen, Ilkka Jormanainen, and Arnold Pears", "title": "Teaching Machine Learning in K-12 Computing Education: Potential and\n  Pitfalls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past decades, numerous practical applications of machine learning\ntechniques have shown the potential of data-driven approaches in a large number\nof computing fields. Machine learning is increasingly included in computing\ncurricula in higher education, and a quickly growing number of initiatives are\nexpanding it in K-12 computing education, too. As machine learning enters K-12\ncomputing education, understanding how intuition and agency in the context of\nsuch systems is developed becomes a key research area. But as schools and\nteachers are already struggling with integrating traditional computational\nthinking and traditional artificial intelligence into school curricula,\nunderstanding the challenges behind teaching machine learning in K-12 is an\neven more daunting challenge for computing education research. Despite the\ncentral position of machine learning in the field of modern computing, the\ncomputing education research body of literature contains remarkably few studies\nof how people learn to train, test, improve, and deploy machine learning\nsystems. This is especially true of the K-12 curriculum space. This article\ncharts the emerging trajectories in educational practice, theory, and\ntechnology related to teaching machine learning in K-12 education. The article\nsituates the existing work in the context of computing education in general,\nand describes some differences that K-12 computing educators should take into\naccount when facing this challenge. The article focuses on key aspects of the\nparadigm shift that will be required in order to successfully integrate machine\nlearning into the broader K-12 computing curricula. A crucial step is\nabandoning the belief that rule-based \"traditional\" programming is a central\naspect and building block in developing next generation computational thinking.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 10:45:47 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Tedre", "Matti", ""], ["Toivonen", "Tapani", ""], ["Kaihila", "Juho", ""], ["Vartiainen", "Henriikka", ""], ["Valtonen", "Teemu", ""], ["Jormanainen", "Ilkka", ""], ["Pears", "Arnold", ""]]}, {"id": "2106.11035", "submitter": "Marc Zeller", "authors": "Kai Hoefig, Cornel Klein, Stefan Rothbauer, Marc Zeller, Marian\n  Vorderer, Chee Hung Koo", "title": "A Meta-model for Process Failure Mode and Effects Analysis (PFMEA)", "comments": "arXiv admin note: text overlap with arXiv:2105.14817", "journal-ref": "2019 24th IEEE International Conference on Emerging Technologies\n  and Factory Automation (ETFA)", "doi": "10.1109/ETFA.2019.8869087", "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short product lifecycles and a high variety of products force industrial\nmanufacturing processes to change frequently. Due to the manual approach of\nmany quality analysis techniques, they can significantly slow down adaption\nprocesses of production systems or make production unprofitable. Therefore,\nautomating them can be a key technology for keeping pace with market demand of\nthe future. The methodology presented here aims at a meta-model supporting\nautomation for PFMEA. The method differentiates product requirements,\nproduction steps and quality measures in such a way, that complex quality\nrequirements can be addressed in any instance of a factory using a common\nmeta-modeling language.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 14:00:19 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Hoefig", "Kai", ""], ["Klein", "Cornel", ""], ["Rothbauer", "Stefan", ""], ["Zeller", "Marc", ""], ["Vorderer", "Marian", ""], ["Koo", "Chee Hung", ""]]}, {"id": "2106.11036", "submitter": "Mary Roszel", "authors": "Mary Roszel, Robert Norvill, Jean Hilger, Radu State", "title": "Know Your Model (KYM): Increasing Trust in AI and Machine Learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread utilization of AI systems has drawn attention to the potential\nimpacts of such systems on society. Of particular concern are the consequences\nthat prediction errors may have on real-world scenarios, and the trust humanity\nplaces in AI systems. It is necessary to understand how we can evaluate\ntrustworthiness in AI and how individuals and entities alike can develop\ntrustworthy AI systems. In this paper, we analyze each element of\ntrustworthiness and provide a set of 20 guidelines that can be leveraged to\nensure optimal AI functionality while taking into account the greater ethical,\ntechnical, and practical impacts to humanity. Moreover, the guidelines help\nensure that trustworthiness is provable and can be demonstrated, they are\nimplementation agnostic, and they can be applied to any AI system in any\nsector.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 14:08:22 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Roszel", "Mary", ""], ["Norvill", "Robert", ""], ["Hilger", "Jean", ""], ["State", "Radu", ""]]}, {"id": "2106.11039", "submitter": "Carina Prunkl", "authors": "Carina Prunkl, Carolyn Ashurst, Markus Anderljung, Helena Webb, Jan\n  Leike, Allan Dafoe", "title": "Institutionalising Ethics in AI through Broader Impact Requirements", "comments": null, "journal-ref": "Nature Machine Intelligence 3.2 (2021): 104-110", "doi": "10.1038/s42256-021-00298-y", "report-no": null, "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Turning principles into practice is one of the most pressing challenges of\nartificial intelligence (AI) governance. In this article, we reflect on a novel\ngovernance initiative by one of the world's largest AI conferences. In 2020,\nthe Conference on Neural Information Processing Systems (NeurIPS) introduced a\nrequirement for submitting authors to include a statement on the broader\nsocietal impacts of their research. Drawing insights from similar governance\ninitiatives, including institutional review boards (IRBs) and impact\nrequirements for funding applications, we investigate the risks, challenges and\npotential benefits of such an initiative. Among the challenges, we list a lack\nof recognised best practice and procedural transparency, researcher opportunity\ncosts, institutional and social pressures, cognitive biases, and the inherently\ndifficult nature of the task. The potential benefits, on the other hand,\ninclude improved anticipation and identification of impacts, better\ncommunication with policy and governance experts, and a general strengthening\nof the norms around responsible research. To maximise the chance of success, we\nrecommend measures to increase transparency, improve guidance, create\nincentives to engage earnestly with the process, and facilitate public\ndeliberation on the requirement's merits and future. Perhaps the most important\ncontribution from this analysis are the insights we can gain regarding\neffective community-based governance and the role and responsibility of the AI\nresearch community more broadly.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 12:36:43 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Prunkl", "Carina", ""], ["Ashurst", "Carolyn", ""], ["Anderljung", "Markus", ""], ["Webb", "Helena", ""], ["Leike", "Jan", ""], ["Dafoe", "Allan", ""]]}, {"id": "2106.11040", "submitter": "Amit Verma Dr.", "authors": "Amit Verma, Phillip Frank and Kamal Lamsal", "title": "An exploratory study of skill requirements for social media positions: A\n  content analysis of job advertisements", "comments": "This is the authors original manuscript (preprint) of an article that\n  has been accepted for publication in a future issue of The Journal of Social\n  Media in Society. Content may change prior to final publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been considerable debate about the comparative advantages of\nmarketing education emphasizing theoretical knowledge and applied skills. The\ncurrent study investigated the skills necessary for entry-level marketing\npositions, specifically that of Social Media Manager (SMMgr) and Social Media\nMarketer (SMMkt). Data was collected from Indeed.com using a web crawler to\nextract job postings for SMMgr and SMMkt. A total of 766 and 654 entry-level\njobs for SMMgr and SMMkt, respectively, across the entire United States, was\ncollected. Independent raters separately analyzed the data for keywords and\ncategories. Findings suggest that the most desired skills are occupational\ndigital marketing skills. Other relevant skill categories included\ncommunication, employee attributes, problem-solving, and information technology\nskills. This study extends the current literature by highlighting the desired\nskills prevalent across the social media industry. The findings also have\nrelevance in designing the marketing education curriculum, specifically in\nisolating core skills that could be integrated into the marketing courses.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 02:09:36 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Verma", "Amit", ""], ["Frank", "Phillip", ""], ["Lamsal", "Kamal", ""]]}, {"id": "2106.11041", "submitter": "Dejan Ni\\v{c}kovi\\'c", "authors": "Nicolas Basset, Thao Dang, Felix Gigler, Cristinel Mateis, Dejan\n  Nickovic", "title": "Sampling of Shape Expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-physical systems (CPS) are increasingly becoming driven by data, using\nmultiple types of sensors to capture huge amounts of data. Extraction and\ncharacterization of useful information from big streams of data is a\nchallenging problem. Shape expressions facilitate formal specification of rich\ntemporal patterns encountered in time series as well as in behaviors of CPS. In\nthis paper, we introduce a method for systematically sampling shape\nexpressions. The proposed approach combines methods for uniform sampling of\nautomata (for exploring qualitative shapes) with hit-and-run Monte Carlo\nsampling procedures (for exploring multi-dimensional parameter spaces defined\nby sets of possibly non-linear constraints). We study and implement several\npossible solutions and evaluate them in the context of visualization and\ntesting applications.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 07:29:37 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Basset", "Nicolas", ""], ["Dang", "Thao", ""], ["Gigler", "Felix", ""], ["Mateis", "Cristinel", ""], ["Nickovic", "Dejan", ""]]}, {"id": "2106.11067", "submitter": "Arash Shaban-Nejad", "authors": "Whitney S. Brakefield, Nariman Ammar, Olufunto A. Olusanya, Arash\n  Shaban-Nejad", "title": "An Urban Population Health Observatory System to Support COVID-19\n  Pandemic Preparedness, Response, and Management: Design and Development Study", "comments": "16 Pages, 7 Figures, 1 Table", "journal-ref": "JMIR Public Health Surveill 2021;7(6):e28269", "doi": "10.2196/28269", "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: COVID-19 is impacting people worldwide and is currently a leading\ncause of death in many countries. This study sought to redefine the Healthy\nPeople 2030 SDoH taxonomy to accommodate the COVID-19 pandemic. Furthermore, we\naim to provide a blueprint and implement a prototype for the Urban Population\nHealth Observatory (UPHO), a web-based platform that integrates classified\ngroup-level SDoH indicators to individual- and aggregate-level population\nhealth data. The process of building the UPHO involves collecting and\nintegrating data from several sources, classifying the collected data into\ndrivers and outcomes, incorporating data science techniques for calculating\nmeasurable indicators from the raw variables, and studying the extent to which\ninterventions are identified or developed to mitigate drivers that lead to the\nundesired outcomes. We generated and classified the indicators of social\ndeterminants of health, which are linked to COVID-19. To display the\nfunctionalities of the UPHO platform, we presented a prototype design to\ndemonstrate its features. We provided a use case scenario for 4 different\nusers. UPHO serves as an apparatus for implementing effective interventions and\ncan be adopted as a global platform for chronic and infectious diseases. The\nUPHO surveillance platform provides a novel approach and novel insights into\nimmediate and long-term health policy responses to the COVID-19 pandemic and\nother future public health crises. The UPHO assists public health organizations\nand policymakers in their efforts in reducing health disparities, achieving\nhealth equity, and improving urban population health.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 16:48:55 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Brakefield", "Whitney S.", ""], ["Ammar", "Nariman", ""], ["Olusanya", "Olufunto A.", ""], ["Shaban-Nejad", "Arash", ""]]}, {"id": "2106.11071", "submitter": "Daniela Rotelli", "authors": "Daniela Rotelli, Giuseppe Fiorentino, Anna Monreale", "title": "Making Sense of Learning Log Data", "comments": "14 pages, 2 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research is constantly engaged in finding more productive and powerful ways\nto support quality learning and teaching. However, although researchers and\ndata scientists try to analyse educational data most transparently and\nresponsibly, the risk of training machine learning algorithms on biased\ndatasets is always around the corner and may lead to misinterpretations of\nstudent behaviour. This may happen in case of partial understanding of how\nlearning log data is generated. Moreover, the pursuit of an ever friendlier\nuser experience moves more and more Learning Management Systems functionality\nfrom the server to the client, but it tends to reduce significant logs as a\nside effect. This paper tries to focus on these issues showing some examples of\nlearning log data extracted from Moodle and some possible misinterpretations\nthat they hide with the aim to open the debate on data understanding and data\nknowledge loss.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 10:15:09 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Rotelli", "Daniela", ""], ["Fiorentino", "Giuseppe", ""], ["Monreale", "Anna", ""]]}, {"id": "2106.11076", "submitter": "Lynnette Hui Xian Ng", "authors": "Lynnette Hui Xian Ng and Kathleen Carley", "title": "Flipping Stance: Social Influence on Bot's and Non Bot's COVID Vaccine\n  Stance", "comments": "To appear in The Second International MIS2 Workshop: Misinformation\n  and Misbehavior Mining on the Web", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social influence characterizes the change of opinions in a complex social\nenvironment, incorporating an individual's past stances and the impact of\ninterpersonal influence through the social network influence. In this work, we\nobserve stance changes towards the coronavirus vaccine on Twitter from April\n2020 to May 2021, where 1\\% of the agents exhibit the stance flipping behavior,\nof which 53.7\\% are identified bots. We then propose a novel social influence\nmodel to characterize the change in stance of agents. This model considers an\nagent's and his neighbor's past tweets and the overall network structure\ntowards a stance score. In our experiments, the model achieves 86\\% accuracy.\nIn our analysis, bot agents require lesser social influence to flip stances and\na larger proportion of bots flip.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 12:56:39 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ng", "Lynnette Hui Xian", ""], ["Carley", "Kathleen", ""]]}, {"id": "2106.11077", "submitter": "Kamran Kowsari", "authors": "Mojtaba Heidarysafa and Kamran Kowsari and Masoud Bashiri and Donald\n  E. Brown", "title": "Toward a Knowledge Discovery Framework for Data Science Job Market in\n  the United States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth of the data science field requires better tools to understand such\na fast-paced growing domain. Moreover, individuals from different backgrounds\nbecame interested in following a career as data scientists. Therefore,\nproviding a quantitative guide for individuals and organizations to understand\nthe skills required in the job market would be crucial. This paper introduces a\nframework to analyze the job market for data science-related jobs within the US\nwhile providing an interface to access insights in this market. The proposed\nframework includes three sub-modules allowing continuous data collection,\ninformation extraction, and a web-based dashboard visualization to investigate\nthe spatial and temporal distribution of data science-related jobs and skills.\nThe result of this work shows important skills for the main branches of data\nscience jobs and attempts to provide a skill-based definition of these data\nscience branches. The current version of this application is deployed on the\nweb and allows individuals and institutes to investigate skills required for\ndata science positions through the industry lens.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 21:23:15 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 15:40:25 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Heidarysafa", "Mojtaba", ""], ["Kowsari", "Kamran", ""], ["Bashiri", "Masoud", ""], ["Brown", "Donald E.", ""]]}, {"id": "2106.11312", "submitter": "Ye Tu", "authors": "Ye Tu, Chun Lo, Yiping Yuan, Shaunak Chatterjee", "title": "Feedback Shaping: A Modeling Approach to Nurture Content Creation", "comments": null, "journal-ref": "KDD 2019: Proceedings of the 25th ACM SIGKDD International\n  Conference on Knowledge Discovery & Data Mining", "doi": "10.1145/3292500.3330764", "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media platforms bring together content creators and content consumers\nthrough recommender systems like newsfeed. The focus of such recommender\nsystems has thus far been primarily on modeling the content consumer\npreferences and optimizing for their experience. However, it is equally\ncritical to nurture content creation by prioritizing the creators' interests,\nas quality content forms the seed for sustainable engagement and conversations,\nbringing in new consumers while retaining existing ones. In this work, we\npropose a modeling approach to predict how feedback from content consumers\nincentivizes creators. We then leverage this model to optimize the newsfeed\nexperience for content creators by reshaping the feedback distribution, leading\nto a more active content ecosystem. Practically, we discuss how we balance the\nuser experience for both consumers and creators, and how we carry out online\nA/B tests with strong network effects. We present a deployed use case on the\nLinkedIn newsfeed, where we used this approach to improve content creation\nsignificantly without compromising the consumers' experience.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 22:53:16 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Tu", "Ye", ""], ["Lo", "Chun", ""], ["Yuan", "Yiping", ""], ["Chatterjee", "Shaunak", ""]]}, {"id": "2106.11395", "submitter": "Agatha Hennigen de Mattos", "authors": "Agatha C. H. de Mattos, Gavin McArdle, Michela Bertolotto", "title": "Mapping Slums with Medium Resolution Satellite Imagery: a Comparative\n  Analysis of Multi-Spectral Data and Grey-level Co-occurrence Matrix\n  Techniques", "comments": "Accepted at the 3rd Workshop on Artificial Intelligence for Social\n  Good (IJCAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The UN-Habitat estimates that over one billion people live in slums around\nthe world. However, state-of-the-art techniques to detect the location of slum\nareas employ high-resolution satellite imagery, which is costly to obtain and\nprocess. As a result, researchers have started to look at utilising free and\nopen-access medium resolution satellite imagery. Yet, there is no clear\nconsensus on which data preparation and machine learning approaches are the\nmost appropriate to use with such imagery data. In this paper, we evaluate two\ntechniques (multi-spectral data and grey-level co-occurrence matrix feature\nextraction) on an open-access dataset consisting of labelled Sentinel-2 images\nwith a spatial resolution of 10 meters. Both techniques were paired with a\ncanonical correlation forests classifier. The results show that the grey-level\nco-occurrence matrix performed better than multi-spectral data for all four\ncities. It had an average accuracy for the slum class of 97% and a mean\nintersection over union of 94%, while multi-spectral data had 75% and 64% for\nthe respective metrics. These results indicate that open-access satellite\nimagery with a resolution of at least 10 meters may be suitable for keeping\ntrack of development goals such as the detection of slums in cities.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 20:11:27 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["de Mattos", "Agatha C. H.", ""], ["McArdle", "Gavin", ""], ["Bertolotto", "Michela", ""]]}, {"id": "2106.11419", "submitter": "Avelino Zorzo F", "authors": "Avelino Francisco Zorzo and Andree Luis Alice Raabe and Christian\n  Brackmann", "title": "Computa\\c{c}\\~ao: O vetor de transforma\\c{c}\\~ao da sociedade", "comments": "9 pages, in Portuguese. Published as a chapter in the book \"Desafios\n  da Educa\\c{c}\\~ao T\\'ecnico-Cient\\'ifica no Ensino M\\'edio\". 1ed.Rio de\n  Janeiro: Academia Brasileira de Ci\\^encias, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Society is changing, has always changed, and will keep changing. However,\nchanges are becoming faster and what used to happen between generations, now\nhappens in the same generation. Computing Science is one of the reasons for\nthis speed and permeates, basically, every other knowledge area. This paper\n(written in Portugu\\^es) describes, briefly, the worldwide initiatives to\nintroduce Computing Science teaching in schools. As the paper's main\nconclusion, it is essential to introduce Computing Science and Computational\nThinking for kids before they enter into a university.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 21:36:39 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Zorzo", "Avelino Francisco", ""], ["Raabe", "Andree Luis Alice", ""], ["Brackmann", "Christian", ""]]}, {"id": "2106.11521", "submitter": "Michael Bernstein", "authors": "Michael S. Bernstein, Margaret Levi, David Magnus, Betsy Rajala, Debra\n  Satz, Charla Waeiss", "title": "ESR: Ethics and Society Review of Artificial Intelligence Research", "comments": "Revision: credit to the Microsoft Research Ethics Review Program", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial intelligence (AI) research is routinely criticized for its real\nand potential impacts on society, and we lack adequate institutional responses\nto this criticism and to the responsibility that it reflects. AI research often\nfalls outside the purview of existing feedback mechanisms such as the\nInstitutional Review Board (IRB), which are designed to evaluate harms to human\nsubjects rather than harms to human society. In response, we have developed the\nEthics and Society Review board (ESR), a feedback panel that works with\nresearchers to mitigate negative ethical and societal aspects of AI research.\nThe ESR's main insight is to serve as a requirement for funding: researchers\ncannot receive grant funding from a major AI funding program at our university\nuntil the researchers complete the ESR process for the proposal. In this\narticle, we describe the ESR as we have designed and run it over its first year\nacross 41 proposals. We analyze aggregate ESR feedback on these proposals,\nfinding that the panel most commonly identifies issues of harms to minority\ngroups, inclusion of diverse stakeholders in the research plan, dual use, and\nrepresentation in data. Surveys and interviews of researchers who interacted\nwith the ESR found that 58% felt that it had influenced the design of their\nresearch project, 100% are willing to continue submitting future projects to\nthe ESR, and that they sought additional scaffolding for reasoning through\nethics and society issues.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 03:23:42 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 23:08:22 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Bernstein", "Michael S.", ""], ["Levi", "Margaret", ""], ["Magnus", "David", ""], ["Rajala", "Betsy", ""], ["Satz", "Debra", ""], ["Waeiss", "Charla", ""]]}, {"id": "2106.11676", "submitter": "Theodor Schnitzler", "authors": "Marvin Kowalewski, Franziska Herbert, Theodor Schnitzler, Markus\n  D\\\"urmuth", "title": "Proof-of-Vax: Studying User Preferences and Perception of Covid\n  Vaccination Certificates", "comments": "22 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital tools play an important role in fighting the current global COVID-19\npandemic. We conducted a representative online study in Germany on a sample of\n599 participants to evaluate the user perception of vaccination certificates.\nWe investigated five different variants of vaccination certificates, based on\ndeployed and planned designs in a between-group design, including paper-based\nand app-based variants. Our main results show that the willingness to use and\nadopt vaccination certificates is generally high. Overall, paper-based\nvaccination certificates were favored over app-based solutions. The willingness\nto use digital apps decreased significantly by a higher disposition to privacy,\nand increased by higher worries about the pandemic and acceptance of the\ncoronavirus vaccination. Vaccination certificates resemble an interesting use\ncase for studying privacy perceptions for health related data. We hope that our\nwork will be able to educate the currently ongoing design of vaccination\ncertificates, will give us deeper insights into privacy of health-related data\nand apps, and prepare us for future potential applications of vaccination\ncertificates and health apps in general.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 11:18:25 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Kowalewski", "Marvin", ""], ["Herbert", "Franziska", ""], ["Schnitzler", "Theodor", ""], ["D\u00fcrmuth", "Markus", ""]]}, {"id": "2106.11688", "submitter": "Marcos Oliveira", "authors": "Marcos Oliveira, Fariba Karimi, Maria Zens, Johann Schaible, Mathieu\n  G\\'enois, Markus Strohmaier", "title": "Mixing dynamics and group imbalance lead to degree inequality in\n  face-to-face interaction", "comments": "24 pages; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CY cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncovering how inequality emerges from human interaction is imperative for\njust societies. Here we show that the way social groups interact in\nface-to-face situations can enable the emergence of degree inequality. We\npresent a mechanism that integrates group mixing dynamics with individual\npreferences, which reproduces group degree inequality found in six empirical\ndata sets of face-to-face interactions. We uncover the impact of group-size\nimbalance on degree inequality, revealing a critical minority group size that\nchanges social gatherings qualitatively. If the minority group is larger than\nthis 'critical mass' size, it can be a well-connected, cohesive group; if it is\nsmaller, minority cohesion widens degree inequality. Finally, we expose the\nunder-representation of social groups in degree rankings due to mixing dynamics\nand propose a way to reduce such biases.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 11:44:02 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Oliveira", "Marcos", ""], ["Karimi", "Fariba", ""], ["Zens", "Maria", ""], ["Schaible", "Johann", ""], ["G\u00e9nois", "Mathieu", ""], ["Strohmaier", "Markus", ""]]}, {"id": "2106.11702", "submitter": "Xingyi Song", "authors": "Ye Jiang, Xingyi Song, Carolina Scarton, Ahmet Aker, Kalina Bontcheva", "title": "Categorising Fine-to-Coarse Grained Misinformation: An Empirical Study\n  of COVID-19 Infodemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spreading COVID-19 misinformation over social media already draws the\nattention of many researchers. According to Google Scholar, about 26000\nCOVID-19 related misinformation studies have been published to date. Most of\nthese studies focusing on 1) detect and/or 2) analysing the characteristics of\nCOVID-19 related misinformation. However, the study of the social behaviours\nrelated to misinformation is often neglected. In this paper, we introduce a\nfine-grained annotated misinformation tweets dataset including social\nbehaviours annotation (e.g. comment or question to the misinformation). The\ndataset not only allows social behaviours analysis but also suitable for both\nevidence-based or non-evidence-based misinformation classification task. In\naddition, we introduce leave claim out validation in our experiments and\ndemonstrate the misinformation classification performance could be\nsignificantly different when applying to real-world unseen misinformation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 12:17:53 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 14:24:37 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 16:58:20 GMT"}, {"version": "v4", "created": "Thu, 8 Jul 2021 08:39:43 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Jiang", "Ye", ""], ["Song", "Xingyi", ""], ["Scarton", "Carolina", ""], ["Aker", "Ahmet", ""], ["Bontcheva", "Kalina", ""]]}, {"id": "2106.11714", "submitter": "Roberto Reale", "authors": "Roberta Centonze, Roberto Reale", "title": "Self-sovereign identity as a tool for digital democracy", "comments": "Accepted at the 6th Italian Conference on ICT for Smart Cities and\n  Communities, University of Salerno, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The importance of digital identity as a foundation for digital public\nservices is considered. As the classical, centralised model digital identity\nhas proven to be subject to several limitations, self-sovereign identities are\nproposed as replacement, especially in the context of e-government platforms\nand direct participation to policymaking (e.g. through e-voting tools).\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 12:30:56 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Centonze", "Roberta", ""], ["Reale", "Roberto", ""]]}, {"id": "2106.11728", "submitter": "Gleb Radchenko", "authors": "Ivan Volkov, Gleb Radchenko, Andrey Tchernykh", "title": "Digital Twins, Internet of Things and Mobile Medicine: a Review of\n  Current Platforms to Support Smart Healthcare", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the population grows, the need for a quality level of medical services\ngrows correspondingly, so does the demand for information technology in\nmedicine. The concept of \"Smart Healthcare\" offers many approaches aimed at\nsolving the acute problems faced by modern healthcare. In this paper, we review\nthe main problems of modern healthcare, analyze existing approaches and\ntechnologies in the areas of digital twins, the Internet of Things and mobile\nmedicine, determine their effectiveness in solving the set problems, consider\nthe technologies that are used to monitor and treat patients and propose the\nconcept of the Smart Healthcare platform.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 13:01:39 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Volkov", "Ivan", ""], ["Radchenko", "Gleb", ""], ["Tchernykh", "Andrey", ""]]}, {"id": "2106.11783", "submitter": "Marco Guerini", "authors": "Yi-Ling Chung, Serra Sinem Tekiroglu, Marco Guerini", "title": "Towards Knowledge-Grounded Counter Narrative Generation for Hate Speech", "comments": "To appear in \"Proceedings of the 59th Annual Meeting of the\n  Association for Computational Linguistics (ACL): Findings\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tackling online hatred using informed textual responses - called counter\nnarratives - has been brought under the spotlight recently. Accordingly, a\nresearch line has emerged to automatically generate counter narratives in order\nto facilitate the direct intervention in the hate discussion and to prevent\nhate content from further spreading. Still, current neural approaches tend to\nproduce generic/repetitive responses and lack grounded and up-to-date evidence\nsuch as facts, statistics, or examples. Moreover, these models can create\nplausible but not necessarily true arguments. In this paper we present the\nfirst complete knowledge-bound counter narrative generation pipeline, grounded\nin an external knowledge repository that can provide more informative content\nto fight online hatred. Together with our approach, we present a series of\nexperiments that show its feasibility to produce suitable and informative\ncounter narratives in in-domain and cross-domain settings.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 13:48:49 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Chung", "Yi-Ling", ""], ["Tekiroglu", "Serra Sinem", ""], ["Guerini", "Marco", ""]]}, {"id": "2106.11815", "submitter": "Kwan Hui Lim Dr", "authors": "Prashant Solanki, Kwan Hui Lim and Aaron Harwood", "title": "User Identification across Social Networking Sites using User Profiles\n  and Posting Patterns", "comments": "Accepted at the 2021 International Joint Conference on Neural\n  Networks (IJCNN'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the prevalence of online social networking sites (OSNs) and mobile\ndevices, people are increasingly reliant on a variety of OSNs for keeping in\ntouch with family and friends, and using it as a source of information. For\nexample, a user might utilise multiple OSNs for different purposes, such as\nusing Flickr to share holiday pictures with family and friends, and Twitter to\npost short messages about their thoughts. Identifying the same user across\nmultiple OSNs is an important task as this allows us to understand the usage\npatterns of users among different OSNs, make recommendations when a user\nregisters for a new OSN, and various other useful applications. To address this\nproblem, we proposed an algorithm based on the multilayer perceptron using\nvarious types of features, namely: (i) user profile, such as name, location,\ndescription; (ii) temporal distribution of user generated content; and (iii)\nembedding based on user name, real name and description. Using a Twitter and\nFlickr dataset of users and their posting activities, we perform an empirical\nstudy on how these features affect the performance of user identification\nacross the two OSNs and discuss our main findings based on the different\nfeatures.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 14:28:19 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Solanki", "Prashant", ""], ["Lim", "Kwan Hui", ""], ["Harwood", "Aaron", ""]]}, {"id": "2106.11844", "submitter": "Deepti Gupta", "authors": "Deepti Gupta, Maanak Gupta, Smriti Bhatt, and Ali Saman Tosun", "title": "Detecting Anomalous User Behavior in Remote Patient Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The growth in Remote Patient Monitoring (RPM) services using wearable and\nnon-wearable Internet of Medical Things (IoMT) promises to improve the quality\nof diagnosis and facilitate timely treatment for a gamut of medical conditions.\nAt the same time, the proliferation of IoMT devices increases the potential for\nmalicious activities that can lead to catastrophic results including theft of\npersonal information, data breach, and compromised medical devices, putting\nhuman lives at risk. IoMT devices generate tremendous amount of data that\nreflect user behavior patterns including both personal and day-to-day social\nactivities along with daily routine health monitoring. In this context, there\nare possibilities of anomalies generated due to various reasons including\nunexpected user behavior, faulty sensor, or abnormal values from\nmalicious/compromised devices. To address this problem, there is an imminent\nneed to develop a framework for securing the smart health care infrastructure\nto identify and mitigate anomalies. In this paper, we present an anomaly\ndetection model for RPM utilizing IoMT and smart home devices. We propose\nHidden Markov Model (HMM) based anomaly detection that analyzes normal user\nbehavior in the context of RPM comprising both smart home and smart health\ndevices, and identifies anomalous user behavior. We design a testbed with\nmultiple IoMT devices and home sensors to collect data and use the HMM model to\ntrain using network and user behavioral data. Proposed HMM based anomaly\ndetection model achieved over 98% accuracy in identifying the anomalies in the\ncontext of RPM.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 14:59:34 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Gupta", "Deepti", ""], ["Gupta", "Maanak", ""], ["Bhatt", "Smriti", ""], ["Tosun", "Ali Saman", ""]]}, {"id": "2106.11847", "submitter": "\\'Angel Gonz\\'alez-Prieto", "authors": "\\'Angel Gonz\\'alez-Prieto, Antonio Br\\'u, Juan Carlos Nu\\~no, Jos\\'e\n  Luis Gonz\\'alez-\\'Alvarez", "title": "Machine learning for risk assessment in gender-based crime", "comments": "17 pages, 5 figures, 4 tables. This work has been submitted to the\n  IEEE for possible publication. Copyright may be transferred without notice,\n  after which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gender-based crime is one of the most concerning scourges of contemporary\nsociety. Governments worldwide have invested lots of economic and human\nresources to radically eliminate this threat. Despite these efforts, providing\naccurate predictions of the risk that a victim of gender violence has of being\nattacked again is still a very hard open problem. The development of new\nmethods for issuing accurate, fair and quick predictions would allow police\nforces to select the most appropriate measures to prevent recidivism. In this\nwork, we propose to apply Machine Learning (ML) techniques to create models\nthat accurately predict the recidivism risk of a gender-violence offender. The\nrelevance of the contribution of this work is threefold: (i) the proposed ML\nmethod outperforms the preexisting risk assessment algorithm based on classical\nstatistical techniques, (ii) the study has been conducted through an official\nspecific-purpose database with more than 40,000 reports of gender violence, and\n(iii) two new quality measures are proposed for assessing the effective police\nprotection that a model supplies and the overload in the invested resources\nthat it generates. Additionally, we propose a hybrid model that combines the\nstatistical prediction methods with the ML method, permitting authorities to\nimplement a smooth transition from the preexisting model to the ML-based model.\nThis hybrid nature enables a decision-making process to optimally balance\nbetween the efficiency of the police system and aggressiveness of the\nprotection measures taken.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 15:05:20 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Gonz\u00e1lez-Prieto", "\u00c1ngel", ""], ["Br\u00fa", "Antonio", ""], ["Nu\u00f1o", "Juan Carlos", ""], ["Gonz\u00e1lez-\u00c1lvarez", "Jos\u00e9 Luis", ""]]}, {"id": "2106.11855", "submitter": "Joseph Breda", "authors": "Joseph Breda, Shwetak Patel", "title": "Intuitive and Ubiquitous Fever Monitoring Using Smartphones and\n  Smartwatches", "comments": "18 pages, 9 figures, Not yet submitted to conference (submitting\n  after clinical trials)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inside all smart devices, such as smartphones or smartwatches, there are\nthermally sensitive resistors known as thermistors which are used to monitor\nthe temperature of the device. These thermistors are sensitive to temperature\nchanges near their location on-device. While they are designed to measure the\ntemperature of the device components such as the battery, they can also sense\nchanges in the temperature of the ambient environment or thermal entities in\ncontact with the device. We have developed a model to estimate core body\ntemperature from signals sensed by these thermistors during a user interaction\nin which the user places the capacitive touchscreen of a smart device against a\nthermal site on their body such as their forehead. During the interaction, the\ndevice logs the temperature sensed by the thermistors as well as the raw\ncapacitance seen by the touch screen to capture features describing the rate of\nheat transfer from the body to the device and device-to-skin contact\nrespectively. These temperature and contact features are then used to model the\nrate of heat transferred from the user's body to the device and thus core-body\ntemperature of the user for ubiquitous and accessible fever monitoring using\nonly a smart device. We validate this system in a lab environment on a\nsimulated skin-like heat source with a temperature estimate mean absolute error\nof 0.743$^{\\circ}$F (roughly 0.4$^{\\circ}$C) and limit of agreement of\n$\\pm2.374^{\\circ}$F (roughly 1.3$^{\\circ}$C) which is comparable to some\noff-the-shelf peripheral and tympanic thermometers. We found a Pearson's\ncorrelation $R^2$ of 0.837 between ground truth temperature and temperature\nestimated by our system. We also deploy this system in an ongoing clinical\nstudy on a population of 7 participants in a clinical environment to show the\nsimilarity between simulated and clinical trials.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:27:00 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Breda", "Joseph", ""], ["Patel", "Shwetak", ""]]}, {"id": "2106.11988", "submitter": "Chenhao Tan", "authors": "Chenhao Tan", "title": "On the Diversity and Limits of Human Explanations", "comments": "15 pages, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A growing effort in NLP aims to build datasets of human explanations.\nHowever, the term explanation encompasses a broad range of notions, each with\ndifferent properties and ramifications. Our goal is to provide an overview of\ndiverse types of explanations and human limitations, and discuss implications\nfor collecting and using explanations in NLP. Inspired by prior work in\npsychology and cognitive sciences, we group existing human explanations in NLP\ninto three categories: proximal mechanism, evidence, and procedure. These three\ntypes differ in nature and have implications for the resultant explanations.\nFor instance, procedure is not considered explanations in psychology and\nconnects with a rich body of work on learning from instructions. The diversity\nof explanations is further evidenced by proxy questions that are needed for\nannotators to interpret and answer open-ended why questions. Finally,\nexplanations may require different, often deeper, understandings than\npredictions, which casts doubt on whether humans can provide useful\nexplanations in some tasks.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 18:00:07 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Tan", "Chenhao", ""]]}, {"id": "2106.12039", "submitter": "Elena Shmileva", "authors": "Elena Shmileva, Viktor Sarzhan", "title": "Clustering of check-in sequences using the mixture Markov chain process", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work is devoted to the clustering of check-in sequences from a geosocial\nnetwork. We used the mixture Markov chain process as a mathematical model for\ntime-dependent types of data. For clustering, we adjusted the\nExpectation-Maximization (EM) algorithm. As a result, we obtained highly\ndetailed communities (clusters) of users of the now defunct geosocial network,\nWeeplaces.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 18:37:13 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Shmileva", "Elena", ""], ["Sarzhan", "Viktor", ""]]}, {"id": "2106.12044", "submitter": "Ashiqur Rahman KhudaBukhsh", "authors": "Clay H. Yoo, Shriphani Palakodety, Rupak Sarkar, Ashiqur R.\n  KhudaBukhsh", "title": "Empathy and Hope: Resource Transfer to Model Inter-country Social Media\n  Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing COVID-19 pandemic resulted in significant ramifications for\ninternational relations ranging from travel restrictions, global ceasefires,\nand international vaccine production and sharing agreements. Amidst a wave of\ninfections in India that resulted in a systemic breakdown of healthcare\ninfrastructure, a social welfare organization based in Pakistan offered to\nprocure medical-grade oxygen to assist India -- a nation which was involved in\nfour wars with Pakistan in the past few decades. In this paper, we focus on\nPakistani Twitter users' response to the ongoing healthcare crisis in India.\nWhile #IndiaNeedsOxygen and #PakistanStandsWithIndia featured among the\ntop-trending hashtags in Pakistan, divisive hashtags such as\n#EndiaSaySorryToKashmir simultaneously started trending. Against the backdrop\nof a contentious history including four wars, divisive content of this nature,\nespecially when a country is facing an unprecedented healthcare crisis, fuels\nfurther deterioration of relations. In this paper, we define a new task of\ndetecting \\emph{supportive} content and demonstrate that existing \\emph{NLP for\nsocial impact} tools can be effectively harnessed for such tasks within a quick\nturnaround time. We also release the first publicly available data set at the\nintersection of geopolitical relations and a raging pandemic in the context of\nIndia and Pakistan.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 06:31:50 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Yoo", "Clay H.", ""], ["Palakodety", "Shriphani", ""], ["Sarkar", "Rupak", ""], ["KhudaBukhsh", "Ashiqur R.", ""]]}, {"id": "2106.12056", "submitter": "Chenhao Tan", "authors": "Madhusudhan Aithal and Chenhao Tan", "title": "On Positivity Bias in Negative Reviews", "comments": "11 pages, 17 figures, ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prior work has revealed that positive words occur more frequently than\nnegative words in human expressions, which is typically attributed to\npositivity bias, a tendency for people to report positive views of reality. But\nwhat about the language used in negative reviews? Consistent with prior work,\nwe show that English negative reviews tend to contain more positive words than\nnegative words, using a variety of datasets. We reconcile this observation with\nprior findings on the pragmatics of negation, and show that negations are\ncommonly associated with positive words in negative reviews. Furthermore, in\nnegative reviews, the majority of sentences with positive words express\nnegative opinions based on sentiment classifiers, indicating some form of\nnegation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 21:04:25 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Aithal", "Madhusudhan", ""], ["Tan", "Chenhao", ""]]}, {"id": "2106.12083", "submitter": "Frank Cangialosi", "authors": "Frank Cangialosi, Neil Agarwal, Venkat Arun, Junchen Jiang, Srinivas\n  Narayana, Anand Sarwate and Ravi Netravali", "title": "Privid: Practical, Privacy-Preserving Video Analytics Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analytics on video recorded by cameras in public areas have the potential to\nfuel many exciting applications, but also pose the risk of intruding on\nindividuals' privacy. Unfortunately, existing solutions fail to practically\nresolve this tension between utility and privacy, relying on perfect detection\nof all private information in each video frame--an elusive requirement. This\npaper presents: (1) a new notion of differential privacy (DP) for video\nanalytics, $(\\rho,K,\\epsilon)$-event-duration privacy, which protects all\nprivate information visible for less than a particular duration, rather than\nrelying on perfect detections of that information, and (2) a practical system\ncalled Privid that enforces duration-based privacy even with the (untrusted)\nanalyst-provided deep neural networks that are commonplace for video analytics\ntoday. Across a variety of videos and queries, we show that Privid achieves\naccuracies within 79-99% of a non-private system.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 22:25:08 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Cangialosi", "Frank", ""], ["Agarwal", "Neil", ""], ["Arun", "Venkat", ""], ["Jiang", "Junchen", ""], ["Narayana", "Srinivas", ""], ["Sarwate", "Anand", ""], ["Netravali", "Ravi", ""]]}, {"id": "2106.12107", "submitter": "Yusuke Kumakoshi", "authors": "Yusuke Kumakoshi, Hideki Koizumi, Yuji Yoshimura", "title": "Diversity and density of urban functions in station areas", "comments": "17 pages, 5 figures, 3 Tables. To be published in Computers,\n  Environment and Urban Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diversity and density of urban functions have been known to affect urban\nvibrancy positively, but the relation between the two has not been empirically\nexamined; if high density is associated with low diversity in an area, its\nvibrancy may not increase. To obtain a better understanding of the metabolism\nof cities and directions for urban planning interventions, this paper offers\nempirical evidence on the association between the diversity and density of\nurban functions in the Tokyo Metropolitan Area, using a robust density index\nthat was determined via a Monte Carlo simulation. By conducting association\nanalyses, it was found that highly dense station areas tended to display low\ndiversity at multiple scales. Further investigation indicated that this\nnegative correlation was owing to different spatial characteristics of\nfunctions and complementary functioning among highly accessible station areas.\nThis paper argues for considering both diversity and density in urban planning\nto make station areas vibrant and resilient.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 00:51:44 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Kumakoshi", "Yusuke", ""], ["Koizumi", "Hideki", ""], ["Yoshimura", "Yuji", ""]]}, {"id": "2106.12150", "submitter": "Maryam Negahbani", "authors": "Deeparnab Chakrabarty and Maryam Negahbani", "title": "Better Algorithms for Individually Fair $k$-Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study data clustering problems with $\\ell_p$-norm objectives (e.g.\n$k$-Median and $k$-Means) in the context of individual fairness. The dataset\nconsists of $n$ points, and we want to find $k$ centers such that (a) the\nobjective is minimized, while (b) respecting the individual fairness constraint\nthat every point $v$ has a center within a distance at most $r(v)$, where\n$r(v)$ is $v$'s distance to its $(n/k)$th nearest point. Jung, Kannan, and Lutz\n[FORC 2020] introduced this concept and designed a clustering algorithm with\nprovable (approximate) fairness and objective guarantees for the $\\ell_\\infty$\nor $k$-Center objective. Mahabadi and Vakilian [ICML 2020] revisited this\nproblem to give a local-search algorithm for all $\\ell_p$-norms. Empirically,\ntheir algorithms outperform Jung et. al.'s by a large margin in terms of cost\n(for $k$-Median and $k$-Means), but they incur a reasonable loss in fairness.\nIn this paper, our main contribution is to use Linear Programming (LP)\ntechniques to obtain better algorithms for this problem, both in theory and in\npractice. We prove that by modifying known LP rounding techniques, one gets a\nworst-case guarantee on the objective which is much better than in MV20, and\nempirically, this objective is extremely close to the optimal. Furthermore, our\ntheoretical fairness guarantees are comparable with MV20 in theory, and\nempirically, we obtain noticeably fairer solutions. Although solving the LP\n{\\em exactly} might be prohibitive, we demonstrate that in practice, a simple\nsparsification technique drastically improves the run-time of our algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 04:16:46 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Chakrabarty", "Deeparnab", ""], ["Negahbani", "Maryam", ""]]}, {"id": "2106.12187", "submitter": "Rafael Diniz", "authors": "Rafael Diniz, Myl\\`ene C. Q. Farias", "title": "Enlaces de r\\'adio de longa dist\\^ancia utilizando a banda de HF", "comments": "in Portuguese. Article present at SET eXperience 2020. Video\n  presentation at: https://youtu.be/shkyEcHl7Fk", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The interest in the use of the HF band in telecommunication has increased\nsignificantly in the last decade, mainly due to the development of new\nstandards for military telecommunications in HF, as well as the expansion of\ndigital broadcasting in the HF band. More specifically, these new standards\nallow the implementation of links of hundreds or thousands of kilometers at a\nlow cost, which suggests a widespread adoption can occur. In Brazil, this type\nof communication can be used in remote regions or regions of difficult access,\nsuch as the Amazon rain-forest region. In addition to the evolution of\ntechnologies concerning the physical layer of the HF telecommunication systems,\nthere has been a great development of techniques that use machine learning\nalgorithms for audio and image coding. It is believed that all these advances\nwill enable the use of the HF band for communication services in places without\ntelecommunication infrastructure. This work presents recent applications of HF\nradio for digital links in Brazil, describing the challenges present for the\ndevelopment of telecommunication systems in the HF band.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 06:25:12 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Diniz", "Rafael", ""], ["Farias", "Myl\u00e8ne C. Q.", ""]]}, {"id": "2106.12223", "submitter": "Mattia Falduti", "authors": "A. De Angeli, M. Falduti, M. Menendez Blanco, S. Tessaris", "title": "Reporting Revenge Porn: a Preliminary Expert Analysis", "comments": null, "journal-ref": null, "doi": "10.1145/3464385.3464739", "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our research, we focus on the response to the non-consensual distribution\nof intimate or sexually explicit digital images of adults, also referred as\nrevenge porn, from the point of view of the victims. In this paper, we present\na preliminary expert analysis of the process for reporting revenge porn abuses\nin selected content sharing platforms. Among these, we included social\nnetworks, image hosting websites, video hosting platforms, forums, and\npornographic sites. We looked at the way to report abuse, concerning both the\nnon-consensual online distribution of private sexual image or video (revenge\npornography), as well as the use of deepfake techniques, where the face of a\nperson can be replaced on original visual content with the aim of portraying\nthe victim in the context of sexual behaviours. This preliminary analysis is\ndirected to understand the current practices and potential issues in the\nprocedures designed by the providers for reporting these abuses.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 08:08:59 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["De Angeli", "A.", ""], ["Falduti", "M.", ""], ["Blanco", "M. Menendez", ""], ["Tessaris", "S.", ""]]}, {"id": "2106.12242", "submitter": "Gilles Stoltz", "authors": "Evgenii Chzhen (LMO, CELESTE), Christophe Giraud (LMO, CELESTE),\n  Gilles Stoltz (LMO, CELESTE)", "title": "A Unified Approach to Fair Online Learning via Blackwell Approachability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a setting and a general approach to fair online learning with\nstochastic sensitive and non-sensitive contexts. The setting is a repeated game\nbetween the Player and Nature, where at each stage both pick actions based on\nthe contexts. Inspired by the notion of unawareness, we assume that the Player\ncan only access the non-sensitive context before making a decision, while we\ndiscuss both cases of Nature accessing the sensitive contexts and Nature\nunaware of the sensitive contexts. Adapting Blackwell's approachability theory\nto handle the case of an unknown contexts' distribution, we provide a general\nnecessary and sufficient condition for learning objectives to be compatible\nwith some fairness constraints. This condition is instantiated on (group-wise)\nno-regret and (group-wise) calibration objectives, and on demographic parity as\nan additional constraint. When the objective is not compatible with the\nconstraint, the provided framework permits to characterise the optimal\ntrade-off between the two.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 09:00:12 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Chzhen", "Evgenii", "", "LMO, CELESTE"], ["Giraud", "Christophe", "", "LMO, CELESTE"], ["Stoltz", "Gilles", "", "LMO, CELESTE"]]}, {"id": "2106.12403", "submitter": "Emanuel Moss", "authors": "Mona Sloane, Emanuel Moss, Rumman Chowdhury", "title": "A Silicon Valley Love Triangle: Hiring Algorithms, Pseudo-Science, and\n  the Quest for Auditability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we suggest a systematic approach for developing\nsocio-technical assessment for hiring ADS. We suggest using a matrix to expose\nunderlying assumptions rooted in pseudoscientific essentialized understandings\nof human nature and capability, and to critically investigate emerging auditing\nstandards and practices that fail to address these assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 13:44:12 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Sloane", "Mona", ""], ["Moss", "Emanuel", ""], ["Chowdhury", "Rumman", ""]]}, {"id": "2106.12458", "submitter": "Sarah E Carter", "authors": "Sarah E. Carter", "title": "Is Downloading this App Consistent with my Values? Conceptualizing a\n  Value-Centered Privacy Assistant", "comments": "This is an Author Accepted Manuscript for I3E2021, to be held in\n  September 2021. The final version will be available at Springer. A link and\n  the DOI will be added in due course", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Digital privacy notices aim to provide users with information to make\ninformed decisions. They are, however, fraught with difficulties. Instead, I\npropose that data privacy decisions can be understood as an expression of user\nvalues. To optimize this value expression, I further propose the creation of a\nvalue-centered privacy assistant (VcPA). Here, I preliminary explore how a VcPA\ncould enhance user value expression by utilizing three user scenarios in the\ncontext of considering whether or not to download an environmental application,\nthe OpenLitterMap app. These scenarios are conceptually constructed from\nestablished privacy user groups - the privacy fundamentalists; the privacy\npragmatists; and the privacy unconcerned. I conclude that the VcPA best\nfacilitates user value expression of the privacy fundamentalists. In contrast,\nthe value expression of the privacy pragmatists and the privacy unconcerned\ncould be enhanced or hindered depending on the context and their internal\nstates. Possible implications for optimal VcPA design are also discussed.\nFollowing this initial conceptual exploration of VcPAs, further empirical\nresearch will be required to demonstrate the effectiveness of the VcPA system\nin real-world settings.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 15:08:58 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Carter", "Sarah E.", ""]]}, {"id": "2106.12674", "submitter": "Mengnan Du", "authors": "Mengnan Du, Subhabrata Mukherjee, Guanchu Wang, Ruixiang Tang, Ahmed\n  Hassan Awadallah, Xia Hu", "title": "Fairness via Representation Neutralization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing bias mitigation methods for DNN models primarily work on learning\ndebiased encoders. This process not only requires a lot of instance-level\nannotations for sensitive attributes, it also does not guarantee that all\nfairness sensitive information has been removed from the encoder. To address\nthese limitations, we explore the following research question: Can we reduce\nthe discrimination of DNN models by only debiasing the classification head,\neven with biased representations as inputs? To this end, we propose a new\nmitigation technique, namely, Representation Neutralization for Fairness (RNF)\nthat achieves fairness by debiasing only the task-specific classification head\nof DNN models. To this end, we leverage samples with the same ground-truth\nlabel but different sensitive attributes, and use their neutralized\nrepresentations to train the classification head of the DNN model. The key idea\nof RNF is to discourage the classification head from capturing spurious\ncorrelation between fairness sensitive information in encoder representations\nwith specific class labels. To address low-resource settings with no access to\nsensitive attribute annotations, we leverage a bias-amplified model to generate\nproxy annotations for sensitive attributes. Experimental results over several\nbenchmark datasets demonstrate our RNF framework to effectively reduce\ndiscrimination of DNN models with minimal degradation in task-specific\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 22:26:29 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Du", "Mengnan", ""], ["Mukherjee", "Subhabrata", ""], ["Wang", "Guanchu", ""], ["Tang", "Ruixiang", ""], ["Awadallah", "Ahmed Hassan", ""], ["Hu", "Xia", ""]]}, {"id": "2106.12705", "submitter": "Meena Jagadeesan", "authors": "Meena Jagadeesan, Celestine Mendler-D\\\"unner, Moritz Hardt", "title": "Alternative Microfoundations for Strategic Classification", "comments": "Accepted for publication at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.GT econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When reasoning about strategic behavior in a machine learning context it is\ntempting to combine standard microfoundations of rational agents with the\nstatistical decision theory underlying classification. In this work, we argue\nthat a direct combination of these standard ingredients leads to brittle\nsolution concepts of limited descriptive and prescriptive value. First, we show\nthat rational agents with perfect information produce discontinuities in the\naggregate response to a decision rule that we often do not observe empirically.\nSecond, when any positive fraction of agents is not perfectly strategic,\ndesirable stable points -- where the classifier is optimal for the data it\nentails -- cease to exist. Third, optimal decision rules under standard\nmicrofoundations maximize a measure of negative externality known as social\nburden within a broad class of possible assumptions about agent behavior.\n  Recognizing these limitations we explore alternatives to standard\nmicrofoundations for binary classification. We start by describing a set of\ndesiderata that help navigate the space of possible assumptions about how\nagents respond to a decision rule. In particular, we analyze a natural\nconstraint on feature manipulations, and discuss properties that are sufficient\nto guarantee the robust existence of stable points. Building on these insights,\nwe then propose the noisy response model. Inspired by smoothed analysis and\nempirical observations, noisy response incorporates imperfection in the agent\nresponses, which we show mitigates the limitations of standard\nmicrofoundations. Our model retains analytical tractability, leads to more\nrobust insights about stable points, and imposes a lower social burden at\noptimality.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 00:30:58 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Jagadeesan", "Meena", ""], ["Mendler-D\u00fcnner", "Celestine", ""], ["Hardt", "Moritz", ""]]}, {"id": "2106.12793", "submitter": "Lukas Daniel Klausner", "authors": "Stefan More and Peter Grassberger and Felix H\\\"orandner and Andreas\n  Abraham and Lukas Daniel Klausner", "title": "Trust Me If You Can: Trusted Transformation Between (JSON) Schemas to\n  Support Global Authentication of Education Credentials", "comments": "16 pages, 4 figures", "journal-ref": "Proceedings of the 36th IFIP TC 11 International Conference on ICT\n  Systems Security and Privacy Protection (IFIP SEC 2021)/IFIP Advances in\n  Information and Communication Technology, vol. 625, 2021, 19-35", "doi": "10.1007/978-3-030-78120-0_2", "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recruiters and institutions around the world struggle with the verification\nof diplomas issued in a diverse and global education setting. Firstly, it is a\nnontrivial problem to identify bogus institutions selling education\ncredentials. While institutions are often accredited by qualified authorities\non a regional level, there is no global authority fulfilling this task.\nSecondly, many different data schemas are used to encode education credentials,\nwhich represents a considerable challenge to automated processing.\nConsequently, significant manual effort is required to verify credentials.\n  In this paper, we tackle these challenges by introducing a decentralized and\nopen system to automatically verify the legitimacy of issuers and interpret\ncredentials in unknown schemas. We do so by enabling participants to publish\ntransformation information, which enables verifiers to transform credentials\ninto their preferred schema. Due to the lack of a global root of trust, we\nutilize a distributed ledger to build a decentralized web of trust, which\nverifiers can query to gather information on the trustworthiness of issuing\ninstitutions and to establish trust in transformation information. Going beyond\ndiploma fraud, our system can be generalized to tackle the generalized problem\nfor other domains lacking a root of trust and agreements on data schemas.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 07:03:23 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["More", "Stefan", ""], ["Grassberger", "Peter", ""], ["H\u00f6randner", "Felix", ""], ["Abraham", "Andreas", ""], ["Klausner", "Lukas Daniel", ""]]}, {"id": "2106.12794", "submitter": "Lukas Daniel Klausner", "authors": "Angelika Adensamer and Lukas Daniel Klausner", "title": "\"Part Man, Part Machine, All Cop\": Automation in Policing", "comments": "21 pages", "journal-ref": "Front. Artif. Intell. 4, 2021", "doi": "10.3389/frai.2021.655486", "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitisation, automation and datafication permeate policing and justice more\nand more each year -- from predictive policing methods through recidivism\nprediction to automated biometric identification at the border. The\nsociotechnical issues surrounding the use of such systems raise questions and\nreveal problems, both old and new. Our article reviews contemporary issues\nsurrounding automation in policing and the legal system, finds common issues\nand themes in various different examples, introduces the distinction between\nhuman \"retail bias\" and algorithmic \"wholesale bias\", and argues for shifting\nthe viewpoint on the debate to focus on both workers' rights and organisational\nresponsibility as well as fundamental rights and the right to an effective\nremedy.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 07:04:01 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Adensamer", "Angelika", ""], ["Klausner", "Lukas Daniel", ""]]}, {"id": "2106.12885", "submitter": "Zhan Zhao", "authors": "Zhan Zhao, Haris N. Koutsopoulos, Jinhua Zhao", "title": "Identifying Hidden Visits from Sparse Call Detail Record Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite a large body of literature on trip inference using call detail record\n(CDR) data, a fundamental understanding of their limitations is lacking. In\nparticular, because of the sparse nature of CDR data, users may travel to a\nlocation without being revealed in the data, which we refer to as a \"hidden\nvisit\". The existence of hidden visits hinders our ability to extract reliable\ninformation about human mobility and travel behavior from CDR data. In this\nstudy, we propose a data fusion approach to obtain labeled data for statistical\ninference of hidden visits. In the absence of complementary data, this can be\naccomplished by extracting labeled observations from more granular cellular\ndata access records, and extracting features from voice call and text messaging\nrecords. The proposed approach is demonstrated using a real-world CDR dataset\nof 3 million users from a large Chinese city. Logistic regression, support\nvector machine, random forest, and gradient boosting are used to infer whether\na hidden visit exists during a displacement observed from CDR data. The test\nresults show significant improvement over the naive no-hidden-visit rule, which\nis an implicit assumption adopted by most existing studies. Based on the\nproposed model, we estimate that over 10% of the displacements extracted from\nCDR data involve hidden visits. The proposed data fusion method offers a\nsystematic statistical approach to inferring individual mobility patterns based\non telecommunication records.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 10:41:47 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Zhao", "Zhan", ""], ["Koutsopoulos", "Haris N.", ""], ["Zhao", "Jinhua", ""]]}, {"id": "2106.12905", "submitter": "Francisco Aparecido Rodrigues", "authors": "Kirstin Roster and Francisco A. Rodrigues", "title": "Neural Networks for Dengue Prediction: A Systematic Review", "comments": "16 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to a lack of treatments and universal vaccine, early forecasts of Dengue\nare an important tool for disease control. Neural networks are powerful\npredictive models that have made contributions to many areas of public health.\nIn this systematic review, we provide an introduction to the neural networks\nrelevant to Dengue forecasting and review their applications in the literature.\nThe objective is to help inform model design for future work. Following the\nPRISMA guidelines, we conduct a systematic search of studies that use neural\nnetworks to forecast Dengue in human populations. We summarize the relative\nperformance of neural networks and comparator models, model architectures and\nhyper-parameters, as well as choices of input features. Nineteen papers were\nincluded. Most studies implement shallow neural networks using historical\nDengue incidence and meteorological input features. Prediction horizons tend to\nbe short. Building on the strengths of neural networks, most studies use\ngranular observations at the city or sub-national level. Performance of neural\nnetworks relative to comparators such as Support Vector Machines varies across\nstudy contexts. The studies suggest that neural networks can provide good\npredictions of Dengue and should be included in the set of candidate models.\nThe use of convolutional, recurrent, or deep networks is relatively unexplored\nbut offers promising avenues for further research, as does the use of a broader\nset of input features such as social media or mobile phone data.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 20:01:31 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Roster", "Kirstin", ""], ["Rodrigues", "Francisco A.", ""]]}, {"id": "2106.12921", "submitter": "Georgios Feretzakis", "authors": "Georgios Feretzakis, George Karlis, Evangelos Loupelis, Dimitris\n  Kalles, Rea Chatzikyriakou, Nikolaos Trakas, Eugenia Karakou, Aikaterini\n  Sakagianni, Lazaros Tzelves, Stavroula Petropoulou, Aikaterini Tika, Ilias\n  Dalainas and Vasileios Kaldis", "title": "Using machine learning techniques to predict hospital admission at the\n  emergency department", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Introduction: One of the most important tasks in the Emergency Department\n(ED) is to promptly identify the patients who will benefit from hospital\nadmission. Machine Learning (ML) techniques show promise as diagnostic aids in\nhealthcare. Material and methods: We investigated the following features\nseeking to investigate their performance in predicting hospital admission:\nserum levels of Urea, Creatinine, Lactate Dehydrogenase, Creatine Kinase,\nC-Reactive Protein, Complete Blood Count with differential, Activated Partial\nThromboplastin Time, D Dimer, International Normalized Ratio, age, gender,\ntriage disposition to ED unit and ambulance utilization. A total of 3,204 ED\nvisits were analyzed. Results: The proposed algorithms generated models which\ndemonstrated acceptable performance in predicting hospital admission of ED\npatients. The range of F-measure and ROC Area values of all eight evaluated\nalgorithms were [0.679-0.708] and [0.734-0.774], respectively. Discussion: The\nmain advantages of this tool include easy access, availability, yes/no result,\nand low cost. The clinical implications of our approach might facilitate a\nshift from traditional clinical decision-making to a more sophisticated model.\nConclusion: Developing robust prognostic models with the utilization of common\nbiomarkers is a project that might shape the future of emergency medicine. Our\nfindings warrant confirmation with implementation in pragmatic ED trials.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 16:37:37 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 16:08:02 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Feretzakis", "Georgios", ""], ["Karlis", "George", ""], ["Loupelis", "Evangelos", ""], ["Kalles", "Dimitris", ""], ["Chatzikyriakou", "Rea", ""], ["Trakas", "Nikolaos", ""], ["Karakou", "Eugenia", ""], ["Sakagianni", "Aikaterini", ""], ["Tzelves", "Lazaros", ""], ["Petropoulou", "Stavroula", ""], ["Tika", "Aikaterini", ""], ["Dalainas", "Ilias", ""], ["Kaldis", "Vasileios", ""]]}, {"id": "2106.13186", "submitter": "Daniel P. Lopresti", "authors": "Nadya Bliss, Mark Briers, Alice Eckstein, James Goulding, Daniel P.\n  Lopresti, Anjali Mazumder, and Gavin Smith", "title": "CCC/Code 8.7: Applying AI in the Fight Against Modern Slavery", "comments": "A Computing Community Consortium (CCC) workshop report, 24 pages", "journal-ref": null, "doi": null, "report-no": "ccc2021report_1", "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  On any given day, tens of millions of people find themselves trapped in\ninstances of modern slavery. The terms \"human trafficking,\" \"trafficking in\npersons,\" and \"modern slavery\" are sometimes used interchangeably to refer to\nboth sex trafficking and forced labor. Human trafficking occurs when a\ntrafficker compels someone to provide labor or services through the use of\nforce, fraud, and/or coercion. The wide range of stakeholders in human\ntrafficking presents major challenges. Direct stakeholders are law enforcement,\nNGOs and INGOs, businesses, local/planning government authorities, and\nsurvivors. Viewed from a very high level, all stakeholders share in a rich\nnetwork of interactions that produce and consume enormous amounts of\ninformation. The problems of making efficient use of such information for the\npurposes of fighting trafficking while at the same time adhering to community\nstandards of privacy and ethics are formidable. At the same time they help us,\ntechnologies that increase surveillance of populations can also undermine basic\nhuman rights.\n  In early March 2020, the Computing Community Consortium (CCC), in\ncollaboration with the Code 8.7 Initiative, brought together over fifty members\nof the computing research community along with anti-slavery practitioners and\nsurvivors to lay out a research roadmap. The primary goal was to explore ways\nin which long-range research in artificial intelligence (AI) could be applied\nto the fight against human trafficking. Building on the kickoff Code 8.7\nconference held at the headquarters of the United Nations in February 2019, the\nfocus for this workshop was to link the ambitious goals outlined in the A\n20-Year Community Roadmap for Artificial Intelligence Research in the US (AI\nRoadmap) to challenges vital in achieving the UN's Sustainable Development Goal\nTarget 8.7, the elimination of modern slavery.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 17:07:56 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Bliss", "Nadya", ""], ["Briers", "Mark", ""], ["Eckstein", "Alice", ""], ["Goulding", "James", ""], ["Lopresti", "Daniel P.", ""], ["Mazumder", "Anjali", ""], ["Smith", "Gavin", ""]]}, {"id": "2106.13219", "submitter": "Paul Pu Liang", "authors": "Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, Ruslan Salakhutdinov", "title": "Towards Understanding and Mitigating Social Biases in Language Models", "comments": "ICML 2021, code available at https://github.com/pliang279/LM_bias", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning methods are deployed in real-world settings such as\nhealthcare, legal systems, and social science, it is crucial to recognize how\nthey shape social biases and stereotypes in these sensitive decision-making\nprocesses. Among such real-world deployments are large-scale pretrained\nlanguage models (LMs) that can be potentially dangerous in manifesting\nundesirable representational biases - harmful biases resulting from\nstereotyping that propagate negative generalizations involving gender, race,\nreligion, and other social constructs. As a step towards improving the fairness\nof LMs, we carefully define several sources of representational biases before\nproposing new benchmarks and metrics to measure them. With these tools, we\npropose steps towards mitigating social biases during text generation. Our\nempirical results and human evaluation demonstrate effectiveness in mitigating\nbias while retaining crucial contextual information for high-fidelity text\ngeneration, thereby pushing forward the performance-fairness Pareto frontier.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 17:52:43 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Liang", "Paul Pu", ""], ["Wu", "Chiyu", ""], ["Morency", "Louis-Philippe", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "2106.13271", "submitter": "Deepak P", "authors": "Deepak P, Sanil V, Joemon M. Jose", "title": "On Fairness and Interpretability", "comments": "in IJCAI 2021 Workshop on AI for Social Good, January 2021. [ Ref:\n  https://crcs.seas.harvard.edu/publications/fairness-and-interpretability ]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ethical AI spans a gamut of considerations. Among these, the most popular\nones, fairness and interpretability, have remained largely distinct in\ntechnical pursuits. We discuss and elucidate the differences between fairness\nand interpretability across a variety of dimensions. Further, we develop two\nprinciples-based frameworks towards developing ethical AI for the future that\nembrace aspects of both fairness and interpretability. First, interpretability\nfor fairness proposes instantiating interpretability within the realm of\nfairness to develop a new breed of ethical AI. Second, fairness and\ninterpretability initiates deliberations on bringing the best aspects of both\ntogether. We hope that these two frameworks will contribute to intensifying\nscholarly discussions on new frontiers of ethical AI that brings together\nfairness and interpretability.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 18:48:46 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["P", "Deepak", ""], ["V", "Sanil", ""], ["Jose", "Joemon M.", ""]]}, {"id": "2106.13346", "submitter": "Jessica Dai", "authors": "Jessica Dai, Sohini Upadhyay, Stephen H. Bach, Himabindu Lakkaraju", "title": "What will it take to generate fairness-preserving explanations?", "comments": "Presented at ICML 2021 Workshop on Theoretic Foundation, Criticism,\n  and Application Trend of Explainable AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In situations where explanations of black-box models may be useful, the\nfairness of the black-box is also often a relevant concern. However, the link\nbetween the fairness of the black-box model and the behavior of explanations\nfor the black-box is unclear. We focus on explanations applied to tabular\ndatasets, suggesting that explanations do not necessarily preserve the fairness\nproperties of the black-box algorithm. In other words, explanation algorithms\ncan ignore or obscure critical relevant properties, creating incorrect or\nmisleading explanations. More broadly, we propose future research directions\nfor evaluating and generating explanations such that they are informative and\nrelevant from a fairness perspective.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 23:03:25 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Dai", "Jessica", ""], ["Upadhyay", "Sohini", ""], ["Bach", "Stephen H.", ""], ["Lakkaraju", "Himabindu", ""]]}, {"id": "2106.13386", "submitter": "Weiwen Liu", "authors": "Weiwen Liu, Feng Liu, Ruiming Tang, Ben Liao, Guangyong Chen, Pheng\n  Ann Heng", "title": "Balancing Accuracy and Fairness for Interactive Recommendation with\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Fairness in recommendation has attracted increasing attention due to bias and\ndiscrimination possibly caused by traditional recommenders. In Interactive\nRecommender Systems (IRS), user preferences and the system's fairness status\nare constantly changing over time. Existing fairness-aware recommenders mainly\nconsider fairness in static settings. Directly applying existing methods to IRS\nwill result in poor recommendation. To resolve this problem, we propose a\nreinforcement learning based framework, FairRec, to dynamically maintain a\nlong-term balance between accuracy and fairness in IRS. User preferences and\nthe system's fairness status are jointly compressed into the state\nrepresentation to generate recommendations. FairRec aims at maximizing our\ndesigned cumulative reward that combines accuracy and fairness. Extensive\nexperiments validate that FairRec can improve fairness, while preserving good\nrecommendation quality.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 02:02:51 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Liu", "Weiwen", ""], ["Liu", "Feng", ""], ["Tang", "Ruiming", ""], ["Liao", "Ben", ""], ["Chen", "Guangyong", ""], ["Heng", "Pheng Ann", ""]]}, {"id": "2106.13455", "submitter": "Rajiv Movva", "authors": "Rajiv Movva", "title": "Fairness Deconstructed: A Sociotechnical View of 'Fair' Algorithms in\n  Criminal Justice", "comments": "9 pages + Works Cited", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Early studies of risk assessment algorithms used in criminal justice revealed\nwidespread racial biases. In response, machine learning researchers have\ndeveloped methods for fairness, many of which rely on equalizing empirical\nmetrics across protected attributes. Here, I recall sociotechnical perspectives\nto delineate the significant gap between fairness in theory and practice,\nfocusing on criminal justice. I (1) illustrate how social context can undermine\nanalyses that are restricted to an AI system's outputs, and (2) argue that much\nof the fair ML literature fails to account for epistemological issues with\nunderlying crime data. Instead of building AI that reifies power imbalances,\nlike risk assessment algorithms, I ask whether data science can be used to\nunderstand the root causes of structural marginalization.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 06:52:49 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Movva", "Rajiv", ""]]}, {"id": "2106.13475", "submitter": "Clayton Miller", "authors": "Clayton Miller, Bianca Picchetti, Chun Fu, Jovan Pantelic", "title": "Limitations of machine learning for building energy prediction: ASHRAE\n  Great Energy Predictor III Kaggle competition error analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning for building energy prediction has exploded in popularity in\nrecent years, yet understanding its limitations and potential for improvement\nare lacking. The ASHRAE Great Energy Predictor III (GEPIII) Kaggle competition\nwas the largest building energy meter machine learning competition ever held\nwith 4,370 participants who submitted 39,403 predictions. The test data set\nincluded two years of hourly electricity, hot water, chilled water, and steam\nreadings from 2,380 meters in 1,448 buildings at 16 locations. This paper\nanalyzes the various sources and types of residual model error from an\naggregation of the competition's top 50 solutions. This analysis reveals the\nlimitations for machine learning using the standard model inputs of historical\nmeter, weather, and basic building metadata. The types of error are classified\naccording to the amount of time errors occur in each instance, abrupt versus\ngradual behavior, the magnitude of error, and whether the error existed on\nsingle buildings or several buildings at once from a single location. The\nresults show machine learning models have errors within a range of\nacceptability on 79.1% of the test data. Lower magnitude model errors occur in\n16.1% of the test data. These discrepancies can likely be addressed through\nadditional training data sources or innovations in machine learning. Higher\nmagnitude errors occur in 4.8% of the test data and are unlikely to be\naccurately predicted regardless of innovation. There is a diversity of error\nbehavior depending on the energy meter type (electricity prediction models have\nunacceptable error in under 10% of test data, while hot water is over 60%) and\nbuilding use type (public service less than 14%, while technology/science is\njust over 46%).\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 07:40:42 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Miller", "Clayton", ""], ["Picchetti", "Bianca", ""], ["Fu", "Chun", ""], ["Pantelic", "Jovan", ""]]}, {"id": "2106.13614", "submitter": "Ahmed Shokry", "authors": "Chen Gu, Ahmed Shokry, Moustafa Youssef", "title": "The Effect of Ground Truth Accuracy on the Evaluation of Localization\n  Systems", "comments": null, "journal-ref": "40th IEEE International Conference on Computer Communications\n  (INFOCOM), Virtual Conference (2021)", "doi": null, "report-no": null, "categories": "eess.SP cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ability to accurately evaluate the performance of location determination\nsystems is crucial for many applications. Typically, the performance of such\nsystems is obtained by comparing ground truth locations with estimated\nlocations. However, these ground truth locations are usually obtained by\nclicking on a map or using other worldwide available technologies like GPS.\nThis introduces ground truth errors that are due to the marking process, map\ndistortions, or inherent GPS inaccuracy.\n  In this paper, we present a theoretical framework for analyzing the effect of\nground truth errors on the evaluation of localization systems. Based on that,\nwe design two algorithms for computing the real algorithmic error from the\nvalidation error and marking/map ground truth errors, respectively. We further\nestablish bounds on different performance metrics.\n  Validation of our theoretical assumptions and analysis using real data\ncollected in a typical environment shows the ability of our theoretical\nframework to correct the estimated error of a localization algorithm in the\npresence of ground truth errors. Specifically, our marking error algorithm\nmatches the real error CDF within 4%, and our map error algorithm provides a\nmore accurate estimate of the median/tail error by 150%/72% when the map is\nshifted by 6m.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 13:07:59 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Gu", "Chen", ""], ["Shokry", "Ahmed", ""], ["Youssef", "Moustafa", ""]]}, {"id": "2106.13632", "submitter": "Ahmed Shokry", "authors": "Ahmed Shokry, Marwan Torki, Moustafa Youssef", "title": "DeepLoc: A Ubiquitous Accurate and Low-Overhead Outdoor Cellular\n  Localization System", "comments": null, "journal-ref": "SIGSPATIAL '18: Proceedings of the 26th ACM SIGSPATIAL\n  International Conference on Advances in Geographic Information\n  SystemsNovember 2018", "doi": "10.1145/3274895.3274909", "report-no": null, "categories": "cs.LG cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have witnessed fast growth in outdoor location-based services.\nWhile GPS is considered a ubiquitous localization system, it is not supported\nby low-end phones, requires direct line of sight to the satellites, and can\ndrain the phone battery quickly.\n  In this paper, we propose DeepLoc: a deep learning-based outdoor localization\nsystem that obtains GPS-like localization accuracy without its limitations. In\nparticular, DeepLoc leverages the ubiquitous cellular signals received from the\ndifferent cell towers heard by the mobile device as hints to localize it. To do\nthat, crowd-sensed geo-tagged received signal strength information coming from\ndifferent cell towers is used to train a deep model that is used to infer the\nuser's position. As part of DeepLoc design, we introduce modules to address a\nnumber of practical challenges including scaling the data collection to large\nareas, handling the inherent noise in the cellular signal and geo-tagged data,\nas well as providing enough data that is required for deep learning models with\nlow-overhead.\n  We implemented DeepLoc on different Android devices. Evaluation results in\nrealistic urban and rural environments show that DeepLoc can achieve a median\nlocalization accuracy within 18.8m in urban areas and within 15.7m in rural\nareas. This accuracy outperforms the state-of-the-art cellular-based systems by\nmore than 470% and comes with 330% savings in power compared to the GPS. This\nhighlights the promise of DeepLoc as a ubiquitous accurate and low-overhead\nlocalization system.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 13:34:40 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Shokry", "Ahmed", ""], ["Torki", "Marwan", ""], ["Youssef", "Moustafa", ""]]}, {"id": "2106.13663", "submitter": "Ahmed Shokry", "authors": "Ahmed Shokry, Moustafa Elhamshary, Moustafa Youssef", "title": "The Tale of Two Localization Technologies: Enabling Accurate\n  Low-Overhead WiFi-based Localization for Low-end Phones", "comments": null, "journal-ref": "SIGSPATIAL '17: Proceedings of the 25th ACM SIGSPATIAL\n  International Conference on Advances in Geographic Information Systems.\n  November 2017", "doi": "10.1145/3139958.3139989", "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  WiFi fingerprinting is one of the mainstream technologies for indoor\nlocalization. However, it requires an initial calibration phase during which\nthe fingerprint database is built manually. This process is labour intensive\nand needs to be repeated with any change in the environment. While a number of\nsystems have been introduced to reduce the calibration effort through RF\npropagation models or crowdsourcing, these still have some limitations. Other\napproaches use the recently developed iBeacon technology as an alternative to\nWiFi for indoor localization. However, these beacon-based solutions are limited\nto a small subset of high-end phones. In this paper, we present HybridLoc: an\naccurate low-overhead indoor localization system. The basic idea HybridLoc\nbuilds on is to leverage the sensors of high-end phones to enable localization\nof lower-end phones. Specifically, the WiFi fingerprint is crowdsourced by\nopportunistically collecting WiFi-scans labeled with location data obtained\nfrom BLE-enabled high-end smart phones. These scans are used to automatically\nconstruct the WiFi-fingerprint, that is used later to localize any lower-end\ncell phone with the ubiquitous WiFi technology. HybridLoc also has provisions\nfor handling the inherent error in the estimated BLE locations used in\nconstructing the fingerprint as well as to handle practical deployment issues\nincluding the noisy wireless environment, heterogeneous devices, among others.\nEvaluation of HybridLoc using Android phones shows that it can provide accurate\nlocalization in the same range as manual fingerprinting techniques under the\nsame conditions. Moreover, the localization accuracy on low-end phones\nsupporting only WiFi is comparable to that achieved with high-end phones\nsupporting BLE. This accuracy is achieved with no training overhead, is robust\nto the different user devices, and is consistent under environment changes.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 14:31:26 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Shokry", "Ahmed", ""], ["Elhamshary", "Moustafa", ""], ["Youssef", "Moustafa", ""]]}, {"id": "2106.13750", "submitter": "Ioannis Kavouras A", "authors": "Ioannis Kavouras, Eftychios Protopapadakis, Maria Kaselimia, Emmanuel\n  Sardis, Nikolaos Doulamis", "title": "Assessing the Lockdown Effects on Air Quality during COVID-19 Era", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we investigate the short-term variations in air quality\nemissions, attributed to the prevention measures, applied in different cities,\nto mitigate the COVID-19 spread. In particular, we emphasize on the\nconcentration effects regarding specific pollutant gases, such as carbon\nmonoxide (CO), ozone (O3), nitrogen dioxide (NO2) and sulphur dioxide (SO2).\nThe assessment of the impact of lockdown on air quality focused on four\nEuropean Cities (Athens, Gladsaxe, Lodz and Rome). Available data on pollutant\nfactors were obtained using global satellite observations. The level of the\nemployed prevention measures is employed using the Oxford COVID-19 Government\nResponse Tracker. The second part of the analysis employed a variety of machine\nlearning tools, utilized for estimating the concentration of each pollutant,\ntwo days ahead. The results showed that a weak to moderate correlation exists\nbetween the corresponding measures and the pollutant factors and that it is\npossible to create models which can predict the behaviour of the pollutant\ngases under daily human activities.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 16:39:44 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Kavouras", "Ioannis", ""], ["Protopapadakis", "Eftychios", ""], ["Kaselimia", "Maria", ""], ["Sardis", "Emmanuel", ""], ["Doulamis", "Nikolaos", ""]]}, {"id": "2106.13901", "submitter": "Ramya Srinivasan", "authors": "Ramya Srinivasan and Devi Parikh", "title": "Building Bridges: Generative Artworks to Explore AI Ethics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years, there has been an increased emphasis on understanding and\nmitigating adverse impacts of artificial intelligence (AI) technologies on\nsociety. Across academia, industry, and government bodies, a variety of\nendeavours are being pursued towards enhancing AI ethics. A significant\nchallenge in the design of ethical AI systems is that there are multiple\nstakeholders in the AI pipeline, each with their own set of constraints and\ninterests. These different perspectives are often not understood, due in part\nto communication gaps.For example, AI researchers who design and develop AI\nmodels are not necessarily aware of the instability induced in consumers' lives\nby the compounded effects of AI decisions. Educating different stakeholders\nabout their roles and responsibilities in the broader context becomes\nnecessary. In this position paper, we outline some potential ways in which\ngenerative artworks can play this role by serving as accessible and powerful\neducational tools for surfacing different perspectives. We hope to spark\ninterdisciplinary discussions about computational creativity broadly as a tool\nfor enhancing AI ethics.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 22:31:55 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Srinivasan", "Ramya", ""], ["Parikh", "Devi", ""]]}, {"id": "2106.13932", "submitter": "Jukka Ruohonen", "authors": "Jukka Ruohonen and Anne-Marie Tuikka", "title": "Digital Divides and Online Media", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital divide has been a common concern during the past two or three\ndecades; traditionally, it refers to a gap between developed and developing\ncountries in the adoption and use of digital technologies. Given the importance\nof the topic, digital divide has been also extensively studied, although,\nhitherto, there is no previous research that would have linked the concept to\nonline media. Given this gap in the literature, this paper evaluates the\n\"maturity\" of online media in 134 countries between 2007 and 2016. Maturity is\ndefined according to the levels of national online media consumption, diversity\nof political perspectives presented in national online media, and consensus in\nreporting major political events in national online media. These aspects are\nexplained by considering explanatory factors related to economy,\ninfrastructure, politics, and administration. According to the empirical\nresults based on a dynamic panel data methodology, all aspects except\nadministration are also associated with the maturity of national online media.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 03:25:24 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ruohonen", "Jukka", ""], ["Tuikka", "Anne-Marie", ""]]}, {"id": "2106.13987", "submitter": "Shaoshan Liu", "authors": "Shaoshan Liu, Jean-Luc Gaudiot", "title": "Rise of the Autonomous Machines", "comments": "to appear in IEEE Computer Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.AR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After decades of uninterrupted progress and growth, information technology\nhas so evolved that it can be said we are entering the age of autonomous\nmachines, but there exist many roadblocks in the way of making this a reality.\nIn this article, we make a preliminary attempt at recognizing and categorizing\nthe technical and non-technical challenges of autonomous machines; for each of\nthe ten areas we have identified, we review current status, roadblocks, and\npotential research directions. It is hoped that this will help the community\ndefine clear, effective, and more formal development goalposts for the future.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 09:46:01 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Liu", "Shaoshan", ""], ["Gaudiot", "Jean-Luc", ""]]}, {"id": "2106.14043", "submitter": "Mustafa Yal\\c{c}{\\i}ner", "authors": "Ali Vakilian, Mustafa Yal\\c{c}{\\i}ner", "title": "Improved Approximation Algorithms for Individually Fair Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the $k$-clustering problem with $\\ell_p$-norm cost, which\nincludes $k$-median, $k$-means and $k$-center cost functions, under an\nindividual notion of fairness proposed by Jung et al. [2020]: given a set of\npoints $P$ of size $n$, a set of $k$ centers induces a fair clustering if for\nevery point $v\\in P$, $v$ can find a center among its $n/k$ closest neighbors.\nRecently, Mahabadi and Vakilian [2020] showed how to get a\n$(p^{O(p)},7)$-bicriteria approximation for the problem of fair $k$-clustering\nwith $\\ell_p$-norm cost: every point finds a center within distance at most $7$\ntimes its distance to its $(n/k)$-th closest neighbor and the $\\ell_p$-norm\ncost of the solution is at most $p^{O(p)}$ times the cost of an optimal fair\nsolution. In this work, for any $\\varepsilon>0$, we present an improved $(16^p\n+\\varepsilon,3)$-bicriteria approximation for the fair $k$-clustering with\n$\\ell_p$-norm cost. To achieve our guarantees, we extend the framework of\n[Charikar et al., 2002, Swamy, 2016] and devise a $16^p$-approximation\nalgorithm for the facility location with $\\ell_p$-norm cost under matroid\nconstraint which might be of an independent interest. Besides, our approach\nsuggests a reduction from our individually fair clustering to a clustering with\na group fairness requirement proposed by Kleindessner et al. [2019], which is\nessentially the median matroid problem [Krishnaswamy et al., 2011].\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 15:22:52 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Vakilian", "Ali", ""], ["Yal\u00e7\u0131ner", "Mustafa", ""]]}, {"id": "2106.14049", "submitter": "Yue Lin", "authors": "Yue Lin, Ningchuan Xiao", "title": "Identifying High Accuracy Regions in Traffic Camera Images to Enhance\n  the Estimation of Road Traffic Metrics: A Quadtree Based Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The growing number of real-time camera feeds in urban areas has made it\npossible to provide high-quality traffic data for effective transportation\nplanning, operations, and management. However, deriving reliable traffic\nmetrics from these camera feeds has been a challenge due to the limitations of\ncurrent vehicle detection techniques, as well as the various camera conditions\nsuch as height and resolution. In this work, a quadtree based algorithm is\ndeveloped to continuously partition the image extent until only regions with\nhigh detection accuracy are remained. These regions are referred to as the\nhigh-accuracy identification regions (HAIR) in this paper. We demonstrate how\nthe use of the HAIR can improve the accuracy of traffic density estimates using\nimages from traffic cameras at different heights and resolutions in Central\nOhio. Our experiments show that the proposed algorithm can be used to derive\nrobust HAIR where vehicle detection accuracy is 41 percent higher than that in\nthe original image extent. The use of the HAIR also significantly improves the\ntraffic density estimation with an overall decrease of 49 percent in root mean\nsquared error.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 15:46:45 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 01:57:44 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Lin", "Yue", ""], ["Xiao", "Ningchuan", ""]]}, {"id": "2106.14152", "submitter": "Kishor Datta Gupta", "authors": "Kishor Datta Gupta, Dipankar Dasgupta", "title": "Who is Responsible for Adversarial Defense?", "comments": "Accepted for poster presentation in ICML 2021 workshop \"Challenges in\n  Deploying and monitoring Machine Learning Systems\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We have seen a surge in research aims toward adversarial attacks and defenses\nin AI/ML systems. While it is crucial to formulate new attack methods and\ndevise novel defense strategies for robustness, it is also imperative to\nrecognize who is responsible for implementing, validating, and justifying the\nnecessity of these defenses. In particular, which components of the system are\nvulnerable to what type of adversarial attacks, and the expertise needed to\nrealize the severity of adversarial attacks. Also how to evaluate and address\nthe adversarial challenges in order to recommend defense strategies for\ndifferent applications. This paper opened a discussion on who should examine\nand implement the adversarial defenses and the reason behind such efforts.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 06:09:04 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Gupta", "Kishor Datta", ""], ["Dasgupta", "Dipankar", ""]]}, {"id": "2106.14198", "submitter": "Huimin Chen", "authors": "Huimin Chen, Cheng Yang, Xuanming Zhang, Zhiyuan Liu, Maosong Sun,\n  Jianbin Jin", "title": "From Symbols to Embeddings: A Tale of Two Representations in\n  Computational Social Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational Social Science (CSS), aiming at utilizing computational methods\nto address social science problems, is a recent emerging and fast-developing\nfield. The study of CSS is data-driven and significantly benefits from the\navailability of online user-generated contents and social networks, which\ncontain rich text and network data for investigation. However, these\nlarge-scale and multi-modal data also present researchers with a great\nchallenge: how to represent data effectively to mine the meanings we want in\nCSS? To explore the answer, we give a thorough review of data representations\nin CSS for both text and network. Specifically, we summarize existing\nrepresentations into two schemes, namely symbol-based and embedding-based\nrepresentations, and introduce a series of typical methods for each scheme.\nAfterwards, we present the applications of the above representations based on\nthe investigation of more than 400 research articles from 6 top venues involved\nwith CSS. From the statistics of these applications, we unearth the strength of\neach kind of representations and discover the tendency that embedding-based\nrepresentations are emerging and obtaining increasing attention over the last\ndecade. Finally, we discuss several key challenges and open issues for future\ndirections. This survey aims to provide a deeper understanding and more\nadvisable applications of data representations for CSS researchers.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 11:04:44 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Chen", "Huimin", ""], ["Yang", "Cheng", ""], ["Zhang", "Xuanming", ""], ["Liu", "Zhiyuan", ""], ["Sun", "Maosong", ""], ["Jin", "Jianbin", ""]]}, {"id": "2106.14365", "submitter": "Alina Arseniev-Koehler", "authors": "Alina Arseniev-Koehler, Susan D. Cochran, Vickie M. Mays, Kai-Wei\n  Chang, Jacob Gates Foster", "title": "Integrating topic modeling and word embedding to characterize violent\n  deaths", "comments": null, "journal-ref": null, "doi": "10.31235/osf.io/nkyaq", "report-no": null, "categories": "cs.CL cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is an escalating need for methods to identify latent patterns in text\ndata from many domains. We introduce a new method to identify topics in a\ncorpus and represent documents as topic sequences. Discourse Atom Topic\nModeling draws on advances in theoretical machine learning to integrate topic\nmodeling and word embedding, capitalizing on the distinct capabilities of each.\nWe first identify a set of vectors (\"discourse atoms\") that provide a sparse\nrepresentation of an embedding space. Atom vectors can be interpreted as latent\ntopics: Through a generative model, atoms map onto distributions over words;\none can also infer the topic that generated a sequence of words. We illustrate\nour method with a prominent example of underutilized text: the U.S. National\nViolent Death Reporting System (NVDRS). The NVDRS summarizes violent death\nincidents with structured variables and unstructured narratives. We identify\n225 latent topics in the narratives (e.g., preparation for death and physical\naggression); many of these topics are not captured by existing structured\nvariables. Motivated by known patterns in suicide and homicide by gender, and\nrecent research on gender biases in semantic space, we identify the gender bias\nof our topics (e.g., a topic about pain medication is feminine). We then\ncompare the gender bias of topics to their prevalence in narratives of female\nversus male victims. Results provide a detailed quantitative picture of\nreporting about lethal violence and its gendered nature. Our method offers a\nflexible and broadly applicable approach to model topics in text data.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 01:53:20 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Arseniev-Koehler", "Alina", ""], ["Cochran", "Susan D.", ""], ["Mays", "Vickie M.", ""], ["Chang", "Kai-Wei", ""], ["Foster", "Jacob Gates", ""]]}, {"id": "2106.14387", "submitter": "Junyi Jessy Li", "authors": "Barea Sinno, Bernardo Oviedo, Katherine Atwell, Malihe Alikhani, Junyi\n  Jessy Li", "title": "Political Ideology and Polarization of Policy Positions: A\n  Multi-dimensional Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analyzing political ideology and polarization is of critical importance in\nadvancing our understanding of the political context in society. Recent\nresearch has made great strides towards understanding the ideological bias\n(i.e., stance) of news media along a left-right spectrum. In this work, we take\na novel approach and study the ideology of the policy under discussion teasing\napart the nuanced co-existence of stance and ideology. Aligned with the\ntheoretical accounts in political science, we treat ideology as a\nmulti-dimensional construct, and introduce the first diachronic dataset of news\narticles whose political ideology under discussion is annotated by trained\npolitical scientists and linguists at the paragraph-level. We showcase that\nthis framework enables quantitative analysis of polarization, a temporal,\nmultifaceted measure of ideological distance. We further present baseline\nmodels for ideology prediction.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 04:03:04 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Sinno", "Barea", ""], ["Oviedo", "Bernardo", ""], ["Atwell", "Katherine", ""], ["Alikhani", "Malihe", ""], ["Li", "Junyi Jessy", ""]]}, {"id": "2106.14567", "submitter": "Stavros Nikolopoulos D.", "authors": "Christos Chondros, Christos Georgiou-Mousses, Stavros D. Nikolopoulos,\n  Iosif Polenakis and Vasileios Vouronikos", "title": "SARiSsa -- A Mobile Application for the Proactive Control of SARS-CoV-2\n  Spread", "comments": "9 pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we propose the design principles behind the development of a\nsmart application utilized by mobile devices in order to control the spread of\nSARS-CoV-2 coronavirus disease that caused the COVID-19 pandemic. Through the\ndeployment of this application utilizing their Bluetooth enabled devices,\nindividuals may keep track of their close contacts, and if nearby contacts\nusing the same application are reported later as infected the proximate\nindividual is informed in order to be quarantined for a short of time,\npreventing hence the spread of the virus. Through the latest year, there have\nbeen developed several applications in the Google Play Store that can be\ndeployed by smart devices utilizing their Bluetooth connectivity for the nearby\ndevice tracking. However, in this work we propose an open architecture for the\ndevelopment of such applications, that also incorporates a more elaborated\ngraph-theoretic and algorithmic background regarding the contact tracing. The\nproposed contact tracing algorithm, that can be embedded in the deployment of\nthe application, provides a more immediate tracking of the contacts of an\ninfected individuals, providing a wider extent in the tracing of the contacts,\nleading hence to a more immediate mitigation of the epidemic.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 10:45:33 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 10:40:03 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Chondros", "Christos", ""], ["Georgiou-Mousses", "Christos", ""], ["Nikolopoulos", "Stavros D.", ""], ["Polenakis", "Iosif", ""], ["Vouronikos", "Vasileios", ""]]}, {"id": "2106.14572", "submitter": "Mireia Yurrita", "authors": "Mireia Yurrita, Arnaud Grignard, Luis Alonso, Yan Zhang, Cristian\n  Jara-Figueroa, Markus Elkatsha and Kent Larson", "title": "Dynamic Urban Planning: an Agent-Based Model Coupling Mobility Mode and\n  Housing Choice. Use case Kendall Square", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As cities become increasingly populated, urban planning plays a key role in\nensuring the equitable and inclusive development of metropolitan areas. MIT\nCity Science group created a data-driven tangible platform, CityScope, to help\ndifferent stakeholders, such as government representatives, urban planners,\ndevelopers, and citizens, collaboratively shape the urban scenario through the\nreal-time impact analysis of different urban interventions. This paper presents\nan agent-based model that characterizes citizens' behavioural patterns with\nrespect to housing and mobility choice that will constitute the first step in\nthe development of a dynamic incentive system for an open interactive\ngovernance process. The realistic identification and representation of the\ncriteria that affect this decision-making process will help understand and\nevaluate the impacts of potential housing incentives that aim to promote urban\ncharacteristics such as equality, diversity, walkability, and efficiency. The\ncalibration and validation of the model have been performed in a well-known\ngeographic area for the Group: Kendall Square in Cambridge, MA.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 10:54:44 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Yurrita", "Mireia", ""], ["Grignard", "Arnaud", ""], ["Alonso", "Luis", ""], ["Zhang", "Yan", ""], ["Jara-Figueroa", "Cristian", ""], ["Elkatsha", "Markus", ""], ["Larson", "Kent", ""]]}, {"id": "2106.14649", "submitter": "James Thomas Brown", "authors": "J. Thomas Brown, Chao Yan, Weiyi Xia, Zhijun Yin, Zhiyu Wan, Aris\n  Gkoulalas-Divanis, Murat Kantarcioglu, Bradley A. Malin", "title": "Dynamically Adjusting Case-Reporting Policy to Maximize Privacy and\n  Utility in the Face of a Pandemic", "comments": "68 pages, 18 figures/tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Managing a pandemic requires continuous dissemination of infectious disease\nsurveillance data. Legislation permits sharing de-identified patient data;\nhowever, current de-identification approaches are time-consuming and do not\nflex with changes in infection rates or population demographics over time. In\nthis paper, we introduce a framework to dynamically adapt de-identification for\nnear-real time sharing of patient-level surveillance data. The framework\nleverages a simulation mechanism, capable of being applied to any geographic\nlevel, to forecast and manage disclosure risks. We use data from Johns Hopkins\nUniversity and the Centers for Disease Control and Prevention to demonstrate\nthe framework's effectiveness in maintaining the privacy risk below a threshold\nbased on public health standards for COVID-19 county-level case data from\nAugust 2020 to April 2021. Across all US counties, the framework's approach\nmeets the threshold for 95.2% of daily data releases, while a policy based on\ncurrent de-identification techniques meets the threshold for only 24.6%.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 19:49:17 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Brown", "J. Thomas", ""], ["Yan", "Chao", ""], ["Xia", "Weiyi", ""], ["Yin", "Zhijun", ""], ["Wan", "Zhiyu", ""], ["Gkoulalas-Divanis", "Aris", ""], ["Kantarcioglu", "Murat", ""], ["Malin", "Bradley A.", ""]]}, {"id": "2106.14701", "submitter": "Jason R.C. Nurse Dr", "authors": "Betsy Uchendu and Jason R. C. Nurse and Maria Bada and Steven Furnell", "title": "Developing a cyber security culture: Current practices and future needs", "comments": null, "journal-ref": "Computers & Security, 2021", "doi": "10.1016/j.cose.2021.102387", "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the creation of a strong security culture has been researched and\ndiscussed for decades, it continues to elude many businesses. Part of the\nchallenge faced is distilling pertinent, recent academic findings and research\ninto useful guidance. In this article, we aim to tackle this issue by\nconducting a state-of-the-art study into organisational cyber security culture\nresearch. This work investigates four questions, including how cyber security\nculture is defined, what factors are essential to building and maintaining such\na culture, the frameworks proposed to cultivate a security culture and the\nmetrics suggested to assess it. Through the application of the PRISMA\nsystematic literature review technique, we identify and analyse 58 research\narticles from the last 10 years (2010-2020). Our findings demonstrate that\nwhile there have been notable changes in the use of terms (e.g., information\nsecurity culture and cyber security culture), many of the most influential\nfactors across papers are similar. Top management support, policy and\nprocedures, and awareness for instance, are critical in engendering cyber\nsecurity culture. Many of the frameworks reviewed revealed common foundations,\nwith organisational culture playing a substantial role in crafting appropriate\ncyber security culture models. Questionnaires and surveys are the most used\ntool to measure cyber security culture, but there are also concerns as to\nwhether more dynamic measures are needed. For practitioners, this article\nhighlights factors and models essential to the creation and management of a\nrobust security culture. For research, we produce an up-to-date\ncharacterisation of the field and also define open issues deserving of further\nattention such as the role of change management processes and national culture\nin an enterprise's cyber security culture.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 13:31:33 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Uchendu", "Betsy", ""], ["Nurse", "Jason R. C.", ""], ["Bada", "Maria", ""], ["Furnell", "Steven", ""]]}, {"id": "2106.14737", "submitter": "Seungmo Kim", "authors": "Seungmo Kim", "title": "Adding Interactivity to Education of Complex Wireless Networks Using\n  Digital Game-Based Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Can we make undergraduate engineering education easier and more fun? This\nresearch aims to see if we can answer the ambitious question! The digital\ngame-based learning (DGBL) has been found to increase the efficacy of learning\nwhen applied in engineering classes thanks to its ability to make students feel\neasy and fun. However, the state-of-the-art DGBL schemes still observe\nchallenges in various aspects including cost, efficacy, readiness of\ninstructors and students, etc. Motivated from the challenges, this research\nproposes to design a DGBL platform that features visualized and systematic\nviews. Specifically, we identify the blockchain applied to wireless\ncommunications and networking systems as a key ecosystem that we capitalize the\nbenefit of the proposed platform.. As such, in this paper, we lay out a\ncomprehensive DGBL pedagogy that includes (i) creation of relevant assignment\nactivities and class materials in a relevant course and (ii) evaluation of the\npedagogical efficacy. In a long-term view, a successful delivery of this\nresearch will increase the confidence of undergraduate engineering students on\nthe \"in-concert\" dynamics of various factors determining the performance of a\nblockchain system built on a wireless network.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 14:09:57 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kim", "Seungmo", ""]]}, {"id": "2106.14750", "submitter": "Gabriele Santin", "authors": "E. Leoni and G. Cencetti and G. Santin and T. Istomin and D. Molteni\n  and G. P. Picco and E. Farella and B. Lepri and A. M. Murphy", "title": "Measuring close proximity interactions in summer camps during the\n  COVID-19 pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy makers have implemented multiple non-pharmaceutical strategies to\nmitigate the COVID-19 worldwide crisis. Interventions had the aim of reducing\nclose proximity interactions, which drive the spread of the disease. A deeper\nknowledge of human physical interactions has revealed necessary, especially in\nall settings involving children, whose education and gathering activities\nshould be preserved. Despite their relevance, almost no data are available on\nclose proximity contacts among children in schools or other educational\nsettings during the pandemic. Contact data are usually gathered via Bluetooth,\nwhich nonetheless offers a low temporal and spatial resolution. Recently,\nultra-wideband (UWB) radios emerged as a more accurate alternative that\nnonetheless exhibits a significantly higher energy consumption, limiting\nin-field studies. In this paper, we leverage a novel approach, embodied by the\nJanus system that combines these radios by exploiting their complementary\nbenefits. The very accurate proximity data gathered in-field by Janus, once\naugmented with several metadata, unlocks unprecedented levels of information,\nenabling the development of novel multi-level risk analyses. By means of this\ntechnology, we have collected real contact data of children and educators in\nthree summer camps during summer 2020 in the province of Trento, Italy. The\nwide variety of performed daily activities induced multiple individual\nbehaviors, allowing a rich investigation of social environments from the\ncontagion risk perspective. We consider risk based on duration and proximity of\ncontacts and classify interactions according to different risk levels. We can\nthen evaluate the summer camps' organization, observe the effect of partition\nin small groups, or social bubbles, and identify the organized activities that\nmitigate the riskier behaviors. [...]\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 14:26:18 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Leoni", "E.", ""], ["Cencetti", "G.", ""], ["Santin", "G.", ""], ["Istomin", "T.", ""], ["Molteni", "D.", ""], ["Picco", "G. P.", ""], ["Farella", "E.", ""], ["Lepri", "B.", ""], ["Murphy", "A. M.", ""]]}, {"id": "2106.14861", "submitter": "Zainul Abi Din", "authors": "Zainul Abi Din (1), Hari Venugopalan (1), Henry Lin (2), Adam\n  Wushensky (2), Steven Liu (2), Samuel T. King (1 and 2) ((1) University of\n  California, Davis, (2) Bouncer Technologies)", "title": "Doing good by fighting fraud: Ethical anti-fraud systems for mobile\n  payments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  App builders commonly use security challenges, a form of step-up\nauthentication, to add security to their apps. However, the ethical\nimplications of this type of architecture has not been studied previously. In\nthis paper, we present a large-scale measurement study of running an existing\nanti-fraud security challenge, Boxer, in real apps running on mobile devices.\nWe find that although Boxer does work well overall, it is unable to scan\neffectively on devices that run its machine learning models at less than one\nframe per second (FPS), blocking users who use inexpensive devices. With the\ninsights from our study, we design Daredevil, anew anti-fraud system for\nscanning payment cards that work swell across the broad range of performance\ncharacteristics and hardware configurations found on modern mobile devices.\nDaredevil reduces the number of devices that run at less than one FPS by an\norder of magnitude compared to Boxer, providing a more equitable system for\nfighting fraud. In total, we collect data from 5,085,444 real devices spread\nacross 496 real apps running production software and interacting with real\nusers.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 17:28:28 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 01:46:13 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Din", "Zainul Abi", "", "1 and 2"], ["Venugopalan", "Hari", "", "1 and 2"], ["Lin", "Henry", "", "1 and 2"], ["Wushensky", "Adam", "", "1 and 2"], ["Liu", "Steven", "", "1 and 2"], ["King", "Samuel T.", "", "1 and 2"]]}, {"id": "2106.15017", "submitter": "Xin Liu", "authors": "Rex Liu, Sarina A Fazio, Huanle Zhang, Albara Ah Ramli, Xin Liu, Jason\n  Yeates Adams", "title": "Early Mobility Recognition for Intensive Care Unit Patients Using\n  Accelerometers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the development of the Internet of Things(IoT) and Artificial\nIntelligence(AI) technologies, human activity recognition has enabled various\napplications, such as smart homes and assisted living. In this paper, we target\na new healthcare application of human activity recognition, early mobility\nrecognition for Intensive Care Unit(ICU) patients. Early mobility is essential\nfor ICU patients who suffer from long-time immobilization. Our system includes\naccelerometer-based data collection from ICU patients and an AI model to\nrecognize patients' early mobility. To improve the model accuracy and\nstability, we identify features that are insensitive to sensor orientations and\npropose a segment voting process that leverages a majority voting strategy to\nrecognize each segment's activity. Our results show that our system improves\nmodel accuracy from 77.78\\% to 81.86\\% and reduces the model instability\n(standard deviation) from 16.69\\% to 6.92\\%, compared to the same AI model\nwithout our feature engineering and segment voting process.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 22:59:31 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Liu", "Rex", ""], ["Fazio", "Sarina A", ""], ["Zhang", "Huanle", ""], ["Ramli", "Albara Ah", ""], ["Liu", "Xin", ""], ["Adams", "Jason Yeates", ""]]}, {"id": "2106.15285", "submitter": "Sam Royston", "authors": "Sam Royston, Ben Greenberg, Omeed Tavasoli, Courtenay Cotton", "title": "Anomaly Detection and Automated Labeling for Voter Registration File\n  Changes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Voter eligibility in United States elections is determined by a patchwork of\nstate databases containing information about which citizens are eligible to\nvote. Administrators at the state and local level are faced with the\nexceedingly difficult task of ensuring that each of their jurisdictions is\nproperly managed, while also monitoring for improper modifications to the\ndatabase. Monitoring changes to Voter Registration Files (VRFs) is crucial,\ngiven that a malicious actor wishing to disrupt the democratic process in the\nUS would be well-advised to manipulate the contents of these files in order to\nachieve their goals. In 2020, we saw election officials perform admirably when\nfaced with administering one of the most contentious elections in US history,\nbut much work remains to secure and monitor the election systems Americans rely\non. Using data created by comparing snapshots taken of VRFs over time, we\npresent a set of methods that make use of machine learning to ease the burden\non analysts and administrators in protecting voter rolls. We first evaluate the\neffectiveness of multiple unsupervised anomaly detection methods in detecting\nVRF modifications by modeling anomalous changes as sparse additive noise. In\nthis setting we determine that statistical models comparing administrative\ndistricts within a short time span and non-negative matrix factorization are\nmost effective for surfacing anomalous events for review. These methods were\ndeployed during 2019-2020 in our organization's monitoring system and were used\nin collaboration with the office of the Iowa Secretary of State. Additionally,\nwe propose a newly deployed model which uses historical and demographic\nmetadata to label the likely root cause of database modifications. We hope to\nuse this model to predict which modifications have known causes and therefore\nbetter identify potentially anomalous modifications.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 21:48:31 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Royston", "Sam", ""], ["Greenberg", "Ben", ""], ["Tavasoli", "Omeed", ""], ["Cotton", "Courtenay", ""]]}, {"id": "2106.15342", "submitter": "Leon Abdillah", "authors": "Adellia, Leon Andretti Abdillah", "title": "Analisis Kualitas Layanan Website E-Commerce Bukalapak Terhadap Kepuasan\n  Pengguna Mahasiswa Universitas Bina Darma Menggunakan Metode Webqual 4.0", "comments": "16 pages, Indonesian language", "journal-ref": "J. Softw. Eng. Ampera, vol. 1, no. 3, 2020", "doi": "10.51519/journalsea.v1i3.52", "report-no": null, "categories": "cs.SI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth of new technology, motivates some product marketing to be done\nonline. One of the factors that support online development is online buying and\nselling sites or Electronic Commerce. One of the supporting factors for\nElectronic Commerce is using a website. Website or also commonly called the web\nis a form of media that can be interpreted as a collection of pages that\ndisplay various kinds of text information, data, still or moving images,\nanimation data, sound, video, both static and dynamic. Electronic Commerce\ncompanies interact with consumers through the web, one of which is the\nBukalapak website, which is an online site provider for buying and selling\nproducts to be marketed. To determine the quality of a website, it is necessary\nto measure. By measuring the quality of a website, it can be seen the user's\nperception of the website. In this study using the Webqual 4.0 method which\nconsists of 3 dimensions, namely usability, information quality and interaction\nquality on user satisfaction. The data used is primary data which is a source\nof data obtained directly from the original source by distributing\nquestionnaires. The total data obtained are 104 respondents. Respondents in\nthis study were Bina Darma University students who were expected to provide an\nobjective assessment of the website to be analyzed.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 10:57:04 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Adellia", "", ""], ["Abdillah", "Leon Andretti", ""]]}, {"id": "2106.15361", "submitter": "Yusuke Kumakoshi", "authors": "Yusuke Kumakoshi, Shigeaki Onoda, Tetsuya Takahashi, Yuji Yoshimura", "title": "Quantifying urban streetscapes with deep learning: focus on aesthetic\n  evaluation", "comments": "4pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The disorder of urban streetscapes would negatively affect people's\nperception of their aesthetic quality. The presence of billboards on building\nfacades has been regarded as an important factor of the disorder, but its\nquantification methodology has not yet been developed in a scalable manner. To\nfill the gap, this paper reports the performance of our deep learning model on\na unique data set prepared in Tokyo to recognize the areas covered by facades\nand billboards in streetscapes, respectively. The model achieved 63.17 % of\naccuracy, measured by Intersection-over-Union (IoU), thus enabling researchers\nand practitioners to obtain insights on urban streetscape design by combining\ndata of people's preferences.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 12:51:00 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Kumakoshi", "Yusuke", ""], ["Onoda", "Shigeaki", ""], ["Takahashi", "Tetsuya", ""], ["Yoshimura", "Yuji", ""]]}, {"id": "2106.15553", "submitter": "Anett Hoppe", "authors": "Anett Hoppe and David Morris and Ralph Ewerth", "title": "Evaluation of Automated Image Descriptions for Visually Impaired\n  Students", "comments": "6 pages, 12 references. Accepted for publication at the 22nd\n  International Conference on Artificial Intelligence in Education (AIED 2021),\n  June 14-16 2021, Utrecht, The Netherlands", "journal-ref": "Hoppe A., Morris D., Ewerth R. (2021) Evaluation of Automated\n  Image Descriptions for Visually Impaired Students. In: Roll I., McNamara D.,\n  Sosnovsky S., Luckin R., Dimitrova V. (eds) AIED 2021. LNCS vol 12749.\n  Springer, Cham", "doi": "10.1007/978-3-030-78270-2_35", "report-no": null, "categories": "cs.HC cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Illustrations are widely used in education, and sometimes, alternatives are\nnot available for visually impaired students. Therefore, those students would\nbenefit greatly from an automatic illustration description system, but only if\nthose descriptions were complete, correct, and easily understandable using a\nscreenreader. In this paper, we report on a study for the assessment of\nautomated image descriptions. We interviewed experts to establish evaluation\ncriteria, which we then used to create an evaluation questionnaire for sighted\nnon-expert raters, and description templates. We used this questionnaire to\nevaluate the quality of descriptions which could be generated with a\ntemplate-based automatic image describer. We present evidence that these\ntemplates have the potential to generate useful descriptions, and that the\nquestionnaire identifies problems with description templates.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 16:40:04 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Hoppe", "Anett", ""], ["Morris", "David", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2106.15590", "submitter": "William Agnew", "authors": "Abeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit\n  Dotan, Michelle Bao", "title": "The Values Encoded in Machine Learning Research", "comments": "Data and code available at\n  https://github.com/wagnew3/The-Values-Encoded-in-Machine-Learning-Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine learning (ML) currently exerts an outsized influence on the world,\nincreasingly affecting communities and institutional practices. It is therefore\ncritical that we question vague conceptions of the field as value-neutral or\nuniversally beneficial, and investigate what specific values the field is\nadvancing. In this paper, we present a rigorous examination of the values of\nthe field by quantitatively and qualitatively analyzing 100 highly cited ML\npapers published at premier ML conferences, ICML and NeurIPS. We annotate key\nfeatures of papers which reveal their values: how they justify their choice of\nproject, which aspects they uplift, their consideration of potential negative\nconsequences, and their institutional affiliations and funding sources. We find\nthat societal needs are typically very loosely connected to the choice of\nproject, if mentioned at all, and that consideration of negative consequences\nis extremely rare. We identify 67 values that are uplifted in machine learning\nresearch, and, of these, we find that papers most frequently justify and assess\nthemselves based on performance, generalization, efficiency, researcher\nunderstanding, novelty, and building on previous work. We present extensive\ntextual evidence and analysis of how these values are operationalized. Notably,\nwe find that each of these top values is currently being defined and applied\nwith assumptions and implications generally supporting the centralization of\npower. Finally, we find increasingly close ties between these highly cited\npapers and tech companies and elite universities.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 17:24:14 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Birhane", "Abeba", ""], ["Kalluri", "Pratyusha", ""], ["Card", "Dallas", ""], ["Agnew", "William", ""], ["Dotan", "Ravit", ""], ["Bao", "Michelle", ""]]}, {"id": "2106.15611", "submitter": "James Bagrow", "authors": "Milo Z. Trujillo, Laurent H\\'ebert-Dufresne and James P. Bagrow", "title": "The penumbra of open source: projects outside of centralized platforms\n  are longer maintained, more academic and more collaborative", "comments": "19 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GitHub has become the central online platform for much of open source,\nhosting most open source code repositories. With this popularity, the public\ndigital traces of GitHub are now a valuable means to study teamwork and\ncollaboration. In many ways, however, GitHub is a convenience sample. We need\nto assess its representativeness, particularly how GitHub's design may alter\nthe working patterns of its users. Here we develop a novel, extensive sample of\npublic open source project repositories outside of centralized platforms like\nGitHub. We characterized these projects along a number of dimensions, and\ncompare to a time-matched sample of corresponding GitHub projects. Compared to\nGitHub, these projects tend to have more collaborators, are maintained for\nlonger periods, and tend to be more focused on academic and scientific\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 17:54:26 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 16:03:03 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Trujillo", "Milo Z.", ""], ["H\u00e9bert-Dufresne", "Laurent", ""], ["Bagrow", "James P.", ""]]}, {"id": "2106.15715", "submitter": "Hans Hanley", "authors": "Hans W. A. Hanley, Deepak Kumar, Zakir Durumeric", "title": "No Calm in The Storm: Investigating QAnon Website Relationships", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  QAnon is a far-right conspiracy theory whose followers largely organize\nonline. In this work, we use web crawls seeded from two of the largest QAnon\nhotbeds on the Internet, Voat and 8kun, to build a hyperlink graph. We then use\nthis graph to identify, understand, and learn from the websites that spread\nQAnon content online. We curate the largest list of QAnon centered websites to\ndate, from which we document the types of QAnon sites, their hosting providers,\nas well as their popularity. We further analyze QAnon websites' connection to\nmainstream news and misinformation online, highlighting the outsized role\nmisinformation websites play in spreading the conspiracy. Finally, we leverage\nthe observed relationship between QAnon and misinformation sites to build a\nrandom forest classifier that distinguishes between misinformation and\nauthentic news sites, getting a performance of 0.98 AUC on a test set. Our\nresults demonstrate new and effective ways to study conspiracy and\nmisinformation on the Internet.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 20:39:17 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Hanley", "Hans W. A.", ""], ["Kumar", "Deepak", ""], ["Durumeric", "Zakir", ""]]}, {"id": "2106.15764", "submitter": "Yisroel Mirsky Dr.", "authors": "Yisroel Mirsky, Ambra Demontis, Jaidip Kotak, Ram Shankar, Deng Gelei,\n  Liu Yang, Xiangyu Zhang, Wenke Lee, Yuval Elovici, Battista Biggio", "title": "The Threat of Offensive AI to Organizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  AI has provided us with the ability to automate tasks, extract information\nfrom vast amounts of data, and synthesize media that is nearly\nindistinguishable from the real thing. However, positive tools can also be used\nfor negative purposes. In particular, cyber adversaries can use AI (such as\nmachine learning) to enhance their attacks and expand their campaigns.\n  Although offensive AI has been discussed in the past, there is a need to\nanalyze and understand the threat in the context of organizations. For example,\nhow does an AI-capable adversary impact the cyber kill chain? Does AI benefit\nthe attacker more than the defender? What are the most significant AI threats\nfacing organizations today and what will be their impact on the future?\n  In this survey, we explore the threat of offensive AI on organizations.\nFirst, we present the background and discuss how AI changes the adversary's\nmethods, strategies, goals, and overall attack model. Then, through a\nliterature review, we identify 33 offensive AI capabilities which adversaries\ncan use to enhance their attacks. Finally, through a user study spanning\nindustry and academia, we rank the AI threats and provide insights on the\nadversaries.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 01:03:28 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Mirsky", "Yisroel", ""], ["Demontis", "Ambra", ""], ["Kotak", "Jaidip", ""], ["Shankar", "Ram", ""], ["Gelei", "Deng", ""], ["Yang", "Liu", ""], ["Zhang", "Xiangyu", ""], ["Lee", "Wenke", ""], ["Elovici", "Yuval", ""], ["Biggio", "Battista", ""]]}, {"id": "2106.15767", "submitter": "Xian Li", "authors": "Xian Li", "title": "Unaware Fairness: Hierarchical Random Forest for Protected Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Procedural fairness has been a public concern, which leads to controversy\nwhen making decisions with respect to protected classes, such as race, social\nstatus, and disability. Some protected classes can be inferred according to\nsome safe proxies like surname and geolocation for the race. Hence, implicitly\nutilizing the predicted protected classes based on the related proxies when\nmaking decisions is an efficient approach to circumvent this issue and seek\njust decisions. In this article, we propose a hierarchical random forest model\nfor prediction without explicitly involving protected classes. Simulation\nexperiments are conducted to show the performance of the hierarchical random\nforest model. An example is analyzed from Boston police interview records to\nillustrate the usefulness of the proposed model.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 01:14:59 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Li", "Xian", ""]]}, {"id": "2106.15808", "submitter": "Baihan Lin", "authors": "Baihan Lin, Djallel Bouneffouf", "title": "Optimal Epidemic Control as a Contextual Combinatorial Bandit with\n  Budget", "comments": "arXiv admin note: text overlap with arXiv:1906.09384 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In light of the COVID-19 pandemic, it is an open challenge and critical\npractical problem to find a optimal way to dynamically prescribe the best\npolicies that balance both the governmental resources and epidemic control in\ndifferent countries and regions. To solve this multi-dimensional tradeoff of\nexploitation and exploration, we formulate this technical challenge as a\ncontextual combinatorial bandit problem that jointly optimizes a multi-criteria\nreward function. Given the historical daily cases in a region and the past\nintervention plans in place, the agent should generate useful intervention\nplans that policy makers can implement in real time to minimizing both the\nnumber of daily COVID-19 cases and the stringency of the recommended\ninterventions. We prove this concept with simulations of multiple realistic\npolicy making scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 04:46:31 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Lin", "Baihan", ""], ["Bouneffouf", "Djallel", ""]]}, {"id": "2106.15917", "submitter": "Vaidehi R", "authors": "R Vaidehi, A Bheemeshwar Reddy and Sudatta Banerjee", "title": "Explaining Caste-based Digital Divide in India", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.CY q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the increasing importance of information and communication technologies\nin access to basic services like education and health, the question of the\ndigital divide based on caste assumes importance in India where large\nsocioeconomic disparities persist between different caste groups. Studies on\ncaste-based digital inequality are still scanty in India. Using nationally\nrepresentative survey data, this paper analyzes the first-level digital divide\n(ownership of computer and access to the internet) and the second-level digital\ndivide (individual's skill to use computer and the internet) between the\ndisadvantaged caste group and the others. Further, this paper identifies the\ncaste group-based differences in socioeconomic factors that contribute to the\ndigital divide between these groups using a non-linear decomposition method.\nThe results show that there exists a large first-level and second-level digital\ndivide between the disadvantaged caste groups and others in India. The\nnon-linear decomposition results indicate that the caste-based digital divide\nin India is rooted in historical socioeconomic deprivation of disadvantaged\ncaste groups. More than half of the caste-based digital gap is attributable to\ndifferences in educational attainment and income between the disadvantaged\ncaste groups and others. The findings of this study highlight the urgent need\nfor addressing educational and income inequality between the different caste\ngroups in India in order to bridge the digital divide.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 09:19:24 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Vaidehi", "R", ""], ["Reddy", "A Bheemeshwar", ""], ["Banerjee", "Sudatta", ""]]}, {"id": "2106.15940", "submitter": "Pablo Arag\\'on", "authors": "Pablo Arag\\'on, Diego S\\'aez-Trumper", "title": "A preliminary approach to knowledge integrity risk assessment in\n  Wikipedia projects", "comments": "Accepted at MIS2'21: Misinformation and Misbehavior Mining on the Web\n  Workshop held in conjunction with KDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Wikipedia is one of the main repositories of free knowledge available today,\nwith a central role in the Web ecosystem. For this reason, it can also be a\nbattleground for actors trying to impose specific points of view or even\nspreading disinformation online. There is a growing need to monitor its\n\"health\" but this is not an easy task. Wikipedia exists in over 300 language\neditions and each project is maintained by a different community, with their\nown strengths, weaknesses and limitations. In this paper, we introduce a\ntaxonomy of knowledge integrity risks across Wikipedia projects and a first set\nof indicators to assess internal risks related to community and content issues,\nas well as external threats such as the geopolitical and media landscape. On\ntop of this taxonomy, we offer a preliminary analysis illustrating how the lack\nof editors' geographical diversity might represent a knowledge integrity risk.\nThese are the first steps of a research project to build a Wikipedia Knowledge\nIntegrity Risk Observatory.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 09:47:27 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Arag\u00f3n", "Pablo", ""], ["S\u00e1ez-Trumper", "Diego", ""]]}, {"id": "2106.15968", "submitter": "Salvatore Vilella", "authors": "Salvatore Vilella, Alfonso Semeraro, Daniela Paolotti, Giancarlo Ruffo", "title": "The Impact of Disinformation on a Controversial Debate on Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we study how pervasive is the presence of disinformation in the\nItalian debate around immigration on Twitter and the role of automated accounts\nin the diffusion of such content. By characterising the Twitter users with an\n\\textit{Untrustworthiness} score, that tells us how frequently they engage with\ndisinformation content, we are able to see that such bad information\nconsumption habits are not equally distributed across the users; adopting a\nnetwork analysis approach, we can identify communities characterised by a very\nhigh presence of users that frequently share content from unreliable news\nsources. Within this context, social bots tend to inject in the network more\nmalicious content, that often remains confined in a limited number of clusters;\ninstead, they target reliable content in order to diversify their reach. The\nevidence we gather suggests that, at least in this particular case study, there\nis a strong interplay between social bots and users engaging with unreliable\ncontent, influencing the diffusion of the latter across the network.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 10:29:07 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Vilella", "Salvatore", ""], ["Semeraro", "Alfonso", ""], ["Paolotti", "Daniela", ""], ["Ruffo", "Giancarlo", ""]]}, {"id": "2106.16122", "submitter": "Sebastian Kr\\\"ugel", "authors": "Sebastian Kr\\\"ugel, Andreas Ostermaier, Matthias Uhl", "title": "Zombies in the Loop? People are Insensitive to the Transparency of\n  AI-Powered Moral Advisors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Departing from the assumption that AI needs to be transparent to be trusted,\nwe find that users trustfully take ethical advice from a transparent and an\nopaque AI-powered algorithm alike. Even when transparency reveals information\nthat warns against the algorithm, they continue to accept its advice. We\nconducted online experiments where the participants took the role of\ndecision-makers who received AI-powered advice on how to deal with an ethical\ndilemma. We manipulated information about the algorithm to study its influence.\nOur findings suggest that AI is overtrusted rather than distrusted, and that\nusers need digital literacy to benefit from transparency.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 15:19:20 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Kr\u00fcgel", "Sebastian", ""], ["Ostermaier", "Andreas", ""], ["Uhl", "Matthias", ""]]}, {"id": "2106.16207", "submitter": "Milo Trujillo", "authors": "Milo Z. Trujillo, Samuel F. Rosenblatt, Guillermo de Anda J\\'auregui,\n  Emily Moog, Briane Paul V. Samson, Laurent H\\'ebert-Dufresne and Allison M.\n  Roth", "title": "When the Echo Chamber Shatters: Examining the Use of Community-Specific\n  Language Post-Subreddit Ban", "comments": "15 pages (including references and appendix), 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community-level bans are a common tool against groups that enable online\nharassment and harmful speech. Unfortunately, the efficacy of community bans\nhas only been partially studied and with mixed results. Here, we provide a\nflexible unsupervised methodology to identify in-group language and track user\nactivity on Reddit both before and after the ban of a community (subreddit). We\nuse a simple word frequency divergence to identify uncommon words\noverrepresented in a given community, not as a proxy for harmful speech but as\na linguistic signature of the community. We apply our method to 15 banned\nsubreddits, and find that community response is heterogeneous between\nsubreddits and between users of a subreddit. Top users were more likely to\nbecome less active overall, while random users often reduced use of in-group\nlanguage without decreasing activity. Finally, we find some evidence that the\neffectiveness of bans aligns with the content of a community. Users of dark\nhumor communities were largely unaffected by bans while users of communities\norganized around white supremacy and fascism were the most affected.\nAltogether, our results show that bans do not affect all groups or users\nequally, and pave the way to understanding the effect of bans across\ncommunities.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 16:59:46 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Trujillo", "Milo Z.", ""], ["Rosenblatt", "Samuel F.", ""], ["J\u00e1uregui", "Guillermo de Anda", ""], ["Moog", "Emily", ""], ["Samson", "Briane Paul V.", ""], ["H\u00e9bert-Dufresne", "Laurent", ""], ["Roth", "Allison M.", ""]]}]