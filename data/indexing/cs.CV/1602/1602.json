[{"id": "1602.00020", "submitter": "Holger Roth", "authors": "Holger R. Roth, Yinong Wang, Jianhua Yao, Le Lu, Joseph E. Burns,\n  Ronald M. Summers", "title": "Deep convolutional networks for automated detection of posterior-element\n  fractures on spine CT", "comments": "To be presented at SPIE Medical Imaging, 2016, San Diego", "journal-ref": null, "doi": "10.1117/12.2217146", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Injuries of the spine, and its posterior elements in particular, are a common\noccurrence in trauma patients, with potentially devastating consequences.\nComputer-aided detection (CADe) could assist in the detection and\nclassification of spine fractures. Furthermore, CAD could help assess the\nstability and chronicity of fractures, as well as facilitate research into\noptimization of treatment paradigms.\n  In this work, we apply deep convolutional networks (ConvNets) for the\nautomated detection of posterior element fractures of the spine. First, the\nvertebra bodies of the spine with its posterior elements are segmented in spine\nCT using multi-atlas label fusion. Then, edge maps of the posterior elements\nare computed. These edge maps serve as candidate regions for predicting a set\nof probabilities for fractures along the image edges using ConvNets in a 2.5D\nfashion (three orthogonal patches in axial, coronal and sagittal planes). We\nexplore three different methods for training the ConvNet using 2.5D patches\nalong the edge maps of 'positive', i.e. fractured posterior-elements and\n'negative', i.e. non-fractured elements.\n  An experienced radiologist retrospectively marked the location of 55\ndisplaced posterior-element fractures in 18 trauma patients. We randomly split\nthe data into training and testing cases. In testing, we achieve an\narea-under-the-curve of 0.857. This corresponds to 71% or 81% sensitivities at\n5 or 10 false-positives per patient, respectively. Analysis of our set of\ntrauma patients demonstrates the feasibility of detecting posterior-element\nfractures in spine CT images using computer vision techniques such as deep\nconvolutional networks.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 21:48:13 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Roth", "Holger R.", ""], ["Wang", "Yinong", ""], ["Yao", "Jianhua", ""], ["Lu", "Le", ""], ["Burns", "Joseph E.", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1602.00032", "submitter": "Yezhou Yang", "authors": "Chengxi Ye and Yezhou Yang and Cornelia Fermuller and Yiannis\n  Aloimonos", "title": "What Can I Do Around Here? Deep Functional Scene Understanding for\n  Cognitive Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For robots that have the capability to interact with the physical environment\nthrough their end effectors, understanding the surrounding scenes is not merely\na task of image classification or object recognition. To perform actual tasks,\nit is critical for the robot to have a functional understanding of the visual\nscene. Here, we address the problem of localizing and recognition of functional\nareas from an arbitrary indoor scene, formulated as a two-stage deep learning\nbased detection pipeline. A new scene functionality testing-bed, which is\ncomplied from two publicly available indoor scene datasets, is used for\nevaluation. Our method is evaluated quantitatively on the new dataset,\ndemonstrating the ability to perform efficient recognition of functional areas\nfrom arbitrary indoor scenes. We also demonstrate that our detection model can\nbe generalized onto novel indoor scenes by cross validating it with the images\nfrom two different datasets.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 22:55:53 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2016 16:28:01 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Ye", "Chengxi", ""], ["Yang", "Yezhou", ""], ["Fermuller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1602.00134", "submitter": "Shih-En Wei", "authors": "Shih-En Wei, Varun Ramakrishna, Takeo Kanade, Yaser Sheikh", "title": "Convolutional Pose Machines", "comments": "camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pose Machines provide a sequential prediction framework for learning rich\nimplicit spatial models. In this work we show a systematic design for how\nconvolutional networks can be incorporated into the pose machine framework for\nlearning image features and image-dependent spatial models for the task of pose\nestimation. The contribution of this paper is to implicitly model long-range\ndependencies between variables in structured prediction tasks such as\narticulated pose estimation. We achieve this by designing a sequential\narchitecture composed of convolutional networks that directly operate on belief\nmaps from previous stages, producing increasingly refined estimates for part\nlocations, without the need for explicit graphical model-style inference. Our\napproach addresses the characteristic difficulty of vanishing gradients during\ntraining by providing a natural learning objective function that enforces\nintermediate supervision, thereby replenishing back-propagated gradients and\nconditioning the learning procedure. We demonstrate state-of-the-art\nperformance and outperform competing methods on standard benchmarks including\nthe MPII, LSP, and FLIC datasets.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2016 16:15:28 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2016 04:58:41 GMT"}, {"version": "v3", "created": "Mon, 28 Mar 2016 10:22:17 GMT"}, {"version": "v4", "created": "Tue, 12 Apr 2016 03:31:53 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Wei", "Shih-En", ""], ["Ramakrishna", "Varun", ""], ["Kanade", "Takeo", ""], ["Sheikh", "Yaser", ""]]}, {"id": "1602.00172", "submitter": "Patrick O. Glauner", "authors": "Patrick O. Glauner", "title": "Deep Learning For Smile Recognition", "comments": "Proceedings of the 12th Conference on Uncertainty Modelling in\n  Knowledge Engineering and Decision Making (FLINS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent successes of deep learning in computer vision, we propose\na novel application of deep convolutional neural networks to facial expression\nrecognition, in particular smile recognition. A smile recognition test accuracy\nof 99.45% is achieved for the Denver Intensity of Spontaneous Facial Action\n(DISFA) database, significantly outperforming existing approaches based on\nhand-crafted features with accuracies ranging from 65.55% to 79.67%. The\nnovelty of this approach includes a comprehensive model selection of the\narchitecture parameters, allowing to find an appropriate architecture for each\nexpression such as smile. This is feasible because all experiments were run on\na Tesla K40c GPU, allowing a speedup of factor 10 over traditional computations\non a CPU.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2016 23:59:04 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 04:46:01 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Glauner", "Patrick O.", ""]]}, {"id": "1602.00177", "submitter": "Sagi Eppel", "authors": "Sagi Eppel", "title": "Tracing liquid level and material boundaries in transparent vessels\n  using the graph cut computer vision approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Detection of boundaries of materials stored in transparent vessels is\nessential for identifying properties such as liquid level and phase boundaries,\nwhich are vital for controlling numerous processes in the industry and\nchemistry laboratory. This work presents a computer vision method for\nidentifying the boundary of materials in transparent vessels using the\ngraph-cut algorithm. The method receives an image of a transparent vessel\ncontaining a material and the contour of the vessel in the image. The boundary\nof the material in the vessel is found by the graph cut method. In general the\nmethod uses the vessel region of the image to create a graph, where pixels are\nvertices, and the cost of an edge between two pixels is inversely correlated\nwith their intensity difference. The bottom 10% of the vessel region in the\nimage is assumed to correspond to the material phase and defined as the graph\nand source. The top 10% of the pixels in the vessels are assumed to correspond\nto the air phase and defined as the graph sink. The minimal cut that splits the\nresulting graph between the source and sink (hence, material and air) is traced\nusing the max-flow/min-cut approach. This cut corresponds to the boundary of\nthe material in the image. The method gave high accuracy in boundary\nrecognition for a wide range of liquid, solid, granular and powder materials in\nvarious glass vessels from everyday life and the chemistry laboratory, such as\nbottles, jars, Glasses, Chromotography colums and separatory funnels.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 00:20:39 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Eppel", "Sagi", ""]]}, {"id": "1602.00206", "submitter": "Zhaoqiang Xia", "authors": "Zhaoqiang Xia, Xiaoyi Feng, Jinye Peng, Abdenour Hadid", "title": "Unsupervised Deep Hashing for Large-scale Visual Search", "comments": null, "journal-ref": "2016 6th International Conference on Image Processing Theory Tools\n  and Applications (IPTA)", "doi": "10.1109/IPTA.2016.7821007", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning based hashing plays a pivotal role in large-scale visual search.\nHowever, most existing hashing algorithms tend to learn shallow models that do\nnot seek representative binary codes. In this paper, we propose a novel hashing\napproach based on unsupervised deep learning to hierarchically transform\nfeatures into hash codes. Within the heterogeneous deep hashing framework, the\nautoencoder layers with specific constraints are considered to model the\nnonlinear mapping between features and binary codes. Then, a Restricted\nBoltzmann Machine (RBM) layer with constraints is utilized to reduce the\ndimension in the hamming space. Extensive experiments on the problem of visual\nsearch demonstrate the competitiveness of our proposed approach compared to\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 06:36:47 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Xia", "Zhaoqiang", ""], ["Feng", "Xiaoyi", ""], ["Peng", "Jinye", ""], ["Hadid", "Abdenour", ""]]}, {"id": "1602.00212", "submitter": "Jeremias Sulam", "authors": "Jeremias Sulam, Boaz Ophir, Michael Zibulevsky, Michael Elad", "title": "Trainlets: Dictionary Learning in High Dimensions", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2540599", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representations has shown to be a very powerful model for real world\nsignals, and has enabled the development of applications with notable\nperformance. Combined with the ability to learn a dictionary from signal\nexamples, sparsity-inspired algorithms are often achieving state-of-the-art\nresults in a wide variety of tasks. Yet, these methods have traditionally been\nrestricted to small dimensions mainly due to the computational constraints that\nthe dictionary learning problem entails. In the context of image processing,\nthis implies handling small image patches. In this work we show how to\nefficiently handle bigger dimensions and go beyond the small patches in\nsparsity-based signal and image processing methods. We build our approach based\non a new cropped wavelet decomposition, which enables a multi-scale analysis\nwith virtually no border effects. We then employ this as the base dictionary\nwithin a double sparsity model to enable the training of adaptive dictionaries.\nTo cope with the increase of training data, while at the same time improving\nthe training performance, we present an Online Sparse Dictionary Learning\n(OSDL) algorithm to train this model effectively, enabling it to handle\nmillions of examples. This work shows that dictionary learning can be up-scaled\nto tackle a new level of signal dimensions, obtaining large adaptable atoms\nthat we call trainlets.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 08:24:24 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2016 09:20:36 GMT"}, {"version": "v3", "created": "Mon, 8 Feb 2016 15:05:51 GMT"}, {"version": "v4", "created": "Thu, 12 May 2016 16:37:09 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Sulam", "Jeremias", ""], ["Ophir", "Boaz", ""], ["Zibulevsky", "Michael", ""], ["Elad", "Michael", ""]]}, {"id": "1602.00214", "submitter": "Gustau Camps-Valls", "authors": "Valero Laparra, Jesus Malo, Gustau Camps-Valls", "title": "Dimensionality Reduction via Regression in Hyperspectral Imagery", "comments": "12 pages, 6 figures, 62 references", "journal-ref": "J. Sel. Topics Signal Processing 9(6): 1026-1036 (2015)", "doi": "10.1109/JSTSP.2015.2417833", "report-no": null, "categories": "stat.ML cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new unsupervised method for dimensionality reduction\nvia regression (DRR). The algorithm belongs to the family of invertible\ntransforms that generalize Principal Component Analysis (PCA) by using\ncurvilinear instead of linear features. DRR identifies the nonlinear features\nthrough multivariate regression to ensure the reduction in redundancy between\nhe PCA coefficients, the reduction of the variance of the scores, and the\nreduction in the reconstruction error. More importantly, unlike other nonlinear\ndimensionality reduction methods, the invertibility, volume-preservation, and\nstraightforward out-of-sample extension, makes DRR interpretable and easy to\napply. The properties of DRR enable learning a more broader class of data\nmanifolds than the recently proposed Non-linear Principal Components Analysis\n(NLPCA) and Principal Polynomial Analysis (PPA). We illustrate the performance\nof the representation in reducing the dimensionality of remote sensing data. In\nparticular, we tackle two common problems: processing very high dimensional\nspectral information such as in hyperspectral image sounding data, and dealing\nwith spatial-spectral image patches of multispectral images. Both settings pose\ncollinearity and ill-determination problems. Evaluation of the expressive power\nof the features is assessed in terms of truncation error, estimating\natmospheric variables, and surface land cover classification error. Results\nshow that DRR outperforms linear PCA and recently proposed invertible\nextensions based on neural networks (NLPCA) and univariate regressions (PPA).\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 09:34:58 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Laparra", "Valero", ""], ["Malo", "Jesus", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "1602.00217", "submitter": "Gustau Camps-Valls", "authors": "Valero Laparra, Juan Guti\\'errez, Gustavo Camps-Valls, Jes\\'us Malo", "title": "Image Denoising with Kernels based on Natural Image Relations", "comments": null, "journal-ref": "Journal of Machine Learning Research 11: 873-903 (2010)", "doi": "10.1145/1756006.1756035", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A successful class of image denoising methods is based on Bayesian approaches\nworking in wavelet representations. However, analytical estimates can be\nobtained only for particular combinations of analytical models of signal and\nnoise, thus precluding its straightforward extension to deal with other\narbitrary noise sources. In this paper, we propose an alternative non-explicit\nway to take into account the relations among natural image wavelet coefficients\nfor denoising: we use support vector regression (SVR) in the wavelet domain to\nenforce these relations in the estimated signal. Since relations among the\ncoefficients are specific to the signal, the regularization property of SVR is\nexploited to remove the noise, which does not share this feature. The specific\nsignal relations are encoded in an anisotropic kernel obtained from mutual\ninformation measures computed on a representative image database. Training\nconsiders minimizing the Kullback-Leibler divergence (KLD) between the\nestimated and actual probability functions of signal and noise in order to\nenforce similarity. Due to its non-parametric nature, the method can eventually\ncope with different noise sources without the need of an explicit\nre-formulation, as it is strictly necessary under parametric Bayesian\nformalisms. Results under several noise levels and noise sources show that: (1)\nthe proposed method outperforms conventional wavelet methods that assume\ncoefficient independence, (2) it is similar to state-of-the-art methods that do\nexplicitly include these relations when the noise source is Gaussian, and (3)\nit gives better numerical and visual performance when more complex, realistic\nnoise sources are considered. Therefore, the proposed machine learning approach\ncan be seen as a more flexible (model-free) alternative to the explicit\ndescription of wavelet coefficient relations for image denoising.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 10:02:14 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Laparra", "Valero", ""], ["Guti\u00e9rrez", "Juan", ""], ["Camps-Valls", "Gustavo", ""], ["Malo", "Jes\u00fas", ""]]}, {"id": "1602.00224", "submitter": "Chunhua Shen", "authors": "Peng Wang, Lingqiao Liu, Chunhua Shen, Heng Tao Shen", "title": "Order-aware Convolutional Pooling for Video Based Action Recognition", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most video based action recognition approaches create the video-level\nrepresentation by temporally pooling the features extracted at each frame. The\npooling methods that they adopt, however, usually completely or partially\nneglect the dynamic information contained in the temporal domain, which may\nundermine the discriminative power of the resulting video representation since\nthe video sequence order could unveil the evolution of a specific event or\naction. To overcome this drawback and explore the importance of incorporating\nthe temporal order information, in this paper we propose a novel temporal\npooling approach to aggregate the frame-level features. Inspired by the\ncapacity of Convolutional Neural Networks (CNN) in making use of the internal\nstructure of images for information abstraction, we propose to apply the\ntemporal convolution operation to the frame-level representations to extract\nthe dynamic information. However, directly implementing this idea on the\noriginal high-dimensional feature would inevitably result in parameter\nexplosion.\n  To tackle this problem, we view the temporal evolution of the feature value\nat each feature dimension as a 1D signal and learn a unique convolutional\nfilter bank for each of these 1D signals. We conduct experiments on two\nchallenging video-based action recognition datasets, HMDB51 and UCF101; and\ndemonstrate that the proposed method is superior to the conventional pooling\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 10:58:11 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Wang", "Peng", ""], ["Liu", "Lingqiao", ""], ["Shen", "Chunhua", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1602.00307", "submitter": "Hatem Alismail", "authors": "Hatem Alismail, Brett Browning, Simon Lucey", "title": "Bit-Planes: Dense Subpixel Alignment of Binary Descriptors", "comments": "10 pages. In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary descriptors have been instrumental in the recent evolution of\ncomputationally efficient sparse image alignment algorithms. Increasingly,\nhowever, the vision community is interested in dense image alignment methods,\nwhich are more suitable for estimating correspondences from high frame rate\ncameras as they do not rely on exhaustive search. However, classic dense\nalignment approaches are sensitive to illumination change. In this paper, we\npropose an easy to implement and low complexity dense binary descriptor, which\nwe refer to as bit-planes, that can be seamlessly integrated within a\nmulti-channel Lucas & Kanade framework. This novel approach combines the\nrobustness of binary descriptors with the speed and accuracy of dense alignment\nmethods. The approach is demonstrated on a template tracking problem achieving\nstate-of-the-art robustness and faster than real-time performance on consumer\nlaptops (400+ fps on a single core Intel i7) and hand-held mobile devices (100+\nfps on an iPad Air 2).\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 19:51:11 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Alismail", "Hatem", ""], ["Browning", "Brett", ""], ["Lucey", "Simon", ""]]}, {"id": "1602.00310", "submitter": "Tiep Vu", "authors": "Tiep H. Vu, Vishal Monga", "title": "Learning a low-rank shared dictionary for object classification", "comments": "4 page + 1 reference page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the fact that different objects possess distinct class-specific\nfeatures, they also usually share common patterns. Inspired by this\nobservation, we propose a novel method to explicitly and simultaneously learn a\nset of common patterns as well as class-specific features for classification.\nOur dictionary learning framework is hence characterized by both a shared\ndictionary and particular (class-specific) dictionaries. For the shared\ndictionary, we enforce a low-rank constraint, i.e. claim that its spanning\nsubspace should have low dimension and the coefficients corresponding to this\ndictionary should be similar. For the particular dictionaries, we impose on\nthem the well-known constraints stated in the Fisher discrimination dictionary\nlearning (FDDL). Further, we propose a new fast and accurate algorithm to solve\nthe sparse coding problems in the learning step, accelerating its convergence.\nThe said algorithm could also be applied to FDDL and its extensions.\nExperimental results on widely used image databases establish the advantages of\nour method over state-of-the-art dictionary learning methods.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 19:55:08 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 00:36:39 GMT"}], "update_date": "2016-05-19", "authors_parsed": [["Vu", "Tiep H.", ""], ["Monga", "Vishal", ""]]}, {"id": "1602.00328", "submitter": "Konstantinos Rematas", "authors": "Konstantinos Rematas, Chuong Nguyen, Tobias Ritschel, Mario Fritz and\n  Tinne Tuytelaars", "title": "Novel Views of Objects from a Single Image", "comments": "to appear in PAMI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking an image of an object is at its core a lossy process. The rich\ninformation about the three-dimensional structure of the world is flattened to\nan image plane and decisions such as viewpoint and camera parameters are final\nand not easily revertible. As a consequence, possibilities of changing\nviewpoint are limited. Given a single image depicting an object, novel-view\nsynthesis is the task of generating new images that render the object from a\ndifferent viewpoint than the one given. The main difficulty is to synthesize\nthe parts that are disoccluded; disocclusion occurs when parts of an object are\nhidden by the object itself under a specific viewpoint. In this work, we show\nhow to improve novel-view synthesis by making use of the correlations observed\nin 3D models and applying them to new image instances. We propose a technique\nto use the structural information extracted from a 3D model that matches the\nimage object in terms of viewpoint and shape. For the latter part, we propose\nan efficient 2D-to-3D alignment method that associates precisely the image\nappearance with the 3D model geometry with minimal user interaction. Our\ntechnique is able to simulate plausible viewpoint changes for a variety of\nobject classes within seconds. Additionally, we show that our synthesized\nimages can be used as additional training data that improves the performance of\nstandard object detectors.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 21:43:13 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 03:03:50 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Rematas", "Konstantinos", ""], ["Nguyen", "Chuong", ""], ["Ritschel", "Tobias", ""], ["Fritz", "Mario", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1602.00386", "submitter": "Alexander Wong", "authors": "Parthipan Siva, Mohammad Javad Shafiee, Mike Jamieson, and Alexander\n  Wong", "title": "Scene Invariant Crowd Segmentation and Counting Using Scale-Normalized\n  Histogram of Moving Gradients (HoMG)", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of automated crowd segmentation and counting has garnered\nsignificant interest in the field of video surveillance. This paper proposes a\nnovel scene invariant crowd segmentation and counting algorithm designed with\nhigh accuracy yet low computational complexity in mind, which is key for\nwidespread industrial adoption. A novel low-complexity, scale-normalized\nfeature called Histogram of Moving Gradients (HoMG) is introduced for highly\neffective spatiotemporal representation of individuals and crowds within a\nvideo. Real-time crowd segmentation is achieved via boosted cascade of weak\nclassifiers based on sliding-window HoMG features, while linear SVM regression\nof crowd-region HoMG features is employed for real-time crowd counting.\nExperimental results using multi-camera crowd datasets show that the proposed\nalgorithm significantly outperform state-of-the-art crowd counting algorithms,\nas well as achieve very promising crowd segmentation results, thus\ndemonstrating the efficacy of the proposed method for highly-accurate,\nreal-time video-driven crowd analysis.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 04:07:32 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Siva", "Parthipan", ""], ["Shafiee", "Mohammad Javad", ""], ["Jamieson", "Mike", ""], ["Wong", "Alexander", ""]]}, {"id": "1602.00417", "submitter": "Jumabek Alikhanov", "authors": "Jumabek Alikhanov, Myeong Hyeon Ga, Seunghyun Ko and Geun-Sik Jo", "title": "Transfer Learning Based on AdaBoost for Feature Selection from Multiple\n  ConvNet Layer Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Networks (ConvNets) are powerful models that learn hierarchies\nof visual features, which could also be used to obtain image representations\nfor transfer learning. The basic pipeline for transfer learning is to first\ntrain a ConvNet on a large dataset (source task) and then use feed-forward\nunits activation of the trained ConvNet as image representation for smaller\ndatasets (target task). Our key contribution is to demonstrate superior\nperformance of multiple ConvNet layer features over single ConvNet layer\nfeatures. Combining multiple ConvNet layer features will result in more complex\nfeature space with some features being repetitive. This requires some form of\nfeature selection. We use AdaBoost with single stumps to implicitly select only\ndistinct features that are useful towards classification from concatenated\nConvNet features. Experimental results show that using multiple ConvNet layer\nactivation features instead of single ConvNet layer features consistently will\nproduce superior performance. Improvements becomes significant as we increase\nthe distance between source task and the target task.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 08:02:06 GMT"}, {"version": "v2", "created": "Fri, 25 Mar 2016 12:03:49 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Alikhanov", "Jumabek", ""], ["Ga", "Myeong Hyeon", ""], ["Ko", "Seunghyun", ""], ["Jo", "Geun-Sik", ""]]}, {"id": "1602.00577", "submitter": "Hengyue Pan", "authors": "Hengyue Pan, Hui Jiang", "title": "A Deep Learning Based Fast Image Saliency Detection Algorithm", "comments": "arXiv admin note: substantial text overlap with arXiv:1505.01173", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a fast deep learning method for object saliency\ndetection using convolutional neural networks. In our approach, we use a\ngradient descent method to iteratively modify the input images based on the\npixel-wise gradients to reduce a pre-defined cost function, which is defined to\nmeasure the class-specific objectness and clamp the class-irrelevant outputs to\nmaintain image background. The pixel-wise gradients can be efficiently computed\nusing the back-propagation algorithm. We further apply SLIC superpixels and LAB\ncolor based low level saliency features to smooth and refine the gradients. Our\nmethods are quite computationally efficient, much faster than other deep\nlearning based saliency methods. Experimental results on two benchmark tasks,\nnamely Pascal VOC 2012 and MSRA10k, have shown that our proposed methods can\ngenerate high-quality salience maps, at least comparable with many slow and\ncomplicated deep learning methods. Comparing with the pure low-level methods,\nour approach excels in handling many difficult images, which contain complex\nbackground, highly-variable salient objects, multiple objects, and/or very\nsmall salient objects.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 16:14:57 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Pan", "Hengyue", ""], ["Jiang", "Hui", ""]]}, {"id": "1602.00585", "submitter": "Yinong Wang", "authors": "Yinong Wang, Jianhua Yao, Holger R. Roth, Joseph E. Burns, Ronald M.\n  Summers", "title": "Improving Vertebra Segmentation through Joint Vertebra-Rib Atlases", "comments": "Manuscript to be presented at SPIE Medical Imaging 2016, 27 February\n  - 3 March, 2016, San Diego, California, USA", "journal-ref": null, "doi": "10.1117/12.2217118", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Accurate spine segmentation allows for improved identification and\nquantitative characterization of abnormalities of the vertebra, such as\nvertebral fractures. However, in existing automated vertebra segmentation\nmethods on computed tomography (CT) images, leakage into nearby bones such as\nribs occurs due to the close proximity of these visibly intense structures in a\n3D CT volume. To reduce this error, we propose the use of joint vertebra-rib\natlases to improve the segmentation of vertebrae via multi-atlas joint label\nfusion. Segmentation was performed and evaluated on CTs containing 106 thoracic\nand lumbar vertebrae from 10 pathological and traumatic spine patients on an\nindividual vertebra level basis. Vertebra atlases produced errors where the\nsegmentation leaked into the ribs. The use of joint vertebra-rib atlases\nproduced a statistically significant increase in the Dice coefficient from 92.5\n$\\pm$ 3.1% to 93.8 $\\pm$ 2.1% for the left and right transverse processes and a\ndecrease in the mean and max surface distance from 0.75 $\\pm$ 0.60mm and 8.63\n$\\pm$ 4.44mm to 0.30 $\\pm$ 0.27mm and 3.65 $\\pm$ 2.87mm, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 16:36:42 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Wang", "Yinong", ""], ["Yao", "Jianhua", ""], ["Roth", "Holger R.", ""], ["Burns", "Joseph E.", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1602.00715", "submitter": "Stanley Chan", "authors": "Stanley H. Chan", "title": "Algorithm-Induced Prior for Image Restoration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a type of image priors that are constructed implicitly\nthrough the alternating direction method of multiplier (ADMM) algorithm, called\nthe algorithm-induced prior. Different from classical image priors which are\ndefined before running the reconstruction algorithm, algorithm-induced priors\nare defined by the denoising procedure used to replace one of the two modules\nin the ADMM algorithm. Since such prior is not explicitly defined, analyzing\nthe performance has been difficult in the past.\n  Focusing on the class of symmetric smoothing filters, this paper presents an\nexplicit expression of the prior induced by the ADMM algorithm. The new prior\nis reminiscent to the conventional graph Laplacian but with stronger\nreconstruction performance. It can also be shown that the overall\nreconstruction has an efficient closed-form implementation if the associated\nsymmetric smoothing filter is low rank. The results are validated with\nexperiments on image inpainting.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 21:24:55 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Chan", "Stanley H.", ""]]}, {"id": "1602.00749", "submitter": "Pichao Wang", "authors": "Pichao Wang, Zhaoyang Li, Yonghong Hou and Wanqing Li", "title": "Combining ConvNets with Hand-Crafted Features for Action Recognition\n  Based on an HMM-SVM Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new framework for RGB-D-based action recognition that\ntakes advantages of hand-designed features from skeleton data and deeply\nlearned features from depth maps, and exploits effectively both the local and\nglobal temporal information. Specifically, depth and skeleton data are firstly\naugmented for deep learning and making the recognition insensitive to view\nvariance. Secondly, depth sequences are segmented using the hand-crafted\nfeatures based on skeleton joints motion histogram to exploit the local\ntemporal information. All training se gments are clustered using an Infinite\nGaussian Mixture Model (IGMM) through Bayesian estimation and labelled for\ntraining Convolutional Neural Networks (ConvNets) on the depth maps. Thus, a\ndepth sequence can be reliably encoded into a sequence of segment labels.\nFinally, the sequence of labels is fed into a joint Hidden Markov Model and\nSupport Vector Machine (HMM-SVM) classifier to explore the global temporal\ninformation for final recognition.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 23:57:22 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Wang", "Pichao", ""], ["Li", "Zhaoyang", ""], ["Hou", "Yonghong", ""], ["Li", "Wanqing", ""]]}, {"id": "1602.00753", "submitter": "Hessam Bagherinezhad", "authors": "Hessam Bagherinezhad, Hannaneh Hajishirzi, Yejin Choi, Ali Farhadi", "title": "Are Elephants Bigger than Butterflies? Reasoning about Sizes of Objects", "comments": "To appear in AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human vision greatly benefits from the information about sizes of objects.\nThe role of size in several visual reasoning tasks has been thoroughly explored\nin human perception and cognition. However, the impact of the information about\nsizes of objects is yet to be determined in AI. We postulate that this is\nmainly attributed to the lack of a comprehensive repository of size\ninformation. In this paper, we introduce a method to automatically infer object\nsizes, leveraging visual and textual information from web. By maximizing the\njoint likelihood of textual and visual observations, our method learns reliable\nrelative size estimates, with no explicit human supervision. We introduce the\nrelative size dataset and show that our method outperforms competitive textual\nand visual baselines in reasoning about size comparisons.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 00:16:39 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Bagherinezhad", "Hessam", ""], ["Hajishirzi", "Hannaneh", ""], ["Choi", "Yejin", ""], ["Farhadi", "Ali", ""]]}, {"id": "1602.00763", "submitter": "Alex Bewley", "authors": "Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, Ben Upcroft", "title": "Simple Online and Realtime Tracking", "comments": "Presented at ICIP 2016, code is available at\n  https://github.com/abewley/sort", "journal-ref": null, "doi": "10.1109/ICIP.2016.7533003", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores a pragmatic approach to multiple object tracking where\nthe main focus is to associate objects efficiently for online and realtime\napplications. To this end, detection quality is identified as a key factor\ninfluencing tracking performance, where changing the detector can improve\ntracking by up to 18.9%. Despite only using a rudimentary combination of\nfamiliar techniques such as the Kalman Filter and Hungarian algorithm for the\ntracking components, this approach achieves an accuracy comparable to\nstate-of-the-art online trackers. Furthermore, due to the simplicity of our\ntracking method, the tracker updates at a rate of 260 Hz which is over 20x\nfaster than other state-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 01:39:28 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 11:59:38 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Bewley", "Alex", ""], ["Ge", "Zongyuan", ""], ["Ott", "Lionel", ""], ["Ramos", "Fabio", ""], ["Upcroft", "Ben", ""]]}, {"id": "1602.00828", "submitter": "Hossein Rahmani", "authors": "Hossein Rahmani and Ajmal Mian and Mubarak Shah", "title": "Learning a Deep Model for Human Action Recognition from Novel Viewpoints", "comments": null, "journal-ref": "Phys. Rev. D 94, 065007 (2016)", "doi": "10.1103/PhysRevD.94.065007", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing human actions from unknown and unseen (novel) views is a\nchallenging problem. We propose a Robust Non-Linear Knowledge Transfer Model\n(R-NKTM) for human action recognition from novel views. The proposed R-NKTM is\na deep fully-connected neural network that transfers knowledge of human actions\nfrom any unknown view to a shared high-level virtual view by finding a\nnon-linear virtual path that connects the views. The R-NKTM is learned from\ndense trajectories of synthetic 3D human models fitted to real motion capture\ndata and generalizes to real videos of human actions. The strength of our\ntechnique is that we learn a single R-NKTM for all actions and all viewpoints\nfor knowledge transfer of any real human action video without the need for\nre-training or fine-tuning the model. Thus, R-NKTM can efficiently scale to\nincorporate new action classes. R-NKTM is learned with dummy labels and does\nnot require knowledge of the camera viewpoint at any stage. Experiments on\nthree benchmark cross-view human action datasets show that our method\noutperforms existing state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 08:42:44 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Rahmani", "Hossein", ""], ["Mian", "Ajmal", ""], ["Shah", "Mubarak", ""]]}, {"id": "1602.00904", "submitter": "Vangelis Oikonomou", "authors": "Vangelis P. Oikonomou, Georgios Liaros, Kostantinos Georgiadis,\n  Elisavet Chatzilari, Katerina Adam, Spiros Nikolopoulos and Ioannis\n  Kompatsiaris", "title": "Comparative evaluation of state-of-the-art algorithms for SSVEP-based\n  BCIs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-computer interfaces (BCIs) have been gaining momentum in making\nhuman-computer interaction more natural, especially for people with\nneuro-muscular disabilities. Among the existing solutions the systems relying\non electroencephalograms (EEG) occupy the most prominent place due to their\nnon-invasiveness. However, the process of translating EEG signals into computer\ncommands is far from trivial, since it requires the optimization of many\ndifferent parameters that need to be tuned jointly. In this report, we focus on\nthe category of EEG-based BCIs that rely on Steady-State-Visual-Evoked\nPotentials (SSVEPs) and perform a comparative evaluation of the most promising\nalgorithms existing in the literature. More specifically, we define a set of\nalgorithms for each of the various different parameters composing a BCI system\n(i.e. filtering, artifact removal, feature extraction, feature selection and\nclassification) and study each parameter independently by keeping all other\nparameters fixed. The results obtained from this evaluation process are\nprovided together with a dataset consisting of the 256-channel, EEG signals of\n11 subjects, as well as a processing toolbox for reproducing the results and\nsupporting further experimentation. In this way, we manage to make available\nfor the community a state-of-the-art baseline for SSVEP-based BCIs that can be\nused as a basis for introducing novel methods and approaches.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 12:31:48 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 09:59:44 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Oikonomou", "Vangelis P.", ""], ["Liaros", "Georgios", ""], ["Georgiadis", "Kostantinos", ""], ["Chatzilari", "Elisavet", ""], ["Adam", "Katerina", ""], ["Nikolopoulos", "Spiros", ""], ["Kompatsiaris", "Ioannis", ""]]}, {"id": "1602.00955", "submitter": "Dengxin Dai", "authors": "Dengxin Dai, Luc Van Gool", "title": "Unsupervised High-level Feature Learning by Ensemble Projection for\n  Semi-supervised Image Classification and Image Clustering", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of image classification with limited or\nno annotations, but abundant unlabeled data. The setting exists in many tasks\nsuch as semi-supervised image classification, image clustering, and image\nretrieval. Unlike previous methods, which develop or learn sophisticated\nregularizers for classifiers, our method learns a new image representation by\nexploiting the distribution patterns of all available data for the task at\nhand. Particularly, a rich set of visual prototypes are sampled from all\navailable data, and are taken as surrogate classes to train discriminative\nclassifiers; images are projected via the classifiers; the projected values,\nsimilarities to the prototypes, are stacked to build the new feature vector.\nThe training set is noisy. Hence, in the spirit of ensemble learning we create\na set of such training sets which are all diverse, leading to diverse\nclassifiers. The method is dubbed Ensemble Projection (EP). EP captures not\nonly the characteristics of individual images, but also the relationships among\nimages. It is conceptually simple and computationally efficient, yet effective\nand flexible. Experiments on eight standard datasets show that: (1) EP\noutperforms previous methods for semi-supervised image classification; (2) EP\nproduces promising results for self-taught image classification, where\nunlabeled samples are a random collection of images rather than being from the\nsame distribution as the labeled ones; and (3) EP improves over the original\nfeatures for image clustering. The code of the method is available on the\nproject page.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 14:53:36 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2016 13:58:00 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Dai", "Dengxin", ""], ["Van Gool", "Luc", ""]]}, {"id": "1602.00970", "submitter": "Paolo Napoletano", "authors": "Paolo Napoletano", "title": "Visual descriptors for content-based retrieval of remote sensing images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an extensive evaluation of visual descriptors for\nthe content-based retrieval of remote sensing (RS) images. The evaluation\nincludes global hand-crafted, local hand-crafted, and Convolutional Neural\nNetwork (CNNs) features coupled with four different Content-Based Image\nRetrieval schemes. We conducted all the experiments on two publicly available\ndatasets: the 21-class UC Merced Land Use/Land Cover (LandUse) dataset and\n19-class High-resolution Satellite Scene dataset (SceneSat). The content of RS\nimages might be quite heterogeneous, ranging from images containing fine\ngrained textures, to coarse grained ones or to images containing objects. It is\ntherefore not obvious in this domain, which descriptor should be employed to\ndescribe images having such a variability. Results demonstrate that CNN-based\nfeatures perform better than both global and and local hand-crafted features\nwhatever is the retrieval scheme adopted. Features extracted from SatResNet-50,\na residual CNN suitable fine-tuned on the RS domain, shows much better\nperformance than a residual CNN pre-trained on multimedia scene and object\nimages. Features extracted from NetVLAD, a CNN that considers both CNN and\nlocal features, works better than others CNN solutions on those images that\ncontain fine-grained textures and objects.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 15:19:16 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2016 11:28:46 GMT"}, {"version": "v3", "created": "Wed, 18 Jan 2017 11:18:52 GMT"}, {"version": "v4", "created": "Mon, 6 Feb 2017 17:58:37 GMT"}, {"version": "v5", "created": "Tue, 8 Aug 2017 09:36:07 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Napoletano", "Paolo", ""]]}, {"id": "1602.00985", "submitter": "Pouya Bashivan", "authors": "Pouya Bashivan, Irina Rish, Steve Heisig", "title": "Mental State Recognition via Wearable EEG", "comments": "Presented at MLINI-2015 workshop, 2015 (arXiv:cs/0101200)", "journal-ref": "Proceedings of 5th NIPS workshop on Machine Learning and\n  Interpretation in Neuroimaging (MLINI15) (2015) 5-1", "doi": null, "report-no": "MLINI/2015/20", "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing quality and affordability of consumer electroencephalogram\n(EEG) headsets make them attractive for situations where medical grade devices\nare impractical. Predicting and tracking cognitive states is possible for tasks\nthat were previously not conducive to EEG monitoring. For instance, monitoring\noperators for states inappropriate to the task (e.g. drowsy drivers), tracking\nmental health (e.g. anxiety) and productivity (e.g. tiredness) are among\npossible applications for the technology. Consumer grade EEG headsets are\naffordable and relatively easy to use, but they lack the resolution and quality\nof signal that can be achieved using medical grade EEG devices. Thus, the key\nquestions remain: to what extent are wearable EEG devices capable of mental\nstate recognition, and what kind of mental states can be accurately recognized\nwith these devices? In this work, we examined responses to two different types\nof input: instructional (logical) versus recreational (emotional) videos, using\na range of machine-learning methods. We tried SVMs, sparse logistic regression,\nand Deep Belief Networks, to discriminate between the states of mind induced by\ndifferent types of video input, that can be roughly labeled as logical vs.\nemotional. Our results demonstrate a significant potential of wearable EEG\ndevices in differentiating cognitive states between situations with large\ncontextual but subtle apparent differences.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 15:55:20 GMT"}, {"version": "v2", "created": "Sun, 5 Jun 2016 14:18:48 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Bashivan", "Pouya", ""], ["Rish", "Irina", ""], ["Heisig", "Steve", ""]]}, {"id": "1602.00991", "submitter": "Peter Ondruska", "authors": "Peter Ondruska and Ingmar Posner", "title": "Deep Tracking: Seeing Beyond Seeing Using Recurrent Neural Networks", "comments": "Published in The Thirtieth AAAI Conference on Artificial Intelligence\n  (AAAI-16), Video: https://youtu.be/cdeWCpfUGWc, Code:\n  http://mrg.robots.ox.ac.uk/mrg_people/peter-ondruska/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents to the best of our knowledge the first end-to-end object\ntracking approach which directly maps from raw sensor input to object tracks in\nsensor space without requiring any feature engineering or system identification\nin the form of plant or sensor models. Specifically, our system accepts a\nstream of raw sensor data at one end and, in real-time, produces an estimate of\nthe entire environment state at the output including even occluded objects. We\nachieve this by framing the problem as a deep learning task and exploit\nsequence models in the form of recurrent neural networks to learn a mapping\nfrom sensor measurements to object tracks. In particular, we propose a learning\nmethod based on a form of input dropout which allows learning in an\nunsupervised manner, only based on raw, occluded sensor data without access to\nground-truth annotations. We demonstrate our approach using a synthetic dataset\ndesigned to mimic the task of tracking objects in 2D laser data -- as commonly\nencountered in robotics applications -- and show that it learns to track many\ndynamic objects despite occlusions and the presence of sensor noise.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 16:10:16 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 22:09:05 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Ondruska", "Peter", ""], ["Posner", "Ingmar", ""]]}, {"id": "1602.00997", "submitter": "Amit Kumar", "authors": "Amit Kumar, Rishabh Bindal, Soumya Indela and Michael Rotkowitz", "title": "Head Pose Estimation of Occluded Faces using Regularized Regression", "comments": "Submitted to ICIP'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents regression methods for estimation of head pose from\noccluded 2-D face images. The process primarily involves reconstructing a face\nfrom its occluded image, followed by classification. Typical methods for\nreconstruction assume that the pixel errors of the occluded regions are\nindependent. However, such an assumption is not true in the case of occlusion,\nbecause of its inherent contiguous nature. Hence, we use nuclear norm as a\nmetric that can describe well the structure of the error. We also use LASSO\nRegression based l1 - regularization to improve reconstruction. Next, we\nimplement Nuclear Norm Regularized Regression (NR), and also our proposed\nmethod, for reconstruction and subsequent classification. Finally, we compare\nthe performance of the methods in terms of accuracy of head pose estimation of\noccluded faces.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 16:27:18 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Kumar", "Amit", ""], ["Bindal", "Rishabh", ""], ["Indela", "Soumya", ""], ["Rotkowitz", "Michael", ""]]}, {"id": "1602.01006", "submitter": "Hossam Isack", "authors": "Hossam Isack, Yuri Boykov, Olga Veksler", "title": "A-expansion for multiple \"hedgehog\" shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overlapping colors and cluttered or weak edges are common segmentation\nproblems requiring additional regularization. For example, star-convexity is\npopular for interactive single object segmentation due to simplicity and\namenability to exact graph cut optimization. This paper proposes an approach to\nmultiobject segmentation where objects could be restricted to separate\n\"hedgehog\" shapes. We show that a-expansion moves are submodular for our\nmulti-shape constraints. Each \"hedgehog\" shape has its surface normals\nconstrained by some vector field, e.g. gradients of a distance transform for\nuser scribbles. Tight constraint give an extreme case of a shape prior\nenforcing skeleton consistency with the scribbles. Wider cones of allowed\nnormals gives more relaxed hedgehog shapes. A single click and +/-90 degrees\nnormal orientation constraints reduce our hedgehog prior to star-convexity. If\nall hedgehogs come from single clicks then our approach defines multi-star\nprior. Our general method has significantly more applications than standard\none-star segmentation. For example, in medical data we can separate multiple\nnon-star organs with similar appearances and weak or noisy edges.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 16:42:27 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Isack", "Hossam", ""], ["Boykov", "Yuri", ""], ["Veksler", "Olga", ""]]}, {"id": "1602.01125", "submitter": "William Smith", "authors": "Anil Bas, William A. P. Smith, Timo Bolkart and Stefanie Wuhrer", "title": "Fitting a 3D Morphable Model to Edges: A Comparison Between Hard and\n  Soft Correspondences", "comments": "To appear in ACCV 2016 Workshop on Facial Informatics", "journal-ref": null, "doi": "10.1007/978-3-319-54427-4_28", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fully automatic method for fitting a 3D morphable model to\nsingle face images in arbitrary pose and lighting. Our approach relies on\ngeometric features (edges and landmarks) and, inspired by the iterated closest\npoint algorithm, is based on computing hard correspondences between model\nvertices and edge pixels. We demonstrate that this is superior to previous work\nthat uses soft correspondences to form an edge-derived cost surface that is\nminimised by nonlinear optimisation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 21:43:15 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2016 13:25:00 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Bas", "Anil", ""], ["Smith", "William A. P.", ""], ["Bolkart", "Timo", ""], ["Wuhrer", "Stefanie", ""]]}, {"id": "1602.01168", "submitter": "Zhuolin Jiang", "authors": "Zhuolin Jiang, Yaming Wang, Larry Davis, Walt Andrews, Viktor Rozgic", "title": "Learning Discriminative Features via Label Consistent Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.MM cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNN) enforces supervised information only\nat the output layer, and hidden layers are trained by back propagating the\nprediction error from the output layer without explicit supervision. We propose\na supervised feature learning approach, Label Consistent Neural Network, which\nenforces direct supervision in late hidden layers. We associate each neuron in\na hidden layer with a particular class label and encourage it to be activated\nfor input signals from the same class. More specifically, we introduce a label\nconsistency regularization called \"discriminative representation error\" loss\nfor late hidden layers and combine it with classification error loss to build\nour overall objective function. This label consistency constraint alleviates\nthe common problem of gradient vanishing and tends to faster convergence; it\nalso makes the features derived from late hidden layers discriminative enough\nfor classification even using a simple $k$-NN classifier, since input signals\nfrom the same class will have very similar representations. Experimental\nresults demonstrate that our approach achieves state-of-the-art performances on\nseveral public benchmarks for action and object category recognition.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 02:41:33 GMT"}, {"version": "v2", "created": "Sun, 5 Jun 2016 02:45:35 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Jiang", "Zhuolin", ""], ["Wang", "Yaming", ""], ["Davis", "Larry", ""], ["Andrews", "Walt", ""], ["Rozgic", "Viktor", ""]]}, {"id": "1602.01197", "submitter": "Chen Huang", "authors": "Chen Huang, Chen Change Loy, Xiaoou Tang", "title": "Discriminative Sparse Neighbor Approximation for Imbalanced Learning", "comments": "11 pages, 10 figures, In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data imbalance is common in many vision tasks where one or more classes are\nrare. Without addressing this issue conventional methods tend to be biased\ntoward the majority class with poor predictive accuracy for the minority class.\nThese methods further deteriorate on small, imbalanced data that has a large\ndegree of class overlap. In this study, we propose a novel discriminative\nsparse neighbor approximation (DSNA) method to ameliorate the effect of\nclass-imbalance during prediction. Specifically, given a test sample, we first\ntraverse it through a cost-sensitive decision forest to collect a good subset\nof training examples in its local neighborhood. Then we generate from this\nsubset several class-discriminating but overlapping clusters and model each as\nan affine subspace. From these subspaces, the proposed DSNA iteratively seeks\nan optimal approximation of the test sample and outputs an unbiased prediction.\nWe show that our method not only effectively mitigates the imbalance issue, but\nalso allows the prediction to extrapolate to unseen data. The latter capability\nis crucial for achieving accurate prediction on small dataset with limited\nsamples. The proposed imbalanced learning method can be applied to both\nclassification and regression tasks at a wide range of imbalance levels. It\nsignificantly outperforms the state-of-the-art methods that do not possess an\nimbalance handling mechanism, and is found to perform comparably or even better\nthan recent deep learning methods by using hand-crafted features only.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 06:22:14 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Huang", "Chen", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}, {"id": "1602.01228", "submitter": "Frank Nielsen", "authors": "Frank Nielsen", "title": "Image and Information", "comments": "9 pages, 7 figures. to be published in french by Belin publisher for\n  a collaborative book project on \"Image and Communication\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-known old adage says that {\\em \"A picture is worth a thousand words!\"}\n(attributed to the Chinese philosopher Confucius ca 500 years BC). But more\nprecisely, what do we mean by information in images? And how can it be\nretrieved effectively by machines? We briefly highlight these puzzling\nquestions in this column. But first of all, let us start by defining more\nprecisely what is meant by an \"Image.\"\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 09:11:45 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Nielsen", "Frank", ""]]}, {"id": "1602.01237", "submitter": "Shanshan Zhang", "authors": "Shanshan Zhang, Rodrigo Benenson, Mohamed Omran, Jan Hosang, and Bernt\n  Schiele", "title": "How Far are We from Solving Pedestrian Detection?", "comments": "CVPR16 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encouraged by the recent progress in pedestrian detection, we investigate the\ngap between current state-of-the-art methods and the \"perfect single frame\ndetector\". We enable our analysis by creating a human baseline for pedestrian\ndetection (over the Caltech dataset), and by manually clustering the recurrent\nerrors of a top detector. Our results characterize both localization and\nbackground-versus-foreground errors. To address localization errors we study\nthe impact of training annotation noise on the detector performance, and show\nthat we can improve even with a small portion of sanitized training data. To\naddress background/foreground discrimination, we study convnets for pedestrian\ndetection, and discuss which factors affect their performance. Other than our\nin-depth analysis, we report top performance on the Caltech dataset, and\nprovide a new sanitized set of training and test annotations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 09:45:56 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 11:33:13 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Zhang", "Shanshan", ""], ["Benenson", "Rodrigo", ""], ["Omran", "Mohamed", ""], ["Hosang", "Jan", ""], ["Schiele", "Bernt", ""]]}, {"id": "1602.01255", "submitter": "Nanne van Noord", "authors": "Nanne van Noord, Eric Postma", "title": "Learning scale-variant and scale-invariant features for deep image\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks (CNNs) require large image corpora to be\ntrained on classification tasks. The variation in image resolutions, sizes of\nobjects and patterns depicted, and image scales, hampers CNN training and\nperformance, because the task-relevant information varies over spatial scales.\nPrevious work attempting to deal with such scale variations focused on\nencouraging scale-invariant CNN representations. However, scale-invariant\nrepresentations are incomplete representations of images, because images\ncontain scale-variant information as well. This paper addresses the combined\ndevelopment of scale-invariant and scale-variant representations. We propose a\nmulti- scale CNN method to encourage the recognition of both types of features\nand evaluate it on a challenging image classification task involving\ntask-relevant characteristics at multiple scales. The results show that our\nmulti-scale CNN outperforms single-scale CNN. This leads to the conclusion that\nencouraging the combined development of a scale-invariant and scale-variant\nrepresentation in CNNs is beneficial to image recognition performance.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 10:42:04 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 15:19:52 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["van Noord", "Nanne", ""], ["Postma", "Eric", ""]]}, {"id": "1602.01410", "submitter": "Miguel Sim\\~oes", "authors": "Miguel Sim\\~oes, Luis B. Almeida, Jos\\'e Bioucas-Dias, Jocelyn\n  Chanussot", "title": "A Framework for Fast Image Deconvolution with Incomplete Observations", "comments": "IEEE Trans. Image Process., to be published. 15 pages, 11 figures.\n  MATLAB code available at\n  https://github.com/alfaiate/DeconvolutionIncompleteObs", "journal-ref": null, "doi": "10.1109/TIP.2016.2603920", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image deconvolution problems, the diagonalization of the underlying\noperators by means of the FFT usually yields very large speedups. When there\nare incomplete observations (e.g., in the case of unknown boundaries), standard\ndeconvolution techniques normally involve non-diagonalizable operators,\nresulting in rather slow methods, or, otherwise, use inexact convolution\nmodels, resulting in the occurrence of artifacts in the enhanced images. In\nthis paper, we propose a new deconvolution framework for images with incomplete\nobservations that allows us to work with diagonalized convolution operators,\nand therefore is very fast. We iteratively alternate the estimation of the\nunknown pixels and of the deconvolved image, using, e.g., an FFT-based\ndeconvolution method. This framework is an efficient, high-quality alternative\nto existing methods of dealing with the image boundaries, such as edge\ntapering. It can be used with any fast deconvolution method. We give an example\nin which a state-of-the-art method that assumes periodic boundary conditions is\nextended, through the use of this framework, to unknown boundary conditions.\nFurthermore, we propose a specific implementation of this framework, based on\nthe alternating direction method of multipliers (ADMM). We provide a proof of\nconvergence for the resulting algorithm, which can be seen as a \"partial\" ADMM,\nin which not all variables are dualized. We report experimental comparisons\nwith other primal-dual methods, where the proposed one performed at the level\nof the state of the art. Four different kinds of applications were tested in\nthe experiments: deconvolution, deconvolution with inpainting, superresolution,\nand demosaicing, all with unknown boundaries.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 18:57:02 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 17:40:01 GMT"}], "update_date": "2016-08-31", "authors_parsed": [["Sim\u00f5es", "Miguel", ""], ["Almeida", "Luis B.", ""], ["Bioucas-Dias", "Jos\u00e9", ""], ["Chanussot", "Jocelyn", ""]]}, {"id": "1602.01449", "submitter": "Christopher MacGahan", "authors": "Christopher J. MacGahan and Matthew A. Kupinski and Nathan R. Hilton\n  and Erik M. Brubaker and William C. Johnson", "title": "Development of an Ideal Observer that Incorporates Nuisance Parameters\n  and Processes List-Mode Data", "comments": null, "journal-ref": null, "doi": "10.1364/JOSAA.33.000689", "report-no": "SAND2016-0849J", "categories": "physics.data-an cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observer models were developed to process data in list-mode format in order\nto perform binary discrimination tasks for use in an arms-control-treaty\ncontext. Data used in this study was generated using GEANT4 Monte Carlo\nsimulations for photons using custom models of plutonium inspection objects and\na radiation imaging system. Observer model performance was evaluated and\npresented using the area under the receiver operating characteristic curve. The\nideal observer was studied under both signal-known-exactly conditions and in\nthe presence of unknowns such as object orientation and absolute count-rate\nvariability; when these additional sources of randomness were present, their\nincorporation into the observer yielded superior performance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 20:08:40 GMT"}], "update_date": "2016-03-21", "authors_parsed": [["MacGahan", "Christopher J.", ""], ["Kupinski", "Matthew A.", ""], ["Hilton", "Nathan R.", ""], ["Brubaker", "Erik M.", ""], ["Johnson", "William C.", ""]]}, {"id": "1602.01464", "submitter": "Rigas Kouskouridas", "authors": "Rigas Kouskouridas, Alykhan Tejani, Andreas Doumanoglou, Danhang Tang\n  and Tae-Kyun Kim", "title": "Latent-Class Hough Forests for 6 DoF Object Pose Estimation", "comments": "PAMI submission, project page:\n  http://www.iis.ee.ic.ac.uk/rkouskou/research/LCHF.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present Latent-Class Hough Forests, a method for object\ndetection and 6 DoF pose estimation in heavily cluttered and occluded\nscenarios. We adapt a state of the art template matching feature into a\nscale-invariant patch descriptor and integrate it into a regression forest\nusing a novel template-based split function. We train with positive samples\nonly and we treat class distributions at the leaf nodes as latent variables.\nDuring testing we infer by iteratively updating these distributions, providing\naccurate estimation of background clutter and foreground occlusions and, thus,\nbetter detection rate. Furthermore, as a by-product, our Latent-Class Hough\nForests can provide accurate occlusion aware segmentation masks, even in the\nmulti-instance scenario. In addition to an existing public dataset, which\ncontains only single-instance sequences with large amounts of clutter, we have\ncollected two, more challenging, datasets for multiple-instance detection\ncontaining heavy 2D and 3D clutter as well as foreground occlusions. We provide\nextensive experiments on the various parameters of the framework such as patch\nsize, number of trees and number of iterations to infer class distributions at\ntest time. We also evaluate the Latent-Class Hough Forests on all datasets\nwhere we outperform state of the art methods.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 20:53:33 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Kouskouridas", "Rigas", ""], ["Tejani", "Alykhan", ""], ["Doumanoglou", "Andreas", ""], ["Tang", "Danhang", ""], ["Kim", "Tae-Kyun", ""]]}, {"id": "1602.01517", "submitter": "Keiller Nogueira", "authors": "Keiller Nogueira, Ot\\'avio A. B. Penatti, Jefersson A. dos Santos", "title": "Towards Better Exploiting Convolutional Neural Networks for Remote\n  Sensing Scene Classification", "comments": null, "journal-ref": null, "doi": "10.1016/j.patcog.2016.07.001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an analysis of three possible strategies for exploiting the power\nof existing convolutional neural networks (ConvNets) in different scenarios\nfrom the ones they were trained: full training, fine tuning, and using ConvNets\nas feature extractors. In many applications, especially including remote\nsensing, it is not feasible to fully design and train a new ConvNet, as this\nusually requires a considerable amount of labeled data and demands high\ncomputational costs. Therefore, it is important to understand how to obtain the\nbest profit from existing ConvNets. We perform experiments with six popular\nConvNets using three remote sensing datasets. We also compare ConvNets in each\nstrategy with existing descriptors and with state-of-the-art baselines. Results\npoint that fine tuning tends to be the best performing strategy. In fact, using\nthe features from the fine-tuned ConvNet with linear SVM obtains the best\nresults. We also achieved state-of-the-art results for the three datasets used.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 00:53:32 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Nogueira", "Keiller", ""], ["Penatti", "Ot\u00e1vio A. B.", ""], ["Santos", "Jefersson A. dos", ""]]}, {"id": "1602.01528", "submitter": "Song Han", "authors": "Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark A.\n  Horowitz, William J. Dally", "title": "EIE: Efficient Inference Engine on Compressed Deep Neural Network", "comments": "External Links: TheNextPlatform: http://goo.gl/f7qX0L ; O'Reilly:\n  https://goo.gl/Id1HNT ; Hacker News: https://goo.gl/KM72SV ; Embedded-vision:\n  http://goo.gl/joQNg8 ; Talk at NVIDIA GTC'16: http://goo.gl/6wJYvn ; Talk at\n  Embedded Vision Summit: https://goo.gl/7abFNe ; Talk at Stanford University:\n  https://goo.gl/6lwuer. Published as a conference paper in ISCA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art deep neural networks (DNNs) have hundreds of millions of\nconnections and are both computationally and memory intensive, making them\ndifficult to deploy on embedded systems with limited hardware resources and\npower budgets. While custom hardware helps the computation, fetching weights\nfrom DRAM is two orders of magnitude more expensive than ALU operations, and\ndominates the required power.\n  Previously proposed 'Deep Compression' makes it possible to fit large DNNs\n(AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved by\npruning the redundant connections and having multiple connections share the\nsame weight. We propose an energy efficient inference engine (EIE) that\nperforms inference on this compressed network model and accelerates the\nresulting sparse matrix-vector multiplication with weight sharing. Going from\nDRAM to SRAM gives EIE 120x energy saving; Exploiting sparsity saves 10x;\nWeight sharing gives 8x; Skipping zero activations from ReLU saves another 3x.\nEvaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared to\nCPU and GPU implementations of the same DNN without compression. EIE has a\nprocessing power of 102GOPS/s working directly on a compressed network,\ncorresponding to 3TOPS/s on an uncompressed network, and processes FC layers of\nAlexNet at 1.88x10^4 frames/sec with a power dissipation of only 600mW. It is\n24,000x and 3,400x more energy efficient than a CPU and GPU respectively.\nCompared with DaDianNao, EIE has 2.9x, 19x and 3x better throughput, energy\nefficiency and area efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 01:28:28 GMT"}, {"version": "v2", "created": "Tue, 3 May 2016 04:27:02 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Han", "Song", ""], ["Liu", "Xingyu", ""], ["Mao", "Huizi", ""], ["Pu", "Jing", ""], ["Pedram", "Ardavan", ""], ["Horowitz", "Mark A.", ""], ["Dally", "William J.", ""]]}, {"id": "1602.01541", "submitter": "Mauricio Delbracio", "authors": "Cecilia Aguerrebere, Mauricio Delbracio, Alberto Bartesaghi, Guillermo\n  Sapiro", "title": "Fundamental Limits in Multi-image Alignment", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2600517", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of multi-image alignment, bringing different images into one\ncoordinate system, is critical in many applications with varied signal-to-noise\nratio (SNR) conditions. A great amount of effort is being invested into\ndeveloping methods to solve this problem. Several important questions thus\narise, including: Which are the fundamental limits in multi-image alignment\nperformance? Does having access to more images improve the alignment?\nTheoretical bounds provide a fundamental benchmark to compare methods and can\nhelp establish whether improvements can be made. In this work, we tackle the\nproblem of finding the performance limits in image registration when multiple\nshifted and noisy observations are available. We derive and analyze the\nCram\\'er-Rao and Ziv-Zakai lower bounds under different statistical models for\nthe underlying image. The accuracy of the derived bounds is experimentally\nassessed through a comparison to the maximum likelihood estimator. We show the\nexistence of different behavior zones depending on the difficulty level of the\nproblem, given by the SNR conditions of the input images. We find that\nincreasing the number of images is only useful below a certain SNR threshold,\nabove which the pairwise MLE estimation proves to be optimal. The analysis we\npresent here brings further insight into the fundamental limitations of the\nmulti-image alignment problem.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 02:25:52 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Aguerrebere", "Cecilia", ""], ["Delbracio", "Mauricio", ""], ["Bartesaghi", "Alberto", ""], ["Sapiro", "Guillermo", ""]]}, {"id": "1602.01557", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an and Ramin Raziperchikolaei", "title": "An ensemble diversity approach to supervised binary hashing", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary hashing is a well-known approach for fast approximate nearest-neighbor\nsearch in information retrieval. Much work has focused on affinity-based\nobjective functions involving the hash functions or binary codes. These\nobjective functions encode neighborhood information between data points and are\noften inspired by manifold learning algorithms. They ensure that the hash\nfunctions differ from each other through constraints or penalty terms that\nencourage codes to be orthogonal or dissimilar across bits, but this couples\nthe binary variables and complicates the already difficult optimization. We\npropose a much simpler approach: we train each hash function (or bit)\nindependently from each other, but introduce diversity among them using\ntechniques from classifier ensembles. Surprisingly, we find that not only is\nthis faster and trivially parallelizable, but it also improves over the more\ncomplex, coupled objective function, and achieves state-of-the-art precision\nand recall in experiments with image retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 04:59:54 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""], ["Raziperchikolaei", "Ramin", ""]]}, {"id": "1602.01599", "submitter": "Conrad Sanderson", "authors": "Johanna Carvajal, Arnold Wiliem, Chris McCool, Brian Lovell, Conrad\n  Sanderson", "title": "Comparative Evaluation of Action Recognition Methods via Riemannian\n  Manifolds, Fisher Vectors and GMMs: Ideal and Challenging Conditions", "comments": null, "journal-ref": "Lecture Notes in Computer Science (LNCS), Vol. 9794, pp. 88-100,\n  2016", "doi": "10.1007/978-3-319-42996-0_8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comparative evaluation of various techniques for action\nrecognition while keeping as many variables as possible controlled. We employ\ntwo categories of Riemannian manifolds: symmetric positive definite matrices\nand linear subspaces. For both categories we use their corresponding nearest\nneighbour classifiers, kernels, and recent kernelised sparse representations.\nWe compare against traditional action recognition techniques based on Gaussian\nmixture models and Fisher vectors (FVs). We evaluate these action recognition\ntechniques under ideal conditions, as well as their sensitivity in more\nchallenging conditions (variations in scale and translation). Despite recent\nadvancements for handling manifolds, manifold based techniques obtain the\nlowest performance and their kernel representations are more unstable in the\npresence of challenging conditions. The FV approach obtains the highest\naccuracy under ideal conditions. Moreover, FV best deals with moderate scale\nand translation changes.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 09:06:50 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 08:01:55 GMT"}, {"version": "v3", "created": "Tue, 4 Oct 2016 07:25:28 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Carvajal", "Johanna", ""], ["Wiliem", "Arnold", ""], ["McCool", "Chris", ""], ["Lovell", "Brian", ""], ["Sanderson", "Conrad", ""]]}, {"id": "1602.01601", "submitter": "Conrad Sanderson", "authors": "Johanna Carvajal, Chris McCool, Brian Lovell, Conrad Sanderson", "title": "Joint Recognition and Segmentation of Actions via Probabilistic\n  Integration of Spatio-Temporal Fisher Vectors", "comments": null, "journal-ref": "Lecture Notes in Computer Science (LNCS), Vol. 9794, pp. 115-127,\n  2016", "doi": "10.1007/978-3-319-42996-0_10", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hierarchical approach to multi-action recognition that performs\njoint classification and segmentation. A given video (containing several\nconsecutive actions) is processed via a sequence of overlapping temporal\nwindows. Each frame in a temporal window is represented through selective\nlow-level spatio-temporal features which efficiently capture relevant local\ndynamics. Features from each window are represented as a Fisher vector, which\ncaptures first and second order statistics. Instead of directly classifying\neach Fisher vector, it is converted into a vector of class probabilities. The\nfinal classification decision for each frame is then obtained by integrating\nthe class probabilities at the frame level, which exploits the overlapping of\nthe temporal windows. Experiments were performed on two datasets: s-KTH (a\nstitched version of the KTH dataset to simulate multi-actions), and the\nchallenging CMU-MMAC dataset. On s-KTH, the proposed approach achieves an\naccuracy of 85.0%, significantly outperforming two recent approaches based on\nGMMs and HMMs which obtained 78.3% and 71.2%, respectively. On CMU-MMAC, the\nproposed approach achieves an accuracy of 40.9%, outperforming the GMM and HMM\napproaches which obtained 33.7% and 38.4%, respectively. Furthermore, the\nproposed system is on average 40 times faster than the GMM based approach.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 09:16:52 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 08:11:36 GMT"}, {"version": "v3", "created": "Tue, 4 Oct 2016 07:30:25 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["Carvajal", "Johanna", ""], ["McCool", "Chris", ""], ["Lovell", "Brian", ""], ["Sanderson", "Conrad", ""]]}, {"id": "1602.01608", "submitter": "Bappaditya Mandal", "authors": "Bappaditya Mandal", "title": "Appearance Based Robot and Human Activity Recognition System", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an appearance based human activity recognition\nsystem. It uses background modeling to segment the foreground object and\nextracts useful discriminative features for representing activities performed\nby humans and robots. Subspace based method like principal component analysis\nis used to extract low dimensional features from large voluminous activity\nimages. These low dimensional features are then used to classify an activity.\nAn apparatus is designed using a webcam, which watches a robot replicating a\nhuman fall under indoor environment. In this apparatus, a robot performs\nvarious activities (like walking, bending, moving arms) replicating humans,\nwhich also includes a sudden fall. Experimental results on robot performing\nvarious activities and standard human activity recognition databases show the\nefficacy of our proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 09:51:40 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 02:44:24 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Mandal", "Bappaditya", ""]]}, {"id": "1602.01625", "submitter": "Sangheum Hwang", "authors": "Sangheum Hwang, Hyo-Eun Kim", "title": "Self-Transfer Learning for Fully Weakly Supervised Object Localization", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances of deep learning have achieved remarkable performances in\nvarious challenging computer vision tasks. Especially in object localization,\ndeep convolutional neural networks outperform traditional approaches based on\nextraction of data/task-driven features instead of hand-crafted features.\nAlthough location information of region-of-interests (ROIs) gives good prior\nfor object localization, it requires heavy annotation efforts from human\nresources. Thus a weakly supervised framework for object localization is\nintroduced. The term \"weakly\" means that this framework only uses image-level\nlabeled datasets to train a network. With the help of transfer learning which\nadopts weight parameters of a pre-trained network, the weakly supervised\nlearning framework for object localization performs well because the\npre-trained network already has well-trained class-specific features. However,\nthose approaches cannot be used for some applications which do not have\npre-trained networks or well-localized large scale images. Medical image\nanalysis is a representative among those applications because it is impossible\nto obtain such pre-trained networks. In this work, we present a \"fully\" weakly\nsupervised framework for object localization (\"semi\"-weakly is the counterpart\nwhich uses pre-trained filters for weakly supervised localization) named as\nself-transfer learning (STL). It jointly optimizes both classification and\nlocalization networks simultaneously. By controlling a supervision level of the\nlocalization network, STL helps the localization network focus on correct ROIs\nwithout any types of priors. We evaluate the proposed STL framework using two\nmedical image datasets, chest X-rays and mammograms, and achieve signiticantly\nbetter localization performance compared to previous weakly supervised\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 10:41:57 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Hwang", "Sangheum", ""], ["Kim", "Hyo-Eun", ""]]}, {"id": "1602.01644", "submitter": "Jan Egger", "authors": "Xiaojun Chen, Lu Xu, Yue Yang, Jan Egger", "title": "A semi-automatic computer-aided method for surgical template design", "comments": "18 pages, 16 figures, 2 tables, 36 references", "journal-ref": "Scientific Reports 6, Article number: 20280, 2016", "doi": "10.1038/srep20280", "report-no": null, "categories": "cs.GR cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a generalized integrated framework of semi-automatic\nsurgical template design. Several algorithms were implemented including the\nmesh segmentation, offset surface generation, collision detection, ruled\nsurface generation, etc., and a special software named TemDesigner was\ndeveloped. With a simple user interface, a customized template can be semi-\nautomatically designed according to the preoperative plan. Firstly, mesh\nsegmentation with signed scalar of vertex is utilized to partition the inner\nsurface from the input surface mesh based on the indicated point loop. Then,\nthe offset surface of the inner surface is obtained through contouring the\ndistance field of the inner surface, and segmented to generate the outer\nsurface. Ruled surface is employed to connect inner and outer surfaces.\nFinally, drilling tubes are generated according to the preoperative plan\nthrough collision detection and merging. It has been applied to the template\ndesign for various kinds of surgeries, including oral implantology, cervical\npedicle screw insertion, iliosacral screw insertion and osteotomy,\ndemonstrating the efficiency, functionality and generality of our method.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 11:33:22 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Chen", "Xiaojun", ""], ["Xu", "Lu", ""], ["Yang", "Yue", ""], ["Egger", "Jan", ""]]}, {"id": "1602.01728", "submitter": "Alexander Wong", "authors": "M. J. Shafiee, P. Siva, C. Scharfenberger, P. Fieguth, and A. Wong", "title": "NeRD: a Neural Response Divergence Approach to Visual Salience Detection", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel approach to visual salience detection via Neural\nResponse Divergence (NeRD) is proposed, where synaptic portions of deep neural\nnetworks, previously trained for complex object recognition, are leveraged to\ncompute low level cues that can be used to compute image region\ndistinctiveness. Based on this concept , an efficient visual salience detection\nframework is proposed using deep convolutional StochasticNets. Experimental\nresults using CSSD and MSRA10k natural image datasets show that the proposed\nNeRD approach can achieve improved performance when compared to\nstate-of-the-art image saliency approaches, while the attaining low\ncomputational complexity necessary for near-real-time computer vision\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 16:20:26 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Shafiee", "M. J.", ""], ["Siva", "P.", ""], ["Scharfenberger", "C.", ""], ["Fieguth", "P.", ""], ["Wong", "A.", ""]]}, {"id": "1602.01729", "submitter": "Paul Honeine", "authors": "Fei Zhu, Abderrahim Halimi, Paul Honeine, Badong Chen, Nanning Zheng", "title": "Correntropy Maximization via ADMM - Application to Robust Hyperspectral\n  Unmixing", "comments": "23 pages", "journal-ref": null, "doi": "10.1109/TGRS.2017.2696262", "report-no": null, "categories": "stat.ML cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In hyperspectral images, some spectral bands suffer from low signal-to-noise\nratio due to noisy acquisition and atmospheric effects, thus requiring robust\ntechniques for the unmixing problem. This paper presents a robust supervised\nspectral unmixing approach for hyperspectral images. The robustness is achieved\nby writing the unmixing problem as the maximization of the correntropy\ncriterion subject to the most commonly used constraints. Two unmixing problems\nare derived: the first problem considers the fully-constrained unmixing, with\nboth the non-negativity and sum-to-one constraints, while the second one deals\nwith the non-negativity and the sparsity-promoting of the abundances. The\ncorresponding optimization problems are solved efficiently using an alternating\ndirection method of multipliers (ADMM) approach. Experiments on synthetic and\nreal hyperspectral images validate the performance of the proposed algorithms\nfor different scenarios, demonstrating that the correntropy-based unmixing is\nrobust to outlier bands.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 16:21:09 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Zhu", "Fei", ""], ["Halimi", "Abderrahim", ""], ["Honeine", "Paul", ""], ["Chen", "Badong", ""], ["Zheng", "Nanning", ""]]}, {"id": "1602.01818", "submitter": "Alexander Wong", "authors": "A. G. Chung, M. J. Shafiee, and A. Wong", "title": "Random Feature Maps via a Layered Random Projection (LaRP) Framework for\n  Object Classification", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The approximation of nonlinear kernels via linear feature maps has recently\ngained interest due to their applications in reducing the training and testing\ntime of kernel-based learning algorithms. Current random projection methods\navoid the curse of dimensionality by embedding the nonlinear feature space into\na low dimensional Euclidean space to create nonlinear kernels. We introduce a\nLayered Random Projection (LaRP) framework, where we model the linear kernels\nand nonlinearity separately for increased training efficiency. The proposed\nLaRP framework was assessed using the MNIST hand-written digits database and\nthe COIL-100 object database, and showed notable improvement in object\nclassification performance relative to other state-of-the-art random projection\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 20:31:44 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Chung", "A. G.", ""], ["Shafiee", "M. J.", ""], ["Wong", "A.", ""]]}, {"id": "1602.01827", "submitter": "Yang Zhong", "authors": "Yang Zhong, Josephine Sullivan, Haibo Li", "title": "Leveraging Mid-Level Deep Representations For Predicting Face Attributes\n  in the Wild", "comments": "In proceedings of 2016 International Conference on Image Processing\n  (ICIP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting facial attributes from faces in the wild is very challenging due\nto pose and lighting variations in the real world. The key to this problem is\nto build proper feature representations to cope with these unfavourable\nconditions. Given the success of Convolutional Neural Network (CNN) in image\nclassification, the high-level CNN feature, as an intuitive and reasonable\nchoice, has been widely utilized for this problem. In this paper, however, we\nconsider the mid-level CNN features as an alternative to the high-level ones\nfor attribute prediction. This is based on the observation that face attributes\nare different: some of them are locally oriented while others are globally\ndefined. Our investigations reveal that the mid-level deep representations\noutperform the prediction accuracy achieved by the (fine-tuned) high-level\nabstractions. We empirically demonstrate that the midlevel representations\nachieve state-of-the-art prediction performance on CelebA and LFWA datasets.\nOur investigations also show that by utilizing the mid-level representations\none can employ a single deep network to achieve both face recognition and\nattribute prediction.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 20:58:02 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2016 07:08:05 GMT"}, {"version": "v3", "created": "Tue, 21 Jun 2016 15:52:58 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Zhong", "Yang", ""], ["Sullivan", "Josephine", ""], ["Li", "Haibo", ""]]}, {"id": "1602.01887", "submitter": "Shu Wang", "authors": "Shu Wang, Shaoting Zhang, Wei Liu and Dimitris N. Metaxas", "title": "Visual Tracking via Reliable Memories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a novel visual tracking framework that\nintelligently discovers reliable patterns from a wide range of video to resist\ndrift error for long-term tracking tasks. First, we design a Discrete Fourier\nTransform (DFT) based tracker which is able to exploit a large number of\ntracked samples while still ensures real-time performance. Second, we propose a\nclustering method with temporal constraints to explore and memorize consistent\npatterns from previous frames, named as reliable memories. By virtue of this\nmethod, our tracker can utilize uncontaminated information to alleviate\ndrifting issues. Experimental results show that our tracker performs favorably\nagainst other state of-the-art methods on benchmark datasets. Furthermore, it\nis significantly competent in handling drifts and able to robustly track\nchallenging long videos over 4000 frames, while most of others lose track at\nearly frames.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2016 23:40:14 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 22:36:07 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Wang", "Shu", ""], ["Zhang", "Shaoting", ""], ["Liu", "Wei", ""], ["Metaxas", "Dimitris N.", ""]]}, {"id": "1602.01890", "submitter": "Archith Bency", "authors": "Archith J. Bency, S. Karthikeyan, Carter De Leo, Santhoshkumar\n  Sunderrajan and B. S. Manjunath", "title": "Search Tracker: Human-derived object tracking in-the-wild through\n  large-scale search and retrieval", "comments": "Under review with the IEEE Transactions on Circuits and Systems for\n  Video Technology", "journal-ref": null, "doi": "10.1109/TCSVT.2016.2555718", "report-no": null, "categories": "cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans use context and scene knowledge to easily localize moving objects in\nconditions of complex illumination changes, scene clutter and occlusions. In\nthis paper, we present a method to leverage human knowledge in the form of\nannotated video libraries in a novel search and retrieval based setting to\ntrack objects in unseen video sequences. For every video sequence, a document\nthat represents motion information is generated. Documents of the unseen video\nare queried against the library at multiple scales to find videos with similar\nmotion characteristics. This provides us with coarse localization of objects in\nthe unseen video. We further adapt these retrieved object locations to the new\nvideo using an efficient warping scheme. The proposed method is validated on\nin-the-wild video surveillance datasets where we outperform state-of-the-art\nappearance-based trackers. We also introduce a new challenging dataset with\ncomplex object appearance changes.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 00:01:13 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Bency", "Archith J.", ""], ["Karthikeyan", "S.", ""], ["De Leo", "Carter", ""], ["Sunderrajan", "Santhoshkumar", ""], ["Manjunath", "B. S.", ""]]}, {"id": "1602.01895", "submitter": "Shijian Tang", "authors": "Shijian Tang, Song Han", "title": "Generate Image Descriptions based on Deep RNN and Memory Cells for\n  Images Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating natural language descriptions for images is a challenging task.\nThe traditional way is to use the convolutional neural network (CNN) to extract\nimage features, followed by recurrent neural network (RNN) to generate\nsentences. In this paper, we present a new model that added memory cells to\ngate the feeding of image features to the deep neural network. The intuition is\nenabling our model to memorize how much information from images should be fed\nat each stage of the RNN. Experiments on Flickr8K and Flickr30K datasets showed\nthat our model outperforms other state-of-the-art models with higher BLEU\nscores.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 00:17:18 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Tang", "Shijian", ""], ["Han", "Song", ""]]}, {"id": "1602.01921", "submitter": "Haanvid Lee", "authors": "Haanvid Lee, Minju Jung, and Jun Tani", "title": "Recognition of Visually Perceived Compositional Human Actions by\n  Multiple Spatio-Temporal Scales Recurrent Neural Networks", "comments": "10 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current paper proposes a novel neural network model for recognizing\nvisually perceived human actions. The proposed multiple spatio-temporal scales\nrecurrent neural network (MSTRNN) model is derived by introducing multiple\ntimescale recurrent dynamics to the conventional convolutional neural network\nmodel. One of the essential characteristics of the MSTRNN is that its\narchitecture imposes both spatial and temporal constraints simultaneously on\nthe neural activity which vary in multiple scales among different layers. As\nsuggested by the principle of the upward and downward causation, it is assumed\nthat the network can develop meaningful structures such as functional hierarchy\nby taking advantage of such constraints during the course of learning. To\nevaluate the characteristics of the model, the current study uses three types\nof human action video dataset consisting of different types of primitive\nactions and different levels of compositionality on them. The performance of\nthe MSTRNN in testing with these dataset is compared with the ones by other\nrepresentative deep learning models used in the field. The analysis of the\ninternal representation obtained through the learning with the dataset\nclarifies what sorts of functional hierarchy can be developed by extracting the\nessential compositionality underlying the dataset.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 04:00:16 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 07:59:03 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2017 16:33:49 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Lee", "Haanvid", ""], ["Jung", "Minju", ""], ["Tani", "Jun", ""]]}, {"id": "1602.01927", "submitter": "Jamil Ahmad", "authors": "Zanobya N. Khan, Rashid Jalal Qureshi, and Jamil Ahmad", "title": "On Feature based Delaunay Triangulation for Palmprint Recognition", "comments": null, "journal-ref": "Journal of Platform Technology, 3(4), 9-18 (2015)", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Authentication of individuals via palmprint based biometric system is\nbecoming very popular due to its reliability as it contains unique and stable\nfeatures. In this paper, we present a novel approach for palmprint recognition\nand its representation. To extract the palm lines, local thresholding technique\nNiblack binarization algorithm is adopted. The endpoints of these lines are\ndetermined and a connection is created among them using the Delaunay\ntriangulation thereby generating a distinct topological structure of each\npalmprint. Next, we extract different geometric as well as quantitative\nfeatures from the triangles of the Delaunay triangulation that assist in\nidentifying different individuals. To ensure that the proposed approach is\ninvariant to rotation and scaling, features were made relative to topological\nand geometrical structure of the palmprint. The similarity of the two\npalmprints is computed using the weighted sum approach and compared with the\nk-nearest neighbor. The experimental results obtained reflect the effectiveness\nof the proposed approach to discriminate between different palmprint images and\nthus achieved a recognition rate of 90% over large databases.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 05:31:41 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Khan", "Zanobya N.", ""], ["Qureshi", "Rashid Jalal", ""], ["Ahmad", "Jamil", ""]]}, {"id": "1602.01940", "submitter": "Liangcheng Liu", "authors": "Liangchen Liu and Arnold Wiliem and Shaokang Chen and Brian C. Lovell", "title": "Automatic and Quantitative evaluation of attribute discovery methods", "comments": "9 pages, WACV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many automatic attribute discovery methods have been developed to extract a\nset of visual attributes from images for various tasks. However, despite good\nperformance in some image classification tasks, it is difficult to evaluate\nwhether these methods discover meaningful attributes and which one is the best\nto find the attributes for image descriptions. An intuitive way to evaluate\nthis is to manually verify whether consistent identifiable visual concepts\nexist to distinguish between positive and negative images of an attribute. This\nmanual checking is tedious, labor intensive and expensive and it is very hard\nto get quantitative comparisons between different methods. In this work, we\ntackle this problem by proposing an attribute meaningfulness metric, that can\nperform automatic evaluation on the meaningfulness of attribute sets as well as\nachieving quantitative comparisons. We apply our proposed metric to recent\nautomatic attribute discovery methods and popular hashing methods on three\nattribute datasets. A user study is also conducted to validate the\neffectiveness of the metric. In our evaluation, we gleaned some insights that\ncould be beneficial in developing automatic attribute discovery methods to\ngenerate meaningful attributes. To the best of our knowledge, this is the first\nwork to quantitatively measure the semantic content of automatically discovered\nattributes.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 07:43:08 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Liu", "Liangchen", ""], ["Wiliem", "Arnold", ""], ["Chen", "Shaokang", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1602.02022", "submitter": "Jan Egger", "authors": "Dzenan Zukic, Jan Egger, Miriam H. A. Bauer, Daniela Kuhnt, Barbara\n  Carl, Bernd Freisleben, Andreas Kolb, Christopher Nimsky", "title": "Preoperative Volume Determination for Pituitary Adenoma", "comments": "7 pages, 6 figures, 1 table, 16 references in Proc. SPIE 7963,\n  Medical Imaging 2011: Computer-Aided Diagnosis, 79632T (9 March 2011). arXiv\n  admin note: text overlap with arXiv:1103.1778", "journal-ref": null, "doi": "10.1117/12.877660", "report-no": null, "categories": "cs.CV cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most common sellar lesion is the pituitary adenoma, and sellar tumors are\napproximately 10-15% of all intracranial neoplasms. Manual slice-by-slice\nsegmentation takes quite some time that can be reduced by using the appropriate\nalgorithms. In this contribution, we present a segmentation method for\npituitary adenoma. The method is based on an algorithm that we have applied\nrecently to segmenting glioblastoma multiforme. A modification of this scheme\nis used for adenoma segmentation that is much harder to perform, due to lack of\ncontrast-enhanced boundaries. In our experimental evaluation, neurosurgeons\nperformed manual slice-by-slice segmentation of ten magnetic resonance imaging\n(MRI) cases. The segmentations were compared to the segmentation results of the\nproposed method using the Dice Similarity Coefficient (DSC). The average DSC\nfor all datasets was 75.92% +/- 7.24%. A manual segmentation took about four\nminutes and our algorithm required about one second.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 14:08:21 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Zukic", "Dzenan", ""], ["Egger", "Jan", ""], ["Bauer", "Miriam H. A.", ""], ["Kuhnt", "Daniela", ""], ["Carl", "Barbara", ""], ["Freisleben", "Bernd", ""], ["Kolb", "Andreas", ""], ["Nimsky", "Christopher", ""]]}, {"id": "1602.02023", "submitter": "Nadia Robertini", "authors": "Nadia Robertini, Edilson De Aguiar, Thomas Helten, Christian Theobalt", "title": "Efficient Multi-view Performance Capture of Fine-Scale Surface Detail", "comments": "3D Vision (3DV), 2014 2nd International Conference on", "journal-ref": null, "doi": "10.1109/3DV.2014.46", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new effective way for performance capture of deforming meshes\nwith fine-scale time-varying surface detail from multi-view video. Our method\nbuilds up on coarse 4D surface reconstructions, as obtained with commonly used\ntemplate-based methods. As they only capture models of coarse-to-medium scale\ndetail, fine scale deformation detail is often done in a second pass by using\nstereo constraints, features, or shading-based refinement. In this paper, we\npropose a new effective and stable solution to this second step. Our framework\ncreates an implicit representation of the deformable mesh using a dense\ncollection of 3D Gaussian functions on the surface, and a set of 2D Gaussians\nfor the images. The fine scale deformation of all mesh vertices that maximizes\nphoto-consistency can be efficiently found by densely optimizing a new\nmodel-to-image consistency energy on all vertex positions. A principal\nadvantage is that our problem formulation yields a smooth closed form energy\nwith implicit occlusion handling and analytic derivatives. Error-prone\ncorrespondence finding, or discrete sampling of surface displacement values are\nalso not needed. We show several reconstructions of human subjects wearing\nloose clothing, and we qualitatively and quantitatively show that we robustly\ncapture more detail than related methods.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 14:08:47 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Robertini", "Nadia", ""], ["De Aguiar", "Edilson", ""], ["Helten", "Thomas", ""], ["Theobalt", "Christian", ""]]}, {"id": "1602.02130", "submitter": "Enzo Ferrante", "authors": "Mahsa Shakeri, Stavros Tsogkas (CVN, GALEN), Enzo Ferrante (CVN,\n  GALEN), Sarah Lippe, Samuel Kadoury, Nikos Paragios (CVN, GALEN), Iasonas\n  Kokkinos (CVN, GALEN)", "title": "Sub-cortical brain structure segmentation using F-CNN's", "comments": "ISBI 2016: International Symposium on Biomedical Imaging, Apr 2016,\n  Prague, Czech Republic", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a deep learning approach for segmenting sub-cortical\nstructures of the human brain in Magnetic Resonance (MR) image data. We draw\ninspiration from a state-of-the-art Fully-Convolutional Neural Network (F-CNN)\narchitecture for semantic segmentation of objects in natural images, and adapt\nit to our task. Unlike previous CNN-based methods that operate on image\npatches, our model is applied on a full blown 2D image, without any alignment\nor registration steps at testing time. We further improve segmentation results\nby interpreting the CNN output as potentials of a Markov Random Field (MRF),\nwhose topology corresponds to a volumetric grid. Alpha-expansion is used to\nperform approximate inference imposing spatial volumetric homogeneity to the\nCNN priors. We compare the performance of the proposed pipeline with a similar\nsystem using Random Forest-based priors, as well as state-of-art segmentation\nalgorithms, and show promising results on two different brain MRI datasets.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 19:32:39 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Shakeri", "Mahsa", "", "CVN, GALEN"], ["Tsogkas", "Stavros", "", "CVN, GALEN"], ["Ferrante", "Enzo", "", "CVN,\n  GALEN"], ["Lippe", "Sarah", "", "CVN, GALEN"], ["Kadoury", "Samuel", "", "CVN, GALEN"], ["Paragios", "Nikos", "", "CVN, GALEN"], ["Kokkinos", "Iasonas", "", "CVN, GALEN"]]}, {"id": "1602.02139", "submitter": "Pedro Chamorro-Posada", "authors": "P. Chamorro-Posada", "title": "A simple method for estimating the fractal dimension from digital\n  images: The compression dimension", "comments": null, "journal-ref": "Chaos Solitons Fract 91 (2016) 562-572", "doi": "10.1016/j.chaos.2016.08.002", "report-no": null, "categories": "cs.GR cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fractal structure of real world objects is often analyzed using digital\nimages. In this context, the compression fractal dimension is put forward. It\nprovides a simple method for the direct estimation of the dimension of fractals\nstored as digital image files. The computational scheme can be implemented\nusing readily available free software. Its simplicity also makes it very\ninteresting for introductory elaborations of basic concepts of fractal\ngeometry, complexity, and information theory. A test of the computational\nscheme using limited-quality images of well-defined fractal sets obtained from\nthe Internet and free software has been performed. Also, a systematic\nevaluation of the proposed method using computer generated images of the\nWeierstrass cosine function shows an accuracy comparable to those of the\nmethods most commonly used to estimate the dimension of fractal data sequences\napplied to the same test problem.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 23:43:26 GMT"}, {"version": "v2", "created": "Fri, 5 Aug 2016 12:27:51 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Chamorro-Posada", "P.", ""]]}, {"id": "1602.02343", "submitter": "Carlos Torres", "authors": "Carlos Torres, Victor Fragoso, Scott D. Hammond, Jeffrey C. Fried, and\n  B.S. Manjunath", "title": "Eye-CU: Sleep Pose Classification for Healthcare using Multimodal\n  Multiview Data", "comments": "Ten-page manuscript including references and ten figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manual analysis of body poses of bed-ridden patients requires staff to\ncontinuously track and record patient poses. Two limitations in the\ndissemination of pose-related therapies are scarce human resources and\nunreliable automated systems. This work addresses these issues by introducing a\nnew method and a new system for robust automated classification of sleep poses\nin an Intensive Care Unit (ICU) environment. The new method,\ncoupled-constrained Least-Squares (cc-LS), uses multimodal and multiview (MM)\ndata and finds the set of modality trust values that minimizes the difference\nbetween expected and estimated labels. The new system, Eye-CU, is an affordable\nmulti-sensor modular system for unobtrusive data collection and analysis in\nhealthcare. Experimental results indicate that the performance of cc-LS matches\nthe performance of existing methods in ideal scenarios. This method outperforms\nthe latest techniques in challenging scenarios by 13% for those with poor\nillumination and by 70% for those with both poor illumination and occlusions.\nResults also show that a reduced Eye-CU configuration can classify poses\nwithout pressure information with only a slight drop in its performance.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 06:33:08 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2016 06:15:37 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Torres", "Carlos", ""], ["Fragoso", "Victor", ""], ["Hammond", "Scott D.", ""], ["Fried", "Jeffrey C.", ""], ["Manjunath", "B. S.", ""]]}, {"id": "1602.02389", "submitter": "Tom Zahavy", "authors": "Tom Zahavy, Bingyi Kang, Alex Sivak, Jiashi Feng, Huan Xu, Shie Mannor", "title": "Ensemble Robustness and Generalization of Stochastic Deep Learning\n  Algorithms", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question why deep learning algorithms generalize so well has attracted\nincreasing research interest. However, most of the well-established approaches,\nsuch as hypothesis capacity, stability or sparseness, have not provided\ncomplete explanations (Zhang et al., 2016; Kawaguchi et al., 2017). In this\nwork, we focus on the robustness approach (Xu & Mannor, 2012), i.e., if the\nerror of a hypothesis will not change much due to perturbations of its training\nexamples, then it will also generalize well. As most deep learning algorithms\nare stochastic (e.g., Stochastic Gradient Descent, Dropout, and\nBayes-by-backprop), we revisit the robustness arguments of Xu & Mannor, and\nintroduce a new approach, ensemble robustness, that concerns the robustness of\na population of hypotheses. Through the lens of ensemble robustness, we reveal\nthat a stochastic learning algorithm can generalize well as long as its\nsensitiveness to adversarial perturbations is bounded in average over training\nexamples. Moreover, an algorithm may be sensitive to some adversarial examples\n(Goodfellow et al., 2015) but still generalize well. To support our claims, we\nprovide extensive simulations for different deep learning algorithms and\ndifferent network architectures exhibiting a strong correlation between\nensemble robustness and the ability to generalize.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 16:50:14 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2016 01:59:39 GMT"}, {"version": "v3", "created": "Wed, 29 Jun 2016 21:02:34 GMT"}, {"version": "v4", "created": "Sun, 5 Nov 2017 12:18:24 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Zahavy", "Tom", ""], ["Kang", "Bingyi", ""], ["Sivak", "Alex", ""], ["Feng", "Jiashi", ""], ["Xu", "Huan", ""], ["Mannor", "Shie", ""]]}, {"id": "1602.02434", "submitter": "Shervin Minaee", "authors": "Shervin Minaee and Yao Wang", "title": "Screen Content Image Segmentation Using Sparse Decomposition and Total\n  Variation Minimization", "comments": "5 pages in IEEE, International Conference on Image Processing, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse decomposition has been widely used for different applications, such as\nsource separation, image classification, image denoising and more. This paper\npresents a new algorithm for segmentation of an image into background and\nforeground text and graphics using sparse decomposition and total variation\nminimization. The proposed method is designed based on the assumption that the\nbackground part of the image is smoothly varying and can be represented by a\nlinear combination of a few smoothly varying basis functions, while the\nforeground text and graphics can be modeled with a sparse component overlaid on\nthe smooth background. The background and foreground are separated using a\nsparse decomposition framework regularized with a few suitable regularization\nterms which promotes the sparsity and connectivity of foreground pixels. This\nalgorithm has been tested on a dataset of images extracted from HEVC standard\ntest sequences for screen content coding, and is shown to have superior\nperformance over some prior methods, including least absolute deviation\nfitting, k-means clustering based segmentation in DjVu and shape primitive\nextraction and coding (SPEC) algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2016 22:12:16 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 23:45:56 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Minaee", "Shervin", ""], ["Wang", "Yao", ""]]}, {"id": "1602.02481", "submitter": "Sungjoon Choi", "authors": "Sungjoon Choi, Qian-Yi Zhou, Stephen Miller, and Vladlen Koltun", "title": "A Large Dataset of Object Scans", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have created a dataset of more than ten thousand 3D scans of real objects.\nTo create the dataset, we recruited 70 operators, equipped them with\nconsumer-grade mobile 3D scanning setups, and paid them to scan objects in\ntheir environments. The operators scanned objects of their choosing, outside\nthe laboratory and without direct supervision by computer vision professionals.\nThe result is a large and diverse collection of object scans: from shoes, mugs,\nand toys to grand pianos, construction vehicles, and large outdoor sculptures.\nWe worked with an attorney to ensure that data acquisition did not violate\nprivacy constraints. The acquired data was irrevocably placed in the public\ndomain and is available freely at http://redwood-data.org/3dscan .\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 07:20:52 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 17:21:24 GMT"}, {"version": "v3", "created": "Thu, 5 May 2016 05:35:48 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Choi", "Sungjoon", ""], ["Zhou", "Qian-Yi", ""], ["Miller", "Stephen", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1602.02522", "submitter": "Sidharth Sadani", "authors": "Qazaleh Mirsharif, Sidharth Sadani, Shishir Shah, Hanako Yoshida,\n  Joseph Burling", "title": "A Semi-Automated Method for Object Segmentation in Infant's Egocentric\n  Videos to Study Object Perception", "comments": "Accepted at CVIP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object segmentation in infant's egocentric videos is a fundamental step in\nstudying how children perceive objects in early stages of development. From the\ncomputer vision perspective, object segmentation in such videos pose quite a\nfew challenges because the child's view is unfocused, often with large head\nmovements, effecting in sudden changes in the child's point of view which leads\nto frequent change in object properties such as size, shape and illumination.\nIn this paper, we develop a semi-automated, domain specific, method to address\nthese concerns and facilitate the object annotation process for cognitive\nscientists allowing them to select and monitor the object under segmentation.\nThe method starts with an annotation from the user of the desired object and\nemploys graph cut segmentation and optical flow computation to predict the\nobject mask for subsequent video frames automatically. To maintain accuracy, we\nuse domain specific heuristic rules to re-initialize the program with new user\ninput whenever object properties change dramatically. The evaluations\ndemonstrate the high speed and accuracy of the presented method for object\nsegmentation in voluminous egocentric videos. We apply the proposed method to\ninvestigate potential patterns in object distribution in child's view at\nprogressive ages.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 10:56:22 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Mirsharif", "Qazaleh", ""], ["Sadani", "Sidharth", ""], ["Shah", "Shishir", ""], ["Yoshida", "Hanako", ""], ["Burling", "Joseph", ""]]}, {"id": "1602.02543", "submitter": "Brijnesh Jain", "authors": "Brijnesh J. Jain", "title": "Homogeneity of Cluster Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expectation and the mean of partitions generated by a cluster ensemble\nare not unique in general. This issue poses challenges in statistical inference\nand cluster stability. In this contribution, we state sufficient conditions for\nuniqueness of expectation and mean. The proposed conditions show that a unique\nmean is neither exceptional nor generic. To cope with this issue, we introduce\nhomogeneity as a measure of how likely is a unique mean for a sample of\npartitions. We show that homogeneity is related to cluster stability. This\nresult points to a possible conflict between cluster stability and diversity in\nconsensus clustering. To assess homogeneity in a practical setting, we propose\nan efficient way to compute a lower bound of homogeneity. Empirical results\nusing the k-means algorithm suggest that uniqueness of the mean partition is\nnot exceptional for real-world data. Moreover, for samples of high homogeneity,\nuniqueness can be enforced by increasing the number of data points or by\nremoving outlier partitions. In a broader context, this contribution can be\nplaced as a further step towards a statistical theory of partitions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 12:28:57 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Jain", "Brijnesh J.", ""]]}, {"id": "1602.02586", "submitter": "Hamid Tizhoosh", "authors": "Hamid R. Tizhoosh, Mehrdad J. Gangeh, Hadi Tadayyon, Gregory J.\n  Czarnota", "title": "Tumour ROI Estimation in Ultrasound Images via Radon Barcodes in\n  Patients with Locally Advanced Breast Cancer", "comments": "To appear in proceedings of The International Symposium on Biomedical\n  Imaging (ISBI), April 13-16, 2016, Prague, Czech Republic", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative ultrasound (QUS) methods provide a promising framework that can\nnon-invasively and inexpensively be used to predict or assess the tumour\nresponse to cancer treatment. The first step in using the QUS methods is to\nselect a region of interest (ROI) inside the tumour in ultrasound images.\nManual segmentation, however, is very time consuming and tedious. In this\npaper, a semi-automated approach will be proposed to roughly localize an ROI\nfor a tumour in ultrasound images of patients with locally advanced breast\ncancer (LABC). Content-based barcodes, a recently introduced binary descriptor\nbased on Radon transform, were used in order to find similar cases and estimate\na bounding box surrounding the tumour. Experiments with 33 B-scan images\nresulted in promising results with an accuracy of $81\\%$.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 14:39:01 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Tizhoosh", "Hamid R.", ""], ["Gangeh", "Mehrdad J.", ""], ["Tadayyon", "Hadi", ""], ["Czarnota", "Gregory J.", ""]]}, {"id": "1602.02644", "submitter": "Alexey Dosovitskiy", "authors": "Alexey Dosovitskiy and Thomas Brox", "title": "Generating Images with Perceptual Similarity Metrics based on Deep\n  Networks", "comments": "minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image-generating machine learning models are typically trained with loss\nfunctions based on distance in the image space. This often leads to\nover-smoothed results. We propose a class of loss functions, which we call deep\nperceptual similarity metrics (DeePSiM), that mitigate this problem. Instead of\ncomputing distances in the image space, we compute distances between image\nfeatures extracted by deep neural networks. This metric better reflects\nperceptually similarity of images and thus leads to better results. We show\nthree applications: autoencoder training, a modification of a variational\nautoencoder, and inversion of deep convolutional networks. In all cases, the\ngenerated images look sharp and resemble natural images.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 16:50:28 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 09:36:36 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Dosovitskiy", "Alexey", ""], ["Brox", "Thomas", ""]]}, {"id": "1602.02651", "submitter": "Pablo Garrido", "authors": "Pablo Garrido, Levi Valgaerts, Ole Rehmsen, Thorsten Thormaehlen,\n  Patrick Perez, Christian Theobalt", "title": "Automatic Face Reenactment", "comments": "Proceedings of the 2014 IEEE Conference on Computer Vision and\n  Pattern Recognition (8 pages)", "journal-ref": null, "doi": "10.1109/CVPR.2014.537", "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an image-based, facial reenactment system that replaces the face\nof an actor in an existing target video with the face of a user from a source\nvideo, while preserving the original target performance. Our system is fully\nautomatic and does not require a database of source expressions. Instead, it is\nable to produce convincing reenactment results from a short source video\ncaptured with an off-the-shelf camera, such as a webcam, where the user\nperforms arbitrary facial gestures. Our reenactment pipeline is conceived as\npart image retrieval and part face transfer: The image retrieval is based on\ntemporal clustering of target frames and a novel image matching metric that\ncombines appearance and motion to select candidate frames from the source\nvideo, while the face transfer uses a 2D warping strategy that preserves the\nuser's identity. Our system excels in simplicity as it does not rely on a 3D\nface model, it is robust under head motion and does not require the source and\ntarget performance to be similar. We show convincing reenactment results for\nvideos that we recorded ourselves and for low-quality footage taken from the\nInternet.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 17:05:37 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Garrido", "Pablo", ""], ["Valgaerts", "Levi", ""], ["Rehmsen", "Ole", ""], ["Thormaehlen", "Thorsten", ""], ["Perez", "Patrick", ""], ["Theobalt", "Christian", ""]]}, {"id": "1602.02660", "submitter": "Sander Dieleman", "authors": "Sander Dieleman, Jeffrey De Fauw, Koray Kavukcuoglu", "title": "Exploiting Cyclic Symmetry in Convolutional Neural Networks", "comments": "10 pages, 6 figures, accepted for publication at ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many classes of images exhibit rotational symmetry. Convolutional neural\nnetworks are sometimes trained using data augmentation to exploit this, but\nthey are still required to learn the rotation equivariance properties from the\ndata. Encoding these properties into the network architecture, as we are\nalready used to doing for translation equivariance by using convolutional\nlayers, could result in a more efficient use of the parameter budget by\nrelieving the model from learning them. We introduce four operations which can\nbe inserted into neural network models as layers, and which can be combined to\nmake these models partially equivariant to rotations. They also enable\nparameter sharing across different orientations. We evaluate the effect of\nthese architectural modifications on three datasets which exhibit rotational\nsymmetry and demonstrate improved performance with smaller models.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 17:37:16 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 11:47:18 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Dieleman", "Sander", ""], ["De Fauw", "Jeffrey", ""], ["Kavukcuoglu", "Koray", ""]]}, {"id": "1602.02720", "submitter": "Mykhail Uss Ph.D.", "authors": "M.L. Uss, B. Vozel, V.V. Lukin, K. Chehdi", "title": "Multimodal Remote Sensing Image Registration with Accuracy Estimation at\n  Local and Global Scales", "comments": "48 pages, 8 figures, 5 tables, 51 references Revised arguments in\n  sections 2 and 3. Additional test cases added in Section 4; comparison with\n  the state-of-the-art improved. References added. Conclusions unchanged.\n  Proofread", "journal-ref": null, "doi": "10.1109/TGRS.2016.2587321", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on potential accuracy of remote sensing images\nregistration. We investigate how this accuracy can be estimated without ground\ntruth available and used to improve registration quality of mono- and\nmulti-modal pair of images. At the local scale of image fragments, the\nCramer-Rao lower bound (CRLB) on registration error is estimated for each local\ncorrespondence between coarsely registered pair of images. This CRLB is defined\nby local image texture and noise properties. Opposite to the standard approach,\nwhere registration accuracy is only evaluated at the output of the registration\nprocess, such valuable information is used by us as an additional input\nknowledge. It greatly helps detecting and discarding outliers and refining the\nestimation of geometrical transformation model parameters. Based on these\nideas, a new area-based registration method called RAE (Registration with\nAccuracy Estimation) is proposed. In addition to its ability to automatically\nregister very complex multimodal image pairs with high accuracy, the RAE method\nprovides registration accuracy at the global scale as covariance matrix of\nestimation error of geometrical transformation model parameters or as\npoint-wise registration Standard Deviation. This accuracy does not depend on\nany ground truth availability and characterizes each pair of registered images\nindividually. Thus, the RAE method can identify image areas for which a\npredefined registration accuracy is guaranteed. The RAE method is proved\nsuccessful with reaching subpixel accuracy while registering eight complex\nmono/multimodal and multitemporal image pairs including optical to optical,\noptical to radar, optical to Digital Elevation Model (DEM) images and DEM to\nradar cases. Other methods employed in comparisons fail to provide in a stable\nmanner accurate results on the same test cases.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 20:05:42 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 20:16:54 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Uss", "M. L.", ""], ["Vozel", "B.", ""], ["Lukin", "V. V.", ""], ["Chehdi", "K.", ""]]}, {"id": "1602.02822", "submitter": "Xiyang Dai", "authors": "Xiyang Dai, Sameh Khamis, Yangmuzi Zhang, Larry S. Davis", "title": "Parameterizing Region Covariance: An Efficient Way To Apply Sparse Codes\n  On Second Order Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representations have been successfully applied to signal processing,\ncomputer vision and machine learning. Currently there is a trend to learn\nsparse models directly on structure data, such as region covariance. However,\nsuch methods when combined with region covariance often require complex\ncomputation. We present an approach to transform a structured sparse model\nlearning problem to a traditional vectorized sparse modeling problem by\nconstructing a Euclidean space representation for region covariance matrices.\nOur new representation has multiple advantages. Experiments on several vision\ntasks demonstrate competitive performance with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 00:07:05 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Dai", "Xiyang", ""], ["Khamis", "Sameh", ""], ["Zhang", "Yangmuzi", ""], ["Davis", "Larry S.", ""]]}, {"id": "1602.02865", "submitter": "Babak Saleh", "authors": "Babak Saleh and Ahmed Elgammal and Jacob Feldman", "title": "The Role of Typicality in Object Classification: Improving The\n  Generalization Capacity of Convolutional Neural Networks", "comments": "In Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep artificial neural networks have made remarkable progress in different\ntasks in the field of computer vision. However, the empirical analysis of these\nmodels and investigation of their failure cases has received attention\nrecently. In this work, we show that deep learning models cannot generalize to\natypical images that are substantially different from training images. This is\nin contrast to the superior generalization ability of the visual system in the\nhuman brain. We focus on Convolutional Neural Networks (CNN) as the\nstate-of-the-art models in object recognition and classification; investigate\nthis problem in more detail, and hypothesize that training CNN models suffer\nfrom unstructured loss minimization. We propose computational models to improve\nthe generalization capacity of CNNs by considering how typical a training image\nlooks like. By conducting an extensive set of experiments we show that\ninvolving a typicality measure can improve the classification results on a new\nset of images by a large margin. More importantly, this significant improvement\nis achieved without fine-tuning the CNN model on the target image set.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 05:30:33 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Saleh", "Babak", ""], ["Elgammal", "Ahmed", ""], ["Feldman", "Jacob", ""]]}, {"id": "1602.02881", "submitter": "Jan Egger", "authors": "Jing Lu, Jan Egger, Andreas Wimmer, Stefan Gro{\\ss}kopf, Bernd\n  Freisleben", "title": "Detection and Visualization of Endoleaks in CT Data for Monitoring of\n  Thoracic and Abdominal Aortic Aneurysm Stents", "comments": "7 pages, 7 figures, 1 table, 12 references, Proc. SPIE 6918, Medical\n  Imaging 2008: Visualization, Image-Guided Procedures, and Modeling, 69181F\n  (17 March 2008)", "journal-ref": null, "doi": "10.1117/12.769414", "report-no": null, "categories": "cs.CV cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an efficient algorithm for the segmentation of the\ninner and outer boundary of thoratic and abdominal aortic aneurysms (TAA & AAA)\nin computed tomography angiography (CTA) acquisitions. The aneurysm\nsegmentation includes two steps: first, the inner boundary is segmented based\non a grey level model with two thresholds; then, an adapted active contour\nmodel approach is applied to the more complicated outer boundary segmentation,\nwith its initialization based on the available inner boundary segmentation. An\nopacity image, which aims at enhancing important features while reducing\nspurious structures, is calculated from the CTA images and employed to guide\nthe deformation of the model. In addition, the active contour model is extended\nby a constraint force that prevents intersections of the inner and outer\nboundary and keeps the outer boundary at a distance, given by the thrombus\nthickness, to the inner boundary. Based upon the segmentation results, we can\nmeasure the aneurysm size at each centerline point on the centerline orthogonal\nmultiplanar reformatting (MPR) plane. Furthermore, a 3D TAA or AAA model is\nreconstructed from the set of segmented contours, and the presence of endoleaks\nis detected and highlighted. The implemented method has been evaluated on nine\nclinical CTA data sets with variations in anatomy and location of the pathology\nand has shown promising results.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 07:42:05 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Lu", "Jing", ""], ["Egger", "Jan", ""], ["Wimmer", "Andreas", ""], ["Gro\u00dfkopf", "Stefan", ""], ["Freisleben", "Bernd", ""]]}, {"id": "1602.02885", "submitter": "Yeejin Lee", "authors": "Y.J. Lee, K. Hirakawa, and T.Q. Nguyen", "title": "Joint Defogging and Demosaicking", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2016.2631880", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image defogging is a technique used extensively for enhancing visual quality\nof images in bad weather condition. Even though defogging algorithms have been\nwell studied, defogging performance is degraded by demosaicking artifacts and\nsensor noise amplification in distant scenes. In order to improve visual\nquality of restored images, we propose a novel approach to perform defogging\nand demosaicking simultaneously. We conclude that better defogging performance\nwith fewer artifacts can be achieved when a defogging algorithm is combined\nwith a demosaicking algorithm simultaneously. We also demonstrate that the\nproposed joint algorithm has the benefit of suppressing noise amplification in\ndistant scene. In addition, we validate our theoretical analysis and\nobservations for both synthesized datasets with ground truth fog-free images\nand natural scene datasets captured in a raw format.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 08:01:20 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Lee", "Y. J.", ""], ["Hirakawa", "K.", ""], ["Nguyen", "T. Q.", ""]]}, {"id": "1602.02938", "submitter": "Ralf Mikut", "authors": "Benjamin Schott and Johannes Stegmaier and Masanari Takamiya and Ralf\n  Mikut", "title": "Challenges of Integrating A Priori Information Efficiently in the\n  Discovery of Spatio-Temporal Objects in Large Databases", "comments": "Proc., 25. Workshop Computational Intelligence, Dortmund, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Using the knowledge discovery framework, it is possible to explore object\ndatabases and extract groups of objects with highly heterogeneous movement\nbehavior by efficiently integrating a priori knowledge through interacting with\nthe framework. The whole process is modular expandable and is therefore\nadaptive to any problem formulation. Further, the flexible use of different\ninformation allocation processes reveal a great potential to efficiently\nincorporate the a priori knowledge of different users in different ways.\nTherefore, the stepwise knowledge discovery process embedded in the knowledge\ndiscovery framework is described in detail to point out the flexibility of such\na system incorporating object databases from different applications. The\ndescribed framework can be used to gain knowledge out of object databases in\nmany different fields. This knowledge can be used to gain further insights and\nimprove the understanding of underlying phenomena. The functionality of the\nproposed framework is exemplarily demonstrated using a benchmark database based\non real biological object data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 11:22:58 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Schott", "Benjamin", ""], ["Stegmaier", "Johannes", ""], ["Takamiya", "Masanari", ""], ["Mikut", "Ralf", ""]]}, {"id": "1602.02995", "submitter": "Colin Lea", "authors": "Colin Lea, Austin Reiter, Rene Vidal, Gregory D. Hager", "title": "Segmental Spatiotemporal CNNs for Fine-grained Action Segmentation", "comments": "Updated from the ECCV 2016 version. We fixed an important\n  mathematical error and made the section on segmental inference clearer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint segmentation and classification of fine-grained actions is important\nfor applications of human-robot interaction, video surveillance, and human\nskill evaluation. However, despite substantial recent progress in large-scale\naction classification, the performance of state-of-the-art fine-grained action\nrecognition approaches remains low. We propose a model for action segmentation\nwhich combines low-level spatiotemporal features with a high-level segmental\nclassifier. Our spatiotemporal CNN is comprised of a spatial component that\nuses convolutional filters to capture information about objects and their\nrelationships, and a temporal component that uses large 1D convolutional\nfilters to capture information about how object relationships change across\ntime. These features are used in tandem with a semi-Markov model that models\ntransitions from one action to another. We introduce an efficient constrained\nsegmental inference algorithm for this model that is orders of magnitude faster\nthan the current approach. We highlight the effectiveness of our Segmental\nSpatiotemporal CNN on cooking and surgical action datasets for which we observe\nsubstantially improved performance relative to recent baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 14:28:44 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 20:57:21 GMT"}, {"version": "v3", "created": "Tue, 26 Jul 2016 11:54:36 GMT"}, {"version": "v4", "created": "Fri, 30 Sep 2016 15:08:34 GMT"}], "update_date": "2016-10-03", "authors_parsed": [["Lea", "Colin", ""], ["Reiter", "Austin", ""], ["Vidal", "Rene", ""], ["Hager", "Gregory D.", ""]]}, {"id": "1602.02999", "submitter": "Bappaditya Mandal", "authors": "Bappaditya Mandal", "title": "Face Recognition: Perspectives from the Real-World", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze some of our real-world deployment of face\nrecognition (FR) systems for various applications and discuss the gaps between\nexpectations of the user and what the system can deliver. We evaluate some of\nour proposed algorithms with ad-hoc modifications for applications such as FR\non wearable devices (like Google Glass), monitoring of elderly people in senior\ncitizens centers, FR of children in child care centers and face matching\nbetween a scanned IC/passport face image and a few live webcam images for\nautomatic hotel/resort checkouts. We describe each of these applications, the\nchallenges involved and proposed solutions. Since FR is intuitive in nature and\nwe human beings use it for interactions with the outside world, people have\nhigh expectations of its performance in real-world scenarios. However, we\nanalyze and discuss here that it is not the case, machine recognition of faces\nfor each of these applications poses unique challenges and demands specific\nresearch components so as to adapt in the actual sites.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 14:31:46 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Mandal", "Bappaditya", ""]]}, {"id": "1602.03012", "submitter": "Andru Putra Twinanda", "authors": "Andru P. Twinanda, Sherif Shehata, Didier Mutter, Jacques Marescaux,\n  Michel de Mathelin, Nicolas Padoy", "title": "EndoNet: A Deep Architecture for Recognition Tasks on Laparoscopic\n  Videos", "comments": "Video: https://www.youtube.com/watch?v=6v0NWrFOUUM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgical workflow recognition has numerous potential medical applications,\nsuch as the automatic indexing of surgical video databases and the optimization\nof real-time operating room scheduling, among others. As a result, phase\nrecognition has been studied in the context of several kinds of surgeries, such\nas cataract, neurological, and laparoscopic surgeries. In the literature, two\ntypes of features are typically used to perform this task: visual features and\ntool usage signals. However, the visual features used are mostly handcrafted.\nFurthermore, the tool usage signals are usually collected via a manual\nannotation process or by using additional equipment. In this paper, we propose\na novel method for phase recognition that uses a convolutional neural network\n(CNN) to automatically learn features from cholecystectomy videos and that\nrelies uniquely on visual information. In previous studies, it has been shown\nthat the tool signals can provide valuable information in performing the phase\nrecognition task. Thus, we present a novel CNN architecture, called EndoNet,\nthat is designed to carry out the phase recognition and tool presence detection\ntasks in a multi-task manner. To the best of our knowledge, this is the first\nwork proposing to use a CNN for multiple recognition tasks on laparoscopic\nvideos. Extensive experimental comparisons to other methods show that EndoNet\nyields state-of-the-art results for both tasks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 14:58:12 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 10:46:02 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Twinanda", "Andru P.", ""], ["Shehata", "Sherif", ""], ["Mutter", "Didier", ""], ["Marescaux", "Jacques", ""], ["de Mathelin", "Michel", ""], ["Padoy", "Nicolas", ""]]}, {"id": "1602.03145", "submitter": "Sylvie Lavigne", "authors": "Guillaume Noyel (CMM), Jesus Angulo (CMM), Dominique Jeulin (CMM)", "title": "A New Spatio-Spectral Morphological Segmentation For Multi-Spectral\n  Remote-Sensing Images", "comments": null, "journal-ref": "International Journal of Remote Sensing, Taylor \\& Francis, 2010,\n  31 (22), pp.5895-5920", "doi": "10.1080/01431161.2010.512314", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general framework of spatio-spectral segmentation for multi-spectral images\nis introduced in this paper. The method is based on classification-driven\nstochastic watershed (WS) by Monte Carlo simulations, and it gives more regular\nand reliable contours than standard WS. The present approach is decomposed into\nseveral sequential steps. First, a dimensionality-reduction stage is performed\nusing the factor-correspondence analysis method. In this context, a new way to\nselect the factor axes (eigenvectors) according to their spatial information is\nintroduced. Then, a spectral classification produces a spectral\npre-segmentation of the image. Subsequently, a probability density function\n(pdf) of contours containing spatial and spectral information is estimated by\nsimulation using a stochastic WS approach driven by the spectral\nclassification. The pdf of the contours is finally segmented by a WS controlled\nby markers from a regularization of the initial classification.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 20:02:00 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Noyel", "Guillaume", "", "CMM"], ["Angulo", "Jesus", "", "CMM"], ["Jeulin", "Dominique", "", "CMM"]]}, {"id": "1602.03205", "submitter": "Med Karim Abdmouleh", "authors": "Med Karim Abdmouleh, Ali Khalfallah and Med Salim Bouhlel", "title": "Image encryption with dynamic chaotic Look-Up Table", "comments": "7 pages, 12 figures, 6th International Conference on Sciences of\n  Electronics, Technologies of Information and Telecommunications (SETIT), 2012", "journal-ref": null, "doi": "10.1109/SETIT.2012.6481937", "report-no": null, "categories": "cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel image encryption scheme. The proposed method\nis based on the chaos theory. Our cryptosystem uses the chaos theory to define\na dynamic chaotic Look-Up Table (LUT) to compute the new value of the current\npixel to cipher. Applying this process on each pixel of the plain image, we\ngenerate the encrypted image. The results of different experimental tests, such\nas Key space analysis, Information Entropy and Histogram analysis, show that\nthe proposed encryption image scheme seems to be protected against various\nattacks. A comparison between the plain and encrypted image, in terms of\ncorrelation coefficient, proves that the plain image is very different from the\nencrypted one.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2016 21:54:51 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Abdmouleh", "Med Karim", ""], ["Khalfallah", "Ali", ""], ["Bouhlel", "Med Salim", ""]]}, {"id": "1602.03206", "submitter": "Filip Sala", "authors": "Filip A. Sala", "title": "Design of false color palettes for grayscale reproduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design of false color palette is quite easy but some effort has to be done to\nachieve good dynamic range, contrast and overall appearance of the palette.\nSuch palettes, for instance, are commonly used in scientific papers for\npresenting the data. However, to lower the cost of the paper most scientists\ndecide to let the data to be printed in grayscale. The same applies to e-book\nreaders based on e-ink where most of them are still grayscale. For majority of\nfalse color palettes reproducing them in grayscale results in ambiguous mapping\nof the colors and may be misleading for the reader. In this article design of\nfalse color palettes suitable for grayscale reproduction is described. Due to\nthe monotonic change of luminance of these palettes grayscale representation is\nvery similar to the data directly presented with a grayscale palette. Some\nsuggestions and examples how to design such palettes are provided.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 17:35:21 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 19:22:26 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Sala", "Filip A.", ""]]}, {"id": "1602.03256", "submitter": "Bappaditya Mandal", "authors": "Bappaditya Mandal", "title": "Improved Eigenfeature Regularization for Face Identification", "comments": "6 pages, 4 figures, ICIP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose to divide each class (a person) into subclasses\nusing spatial partition trees which helps in better capturing the\nintra-personal variances arising from the appearances of the same individual.\nWe perform a comprehensive analysis on within-class and within-subclass\neigenspectrums of face images and propose a novel method of eigenspectrum\nmodeling which extracts discriminative features of faces from both\nwithin-subclass and total or between-subclass scatter matrices. Effective\nlow-dimensional face discriminative features are extracted for face recognition\n(FR) after performing discriminant evaluation in the entire eigenspace.\nExperimental results on popular face databases (AR, FERET) and the challenging\nunconstrained YouTube Face database show the superiority of our proposed\napproach on all three databases.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 03:52:11 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Mandal", "Bappaditya", ""]]}, {"id": "1602.03308", "submitter": "David Barina", "authors": "David Barina", "title": "Gabor Wavelets in Image Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work shows the use of a two-dimensional Gabor wavelets in image\nprocessing. Convolution with such a two-dimensional wavelet can be separated\ninto two series of one-dimensional ones. The key idea of this work is to\nutilize a Gabor wavelet as a multiscale partial differential operator of a\ngiven order. Gabor wavelets are used here to detect edges, corners and blobs. A\nperformance of such an interest point detector is compared to detectors\nutilizing a Haar wavelet and a derivative of a Gaussian function. The proposed\napproach may be useful when a fast implementation of the Gabor transform is\navailable or when the transform is already precomputed.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 09:45:38 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Barina", "David", ""]]}, {"id": "1602.03346", "submitter": "Li Liu", "authors": "Li Liu and Yi Zhou and Ling Shao", "title": "DAP3D-Net: Where, What and How Actions Occur in Videos?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Action parsing in videos with complex scenes is an interesting but\nchallenging task in computer vision. In this paper, we propose a generic 3D\nconvolutional neural network in a multi-task learning manner for effective Deep\nAction Parsing (DAP3D-Net) in videos. Particularly, in the training phase,\naction localization, classification and attributes learning can be jointly\noptimized on our appearancemotion data via DAP3D-Net. For an upcoming test\nvideo, we can describe each individual action in the video simultaneously as:\nWhere the action occurs, What the action is and How the action is performed. To\nwell demonstrate the effectiveness of the proposed DAP3D-Net, we also\ncontribute a new Numerous-category Aligned Synthetic Action dataset, i.e.,\nNASA, which consists of 200; 000 action clips of more than 300 categories and\nwith 33 pre-defined action attributes in two hierarchical levels (i.e.,\nlow-level attributes of basic body part movements and high-level attributes\nrelated to action motion). We learn DAP3D-Net using the NASA dataset and then\nevaluate it on our collected Human Action Understanding (HAU) dataset.\nExperimental results show that our approach can accurately localize, categorize\nand describe multiple actions in realistic videos.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 12:25:52 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Liu", "Li", ""], ["Zhou", "Yi", ""], ["Shao", "Ling", ""]]}, {"id": "1602.03379", "submitter": "Anupam Mitra", "authors": "Anupam Mitra, Anagh Pathak, Kaushik Majumdar", "title": "Comparison of feature extraction and dimensionality reduction methods\n  for single channel extracellular spike sorting", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spikes in the membrane electrical potentials of neurons play a major role in\nthe functioning of nervous systems of animals. Obtaining the spikes from\ndifferent neurons has been a challenging problem for decades. Several schemes\nhave been proposed for spike sorting to isolate the spikes of individual\nneurons from electrical recordings in extracellular media. However, there is\nmuch scope for improvement in the accuracies obtained using the prevailing\nmethods of spike sorting. To determine more effective spike sorting strategies\nusing well known methods, we compared different types of signal features and\ntechniques for dimensionality reduction in feature space. We tried to determine\nan optimum or near optimum feature extraction and dimensionality reduction\nmethods and an optimum or near optimum number of features for spike sorting. We\nassessed relative performance of well known methods on simulated recordings\nspecially designed for development and benchmarking of spike sorting schemes,\nwith varying number of spike classes and the well established method of\n$k$-means clustering of selected features. We found that almost all well known\nmethods performed quite well. Nevertheless, from spike waveforms of 64 samples,\nsampled at 24 kHz, using principal component analysis (PCA) to select around 46\nto 55 features led to the better spike sorting performance than most other\nmethods (Wilcoxon signed rank sum test, $p < 0.001$).\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 14:21:34 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Mitra", "Anupam", ""], ["Pathak", "Anagh", ""], ["Majumdar", "Kaushik", ""]]}, {"id": "1602.03409", "submitter": "Hoo Chang Shin", "authors": "Hoo-Chang Shin, Holger R. Roth, Mingchen Gao, Le Lu, Ziyue Xu,\n  Isabella Nogues, Jianhua Yao, Daniel Mollura, Ronald M. Summers", "title": "Deep Convolutional Neural Networks for Computer-Aided Detection: CNN\n  Architectures, Dataset Characteristics and Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remarkable progress has been made in image recognition, primarily due to the\navailability of large-scale annotated datasets and the revival of deep CNN.\nCNNs enable learning data-driven, highly representative, layered hierarchical\nimage features from sufficient training data. However, obtaining datasets as\ncomprehensively annotated as ImageNet in the medical imaging domain remains a\nchallenge. There are currently three major techniques that successfully employ\nCNNs to medical image classification: training the CNN from scratch, using\noff-the-shelf pre-trained CNN features, and conducting unsupervised CNN\npre-training with supervised fine-tuning. Another effective method is transfer\nlearning, i.e., fine-tuning CNN models pre-trained from natural image dataset\nto medical image tasks. In this paper, we exploit three important, but\npreviously understudied factors of employing deep convolutional neural networks\nto computer-aided detection problems. We first explore and evaluate different\nCNN architectures. The studied models contain 5 thousand to 160 million\nparameters, and vary in numbers of layers. We then evaluate the influence of\ndataset scale and spatial image context on performance. Finally, we examine\nwhen and why transfer learning from pre-trained ImageNet (via fine-tuning) can\nbe useful. We study two specific computer-aided detection (CADe) problems,\nnamely thoraco-abdominal lymph node (LN) detection and interstitial lung\ndisease (ILD) classification. We achieve the state-of-the-art performance on\nthe mediastinal LN detection, with 85% sensitivity at 3 false positive per\npatient, and report the first five-fold cross-validation classification results\non predicting axial CT slices with ILD categories. Our extensive empirical\nevaluation, CNN model analysis and valuable insights can be extended to the\ndesign of high performance CAD systems for other medical imaging tasks.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 15:33:32 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Shin", "Hoo-Chang", ""], ["Roth", "Holger R.", ""], ["Gao", "Mingchen", ""], ["Lu", "Le", ""], ["Xu", "Ziyue", ""], ["Nogues", "Isabella", ""], ["Yao", "Jianhua", ""], ["Mollura", "Daniel", ""], ["Summers", "Ronald M.", ""]]}, {"id": "1602.03418", "submitter": "Swami Sankaranarayanan", "authors": "Swami Sankaranarayanan, Azadeh Alavi, Rama Chellappa", "title": "Triplet Similarity Embedding for Face Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an unconstrained face verification algorithm and\nevaluate it on the recently released IJB-A dataset that aims to push the\nboundaries of face verification methods. The proposed algorithm couples a deep\nCNN-based approach with a low-dimensional discriminative embedding learnt using\ntriplet similarity constraints in a large margin fashion. Aside from yielding\nperformance improvement, this embedding provides significant advantages in\nterms of memory and post-processing operations like hashing and visualization.\nExperiments on the IJB-A dataset show that the proposed algorithm outperforms\nstate of the art methods in verification and identification metrics, while\nrequiring less training time.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 15:48:47 GMT"}, {"version": "v2", "created": "Sun, 13 Mar 2016 18:06:34 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Sankaranarayanan", "Swami", ""], ["Alavi", "Azadeh", ""], ["Chellappa", "Rama", ""]]}, {"id": "1602.03458", "submitter": "Thomas K\\\"ohler", "authors": "Thomas K\\\"ohler, Axel Heinrich, Andreas Maier, Joachim Hornegger, Ralf\n  P. Tornow", "title": "Super-Resolved Retinal Image Mosaicing", "comments": "accepted for 2016 IEEE 13th International Symposium on Biomedical\n  Imaging (ISBI 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The acquisition of high-resolution retinal fundus images with a large field\nof view (FOV) is challenging due to technological, physiological and economic\nreasons. This paper proposes a fully automatic framework to reconstruct retinal\nimages of high spatial resolution and increased FOV from multiple\nlow-resolution images captured with non-mydriatic, mobile and video-capable but\nlow-cost cameras. Within the scope of one examination, we scan different\nregions on the retina by exploiting eye motion conducted by a patient guidance.\nAppropriate views for our mosaicing method are selected based on optic disk\ntracking to trace eye movements. For each view, one super-resolved image is\nreconstructed by fusion of multiple video frames. Finally, all super-resolved\nviews are registered to a common reference using a novel polynomial\nregistration scheme and combined by means of image mosaicing. We evaluated our\nframework for a mobile and low-cost video fundus camera. In our experiments, we\nreconstructed retinal images of up to 30{\\deg} FOV from 10 complementary views\nof 15{\\deg} FOV. An evaluation of the mosaics by human experts as well as a\nquantitative comparison to conventional color fundus images encourage the\nclinical usability of our framework.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 17:30:27 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["K\u00f6hler", "Thomas", ""], ["Heinrich", "Axel", ""], ["Maier", "Andreas", ""], ["Hornegger", "Joachim", ""], ["Tornow", "Ralf P.", ""]]}, {"id": "1602.03468", "submitter": "Abdolrahim Kadkhodamohammadi", "authors": "Abdolrahim Kadkhodamohammadi, Afshin Gangi, Michel de Mathelin and\n  Nicolas Padoy", "title": "Articulated Clinician Detection Using 3D Pictorial Structures on RGB-D\n  Data", "comments": "The supplementary video is available at https://youtu.be/iabbGSqRSgE", "journal-ref": null, "doi": "10.1016/j.media.2016.07.001", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliable human pose estimation (HPE) is essential to many clinical\napplications, such as surgical workflow analysis, radiation safety monitoring\nand human-robot cooperation. Proposed methods for the operating room (OR) rely\neither on foreground estimation using a multi-camera system, which is a\nchallenge in real ORs due to color similarities and frequent illumination\nchanges, or on wearable sensors or markers, which are invasive and therefore\ndifficult to introduce in the room. Instead, we propose a novel approach based\non Pictorial Structures (PS) and on RGB-D data, which can be easily deployed in\nreal ORs. We extend the PS framework in two ways. First, we build robust and\ndiscriminative part detectors using both color and depth images. We also\npresent a novel descriptor for depth images, called histogram of depth\ndifferences (HDD). Second, we extend PS to 3D by proposing 3D pairwise\nconstraints and a new method that makes exact inference tractable. Our approach\nis evaluated for pose estimation and clinician detection on a challenging RGB-D\ndataset recorded in a busy operating room during live surgeries. We conduct\nseries of experiments to study the different part detectors in conjunction with\nthe various 2D or 3D pairwise constraints. Our comparisons demonstrate that 3D\nPS with RGB-D part detectors significantly improves the results in a visually\nchallenging operating environment.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 17:56:47 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2016 17:57:18 GMT"}, {"version": "v3", "created": "Mon, 4 Jul 2016 08:56:24 GMT"}, {"version": "v4", "created": "Wed, 6 Jul 2016 07:45:15 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Kadkhodamohammadi", "Abdolrahim", ""], ["Gangi", "Afshin", ""], ["de Mathelin", "Michel", ""], ["Padoy", "Nicolas", ""]]}, {"id": "1602.03570", "submitter": "Azadeh Alavi Dr.", "authors": "Azadeh Alavi, Vishal M Patel, Rama Chellappa", "title": "Optimized Kernel-based Projection Space of Riemannian Manifolds", "comments": "14 pages, 6 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is proven that encoding images and videos through Symmetric Positive\nDefinite (SPD) matrices, and considering the Riemannian geometry of the\nresulting space, can lead to increased classification performance. Taking into\naccount manifold geometry is typically done via embedding the manifolds in\ntangent spaces, or Reproducing Kernel Hilbert Spaces (RKHS). Recently, it was\nshown that embedding such manifolds into a Random Projection Spaces (RPS),\nrather than RKHS or tangent space, leads to higher classification and\nclustering performance. However, based on structure and dimensionality of the\nrandomly generated hyperplanes, the classification performance over RPS may\nvary significantly. In addition, fine-tuning RPS is data expensive (as it\nrequires validation-data), time consuming, and resource demanding. In this\npaper, we introduce an approach to learn an optimized kernel-based projection\n(with fixed dimensionality), by employing the concept of subspace clustering.\nAs such, we encode the association of data points to the underlying subspace of\neach point, to generate meaningful hyperplanes. Further, we adopt the concept\nof dictionary learning and sparse coding, and discriminative analysis, for the\noptimized kernel-based projection space (OPS) on SPD manifolds. We validate our\nalgorithm on several classification tasks. The experiment results also\ndemonstrate that the proposed method outperforms state-of-the-art methods on\nsuch manifolds.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 23:14:17 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 14:25:13 GMT"}, {"version": "v3", "created": "Tue, 15 Mar 2016 05:24:22 GMT"}], "update_date": "2016-03-16", "authors_parsed": [["Alavi", "Azadeh", ""], ["Patel", "Vishal M", ""], ["Chellappa", "Rama", ""]]}, {"id": "1602.03585", "submitter": "Yangmuzi Zhang", "authors": "Yangmuzi Zhang, Zhuolin Jiang, Xi Chen, Larry S. Davis", "title": "Generating Discriminative Object Proposals via Submodular Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multi-scale greedy-based object proposal generation approach is presented.\nBased on the multi-scale nature of objects in images, our approach is built on\ntop of a hierarchical segmentation. We first identify the representative and\ndiverse exemplar clusters within each scale by using a diversity ranking\nalgorithm. Object proposals are obtained by selecting a subset from the\nmulti-scale segment pool via maximizing a submodular objective function, which\nconsists of a weighted coverage term, a single-scale diversity term and a\nmulti-scale reward term. The weighted coverage term forces the selected set of\nobject proposals to be representative and compact; the single-scale diversity\nterm encourages choosing segments from different exemplar clusters so that they\nwill cover as many object patterns as possible; the multi-scale reward term\nencourages the selected proposals to be discriminative and selected from\nmultiple layers generated by the hierarchical image segmentation. The\nexperimental results on the Berkeley Segmentation Dataset and PASCAL VOC2012\nsegmentation dataset demonstrate the accuracy and efficiency of our object\nproposal model. Additionally, we validate our object proposals in simultaneous\nsegmentation and detection and outperform the state-of-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 00:50:17 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Zhang", "Yangmuzi", ""], ["Jiang", "Zhuolin", ""], ["Chen", "Xi", ""], ["Davis", "Larry S.", ""]]}, {"id": "1602.03616", "submitter": "Anh Nguyen", "authors": "Anh Nguyen, Jason Yosinski, Jeff Clune", "title": "Multifaceted Feature Visualization: Uncovering the Different Types of\n  Features Learned By Each Neuron in Deep Neural Networks", "comments": "23 pages (including SI), 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We can better understand deep neural networks by identifying which features\neach of their neurons have learned to detect. To do so, researchers have\ncreated Deep Visualization techniques including activation maximization, which\nsynthetically generates inputs (e.g. images) that maximally activate each\nneuron. A limitation of current techniques is that they assume each neuron\ndetects only one type of feature, but we know that neurons can be multifaceted,\nin that they fire in response to many different types of features: for example,\na grocery store class neuron must activate either for rows of produce or for a\nstorefront. Previous activation maximization techniques constructed images\nwithout regard for the multiple different facets of a neuron, creating\ninappropriate mixes of colors, parts of objects, scales, orientations, etc.\nHere, we introduce an algorithm that explicitly uncovers the multiple facets of\neach neuron by producing a synthetic visualization of each of the types of\nimages that activate a neuron. We also introduce regularization methods that\nproduce state-of-the-art results in terms of the interpretability of images\nobtained by activation maximization. By separately synthesizing each type of\nimage a neuron fires in response to, the visualizations have more appropriate\ncolors and coherent global structure. Multifaceted feature visualization thus\nprovides a clearer and more comprehensive description of the role of each\nneuron.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 05:10:42 GMT"}, {"version": "v2", "created": "Sat, 7 May 2016 06:30:51 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Nguyen", "Anh", ""], ["Yosinski", "Jason", ""], ["Clune", "Jeff", ""]]}, {"id": "1602.03725", "submitter": "Helge Rhodin", "authors": "Helge Rhodin, Nadia Robertini, Christian Richardt, Hans-Peter Seidel,\n  Christian Theobalt", "title": "A Versatile Scene Model with Differentiable Visibility Applied to\n  Generative Pose Estimation", "comments": "9 pages, In proceedings of ICCV 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative reconstruction methods compute the 3D configuration (such as pose\nand/or geometry) of a shape by optimizing the overlap of the projected 3D shape\nmodel with images. Proper handling of occlusions is a big challenge, since the\nvisibility function that indicates if a surface point is seen from a camera can\noften not be formulated in closed form, and is in general discrete and\nnon-differentiable at occlusion boundaries. We present a new scene\nrepresentation that enables an analytically differentiable closed-form\nformulation of surface visibility. In contrast to previous methods, this yields\nsmooth, analytically differentiable, and efficient to optimize pose similarity\nenergies with rigorous occlusion handling, fewer local minima, and\nexperimentally verified improved convergence of numerical optimization. The\nunderlying idea is a new image formation model that represents opaque objects\nby a translucent medium with a smooth Gaussian density distribution which turns\nvisibility into a smooth phenomenon. We demonstrate the advantages of our\nversatile scene model in several generative pose estimation problems, namely\nmarker-less multi-object pose estimation, marker-less human motion capture with\nfew cameras, and image-based 3D geometry estimation.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 13:49:25 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Rhodin", "Helge", ""], ["Robertini", "Nadia", ""], ["Richardt", "Christian", ""], ["Seidel", "Hans-Peter", ""], ["Theobalt", "Christian", ""]]}, {"id": "1602.03742", "submitter": "Carlos Palma", "authors": "Carlos Palma, Augusto Salazar, Francisco Vargas", "title": "HMM and DTW for evaluation of therapeutical gestures using kinect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic recognition of the quality of movement in human beings is a\nchallenging task, given the difficulty both in defining the constraints that\nmake a movement correct, and the difficulty in using noisy data to determine if\nthese constraints were satisfied. This paper presents a method for the\ndetection of deviations from the correct form in movements from physical\ntherapy routines based on Hidden Markov Models, which is compared to Dynamic\nTime Warping. The activities studied include upper an lower limbs movements,\nthe data used comes from a Kinect sensor. Correct repetitions of the activities\nof interest were recorded, as well as deviations from these correct forms. The\nability of the proposed approach to detect these deviations was studied.\nResults show that a system based on HMM is much more likely to determine if a\ncertain movement has deviated from the specification.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 14:22:26 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Palma", "Carlos", ""], ["Salazar", "Augusto", ""], ["Vargas", "Francisco", ""]]}, {"id": "1602.03805", "submitter": "Kwang In Kim", "authors": "Kwang In Kim and James Tompkin and Hanspeter Pfister and Christian\n  Theobalt", "title": "Local High-order Regularization on Data Manifolds", "comments": "Accepted version of paper published at CVPR 2015,\n  http://dx.doi.org/10.1109/CVPR.2015.7299186", "journal-ref": null, "doi": "10.1109/CVPR.2015.7299186", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The common graph Laplacian regularizer is well-established in semi-supervised\nlearning and spectral dimensionality reduction. However, as a first-order\nregularizer, it can lead to degenerate functions in high-dimensional manifolds.\nThe iterated graph Laplacian enables high-order regularization, but it has a\nhigh computational complexity and so cannot be applied to large problems. We\nintroduce a new regularizer which is globally high order and so does not suffer\nfrom the degeneracy of the graph Laplacian regularizer, but is also sparse for\nefficient computation in semi-supervised learning applications. We reduce\ncomputational complexity by building a local first-order approximation of the\nmanifold as a surrogate geometry, and construct our high-order regularizer\nbased on local derivative evaluations therein. Experiments on human body shape\nand pose analysis demonstrate the effectiveness and efficiency of our method.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 18:04:36 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Kim", "Kwang In", ""], ["Tompkin", "James", ""], ["Pfister", "Hanspeter", ""], ["Theobalt", "Christian", ""]]}, {"id": "1602.03808", "submitter": "Kwang In Kim", "authors": "Kwang In Kim and James Tompkin and Hanspeter Pfister and Christian\n  Theobalt", "title": "Semi-supervised Learning with Explicit Relationship Regularization", "comments": "Accepted version of paper published at CVPR 2015,\n  http://dx.doi.org/10.1109/CVPR.2015.7298831", "journal-ref": null, "doi": "10.1109/CVPR.2015.7298831", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many learning tasks, the structure of the target space of a function holds\nrich information about the relationships between evaluations of functions on\ndifferent data points. Existing approaches attempt to exploit this relationship\ninformation implicitly by enforcing smoothness on function evaluations only.\nHowever, what happens if we explicitly regularize the relationships between\nfunction evaluations? Inspired by homophily, we regularize based on a smooth\nrelationship function, either defined from the data or with labels. In\nexperiments, we demonstrate that this significantly improves the performance of\nstate-of-the-art algorithms in semi-supervised classification and in spectral\ndata embedding for constrained clustering and dimensionality reduction.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 18:08:58 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Kim", "Kwang In", ""], ["Tompkin", "James", ""], ["Pfister", "Hanspeter", ""], ["Theobalt", "Christian", ""]]}, {"id": "1602.03860", "submitter": "Srinath Sridhar", "authors": "Srinath Sridhar, Helge Rhodin, Hans-Peter Seidel, Antti Oulasvirta,\n  Christian Theobalt", "title": "Real-Time Hand Tracking Using a Sum of Anisotropic Gaussians Model", "comments": "8 pages, Accepted version of paper published at 3DV 2014", "journal-ref": "2nd International Conference on , vol.1, no., pp.319-326, 8-11\n  Dec. 2014", "doi": "10.1109/3DV.2014.37", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time marker-less hand tracking is of increasing importance in\nhuman-computer interaction. Robust and accurate tracking of arbitrary hand\nmotion is a challenging problem due to the many degrees of freedom, frequent\nself-occlusions, fast motions, and uniform skin color. In this paper, we\npropose a new approach that tracks the full skeleton motion of the hand from\nmultiple RGB cameras in real-time. The main contributions include a new\ngenerative tracking method which employs an implicit hand shape representation\nbased on Sum of Anisotropic Gaussians (SAG), and a pose fitting energy that is\nsmooth and analytically differentiable making fast gradient based pose\noptimization possible. This shape representation, together with a full\nperspective projection model, enables more accurate hand modeling than a\nrelated baseline method from literature. Our method achieves better accuracy\nthan previous methods and runs at 25 fps. We show these improvements both\nqualitatively and quantitatively on publicly available datasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 20:03:53 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Sridhar", "Srinath", ""], ["Rhodin", "Helge", ""], ["Seidel", "Hans-Peter", ""], ["Oulasvirta", "Antti", ""], ["Theobalt", "Christian", ""]]}, {"id": "1602.03903", "submitter": "Marco Duarte", "authors": "Siwei Feng, Yuki Itoh, Mario Parente, and Marco F. Duarte", "title": "Wavelet-Based Semantic Features for Hyperspectral Signature\n  Discrimination", "comments": "21 pages, 8 figures, 4 tables, preprint, revised April 8 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral signature classification is a quantitative analysis approach\nfor hyperspectral imagery which performs detection and classification of the\nconstituent materials at the pixel level in the scene. The classification\nprocedure can be operated directly on hyperspectral data or performed by using\nsome features extracted from the corresponding hyperspectral signatures\ncontaining information like the signature's energy or shape. In this paper, we\ndescribe a technique that applies non-homogeneous hidden Markov chain (NHMC)\nmodels to hyperspectral signature classification. The basic idea is to use\nstatistical models (such as NHMC) to characterize wavelet coefficients which\ncapture the spectrum semantics (i.e., structural information) at multiple\nlevels. Experimental results show that the approach based on NHMC models can\noutperform existing approaches relevant in classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 21:25:36 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 19:50:27 GMT"}], "update_date": "2016-04-11", "authors_parsed": [["Feng", "Siwei", ""], ["Itoh", "Yuki", ""], ["Parente", "Mario", ""], ["Duarte", "Marco F.", ""]]}, {"id": "1602.03930", "submitter": "Vladimir Nekrasov", "authors": "Vladimir Nekrasov, Janghoon Ju, Jaesik Choi", "title": "Global Deconvolutional Networks for Semantic Segmentation", "comments": "BMVC 2016 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic image segmentation is a principal problem in computer vision, where\nthe aim is to correctly classify each individual pixel of an image into a\nsemantic label. Its widespread use in many areas, including medical imaging and\nautonomous driving, has fostered extensive research in recent years. Empirical\nimprovements in tackling this task have primarily been motivated by successful\nexploitation of Convolutional Neural Networks (CNNs) pre-trained for image\nclassification and object recognition. However, the pixel-wise labelling with\nCNNs has its own unique challenges: (1) an accurate deconvolution, or\nupsampling, of low-resolution output into a higher-resolution segmentation mask\nand (2) an inclusion of global information, or context, within locally\nextracted features. To address these issues, we propose a novel architecture to\nconduct the equivalent of the deconvolution operation globally and acquire\ndense predictions. We demonstrate that it leads to improved performance of\nstate-of-the-art semantic segmentation models on the PASCAL VOC 2012 benchmark,\nreaching 74.0% mean IU accuracy on the test set.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 00:03:38 GMT"}, {"version": "v2", "created": "Sat, 13 Aug 2016 08:54:42 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Nekrasov", "Vladimir", ""], ["Ju", "Janghoon", ""], ["Choi", "Jaesik", ""]]}, {"id": "1602.03935", "submitter": "Yang Zhong", "authors": "Yang Zhong, Josephine Sullivan, Haibo Li", "title": "Face Attribute Prediction Using Off-the-Shelf CNN Features", "comments": "In proceeding of 2016 International Conference on Biometrics (ICB)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting attributes from face images in the wild is a challenging computer\nvision problem. To automatically describe face attributes from face containing\nimages, traditionally one needs to cascade three technical blocks --- face\nlocalization, facial descriptor construction, and attribute classification ---\nin a pipeline. As a typical classification problem, face attribute prediction\nhas been addressed using deep learning. Current state-of-the-art performance\nwas achieved by using two cascaded Convolutional Neural Networks (CNNs), which\nwere specifically trained to learn face localization and attribute description.\nIn this paper, we experiment with an alternative way of employing the power of\ndeep representations from CNNs. Combining with conventional face localization\ntechniques, we use off-the-shelf architectures trained for face recognition to\nbuild facial descriptors. Recognizing that the describable face attributes are\ndiverse, our face descriptors are constructed from different levels of the CNNs\nfor different attributes to best facilitate face attribute prediction.\nExperiments on two large datasets, LFWA and CelebA, show that our approach is\nentirely comparable to the state-of-the-art. Our findings not only demonstrate\nan efficient face attribute prediction approach, but also raise an important\nquestion: how to leverage the power of off-the-shelf CNN representations for\nnovel tasks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 00:44:16 GMT"}, {"version": "v2", "created": "Tue, 21 Jun 2016 14:27:33 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Zhong", "Yang", ""], ["Sullivan", "Josephine", ""], ["Li", "Haibo", ""]]}, {"id": "1602.03995", "submitter": "Alexandre de Siqueira", "authors": "Alexandre Fioravante de Siqueira, Wagner Massayuki Nakasuga, Aylton\n  Pagamisse, Carlos Alberto Tello Saenz, Aldo Eloizo Job", "title": "An automatic method for segmentation of fission tracks in epidote\n  crystal photomicrographs", "comments": "16 pages, 5 figures", "journal-ref": "Computers & Geosciences, v. 69, pp. 55-61, aug 2014", "doi": "10.1016/j.cageo.2014.04.008", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manual identification of fission tracks has practical problems, such as\nvariation due to observer-observation efficiency. An automatic processing\nmethod that could identify fission tracks in a photomicrograph could solve this\nproblem and improve the speed of track counting. However, separation of\nnon-trivial images is one of the most difficult tasks in image processing.\nSeveral commercial and free softwares are available, but these softwares are\nmeant to be used in specific images. In this paper, an automatic method based\non starlet wavelets is presented in order to separate fission tracks in mineral\nphotomicrographs. Automatization is obtained by Matthews correlation\ncoefficient, and results are evaluated by precision, recall and accuracy. This\ntechnique is an improvement of a method aimed at segmentation of scanning\nelectron microscopy images. This method is applied in photomicrographs of\nepidote phenocrystals, in which accuracy higher than 89% was obtained in\nfission track segmentation, even for difficult images. Algorithms corresponding\nto the proposed method are available for download. Using the method presented\nhere, an user could easily determine fission tracks in photomicrographs of\nmineral samples.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 10:05:24 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["de Siqueira", "Alexandre Fioravante", ""], ["Nakasuga", "Wagner Massayuki", ""], ["Pagamisse", "Aylton", ""], ["Saenz", "Carlos Alberto Tello", ""], ["Job", "Aldo Eloizo", ""]]}, {"id": "1602.04052", "submitter": "Afonso Teodoro", "authors": "Afonso M. Teodoro and Jos\\'e M. Bioucas-Dias and M\\'ario A. T.\n  Figueiredo", "title": "Image Restoration and Reconstruction using Variable Splitting and\n  Class-adapted Image Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes using a Gaussian mixture model as a prior, for solving\ntwo image inverse problems, namely image deblurring and compressive imaging. We\ncapitalize on the fact that variable splitting algorithms, like ADMM, are able\nto decouple the handling of the observation operator from that of the\nregularizer, and plug a state-of-the-art algorithm into the pure denoising\nstep. Furthermore, we show that, when applied to a specific type of image, a\nGaussian mixture model trained from an database of images of the same type is\nable to outperform current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 13:37:49 GMT"}, {"version": "v2", "created": "Mon, 23 May 2016 13:04:39 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Teodoro", "Afonso M.", ""], ["Bioucas-Dias", "Jos\u00e9 M.", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1602.04105", "submitter": "Timothy O'Shea", "authors": "Timothy J O'Shea, Johnathan Corgan, T. Charles Clancy", "title": "Convolutional Radio Modulation Recognition Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the adaptation of convolutional neural networks to the complex\ntemporal radio signal domain. We compare the efficacy of radio modulation\nclassification using naively learned features against using expert features\nwhich are widely used in the field today and we show significant performance\nimprovements. We show that blind temporal learning on large and densely encoded\ntime series using deep convolutional neural networks is viable and a strong\ncandidate approach for this task especially at low signal to noise ratio.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 16:28:59 GMT"}, {"version": "v2", "created": "Sun, 24 Apr 2016 21:38:29 GMT"}, {"version": "v3", "created": "Fri, 10 Jun 2016 21:44:09 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["O'Shea", "Timothy J", ""], ["Corgan", "Johnathan", ""], ["Clancy", "T. Charles", ""]]}, {"id": "1602.04124", "submitter": "Srinath Sridhar", "authors": "Srinath Sridhar, Franziska Mueller, Antti Oulasvirta, Christian\n  Theobalt", "title": "Fast and Robust Hand Tracking Using Detection-Guided Optimization", "comments": "9 pages, Accepted version of paper published at CVPR 2015", "journal-ref": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE\n  Conference on , vol., no., pp.3213-3221, 7-12 June 2015", "doi": "10.1109/CVPR.2015.7298941", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markerless tracking of hands and fingers is a promising enabler for\nhuman-computer interaction. However, adoption has been limited because of\ntracking inaccuracies, incomplete coverage of motions, low framerate, complex\ncamera setups, and high computational requirements. In this paper, we present a\nfast method for accurately tracking rapid and complex articulations of the hand\nusing a single depth camera. Our algorithm uses a novel detection-guided\noptimization strategy that increases the robustness and speed of pose\nestimation. In the detection step, a randomized decision forest classifies\npixels into parts of the hand. In the optimization step, a novel objective\nfunction combines the detected part labels and a Gaussian mixture\nrepresentation of the depth to estimate a pose that best fits the depth. Our\napproach needs comparably less computational resources which makes it extremely\nfast (50 fps without GPU support). The approach also supports varying static,\nor moving, camera-to-scene arrangements. We show the benefits of our method by\nevaluating on public datasets and comparing against previous work.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 17:05:04 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Sridhar", "Srinath", ""], ["Mueller", "Franziska", ""], ["Oulasvirta", "Antti", ""], ["Theobalt", "Christian", ""]]}, {"id": "1602.04278", "submitter": "Taehwan Kim", "authors": "Taehwan Kim, Weiran Wang, Hao Tang, Karen Livescu", "title": "Signer-independent Fingerspelling Recognition with Deep Neural Network\n  Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recognition of fingerspelled letter sequences in\nAmerican Sign Language in a signer-independent setting. Fingerspelled sequences\nare both challenging and important to recognize, as they are used for many\ncontent words such as proper nouns and technical terms. Previous work has shown\nthat it is possible to achieve almost 90% accuracies on fingerspelling\nrecognition in a signer-dependent setting. However, the more realistic\nsigner-independent setting presents challenges due to significant variations\namong signers, coupled with the dearth of available training data. We\ninvestigate this problem with approaches inspired by automatic speech\nrecognition. We start with the best-performing approaches from prior work,\nbased on tandem models and segmental conditional random fields (SCRFs), with\nfeatures based on deep neural network (DNN) classifiers of letters and\nphonological features. Using DNN adaptation, we find that it is possible to\nbridge a large part of the gap between signer-dependent and signer-independent\nperformance. Using only about 115 transcribed words for adaptation from the\ntarget signer, we obtain letter accuracies of up to 82.7% with frame-level\nadaptation labels and 69.7% with only word labels.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 03:30:34 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Kim", "Taehwan", ""], ["Wang", "Weiran", ""], ["Tang", "Hao", ""], ["Livescu", "Karen", ""]]}, {"id": "1602.04330", "submitter": "Florian Kelma", "authors": "Thomas Hotz, Florian Kelma and John T. Kent", "title": "Manifolds of Projective Shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CV math.GT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The projective shape of a configuration of k points or \"landmarks\" in RP(d)\nconsists of the information that is invariant under projective transformations\nand hence is reconstructable from uncalibrated camera views. Mathematically,\nthe space of projective shapes for these k landmarks can be described as the\nquotient space of k copies of RP(d) modulo the action of the projective linear\ngroup PGL(d). Using homogeneous coordinates, such configurations can be\ndescribed as real k-times-(d+1)-dimensional matrices given up to\nleft-multiplication of non-singular diagonal matrices, while the group PGL(d)\nacts as GL(d+1) from the right. The main purpose of this paper is to give a\ndetailed examination of the topology of projective shape space, and, using\nmatrix notation, it is shown how to derive subsets that are in a certain sense\nmaximal, differentiable Hausdorff manifolds which can be provided with a\nRiemannian metric. A special subclass of the projective shapes consists of the\nTyler regular shapes, for which geometrically motivated pre-shapes can be\ndefined, thus allowing for the construction of a natural Riemannian metric.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 13:32:22 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 08:32:00 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 11:12:13 GMT"}, {"version": "v4", "created": "Mon, 5 Nov 2018 17:04:45 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Hotz", "Thomas", ""], ["Kelma", "Florian", ""], ["Kent", "John T.", ""]]}, {"id": "1602.04348", "submitter": "Shuye Zhang", "authors": "Shuye Zhang, Mude Lin, Tianshui Chen, Lianwen Jin, Liang Lin", "title": "Character Proposal Network for Robust Text Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximally stable extremal regions (MSER), which is a popular method to\ngenerate character proposals/candidates, has shown superior performance in\nscene text detection. However, the pixel-level operation limits its capability\nfor handling some challenging cases (e.g., multiple connected characters,\nseparated parts of one character and non-uniform illumination). To better\ntackle these cases, we design a character proposal network (CPN) by taking\nadvantage of the high capacity and fast computing of fully convolutional\nnetwork (FCN). Specifically, the network simultaneously predicts characterness\nscores and refines the corresponding locations. The characterness scores can be\nused for proposal ranking to reject non-character proposals and the refining\nprocess aims to obtain the more accurate locations. Furthermore, considering\nthe situation that different characters have different aspect ratios, we\npropose a multi-template strategy, designing a refiner for each aspect ratio.\nThe extensive experiments indicate our method achieves recall rates of 93.88%,\n93.60% and 96.46% on ICDAR 2013, SVT and Chinese2k datasets respectively using\nless than 1000 proposals, demonstrating promising performance of our character\nproposal network.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2016 15:55:17 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Zhang", "Shuye", ""], ["Lin", "Mude", ""], ["Chen", "Tianshui", ""], ["Jin", "Lianwen", ""], ["Lin", "Liang", ""]]}, {"id": "1602.04422", "submitter": "Chunhua Shen", "authors": "Peng Wang, Lingqiao Liu, Chunhua Shen, Anton van den Hengel, Heng Tao\n  Shen", "title": "Hi Detector, What's Wrong with that Object? Identifying Irregular Object\n  From Images by Modelling the Detection Score Distribution", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the challenging problem of identifying the irregular\nstatus of objects from images in an \"open world\" setting, that is,\ndistinguishing the irregular status of an object category from its regular\nstatus as well as objects from other categories in the absence of \"irregular\nobject\" training data. To address this problem, we propose a novel approach by\ninspecting the distribution of the detection scores at multiple image regions\nbased on the detector trained from the \"regular object\" and \"other objects\".\nThe key observation motivating our approach is that for \"regular object\" images\nas well as \"other objects\" images, the region-level scores follow their own\nessential patterns in terms of both the score values and the spatial\ndistributions while the detection scores obtained from an \"irregular object\"\nimage tend to break these patterns. To model this distribution, we propose to\nuse Gaussian Processes (GP) to construct two separate generative models for the\ncase of the \"regular object\" and the \"other objects\". More specifically, we\ndesign a new covariance function to simultaneously model the detection score at\na single region and the score dependencies at multiple regions. We finally\ndemonstrate the superior performance of our method on a large dataset newly\nproposed in this paper.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 06:39:05 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Wang", "Peng", ""], ["Liu", "Lingqiao", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""], ["Shen", "Heng Tao", ""]]}, {"id": "1602.04489", "submitter": "Aharon Bar Hillel", "authors": "Aharon Bar-Hillel and Eyal Krupka and Noam Bloom", "title": "Convolutional Tables Ensemble: classification in microseconds", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study classifiers operating under severe classification time constraints,\ncorresponding to 1-1000 CPU microseconds, using Convolutional Tables Ensemble\n(CTE), an inherently fast architecture for object category recognition. The\narchitecture is based on convolutionally-applied sparse feature extraction,\nusing trees or ferns, and a linear voting layer. Several structure and\noptimization variants are considered, including novel decision functions, tree\nlearning algorithm, and distillation from CNN to CTE architecture. Accuracy\nimprovements of 24-45% over related art of similar speed are demonstrated on\nstandard object recognition benchmarks. Using Pareto speed-accuracy curves, we\nshow that CTE can provide better accuracy than Convolutional Neural Networks\n(CNN) for a certain range of classification time constraints, or alternatively\nprovide similar error rates with 5-200X speedup.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 19:21:17 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Bar-Hillel", "Aharon", ""], ["Krupka", "Eyal", ""], ["Bloom", "Noam", ""]]}, {"id": "1602.04502", "submitter": "Bin Fan", "authors": "Bin Fan, Qingqun Kong, Wei Sui, Zhiheng Wang, Xinchao Wang, Shiming\n  Xiang, Chunhong Pan, Pascal Fua", "title": "Do We Need Binary Features for 3D Reconstruction?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary features have been incrementally popular in the past few years due to\ntheir low memory footprints and the efficient computation of Hamming distance\nbetween binary descriptors. They have been shown with promising results on some\nreal time applications, e.g., SLAM, where the matching operations are relative\nfew. However, in computer vision, there are many applications such as 3D\nreconstruction requiring lots of matching operations between local features.\nTherefore, a natural question is that is the binary feature still a promising\nsolution to this kind of applications? To get the answer, this paper conducts a\ncomparative study of binary features and their matching methods on the context\nof 3D reconstruction in a recently proposed large scale mutliview stereo\ndataset. Our evaluations reveal that not all binary features are capable of\nthis task. Most of them are inferior to the classical SIFT based method in\nterms of reconstruction accuracy and completeness with a not significant better\ncomputational performance.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 20:24:57 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Fan", "Bin", ""], ["Kong", "Qingqun", ""], ["Sui", "Wei", ""], ["Wang", "Zhiheng", ""], ["Wang", "Xinchao", ""], ["Xiang", "Shiming", ""], ["Pan", "Chunhong", ""], ["Fua", "Pascal", ""]]}, {"id": "1602.04504", "submitter": "Kimberly Wilber", "authors": "Michael J. Wilber, Vitaly Shmatikov, Serge Belongie", "title": "Can we still avoid automatic face detection?", "comments": "To appear at WACV 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After decades of study, automatic face detection and recognition systems are\nnow accurate and widespread. Naturally, this means users who wish to avoid\nautomatic recognition are becoming less able to do so. Where do we stand in\nthis cat-and-mouse race? We currently live in a society where everyone carries\na camera in their pocket. Many people willfully upload most or all of the\npictures they take to social networks which invest heavily in automatic face\nrecognition systems. In this setting, is it still possible for\nprivacy-conscientious users to avoid automatic face detection and recognition?\nIf so, how? Must evasion techniques be obvious to be effective, or are there\nstill simple measures that users can use to protect themselves?\n  In this work, we find ways to evade face detection on Facebook, a\nrepresentative example of a popular social network that uses automatic face\ndetection to enhance their service. We challenge widely-held beliefs about\nevading face detection: do our old techniques such as blurring the face region\nor wearing \"privacy glasses\" still work? We show that in general,\nstate-of-the-art detectors can often find faces even if the subject wears\noccluding clothing or even if the uploader damages the photo to prevent faces\nfrom being detected.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 20:40:00 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 13:17:57 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Wilber", "Michael J.", ""], ["Shmatikov", "Vitaly", ""], ["Belongie", "Serge", ""]]}, {"id": "1602.04506", "submitter": "Ranjay Krishna", "authors": "Ranjay Krishna, Kenji Hata, Stephanie Chen, Joshua Kravitz, David A.\n  Shamma, Li Fei-Fei, Michael S. Bernstein", "title": "Embracing Error to Enable Rapid Crowdsourcing", "comments": "10 pages, 7 figures, CHI '16, CHI: ACM Conference on Human Factors in\n  Computing Systems (2016)", "journal-ref": null, "doi": "10.1145/2858036.2858115", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microtask crowdsourcing has enabled dataset advances in social science and\nmachine learning, but existing crowdsourcing schemes are too expensive to scale\nup with the expanding volume of data. To scale and widen the applicability of\ncrowdsourcing, we present a technique that produces extremely rapid judgments\nfor binary and categorical labels. Rather than punishing all errors, which\ncauses workers to proceed slowly and deliberately, our technique speeds up\nworkers' judgments to the point where errors are acceptable and even expected.\nWe demonstrate that it is possible to rectify these errors by randomizing task\norder and modeling response latency. We evaluate our technique on a breadth of\ncommon labeling tasks such as image verification, word similarity, sentiment\nanalysis and topic classification. Where prior work typically achieves a 0.25x\nto 1x speedup over fixed majority vote, our approach often achieves an order of\nmagnitude (10x) speedup.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 20:56:01 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Krishna", "Ranjay", ""], ["Hata", "Kenji", ""], ["Chen", "Stephanie", ""], ["Kravitz", "Joshua", ""], ["Shamma", "David A.", ""], ["Fei-Fei", "Li", ""], ["Bernstein", "Michael S.", ""]]}, {"id": "1602.04513", "submitter": "Jose Garcia Miranda", "authors": "Ana Paula Quixad\\'a, Andrea Naomi Onodera, Norberto Pe\\~na, Jos\\'e\n  Garcia Vivas Miranda, Katia Nunes S\\'a", "title": "Validity and reliability of free software for bidimensional gait\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the evaluation systems of human movement that have been advancing in\nrecent decades, their use are not feasible for clinical practice because it has\na high cost and scarcity of trained operators to interpret their results. An\nideal videogrammetry system should be easy to use, low cost, with minimal\nequipment, and fast realization. The CvMob is a free tool for dynamic\nevaluation of human movements that express measurements in figures, tables, and\ngraphics. This paper aims to determine if CvMob is a reliable tool for the\nevaluation of two dimensional human gait. This is a validity and reliability\nstudy. The sample was composed of 56 healthy individuals who walked on a\n9-meterlong walkway and were simultaneously filmed by CvMob and Vicon system\ncameras. Linear trajectories and angular measurements were compared to validate\nthe CvMob system, and inter and intrarater findings of the same measurements\nwere used to determine reliability. A strong correlation (rs mean = 0.988) of\nthe linear trajectories between systems and inter and intrarater analysis were\nfound. According to the Bland-Altman method, the angles that had good agreement\nbetween systems were maximum flexion and extension (stance and swing) of the\nknee and dorsiflexion range of motion and stride length. The CvMob is a\nreliable tool for analysis of linear motion and lengths in two-dimensional\nevaluations of human gait. The angular measurements demonstrate high agreement\nfor the knee joint; however, the hip and ankle measurements were limited by\ndifferences between systems.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 21:18:02 GMT"}], "update_date": "2016-02-20", "authors_parsed": [["Quixad\u00e1", "Ana Paula", ""], ["Onodera", "Andrea Naomi", ""], ["Pe\u00f1a", "Norberto", ""], ["Miranda", "Jos\u00e9 Garcia Vivas", ""], ["S\u00e1", "Katia Nunes", ""]]}, {"id": "1602.04593", "submitter": "Alex James Dr", "authors": "Alex Pappachen James", "title": "Edge Detection for Pattern Recognition: A Survey", "comments": "Int. J. of Applied Pattern recognition, Vol 3, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This review provides an overview of the literature on the edge detection\nmethods for pattern recognition that inspire from the understanding of human\nvision. We note that edge detection is one of the most fundamental process\nwithin the low level vision and provides the basis for the higher level visual\nintelligence in primates. The recognition of the patterns within the images\nrelate closely to the spatiotemporal processes of edge formations, and its\nimplementation needs a crossdisciplanry approach in neuroscience, computing and\npattern recognition. In this review, the edge detectors are grouped in as edge\nfeatures, gradients and sketch models, and some example applications are\nprovided for reference. We note a significant increase in the amount of\npublished research in the last decade that utilizes edge features in a wide\nrange of problems in computer vision and image understanding having a direct\nimplication to pattern recognition with images.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 09:17:18 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["James", "Alex Pappachen", ""]]}, {"id": "1602.04868", "submitter": "Sayantan Sarkar", "authors": "Sayantan Sarkar, Vishal M. Patel, Rama Chellappa", "title": "Deep Feature-based Face Detection on Mobile Devices", "comments": "ISBA 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep feature-based face detector for mobile devices to detect\nuser's face acquired by the front facing camera. The proposed method is able to\ndetect faces in images containing extreme pose and illumination variations as\nwell as partial faces. The main challenge in developing deep feature-based\nalgorithms for mobile devices is the constrained nature of the mobile platform\nand the non-availability of CUDA enabled GPUs on such devices. Our\nimplementation takes into account the special nature of the images captured by\nthe front-facing camera of mobile devices and exploits the GPUs present in\nmobile devices without CUDA-based frameorks, to meet these challenges.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 00:14:22 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Sarkar", "Sayantan", ""], ["Patel", "Vishal M.", ""], ["Chellappa", "Rama", ""]]}, {"id": "1602.04886", "submitter": "Andrew Jaegle", "authors": "Andrew Jaegle, Stephen Phillips, Kostas Daniilidis", "title": "Fast, Robust, Continuous Monocular Egomotion Computation", "comments": "Accepted as a conference paper at ICRA 2016. Main paper: 8 pages, 7\n  figures. Supplement: 4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose robust methods for estimating camera egomotion in noisy,\nreal-world monocular image sequences in the general case of unknown observer\nrotation and translation with two views and a small baseline. This is a\ndifficult problem because of the nonconvex cost function of the perspective\ncamera motion equation and because of non-Gaussian noise arising from noisy\noptical flow estimates and scene non-rigidity. To address this problem, we\nintroduce the expected residual likelihood method (ERL), which estimates\nconfidence weights for noisy optical flow data using likelihood distributions\nof the residuals of the flow field under a range of counterfactual model\nparameters. We show that ERL is effective at identifying outliers and\nrecovering appropriate confidence weights in many settings. We compare ERL to a\nnovel formulation of the perspective camera motion equation using a lifted\nkernel, a recently proposed optimization framework for joint parameter and\nconfidence weight estimation with good empirical properties. We incorporate\nthese strategies into a motion estimation pipeline that avoids falling into\nlocal minima. We find that ERL outperforms the lifted kernel method and\nbaseline monocular egomotion estimation strategies on the challenging KITTI\ndataset, while adding almost no runtime cost over baseline egomotion methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 02:18:04 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Jaegle", "Andrew", ""], ["Phillips", "Stephen", ""], ["Daniilidis", "Kostas", ""]]}, {"id": "1602.04906", "submitter": "Junyan Wang", "authors": "Junyan Wang, Sai-kit Yeung, Jue Wang and Kun Zhou", "title": "Segmentation Rectification for Video Cutout via One-Class Structured\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works on interactive video object cutout mainly focus on designing\ndynamic foreground-background (FB) classifiers for segmentation propagation.\nHowever, the research on optimally removing errors from the FB classification\nis sparse, and the errors often accumulate rapidly, causing significant errors\nin the propagated frames. In this work, we take the initial steps to addressing\nthis problem, and we call this new task \\emph{segmentation rectification}. Our\nkey observation is that the possibly asymmetrically distributed false positive\nand false negative errors were handled equally in the conventional methods. We,\nalternatively, propose to optimally remove these two types of errors. To this\neffect, we propose a novel bilayer Markov Random Field (MRF) model for this new\ntask. We also adopt the well-established structured learning framework to learn\nthe optimal model from data. Additionally, we propose a novel one-class\nstructured SVM (OSSVM) which greatly speeds up the structured learning process.\nOur method naturally extends to RGB-D videos as well. Comprehensive experiments\non both RGB and RGB-D data demonstrate that our simple and effective method\nsignificantly outperforms the segmentation propagation methods adopted in the\nstate-of-the-art video cutout systems, and the results also suggest the\npotential usefulness of our method in image cutout system.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 04:31:20 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Wang", "Junyan", ""], ["Yeung", "Sai-kit", ""], ["Wang", "Jue", ""], ["Zhou", "Kun", ""]]}, {"id": "1602.04921", "submitter": "Weiyao Lin", "authors": "Weiyao Lin, Yang Mi, Weiyue Wang, Jianxin Wu, Jingdong Wang, Tao Mei", "title": "A diffusion and clustering-based approach for finding coherent motions\n  and understanding crowd scenes", "comments": "This manuscript is the accepted version for TIP (IEEE Transactions on\n  Image Processing), 2016", "journal-ref": null, "doi": "10.1109/TIP.2016.2531281", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of detecting coherent motions in crowd\nscenes and presents its two applications in crowd scene understanding: semantic\nregion detection and recurrent activity mining. It processes input motion\nfields (e.g., optical flow fields) and produces a coherent motion filed, named\nas thermal energy field. The thermal energy field is able to capture both\nmotion correlation among particles and the motion trends of individual\nparticles which are helpful to discover coherency among them. We further\nintroduce a two-step clustering process to construct stable semantic regions\nfrom the extracted time-varying coherent motions. These semantic regions can be\nused to recognize pre-defined activities in crowd scenes. Finally, we introduce\na cluster-and-merge process which automatically discovers recurrent activities\nin crowd scenes by clustering and merging the extracted coherent motions.\nExperiments on various videos demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 06:25:30 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Lin", "Weiyao", ""], ["Mi", "Yang", ""], ["Wang", "Weiyue", ""], ["Wu", "Jianxin", ""], ["Wang", "Jingdong", ""], ["Mei", "Tao", ""]]}, {"id": "1602.04981", "submitter": "Tuomas V\\\"alim\\\"aki", "authors": "Tuomas V\\\"alim\\\"aki and Risto Ritala", "title": "Optimizing Gaze Direction in a Visual Navigation Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigation in an unknown environment consists of multiple separable subtasks,\nsuch as collecting information about the surroundings and navigating to the\ncurrent goal. In the case of pure visual navigation, all these subtasks need to\nutilize the same vision system, and therefore a way to optimally control the\ndirection of focus is needed. We present a case study, where we model the\nactive sensing problem of directing the gaze of a mobile robot with three\nmachine vision cameras as a partially observable Markov decision process\n(POMDP) using a mutual information (MI) based reward function. The key aspect\nof the solution is that the cameras are dynamically used either in monocular or\nstereo configuration. The benefits of using the proposed active sensing\nimplementation are demonstrated with simulations and experiments on a real\nrobot.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 11:00:56 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["V\u00e4lim\u00e4ki", "Tuomas", ""], ["Ritala", "Risto", ""]]}, {"id": "1602.04983", "submitter": "Sreyasi Nag Chowdhury", "authors": "Sreyasi Nag Chowdhury, Mateusz Malinowski, Andreas Bulling, Mario\n  Fritz", "title": "Contextual Media Retrieval Using Natural Language Queries", "comments": "8 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread integration of cameras in hand-held and head-worn devices as\nwell as the ability to share content online enables a large and diverse visual\ncapture of the world that millions of users build up collectively every day. We\nenvision these images as well as associated meta information, such as GPS\ncoordinates and timestamps, to form a collective visual memory that can be\nqueried while automatically taking the ever-changing context of mobile users\ninto account. As a first step towards this vision, in this work we present\nXplore-M-Ego: a novel media retrieval system that allows users to query a\ndynamic database of images and videos using spatio-temporal natural language\nqueries. We evaluate our system using a new dataset of real user queries as\nwell as through a usability study. One key finding is that there is a\nconsiderable amount of inter-user variability, for example in the resolution of\nspatial relations in natural language utterances. We show that our retrieval\nsystem can cope with this variability using personalisation through an online\nlearning-based retrieval formulation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 11:04:29 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Chowdhury", "Sreyasi Nag", ""], ["Malinowski", "Mateusz", ""], ["Bulling", "Andreas", ""], ["Fritz", "Mario", ""]]}, {"id": "1602.04984", "submitter": "Hyo-Eun Kim", "authors": "Hyo-Eun Kim and Sangheum Hwang", "title": "Deconvolutional Feature Stacking for Weakly-Supervised Semantic\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A weakly-supervised semantic segmentation framework with a tied\ndeconvolutional neural network is presented. Each deconvolution layer in the\nframework consists of unpooling and deconvolution operations. 'Unpooling'\nupsamples the input feature map based on unpooling switches defined by\ncorresponding convolution layer's pooling operation. 'Deconvolution' convolves\nthe input unpooled features by using convolutional weights tied with the\ncorresponding convolution layer's convolution operation. The\nunpooling-deconvolution combination helps to eliminate less discriminative\nfeatures in a feature extraction stage, since output features of the\ndeconvolution layer are reconstructed from the most discriminative unpooled\nfeatures instead of the raw one. This results in reduction of false positives\nin a pixel-level inference stage. All the feature maps restored from the entire\ndeconvolution layers can constitute a rich discriminative feature set according\nto different abstraction levels. Those features are stacked to be selectively\nused for generating class-specific activation maps. Under the weak supervision\n(image-level labels), the proposed framework shows promising results on lesion\nsegmentation in medical images (chest X-rays) and achieves state-of-the-art\nperformance on the PASCAL VOC segmentation dataset in the same experimental\ncondition.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 11:05:24 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 07:46:24 GMT"}, {"version": "v3", "created": "Sat, 12 Mar 2016 08:22:30 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Kim", "Hyo-Eun", ""], ["Hwang", "Sangheum", ""]]}, {"id": "1602.05110", "submitter": "Daniel Jiwoong Im", "authors": "Daniel Jiwoong Im, Chris Dongjoo Kim, Hui Jiang, Roland Memisevic", "title": "Generating images with recurrent adversarial networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gatys et al. (2015) showed that optimizing pixels to match features in a\nconvolutional network with respect reference image features is a way to render\nimages of high visual quality. We show that unrolling this gradient-based\noptimization yields a recurrent computation that creates images by\nincrementally adding onto a visual \"canvas\". We propose a recurrent generative\nmodel inspired by this view, and show that it can be trained using adversarial\ntraining to generate very good image samples. We also propose a way to\nquantitatively compare adversarial networks by having the generators and\ndiscriminators of these networks compete against each other.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 17:51:39 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 14:41:52 GMT"}, {"version": "v3", "created": "Sun, 10 Apr 2016 19:17:27 GMT"}, {"version": "v4", "created": "Sun, 29 May 2016 01:17:59 GMT"}, {"version": "v5", "created": "Tue, 13 Dec 2016 03:21:03 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Im", "Daniel Jiwoong", ""], ["Kim", "Chris Dongjoo", ""], ["Jiang", "Hui", ""], ["Memisevic", "Roland", ""]]}, {"id": "1602.05168", "submitter": "Rashi Chaudhary", "authors": "Rashi Chaudhary, Himanshu Dasgupta", "title": "An Approach for Noise Removal on Depth Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image based rendering is a fundamental problem in computer vision and\ngraphics. Modern techniques often rely on depth image for the 3D construction.\nHowever for most of the existing depth cameras, the large and unpredictable\nnoises can be problematic, which can cause noticeable artifacts in the rendered\nresults. In this paper, we proposed an efficacious method for depth image noise\nremoval that can be applied for most RGBD systems. The proposed solution will\nbenefit many subsequent vision problems such as 3D reconstruction, novel view\nrendering, object recognition. Our experimental results demonstrate the\nefficacy and accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 20:28:16 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Chaudhary", "Rashi", ""], ["Dasgupta", "Himanshu", ""]]}, {"id": "1602.05256", "submitter": "Wichai Shanklin", "authors": "Wichai Shanklin", "title": "2D SEM images turn into 3D object models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scanning electron microscopy (SEM) is probably one the most fascinating\nexamination approach that has been used since more than two decades to detailed\ninspection of micro scale objects. Most of the scanning electron microscopes\ncould only produce 2D images that could not assist operational analysis of\nmicroscopic surface properties. Computer vision algorithms combined with very\nadvanced geometry and mathematical approaches turn any SEM into a full 3D\nmeasurement device. This work focuses on a methodical literature review for\nautomatic 3D surface reconstruction of scanning electron microscope images.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 00:41:58 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Shanklin", "Wichai", ""]]}, {"id": "1602.05312", "submitter": "Faisal Zaman", "authors": "Faisal Zaman, Ya Ping Wong, Boon Yian Ng", "title": "Density-based Denoising of Point Cloud", "comments": "9 pages, 5 figures, to be appeared in the Proceeding of 9th\n  International Conference on Robotics, Vision, Signal Processing & Power\n  Applications (ROVISP), 2-3 Feb 2016, Penang, Malaysia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Point cloud source data for surface reconstruction is usually contaminated\nwith noise and outliers. To overcome this deficiency, a density-based point\ncloud denoising method is presented to remove outliers and noisy points. First,\nparticle-swam optimization technique is employed for automatically\napproximating optimal bandwidth of multivariate kernel density estimation to\nensure the robust performance of density estimation. Then, mean-shift based\nclustering technique is used to remove outliers through a thresholding scheme.\nAfter removing outliers from the point cloud, bilateral mesh filtering is\napplied to smooth the remaining points. The experimental results show that this\napproach, comparably, is robust and efficient.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 06:13:41 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Zaman", "Faisal", ""], ["Wong", "Ya Ping", ""], ["Ng", "Boon Yian", ""]]}, {"id": "1602.05314", "submitter": "Tobias Weyand", "authors": "Tobias Weyand, Ilya Kostrikov, James Philbin", "title": "PlaNet - Photo Geolocation with Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-46484-8_3", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to build a system to determine the location where a photo was\ntaken using just its pixels? In general, the problem seems exceptionally\ndifficult: it is trivial to construct situations where no location can be\ninferred. Yet images often contain informative cues such as landmarks, weather\npatterns, vegetation, road markings, and architectural details, which in\ncombination may allow one to determine an approximate location and occasionally\nan exact location. Websites such as GeoGuessr and View from your Window suggest\nthat humans are relatively good at integrating these cues to geolocate images,\nespecially en-masse. In computer vision, the photo geolocation problem is\nusually approached using image retrieval methods. In contrast, we pose the\nproblem as one of classification by subdividing the surface of the earth into\nthousands of multi-scale geographic cells, and train a deep network using\nmillions of geotagged images. While previous approaches only recognize\nlandmarks or perform approximate matching using global image descriptors, our\nmodel is able to use and integrate multiple visible cues. We show that the\nresulting model, called PlaNet, outperforms previous approaches and even\nattains superhuman levels of accuracy in some cases. Moreover, we extend our\nmodel to photo albums by combining it with a long short-term memory (LSTM)\narchitecture. By learning to exploit temporal coherence to geolocate uncertain\nphotos, we demonstrate that this model achieves a 50% performance improvement\nover the single-image model.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 06:27:55 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Weyand", "Tobias", ""], ["Kostrikov", "Ilya", ""], ["Philbin", "James", ""]]}, {"id": "1602.05332", "submitter": "Bin Dong Dr.", "authors": "Bin Dong, Zuowei Shen, Peichu Xie", "title": "Image Restoration: A General Wavelet Frame Based Model and Its\n  Asymptotic Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image restoration is one of the most important areas in imaging science.\nMathematical tools have been widely used in image restoration, where wavelet\nframe based approach is one of the successful examples. In this paper, we\nintroduce a generic wavelet frame based image restoration model, called the\n\"general model\", which includes most of the existing wavelet frame based models\nas special cases. Moreover, the general model also includes examples that are\nnew to the literature. Motivated by our earlier studies [1-3], We provide an\nasymptotic analysis of the general model as image resolution goes to infinity,\nwhich establishes a connection between the general model in discrete setting\nand a new variatonal model in continuum setting. The variational model also\nincludes some of the existing variational models as special cases, such as the\ntotal generalized variational model proposed by [4]. In the end, we introduce\nan algorithm solving the general model and present one numerical simulation as\nan example.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 08:32:52 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Dong", "Bin", ""], ["Shen", "Zuowei", ""], ["Xie", "Peichu", ""]]}, {"id": "1602.05439", "submitter": "Arnaud Browet", "authors": "Arnaud Browet, Christophe De Vleeschouwer, Laurent Jacques, Navrita\n  Mathiah, Bechara Saykali, Isabelle Migeotte", "title": "Cell segmentation with random ferns and graph-cuts", "comments": "submitted to ICIP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The progress in imaging techniques have allowed the study of various aspect\nof cellular mechanisms. To isolate individual cells in live imaging data, we\nintroduce an elegant image segmentation framework that effectively extracts\ncell boundaries, even in the presence of poor edge details. Our approach works\nin two stages. First, we estimate pixel interior/border/exterior class\nprobabilities using random ferns. Then, we use an energy minimization framework\nto compute boundaries whose localization is compliant with the pixel class\nprobabilities. We validate our approach on a manually annotated dataset.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 14:47:32 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Browet", "Arnaud", ""], ["De Vleeschouwer", "Christophe", ""], ["Jacques", "Laurent", ""], ["Mathiah", "Navrita", ""], ["Saykali", "Bechara", ""], ["Migeotte", "Isabelle", ""]]}, {"id": "1602.05531", "submitter": "Paolo Napoletano", "authors": "Simone Bianco, Luigi Celona, Paolo Napoletano, Raimondo Schettini", "title": "On the Use of Deep Learning for Blind Image Quality Assessment", "comments": null, "journal-ref": "SIViP 12(2), 2018, 355-362", "doi": "10.1007/s11760-017-1166-8", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we investigate the use of deep learning for distortion-generic\nblind image quality assessment. We report on different design choices, ranging\nfrom the use of features extracted from pre-trained Convolutional Neural\nNetworks (CNNs) as a generic image description, to the use of features\nextracted from a CNN fine-tuned for the image quality task. Our best proposal,\nnamed DeepBIQ, estimates the image quality by average pooling the scores\npredicted on multiple sub-regions of the original image. The score of each\nsub-region is computed using a Support Vector Regression (SVR) machine taking\nas input features extracted using a CNN fine-tuned for category-based image\nquality assessment. Experimental results on the LIVE In the Wild Image Quality\nChallenge Database and on the LIVE Image Quality Assessment Database show that\nDeepBIQ outperforms the state-of-the-art methods compared, having a Linear\nCorrelation Coefficient (LCC) with human subjective scores of almost 0.91 and\n0.98 respectively. Furthermore, in most of the cases, the quality score\npredictions of DeepBIQ are closer to the average observer than those of a\ngeneric human observer.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 19:12:50 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 08:44:00 GMT"}, {"version": "v3", "created": "Thu, 26 May 2016 13:22:46 GMT"}, {"version": "v4", "created": "Wed, 11 Jan 2017 15:11:47 GMT"}, {"version": "v5", "created": "Tue, 4 Apr 2017 14:12:38 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Bianco", "Simone", ""], ["Celona", "Luigi", ""], ["Napoletano", "Paolo", ""], ["Schettini", "Raimondo", ""]]}, {"id": "1602.05572", "submitter": "Long  Lee", "authors": "S. Huzurbazar and Long Lee and Dongyang Kuang", "title": "A landmark-based algorithm for automatic pattern recognition and\n  abnormality detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of mathematical and statistical algorithms with the aim of\nestablishing a computer-based framework for fast and reliable automatic\nabnormality detection on landmark represented image templates. Under this\nframework, we apply a landmark-based algorithm for finding a group average as\nan estimator that is said to best represent the common features of the group in\nstudy. This algorithm extracts information of momentum at each landmark through\nthe process of template matching. If ever converges, the proposed algorithm\nproduces a local coordinate system for each member of the observing group, in\nterms of the residual momentum. We use a Bayesian approach on the collected\nresidual momentum representations for making inference. For illustration, we\napply this framework to a small database of brain images for detecting\nstructure abnormality. The brain structure changes identified by our framework\nare highly consistent with studies in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 02:18:27 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 15:30:32 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Huzurbazar", "S.", ""], ["Lee", "Long", ""], ["Kuang", "Dongyang", ""]]}, {"id": "1602.05659", "submitter": "Fuqiang Liu", "authors": "Fuqiang Liu, Fukun Bi, Yiding Yang, Liang Chen", "title": "Boost Picking: A Universal Method on Converting Supervised\n  Classification to Semi-supervised Classification", "comments": "This paper has been withdraw by the author due to format error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a universal method, Boost Picking, to train supervised\nclassification models mainly by un-labeled data. Boost Picking only adopts two\nweak classifiers to estimate and correct the error. It is theoretically proved\nthat Boost Picking could train a supervised model mainly by un-labeled data as\neffectively as the same model trained by 100% labeled data, only if recalls of\nthe two weak classifiers are all greater than zero and the sum of precisions is\ngreater than one. Based on Boost Picking, we present \"Test along with Training\n(TawT)\" to improve the generalization of supervised models. Both Boost Picking\nand TawT are successfully tested in varied little data sets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 02:24:54 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 13:16:23 GMT"}, {"version": "v3", "created": "Sat, 12 Nov 2016 09:25:54 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Liu", "Fuqiang", ""], ["Bi", "Fukun", ""], ["Yang", "Yiding", ""], ["Chen", "Liang", ""]]}, {"id": "1602.05660", "submitter": "Fuqiang Liu", "authors": "Fuqiang Liu, Fukun Bi, Liang Chen, Hao Shi and Wei Liu", "title": "Feature-Area Optimization: A Novel SAR Image Registration Method", "comments": "5 pages, 5 figures", "journal-ref": "IEEE Geoscience and Remote Sensing Letter, Year: 2016, Volume: 13,\n  Pages: 242 - 246", "doi": "10.1109/LGRS.2015.2507982", "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This letter proposes a synthetic aperture radar (SAR) image registration\nmethod named Feature-Area Optimization (FAO). First, the traditional area-based\noptimization model is reconstructed and decomposed into three key but uncertain\nfactors: initialization, slice set and regularization. Next, structural\nfeatures are extracted by scale invariant feature transform (SIFT) in\ndual-resolution space (SIFT-DRS), a novel SIFT-Like method dedicated to FAO.\nThen, the three key factors are determined based on these features. Finally,\nsolving the factor-determined optimization model can get the registration\nresult. A series of experiments demonstrate that the proposed method can\nregister multi-temporal SAR images accurately and efficiently.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 02:25:16 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Liu", "Fuqiang", ""], ["Bi", "Fukun", ""], ["Chen", "Liang", ""], ["Shi", "Hao", ""], ["Liu", "Wei", ""]]}, {"id": "1602.05920", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Manal H. Alassaf", "title": "Weighted Unsupervised Learning for 3D Object Detection", "comments": "IJACSA", "journal-ref": null, "doi": "10.14569/IJACSA.2016.070180", "report-no": null, "categories": "cs.CV cs.GR cs.LG cs.MM cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel weighted unsupervised learning for object\ndetection using an RGB-D camera. This technique is feasible for detecting the\nmoving objects in the noisy environments that are captured by an RGB-D camera.\nThe main contribution of this paper is a real-time algorithm for detecting each\nobject using weighted clustering as a separate cluster. In a preprocessing\nstep, the algorithm calculates the pose 3D position X, Y, Z and RGB color of\neach data point and then it calculates each data point's normal vector using\nthe point's neighbor. After preprocessing, our algorithm calculates k-weights\nfor each data point; each weight indicates membership. Resulting in clustered\nobjects of the scene.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 19:40:26 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 23:51:27 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Kowsari", "Kamran", ""], ["Alassaf", "Manal H.", ""]]}, {"id": "1602.05931", "submitter": "Joseph Paul Cohen", "authors": "Joseph Paul Cohen and Henry Z. Lo and Wei Ding", "title": "RandomOut: Using a convolutional gradient norm to rescue convolutional\n  filters", "comments": "Extended version of the ICLR 2016 workshop track paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filters in convolutional neural networks are sensitive to their\ninitialization. The random numbers used to initialize filters are a bias and\ndetermine if you will \"win\" and converge to a satisfactory local minimum so we\ncall this The Filter Lottery. We observe that the 28x28 Inception-V3 model\nwithout Batch Normalization fails to train 26% of the time when varying the\nrandom seed alone. This is a problem that affects the trial and error process\nof designing a network. Because random seeds have a large impact it makes it\nhard to evaluate a network design without trying many different random starting\nweights. This work aims to reduce the bias imposed by the initial weights so a\nnetwork converges more consistently. We propose to evaluate and replace\nspecific convolutional filters that have little impact on the prediction. We\nuse the gradient norm to evaluate the impact of a filter on error, and\nre-initialize filters when the gradient norm of its weights falls below a\nspecific threshold. This consistently improves accuracy on the 28x28\nInception-V3 with a median increase of +3.3%. In effect our method RandomOut\nincreases the number of filters explored without increasing the size of the\nnetwork. We observe that the RandomOut method has more consistent\ngeneralization performance, having a standard deviation of 1.3% instead of 2%\nwhen varying random seeds, and does so faster and with fewer parameters.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 20:05:53 GMT"}, {"version": "v2", "created": "Thu, 3 Mar 2016 01:31:55 GMT"}, {"version": "v3", "created": "Mon, 29 May 2017 04:49:22 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Cohen", "Joseph Paul", ""], ["Lo", "Henry Z.", ""], ["Ding", "Wei", ""]]}, {"id": "1602.05941", "submitter": "Hong Jiang", "authors": "Adriana Gonzalez, Hong Jiang, Gang Huang and Laurent Jacques", "title": "Multi-resolution Compressive Sensing Reconstruction", "comments": "5 pages; 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of reconstructing an image from compressive\nmeasurements using a multi-resolution grid. In this context, the reconstructed\nimage is divided into multiple regions, each one with a different resolution.\nThis problem arises in situations where the image to reconstruct contains a\ncertain region of interest (RoI) that is more important than the rest. Through\na theoretical analysis and simulation experiments we show that the\nmulti-resolution reconstruction provides a higher quality of the RoI compared\nto the traditional single-resolution approach.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 20:50:50 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Gonzalez", "Adriana", ""], ["Jiang", "Hong", ""], ["Huang", "Gang", ""], ["Jacques", "Laurent", ""]]}, {"id": "1602.05990", "submitter": "Pedro Miraldo", "authors": "Jo\\~ao R. Cardoso, Pedro Miraldo, and Helder Araujo", "title": "Pl\u007f\\\"ucker Correction Problem: Analysis and Improvements in Efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A given six dimensional vector represents a 3D straight line in Pl\u007fucker\ncoordinates if its coordinates satisfy the Klein quadric constraint. In many\nproblems aiming to find the Pl\u007fucker coordinates of lines, noise in the data\nand other type of errors contribute for obtaining 6D vectors that do not\ncorrespond to lines, because of that constraint. A common procedure to overcome\nthis drawback is to find the Pl\u007fucker coordinates of the lines that are closest\nto those vectors. This is known as the Pl\u007fucker correction problem. In this\narticle we propose a simple, closed-form, and global solution for this problem.\nWhen compared with the state-of-the-art method, one can conclude that our\nalgorithm is easier and requires much less operations than previous techniques\n(it does not require Singular Value Decomposition techniques).\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 22:22:18 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Cardoso", "Jo\u00e3o R.", ""], ["Miraldo", "Pedro", ""], ["Araujo", "Helder", ""]]}, {"id": "1602.06149", "submitter": "Simone Bianco", "authors": "Simone Bianco", "title": "Large age-gap face verification by feature injection in deep networks", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new method for face verification across large age\ngaps and also a dataset containing variations of age in the wild, the Large\nAge-Gap (LAG) dataset, with images ranging from child/young to adult/old. The\nproposed method exploits a deep convolutional neural network (DCNN) pre-trained\nfor the face recognition task on a large dataset and then fine-tuned for the\nlarge age-gap face verification task. Finetuning is performed in a Siamese\narchitecture using a contrastive loss function. A feature injection layer is\nintroduced to boost verification accuracy, showing the ability of the DCNN to\nlearn a similarity metric leveraging external features. Experimental results on\nthe LAG dataset show that our method is able to outperform the face\nverification solutions in the state of the art considered.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 13:39:22 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Bianco", "Simone", ""]]}, {"id": "1602.06157", "submitter": "Manuel W\\\"uthrich", "authors": "Jan Issac, Manuel W\\\"uthrich, Cristina Garcia Cifuentes, Jeannette\n  Bohg, Sebastian Trimpe and Stefan Schaal", "title": "Depth-Based Object Tracking Using a Robust Gaussian Filter", "comments": null, "journal-ref": null, "doi": "10.1109/ICRA.2016.7487184", "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of model-based 3D-tracking of objects given dense\ndepth images as input. Two difficulties preclude the application of a standard\nGaussian filter to this problem. First of all, depth sensors are characterized\nby fat-tailed measurement noise. To address this issue, we show how a recently\npublished robustification method for Gaussian filters can be applied to the\nproblem at hand. Thereby, we avoid using heuristic outlier detection methods\nthat simply reject measurements if they do not match the model. Secondly, the\ncomputational cost of the standard Gaussian filter is prohibitive due to the\nhigh-dimensional measurement, i.e. the depth image. To address this problem, we\npropose an approximation to reduce the computational complexity of the filter.\nIn quantitative experiments on real data we show how our method clearly\noutperforms the standard Gaussian filter. Furthermore, we compare its\nperformance to a particle-filter-based tracking method, and observe comparable\ncomputational efficiency and improved accuracy and smoothness of the estimates.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 14:09:37 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Issac", "Jan", ""], ["W\u00fcthrich", "Manuel", ""], ["Cifuentes", "Cristina Garcia", ""], ["Bohg", "Jeannette", ""], ["Trimpe", "Sebastian", ""], ["Schaal", "Stefan", ""]]}, {"id": "1602.06439", "submitter": "Kwang In Kim", "authors": "Kwang In Kim and James Tompkin and Hanspeter Pfister and Christian\n  Theobalt", "title": "Context-guided diffusion for label propagation on graphs", "comments": null, "journal-ref": null, "doi": "10.1109/ICCV.2015.318", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches for diffusion on graphs, e.g., for label propagation, are\nmainly focused on isotropic diffusion, which is induced by the commonly-used\ngraph Laplacian regularizer. Inspired by the success of diffusivity tensors for\nanisotropic diffusion in image processing, we presents anisotropic diffusion on\ngraphs and the corresponding label propagation algorithm. We develop positive\ndefinite diffusivity operators on the vector bundles of Riemannian manifolds,\nand discretize them to diffusivity operators on graphs. This enables us to\neasily define new robust diffusivity operators which significantly improve\nsemi-supervised learning performance over existing diffusion algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2016 19:02:01 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Kim", "Kwang In", ""], ["Tompkin", "James", ""], ["Pfister", "Hanspeter", ""], ["Theobalt", "Christian", ""]]}, {"id": "1602.06541", "submitter": "Martin Thoma", "authors": "Martin Thoma", "title": "A Survey of Semantic Segmentation", "comments": "Fixed typo in accuracy metrics formula; added value range of accuracy\n  metrics; consistent naming of variables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This survey gives an overview over different techniques used for pixel-level\nsemantic segmentation. Metrics and datasets for the evaluation of segmentation\nalgorithms and traditional approaches for segmentation such as unsupervised\nmethods, Decision Forests and SVMs are described and pointers to the relevant\npapers are given. Recently published approaches with convolutional neural\nnetworks are mentioned and typical problematic situations for segmentation\nalgorithms are examined. A taxonomy of segmentation algorithms is given.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 15:28:04 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 21:57:48 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Thoma", "Martin", ""]]}, {"id": "1602.06564", "submitter": "Jiangye Yuan", "authors": "Jiangye Yuan", "title": "Automatic Building Extraction in Aerial Scenes Using Convolutional\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic building extraction from aerial and satellite imagery is highly\nchallenging due to extremely large variations of building appearances. To\nattack this problem, we design a convolutional network with a final stage that\nintegrates activations from multiple preceding stages for pixel-wise\nprediction, and introduce the signed distance function of building boundaries\nas the output representation, which has an enhanced representation power. We\nleverage abundant building footprint data available from geographic information\nsystems (GIS) to compile training data. The trained network achieves superior\nperformance on datasets that are significantly larger and more complex than\nthose used in prior work, demonstrating that the proposed method provides a\npromising and scalable solution for automating this labor-intensive task.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2016 18:41:04 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Yuan", "Jiangye", ""]]}, {"id": "1602.06632", "submitter": "Tejal Bhamre", "authors": "Tejal Bhamre, Teng Zhang, Amit Singer", "title": "Denoising and Covariance Estimation of Single Particle Cryo-EM Images", "comments": "Revision for JSB", "journal-ref": null, "doi": "10.1016/j.jsb.2016.04.013", "report-no": null, "categories": "cs.CV q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of image restoration in cryo-EM entails correcting for the\neffects of the Contrast Transfer Function (CTF) and noise. Popular methods for\nimage restoration include `phase flipping', which corrects only for the Fourier\nphases but not amplitudes, and Wiener filtering, which requires the spectral\nsignal to noise ratio. We propose a new image restoration method which we call\n`Covariance Wiener Filtering' (CWF). In CWF, the covariance matrix of the\nprojection images is used within the classical Wiener filtering framework for\nsolving the image restoration deconvolution problem. Our estimation procedure\nfor the covariance matrix is new and successfully corrects for the CTF. We\ndemonstrate the efficacy of CWF by applying it to restore both simulated and\nexperimental cryo-EM images. Results with experimental datasets demonstrate\nthat CWF provides a good way to evaluate the particle images and to see what\nthe dataset contains even without 2D classification and averaging.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 03:04:44 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2016 04:03:55 GMT"}, {"version": "v3", "created": "Wed, 6 Apr 2016 19:41:52 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Bhamre", "Tejal", ""], ["Zhang", "Teng", ""], ["Singer", "Amit", ""]]}, {"id": "1602.06645", "submitter": "Song Liu", "authors": "Song Liu, Wanqing Li, Philip Ogunbona and Yang-Wai Chow", "title": "Creating Simplified 3D Models with High Quality Textures", "comments": "2015 International Conference on Digital Image Computing: Techniques\n  and Applications (DICTA), Page 1 - 8", "journal-ref": null, "doi": "10.1109/DICTA.2015.7371249", "report-no": null, "categories": "cs.GR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an extension to the KinectFusion algorithm which allows\ncreating simplified 3D models with high quality RGB textures. This is achieved\nthrough (i) creating model textures using images from an HD RGB camera that is\ncalibrated with Kinect depth camera, (ii) using a modified scheme to update\nmodel textures in an asymmetrical colour volume that contains a higher number\nof voxels than that of the geometry volume, (iii) simplifying dense polygon\nmesh model using quadric-based mesh decimation algorithm, and (iv) creating and\nmapping 2D textures to every polygon in the output 3D model. The proposed\nmethod is implemented in real-time by means of GPU parallel processing.\nVisualization via ray casting of both geometry and colour volumes provides\nusers with a real-time feedback of the currently scanned 3D model. Experimental\nresults show that the proposed method is capable of keeping the model texture\nquality even for a heavily decimated model and that, when reconstructing small\nobjects, photorealistic RGB textures can still be reconstructed.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 04:45:43 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Liu", "Song", ""], ["Li", "Wanqing", ""], ["Ogunbona", "Philip", ""], ["Chow", "Yang-Wai", ""]]}, {"id": "1602.06647", "submitter": "Song Liu", "authors": "Song Liu, Wanqing Li, Stephen Davis, Christian Ritz and Hongda Tian", "title": "Planogram Compliance Checking Based on Detection of Recurring Patterns", "comments": "Accepted by MM (IEEE Multimedia Magazine) 2016", "journal-ref": null, "doi": "10.1109/MMUL.2016.19", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel method for automatic planogram compliance checking in\nretail chains is proposed without requiring product template images for\ntraining. Product layout is extracted from an input image by means of\nunsupervised recurring pattern detection and matched via graph matching with\nthe expected product layout specified by a planogram to measure the level of\ncompliance. A divide and conquer strategy is employed to improve the speed.\nSpecifically, the input image is divided into several regions based on the\nplanogram. Recurring patterns are detected in each region respectively and then\nmerged together to estimate the product layout. Experimental results on real\ndata have verified the efficacy of the proposed method. Compared with a\ntemplate-based method, higher accuracies are achieved by the proposed method\nover a wide range of products.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 04:49:14 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Liu", "Song", ""], ["Li", "Wanqing", ""], ["Davis", "Stephen", ""], ["Ritz", "Christian", ""], ["Tian", "Hongda", ""]]}, {"id": "1602.06697", "submitter": "Mingsheng Long", "authors": "Yue Cao, Mingsheng Long, Jianmin Wang, Philip S. Yu", "title": "Correlation Hashing Network for Efficient Cross-Modal Retrieval", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing is widely applied to approximate nearest neighbor search for\nlarge-scale multimodal retrieval with storage and computation efficiency.\nCross-modal hashing improves the quality of hash coding by exploiting semantic\ncorrelations across different modalities. Existing cross-modal hashing methods\nfirst transform data into low-dimensional feature vectors, and then generate\nbinary codes by another separate quantization step. However, suboptimal hash\ncodes may be generated since the quantization error is not explicitly minimized\nand the feature representation is not jointly optimized with the binary codes.\nThis paper presents a Correlation Hashing Network (CHN) approach to cross-modal\nhashing, which jointly learns good data representation tailored to hash coding\nand formally controls the quantization error. The proposed CHN is a hybrid deep\narchitecture that constitutes a convolutional neural network for learning good\nimage representations, a multilayer perception for learning good text\nrepresentations, two hashing layers for generating compact binary codes, and a\nstructured max-margin loss that integrates all things together to enable\nlearning similarity-preserving and high-quality hash codes. Extensive empirical\nstudy shows that CHN yields state of the art cross-modal retrieval performance\non standard benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 09:31:45 GMT"}, {"version": "v2", "created": "Mon, 20 Feb 2017 11:25:05 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Cao", "Yue", ""], ["Long", "Mingsheng", ""], ["Wang", "Jianmin", ""], ["Yu", "Philip S.", ""]]}, {"id": "1602.06904", "submitter": "Amit Lal", "authors": "Amit Lal, Chunyan Shan, Peng Xi", "title": "Structured illumination microscopy image reconstruction algorithm", "comments": "OpenSIM code may be downloaded from:\n  https://github.com/LanMai/OpenSIM", "journal-ref": null, "doi": "10.1109/JSTQE.2016.2521542", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured illumination microscopy (SIM) is a very important super-resolution\nmicroscopy technique, which provides high speed super-resolution with about\ntwo-fold spatial resolution enhancement. Several attempts aimed at improving\nthe performance of SIM reconstruction algorithm have been reported. However,\nmost of these highlight only one specific aspect of the SIM reconstruction --\nsuch as the determination of the illumination pattern phase shift accurately --\nwhereas other key elements -- such as determination of modulation factor,\nestimation of object power spectrum, Wiener filtering frequency components with\ninclusion of object power spectrum information, translocating and the merging\nof the overlapping frequency components -- are usually glossed over\nsuperficially. In addition, most of the work reported lie scattered throughout\nthe literature and a comprehensive review of the theoretical background is\nfound lacking. The purpose of the present work is two-fold: 1) to collect the\nessential theoretical details of SIM algorithm at one place, thereby making\nthem readily accessible to readers for the first time; and 2) to provide an\nopen source SIM reconstruction code (named OpenSIM), which enables users to\ninteractively vary the code parameters and study it's effect on reconstructed\nSIM image.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 01:47:43 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Lal", "Amit", ""], ["Shan", "Chunyan", ""], ["Xi", "Peng", ""]]}, {"id": "1602.06920", "submitter": "R\\'emi Cura", "authors": "R\\'emi Cura and Julien Perret and Nicolas Paparoditis", "title": "Implicit LOD using points ordering for processing and visualisation in\n  Point Cloud Servers", "comments": "this article is a split of the previous one because the previous\n  article covered two topics to lousily related", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CV cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lidar datasets now commonly reach Billions of points and are very dense.\nUsing these point cloud becomes challenging, as the high number of points is\nintractable for most applications and for visualisation.In this work we propose\na new paradigm to easily get a portable geometric Level Of Details (LOD) inside\na Point Cloud Server.The main idea is to not store the LOD information in an\nexternal additional file, but instead to store it implicitly by exploiting the\norder of the points.The point cloud is divided into groups (patches). These\npatches are ordered so that their order gradually provides more and more\ndetails on the patch. We demonstrate the interest of our method with several\nclassical uses of LOD, such as visualisation of massive point cloud, algorithm\nacceleration, fast density peak detection and correction.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 20:19:23 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 10:54:47 GMT"}, {"version": "v3", "created": "Thu, 11 Jan 2018 21:38:21 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Cura", "R\u00e9mi", ""], ["Perret", "Julien", ""], ["Paparoditis", "Nicolas", ""]]}, {"id": "1602.07017", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, Yong Xu, Jian Yang, Xuelong Li, David Zhang", "title": "A survey of sparse representation: algorithms and applications", "comments": "Published on IEEE Access, Vol. 3, pp. 490-530, 2015", "journal-ref": null, "doi": "10.1109/ACCESS.2015.2430359", "report-no": null, "categories": "cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse representation has attracted much attention from researchers in fields\nof signal processing, image processing, computer vision and pattern\nrecognition. Sparse representation also has a good reputation in both\ntheoretical research and practical applications. Many different algorithms have\nbeen proposed for sparse representation. The main purpose of this article is to\nprovide a comprehensive study and an updated review on sparse representation\nand to supply a guidance for researchers. The taxonomy of sparse representation\nmethods can be studied from various viewpoints. For example, in terms of\ndifferent norm minimizations used in sparsity constraints, the methods can be\nroughly categorized into five groups: sparse representation with $l_0$-norm\nminimization, sparse representation with $l_p$-norm (0$<$p$<$1) minimization,\nsparse representation with $l_1$-norm minimization and sparse representation\nwith $l_{2,1}$-norm minimization. In this paper, a comprehensive overview of\nsparse representation is provided. The available sparse representation\nalgorithms can also be empirically categorized into four groups: greedy\nstrategy approximation, constrained optimization, proximity algorithm-based\noptimization, and homotopy algorithm-based sparse representation. The\nrationales of different algorithms in each category are analyzed and a wide\nrange of sparse representation applications are summarized, which could\nsufficiently reveal the potential nature of the sparse representation theory.\nSpecifically, an experimentally comparative study of these sparse\nrepresentation algorithms was presented. The Matlab code used in this paper can\nbe available at: http://www.yongxu.org/lunwen.html.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 02:44:53 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Zhang", "Zheng", ""], ["Xu", "Yong", ""], ["Yang", "Jian", ""], ["Li", "Xuelong", ""], ["Zhang", "David", ""]]}, {"id": "1602.07038", "submitter": "Barak Sober", "authors": "Barak Sober, David Levin", "title": "Computer Aided Restoration of Handwritten Character Strokes", "comments": "11 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work suggests a new variational approach to the task of computer aided\nrestoration of incomplete characters, residing in a highly noisy document. We\nmodel character strokes as the movement of a pen with a varying radius.\nFollowing this model, a cubic spline representation is being utilized to\nperform gradient descent steps, while maintaining interpolation at some initial\n(manually sampled) points. The proposed algorithm was utilized in the process\nof restoring approximately 1000 ancient Hebrew characters (dating to ca.\n8th-7th century BCE), some of which are presented herein and show that the\nalgorithm yields plausible results when applied on deteriorated documents.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 04:47:28 GMT"}, {"version": "v2", "created": "Wed, 6 Jul 2016 23:24:19 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Sober", "Barak", ""], ["Levin", "David", ""]]}, {"id": "1602.07119", "submitter": "Pascal Mettes", "authors": "Pascal Mettes, Dennis C. Koelma, Cees G. M. Snoek", "title": "The ImageNet Shuffle: Reorganized Pre-training for Video Event Detection", "comments": null, "journal-ref": null, "doi": "10.1145/2911996.2912036", "report-no": "ICMR/2016/06", "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper strives for video event detection using a representation learned\nfrom deep convolutional neural networks. Different from the leading approaches,\nwho all learn from the 1,000 classes defined in the ImageNet Large Scale Visual\nRecognition Challenge, we investigate how to leverage the complete ImageNet\nhierarchy for pre-training deep networks. To deal with the problems of\nover-specific classes and classes with few images, we introduce a bottom-up and\ntop-down approach for reorganization of the ImageNet hierarchy based on all its\n21,814 classes and more than 14 million images. Experiments on the TRECVID\nMultimedia Event Detection 2013 and 2015 datasets show that video\nrepresentations derived from the layers of a deep neural network pre-trained\nwith our reorganized hierarchy i) improves over standard pre-training, ii) is\ncomplementary among different reorganizations, iii) maintains the benefits of\nfusion with other modalities, and iv) leads to state-of-the-art event detection\nresults. The reorganized hierarchies and their derived Caffe models are\npublicly available at http://tinyurl.com/imagenetshuffle.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 11:12:55 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Mettes", "Pascal", ""], ["Koelma", "Dennis C.", ""], ["Snoek", "Cees G. M.", ""]]}, {"id": "1602.07125", "submitter": "Heikki Huttunen", "authors": "Heikki Huttunen and Fatemeh Shokrollahi Yancheshmeh and Ke Chen", "title": "Car Type Recognition with Deep Neural Networks", "comments": "To appear in proceedings of IEEE Intelligent Vehicles Symposium 2016", "journal-ref": "In proceedings of IEEE Intelligent Vehicles Symposium 2016", "doi": "10.1109/IVS.2016.7535529", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study automatic recognition of cars of four types: Bus,\nTruck, Van and Small car. For this problem we consider two data driven\nframeworks: a deep neural network and a support vector machine using SIFT\nfeatures. The accuracy of the methods is validated with a database of over 6500\nimages, and the resulting prediction accuracy is over 97 %. This clearly\nexceeds the accuracies of earlier studies that use manually engineered feature\nextraction pipelines.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 11:34:58 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 07:19:43 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Huttunen", "Heikki", ""], ["Yancheshmeh", "Fatemeh Shokrollahi", ""], ["Chen", "Ke", ""]]}, {"id": "1602.07188", "submitter": "Roman Novak", "authors": "Yaroslav Nikulin and Roman Novak", "title": "Exploring the Neural Algorithm of Artistic Style", "comments": "A short class project report (14 pages, 14 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the method of style transfer presented in the article \"A Neural\nAlgorithm of Artistic Style\" by Leon A. Gatys, Alexander S. Ecker and Matthias\nBethge (arXiv:1508.06576).\n  We first demonstrate the power of the suggested style space on a few\nexamples. We then vary different hyper-parameters and program properties that\nwere not discussed in the original paper, among which are the recognition\nnetwork used, starting point of the gradient descent and different ways to\npartition style and content layers. We also give a brief comparison of some of\nthe existing algorithm implementations and deep learning frameworks used.\n  To study the style space further we attempt to generate synthetic images by\nmaximizing a single entry in one of the Gram matrices $\\mathcal{G}_l$ and some\ninteresting results are observed. Next, we try to mimic the sparsity and\nintensity distribution of Gram matrices obtained from a real painting and\ngenerate more complex textures.\n  Finally, we propose two new style representations built on top of network's\nfeatures and discuss how one could be used to achieve local and potentially\ncontent-aware style transfer.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 15:17:55 GMT"}, {"version": "v2", "created": "Sun, 13 Mar 2016 21:13:57 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Nikulin", "Yaroslav", ""], ["Novak", "Roman", ""]]}, {"id": "1602.07261", "submitter": "Christian Szegedy", "authors": "Christian Szegedy, Sergey Ioffe and Vincent Vanhoucke and Alex Alemi", "title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very deep convolutional networks have been central to the largest advances in\nimage recognition performance in recent years. One example is the Inception\narchitecture that has been shown to achieve very good performance at relatively\nlow computational cost. Recently, the introduction of residual connections in\nconjunction with a more traditional architecture has yielded state-of-the-art\nperformance in the 2015 ILSVRC challenge; its performance was similar to the\nlatest generation Inception-v3 network. This raises the question of whether\nthere are any benefit in combining the Inception architecture with residual\nconnections. Here we give clear empirical evidence that training with residual\nconnections accelerates the training of Inception networks significantly. There\nis also some evidence of residual Inception networks outperforming similarly\nexpensive Inception networks without residual connections by a thin margin. We\nalso present several new streamlined architectures for both residual and\nnon-residual Inception networks. These variations improve the single-frame\nrecognition performance on the ILSVRC 2012 classification task significantly.\nWe further demonstrate how proper activation scaling stabilizes the training of\nvery wide residual Inception networks. With an ensemble of three residual and\none Inception-v4, we achieve 3.08 percent top-5 error on the test set of the\nImageNet classification (CLS) challenge\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 18:44:39 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 16:42:29 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Szegedy", "Christian", ""], ["Ioffe", "Sergey", ""], ["Vanhoucke", "Vincent", ""], ["Alemi", "Alex", ""]]}, {"id": "1602.07332", "submitter": "Ranjay Krishna", "authors": "Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata,\n  Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A.\n  Shamma, Michael S. Bernstein, Fei-Fei Li", "title": "Visual Genome: Connecting Language and Vision Using Crowdsourced Dense\n  Image Annotations", "comments": "44 pages, 37 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite progress in perceptual tasks such as image classification, computers\nstill perform poorly on cognitive tasks such as image description and question\nanswering. Cognition is core to tasks that involve not just recognizing, but\nreasoning about our visual world. However, models used to tackle the rich\ncontent in images for cognitive tasks are still being trained using the same\ndatasets designed for perceptual tasks. To achieve success at cognitive tasks,\nmodels need to understand the interactions and relationships between objects in\nan image. When asked \"What vehicle is the person riding?\", computers will need\nto identify the objects in an image as well as the relationships riding(man,\ncarriage) and pulling(horse, carriage) in order to answer correctly that \"the\nperson is riding a horse-drawn carriage\".\n  In this paper, we present the Visual Genome dataset to enable the modeling of\nsuch relationships. We collect dense annotations of objects, attributes, and\nrelationships within each image to learn these models. Specifically, our\ndataset contains over 100K images where each image has an average of 21\nobjects, 18 attributes, and 18 pairwise relationships between objects. We\ncanonicalize the objects, attributes, relationships, and noun phrases in region\ndescriptions and questions answer pairs to WordNet synsets. Together, these\nannotations represent the densest and largest dataset of image descriptions,\nobjects, attributes, relationships, and question answers.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 22:00:40 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Krishna", "Ranjay", ""], ["Zhu", "Yuke", ""], ["Groth", "Oliver", ""], ["Johnson", "Justin", ""], ["Hata", "Kenji", ""], ["Kravitz", "Joshua", ""], ["Chen", "Stephanie", ""], ["Kalantidis", "Yannis", ""], ["Li", "Li-Jia", ""], ["Shamma", "David A.", ""], ["Bernstein", "Michael S.", ""], ["Li", "Fei-Fei", ""]]}, {"id": "1602.07335", "submitter": "Minati Mishra Dr.", "authors": "Minati Mishra and M. C. Adhikary", "title": "Robust Detection of Intensity Variant Clones in Forged and JPEG\n  Compressed Images", "comments": "page 48-60", "journal-ref": "ANSVESA, 9(1), 48-60, 2014, ISSN-0974-715X", "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitization of images has made image editing easier. Ease of image editing\ntempted users and professionals to manipulate digital images leading to digital\nimage forgeries. Today digital image forgery has posed a great threat to the\nauthenticity of the popular digital media, the digital images. A lot of\nresearch is going on worldwide to detect image forgery and to separate the\nforged images from their authentic counterparts. This paper provides a novel\nintensity invariant detection model (IIDM) for detection of intensity variant\nclones that is robust against JPEG compression, noise attacks and blurring.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 13:26:30 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Mishra", "Minati", ""], ["Adhikary", "M. C.", ""]]}, {"id": "1602.07360", "submitter": "Forrest Iandola", "authors": "Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf,\n  William J. Dally, Kurt Keutzer", "title": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB\n  model size", "comments": "In ICLR Format", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on deep neural networks has focused primarily on improving\naccuracy. For a given accuracy level, it is typically possible to identify\nmultiple DNN architectures that achieve that accuracy level. With equivalent\naccuracy, smaller DNN architectures offer at least three advantages: (1)\nSmaller DNNs require less communication across servers during distributed\ntraining. (2) Smaller DNNs require less bandwidth to export a new model from\nthe cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on\nFPGAs and other hardware with limited memory. To provide all of these\nadvantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet\nachieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.\nAdditionally, with model compression techniques we are able to compress\nSqueezeNet to less than 0.5MB (510x smaller than AlexNet).\n  The SqueezeNet architecture is available for download here:\nhttps://github.com/DeepScale/SqueezeNet\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 00:09:45 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2016 20:24:20 GMT"}, {"version": "v3", "created": "Wed, 6 Apr 2016 07:21:49 GMT"}, {"version": "v4", "created": "Fri, 4 Nov 2016 21:26:08 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Iandola", "Forrest N.", ""], ["Han", "Song", ""], ["Moskewicz", "Matthew W.", ""], ["Ashraf", "Khalid", ""], ["Dally", "William J.", ""], ["Keutzer", "Kurt", ""]]}, {"id": "1602.07373", "submitter": "Song Wang", "authors": "Song Wang, Dongchun Ren, Li Chen, Wei Fan, Jun Sun, Satoshi Naoi", "title": "On Study of the Binarized Deep Neural Network for Image Classification", "comments": "9 pages, 6 figures. Rejected conference (CVPR 2015) submission.\n  Submission date: November, 2014. This work is patented in China (NO.\n  201410647710.3)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the deep neural network (derived from the artificial neural\nnetwork) has attracted many researchers' attention by its outstanding\nperformance. However, since this network requires high-performance GPUs and\nlarge storage, it is very hard to use it on individual devices. In order to\nimprove the deep neural network, many trials have been made by refining the\nnetwork structure or training strategy. Unlike those trials, in this paper, we\nfocused on the basic propagation function of the artificial neural network and\nproposed the binarized deep neural network. This network is a pure binary\nsystem, in which all the values and calculations are binarized. As a result,\nour network can save a lot of computational resource and storage. Therefore, it\nis possible to use it on various devices. Moreover, the experimental results\nproved the feasibility of the proposed network.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 02:39:47 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Wang", "Song", ""], ["Ren", "Dongchun", ""], ["Chen", "Li", ""], ["Fan", "Wei", ""], ["Sun", "Jun", ""], ["Naoi", "Satoshi", ""]]}, {"id": "1602.07377", "submitter": "Pooya Khorrami", "authors": "Pooya Khorrami, Tom Le Paine, Kevin Brady, Charlie Dagli, Thomas S.\n  Huang", "title": "How Deep Neural Networks Can Improve Emotion Recognition on Video Data", "comments": "Accepted at ICIP 2016. Fixed typo in Experiments section", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of dimensional emotion recognition on video data using\ndeep learning. While several previous methods have shown the benefits of\ntraining temporal neural network models such as recurrent neural networks\n(RNNs) on hand-crafted features, few works have considered combining\nconvolutional neural networks (CNNs) with RNNs. In this work, we present a\nsystem that performs emotion recognition on video data using both CNNs and\nRNNs, and we also analyze how much each neural network component contributes to\nthe system's overall performance. We present our findings on videos from the\nAudio/Visual+Emotion Challenge (AV+EC2015). In our experiments, we analyze the\neffects of several hyperparameters on overall performance while also achieving\nsuperior performance to the baseline and other competing methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 03:10:32 GMT"}, {"version": "v2", "created": "Thu, 19 May 2016 20:49:12 GMT"}, {"version": "v3", "created": "Tue, 16 Aug 2016 03:29:55 GMT"}, {"version": "v4", "created": "Fri, 9 Sep 2016 03:00:54 GMT"}, {"version": "v5", "created": "Tue, 10 Jan 2017 04:50:47 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Khorrami", "Pooya", ""], ["Paine", "Tom Le", ""], ["Brady", "Kevin", ""], ["Dagli", "Charlie", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1602.07383", "submitter": "Weiguang Ding", "authors": "Weiguang Ding, Graham Taylor", "title": "Automatic Moth Detection from Trap Images for Pest Management", "comments": "Preprints accepted by Computers and electronics in agriculture", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring the number of insect pests is a crucial component in\npheromone-based pest management systems. In this paper, we propose an automatic\ndetection pipeline based on deep learning for identifying and counting pests in\nimages taken inside field traps. Applied to a commercial codling moth dataset,\nour method shows promising performance both qualitatively and quantitatively.\nCompared to previous attempts at pest detection, our approach uses no\npest-specific engineering which enables it to adapt to other species and\nenvironments with minimal human effort. It is amenable to implementation on\nparallel hardware and therefore capable of deployment in settings where\nreal-time performance is required.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 03:35:42 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Ding", "Weiguang", ""], ["Taylor", "Graham", ""]]}, {"id": "1602.07416", "submitter": "Chongxuan Li", "authors": "Chongxuan Li, Jun Zhu and Bo Zhang", "title": "Learning to Generate with Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory units have been widely used to enrich the capabilities of deep\nnetworks on capturing long-term dependencies in reasoning and prediction tasks,\nbut little investigation exists on deep generative models (DGMs) which are good\nat inferring high-level invariant representations from unlabeled data. This\npaper presents a deep generative model with a possibly large external memory\nand an attention mechanism to capture the local detail information that is\noften lost in the bottom-up abstraction process in representation learning. By\nadopting a smooth attention model, the whole network is trained end-to-end by\noptimizing a variational bound of data likelihood via auto-encoding variational\nBayesian methods, where an asymmetric recognition network is learnt jointly to\ninfer high-level invariant representations. The asymmetric architecture can\nreduce the competition between bottom-up invariant feature extraction and\ntop-down generation of instance details. Our experiments on several datasets\ndemonstrate that memory can significantly boost the performance of DGMs and\neven achieve state-of-the-art results on various tasks, including density\nestimation, image generation, and missing value imputation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 06:57:14 GMT"}, {"version": "v2", "created": "Sat, 28 May 2016 03:41:27 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Li", "Chongxuan", ""], ["Zhu", "Jun", ""], ["Zhang", "Bo", ""]]}, {"id": "1602.07475", "submitter": "Lluis Gomez", "authors": "Lluis Gomez and Dimosthenis Karatzas", "title": "A fine-grained approach to scene text script identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the problem of script identification in unconstrained\nscenarios. Script identification is an important prerequisite to recognition,\nand an indispensable condition for automatic text understanding systems\ndesigned for multi-language environments. Although widely studied for document\nimages and handwritten documents, it remains an almost unexplored territory for\nscene text images.\n  We detail a novel method for script identification in natural images that\ncombines convolutional features and the Naive-Bayes Nearest Neighbor\nclassifier. The proposed framework efficiently exploits the discriminative\npower of small stroke-parts, in a fine-grained classification framework.\n  In addition, we propose a new public benchmark dataset for the evaluation of\njoint text detection and script identification in natural scenes. Experiments\ndone in this new dataset demonstrate that the proposed method yields state of\nthe art results, while it generalizes well to different datasets and variable\nnumber of scripts. The evidence provided shows that multi-lingual scene text\nrecognition in the wild is a viable proposition. Source code of the proposed\nmethod is made available online.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 12:12:07 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Gomez", "Lluis", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "1602.07480", "submitter": "Lluis Gomez", "authors": "Lluis Gomez, Anguelos Nicolaou, Dimosthenis Karatzas", "title": "Improving patch-based scene text script identification with ensembles of\n  conjoined networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the problem of script identification in scene text\nimages. Facing this problem with state of the art CNN classifiers is not\nstraightforward, as they fail to address a key characteristic of scene text\ninstances: their extremely variable aspect ratio. Instead of resizing input\nimages to a fixed aspect ratio as in the typical use of holistic CNN\nclassifiers, we propose here a patch-based classification framework in order to\npreserve discriminative parts of the image that are characteristic of its\nclass. We describe a novel method based on the use of ensembles of conjoined\nnetworks to jointly learn discriminative stroke-parts representations and their\nrelative importance in a patch-based classification scheme. Our experiments\nwith this learning procedure demonstrate state-of-the-art results in two public\nscript identification datasets. In addition, we propose a new public benchmark\ndataset for the evaluation of multi-lingual scene text end-to-end reading\nsystems. Experiments done in this dataset demonstrate the key role of script\nidentification in a complete end-to-end system that combines our script\nidentification method with a previously published text detector and an\noff-the-shelf OCR engine.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 12:33:25 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 13:17:57 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Gomez", "Lluis", ""], ["Nicolaou", "Anguelos", ""], ["Karatzas", "Dimosthenis", ""]]}, {"id": "1602.07535", "submitter": "Alireza Ghasemi", "authors": "Alireza Ghasemi and Adam Scholefield and Martin Vetterli", "title": "SHAPE: Linear-Time Camera Pose Estimation With Quadratic Error-Decay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel camera pose estimation or perspective-n-point (PnP)\nalgorithm, based on the idea of consistency regions and half-space\nintersections. Our algorithm has linear time-complexity and a squared\nreconstruction error that decreases at least quadratically, as the number of\nfeature point correspondences increase.\n  Inspired by ideas from triangulation and frame quantisation theory, we define\nconsistent reconstruction and then present SHAPE, our proposed consistent pose\nestimation algorithm. We compare this algorithm with state-of-the-art pose\nestimation techniques in terms of accuracy and error decay rate. The\nexperimental results verify our hypothesis on the optimal worst-case quadratic\ndecay and demonstrate its promising performance compared to other approaches.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 14:53:29 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Ghasemi", "Alireza", ""], ["Scholefield", "Adam", ""], ["Vetterli", "Martin", ""]]}, {"id": "1602.07542", "submitter": "Alireza Ghasemi", "authors": "Alireza Ghasemi and Adam Scholefield and Martin Vetterli", "title": "On the Accuracy of Point Localisation in a Circular Camera-Array", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many advances have been made in light-field and camera-array image\nprocessing, there is still a lack of thorough analysis of the localisation\naccuracy of different multi-camera systems. By considering the problem from a\nframe-quantisation perspective, we are able to quantify the point localisation\nerror of circular camera configurations. Specifically, we obtain closed form\nexpressions bounding the localisation error in terms of the parameters\ndescribing the acquisition setup.\n  These theoretical results are independent of the localisation algorithm and\nthus provide fundamental limits on performance. Furthermore, the new\nframe-quantisation perspective is general enough to be extended to more complex\ncamera configurations.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 15:02:53 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Ghasemi", "Alireza", ""], ["Scholefield", "Adam", ""], ["Vetterli", "Martin", ""]]}, {"id": "1602.07573", "submitter": "Jun Chen", "authors": "Fuhao Chen, Jun Chen, Feng Huang", "title": "A straightforward method to assess motion blur for different types of\n  displays", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simulation method based on the liquid crystal response and the human visual\nsystem is suitable to characterize motion blur for LCDs but not other display\ntypes. We propose a more straightforward and widely applicable method to\nquantify motion blur based on the width of the moving object. We thus compare\nvarious types of displays objectively. A perceptual experiment was conducted to\nvalidate the proposed method. We test varying motion velocities for nine\ncommercial displays. We compare the three motion blur evaluation methods\n(simulation, human perception, and our method) using z-scores. Our comparisons\nindicate that our method accurately characterizes motion blur for various\ndisplay types.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2015 11:08:11 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Chen", "Fuhao", ""], ["Chen", "Jun", ""], ["Huang", "Feng", ""]]}, {"id": "1602.07613", "submitter": "Alireza Aghasi", "authors": "Alireza Aghasi and Justin Romberg", "title": "Learning Shapes by Convex Composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a mathematical and algorithmic scheme for learning the principal\ngeometric elements in an image or 3D object. We build on recent work that\nconvexifies the basic problem of finding a combination of a small number shapes\nthat overlap and occlude one another in such a way that they \"match\" a given\nscene as closely as possible. This paper derives general sufficient conditions\nunder which this convex shape composition identifies a target composition. From\na computational standpoint, we present two different methods for solving the\nassociated optimization programs. The first method simply recasts the problem\nas a linear program, while the second uses the alternating direction method of\nmultipliers with a series of easily computed proximal operators.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2016 08:01:58 GMT"}, {"version": "v2", "created": "Mon, 4 Jul 2016 08:37:00 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Aghasi", "Alireza", ""], ["Romberg", "Justin", ""]]}, {"id": "1602.07620", "submitter": "Ashutosh Mishra", "authors": "Ashutosh Mishra, Sudipta Mahapatra, Swapna Banerjee", "title": "A Low Complexity VLSI Architecture for Multi-Focus Image Fusion in DCT\n  Domain", "comments": "Submitting to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the confined focal length of optical sensors, focusing all objects in\na scene with a single sensor is a difficult task. To handle such a situation,\nimage fusion methods are used in multi-focus environment. Discrete Cosine\nTransform (DCT) is a widely used image compression transform, image fusion in\nDCT domain is an efficient method. This paper presents a low complexity\napproach for multi-focus image fusion and its VLSI implementation using DCT.\nThe proposed method is evaluated using reference/non-reference fusion measure\ncriteria and the obtained results asserts it's effectiveness. The maximum\nsynthesized frequency on FPGA is found to be 221 MHz and consumes 42% of FPGA\nresources. The proposed method consumes very less power and can process 4K\nresolution images at the rate of 60 frames per second which makes the hardware\nsuitable for handheld portable devices such as camera module and wireless image\nsensors.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 13:25:30 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Mishra", "Ashutosh", ""], ["Mahapatra", "Sudipta", ""], ["Banerjee", "Swapna", ""]]}, {"id": "1602.07679", "submitter": "Ingmar Steiner", "authors": "Alexander Hewer (DFKI, MMCI), Ingmar Steiner (DFKI, MMCI), Timo\n  Bolkart (MMCI), Stefanie Wuhrer (MORPHEO), Korin Richmond (CSTR)", "title": "A statistical shape space model of the palate surface trained on 3D MRI\n  scans of the vocal tract", "comments": "Proceedings of the 18th International Congress of Phonetic Sciences,\n  Aug 2015, Glasgow, United Kingdom. 2015, http://www.icphs2015.info/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a minimally-supervised method for computing a statistical shape\nspace model of the palate surface. The model is created from a corpus of\nvolumetric magnetic resonance imaging (MRI) scans collected from 12 speakers.\nWe extract a 3D mesh of the palate from each speaker, then train the model\nusing principal component analysis (PCA). The palate model is then tested using\n3D MRI from another corpus and evaluated using a high-resolution optical scan.\nWe find that the error is low even when only a handful of measured coordinates\nare available. In both cases, our approach yields promising results. It can be\napplied to extract the palate shape from MRI data, and could be useful to other\nanalysis modalities, such as electromagnetic articulography (EMA) and\nultrasound tongue imaging (UTI).\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 07:24:53 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Hewer", "Alexander", "", "DFKI, MMCI"], ["Steiner", "Ingmar", "", "DFKI, MMCI"], ["Bolkart", "Timo", "", "MMCI"], ["Wuhrer", "Stefanie", "", "MORPHEO"], ["Richmond", "Korin", "", "CSTR"]]}, {"id": "1602.07873", "submitter": "Pavel Svoboda", "authors": "Pavel Svoboda, Michal Hradis, Lukas Marsik, Pavel Zemcik", "title": "CNN for License Plate Motion Deblurring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore the previously proposed approach of direct blind\ndeconvolution and denoising with convolutional neural networks in a situation\nwhere the blur kernels are partially constrained. We focus on blurred images\nfrom a real-life traffic surveillance system, on which we, for the first time,\ndemonstrate that neural networks trained on artificial data provide superior\nreconstruction quality on real images compared to traditional blind\ndeconvolution methods. The training data is easy to obtain by blurring sharp\nphotos from a target system with a very rough approximation of the expected\nblur kernels, thereby allowing custom CNNs to be trained for a specific\napplication (image content and blur range). Additionally, we evaluate the\nbehavior and limits of the CNNs with respect to blur direction range and\nlength.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 10:33:04 GMT"}], "update_date": "2016-02-26", "authors_parsed": [["Svoboda", "Pavel", ""], ["Hradis", "Michal", ""], ["Marsik", "Lukas", ""], ["Zemcik", "Pavel", ""]]}, {"id": "1602.08127", "submitter": "Xiping Fu", "authors": "Xiping Fu, Brendan McCane, Steven Mills, Michael Albert and Lech\n  Szymanski", "title": "Auto-JacoBin: Auto-encoder Jacobian Binary Hashing", "comments": "Submitting to journal (TPAMI). 17 pages, 11 figures. The Matlab codes\n  for AutoJacoBin and NOKMeans are available:\n  https://bitbucket.org/fxpfxp/autojacobin\n  https://bitbucket.org/fxpfxp/nokmeans The SIFT10M dataset is available at:\n  http://archive.ics.uci.edu/ml/datasets/SIFT10M", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Binary codes can be used to speed up nearest neighbor search tasks in large\nscale data sets as they are efficient for both storage and retrieval. In this\npaper, we propose a robust auto-encoder model that preserves the geometric\nrelationships of high-dimensional data sets in Hamming space. This is done by\nconsidering a noise-removing function in a region surrounding the manifold\nwhere the training data points lie. This function is defined with the property\nthat it projects the data points near the manifold into the manifold wisely,\nand we approximate this function by its first order approximation. Experimental\nresults show that the proposed method achieves better than state-of-the-art\nresults on three large scale high dimensional data sets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 21:47:16 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 06:22:28 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Fu", "Xiping", ""], ["McCane", "Brendan", ""], ["Mills", "Steven", ""], ["Albert", "Michael", ""], ["Szymanski", "Lech", ""]]}, {"id": "1602.08132", "submitter": "Zhenhao Ge", "authors": "Zhenhao Ge, Sudhendu R. Sharma, Mark J. T. Smith", "title": "Adaptive Frequency Cepstral Coefficients for Word Mispronunciation\n  Detection", "comments": "4th International Congress on Image and Signal Processing (CISP) 2011", "journal-ref": null, "doi": "10.1109/CISP.2011.6100685", "report-no": null, "categories": "cs.SD cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems based on automatic speech recognition (ASR) technology can provide\nimportant functionality in computer assisted language learning applications.\nThis is a young but growing area of research motivated by the large number of\nstudents studying foreign languages. Here we propose a Hidden Markov Model\n(HMM)-based method to detect mispronunciations. Exploiting the specific dialog\nscripting employed in language learning software, HMMs are trained for\ndifferent pronunciations. New adaptive features have been developed and\nobtained through an adaptive warping of the frequency scale prior to computing\nthe cepstral coefficients. The optimization criterion used for the warping\nfunction is to maximize separation of two major groups of pronunciations\n(native and non-native) in terms of classification rate. Experimental results\nshow that the adaptive frequency scale yields a better coefficient\nrepresentation leading to higher classification rates in comparison with\nconventional HMMs using Mel-frequency cepstral coefficients.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 22:17:31 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Ge", "Zhenhao", ""], ["Sharma", "Sudhendu R.", ""], ["Smith", "Mark J. T.", ""]]}, {"id": "1602.08141", "submitter": "Thomas Castelli", "authors": "Thomas Castelli, Aidean Sharghi, Don Harper, Alain Tremeau and Mubarak\n  Shah", "title": "Autonomous navigation for low-altitude UAVs in urban areas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, consumer Unmanned Aerial Vehicles have become very popular,\neveryone can buy and fly a drone without previous experience, which raises\nconcern in regards to regulations and public safety. In this paper, we present\na novel approach towards enabling safe operation of such vehicles in urban\nareas. Our method uses geodetically accurate dataset images with Geographical\nInformation System (GIS) data of road networks and buildings provided by Google\nMaps, to compute a weighted A* shortest path from start to end locations of a\nmission. Weights represent the potential risk of injuries for individuals in\nall categories of land-use, i.e. flying over buildings is considered safer than\nabove roads. We enable safe UAV operation in regards to 1- land-use by\ncomputing a static global path dependent on environmental structures, and 2-\navoiding flying over moving objects such as cars and pedestrians by dynamically\noptimizing the path locally during the flight. As all input sources are first\ngeo-registered, pixels and GPS coordinates are equivalent, it therefore allows\nus to generate an automated and user-friendly mission with GPS waypoints\nreadable by consumer drones' autopilots. We simulated 54 missions and show\nsignificant improvement in maximizing UAV's standoff distance to moving objects\nwith a quantified safety parameter over 40 times better than the naive straight\nline navigation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2016 22:43:14 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Castelli", "Thomas", ""], ["Sharghi", "Aidean", ""], ["Harper", "Don", ""], ["Tremeau", "Alain", ""], ["Shah", "Mubarak", ""]]}, {"id": "1602.08225", "submitter": "Wei Liu", "authors": "Wei Liu, Wei-Long Zheng, Bao-Liang Lu", "title": "Multimodal Emotion Recognition Using Multimodal Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To enhance the performance of affective models and reduce the cost of\nacquiring physiological signals for real-world applications, we adopt\nmultimodal deep learning approach to construct affective models from multiple\nphysiological signals. For unimodal enhancement task, we indicate that the best\nrecognition accuracy of 82.11% on SEED dataset is achieved with shared\nrepresentations generated by Deep AutoEncoder (DAE) model. For multimodal\nfacilitation tasks, we demonstrate that the Bimodal Deep AutoEncoder (BDAE)\nachieves the mean accuracies of 91.01% and 83.25% on SEED and DEAP datasets,\nrespectively, which are much superior to the state-of-the-art approaches. For\ncross-modal learning task, our experimental results demonstrate that the mean\naccuracy of 66.34% is achieved on SEED dataset through shared representations\ngenerated by EEG-based DAE as training samples and shared representations\ngenerated by eye-based DAE as testing sample, and vice versa.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 07:43:14 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Liu", "Wei", ""], ["Zheng", "Wei-Long", ""], ["Lu", "Bao-Liang", ""]]}, {"id": "1602.08325", "submitter": "Ahmad Hassanat", "authors": "Ahmad B. A. Hassanat, Mahmoud B. Alhasanat, Mohammad Ali Abbadi, Eman\n  Btoush, Mouhammd Al-Awadi, Ahmad S. Tarawneh", "title": "Victory Sign Biometric for Terrorists Identification", "comments": "7 pages, 5 figures, 4 tables, 26 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Covering the face and all body parts, sometimes the only evidence to identify\na person is their hand geometry, and not the whole hand- only two fingers (the\nindex and the middle fingers) while showing the victory sign, as seen in many\nterrorists videos. This paper investigates for the first time a new way to\nidentify persons, particularly (terrorists) from their victory sign. We have\ncreated a new database in this regard using a mobile phone camera, imaging the\nvictory signs of 50 different persons over two sessions. Simple measurements\nfor the fingers, in addition to the Hu Moments for the areas of the fingers\nwere used to extract the geometric features of the shown part of the hand shown\nafter segmentation. The experimental results using the KNN classifier were\nencouraging for most of the recorded persons; with about 40% to 93% total\nidentification accuracy, depending on the features, distance metric and K used.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 13:57:40 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Hassanat", "Ahmad B. A.", ""], ["Alhasanat", "Mahmoud B.", ""], ["Abbadi", "Mohammad Ali", ""], ["Btoush", "Eman", ""], ["Al-Awadi", "Mouhammd", ""], ["Tarawneh", "Ahmad S.", ""]]}, {"id": "1602.08405", "submitter": "Frank Keller", "authors": "Dim P. Papadopoulos, Jasper R. R. Uijlings, Frank Keller and Vittorio\n  Ferrari", "title": "We don't need no bounding-boxes: Training object class detectors using\n  only human verification", "comments": "CVPR 2016, pp. 854-863. Las Vegas, NV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training object class detectors typically requires a large set of images in\nwhich objects are annotated by bounding-boxes. However, manually drawing\nbounding-boxes is very time consuming. We propose a new scheme for training\nobject detectors which only requires annotators to verify bounding-boxes\nproduced automatically by the learning algorithm. Our scheme iterates between\nre-training the detector, re-localizing objects in the training images, and\nhuman verification. We use the verification signal both to improve re-training\nand to reduce the search space for re-localisation, which makes these steps\ndifferent to what is normally done in a weakly supervised setting. Extensive\nexperiments on PASCAL VOC 2007 show that (1) using human verification to update\ndetectors and reduce the search space leads to the rapid production of\nhigh-quality bounding-box annotations; (2) our scheme delivers detectors\nperforming almost as good as those trained in a fully supervised setting,\nwithout ever drawing any bounding-box; (3) as the verification task is very\nquick, our scheme substantially reduces total annotation time by a factor\n6x-9x.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 17:13:52 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 02:48:18 GMT"}, {"version": "v3", "created": "Mon, 24 Apr 2017 12:14:53 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Papadopoulos", "Dim P.", ""], ["Uijlings", "Jasper R. R.", ""], ["Keller", "Frank", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1602.08425", "submitter": "Florian Bernard", "authors": "Florian Bernard, Luis Salamanca, Johan Thunberg, Alexander Tack,\n  Dennis Jentsch, Hans Lamecker, Stefan Zachow, Frank Hertel, Jorge Goncalves,\n  Peter Gemmar", "title": "Shape-aware Surface Reconstruction from Sparse 3D Point-Clouds", "comments": null, "journal-ref": null, "doi": "10.1016/j.media.2017.02.005", "report-no": null, "categories": "cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reconstruction of an object's shape or surface from a set of 3D points\nplays an important role in medical image analysis, e.g. in anatomy\nreconstruction from tomographic measurements or in the process of aligning\nintra-operative navigation and preoperative planning data. In such scenarios,\none usually has to deal with sparse data, which significantly aggravates the\nproblem of reconstruction. However, medical applications often provide\ncontextual information about the 3D point data that allow to incorporate prior\nknowledge about the shape that is to be reconstructed. To this end, we propose\nthe use of a statistical shape model (SSM) as a prior for surface\nreconstruction. The SSM is represented by a point distribution model (PDM),\nwhich is associated with a surface mesh. Using the shape distribution that is\nmodelled by the PDM, we formulate the problem of surface reconstruction from a\nprobabilistic perspective based on a Gaussian Mixture Model (GMM). In order to\ndo so, the given points are interpreted as samples of the GMM. By using mixture\ncomponents with anisotropic covariances that are \"oriented\" according to the\nsurface normals at the PDM points, a surface-based fitting is accomplished.\nEstimating the parameters of the GMM in a maximum a posteriori manner yields\nthe reconstruction of the surface from the given data points. We compare our\nmethod to the extensively used Iterative Closest Points method on several\ndifferent anatomical datasets/SSMs (brain, femur, tibia, hip, liver) and\ndemonstrate superior accuracy and robustness on sparse data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 18:30:07 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 11:45:36 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Bernard", "Florian", ""], ["Salamanca", "Luis", ""], ["Thunberg", "Johan", ""], ["Tack", "Alexander", ""], ["Jentsch", "Dennis", ""], ["Lamecker", "Hans", ""], ["Zachow", "Stefan", ""], ["Hertel", "Frank", ""], ["Goncalves", "Jorge", ""], ["Gemmar", "Peter", ""]]}, {"id": "1602.08465", "submitter": "Pooya Khorrami", "authors": "Wei Han, Pooya Khorrami, Tom Le Paine, Prajit Ramachandran, Mohammad\n  Babaeizadeh, Honghui Shi, Jianan Li, Shuicheng Yan, Thomas S. Huang", "title": "Seq-NMS for Video Object Detection", "comments": "Technical Report for Imagenet VID Competition 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video object detection is challenging because objects that are easily\ndetected in one frame may be difficult to detect in another frame within the\nsame clip. Recently, there have been major advances for doing object detection\nin a single image. These methods typically contain three phases: (i) object\nproposal generation (ii) object classification and (iii) post-processing. We\npropose a modification of the post-processing phase that uses high-scoring\nobject detections from nearby frames to boost scores of weaker detections\nwithin the same clip. We show that our method obtains superior results to\nstate-of-the-art single image object detection techniques. Our method placed\n3rd in the video object detection (VID) task of the ImageNet Large Scale Visual\nRecognition Challenge 2015 (ILSVRC2015).\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 20:10:27 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 05:15:45 GMT"}, {"version": "v3", "created": "Mon, 22 Aug 2016 23:16:49 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Han", "Wei", ""], ["Khorrami", "Pooya", ""], ["Paine", "Tom Le", ""], ["Ramachandran", "Prajit", ""], ["Babaeizadeh", "Mohammad", ""], ["Shi", "Honghui", ""], ["Li", "Jianan", ""], ["Yan", "Shuicheng", ""], ["Huang", "Thomas S.", ""]]}, {"id": "1602.08486", "submitter": "Garrison Cottrell", "authors": "Honghao Shan, Matthew H. Tong, Garrison W. Cottrell", "title": "A Single Model Explains both Visual and Auditory Precortical Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precortical neural systems encode information collected by the senses, but\nthe driving principles of the encoding used have remained a subject of debate.\nWe present a model of retinal coding that is based on three constraints:\ninformation preservation, minimization of the neural wiring, and response\nequalization. The resulting novel version of sparse principal components\nanalysis successfully captures a number of known characteristics of the retinal\ncoding system, such as center-surround receptive fields, color opponency\nchannels, and spatiotemporal responses that correspond to magnocellular and\nparvocellular pathways. Furthermore, when trained on auditory data, the same\nmodel learns receptive fields well fit by gammatone filters, commonly used to\nmodel precortical auditory coding. This suggests that efficient coding may be a\nunifying principle of precortical encoding across modalities.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 10:17:53 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 19:19:11 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Shan", "Honghao", ""], ["Tong", "Matthew H.", ""], ["Cottrell", "Garrison W.", ""]]}, {"id": "1602.08510", "submitter": "Gregory Vaksman", "authors": "Gregory Vaksman, Michael Zibulevsky, and Michael Elad", "title": "Patch-Ordering as a Regularization for Inverse Problems in Image\n  Processing", "comments": null, "journal-ref": null, "doi": "10.1137/15M1038074", "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in image processing suggests that operating on (overlapping)\npatches in an image may lead to state-of-the-art results. This has been\ndemonstrated for a variety of problems including denoising, inpainting,\ndeblurring, and super-resolution. The work reported in [1,2] takes an extra\nstep forward by showing that ordering these patches to form an approximate\nshortest path can be leveraged for better processing. The core idea is to apply\na simple filter on the resulting 1D smoothed signal obtained after the\npatch-permutation. This idea has been also explored in combination with a\nwavelet pyramid, leading eventually to a sophisticated and highly effective\nregularizer for inverse problems in imaging. In this work we further study the\npatch-permutation concept, and harness it to propose a new simple yet effective\nregularization for image restoration problems. Our approach builds on the\nclassic Maximum A'posteriori probability (MAP), with a penalty function\nconsisting of a regular log-likelihood term and a novel permutation-based\nregularization term. Using a plain 1D Laplacian, the proposed regularization\nforces robust smoothness (L1) on the permuted pixels. Since the permutation\noriginates from patch-ordering, we propose to accumulate the smoothness terms\nover all the patches' pixels. Furthermore, we take into account the found\ndistances between adjacent patches in the ordering, by weighting the Laplacian\noutcome. We demonstrate the proposed scheme on a diverse set of problems: (i)\nsevere Poisson image denoising, (ii) Gaussian image denoising, (iii) image\ndeblurring, and (iv) single image super-resolution. In all these cases, we use\nrecent methods that handle these problems as initialization to our scheme. This\nis followed by an L-BFGS optimization of the above-described penalty function,\nleading to state-of-the-art results, and especially so for highly ill-posed\ncases.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 21:31:01 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Vaksman", "Gregory", ""], ["Zibulevsky", "Michael", ""], ["Elad", "Michael", ""]]}, {"id": "1602.08574", "submitter": "Luca Calatroni", "authors": "Luca Calatroni, Yves van Gennip, Carola-Bibiane Sch\\\"onlieb, Hannah\n  Rowland, Arjuna Flenner", "title": "Graph clustering, variational image segmentation methods and Hough\n  transform scale detection for object measurement in images", "comments": null, "journal-ref": null, "doi": "10.1007/s10851-016-0678-0", "report-no": null, "categories": "math.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of scale detection in images where a region of\ninterest is present together with a measurement tool (e.g. a ruler). For the\nsegmentation part, we focus on the graph based method by Flenner and Bertozzi\nwhich reinterprets classical continuous Ginzburg-Landau minimisation models in\na totally discrete framework. To overcome the numerical difficulties due to the\nlarge size of the images considered we use matrix completion and splitting\ntechniques. The scale on the measurement tool is detected via a Hough transform\nbased algorithm. The method is then applied to some measurement tasks arising\nin real-world applications such as zoology, medicine and archaeology.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2016 09:27:01 GMT"}, {"version": "v2", "created": "Fri, 23 Sep 2016 14:53:26 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Calatroni", "Luca", ""], ["van Gennip", "Yves", ""], ["Sch\u00f6nlieb", "Carola-Bibiane", ""], ["Rowland", "Hannah", ""], ["Flenner", "Arjuna", ""]]}, {"id": "1602.08575", "submitter": "James Murphy", "authors": "Wojciech Czaja, James M. Murphy, Daniel Weinberg", "title": "Superresolution of Noisy Remotely Sensed Images Through Directional\n  Representations", "comments": "5 pages (double column). IEEE copyright added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an algorithm for single-image superresolution of remotely sensed\ndata, based on the discrete shearlet transform. The shearlet transform extracts\ndirectional features of signals, and is known to provide near-optimally sparse\nrepresentations for a broad class of images. This often leads to superior\nperformance in edge detection and image representation when compared to\nisotropic frames. We justify the use of shearlets mathematically, before\npresenting a denoising single-image superresolution algorithm that combines the\nshearlet transform with sparse mixing estimators (SME). Our algorithm is\ncompared with a variety of single-image superresolution methods, including\nwavelet SME superresolution. Our numerical results demonstrate competitive\nperformance in terms of PSNR and SSIM.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2016 09:33:07 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 12:06:44 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Czaja", "Wojciech", ""], ["Murphy", "James M.", ""], ["Weinberg", "Daniel", ""]]}, {"id": "1602.08581", "submitter": "Rahul Radhakrishnan Iyer", "authors": "Rahul Radhakrishnan Iyer, Sanjeel Parekh, Vikas Mohandoss, Anush\n  Ramsurat, Bhiksha Raj, Rita Singh", "title": "Content-based Video Indexing and Retrieval Using Corr-LDA", "comments": "8 Pages, Updated References, Added Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing video indexing and retrieval methods on popular web-based multimedia\nsharing websites are based on user-provided sparse tagging. This paper proposes\na very specific way of searching for video clips, based on the content of the\nvideo. We present our work on Content-based Video Indexing and Retrieval using\nthe Correspondence-Latent Dirichlet Allocation (corr-LDA) probabilistic\nframework. This is a model that provides for auto-annotation of videos in a\ndatabase with textual descriptors, and brings the added benefit of utilizing\nthe semantic relations between the content of the video and text. We use the\nconcept-level matching provided by corr-LDA to build correspondences between\ntext and multimedia, with the objective of retrieving content with increased\naccuracy. In our experiments, we employ only the audio components of the\nindividual recordings and compare our results with an SVM-based approach.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2016 10:27:49 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 22:47:51 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Iyer", "Rahul Radhakrishnan", ""], ["Parekh", "Sanjeel", ""], ["Mohandoss", "Vikas", ""], ["Ramsurat", "Anush", ""], ["Raj", "Bhiksha", ""], ["Singh", "Rita", ""]]}, {"id": "1602.08680", "submitter": "Shangwen Li", "authors": "Shangwen Li, Sanjay Purushotham, Chen Chen, Yuzhuo Ren, and C.-C. Jay\n  Kuo", "title": "Measuring and Predicting Tag Importance for Image Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Textual data such as tags, sentence descriptions are combined with visual\ncues to reduce the semantic gap for image retrieval applications in today's\nMultimodal Image Retrieval (MIR) systems. However, all tags are treated as\nequally important in these systems, which may result in misalignment between\nvisual and textual modalities during MIR training. This will further lead to\ndegenerated retrieval performance at query time. To address this issue, we\ninvestigate the problem of tag importance prediction, where the goal is to\nautomatically predict the tag importance and use it in image retrieval. To\nachieve this, we first propose a method to measure the relative importance of\nobject and scene tags from image sentence descriptions. Using this as the\nground truth, we present a tag importance prediction model to jointly exploit\nvisual, semantic and context cues. The Structural Support Vector Machine (SSVM)\nformulation is adopted to ensure efficient training of the prediction model.\nThen, the Canonical Correlation Analysis (CCA) is employed to learn the\nrelation between the image visual feature and tag importance to obtain robust\nretrieval performance. Experimental results on three real-world datasets show a\nsignificant performance improvement of the proposed MIR with Tag Importance\nPrediction (MIR/TIP) system over other MIR systems.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 07:38:25 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 18:13:21 GMT"}, {"version": "v3", "created": "Mon, 9 Jan 2017 22:32:36 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Li", "Shangwen", ""], ["Purushotham", "Sanjay", ""], ["Chen", "Chen", ""], ["Ren", "Yuzhuo", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "1602.08761", "submitter": "Tolga Bolukbasi", "authors": "Tolga Bolukbasi, Kai-Wei Chang, Joseph Wang, Venkatesh Saligrama", "title": "Resource Constrained Structured Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of structured prediction under test-time budget\nconstraints. We propose a novel approach applicable to a wide range of\nstructured prediction problems in computer vision and natural language\nprocessing. Our approach seeks to adaptively generate computationally costly\nfeatures during test-time in order to reduce the computational cost of\nprediction while maintaining prediction performance. We show that training the\nadaptive feature generation system can be reduced to a series of structured\nlearning problems, resulting in efficient training using existing structured\nlearning algorithms. This framework provides theoretical justification for\nseveral existing heuristic approaches found in literature. We evaluate our\nproposed adaptive system on two structured prediction tasks, optical character\nrecognition (OCR) and dependency parsing and show strong performance in\nreduction of the feature costs without degrading accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 19:44:57 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 01:31:01 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Bolukbasi", "Tolga", ""], ["Chang", "Kai-Wei", ""], ["Wang", "Joseph", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1602.08855", "submitter": "Corneliu Florea", "authors": "Corneliu Florea, Razvan Condorovici, Constantin Vertan, Raluca Boia,\n  Laura Florea, Ruxandra Vranceanu", "title": "Pandora: Description of a Painting Database for Art Movement Recognition\n  with Baselines and Perspectives", "comments": "11 pages, 1 figure, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To facilitate computer analysis of visual art, in the form of paintings, we\nintroduce Pandora (Paintings Dataset for Recognizing the Art movement)\ndatabase, a collection of digitized paintings labelled with respect to the\nartistic movement. Noting that the set of databases available as benchmarks for\nevaluation is highly reduced and most existing ones are limited in variability\nand number of images, we propose a novel large scale dataset of digital\npaintings. The database consists of more than 7700 images from 12 art\nmovements. Each genre is illustrated by a number of images varying from 250 to\nnearly 1000. We investigate how local and global features and classification\nsystems are able to recognize the art movement. Our experimental results\nsuggest that accurate recognition is achievable by a combination of various\ncategories.To facilitate computer analysis of visual art, in the form of\npaintings, we introduce Pandora (Paintings Dataset for Recognizing the Art\nmovement) database, a collection of digitized paintings labelled with respect\nto the artistic movement. Noting that the set of databases available as\nbenchmarks for evaluation is highly reduced and most existing ones are limited\nin variability and number of images, we propose a novel large scale dataset of\ndigital paintings. The database consists of more than 7700 images from 12 art\nmovements. Each genre is illustrated by a number of images varying from 250 to\nnearly 1000. We investigate how local and global features and classification\nsystems are able to recognize the art movement. Our experimental results\nsuggest that accurate recognition is achievable by a combination of various\ncategories.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 08:24:01 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Florea", "Corneliu", ""], ["Condorovici", "Razvan", ""], ["Vertan", "Constantin", ""], ["Boia", "Raluca", ""], ["Florea", "Laura", ""], ["Vranceanu", "Ruxandra", ""]]}, {"id": "1602.08960", "submitter": "Roberto P. Palomares", "authors": "Roberto P. Palomares, Enric Meinhardt-Llopis, Coloma Ballester, Gloria\n  Haro", "title": "FALDOI: A new minimization strategy for large displacement variational\n  optical flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a large displacement optical flow method that introduces a new\nstrategy to compute a good local minimum of any optical flow energy functional.\nThe method requires a given set of discrete matches, which can be extremely\nsparse, and an energy functional which locally guides the interpolation from\nthose matches. In particular, the matches are used to guide a structured\ncoordinate-descent of the energy functional around these keypoints. It results\nin a two-step minimization method at the finest scale which is very robust to\nthe inevitable outliers of the sparse matcher and able to capture large\ndisplacements of small objects. Its benefits over other variational methods\nthat also rely on a set of sparse matches are its robustness against very few\nmatches, high levels of noise and outliers. We validate our proposal using\nseveral optical flow variational models. The results consistently outperform\nthe coarse-to-fine approaches and achieve good qualitative and quantitative\nperformance on the standard optical flow benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 13:54:39 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 16:55:12 GMT"}, {"version": "v3", "created": "Thu, 29 Sep 2016 11:38:52 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Palomares", "Roberto P.", ""], ["Meinhardt-Llopis", "Enric", ""], ["Ballester", "Coloma", ""], ["Haro", "Gloria", ""]]}, {"id": "1602.08977", "submitter": "Crist\\'obal Mackenzie", "authors": "Crist\\'obal Mackenzie, Karim Pichara, Pavlos Protopapas", "title": "Clustering Based Feature Learning on Variable Stars", "comments": null, "journal-ref": "ApJ 820 (2016) 138", "doi": "10.3847/0004-637X/820/2/138", "report-no": null, "categories": "astro-ph.SR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of automatic classification of variable stars strongly depends on\nthe lightcurve representation. Usually, lightcurves are represented as a vector\nof many statistical descriptors designed by astronomers called features. These\ndescriptors commonly demand significant computational power to calculate,\nrequire substantial research effort to develop and do not guarantee good\nperformance on the final classification task. Today, lightcurve representation\nis not entirely automatic; algorithms that extract lightcurve features are\ndesigned by humans and must be manually tuned up for every survey. The vast\namounts of data that will be generated in future surveys like LSST mean\nastronomers must develop analysis pipelines that are both scalable and\nautomated. Recently, substantial efforts have been made in the machine learning\ncommunity to develop methods that prescind from expert-designed and manually\ntuned features for features that are automatically learned from data. In this\nwork we present what is, to our knowledge, the first unsupervised feature\nlearning algorithm designed for variable stars. Our method first extracts a\nlarge number of lightcurve subsequences from a given set of photometric data,\nwhich are then clustered to find common local patterns in the time series.\nRepresentatives of these patterns, called exemplars, are then used to transform\nlightcurves of a labeled set into a new representation that can then be used to\ntrain an automatic classifier. The proposed algorithm learns the features from\nboth labeled and unlabeled lightcurves, overcoming the bias generated when the\nlearning process is done only with labeled data. We test our method on MACHO\nand OGLE datasets; the results show that the classification performance we\nachieve is as good and in some cases better than the performance achieved using\ntraditional features, while the computational cost is significantly lower.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 14:26:17 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Mackenzie", "Crist\u00f3bal", ""], ["Pichara", "Karim", ""], ["Protopapas", "Pavlos", ""]]}, {"id": "1602.09065", "submitter": "Amir Ghaderi", "authors": "Srujana Gattupalli, Amir Ghaderi, Vassilis Athitsos", "title": "Evaluation of Deep Learning based Pose Estimation for Sign Language\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human body pose estimation and hand detection are two important tasks for\nsystems that perform computer vision-based sign language recognition(SLR).\nHowever, both tasks are challenging, especially when the input is color videos,\nwith no depth information. Many algorithms have been proposed in the literature\nfor these tasks, and some of the most successful recent algorithms are based on\ndeep learning. In this paper, we introduce a dataset for human pose estimation\nfor SLR domain. We evaluate the performance of two deep learning based pose\nestimation methods, by performing user-independent experiments on our dataset.\nWe also perform transfer learning, and we obtain results that demonstrate that\ntransfer learning can improve pose estimation accuracy. The dataset and results\nfrom these methods can create a useful baseline for future works.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 17:45:10 GMT"}, {"version": "v2", "created": "Sun, 17 Apr 2016 16:56:41 GMT"}, {"version": "v3", "created": "Tue, 19 Apr 2016 23:43:10 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["Gattupalli", "Srujana", ""], ["Ghaderi", "Amir", ""], ["Athitsos", "Vassilis", ""]]}, {"id": "1602.09130", "submitter": "Abhineet Singh", "authors": "Abhineet Singh, Martin Jagersand", "title": "Modular Tracking Framework: A Unified Approach to Registration based\n  Tracking", "comments": "Under consideration at Computer Vision and Image Understanding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a modular, extensible and highly efficient open source\nframework for registration based tracking called Modular Tracking Framework\n(MTF). Targeted at robotics applications, it is implemented entirely in C++ and\ndesigned from the ground up to easily integrate with systems that support any\nof several major vision and robotics libraries including OpenCV, ROS, ViSP and\nEigen. It implements more methods, is faster, and more precise than other\nexisting systems. Further, the theoretical basis for its design is a new way to\nconceptualize registration based trackers that decomposes them into three\nconstituent sub modules - Search Method (SM), Appearance Model (AM) and State\nSpace Model (SSM).\n  In the process, we integrate many important advances published after Baker \\&\nMatthews' landmark work in 2004. In addition to being a practical solution for\nfast and high precision tracking, MTF can also serve as a useful research tool\nby allowing existing and new methods for any of the sub modules to be studied\nbetter. When a new method is introduced for one of these, the breakdown can\nhelp to experimentally find the combination of methods for the others that is\noptimum for it. By extensive use of generic programming, MTF makes it easy to\nplug in a new method for any of the sub modules so that it can not only be\ntested comprehensively with existing methods but also become immediately\navailable for deployment in any project that uses the framework. With 16 AMs,\n11 SMs and 13 SSMs implemented already, MTF provides over 2000 distinct single\nlayer trackers. It also allows two or more of these to be combined together in\nseveral ways to create a practically unlimited variety of novel multi layer\ntrackers.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 20:39:52 GMT"}, {"version": "v2", "created": "Fri, 25 Mar 2016 18:19:20 GMT"}, {"version": "v3", "created": "Wed, 28 Sep 2016 16:50:57 GMT"}, {"version": "v4", "created": "Fri, 18 May 2018 02:15:42 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Singh", "Abhineet", ""], ["Jagersand", "Martin", ""]]}]